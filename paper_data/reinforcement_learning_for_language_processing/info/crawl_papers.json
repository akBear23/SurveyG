[
  {
    "id": "0d1c76d45afa012ded7ab741194baf142117c495",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "authors": [
      "Rafael Rafailov",
      "Archit Sharma",
      "E. Mitchell",
      "Stefano Ermon",
      "Christopher D. Manning",
      "Chelsea Finn"
    ],
    "year": 2023,
    "citationCount": 5332,
    "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
    "url": "https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495",
    "pdf_url": "https://arxiv.org/pdf/2305.18290.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2023-05-29",
    "externalIds": {
      "DBLP": "conf/nips/RafailovSMMEF23",
      "ArXiv": "2305.18290",
      "DOI": "10.48550/arXiv.2305.18290",
      "CorpusId": 258959321
    },
    "references": [
      {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b",
        "title": "Aligning Language Models with Preferences through f-divergence Minimization"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "1e34c51b52002796fea6f523b9f794f1d75d9ba8",
        "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting"
      },
      {
        "paperId": "0a0f09d57ba4b5451e5c0d11397b8e437c38e9b9",
        "title": "Human Preferences as Dueling Bandits"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "53e22ad9fa53da0a1652a3fac2dc2735a55c67e4",
        "title": "Generalized Results for the Existence and Consistency of the MLE in the Bradley-Terry-Luce Model"
      },
      {
        "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
      },
      {
        "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
      },
      {
        "paperId": "774591fdd988eaaff3917e7c5171d044b0843e63",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
      },
      {
        "paperId": "4ba7f8624ff30beeb24a4fc6076b896b2944f555",
        "title": "Human-centric Dialog Training via Offline Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
        "title": "Neural Text Generation with Unlikelihood Training"
      },
      {
        "paperId": "15919637566348de8ca1e054151c24cc864b0f0e",
        "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "c72582122ff631117a05deb2aefa04b01362e3fa",
        "title": "Learning to Extract Coherent Summary via Deep Reinforcement Learning"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e27467ca84c5c1f7239a6e643843c1b97e35671f",
        "title": "Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
        "title": "A Deep Reinforced Model for Abstractive Summarization"
      },
      {
        "paperId": "a870df7e7d43c9144e2520ef4e4779f1672dd654",
        "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control"
      },
      {
        "paperId": "a639ed48f3235551f3327cd75b25391804cb8313",
        "title": "Learning Dynamic Robot-to-Human Object Handover from Human Feedback"
      },
      {
        "paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
        "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"
      },
      {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "cc2e61e06917238ca745155d9f1f0e679e63866e",
        "title": "Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm"
      },
      {
        "paperId": "dc4aba92e28c069ab5fac509abba558e80a6ae4a",
        "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
        "title": "Et al"
      },
      {
        "paperId": "1f869232f148ec52066fab06a49855937f84098b",
        "title": "Reinforcement learning by reward-weighted regression for operational space control"
      },
      {
        "paperId": "666010dc3621e1a08fb6830f03a1f7857c7377cc",
        "title": "I and i"
      },
      {
        "paperId": "f8222304ca201f8d524b3aa270673023334e7ed1",
        "title": "Individual Choice Behavior: A Theoretical Analysis"
      },
      {
        "paperId": "1a4e67d3705492d0af88623b0e62818a16084fca",
        "title": "The Analysis of Permutations"
      },
      {
        "paperId": "d244943df2e046157057220e46b64a868fed3549",
        "title": "Individual Choice Behavior: A Theoretical Analysis."
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "CarperAI/trlx: v0.6.0: LLaMa (Alpaca), Benchmark Util"
      },
      {
        "paperId": "7fc09c81b43e25834a763b421c1a8b3868ecd6ab",
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study"
      },
      {
        "paperId": "fe0825f9ddb1cccb545f4249da55b6b55e577bbd",
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
      },
      {
        "paperId": null,
        "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "3d2218b17e7898a222e5fc2079a3f1531990708f",
        "title": "I and J"
      },
      {
        "paperId": null,
        "title": "Direct Preference Optimization: Your Language Model science/article/pii/S0022000012000281"
      },
      {
        "paperId": "ca5cb4e0826b424adb81cbb4f2e3c88c391a4075",
        "title": "Influence of cultivation temperature on the ligninolytic activity of selected fungal strains"
      },
      {
        "paperId": "5a554c8d22d47ac499aeb7fb0532ca9be65e5a2e",
        "title": "A. and Q"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "5c1b3dc95d1ba44fd7d9e3f6dec60fdc46a0eb78",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS"
      },
      {
        "paperId": null,
        "title": "B DPO Implementation Details and Hyperparameters DPO is relatively straightforward to implement; PyTorch code for the DPO loss"
      },
      {
        "paperId": null,
        "title": "Harmlessness from ai"
      },
      {
        "paperId": null,
        "title": "A suite for"
      }
    ],
    "cited_by": [
      {
        "paperId": "6b84834a987678b448fb0a462ae73ea526899047",
        "title": "Dynamic Batch Processing with FlexiDecode Scheduler for Efficient LLM Inference in IIoT"
      },
      {
        "paperId": "0e5529b049363a23690c43c2afb6f2a69585c4f3",
        "title": "Triad of three system: AI-driven hydraulics for autonomous electric excavators with sustainable energy savings"
      },
      {
        "paperId": "216e49a9b865624cd0ac6421c916d530fd1582cc",
        "title": "Law LLM unlearning via interfere prompt, review output and update parameter: new challenges, method and baseline"
      },
      {
        "paperId": "cc35a02997cc27d6078d11ec29f5e93792507dc0",
        "title": "A deep reinforcement learning framework for optimized dummy pad placement in PCB electroplating"
      },
      {
        "paperId": "2e1c404013c334856bcc6bdafe02f4091ce63ef2",
        "title": "InvThink: Towards AI Safety via Inverse Reasoning"
      },
      {
        "paperId": "2f11fdb66ca5d382f3b0ca763c2d13cde9e6f522",
        "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models"
      },
      {
        "paperId": "2802bf5fb28e38e6586698fd46b625f88c6d30f9",
        "title": "Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO"
      },
      {
        "paperId": "69d3a593b14b22479cac879bf1dbf87e0f78e00f",
        "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning"
      },
      {
        "paperId": "949b38448641e2db90865743b714fdecbf221a7b",
        "title": "Predictive Preference Learning from Human Interventions"
      },
      {
        "paperId": "cca984474ed2c423eefcc4de4fd393baad65d7e7",
        "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning"
      },
      {
        "paperId": "31f5b1e34c83329dd618768cf9290f10ef626af8",
        "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation"
      },
      {
        "paperId": "27f6531af74b0aee04a572884d908421f03881ad",
        "title": "Towards Better Optimization For Listwise Preference in Diffusion Models"
      },
      {
        "paperId": "792ff877a38a72a6b647639c85dd4a30e7ffe795",
        "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"
      },
      {
        "paperId": "29c3368b63f7aa43d7e1b4928cc8a1a1e5693610",
        "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning"
      },
      {
        "paperId": "2ae2e468bb2abe78507a4336df8573563d025b51",
        "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language Models"
      },
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "6e579f5bc5e0da2d654e50e2f0080fbf7ec2f290",
        "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems"
      },
      {
        "paperId": "34e6cc059b06730d1bcfe6afa2997bb8c5154969",
        "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models"
      },
      {
        "paperId": "6d3646d4ebe0c3ab7c9af63589a93e6c57d504dc",
        "title": "It Takes Two: Your GRPO Is Secretly DPO"
      },
      {
        "paperId": "a836ce4029ab78abf918b02b0e3bf5e22a0367e1",
        "title": "Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations"
      },
      {
        "paperId": "cde02dfa5c320ba84050176a4f97722704bc02d4",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense"
      },
      {
        "paperId": "4ff732542674f8b122c35f9d9ab77ce12f30d01b",
        "title": "POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency"
      },
      {
        "paperId": "790af73b0cc0f44f2cbe2b2262b9f612ec3fe146",
        "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment"
      },
      {
        "paperId": "72ad3fe187a7c9a3dcd7867aa5f80ef88849a1df",
        "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation"
      },
      {
        "paperId": "daab8977d0d2a0083f62423d984ba5abef5bffdb",
        "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs"
      },
      {
        "paperId": "b5e6a38593d5a6b55ec88b9f69a45a0f1526273b",
        "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
      },
      {
        "paperId": "31eca0d1ea5f1f291f210bf67fb9f2d77c031a37",
        "title": "Prompt Curriculum Learning for Efficient LLM Post-Training"
      },
      {
        "paperId": "b9c99443a5cf130f8b043f451a2f519cb8726d7b",
        "title": "Automated Structured Radiology Report Generation with Rich Clinical Context"
      },
      {
        "paperId": "6d1f87bb781c0a21f873a66eff6cdd7968c55d9b",
        "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning"
      },
      {
        "paperId": "7e3052358519a9c211eec305b7074c061d42c669",
        "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards"
      },
      {
        "paperId": "a0257b94224ee92466fd682af58313bad85e8b48",
        "title": "Pay-Per-Search Models are Abstention Models"
      },
      {
        "paperId": "1839e9f4637a2467a0de5f80e4fae1a4e721dc3f",
        "title": "Copy-Paste to Mitigate Large Language Model Hallucinations"
      },
      {
        "paperId": "5be2fb5828eb2f9fa1d0ba5a4a25e8a28f4950c5",
        "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation"
      },
      {
        "paperId": "b79dc2c52715be17ee1e942d0f8124165e0cba5e",
        "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability"
      },
      {
        "paperId": "eb87971a52b2289752afc7d4a3bcfbba6d5bd430",
        "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning"
      },
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "f19ad49cf23159283e6d700ef84c6fc14e183c21",
        "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models"
      },
      {
        "paperId": "938d8b5ee7ab003204a8bd0babdbf07da504361a",
        "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach"
      },
      {
        "paperId": "91691e13ee8038fa29cce9fc26994c6396a6e8da",
        "title": "Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization"
      },
      {
        "paperId": "8eb59ef73f82b73a7a502aa7d4942995b30d3d3d",
        "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses"
      },
      {
        "paperId": "3bc58da5b0188d3306a2c6721163e6543295b787",
        "title": "Interactive Learning for LLM Reasoning"
      },
      {
        "paperId": "2675882539acfa91336591eb1cecca015e1d1c0f",
        "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs"
      },
      {
        "paperId": "41f3208b965effd7d7ac37eb5e81fbd0cf7da07f",
        "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap"
      },
      {
        "paperId": "052e8e4a9cfd766aa07550a10778514c54e3dba6",
        "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"
      },
      {
        "paperId": "723b4e265c97b826e517df48ffb3122267b63839",
        "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning"
      },
      {
        "paperId": "22ccadf6c4b9def722bf84a3c2fe9984ade3a349",
        "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient"
      },
      {
        "paperId": "bc5f12f5697967e5696297737a1980392646bf04",
        "title": "Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation"
      },
      {
        "paperId": "028dc666c1c1362e5b4d060d311af862764055a2",
        "title": "RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation"
      },
      {
        "paperId": "54936a36fead748c0cb8a84933a3967609928838",
        "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning"
      },
      {
        "paperId": "ea7960535d0854a65a44f58ef7dcc5fdeeaba110",
        "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection"
      },
      {
        "paperId": "e0033b0929dc733dde571dd2cad11c8b14accaf8",
        "title": "Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition"
      },
      {
        "paperId": "51fa6682984b9a8b3085cdd3d2918444317ef3ee",
        "title": "Alignment-Aware Decoding"
      },
      {
        "paperId": "6cd762a0856b904dc27cc450711ae48fbc93e784",
        "title": "Fading to Grow: Growing Preference Ratios via Preference Fading Discrete Diffusion for Recommendation"
      },
      {
        "paperId": "40d5a9a99d4f71eb3380e4ed0a114990bebd9103",
        "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models"
      },
      {
        "paperId": "d56cfecbf01fd180e1b5084ec6bfc3a3bebff425",
        "title": "Noise-Guided Transport for Imitation Learning"
      },
      {
        "paperId": "3a862efc1eac2ac9c1aa6ba04266032d210f7f49",
        "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts"
      },
      {
        "paperId": "c69e6cd69732244dff92574aa4b7ddec69396b69",
        "title": "SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation"
      },
      {
        "paperId": "19a4f46f1e15e7b232dcf61f1232f8f856e5b5ad",
        "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space"
      },
      {
        "paperId": "e069517326e61e6bec38391c4cc0115982301ed8",
        "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse"
      },
      {
        "paperId": "adcf542438d49fc056e2c78eec19ef99ca8adc45",
        "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning"
      },
      {
        "paperId": "ef985c29ff4fb41dbff7c19ec0873a2f900a9cc3",
        "title": "Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis"
      },
      {
        "paperId": "42516e61567cdc719008efcd6fabb344d95c0634",
        "title": "AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation"
      },
      {
        "paperId": "3b332788604c4ce0503a7cec6f13c78c162c937a",
        "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions"
      },
      {
        "paperId": "12c799339cc26da96892e4541b598ce3e2f35bcc",
        "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning"
      },
      {
        "paperId": "059a907c71b5daab58788c17b65e918b8d882281",
        "title": "Nudging the Boundaries of LLM Reasoning"
      },
      {
        "paperId": "7ae8684ce048e14d2694f457df0f1aff01085a3b",
        "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models"
      },
      {
        "paperId": "ce76d6d4ba37e0fc677c26079b36643c49a7879c",
        "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack"
      },
      {
        "paperId": "9bd65cd83dc04896b7de2e60982792ccfd0ab879",
        "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation"
      },
      {
        "paperId": "94db68629d38d65ce4cd46e2b9c14f2de823d600",
        "title": "GRPO-$\\lambda$: Credit Assignment improves LLM Reasoning"
      },
      {
        "paperId": "b1fd96cbcb1ec824dbca9b1e1bb5ddaaab159baf",
        "title": "Operationalizing machine-assisted translation in healthcare"
      },
      {
        "paperId": "dacb8fd9a524eca1ec31e71a7b67d40e50c556cf",
        "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs"
      },
      {
        "paperId": "62eb8c2a040f9891e85215f89bee8e50851ab937",
        "title": "UI-UG: A Unified MLLM for UI Understanding and Generation"
      },
      {
        "paperId": "49d8b888a6061e84850ad300c37bfa95489c8aa1",
        "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention"
      },
      {
        "paperId": "a9b98746f9da39b5935d0f99ef3983bbe1cfbf01",
        "title": "Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling"
      },
      {
        "paperId": "57ac1724f5ac90c2c002d5515c202dde4524a7ce",
        "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models"
      },
      {
        "paperId": "0a0e608a20f2cebce8911368573280a2b897b1b0",
        "title": "Understanding the Dilemma of Unlearning for Large Language Models"
      },
      {
        "paperId": "068080a0ce76f2385b34f07d289a61550ad553d9",
        "title": "Prompt and Parameter Co-Optimization for Large Language Models"
      },
      {
        "paperId": "2ec727753b0eede7d663ef6a1dee17814e882084",
        "title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization"
      },
      {
        "paperId": "aec4dd8050a17c66afe4785dc571af2c18c00195",
        "title": "Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment"
      },
      {
        "paperId": "7db85f8f5d173067e9a0aee4f29355c375632ffa",
        "title": "T-POP: Test-Time Personalization with Online Preference Feedback"
      },
      {
        "paperId": "50edc85668afdda254ce4ba7703aa10f41a77fcb",
        "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training"
      },
      {
        "paperId": "21b81883c1f4c789fec33d239ef173ea51b73918",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"
      },
      {
        "paperId": "e3e026e105e209a5d8c53b50427977e3b957a5b1",
        "title": "Reinforcement Mid-Training"
      },
      {
        "paperId": "e28585ddb995bce6c3af5f49661a1b636cbacf14",
        "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning"
      },
      {
        "paperId": "498a98386d007a2fdb88fabcd5f13b1f972697fc",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression"
      },
      {
        "paperId": "0f90d121c86218c295559ac8bb7705e36b67a89e",
        "title": "Can Large Language Models Express Uncertainty Like Human?"
      },
      {
        "paperId": "8a748400806e2c97739b5129ef5dbc87ca261588",
        "title": "Rethinking and Benchmarking Large Language Models for Graph Reasoning"
      },
      {
        "paperId": "4f36b972658bebe1dda299fbdf8c8e468b777935",
        "title": "Rethinking Entropy Regularization in Large Reasoning Models"
      },
      {
        "paperId": "d162c0f1682ca88fdc07bdbfa429b245ee51f4d7",
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment"
      },
      {
        "paperId": "690a3e101091bd74c51ef7acf75a9ee0e8eeac5d",
        "title": "Reference-Free Rating of LLM Responses via Latent Information"
      },
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "84a0418080d639df90a1a7219dcb05fdc847c133",
        "title": "Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs"
      },
      {
        "paperId": "c50c3be47d24af9110b4e821b71c06d718b03707",
        "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play"
      },
      {
        "paperId": "08d60a8f1169eb65c120b67a1b6b8a8c448f9913",
        "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution"
      },
      {
        "paperId": "6a88469b41fb025b4e37326189e5145db4bb30f4",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following"
      },
      {
        "paperId": "f43739359a51fe54c5b9d18b7b62595e3c88c1c4",
        "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends"
      },
      {
        "paperId": "5b3f710d1923323a0113f6db056aa712a04b0f13",
        "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling"
      },
      {
        "paperId": "12590b05dbcf0674b20f46a6336470afa02958a1",
        "title": "The Era of Real-World Human Interaction: RL from User Conversations"
      },
      {
        "paperId": "b0b692bd662cc54c9925bd9fb30c4458ab6bb6e4",
        "title": "Probing the Limits of Stylistic Alignment in Vision-Language Models"
      },
      {
        "paperId": "d41634b4a485ab8f29698d224ef7c14282263284",
        "title": "RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance"
      },
      {
        "paperId": "f0301bcbdc87883911ffef88a0a970bcc601895c",
        "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models"
      },
      {
        "paperId": "a97637b75539e134aa9b937d53291bc11520577d",
        "title": "Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization"
      },
      {
        "paperId": "64d404acac3287062c74f834c71e9064c75bfd72",
        "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality"
      },
      {
        "paperId": "0e595af8fe01a95136bd513f7a8b426425489c41",
        "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "de595f98909a561d0adc9958648e4be6539cbde7",
        "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation"
      },
      {
        "paperId": "cf8e1d1ff1b1d58ddda32d7bfc7d419bdb908d84",
        "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning"
      },
      {
        "paperId": "aa475f7900389f242dbe344c46e3474043759398",
        "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm"
      },
      {
        "paperId": "cb69c6cc6fefbe8eb1d51efea7e386dc79e82eca",
        "title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM"
      },
      {
        "paperId": "232f1332c85b96a06d59442260d81728a6fabcba",
        "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs"
      },
      {
        "paperId": "fd6797069bc52f1ff036cf1453adc665eda339f9",
        "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications"
      },
      {
        "paperId": "66efa648fe9162022e7d644cb3d07a62d437f17c",
        "title": "Fast Thinking for Large Language Models"
      },
      {
        "paperId": "eea7e72094cb567ef16e59601f4d907a927af5d5",
        "title": "AudioMoG: Guiding Audio Generation with Mixture-of-Guidance"
      },
      {
        "paperId": "81696e83b871ad9c24109cba8f79d1b75003f0f0",
        "title": "Anchored Supervised Fine-Tuning"
      },
      {
        "paperId": "c024b621d8fbb4c489d6a0c6980e647489c1a509",
        "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation"
      },
      {
        "paperId": "39088601f53158d14d8fca88fea4408ecc577f88",
        "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks"
      },
      {
        "paperId": "01ea6a959b2efd8d2717d2d0b9c68f95ecaabd3f",
        "title": "PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents"
      },
      {
        "paperId": "682d413e6cce94d8731703472509781c707d63f6",
        "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning"
      },
      {
        "paperId": "1fb69e7d61e2931b106490b9ac6b1d83daed0d04",
        "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering"
      },
      {
        "paperId": "b7a8c74e52ae3eae896aa6e71ba56547d5232664",
        "title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning"
      },
      {
        "paperId": "97bd4982ea62976b4fad419892ef260556305df4",
        "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step"
      },
      {
        "paperId": "37767cd0142a93e37e40efdab391ad7d6c86f2a6",
        "title": "LLM/Agent-as-Data-Analyst: A Survey"
      },
      {
        "paperId": "13b7fd1a09c08196f19df782906dfab49500d005",
        "title": "Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE"
      },
      {
        "paperId": "ee448ae2404d087a5cb69cf022a1393b04fc50ff",
        "title": "Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection"
      },
      {
        "paperId": "3a6dc3516c20a45062564e67d742a567674865bf",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization"
      },
      {
        "paperId": "f55bf9a4262ca00b72345ee95b0b1ad65b907e95",
        "title": "Multiplayer Nash Preference Optimization"
      },
      {
        "paperId": "da7b4ac104c8937a6f7bd3041ed2e25aec4c5335",
        "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction"
      },
      {
        "paperId": "5becda4f5dd8eca7c2f9990abcd4b1b2543ead2b",
        "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks"
      },
      {
        "paperId": "8b0bdb58ee114bf0ef571f8f52e55465a35b4aba",
        "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning"
      },
      {
        "paperId": "52a1be191c65aff77164bc4d89c98e24a5fed2a3",
        "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning"
      },
      {
        "paperId": "b5939cba32a1c1c6a6c3418b7cba16dbcafb8850",
        "title": "A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models"
      },
      {
        "paperId": "b0b668b142d232d102b4cf105965651b7e1fea7c",
        "title": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training"
      },
      {
        "paperId": "682fc9305857a966a0ea137bbd02800b2aa3ce18",
        "title": "Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces"
      },
      {
        "paperId": "c89873a13ec6ed1c2ac70806fa30ad19f7be6ebf",
        "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs"
      },
      {
        "paperId": "fd140dc3962ff37f82c61634bc359d65aac2a1a5",
        "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models"
      },
      {
        "paperId": "fab2806bcacb56ecdc7cb1df36f1ef949876346f",
        "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts"
      },
      {
        "paperId": "5b5f46c2afe1617a090e5f54f1514c90a404819d",
        "title": "Follow-Your-Preference: Towards Preference-Aligned Image Inpainting"
      },
      {
        "paperId": "b23591aefc039307fda9d50d8494769bf4d28420",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models"
      },
      {
        "paperId": "7f72fc249b4eca634b25b9a3263422adfe607491",
        "title": "POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization"
      },
      {
        "paperId": "df657b5690774b05ce7f0e06542c29ac7c094e1d",
        "title": "Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?"
      },
      {
        "paperId": "24b432d44677f1640c34752d277578f9d8598f76",
        "title": "ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation"
      },
      {
        "paperId": "83545b7fe1ac8d0828b52eb532f6781c64a0d814",
        "title": "Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning"
      },
      {
        "paperId": "4017b4a752a1693751855aed11b339c0815175d7",
        "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning"
      },
      {
        "paperId": "1dfb741d7bde530a67a4a4af9f8fea09b3bdebc4",
        "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning"
      },
      {
        "paperId": "cbb066e3225ba9246879fe4c670757e87963c806",
        "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety"
      },
      {
        "paperId": "51357ff32df3924f29e550aaf71c9882c2b99348",
        "title": "Variational Reasoning for Language Models"
      },
      {
        "paperId": "df0a07beb75eea9c71ca70344726e5c822974c40",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
      },
      {
        "paperId": "4bf87fe3052bf77e29c1d4028702c3975d8fd292",
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning"
      },
      {
        "paperId": "18619335bd6efbb756b8a441656a549c5eb7cd53",
        "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models"
      },
      {
        "paperId": "97307df78c99c73a52c0c01f847b5b3d4f235756",
        "title": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong"
      },
      {
        "paperId": "d3fc025556c6a80e105bd92029ca525f1c054e7e",
        "title": "Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization"
      },
      {
        "paperId": "22542f706dd0a41b0139ba2da54caa45c7481a61",
        "title": "Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching"
      },
      {
        "paperId": "d146d65bf8b67573aa481d0c88bbb5288b108fd3",
        "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance"
      },
      {
        "paperId": "0ff3252cf9ecb186ee8ec414fa1af893f6813fe2",
        "title": "Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models"
      },
      {
        "paperId": "1391f84c5232c8e6104eb30046276827b11e02c4",
        "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors"
      },
      {
        "paperId": "73a1f917bd0888429e9125154c32e16651bf9e9b",
        "title": "SoK: Potentials and Challenges of Large Language Models for Reverse Engineering"
      },
      {
        "paperId": "c7f25a14be13d3991ceaec31bba2bd5f36fb1ba3",
        "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment"
      },
      {
        "paperId": "4cba3bd7a32f7ef1ab33383dc02e79dee8cb69fe",
        "title": "Adaptive Margin RLHF via Preference over Preferences"
      },
      {
        "paperId": "2597daec80cbdbb8ae6f6e68abb273ee7d4d3151",
        "title": "Compute-Optimal Quantization-Aware Training"
      },
      {
        "paperId": "4649075949f698c4c0ea4a46cab4ed6c99aecc5c",
        "title": "What If Moderation Didn't Mean Suppression? A Case for Personalized Content Transformation"
      },
      {
        "paperId": "5ef246ec5941eeb61e532e0d8b9bdf18617a8222",
        "title": "Review of Hallucination Understanding in Large Language and Vision Models"
      },
      {
        "paperId": "27520c450f20e202897a1a3333d10b4fc36f84ab",
        "title": "SeamCrafter: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning"
      },
      {
        "paperId": "c5d95792a925bcab0bf0a0b80e96d792bea6eddd",
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training"
      },
      {
        "paperId": "ba6ee25a8263c4f2abe789b58623a035a6ba2f5c",
        "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models"
      },
      {
        "paperId": "de3892b98ecbb5ada62ed70004f00eb2c0386755",
        "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering"
      },
      {
        "paperId": "46d9c43dd937a9f7f2838993cf8b08367732329f",
        "title": "Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models"
      },
      {
        "paperId": "e8b056326e53a977ac59f846488ecefac18b5107",
        "title": "Who's Laughing Now? An Overview of Computational Humour Generation and Explanation"
      },
      {
        "paperId": "6604adee8f0ef432438eeb979ff61ac4885c0694",
        "title": "Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos"
      },
      {
        "paperId": "8669fee07075ff75275afdac933f111af1cbdb4d",
        "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement"
      },
      {
        "paperId": "1e4d9a121182fbf25ec662372b11b6aa7f46f8bf",
        "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute"
      },
      {
        "paperId": "146ded602ca4aba6173643186dd574beaeca9a1c",
        "title": "LLM Output Homogenization is Task Dependent"
      },
      {
        "paperId": "05810924337da1a485ded97462ba78b2b5a43f5f",
        "title": "FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction"
      },
      {
        "paperId": "0084a3b604df9d475e46e3124a6c3a3f4fc02a0f",
        "title": "Actor-Critic without Actor"
      },
      {
        "paperId": "154528bde840e4fd9c8596a5ff6759bc794d959c",
        "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips"
      },
      {
        "paperId": "5ab7596a76b196cf6b939e93d138febd3f84fec0",
        "title": "GRPO is Secretly a Process Reward Model"
      },
      {
        "paperId": "805a01c2a7ca3a7ca2d2ce49b2a26ebb618fae8d",
        "title": "Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation"
      },
      {
        "paperId": "502abc9d3854fe2d8e5b320ce6c43032869d5917",
        "title": "Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes"
      },
      {
        "paperId": "548c88450cf7b7e9e98d50ccea9ac9effb14731b",
        "title": "Preemptive Detection and Steering of LLM Misalignment via Latent Reachability"
      },
      {
        "paperId": "2cb07d5dcf45d448f6c0f3c58b4dadc4b04ed08b",
        "title": "Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration"
      },
      {
        "paperId": "002d1892d04fb43678c64837db53fd2f0b58608d",
        "title": "DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models"
      },
      {
        "paperId": "60c1a310b9be66b9319ba33fbba2f2ed02b02e1c",
        "title": "Correct Reasoning Paths Visit Shared Decision Pivots"
      },
      {
        "paperId": "d486351aa606403c2872317325a4ccb74c53b28b",
        "title": "QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models"
      },
      {
        "paperId": "a37d8826c4ba0e44b7044c15e546f2814b62f3b4",
        "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Automated Log Analysis"
      },
      {
        "paperId": "9ae73fb066a4d577bce84c42ec285a05f0b24c46",
        "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases"
      },
      {
        "paperId": "00965f7fc184245a81e1308a25e9233fe7f94a82",
        "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement"
      },
      {
        "paperId": "85359dcfea8498a0119d24d4f84f4f52e64a971d",
        "title": "Future Policy Aware Preference Learning for Mathematical Reasoning"
      },
      {
        "paperId": "15eb51700fcea706cc33a9c67be1a6e961f95087",
        "title": "Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing"
      },
      {
        "paperId": "5bfdc996e4c01e94924dfb8f79178dfbc10c2475",
        "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation"
      },
      {
        "paperId": "81a580291165fcd42f0fa1c6b942c147adf81068",
        "title": "EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation"
      },
      {
        "paperId": "73a5e07245dedea593124ce9ed83edf1aba2857a",
        "title": "MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via SlowFast Contrastive Audio-Visual Pretraining and Direct Preference Optimization"
      },
      {
        "paperId": "15f77ae9912f9e289361298753707c12af961fad",
        "title": "bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs"
      },
      {
        "paperId": "0cb6c5363710a24d767c0da0d6be962925a11d17",
        "title": "ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection"
      },
      {
        "paperId": "8524f08fffe66f1fa5798598dfe227a8763ca7e4",
        "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs"
      },
      {
        "paperId": "ed6b459adb19542a20b096bb6d51b82d87e82e83",
        "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage"
      },
      {
        "paperId": "89d01a6cd69161b1fba09dc4a2bed062497e4000",
        "title": "Failure Modes of Maximum Entropy RLHF"
      },
      {
        "paperId": "a81c481189ec4719b80ceec31f7c0277ba782d4a",
        "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning"
      },
      {
        "paperId": "9f251ca2f98310e80854d751683baf9466222fc7",
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning"
      },
      {
        "paperId": "114d36118ab5ce4b06c63fd95cd79d86aef40f85",
        "title": "SIM-CoT: Supervised Implicit Chain-of-Thought"
      },
      {
        "paperId": "8e1841ec0a3f04da54021b1e57a744ad76d02401",
        "title": "ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools"
      },
      {
        "paperId": "2e3d2cb75be2d8939387f9173b68cd81315ec694",
        "title": "Agentic Reinforcement Learning with Implicit Step Rewards"
      },
      {
        "paperId": "ca57d5173fdec1a2d277ed282a650788105864f7",
        "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS"
      },
      {
        "paperId": "dac3786fcf23a447babf1f20aa5b257792dbb3a4",
        "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users"
      },
      {
        "paperId": "f73b08b8b557ef8cd0f4dff19819b707628f5622",
        "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World"
      },
      {
        "paperId": "e3eccae50df00df046262b119051c8db21ba29a5",
        "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation"
      },
      {
        "paperId": "239769a09308ba63fe15a35b5d914670e0c38565",
        "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system"
      },
      {
        "paperId": "095877ca77a899eaa9aebb82a88d953ea94dfb6f",
        "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization"
      },
      {
        "paperId": "a6478bab4f1985f3eb008e7ebca4a84a1af273cd",
        "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generation"
      },
      {
        "paperId": "211c52ab5f4c3cb26850273eff68a4a5fe45f6f8",
        "title": "Group Relative Policy Optimization for Text-to-Speech with Large Language Models"
      },
      {
        "paperId": "84f1aa49626a82501631cc0f2f1a7c9768ad7f1c",
        "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection"
      },
      {
        "paperId": "1370ba4aded88eb801a855f532ec65a09044d503",
        "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing"
      },
      {
        "paperId": "282c770cf6d180a5c1bf56cf358727cb25d1d83d",
        "title": "Direct Preference Optimization for Speech Autoregressive Diffusion Models"
      },
      {
        "paperId": "7f1cdaad0af5a80c509ebb00590f6ee9599bf882",
        "title": "MAPO: Mixed Advantage Policy Optimization"
      },
      {
        "paperId": "b44aed7834786add30869a5305d3a6b3512a42a4",
        "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities"
      },
      {
        "paperId": "a8bf6fd51e10215ca71db02becc7a59b67a8df84",
        "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning"
      },
      {
        "paperId": "529ab5c32b37c479658afa3a616a5aab7c04051a",
        "title": "Synthesizing Artifact Dataset for Pixel-level Detection"
      },
      {
        "paperId": "44d075e6fdba36898f4ce74686d76589c25dfe4b",
        "title": "Discovery of Sustainable Refrigerants through Physics-Informed RL Fine-Tuning of Sequence Models"
      },
      {
        "paperId": "5e4cc3556497e4e1ebdd3535dd5fcb1ffcf3a665",
        "title": "Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis"
      },
      {
        "paperId": "8568b39236d551b8d65f2917b3659f34c5c41f1b",
        "title": "GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings"
      },
      {
        "paperId": "7dbbd99d3a6eb031ccb17efff85e60c5c60620c4",
        "title": "LANCE: Exploration and Reflection for LLM-based Textual Attacks on News Recommender Systems"
      },
      {
        "paperId": "8072c53bba037822d189015161c3299a713e47a4",
        "title": "Personalized Image Generation for Recommendations Beyond Catalogs"
      },
      {
        "paperId": "1a95fe64fb232102ebbb699221e5b74dbbf2522d",
        "title": "Diagnosing Model Editing via Knowledge Spectrum"
      },
      {
        "paperId": "edfa18a4dd07afe54d18bab5c0dd3e24596ef9ef",
        "title": "Qwen3-Omni Technical Report"
      },
      {
        "paperId": "60c55a3c704f3b110acb508f814ba4ddbebeffe7",
        "title": "Understanding Post-Training Structural Changes in Large Language Models"
      },
      {
        "paperId": "ed68534d5a8da7b097b1a6ec4ddb7b24a185a361",
        "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving"
      },
      {
        "paperId": "555b532aa0adaa8cab4b6842411e1089a6f6f8d3",
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs"
      },
      {
        "paperId": "c146865abb31c94a6d39d1f22c377d72d4911911",
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models"
      },
      {
        "paperId": "8237f2fc77f3c4b21d3e5c85acb9ee70ed1ba2b8",
        "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling"
      },
      {
        "paperId": "2007b767d8aeeb23290b76e2b9eb44e1a272b5a6",
        "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs"
      },
      {
        "paperId": "f48a15818b939ed500c34979855bed43f60831c9",
        "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning"
      },
      {
        "paperId": "da3f08f19659afa25420bbcea01476e8ea251051",
        "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback"
      },
      {
        "paperId": "8d5cd289121b63f6fed8415a1b755d52f502386d",
        "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification"
      },
      {
        "paperId": "03ab13535fca864079fc3b2dac80b23e845e25d2",
        "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge"
      },
      {
        "paperId": "58a0897596fc25862f6b09ceb0a45ec3887083ff",
        "title": "Preference Distillation via Value based Reinforcement Learning"
      },
      {
        "paperId": "865da3faec0bd4aea4e56b9252149fe221309746",
        "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization"
      },
      {
        "paperId": "cf946cf34282fa640346c6daf2c70a651d7daeda",
        "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization"
      },
      {
        "paperId": "8f14daa21d2d5f60f5a745e3b1318f7b3ff8b915",
        "title": "From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?"
      },
      {
        "paperId": "21c9c8720597fe96784d024a30f1a8d00af1e752",
        "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO"
      },
      {
        "paperId": "81294508893b253564c821bb8d489e1192a00d30",
        "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation"
      },
      {
        "paperId": "b3752af956c391cbae414f6c89e2e14e3baa7db6",
        "title": "Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models"
      },
      {
        "paperId": "f6d55b6c7889d210026e5e47bcc5b12e1899a226",
        "title": "Towards Universal Debiasing for Language Models-based Tabular Data Generation"
      },
      {
        "paperId": "a01ffcc6ac878598e4893103587a66acee959ae7",
        "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning"
      },
      {
        "paperId": "4d7ac4cc98e25d366be3d4781ad29e7813d412b1",
        "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation"
      },
      {
        "paperId": "79a3998277b31ae74af82f02c9dd4221be561a7a",
        "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "8273b825a4c8ec77f8ac9ab379daafedea08554b",
        "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach"
      },
      {
        "paperId": "5cc118820dba8b7fec9fce6e08b620432cb881f8",
        "title": "Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports"
      },
      {
        "paperId": "1236eee4339a0a329d48316075bd413d7dc03247",
        "title": "Relevance to Utility: Process-Supervised Rewrite for RAG"
      },
      {
        "paperId": "104a7e7deab637927b434d6ff3958cdd28ce9af1",
        "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning"
      },
      {
        "paperId": "345500d43c91f618e2f735269c23dc9324c75fd7",
        "title": "EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition"
      },
      {
        "paperId": "36e93518830324afd5634bce2a77bd89f0275dea",
        "title": "Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment"
      },
      {
        "paperId": "05853b456e95243671d0fbae66cb6b9e3e56000b",
        "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation"
      },
      {
        "paperId": "98605c7b7c5f4a56f55b8b0862f6d281f16f9e0c",
        "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses"
      },
      {
        "paperId": "657b5236760a233605ac3a70eec2ed17bf487279",
        "title": "Rethinking Molecule Synthesizability with Chain-of-Reaction"
      },
      {
        "paperId": "8b9d06cdd4a17d32ff21ddd79907d15fe6560089",
        "title": "The Singing Voice Conversion Challenge 2025: From Singer Identity Conversion To Singing Style Conversion"
      },
      {
        "paperId": "23451fcdd9ea0011920be91011c10890af586ef1",
        "title": "siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer"
      },
      {
        "paperId": "6aebe394b781205ec9accd634e9395b7c21c1310",
        "title": "The Alignment Bottleneck"
      },
      {
        "paperId": "719f902b9f695da79d15ee3f144e9992db86c33e",
        "title": "Random Direct Preference Optimization for Radiography Report Generation"
      },
      {
        "paperId": "a6fbcfabbb20c8848364ec44c8a1dea18be00efe",
        "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning"
      },
      {
        "paperId": "061cbb0c2c5a27d521c9d37af4f3e7450e8681fc",
        "title": "Probing Hidden States for Calibrated, Alignment-Resistant Predictions in LLMs"
      },
      {
        "paperId": "62a8551f102144c3e0dc84fba96f9d3d8e4a86c1",
        "title": "Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction"
      },
      {
        "paperId": "6e757f1dfb2858450f7a1582d3914f751e588d22",
        "title": "Controlling Language Difficulty in Dialogues with Linguistic Features"
      },
      {
        "paperId": "f916082e1296df1cfcb6778b82d0c3d60720fcd9",
        "title": "MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models"
      },
      {
        "paperId": "7677577c3198cff989fdef4318e1bebfef74ee76",
        "title": "Aligning Audio Captions with Human Preferences"
      },
      {
        "paperId": "35c25a00d86f67b4c8997c20c4e71bc4e9529aa4",
        "title": "MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation"
      },
      {
        "paperId": "b1f5028a63f76d6fc43d1cbbaf7475cd6b1f6ef4",
        "title": "Reveal and Release: Iterative LLM Unlearning with Self-generated Data"
      },
      {
        "paperId": "f7eba2f49ba6a813fae49a01ecf6074ea3b54842",
        "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models"
      },
      {
        "paperId": "4bba62e2496ae2823a8a77f4d9aec3f4a0115dec",
        "title": "SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding"
      },
      {
        "paperId": "11c55e68524f60ab98b5a4a50aab2f4c9f6ac9a4",
        "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning"
      },
      {
        "paperId": "13d6c7e8acd2027738a736e7314a496ceee7d6cb",
        "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment"
      },
      {
        "paperId": "f559de59721c3833db3abdebe3d76b3b9d6869b4",
        "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference"
      },
      {
        "paperId": "7d712b68ffa1e5b249a44f2ff5ae51685a3609aa",
        "title": "LLM Jailbreak Detection for (Almost) Free!"
      },
      {
        "paperId": "69c880e3a1040e3813062c815c01050335bfab0b",
        "title": "Impacts of Artificial Intelligence Development on Humanity and Social Values"
      },
      {
        "paperId": "803a311a6c689924df79745ede11447e07e15f48",
        "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration"
      },
      {
        "paperId": "0221b14ea30d3c0b4941c00e0065daa968bd9154",
        "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning"
      },
      {
        "paperId": "ecf44a2be37624212314a4819d4789c4442cc862",
        "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving"
      },
      {
        "paperId": "bb1f7dc2d0219694e0f7275f62335b58e818f377",
        "title": "When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning"
      },
      {
        "paperId": "cf2b9c448faf4cbe594519412cda14d12f905091",
        "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models"
      },
      {
        "paperId": "25a21ceb0912442e31f526394e428d6be9521d97",
        "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety"
      },
      {
        "paperId": "4feff2537690d507837e9a80e044d1e5203c97b2",
        "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
      },
      {
        "paperId": "a868d9e302737cfb032ea60d35de954db6d4eaae",
        "title": "The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features"
      },
      {
        "paperId": "f267597675790c4a1dfe3323621d15f3d956168d",
        "title": "Programmable Cognitive Bias in Social Agents"
      },
      {
        "paperId": "6c1e1ad16e9142216ef9200081cdd26704b240b4",
        "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe"
      },
      {
        "paperId": "b1368d4e0354b48f89c2c9b19968a0f1b186576a",
        "title": "LATTS: Locally Adaptive Test-Time Scaling"
      },
      {
        "paperId": "7bc2894d98f9bece05a66e25cba12cde41a1f70d",
        "title": "DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models"
      },
      {
        "paperId": "7a43796e13edd71cc1f06299bb1ae853efff1458",
        "title": "Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation"
      },
      {
        "paperId": "dff2fe5b12890ac5dba3667ebbc664ec97dc714f",
        "title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models"
      },
      {
        "paperId": "0c324a9077128bbb7fe17ba8226276d7946daed7",
        "title": "Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time"
      },
      {
        "paperId": "a16aba7e5fd046df3485d7146c1372117b992111",
        "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization"
      },
      {
        "paperId": "eaf38a01433a42f4d1f84d9d34bd596bb28b42e1",
        "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles"
      },
      {
        "paperId": "f7d47ea116ff69201be7fb67fcd67976fdcdf5c8",
        "title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation"
      },
      {
        "paperId": "5e1657fce82d66bd3817282783321611d5e98d87",
        "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding"
      },
      {
        "paperId": "e6c63c73758caad217cb435e800262aa76c2abeb",
        "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning"
      },
      {
        "paperId": "97adc3a35eaf830c6572dc439e66136064a754b4",
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking"
      },
      {
        "paperId": "32cd1fbf3f8cae90c635d00745d80af76a1cda76",
        "title": "PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization"
      },
      {
        "paperId": "ad1905eea0ad5acdb15f36e92195efb904cfe8b3",
        "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction"
      },
      {
        "paperId": "a39ace65d9ddb56c7edf98e472a2291dd7609408",
        "title": "Pluralistic Off-policy Evaluation and Alignment"
      },
      {
        "paperId": "7f1775c7bd78ad9c02ceebf9d28c62e9cd375a24",
        "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations"
      },
      {
        "paperId": "b07a6a99e8ada2410c1d6d01893d314814ca4649",
        "title": "Opal: An Operator Algebra View of RLHF"
      },
      {
        "paperId": "e7e1beddcd1e814ac2ba0ccbb96504b133fd6a8c",
        "title": "PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint"
      },
      {
        "paperId": "946036fbba544535887db63d4c6b103b16bbb44a",
        "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents"
      },
      {
        "paperId": "d481378097ec02e2c8f5c227d06e5c83ef53f068",
        "title": "Detection of Fake News in Romanian: LLM-Based Approaches to COVID-19 Misinformation"
      },
      {
        "paperId": "6346715af7c939253d1d4a19578c9775467395f9",
        "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA"
      },
      {
        "paperId": "6d38835ec1fc91a55b2b0b55b3116045a8472bff",
        "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning"
      },
      {
        "paperId": "3c21d2b0de9262c857f8f6c5484631c1edad518d",
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation"
      },
      {
        "paperId": "f389e01b118c7e1e46724443515d27e9519d4904",
        "title": "VARCO-VISION-2.0 Technical Report"
      },
      {
        "paperId": "caa9031830cf0ac64027d52a5d17a30c4982202f",
        "title": "Visual Programmability: A Guide for Code-as-Thought in Chart Understanding"
      },
      {
        "paperId": "bca7f4dd4db559d6772b470d9fe5391e3608cc8c",
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"
      },
      {
        "paperId": "33cdc41bc537d32f9f71e8dcc1799ecc55059109",
        "title": "GmSLM : Generative Marmoset Spoken Language Modeling"
      },
      {
        "paperId": "9abc47fca9931599c9f1fbb0f19b04491c76e51d",
        "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems"
      },
      {
        "paperId": "abf03d59a76ce15934c0beea918af4007efc5de0",
        "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making"
      },
      {
        "paperId": "f32405274168cb04a9a93fd75454f10ac6ad3468",
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models"
      },
      {
        "paperId": "883b6ff42e9197e5cecc1a997123f0676bbd6551",
        "title": "Persistent-DPO: A novel loss function and hybrid learning for generative quantum eigensolver"
      },
      {
        "paperId": "30da58c425d4e2a5e3c0b774cf8302d2fcf9ce51",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "dd6dd841ea57410d4d8aa0873189f035464e4119",
        "title": "Generative Data Refinement: Just Ask for Better Data"
      },
      {
        "paperId": "92c5d06e2390690826bdff9892b936a0fa0724d6",
        "title": "Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning"
      },
      {
        "paperId": "182882881d594c494709dffe7ff316f573e3bee3",
        "title": "A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving"
      },
      {
        "paperId": "3ae0aed92f344237dc4486e97811f2c438ad8f59",
        "title": "Frontiers in Artificial Intelligence Algorithm Optimization: A Comprehensive Review of Training-Time and Inference-Time Advances"
      },
      {
        "paperId": "7ceb4e11592bc5616e9d4ecaa4d0acf9c27a0048",
        "title": "Language Self-Play For Data-Free Training"
      },
      {
        "paperId": "2a66b719a1eb211e26bb7a10c566cefee071ada2",
        "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems"
      },
      {
        "paperId": "3bbe04b4e6597d263d4dcaddf317ab1a7420f5b3",
        "title": "Getting In Contract with Large Language Models - An Agency Theory Perspective On Large Language Model Alignment"
      },
      {
        "paperId": "af6b3d89277c02db584321dd4b9f3b823fa24a37",
        "title": "PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design"
      },
      {
        "paperId": "4ce781aee9304409f988cf412bd6c2569ac62efc",
        "title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm"
      },
      {
        "paperId": "ea6c4230657ee48a88e83ccd70e8ed3902f0aad4",
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey"
      },
      {
        "paperId": "cac386ba89cae3191f3fb19cd70f2af594941fb5",
        "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement"
      },
      {
        "paperId": "8460a61d76933d2f2c83e78a6658f3468b5b71af",
        "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents"
      },
      {
        "paperId": "fa18d1031a04ecd0cf4eaae9123e82909e9c3cba",
        "title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "796377f8efc10f343fa49572a89dbb3be97c3220",
        "title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation"
      },
      {
        "paperId": "63f251bbb143f7f993bf87c74796fae4abe4cb4e",
        "title": "RL Fine-Tuning Heals OOD Forgetting in SFT"
      },
      {
        "paperId": "81b4f5084309d76cc07436c1feb94e83574a661c",
        "title": "Reconstruction Alignment Improves Unified Multimodal Models"
      },
      {
        "paperId": "1ef29b16fdca0122deb2ff217bb42a57e6fadc55",
        "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues"
      },
      {
        "paperId": "2affa73c5527e5e0cc8ffa378744f1f9c65dbf1e",
        "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching"
      },
      {
        "paperId": "b7ba318b26aeaa3c9d5ffd3952c3658949b74a5f",
        "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models"
      },
      {
        "paperId": "ec4e56b6fdf1f87272440b0790aaf6e705e8b4b4",
        "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal"
      },
      {
        "paperId": "e5fbf5e861addba7479a28f9cf1a790012becc3e",
        "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs"
      },
      {
        "paperId": "f204f10833e9cbbb471839fe51162cbffd361dee",
        "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization"
      },
      {
        "paperId": "bdf3e94f52ea99a40d5790c62ce0dd4d8e4eb4f8",
        "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula"
      },
      {
        "paperId": "615aaa56bb8f06f44276c46ec43f85dd9050a53e",
        "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments"
      },
      {
        "paperId": "765f7140f70bb9cb963cc200d9b969f50c45e03d",
        "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation"
      },
      {
        "paperId": "9399970808f0b1965a69f29a659fcd909af4301b",
        "title": "ICR: Iterative Clarification and Rewriting for Conversational Search"
      },
      {
        "paperId": "f2af5174daaeac47d02ad672220bc4ab8b7c9b2b",
        "title": "FinRL Contests: Data\u2010Driven Financial Reinforcement Learning Agents for Stock and Crypto Trading"
      },
      {
        "paperId": "0a66ab5760423129b19d7cd0dc3ad0446fd0500b",
        "title": "Enhancing Diversity in Large Language Models via Determinantal Point Processes"
      },
      {
        "paperId": "a2713faa34f3c9ca4c0ea25268e8e384015009db",
        "title": "Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation"
      },
      {
        "paperId": "942752924e5743d6557cc9a53c2a05fb11e3b50b",
        "title": "PLaMo 2 Technical Report"
      },
      {
        "paperId": "b992f2a50b705ee7f27785f2019274849358dada",
        "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study"
      },
      {
        "paperId": "2429de2c3f2b7ddcdf1f8b2d34c6eb8b75cc47f9",
        "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models"
      },
      {
        "paperId": "ad74032137b4129b54c5c06be208843d73792e7f",
        "title": "RL's Razor: Why Online Reinforcement Learning Forgets Less"
      },
      {
        "paperId": "be01082d5d90361a1b2c4398e0899e758169a14d",
        "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth"
      },
      {
        "paperId": "da85483205b7e793ccbce09daea7c053aeb0bc2d",
        "title": "Measuring How (Not Just Whether) VLMs Build Common Ground"
      },
      {
        "paperId": "8d01b72c4ef340ec5c0c195476ead098e3259fe4",
        "title": "Delta Activations: A Representation for Finetuned Large Language Models"
      },
      {
        "paperId": "5f9c6b4790a0cf73cadff505c49f0cff34884039",
        "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning"
      },
      {
        "paperId": "76e098eb53ab659102c999d876c4ecf8402b8a78",
        "title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment"
      },
      {
        "paperId": "b5611668b0cda3a8152ddb42e7aba33a00d659e1",
        "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize"
      },
      {
        "paperId": "9b683772ab72b8a3ea1b4e973ba30603a17defe7",
        "title": "Towards a Unified View of Large Language Model Post-Training"
      },
      {
        "paperId": "0b82eec3b92215b910c19102c33bcd13aa6f8bd0",
        "title": "Why Language Models Hallucinate"
      },
      {
        "paperId": "4912a5fdaf4d04e55cefb343608059584fbb6637",
        "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model"
      },
      {
        "paperId": "129dea44270e8cbc0a2de70696d99d58711e9e6e",
        "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins"
      },
      {
        "paperId": "b46e266b19d575643eb1c159cb74fcc2b37f26a3",
        "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators"
      },
      {
        "paperId": "f9328d554737bfd4f2ca7f75da4d122cc8e788a2",
        "title": "Adaptive Preference Optimization with Uncertainty-aware Utility Anchor"
      },
      {
        "paperId": "a12a067120f2a6a8fde119d636e5952c35c2ca94",
        "title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers"
      },
      {
        "paperId": "59c6403b4445fcdc4e0cb918cb7aa080206432ba",
        "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences"
      },
      {
        "paperId": "a76fffe2e593137ee4b6d54b13bea42e5a5f7da6",
        "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE"
      },
      {
        "paperId": "0ccc3c6e3d2b0d4ae7be84f3e27b11f5c8bb0791",
        "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions"
      },
      {
        "paperId": "259efcfb8690fbce0dc49f891d696423de780155",
        "title": "Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient"
      },
      {
        "paperId": "6d6679bd5bf00db7ea3a8305c6db8c8e73a3997a",
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations"
      },
      {
        "paperId": "0ff5a5702e88d3b3aafcea707cc724960f4c0aa3",
        "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving"
      },
      {
        "paperId": "fe0b4cfccfbb7e701265cabcaf22b5e54004fe96",
        "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR"
      },
      {
        "paperId": "596b5da2307777340ce939d318d2972a8aee4cf4",
        "title": "Omnidirectional Spatial Modeling from Correlated Panoramas"
      },
      {
        "paperId": "0c281d137f1e7c1e2d10cc49c2b22df8afda35bd",
        "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving"
      },
      {
        "paperId": "78e8d4617187b056c78f68dccf5b8f35b3967de9",
        "title": "Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs"
      },
      {
        "paperId": "495795965fef8777ada7b2c66dc880d3d66b4235",
        "title": "FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework"
      },
      {
        "paperId": "af6216e3e361cda386a50e01a2abeb367dde2125",
        "title": "HLMTrans: A Sim-to-Real Transfer Framework for Spatial Crowdsourcing with Human-Guided Language Models"
      },
      {
        "paperId": "a51c25e7e7902237ec2c2e21815f1ec5219c0535",
        "title": "Indirect Online Preference Optimization via Reinforcement Learning"
      },
      {
        "paperId": "0176b635a232eeebda6314c56a911c974d77c8a8",
        "title": "Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance"
      },
      {
        "paperId": "fa81fba5fdd9a05867e71838054b31b3a717fbfb",
        "title": "Large language models for ingredient substitution in food recipes using supervised fine-tuning and direct preference optimization"
      },
      {
        "paperId": "e82b07a4f6062bd586e8fb0a448e49f36a52edb0",
        "title": "Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation"
      },
      {
        "paperId": "6220a5c93c912b12b95a11bc150cbd28f6bb0ee5",
        "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers"
      },
      {
        "paperId": "2fd3bbf8cb92482a03eb2b6d20404000dcffb1b5",
        "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus"
      },
      {
        "paperId": "7c74d038204f86f47f6be6b489f95b70bfbfd1f8",
        "title": "Reinforcement Learning for Machine Learning Engineering Agents"
      },
      {
        "paperId": "ca38d3dbf0ca25d4eeeb43becdc50aa45052e22d",
        "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use"
      },
      {
        "paperId": "c84d5d4b848cbc98fb40a688f5a94fe2168c0b98",
        "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic"
      },
      {
        "paperId": "fbf6654d97ead3ff5fa5b495c8b5f5cfb7eef728",
        "title": "Recent advances in finetuning multimodal large language models"
      },
      {
        "paperId": "0dc8652aa3b615c7f712e3f6eb418700ff07eadd",
        "title": "Translating to a Low-Resource Language with Compiler Feedback: A Case Study on Cangjie"
      },
      {
        "paperId": "559106b6e069cbeb5affe04ec69f5c3cf6e073d6",
        "title": "Preference-based Deep Reinforcement Learning for Historical Route Estimation"
      },
      {
        "paperId": "ce7a49a37580b48a4ffbae3ef93d62d90b92ed9a",
        "title": "MsRAG: Knowledge Augumented Image Captioning with Object-level Multi-source RAG"
      },
      {
        "paperId": "442cf4097fbcc1cd905516df43fa70721120c4cb",
        "title": "Safe and Effective Post-Fine-tuning Alignment in Large Language Models"
      },
      {
        "paperId": "327a920eba81e2e7fbe371dfaa6b567a170bb2d2",
        "title": "Relation-Augmented Dueling Bayesian Optimization via Preference Propagation"
      },
      {
        "paperId": "28ef6c02820b95e3f44602ed9dfa38bd8b94f971",
        "title": "Parameter-Efficient Action Planning with Large Language Models for Vision-and-Language Navigation"
      },
      {
        "paperId": "1f97b80fac581676de9617bbc29ba03454d909a8",
        "title": "Leveraging an LLM-enhanced bilingual conversational agent for EFL children\u2019s dialogic reading: Insights from children, parents, and educators"
      },
      {
        "paperId": "bf138ec7927eb3c1f73af807bb0d617732d3d2b7",
        "title": "Text Reinforcement for Multimodal Time Series Forecasting"
      },
      {
        "paperId": "edb55c2b0cfff454524ee0b201ecf116c027c170",
        "title": "OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination"
      },
      {
        "paperId": "828fbd6d365e7ed4506750e50442d34edb88e0ff",
        "title": "MPO: Multidimensional Preference Optimization for Language Model-based Text-to-Speech"
      },
      {
        "paperId": "d59495494d2a4f1845bfea321be220906daa066f",
        "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning"
      },
      {
        "paperId": "5e788559561f7a4cb33354992b9de99918974055",
        "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs"
      },
      {
        "paperId": "a9845d59e17e37ae2123d4db04d4737bb2b3df8b",
        "title": "GIER: Gap-Driven Self-Refinement for Large Language Models"
      },
      {
        "paperId": "7bb7a0d948fd97ac617de6d22d941590190c3f70",
        "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models"
      },
      {
        "paperId": "a2b7b7670eb8b27ded1e8a0e34399d9951a2c581",
        "title": "VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding"
      },
      {
        "paperId": "b06aab48d21825184816b6e847ad66385575ba01",
        "title": "SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "81a85622562e2778038460b644b4de832f216509",
        "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding"
      },
      {
        "paperId": "76067dc410f475261780a185060c5eaf029d1912",
        "title": "Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models"
      },
      {
        "paperId": "4fe3acec1905a0f7a80c3ac4ba3832a88b77c7bb",
        "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "b7be1cc0b73b5bde3db46dbea0a22019021dcfca",
        "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning"
      },
      {
        "paperId": "3d6291914e7f461460ef6852c3ef9457b8a292b4",
        "title": "Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning"
      },
      {
        "paperId": "802f50f2b8117892d03637098dc55e136fa03275",
        "title": "One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist"
      },
      {
        "paperId": "07ffad3a6aa5b1549151b444ab56b10b3736d637",
        "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online"
      },
      {
        "paperId": "0cd0d2321599cd83dd2bb796debdda09eb973803",
        "title": "RECAP: REwriting Conversations for Intent Understanding in Agentic Planning"
      },
      {
        "paperId": "4ba1691f3a7e630f6645ee06afa43f46e0dbd085",
        "title": "Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning"
      },
      {
        "paperId": "b822c1750e7c030bf95b1114f273267fd16bf036",
        "title": "Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search"
      },
      {
        "paperId": "0b9a4ec41a0d995f60b780fbb262764e97044213",
        "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance"
      },
      {
        "paperId": "391f6206350c70d6d8eab39704320b385bdd732d",
        "title": "Improving Alignment in LVLMs with Debiased Self-Judgment"
      },
      {
        "paperId": "720e33ee507178e51cc57e2a228698c26b790b6a",
        "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant"
      },
      {
        "paperId": "3da524202f37e99c1f6567fce928c892a4930744",
        "title": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement"
      },
      {
        "paperId": "0544547cb6d932312de0ca5e3f2010f163e33262",
        "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "9aa2b5470874ca27f41354ab86c61871ff39a5ba",
        "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems"
      },
      {
        "paperId": "314b3077a2b954736e6d26fa65516696d8016bff",
        "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning"
      },
      {
        "paperId": "146cda64133619f15a00f96ca0b66c0b620d34ea",
        "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning"
      },
      {
        "paperId": "4a8e6bbe2e23e1901675e1de57f14449075f894e",
        "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning"
      },
      {
        "paperId": "24b2fee1d83153f500a51370019e321b959a3bdf",
        "title": "Patch Generation in APR: A Survey from the Perspectives of Utilizing LLMs and Using APR-Specific Information"
      },
      {
        "paperId": "bd510fe218ff1d93fb1ac41006abb283b7e67d9e",
        "title": "Mirage or Method? How Model-Task Alignment Induces Divergent RL Conclusions"
      },
      {
        "paperId": "fcd4af45421052e56f26b37e2486b848df87aaab",
        "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment"
      },
      {
        "paperId": "01d47770bf7cb7541ed536a47baf734c97d5df06",
        "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks"
      },
      {
        "paperId": "8ddf11509dae8dee616a0dedf91c1537723db962",
        "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework"
      },
      {
        "paperId": "8ed4a4d278a8898e19b31e2af9e6d7275a5ae888",
        "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation"
      },
      {
        "paperId": "c8e677c10440a516e3f35f7c7e40467908607085",
        "title": "Evaluating Language Model Reasoning about Confidential Information"
      },
      {
        "paperId": "e023b181734934ae643eeb4c31f5d9191e6cd940",
        "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads"
      },
      {
        "paperId": "d15f552943702eccd126d1d2588fff2a3c5f634d",
        "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization"
      },
      {
        "paperId": "c6368520206644a036cb8011c211a82bb6bf46c9",
        "title": "Model Science: getting serious about verification, explanation and control of AI systems"
      },
      {
        "paperId": "d12a368ad0aa485b129f400a374de85711e6f564",
        "title": "Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization"
      },
      {
        "paperId": "2ff29547bac9f14e397105828514fb6bd01a8778",
        "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding"
      },
      {
        "paperId": "b3acd89a0b8b8ee6d0479fa3820344f31c5782be",
        "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation"
      },
      {
        "paperId": "17b4d09c27ad1191325ec50486d358f7cd20e68c",
        "title": "Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization"
      },
      {
        "paperId": "4f83828e88968731f077c6ea39277e5d8cbb279e",
        "title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement"
      },
      {
        "paperId": "81dc87ab78ae73e19e19306e16dff235427e35f9",
        "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction"
      },
      {
        "paperId": "7cefb07ebc9bb3ea7c667937d1016ba4758858df",
        "title": "HAEPO: History-Aggregated Exploratory Policy Optimization"
      },
      {
        "paperId": "ed0c6e958990f6fdf0b56542e90ece80e3e7e456",
        "title": "Recycling History: Efficient Recommendations from Contextual Dueling Bandits"
      },
      {
        "paperId": "7c32b20b0d6f6b4fdb51bd35813222f5ac7fcc71",
        "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality"
      },
      {
        "paperId": "819a2f0e0426f2c52fce994b7298d41ca40cbeca",
        "title": "Weights-Rotated Preference Optimization for Large Language Models"
      },
      {
        "paperId": "e778e55a5f7deb0ca9213a26550f69550fce3c65",
        "title": "Multi-domain Distribution Learning for De Novo Drug Design"
      },
      {
        "paperId": "4c8dfac8ae1e7aa630ab298a67d4acef4bed109b",
        "title": "Proximal Supervised Fine-Tuning"
      },
      {
        "paperId": "051e7d6caaa4c4c05c57062e21e094a2fdff10cd",
        "title": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework"
      },
      {
        "paperId": "83cf4086e8253b8845bc97d545fa479a2c0e080d",
        "title": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow"
      },
      {
        "paperId": "13c6c22e41bf029ecd5e3a4d9f2ac27afe1c0392",
        "title": "UniAPO: Unified Multimodal Automated Prompt Optimization"
      },
      {
        "paperId": "eb2bc55e78774baf178a935abf91ef86e58f3641",
        "title": "ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning"
      },
      {
        "paperId": "18d83103fb98905ccbce420987470eb2ea021187",
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "paperId": "5a014d683e37d1d95b07a917c60204e60f0dfa31",
        "title": "Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation"
      },
      {
        "paperId": "ff99ade4fa5e06bddb97590be97e8a6023cbd06b",
        "title": "Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries"
      },
      {
        "paperId": "057e748f70e85d1b48df07da51481c69c0b53b88",
        "title": "Speculative Safety-Aware Decoding"
      },
      {
        "paperId": "04d068e95078a5becd4e3750d35d4cf9ba658441",
        "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?"
      },
      {
        "paperId": "2340b5954f8ae3ea966d3585b77edfb77e8269a0",
        "title": "PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization"
      },
      {
        "paperId": "5e036f9058c567e344e21d3c000e33e2f0af7aa6",
        "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience"
      },
      {
        "paperId": "82a0aed09f850c9265b1221adbc7ef8b0f5330cb",
        "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment"
      },
      {
        "paperId": "cd01ca573e3990660ca64dee82293e2135caba96",
        "title": "Multi-Metric Preference Alignment for Generative Speech Restoration"
      },
      {
        "paperId": "0b91243e579c4a46516507e09db3467f9c48cb75",
        "title": "Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models"
      },
      {
        "paperId": "1a586844e36bbcd3d1b0ae177c197e4c4eb5c479",
        "title": "SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation"
      },
      {
        "paperId": "5d41307c15e124d897076f23445a26ead0e0214a",
        "title": "Convergence and Generalization of Anti-Regularization for Parametric Models"
      },
      {
        "paperId": "ec3c27b26e546ec205dd039460c409e4bdd1384b",
        "title": "Module-Aware Parameter-Efficient Machine Unlearning on Transformers"
      },
      {
        "paperId": "82f5f388d23e3807eac7049d2a0f4ef6bb48947d",
        "title": "SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer"
      },
      {
        "paperId": "99a80610ba44f099ef9bc73026fb257dacb92c06",
        "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD"
      },
      {
        "paperId": "c05fddaa0451cf5cfe2e09e7631005f350f49b13",
        "title": "What Matters in Data for DPO?"
      },
      {
        "paperId": "f358728ae0fdcf48d9acdbfa99e63a16dfb722e5",
        "title": "Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens"
      },
      {
        "paperId": "8c5110de73ecb8d222650da3999ee68ebf44b15c",
        "title": "Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling"
      },
      {
        "paperId": "a60082b6ebe795bf88d4e9681b27aec1b5fbd706",
        "title": "Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs"
      },
      {
        "paperId": "13b2c3b21b6f3693e2a9ca03584f0f0ec94e790f",
        "title": "KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF"
      },
      {
        "paperId": "82a34ad74d85ff4c080a9c94eddb6638a3684671",
        "title": "Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities"
      },
      {
        "paperId": "323d3365ad02dd1e26a1b4d56f40ba7f90d7b5d3",
        "title": "Post Hoc Regression Refinement via Pairwise Rankings"
      },
      {
        "paperId": "5448cdd83aade7ff38d090091f7d241383b86a92",
        "title": "Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation"
      },
      {
        "paperId": "f22cfb3650ce408a5a5ebf9f47309a21f0572fec",
        "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning"
      },
      {
        "paperId": "bb5f8c6e72694a2b5e3170c4a4ef54683fc79484",
        "title": "Political Ideology Shifts in Large Language Models"
      },
      {
        "paperId": "86869e2efacc2cbef59303e8ebb893ff6861922d",
        "title": "CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency"
      },
      {
        "paperId": "6bfb52937fc293fa0ac17e2c8031f9e3bd8294f0",
        "title": "FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing"
      },
      {
        "paperId": "385e395ca369ea6498db3fd510a8df33cc671df4",
        "title": "Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars"
      },
      {
        "paperId": "edaa14273dfaabc066166da80a1b1c629ea5a2d9",
        "title": "OPERA: A Reinforcement Learning-Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval"
      },
      {
        "paperId": "b07ee4903dc59af8b4ab7925e9888d11c9dd7725",
        "title": "On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View"
      },
      {
        "paperId": "134f0840459619a88dafc2d83e9ec11b2d9ca9c4",
        "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling"
      },
      {
        "paperId": "bc0d4fa9af7c68b560ae0a51f1fdf4d7209ad499",
        "title": "SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks"
      },
      {
        "paperId": "94f47eb8d492b236de77df95759be47915cd9bb0",
        "title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks"
      },
      {
        "paperId": "fc40fc66718a3645fbd190a500185c18c2df41e1",
        "title": "RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores"
      },
      {
        "paperId": "c3316647836afebcf168f2e1b533fe5344a6063b",
        "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation"
      },
      {
        "paperId": "2c2ce0e63858bcb5a6f5b6c5d870a976d0469f42",
        "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion"
      },
      {
        "paperId": "92f59f320e48a65ae46cb44c58d858a10937cc80",
        "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning"
      },
      {
        "paperId": "ba1e9567cb6f9707bbb5cc41590841e3aee8a7c2",
        "title": "Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration"
      },
      {
        "paperId": "f92143205290f9e2056c34ccd8aed18d266f979a",
        "title": "Waver: Wave Your Way to Lifelike Video Generation"
      },
      {
        "paperId": "9e06fa16e44f663faf4ad6cd91e6e4248628f016",
        "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"
      },
      {
        "paperId": "ccb70eb3381330c04f893471bb7b55b465a8bbbc",
        "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner"
      },
      {
        "paperId": "9d130660deff1318cbbf08e29a9706cedd271ae0",
        "title": "Linear Preference Optimization: Decoupled Gradient Control via Absolute Regularization"
      },
      {
        "paperId": "9b4b33719ff915cbbcb5ccae8fe348f2b920c36e",
        "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data"
      },
      {
        "paperId": "4b4c87ad540a51f55c4f62173e30892c53bbf0b2",
        "title": "Open-Universe Assistance Games"
      },
      {
        "paperId": "9f191856a8bdb0c263461f184c75a5aa39d47427",
        "title": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA"
      },
      {
        "paperId": "fd9b1ef62f2d08342ccb17a3eb135dfb6b6c8000",
        "title": "ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs"
      },
      {
        "paperId": "ee26c8ca8aee85141075782bcf4d5baa8c4f61a3",
        "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration"
      },
      {
        "paperId": "4d4cc77520edb7c611b3e0ef1df38b9ea50f244c",
        "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization"
      },
      {
        "paperId": "b43bd402b3569b20f80581eedf1f9901e337975e",
        "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer"
      },
      {
        "paperId": "ab079c296f32657744c687dce3dc2021102b5477",
        "title": "PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis"
      },
      {
        "paperId": "ea6280d0527a46fa6614b051cf44cc1c52522533",
        "title": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search"
      },
      {
        "paperId": "289cf1394f623c2e23cb12706756d107899d4604",
        "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance"
      },
      {
        "paperId": "72329bf059a3ae7d5e7d0bc21a2beed31141205f",
        "title": "Learning from Preferences and Mixed Demonstrations in General Settings"
      },
      {
        "paperId": "23ce682976ca6aa62a7984058a9a8d52aa9cabd9",
        "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation"
      },
      {
        "paperId": "f41c05e35d0241d02184679c515039aebe90e77a",
        "title": "LENS: Learning to Segment Anything with Unified Reinforced Reasoning"
      },
      {
        "paperId": "da193c6c8741f1ee075d8cff9e1d797f10418922",
        "title": "Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems"
      },
      {
        "paperId": "e942bbc1ea5a11502b1dab90f5ac1e30ab895458",
        "title": "SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression"
      },
      {
        "paperId": "5d8aff62a93f0bb2a9f0c4943f9e0bc19a602130",
        "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction"
      },
      {
        "paperId": "00b45cdeaf3ca077deddc977789ce9322e049dd4",
        "title": "Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes"
      },
      {
        "paperId": "1c61ffce1e3a7da384a9b8d6b9d16733e7705aaa",
        "title": "Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards"
      },
      {
        "paperId": "81aa8d0c94bbf8b928f9e946140b3af2bcddec17",
        "title": "Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning"
      },
      {
        "paperId": "0843fbe9204422415e52f3a80d742ae41015d620",
        "title": "Human Feedback Driven Dynamic Speech Emotion Recognition"
      },
      {
        "paperId": "8ff49f73a65da8461f94cb6f9bcaaf1f79b91ef7",
        "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following"
      },
      {
        "paperId": "ea974e5365960629edc0588b9f039b91c49cf012",
        "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback"
      },
      {
        "paperId": "3d375f2f6a75266389cfb90d4e4f9673461147f1",
        "title": "TaoSR1: The Thinking Model for E-commerce Relevance Search"
      },
      {
        "paperId": "fd491170a8e29b59b9fedbb609aba8b616ca9cd3",
        "title": "Cost-Aware Contrastive Routing for LLMs"
      },
      {
        "paperId": "2ae6290729559754b5ec9b25a2b06b64728879ec",
        "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position"
      },
      {
        "paperId": "fc479f06955565987aaaf3f4f473787e35d18251",
        "title": "GraphCogent: Mitigating LLMs'Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding"
      },
      {
        "paperId": "8a1fa7e5c1f700b16a75212ad00e4d35cb36ccba",
        "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models"
      },
      {
        "paperId": "affd2027edc78fb8f60d9b6f15225436c2564b4c",
        "title": "SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress"
      },
      {
        "paperId": "6a9ace63fe89eb19a57bf73c4cc34e65284df206",
        "title": "QuarkMed Medical Foundation Model Technical Report"
      },
      {
        "paperId": "ee970bc55f13d19956a218830e4355cde9363e19",
        "title": "Mitigating Jailbreaks with Intent-Aware LLMs"
      },
      {
        "paperId": "ca590b1b00a31f599702f90b5b44bef61a590188",
        "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions"
      },
      {
        "paperId": "22dfb4db6b2596b2ca686b5f937ab82bb700ae04",
        "title": "User-Assistant Bias in LLMs"
      },
      {
        "paperId": "7af0dee85b349af0ff3e98decac1e393db84c118",
        "title": "Fusing Rewards and Preferences in Reinforcement Learning"
      },
      {
        "paperId": "64b396d5d247f65b69d59385c9e5f9b1511869e9",
        "title": "Ovis2.5 Technical Report"
      },
      {
        "paperId": "1b1bbff9baa967cacf7f8fabf5b65b534dc331b6",
        "title": "Preference Models assume Proportional Hazards of Utilities"
      },
      {
        "paperId": "e3c254934d87bc718b9bc54968f846ebced1fc73",
        "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model"
      },
      {
        "paperId": "05698063f31dc31c9d07c0b3661f548ea86fceee",
        "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"
      },
      {
        "paperId": "97fb48eb960bdda360661eda39606e8e5cba9f5c",
        "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory"
      },
      {
        "paperId": "1789ab7feee000d5f9c9dc3fd55d9a89dea34891",
        "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"
      },
      {
        "paperId": "de684792f883b66091e8d92ff461af1fe592f04a",
        "title": "Thyme: Think Beyond Images"
      },
      {
        "paperId": "c2e5f4406735b5259e4c2e754d6a6b89bcd04dfe",
        "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction"
      },
      {
        "paperId": "b4870fcaab83db589e4c283d1c8355ec21b5089a",
        "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection"
      },
      {
        "paperId": "94e277a8b6b5f8f1ed768ff9a45e2f7f8da36d6e",
        "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation"
      },
      {
        "paperId": "1b087d2530afabca1c8549daf44d8c4796eb6a60",
        "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models"
      },
      {
        "paperId": "cea641b63d050162096b9706ee3fbc6bfd197fef",
        "title": "Controlling Multimodal LLMs via Reward-guided Decoding"
      },
      {
        "paperId": "0c1a49375e861e6fb110c9a8dee2befae41c76c7",
        "title": "Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework"
      },
      {
        "paperId": "24f215c49b69d94f8e116b761b13ce8acdce217d",
        "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"
      },
      {
        "paperId": "94302b496240f965160c48511aa15373f2fde842",
        "title": "ALAS: Autonomous Learning Agent for Self-Updating Language Models"
      },
      {
        "paperId": "66d7b70b059206c4e93f77331b3157122d17ddfb",
        "title": "Reinforced Language Models for Sequential Decision Making"
      },
      {
        "paperId": "5add284a495ed02a76a38259c090438f77b185d9",
        "title": "Learning from Natural Language Feedback for Personalized Question Answering"
      },
      {
        "paperId": "278d12fdeb3fe3e5ad1e50fffb9ec8839de324c0",
        "title": "ReviewRL: Towards Automated Scientific Review with RL"
      },
      {
        "paperId": "31faafdfb237a460d8c8bff3788c92d249757844",
        "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design"
      },
      {
        "paperId": "d67afa3cb98b6e5a15966ce06f1aff61cf2ad270",
        "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model"
      },
      {
        "paperId": "057f8de4c6702f20fef90d31ab6d54a555739e66",
        "title": "Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation"
      },
      {
        "paperId": "f809e863789c31d39f836aa4da3876fc83d8f9f7",
        "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey"
      },
      {
        "paperId": "02ebf3417763042ce1623dd81b51e1f9a595a923",
        "title": "CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse Prevention"
      },
      {
        "paperId": "70a7adee4af0f632367ae5fb3a8e80cb96ba9438",
        "title": "Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment"
      },
      {
        "paperId": "b0314bf027b0f35fbcb8af895e7e3561015ee7cb",
        "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization"
      },
      {
        "paperId": "3b6ae2cba73dedf88648a37c99eac564cd4f4505",
        "title": "On Negative-aware Preference Optimization for Recommendation"
      },
      {
        "paperId": "51547d6ed52d823528dcd0c9ee1f8ae364a2e073",
        "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"
      },
      {
        "paperId": "a7a328415eb88e815476c525d2ff493b0735f89c",
        "title": "Finetuning Large Language Model as an Effective Symbolic Regressor"
      },
      {
        "paperId": "0272b917fc6a0854bcc3eb7291fc66945dc11eb8",
        "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning"
      },
      {
        "paperId": "37f8a18c74565ffbbbc20fa5214b72c00b4618d7",
        "title": "From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation"
      },
      {
        "paperId": "a8cf1449d1b82cc53bb02712b1f77291edf6cd6c",
        "title": "Amazon Nova AI Challenge - Trusted AI: Advancing secure, AI-assisted software development"
      },
      {
        "paperId": "f48cdc11abf6d18696b85ff1ee5efed2a2a4f726",
        "title": "Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts"
      },
      {
        "paperId": "f5adc0d4b36ca900a7db0abd74e39e825607613c",
        "title": "KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration"
      },
      {
        "paperId": "94954679cf7732569bd9508e74446c7d18fc4b9e",
        "title": "Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization"
      },
      {
        "paperId": "57daf1717be0b2b154493e4624f07fcd76fa75fa",
        "title": "IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization"
      },
      {
        "paperId": "7bab5ca540dbbc4677f019649e04409f7350976a",
        "title": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training"
      },
      {
        "paperId": "991638fced3b73f9230ace7fe646031cd3fc9bd7",
        "title": "CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization"
      },
      {
        "paperId": "2f7a1407b77477a708002c8a98198ef3292c41b3",
        "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision"
      },
      {
        "paperId": "d9f79f2f864a880845ed895ea4defd47c02e3a90",
        "title": "From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training"
      },
      {
        "paperId": "ef0e33fa685c8dc6b8199b199fd03e482089f5e7",
        "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning"
      },
      {
        "paperId": "640779e0ca75fd7b1d5d035ec20fc0f3350de837",
        "title": "Compass-Thinker-7B Technical Report"
      },
      {
        "paperId": "5d1b76cd23f5d40ee91821307efccb821ecd69c2",
        "title": "DB3 Team's Solution For Meta KDD Cup'25"
      },
      {
        "paperId": "539107d4bc5fbc8e3a6ed6cb687c7b061939f048",
        "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"
      },
      {
        "paperId": "f90881642f28616ba051ad01e3d08534b5bebd39",
        "title": "Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"
      },
      {
        "paperId": "9a5071bacb2bcb98ddd95076dbd446240b4da2c6",
        "title": "Reinforcement Learning in Vision: A Survey"
      },
      {
        "paperId": "783cda79f82e0994d1c44cff832960301598d662",
        "title": "Generating Query-Relevant Document Summaries via Reinforcement Learning"
      },
      {
        "paperId": "bd882244d2d84d7a455fdd1af5198f8fbdcdd228",
        "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation"
      },
      {
        "paperId": "f695bbbe1abd4f4d81d87c89d0652b0223c0a643",
        "title": "\\(X\\)-evolve: Solution space evolution powered by large language models"
      },
      {
        "paperId": "412367905f6c8f63407b3d5061bf686cfcc92952",
        "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning"
      },
      {
        "paperId": "e4151d6c5dc061fc6bc8f537a19a6b740c64c0f9",
        "title": "Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints"
      },
      {
        "paperId": "c7b279896e90bd1723c4fd5e2f1e3e03a3a12a43",
        "title": "Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback"
      },
      {
        "paperId": "3f7f25ea2af8d72693308b26f016e0e61eaf71f1",
        "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"
      },
      {
        "paperId": "253e8f6c798caa4e15590c6ee039de4abe843f2d",
        "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways"
      },
      {
        "paperId": "67dffb645b1edf4f631e19b0663208a9b9d5bbcf",
        "title": "AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation"
      },
      {
        "paperId": "ac1b1b4605c15a1c42468fdd45b299ec74b22281",
        "title": "PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization"
      },
      {
        "paperId": "a53befcab1a13da61c5531c36f02229c0e2a6ecc",
        "title": "A Principled Loss Function for Direct Language Model Alignment"
      },
      {
        "paperId": "e3e404e30641e720b7fdc3dab032741726215ccd",
        "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models"
      },
      {
        "paperId": "248dfbbecd50b0df3960d518ea534a264d43e9e5",
        "title": "PROPS: Progressively Private Self-alignment of Large Language Models"
      },
      {
        "paperId": "4b43dc6374b9a528362571d5e25f9a0850cfdb7a",
        "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs"
      },
      {
        "paperId": "c4afe127c05518551ccec9f17bf717c479895289",
        "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning"
      },
      {
        "paperId": "0a409e79d98f47dff34a77624792d6e9e71edc78",
        "title": "Sample-efficient LLM Optimization with Reset Replay"
      },
      {
        "paperId": "d9751f3e1f729f5543e2f5bc34e8b6fbdd438559",
        "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning"
      },
      {
        "paperId": "0e95bdd2119212a9e8449d3714bcb3dcbf739002",
        "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"
      },
      {
        "paperId": "0cb515c64fad79ebe8c4556c273f863517ec19f2",
        "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment"
      },
      {
        "paperId": "efab4f79a90517f9b968ff902b54367ba200cfa4",
        "title": "Post-training for Efficient Communication via Convention Formation"
      },
      {
        "paperId": "0862ce65ae8dde070b1f6561481092d5bbda421e",
        "title": "Learning to Reason for Factuality"
      },
      {
        "paperId": "1414653547b72a23d2c0ca987d270fb236c9105a",
        "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"
      },
      {
        "paperId": "c8c62d5547106307573a70761d902eb2ec59ea5e",
        "title": "Cognitive Duality for Adaptive Web Agents"
      },
      {
        "paperId": "477a992a8f802e3e4292bf698f6f2011deeddcbd",
        "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"
      },
      {
        "paperId": "6b3e64ab07bdb533d26bc4a78dad189e02a9bcb8",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "6cbd611a1ed36908b4a74dd747401229f01f67a7",
        "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"
      },
      {
        "paperId": "cef44e69b1335dbe377230901879e2e57af281bc",
        "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs"
      },
      {
        "paperId": "5cdcd77dd36a26ed3ea9da9cf8aac577a61b4c18",
        "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"
      },
      {
        "paperId": "c1647173db395136f3c2be0d1dafcadca91a41ef",
        "title": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL"
      },
      {
        "paperId": "5ee20e781a8e2e2f4e7f3ce33256a94b8e1aa84a",
        "title": "Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation"
      },
      {
        "paperId": "caad3e3b6fb001ecd6b6e4e98e0db084584364cc",
        "title": "More Like Vis, Less Like Vis: Comparing Interactions for Integrating User Preferences Into Partial Specification Recommenders."
      },
      {
        "paperId": "369129607bd707a6232435c55f231c5d39c4574b",
        "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision"
      },
      {
        "paperId": "0006ef1aaa7261189e6acbac85c031985347db75",
        "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data"
      },
      {
        "paperId": "8234f7df8180d9f9a388a70c1437c05e5f031828",
        "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy"
      },
      {
        "paperId": "701fa19a19f9996f5d322e13e8c7427324338a06",
        "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models"
      },
      {
        "paperId": "80f4f6ac3cd394f41a8a203ae1d90e7f95fc0293",
        "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference"
      },
      {
        "paperId": "4d1863da264b7f4c5eea6e83f980dba244ba53d7",
        "title": "COPO: Consistency-Aware Policy Optimization"
      },
      {
        "paperId": "619763067a3c21a652d80f68b7d533ddfe9fd520",
        "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding"
      },
      {
        "paperId": "3ee2ebb47185674ce29858e89d16c89160b8f59b",
        "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis"
      },
      {
        "paperId": "c244200154abb5e5e45ff14f4e6202e1a42a678d",
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience"
      },
      {
        "paperId": "f6db5bae234f03a725dff5b9d7d0081dced0c56a",
        "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?"
      },
      {
        "paperId": "2bbea815365a78cf76d752ccc626b4ff85ddd1e6",
        "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models"
      },
      {
        "paperId": "e973c11f814e1876682172d9038953027ff1236c",
        "title": "Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning"
      },
      {
        "paperId": "5159fecd2a40e8b77ec7e040ef0211cd48353123",
        "title": "Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy"
      },
      {
        "paperId": "6c1096e0a179076dc1317bfb4506a277b30c5ef0",
        "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding"
      },
      {
        "paperId": "6398232cbef2609e46e621dfcd2737fdd040a759",
        "title": "CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation"
      },
      {
        "paperId": "b59f471f00533d4b9dc34f3ccc4122fdc9fedf56",
        "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards"
      },
      {
        "paperId": "10f5a33793b39fc7d2cd53c707486b749e51cbda",
        "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap"
      },
      {
        "paperId": "058ddbb59946523f59e9eed9632e85148ed14b7e",
        "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs"
      },
      {
        "paperId": "b336ab70b33fa163a81e4da9bf84cb8be0298ba2",
        "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective"
      },
      {
        "paperId": "2515805fad4abedd130a037129ca54e14a929551",
        "title": "Generative Bid Shading in Real-Time Bidding Advertising"
      },
      {
        "paperId": "1a41a24220a754e1188fdde478ff231154515ddb",
        "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models"
      },
      {
        "paperId": "de2d5cc1f224ce71d9a8018d839c002a6bcde081",
        "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis"
      },
      {
        "paperId": "10c98cc047490b30eb286e3ac7a09de71163b5da",
        "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning"
      },
      {
        "paperId": "26bde5ebc6609218ad565cd6ba04f3c3d586e3a9",
        "title": "Self-Questioning Language Models"
      },
      {
        "paperId": "eb61655dd91b1cb1fbffe3e373f19a6bf9511a8c",
        "title": "AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video"
      },
      {
        "paperId": "85c707bc10e112f7c08823b89a563be7437a78c9",
        "title": "EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving"
      },
      {
        "paperId": "32c8c36bfcf928a9083a1001c18242e04e0a2429",
        "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models"
      },
      {
        "paperId": "9bb52bb4bb85fb20e6ba81cfe9f4d6cface7803c",
        "title": "TRACEALIGN - Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs"
      },
      {
        "paperId": "65556ef457ae6d61a8b43492495c0f8ef3665333",
        "title": "Qwen-Image Technical Report"
      },
      {
        "paperId": "66873b3a2e3aca875b8df9060fcea0a95f3cb245",
        "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation"
      },
      {
        "paperId": "70a4b5430d7cb928b0955bcacd9aa6de7af7f499",
        "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models"
      },
      {
        "paperId": "1a4eecd5c2eb2eee32108f698ee3ce004179dbdc",
        "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation"
      },
      {
        "paperId": "6c80b6fd388d671e17d10071971473a5f71d8b29",
        "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models"
      },
      {
        "paperId": "d5e2c949e744e4f5d6e3f28f6031721316ccd333",
        "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy"
      },
      {
        "paperId": "f74bd469624b9df0a9c43e5c0a42f43f475aa5c6",
        "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback"
      },
      {
        "paperId": "22d76cb3adbbd26226f31278fc3e494b71d4879c",
        "title": "VAGPO: Vision-augmented Asymmetric Group Preference Optimization for the Routing Problems"
      },
      {
        "paperId": "63802b0c8d5d78cf48f7abb14a87b4802896f70d",
        "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning"
      },
      {
        "paperId": "35529a2e2810d0f914940699086eca47186d6820",
        "title": "Enhancing Math Reasoning in Small-sized LLMs via Preview Difficulty-Aware Intervention"
      },
      {
        "paperId": "7646ffe0469e929b1133ca51e9c73317f073ea9c",
        "title": "IMU: Influence-guided Machine Unlearning"
      },
      {
        "paperId": "8862d3811bb38c3327164c4d01799b5f6f25fe87",
        "title": "FlowXpert: Expertizing Troubleshooting Workflow Orchestration with Knowledge Base and Multi-Agent Coevolution"
      },
      {
        "paperId": "223f69ae654313fd56cca2b0f0eddb01d9313bf4",
        "title": "Retrieval And Structuring Augmented Generation with Large Language Models"
      },
      {
        "paperId": "f87a1ae69500f5ee7f591e8b01a79599473c2fe8",
        "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference"
      },
      {
        "paperId": "025d4262866c34671c3b73b122c764a69867ea23",
        "title": "Towards Evaluation for Real-World LLM Unlearning"
      },
      {
        "paperId": "0018126f8ab12070667d354811d91eb9cb54b4c1",
        "title": "Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models"
      },
      {
        "paperId": "e5b2532549fbabe13325b7d9817f825edd28d1a6",
        "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models"
      },
      {
        "paperId": "9cb1fecd6fa636b386365b9bf2635a92595839a3",
        "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization"
      },
      {
        "paperId": "4ba8ac8cd8b2bdcc3104588f38ad3def3793207e",
        "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data"
      },
      {
        "paperId": "637731c710a2e48bbf5a57ab01840881c7650977",
        "title": "Motif 2.6B Technical Report"
      },
      {
        "paperId": "b0e397cc9e40ba5cd5c436fd892d2a94f746dbfd",
        "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English"
      },
      {
        "paperId": "ba65e808f68484343895e48e900e7cb785aa395d",
        "title": "Activation-Guided Local Editing for Jailbreaking Attacks"
      },
      {
        "paperId": "b48c8077ef4fc8796361da010ddb5d3d0f3c156e",
        "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors"
      },
      {
        "paperId": "9b5990aae48900bfa77da8211709f10f77d2cfc3",
        "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation"
      },
      {
        "paperId": "435fc162f0dbdd0768aefc68781c31cbe4e06f82",
        "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement"
      },
      {
        "paperId": "52c1d2e790679bfa56298d857d08eaf9148a9395",
        "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report"
      },
      {
        "paperId": "78bc57a9d8cc5955c4ecbf093247314887beae10",
        "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases"
      },
      {
        "paperId": "86973ed161eca0937bcf22e7f9ed6d684fb9f509",
        "title": "Objective Metrics for Evaluating Large Language Models Using External Data Sources"
      },
      {
        "paperId": "15e918177790de41c2d31de72a593d4392765b99",
        "title": "ENHANCING CLINICAL REASONING IN MEDICAL VISION-LANGUAGE MODEL THROUGH STRUCTURED PROMPTS"
      },
      {
        "paperId": "92342a489ca41fe8315881263e81824621a87979",
        "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks"
      },
      {
        "paperId": "4c3c2663fbdc4a06a8dec68815f65e22a6e13731",
        "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning"
      },
      {
        "paperId": "8e8937113d77fc88f3ddd0087adf5fa963543241",
        "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding"
      },
      {
        "paperId": "b214e7ca8ef5b8ed762b54ee5debd2079122eb69",
        "title": "ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning"
      },
      {
        "paperId": "5458d07ae90e746e54a1f77906ae2258c42605e7",
        "title": "Repair-R1: Better Test Before Repair"
      },
      {
        "paperId": "b71603e07ee0944842796f84c9877ee08704238e",
        "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity"
      },
      {
        "paperId": "8a61f300c04a888bdca5b731bd7e9938a0bac21a",
        "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation"
      },
      {
        "paperId": "9dc3caf9c466e7667f64c4353cb2b3fb34a183b4",
        "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions"
      },
      {
        "paperId": "d1a33bf398116a76f87975e08b3a7619c70034a4",
        "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs"
      },
      {
        "paperId": "e5c8edfe19e5af718945dfc93057b28d1a84dad2",
        "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding"
      },
      {
        "paperId": "594d82ab5ed8ac54c4419e5e10152ac54dcf2f60",
        "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again"
      },
      {
        "paperId": "d33bd07c39b1e1c1907439a17ce6b35217ca2d41",
        "title": "Libra: Assessing and Improving Reward Model by Learning to Think"
      },
      {
        "paperId": "bf05e759011b54b38a142c844ef230efb5cc0d89",
        "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security"
      },
      {
        "paperId": "d38c397873c4e93125a3be97ec645d01e9e4a1e1",
        "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback"
      },
      {
        "paperId": "cefc7a1b837ec10b2792cc4d4ed50753a1849cb9",
        "title": "Strategic Deflection: Defending LLMs from Logit Manipulation"
      },
      {
        "paperId": "f0378278a29a692c00e0db9674088825c5a36ef2",
        "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment"
      },
      {
        "paperId": "c1b22d1383ec17452d6f9da67d427a832f165b1c",
        "title": "Unlearning of Knowledge Graph Embedding via Preference Optimization"
      },
      {
        "paperId": "45d24b0c35193db1d99dd7c6ed68f5f47d513f3f",
        "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts"
      },
      {
        "paperId": "b284e0b3c5e09079253f025a664fb476d2e65bda",
        "title": "Teaching Language Models To Gather Information Proactively"
      },
      {
        "paperId": "10a245dba3bae69a9ccafa4a2886f9770118fe46",
        "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation"
      },
      {
        "paperId": "8ddccf8cdc58a9530e371c54d8b749558a5a6c4b",
        "title": "Beyond Value Functions: Single-Loop Bilevel Optimization under Flatness Conditions"
      },
      {
        "paperId": "6698df0e7fca31d64d72e54d04de6430476e9637",
        "title": "CTR-Driven Ad Text Generation via Online Feedback Preference Optimization"
      },
      {
        "paperId": "d4fa13b5f06fc1f415cbf9ae89286574101b062c",
        "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge"
      },
      {
        "paperId": "f40935638c554e8df8590b1766996e04f3de8c9c",
        "title": "Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning"
      },
      {
        "paperId": "a1aaa56c12313aac3059ba8cbc4fa876c03c2039",
        "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering"
      },
      {
        "paperId": "33bf820ea3f9a5f098348ab1e58a23b6d5ba418e",
        "title": "The Blessing and Curse of Dimensionality in Safety Alignment"
      },
      {
        "paperId": "d4807d7d4596fc80c143e90cef1a2cdea1686ab7",
        "title": "Post-Completion Learning for Language Models"
      },
      {
        "paperId": "38c8e28838cf9f72b5b80baef94594222734dd37",
        "title": "Self-Improvement for Audio Large Language Model using Unlabeled Speech"
      },
      {
        "paperId": "bd30b98aa947bec7c5e09eafe283fe621884893d",
        "title": "SGPO: Self-Generated Preference Optimization based on Self-Improver"
      },
      {
        "paperId": "5f2e7a17cfb3ab9d1d01b7e3e542f95a24d4aa8f",
        "title": "SDD: Self-Degraded Defense against Malicious Fine-tuning"
      },
      {
        "paperId": "02b2ce0f2ad2def1a66e41929033905e1e0dc232",
        "title": "Advancing methodological development of artificial intelligence in patient-centered comparative clinical effectiveness research: Patient-Centered Outcomes Research Institute\u2019s unique contribution to research done differently"
      },
      {
        "paperId": "68775864885d0dc8706444d459625585db09c660",
        "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training"
      },
      {
        "paperId": "f52c85e8d54409c4e16dd58a35e13281a212274c",
        "title": "Infogen: Generating Complex Statistical Infographics from Documents"
      },
      {
        "paperId": "f078a1a30ba9aa02c032c1e2741919364845cfab",
        "title": "A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction"
      },
      {
        "paperId": "77bed54f6769eeaa153a617c4ca73941558f7c51",
        "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents"
      },
      {
        "paperId": "aaeeca88cc8cf24a14905f82d7280d39283ac936",
        "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning"
      },
      {
        "paperId": "06050dca1a9da67bfb0a9d22646aa2a92b32460c",
        "title": "Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models"
      },
      {
        "paperId": "256a3fc27acd3946efab403c57687e6b3a1b4afc",
        "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction"
      },
      {
        "paperId": "12bb80f829b74e4d646265ff42d60cb16d10d6ce",
        "title": "Efficient Learning for Product Attributes with Compact Multimodal Models"
      },
      {
        "paperId": "f62d273fa0bc8989c92535f3656e526550cb19aa",
        "title": "AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation"
      },
      {
        "paperId": "b425eb0768a677018fb5c51cfccf4c3c2d2e57ff",
        "title": "Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models"
      },
      {
        "paperId": "b8ec3b451c82cc11ef230a2a29d0ab008ee65d38",
        "title": "Technical Report of TeleChat2, TeleChat2.5 and T1"
      },
      {
        "paperId": "c9c21c4706d42afce45145b23b6bc50957ff4340",
        "title": "Checklists Are Better Than Reward Models For Aligning Language Models"
      },
      {
        "paperId": "dff0e651f712a437700bf5f31a8348e3e21aaae1",
        "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs"
      },
      {
        "paperId": "bb7f50570027676609cce80f4778f0ff73d1b513",
        "title": "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition"
      },
      {
        "paperId": "907e0d32c1d4f219b68d0058940e82b9c2167649",
        "title": "Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking"
      },
      {
        "paperId": "9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
      },
      {
        "paperId": "e8ad32b37db16407958c407d0d34947feb9a481a",
        "title": "Diffusion Preference Alignment via Attenuated Kullback\u2013Leibler Regularization"
      },
      {
        "paperId": "18a3c9edd6d6982d477a4f3540ccb91540fbefba",
        "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back"
      },
      {
        "paperId": "f2f6607c759581164d56cece90ad40e87065c451",
        "title": "URPO: A Unified Reward&Policy Optimization Framework for Large Language Models"
      },
      {
        "paperId": "f8aa4488fe95ba15123f9ea21992c880610f7f90",
        "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models"
      },
      {
        "paperId": "07ec02b26370e283aa2d1e9e55c1b45af8aff22f",
        "title": "FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain"
      },
      {
        "paperId": "1417d23f4b5f1650e41f37d3f34b25f0a2f7826c",
        "title": "Turning Internal Gap into Self-Improvement: Promoting the Generation-Understanding Unification in MLLMs"
      },
      {
        "paperId": "a556ef1d3f2049dd1f281bebb1d8835a02f4efea",
        "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?"
      },
      {
        "paperId": "38070e5b1e516f0c62e138afd124e0e00f26d16f",
        "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs"
      },
      {
        "paperId": "097f801ad7f0a71ed7f4cfdf5d61351fff4f2b08",
        "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization"
      },
      {
        "paperId": "e9dc981f5c7d3a0154453b3b8b5c62f150e07070",
        "title": "METER: Multi-modal Evidence-based Thinking and Explainable Reasoning - Algorithm and Benchmark"
      },
      {
        "paperId": "0aa91cbb6a43670e77c09c4c0353adfc89c64d7e",
        "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback"
      },
      {
        "paperId": "bb3fc03bcf1ebddfb094605b79c13042f02cf1a0",
        "title": "WakenLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking"
      },
      {
        "paperId": "7f191de50a5240b3b14fff427bb3a8b54628ff09",
        "title": "TTS-1 Technical Report"
      },
      {
        "paperId": "53d4111f7e9440dee553f0a87656f74bc886c87c",
        "title": "Step-Audio 2 Technical Report"
      },
      {
        "paperId": "288a89ebb95cd05f8b4482f32117333f5d1ddba5",
        "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning"
      },
      {
        "paperId": "dbc583b6df1bb6c32fd1b6bcaa9e4063450bc3b8",
        "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models"
      },
      {
        "paperId": "1b73c13713adf39d26f619d6bbc69d0940ee7cbd",
        "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms"
      },
      {
        "paperId": "00fc92a444573dfaada5251b45a45067c4b6d131",
        "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "560fc5db493c20f788c7d0a80e0e3a5c1ca1b200",
        "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses"
      },
      {
        "paperId": "5d48ba11039bda0e0a173b034af79991259186b8",
        "title": "TorchAO: PyTorch-Native Training-to-Serving Model Optimization"
      },
      {
        "paperId": "5f8582f9b78cd78360aee5496ec9a0f9dc05acb2",
        "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning"
      },
      {
        "paperId": "52f36c2d1f109db3ff7efd862571e563150a0c9b",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization"
      },
      {
        "paperId": "1d68470a6b86ae462c2446c1b8c8b798e61748a7",
        "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display"
      },
      {
        "paperId": "985796f3006ffe41eecaa9096be68ccc2f1096c1",
        "title": "Statistical and Algorithmic Foundations of Reinforcement Learning"
      },
      {
        "paperId": "9b1c5298bc3998bc5107fbdb21aa80868b06985b",
        "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions"
      },
      {
        "paperId": "7636c3209e4fa1bc500351501ffbc82c2fc86333",
        "title": "SoftPipe: A Soft-Guided Reinforcement Learning Framework for Automated Data Preparation"
      },
      {
        "paperId": "c87a76ebb3df5edcbd7a03699e798d2be1806a9b",
        "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition"
      },
      {
        "paperId": "3f2813b31d67831cc71f3c4de31b344001bc2db2",
        "title": "Preference-Based Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "3a7e649794e41bb15acd60283a5d861d18cc0c8f",
        "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning"
      },
      {
        "paperId": "65639cdf0af31b686255247c58079c69bca554d3",
        "title": "Artificial intelligence-driven computational methods for antibody design and optimization"
      },
      {
        "paperId": "0d3a8f9d07269f94cf37968416a69b32132c0c63",
        "title": "DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization"
      },
      {
        "paperId": "76f16299c2646582443999f55ebeabe8d316e923",
        "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers"
      },
      {
        "paperId": "a80f9ceeee2009ba830b018115f167e318aec91f",
        "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes"
      },
      {
        "paperId": "9c19ed145128ec8f67fc66b14f6b2740e6a4fcbe",
        "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "cbcd3aedfff02882bbf9350c1e15f3262bc53414",
        "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning"
      },
      {
        "paperId": "3555773184b1814eeb37e048c217894d4d96b43d",
        "title": "Voxtral"
      },
      {
        "paperId": "c703b729f3b97ffe4c9a6c430e2a18f6d4f7223e",
        "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training"
      },
      {
        "paperId": "634344ada2254157cf6d3bb3a580048619a9fc2b",
        "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR"
      },
      {
        "paperId": "95ad238577fbe2ab970d6f87f5d9bb7e79ee6546",
        "title": "Learning to summarize user information for personalized reinforcement learning from human feedback"
      },
      {
        "paperId": "bca3895efa330477c1a6a76f2a0642dff933e4bb",
        "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese"
      },
      {
        "paperId": "abec2024945d9e954ed718e40df7065429cdaed0",
        "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention"
      },
      {
        "paperId": "a37e910b88386491ae0b67f544722c914fee3aeb",
        "title": "A Survey of Deep Learning for Geometry Problem Solving"
      },
      {
        "paperId": "97531b9a091635ff859664bc2d381b66599bf38a",
        "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning"
      },
      {
        "paperId": "48ef15d92002581d51142bcfe5dc7584e6eeed63",
        "title": "LLMs Encode Harmfulness and Refusal Separately"
      },
      {
        "paperId": "0790031705f1f5334bb3db04d529658935ed0236",
        "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack"
      },
      {
        "paperId": "29ae3a74225a0ac1c8fe4003873ed5570c85261f",
        "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models"
      },
      {
        "paperId": "172bcbc3da4f28140902531e59ebcf951c6fb1bd",
        "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schr\\\"odinger Bridge with Conditional Diffusion"
      },
      {
        "paperId": "c8900b99265605d5b6c43f009c99ef5ae72e08d6",
        "title": "Function-to-Style Guidance of LLMs for Code Translation"
      },
      {
        "paperId": "3faa11d35446c037d1690a6551c0afe84bb678a8",
        "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization"
      },
      {
        "paperId": "fbab5170bab9400531c909da7c0be8b06e3587e2",
        "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization"
      },
      {
        "paperId": "0d426b4dd26dc2baaef545eeb445136d612bc6b7",
        "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities"
      },
      {
        "paperId": "fd8641f1a7c0ef30d6fb312cddd047d8bc492942",
        "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes"
      },
      {
        "paperId": "024d582a0c7cc22c727352b37e5d5c10dfdf8c5a",
        "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding"
      },
      {
        "paperId": "41c312ee28d3a3cefe4465a2b06a670e6dc82c14",
        "title": "Aligned Query Expansion: Efficient Query Expansion for Information Retrieval through LLM Alignment"
      },
      {
        "paperId": "7e7732b8898d22aa8a482b31dd8486475f52ca02",
        "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization"
      },
      {
        "paperId": "e083eace1333ca8c67b297cf95e72a144d213ecd",
        "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models"
      },
      {
        "paperId": "5b181970d6c53a306fe7bee7ba333fb856d73bcd",
        "title": "Multi-Armed Sampling Problem and the End of Exploration"
      },
      {
        "paperId": "1a08170d7a9f9653b825967ead501d45b07c425d",
        "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong"
      },
      {
        "paperId": "9a6ac74553457e89de1494d5dded6f17675c10e6",
        "title": "ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning"
      },
      {
        "paperId": "9f7e58508065a4104d610beac9451f41a7b892d5",
        "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis"
      },
      {
        "paperId": "34478a8297196ed1043c5a4b0e3651cec35064c4",
        "title": "LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents"
      },
      {
        "paperId": "b2d2f50718bc1313679239b0855025f0651e005c",
        "title": "Ask and Retrieve Knowledge: Towards Proactive Asking with Imperfect Information in Medical Multi-turn Dialogues"
      },
      {
        "paperId": "1c93729fa65d9a7523a3b4fb471295f46310214f",
        "title": "Fine-tuning Large Language Model for Automated Algorithm Design"
      },
      {
        "paperId": "3ae141b988f37040cbef0c531575a6189f297b2e",
        "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services"
      },
      {
        "paperId": "ce34856a600ba43fd050b05bd576a0a7da150fc0",
        "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters"
      },
      {
        "paperId": "c94efc2f42fbf8ffe4df9b9afc77a5a75a9af9f0",
        "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing"
      },
      {
        "paperId": "93fa9010b37cf1a8fa3cd8888540ae71e66d7070",
        "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning"
      },
      {
        "paperId": "8f212835b4d262d0c1ed4e42357d11001c19a2a0",
        "title": "Self-Improving Model Steering"
      },
      {
        "paperId": "c4e2208535347f8b57cec11ef793a044ab6a80a9",
        "title": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling"
      },
      {
        "paperId": "366a68c0475dd0e5028318e326efd6a2184999c7",
        "title": "Droid: A Resource Suite for AI-Generated Code Detection"
      },
      {
        "paperId": "265fdd781ac5f96f0e6ae37fbca0a370ee7c1b26",
        "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions"
      },
      {
        "paperId": "8327bd346d109e62ed0730bc5e2e11efdbd43e64",
        "title": "Scaling RL to Long Videos"
      },
      {
        "paperId": "137d83835c6faba10e9c0a59ad9be28dc674f53c",
        "title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought"
      },
      {
        "paperId": "b0517cd2739494cea8d9a49fe4307cda3b58131c",
        "title": "Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization"
      },
      {
        "paperId": "c979aa555c031ee3434caa2b99d52373ab466102",
        "title": "The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs"
      },
      {
        "paperId": "099906e0c4faa56c071389a020c30079da952f38",
        "title": "CTRLS: Chain-of-Thought Reasoning via Latent State-Transition"
      },
      {
        "paperId": "b4b58add5fa2098f93d36515397898ec56600d94",
        "title": "Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization"
      },
      {
        "paperId": "0bab5941e6a81043b07a2aada57c4bd1496a75f4",
        "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models"
      },
      {
        "paperId": "ef6a4d3959a7334a1961648120b39dcdda8637f9",
        "title": "Distilling Empathy from Large Language Models"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "4cb1ce04a44ac85ede834bb7eca4b67565f5da6f",
        "title": "Principled Foundations for Preference Optimization"
      },
      {
        "paperId": "3f402a2b8e2613eeb7bab65e4f4f3398c0b6b526",
        "title": "Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation"
      },
      {
        "paperId": "2a76d9aea66a7446def7a7252c34c7c497505e9f",
        "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving"
      },
      {
        "paperId": "05b9b425544de6e4bdd47af2f3018047ad192c7a",
        "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities"
      },
      {
        "paperId": "9e35522b4650864058381cc79f2ecd9877e3acca",
        "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment"
      },
      {
        "paperId": "004dcf35abbce390779107c6a929fa62876b1f69",
        "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment"
      },
      {
        "paperId": "128c211566b8426c31eebe4eece6df23e083f1a6",
        "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models"
      },
      {
        "paperId": "1f23e73dacdddf75457ed574f3954eb0efd15f6a",
        "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution"
      },
      {
        "paperId": "02dac7990c1b934e50a70e21560ceb3b62eba96d",
        "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling"
      },
      {
        "paperId": "5c3ae0ec1bd3ddd7863dfdd8e7ed9b7f50555119",
        "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains"
      },
      {
        "paperId": "7ca9aa44c71e6d428791bc2fa86367e236dff88b",
        "title": "OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning"
      },
      {
        "paperId": "d64c263fcea7ad24a7130d8db20e3ba6279347ff",
        "title": "Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling"
      },
      {
        "paperId": "5f07ad4fc9b89797b9c708f8dc20b0da0301585f",
        "title": "Differentiable Reward Optimization for LLM based TTS system"
      },
      {
        "paperId": "ff5db18341d678f2a9528dcd07ab5b63c031331c",
        "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations"
      },
      {
        "paperId": "b37d5ab221da0397b95c75d9b766be4b8894a06d",
        "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning"
      },
      {
        "paperId": "476ceccc25e02a3bf8aa7e00cda48fc833b0ca58",
        "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models"
      },
      {
        "paperId": "fafea7c3040cec360f47e5c44e5458640cdad61e",
        "title": "SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction"
      },
      {
        "paperId": "8fe4308c33e942ab60dbcbf6d625f74edd62dd38",
        "title": "Discrete Diffusion Trajectory Alignment via Stepwise Decomposition"
      },
      {
        "paperId": "2294ff230830af57528af7a0d957a3751a86a8b4",
        "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations"
      },
      {
        "paperId": "8a654f0d84afd036fbd03bda76fe9e226766c689",
        "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning"
      },
      {
        "paperId": "7f01d7adc28899552eb0d3d9ca488b3c2300b0f2",
        "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models"
      },
      {
        "paperId": "eb5d1d488dfee934566e51afe22d41f13b603035",
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation"
      },
      {
        "paperId": "db53c4be0ecd3d267b8dfc7d6f23ac2dc83ad04a",
        "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning"
      },
      {
        "paperId": "dd7e737cf6255310fd9ae11ecaa0eaf38b3c01df",
        "title": "InfoSteer: Steering Information Utility in Language Model Post-Training"
      },
      {
        "paperId": "9cd66ddc323b0c5ca8284d8de113b892e5436a69",
        "title": "NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving"
      },
      {
        "paperId": "d081c5ce0e9946fa5159bd83965d5e7262a02ee0",
        "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data"
      },
      {
        "paperId": "b1ff8e48be78ff1cf401c7ac0172502a6fae6b97",
        "title": "On the Semantics of Large Language Models"
      },
      {
        "paperId": "3bbce432f0d14d062390a89ca56ec17b449a6fed",
        "title": "Logit Reweighting for Topic-Focused Summarization"
      },
      {
        "paperId": "b0a699f3e699296939cc5067ada1afd732be12e0",
        "title": "MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design"
      },
      {
        "paperId": "7e89bb838e2a662b18672a15f7c2245638198a7d",
        "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling"
      },
      {
        "paperId": "5093efc827e80609c1893aeaa2f75d158acb3411",
        "title": "A Survey of Pun Generation: Datasets, Evaluations and Methodologies"
      },
      {
        "paperId": "fd499b102e96b0eb5b4e575f804b4fbe8e459487",
        "title": "From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach"
      },
      {
        "paperId": "f57b6a7fb7d974f4cd591a4ef6118a7ef4eb54a7",
        "title": "PRIME: Large Language Model Personalization with Cognitive Dual-Memory and Personalized Thought Process"
      },
      {
        "paperId": "d68c287ddb064d0603dd2c749eac0caa98550612",
        "title": "A Weighted Preference Optimization Service Recommendation Method Based on Knowledge Graph and Large Language Model"
      },
      {
        "paperId": "62353fbe0e4ba8c266b2b26230c2c1b4cf64899a",
        "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts"
      },
      {
        "paperId": "2bc3cbc06ff8ecec33e42970dc53dc5a851414b7",
        "title": "Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs"
      },
      {
        "paperId": "a2605e043bb01709993849fccd05228f7f029760",
        "title": "XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL"
      },
      {
        "paperId": "0311a0660b02ec1face3936a81e8ab5186789475",
        "title": "ESSA: Evolutionary Strategies for Scalable Alignment"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "7ea4109decd0d54fdae346f595bc2306fca4e20e",
        "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching"
      },
      {
        "paperId": "b285ed0a4d8bc1281a7a336d7b04eb37c76d3499",
        "title": "CTR-Guided Generative Query Suggestion in Conversational Search"
      },
      {
        "paperId": "551f13190148a119cb1d40e8cfb25288e650cc07",
        "title": "Learning to Translate Ambiguous Terminology by Preference Optimization on Post-Edits"
      },
      {
        "paperId": "fc3d1be58cc8fe00f15923493a0b59db2c50004c",
        "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks"
      },
      {
        "paperId": "85fb1dbc5251324908e0e8f36105119643a7c36c",
        "title": "ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization"
      },
      {
        "paperId": "5e1064e0876626a2bf3cf95a5978d286e8206bc0",
        "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk"
      },
      {
        "paperId": "3a0bde838c114f61fd8081c3c34a258eeb09c015",
        "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning"
      },
      {
        "paperId": "54178fbb81a3ccf811eeb7891fc9d830f6ac21be",
        "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models"
      },
      {
        "paperId": "26ab0c255faa2def6f7a31d6edc84829b55eef99",
        "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization"
      },
      {
        "paperId": "bd83cd05a59aa0b6cc039b1d0d5992f3d2705abf",
        "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models"
      },
      {
        "paperId": "1b59418178da2becc8e8c70b66723fb7c6103f67",
        "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users"
      },
      {
        "paperId": "93bc33eebfdeed6507c0791e742a2160508df2db",
        "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective"
      },
      {
        "paperId": "14b0064fd15c42e1691c0ee93ca61cfa071bdbeb",
        "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards"
      },
      {
        "paperId": "c5f680851259cfa24c1ad95c9f59cc28c243f541",
        "title": "PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage"
      },
      {
        "paperId": "ac9a7ebd4187ba0a280428e044b60cd71701f418",
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models"
      },
      {
        "paperId": "412b8c9b0d06fad403b53fbef4061773d9810ab2",
        "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America"
      },
      {
        "paperId": "549a0b6c4d8b8b4fc63d9ef294daa8c2d5f19174",
        "title": "MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation"
      },
      {
        "paperId": "e73b7ee48a1b4375b4bb2fdaa0021df1b9c8d1a1",
        "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training"
      },
      {
        "paperId": "18f372f3d0f3c35f1551bb26a40c9915b1a9d482",
        "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation"
      },
      {
        "paperId": "d98b89022b8ff4be710f6037c2872d529c848b66",
        "title": "Data Diversification Methods In Alignment Enhance Math Performance In LLMs"
      },
      {
        "paperId": "262351bc2ac004d6927ae27c2c91b290b1a59b66",
        "title": "Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo"
      },
      {
        "paperId": "48c75c54c35f6843e9cb4b59fd88ba45c0418106",
        "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities"
      },
      {
        "paperId": "727970c89859d51a2930271d4f8f18e17511b582",
        "title": "Generative flow-based warm start of the variational quantum eigensolver"
      },
      {
        "paperId": "c8f6fd538b5651315a915721bd32d6b5ed22ac5a",
        "title": "OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering"
      },
      {
        "paperId": "7537153659fb3183e68d4ed1c615a7a0a2a69958",
        "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization"
      },
      {
        "paperId": "5dae09dc8b3f016d2422c44d371cdcdd3c76007d",
        "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning"
      },
      {
        "paperId": "e65b647faf4d6f912873856888297b183bf6dfbf",
        "title": "Reasoning as an Adaptive Defense for Safety"
      },
      {
        "paperId": "154f3d9c6165fe626e4cedf531074cd02401a6ae",
        "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy"
      },
      {
        "paperId": "87251cd2f7ef82159aebb56f5e92c89af88a38f3",
        "title": "Activation Reward Models for Few-Shot Model Alignment"
      },
      {
        "paperId": "53177bd54b3b0121318a882225e9280fb4c90d9d",
        "title": "ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis"
      },
      {
        "paperId": "c6af32946fbe86a888960a9236d4cde81b4a5f74",
        "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation"
      },
      {
        "paperId": "3446a2d68c808acde13eb18aab24c2983eeb2f98",
        "title": "Medical radiology report generation: A systematic review of current deep learning methods, trends, and future directions"
      },
      {
        "paperId": "9dfc0e216691e0fe6144ee64172e8c6c5bae7b86",
        "title": "${\\mu}^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation"
      },
      {
        "paperId": "b15d4a3191860acd261e6b001a8b7af368fcc02a",
        "title": "TourMLLM: A Retrieval-Augmented Multimodal Large Language Model for Multitask Learning in the Tourism Domain"
      },
      {
        "paperId": "9667c7d11ca573e146b038d35a6f5074a010348b",
        "title": "Effective Multi-Class Sentiment Analysis Using Fine-Tuned Large Language Model with KNIME Analytics Platform"
      },
      {
        "paperId": "5d62b2c52955fb6367dc0d5bde26ef6ebcbaa2b6",
        "title": "Fostering collective intelligence in CPSS: an LLM-driven multi-agent cooperative tuning framework"
      },
      {
        "paperId": "fcc9fd8b917d81613287cb34fff3b71ba05774f3",
        "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models"
      },
      {
        "paperId": "22133a71e3ddc9e42b316895fbc14ebd30d0a62a",
        "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications"
      },
      {
        "paperId": "a26f44e9deda0113f2d93992e92d004ecb92426a",
        "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation"
      },
      {
        "paperId": "bb40fa8e90e23b461c5644ce9df78e2420d2afe3",
        "title": "Linearly Decoding Refused Knowledge in Aligned Language Models"
      },
      {
        "paperId": "13ac800d9b6206d2a0664340ce6cc3a3ec4a367e",
        "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context"
      },
      {
        "paperId": "b61abd58e9b5f701a70648c4de6a77708cb7644c",
        "title": "A Survey on Vision-Language-Action Models for Autonomous Driving"
      },
      {
        "paperId": "39016e75a8fbb03057ba78251f485596c390e1d0",
        "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs"
      },
      {
        "paperId": "f1de96cd87a05e95a8b56f1cb16d26926ac718aa",
        "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap"
      },
      {
        "paperId": "311f1aad440a2acbcaa07557257ab7a30defdb79",
        "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis"
      },
      {
        "paperId": "d843f446f297958bbe28218da0146e5b618c606b",
        "title": "Teaching a Language Model to Speak the Language of Tools"
      },
      {
        "paperId": "325255b6c41ab90bd239c26aef82add725eda8d2",
        "title": "Generalist Reward Models: Found Inside Large Language Models"
      },
      {
        "paperId": "c0a6a9492855b21351f36fbe7cc08663e81f4c5e",
        "title": "Towards a Progress Bar for Reasoning: Progress Prediction in Large Reasoning Models"
      },
      {
        "paperId": "c0fa06e74f45cddd8864855d03db66e5db498cbe",
        "title": "Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems"
      },
      {
        "paperId": "f8d476070a933074b90cff91c0a8ce7ee3edbd9f",
        "title": "Listener-Rewarded Thinking in VLMs for Image Preferences"
      },
      {
        "paperId": "92f432d4d43f0bca8c2859d9269a94ea93fcfb1f",
        "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game"
      },
      {
        "paperId": "7b3ef5de5bd7046c7af67d55b0d8e3f34cd1a749",
        "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models"
      },
      {
        "paperId": "551e4d832b789508dc1f56c4cca9351e87fdc9c9",
        "title": "Generating Privacy Stories From Software Documentation"
      },
      {
        "paperId": "e853e824dce1a56cdb16dc8148dd0ff4e9b40428",
        "title": "Training Language Model to Critique for Better Refinement"
      },
      {
        "paperId": "3161eb20b49a220648156a862954d2f09b160083",
        "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models"
      },
      {
        "paperId": "7172cee0149e6a135f019bb5d9db6587adb84cf6",
        "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy"
      },
      {
        "paperId": "7dd6b0495ee60a6705f12a3848b6b8a7c6c24735",
        "title": "The Hidden Link Between RLHF and Contrastive Learning"
      },
      {
        "paperId": "93ad88b10c00acbb7d3888b5c446494dc6591666",
        "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning"
      },
      {
        "paperId": "33eb087f52e37349f13c9a44e6bd09d5f89fcbfd",
        "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework"
      },
      {
        "paperId": "d7702b45e90d44dc44a79f59bf32c06da1582feb",
        "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments"
      },
      {
        "paperId": "a6a5e2d4319e2fa9bf780a9b28f71677de4baaf6",
        "title": "Aligning Spoken Dialogue Models from User Interactions"
      },
      {
        "paperId": "bba9df17cf0452603e402f508da87ef311282c6b",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"
      },
      {
        "paperId": "1a9014ce0f009b5ceb41d15cb8fb815c9af3f3bd",
        "title": "Designing and Optimizing Alignment Datasets for IoT Security: A Synergistic Approach with Static Analysis Insights"
      },
      {
        "paperId": "c1a78f39b367aaeb5dda88ae93f1ba4cecdf6a4e",
        "title": "Bridging Offline and Online Reinforcement Learning for LLMs"
      },
      {
        "paperId": "0caf9d0d584ed0f2cc544615714c2403414039f6",
        "title": "WordCon: Word-level Typography Control in Scene Text Rendering"
      },
      {
        "paperId": "9850565a8875d4f9226628f5ba3e283bcffe5bcc",
        "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization"
      },
      {
        "paperId": "4a5808c4e810910ae4ce3ae89ce65c560ac61280",
        "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds"
      },
      {
        "paperId": "b8ae0a9de532d3ffd66c7fb87c5ede148bee8584",
        "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE"
      },
      {
        "paperId": "2fc28ec07a8c0a0978d1fff6e3ffeb6a056bbe22",
        "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?"
      },
      {
        "paperId": "38eadd2a5eaa850911a5e05c16e47434b356d3b7",
        "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation"
      },
      {
        "paperId": "61024282f1523543a5b06326cb58c9b6ad14cdd1",
        "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation"
      },
      {
        "paperId": "aabd199f5094c726021f9a4175491549f6feb593",
        "title": "Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models"
      },
      {
        "paperId": "586e5ed3b8ab819e9e0536b7f332e4fc2e48e721",
        "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models"
      },
      {
        "paperId": "8e376586a910fe1febc2f96984e9e3ae0973bf8d",
        "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models"
      },
      {
        "paperId": "d341989bb259c35dc723c386ab45636a78613046",
        "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "84cb2dd0e08a7b18aa52252066ba87caeeb5b890",
        "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency"
      },
      {
        "paperId": "ce4a35fe58b5f185221eb20a1ce96c1b7955b754",
        "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards"
      },
      {
        "paperId": "c430897158e2bab32307dc91f60c256f4c0c80ae",
        "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning"
      },
      {
        "paperId": "52abaa2740c9b66d68c59f0a9f0dab507db6a95e",
        "title": "CAT+: Investigating and Enhancing Audio-Visual Understanding in Large Language Models"
      },
      {
        "paperId": "3076920fbb573b6bf94bfcb66ad5468238b1dce9",
        "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring"
      },
      {
        "paperId": "faed60347348c362e11c3158a43c3bec851afec0",
        "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs"
      },
      {
        "paperId": "264e38ebc1bfa24c0eefb17c9d8e7cfb11b1569f",
        "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems"
      },
      {
        "paperId": "bc604e7322502e4cf816d80ee86bbecd88688d52",
        "title": "Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning"
      },
      {
        "paperId": "4ab81d06ec54ff046624c1b9b57e03f56deaf741",
        "title": "Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment"
      },
      {
        "paperId": "7606b04df6fef772ce58d2968fac03d1802c9523",
        "title": "Agile3D: Adaptive Contention- and Content-Aware 3D Object Detection for Embedded GPUs"
      },
      {
        "paperId": "d35c49d95c09d2fe0fbf785173359b38858915d6",
        "title": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization"
      },
      {
        "paperId": "1ff1f133cebf5370d646ac6d84d2691b0f15c041",
        "title": "AI constructs gendered struggle narratives: Implications for self-concept and systems design."
      },
      {
        "paperId": "39e8b83aea2f232eb7b57b19580044ff596eefd9",
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
      },
      {
        "paperId": "3e801e70a9a83f3783295fe66b47666310f2b070",
        "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1"
      },
      {
        "paperId": "bf425d7067eaf134cb185d195e363abb78bd70b7",
        "title": "End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards"
      },
      {
        "paperId": "7523171301dec001d5d8f9e5ae5d00d20ae644bc",
        "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks"
      },
      {
        "paperId": "a04c6b4a50a8989b6898585083252c4ad817f548",
        "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning"
      },
      {
        "paperId": "92b852470985ff3712cc6fc7316fba78bae22ef2",
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning"
      },
      {
        "paperId": "1465aff7f7a0916287a43f8b0f0c0d6e5fe5719e",
        "title": "Inference-Time Reward Hacking in Large Language Models"
      },
      {
        "paperId": "50553458c41b689d9dda6d6e8c84c152848ab483",
        "title": "A Simple\"Motivation\"Can Enhance Reinforcement Finetuning of Large Reasoning Models"
      },
      {
        "paperId": "77836fb41a7026fdd840b84ba8648d9a9ba15fdf",
        "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating"
      },
      {
        "paperId": "3577bf01f8a0649c6cacfde328e79c24dddd55ba",
        "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection"
      },
      {
        "paperId": "a07c53a72fd28054a37beb7e9b57b700d20d0770",
        "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning"
      },
      {
        "paperId": "b6953a5002622e5da1da599df87bc1c9cd25a27f",
        "title": "CLGRPO: Reasoning Ability Enhancement for Small VLMs"
      },
      {
        "paperId": "1ec5df714563e64beeed2c5188c8b167bd8dbf14",
        "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication"
      },
      {
        "paperId": "eee126bdd382de4951a50a0ebe81c657543bc63a",
        "title": "LettinGo: Explore User Profile Generation for Recommendation System"
      },
      {
        "paperId": "0a4cc67b7de4315a1af9c75093448147585ae573",
        "title": "TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization"
      },
      {
        "paperId": "3cda07c100ecb3df005c1075b449351a6d9728ef",
        "title": "Statistical Multicriteria Evaluation of LLM-Generated Text"
      },
      {
        "paperId": "2a5ee875e2d9151d086683920a876b14e23882d8",
        "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities"
      },
      {
        "paperId": "1fe427e359d9454e74653cba6cccf25e32ef20c7",
        "title": "PrefDrive: Enhancing Autonomous Driving Through Preference-Guided Large Language Models"
      },
      {
        "paperId": "dc491963de5b0e69b10094092753f010c3f72977",
        "title": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "paperId": "439f06977cac678b9c65dc311fe8d55c5d5e07c7",
        "title": "InsightAlign: A Transferable Physical Design Recipe Recommender Based on Design Insights"
      },
      {
        "paperId": "a0e4c777b06c080d88bf6086886b0cb29f132238",
        "title": "EVA: An Efficient and Versatile Generative Engine for Targeted Discovery of Novel Analog Circuits"
      },
      {
        "paperId": "6c670d9136601f613c0d86eaac82c9b04fa46aa9",
        "title": "Hardware Generation with High Flexibility using Reinforcement Learning Enhanced LLMs"
      },
      {
        "paperId": "5945e5bd14c88e3c2ee3d97d22b01613db13a21e",
        "title": "Beyond Syntax: Action Semantics Learning for App Agents"
      },
      {
        "paperId": "d699cbfae999cc62447b082e1a03c665ca4b649e",
        "title": "AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking"
      },
      {
        "paperId": "1c084d3d922458135d867d202bc62406e0e2d39b",
        "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach"
      },
      {
        "paperId": "b68e715d48aa3b601b2a0bda5447a67fa49a3a1c",
        "title": "Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples"
      },
      {
        "paperId": "f577f727677ecf602f0e1115e2d8bdd11a24492d",
        "title": "Large Language Model Unlearning for Source Code"
      },
      {
        "paperId": "058cead434dd80ae506f793a59d1d5e2281dc1f5",
        "title": "A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation"
      },
      {
        "paperId": "c7015e293bd768f023c4e5b3458ffceff5b4a242",
        "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning"
      },
      {
        "paperId": "c0769f42c71a091546bc1c8966e50e9b8efae476",
        "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning"
      },
      {
        "paperId": "cddda355372cd99a10897c646051554bbbb358e9",
        "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs"
      },
      {
        "paperId": "6d7c4cd52faced2f265d3306b1d8de4888be2902",
        "title": "Large models in medical imaging: Advances and prospects"
      },
      {
        "paperId": "c0edea42d42ce48b8bdcac1dfab06dd16d818397",
        "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?"
      },
      {
        "paperId": "2704333f86b6657cceccb4246e70d2655527154a",
        "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention"
      },
      {
        "paperId": "5811992f41dbf738801e0c884c629ea3742dda04",
        "title": "From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation"
      },
      {
        "paperId": "7273508bf00fa48af4796762be8c7cff54c9cb43",
        "title": "GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks"
      },
      {
        "paperId": "838b30ca829c199d9f6c44ff2d9da4a1a7d78367",
        "title": "Reranking-based Generation for Unbiased Perspective Summarization"
      },
      {
        "paperId": "2a5fab120f3be06a8e473dbc90fcc6a4e06c95b1",
        "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models"
      },
      {
        "paperId": "49d295e827d4a6a7f5533291044a8e14824b9844",
        "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations"
      },
      {
        "paperId": "059818cdaa6310d144e98b8e74af9a5782695a11",
        "title": "video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models"
      },
      {
        "paperId": "9cf20b5aeede51845d20601ccbb939085fc4e631",
        "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data"
      },
      {
        "paperId": "17535e013fda643b5f11f9b9adb35d1e94e0a14b",
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning"
      },
      {
        "paperId": "34fa74eda21ccf9c237b2e4cb43ca98e545bfafb",
        "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning"
      },
      {
        "paperId": "26c22320a8267a5bb0b430badc960fe53031ad88",
        "title": "Learning-Time Encoding Shapes Unlearning in LLMs"
      },
      {
        "paperId": "6e5a822feead0cce3393356d2af14dee31a5877d",
        "title": "Lessons from Training Grounded LLMs with Verifiable Rewards"
      },
      {
        "paperId": "e68035d234a090963bb0989900150c60e7f5d03a",
        "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models"
      },
      {
        "paperId": "213264fe49c8b6a53a44aa03461fab14be0b4eb8",
        "title": "Multi-Interest Recommendation: A Survey"
      },
      {
        "paperId": "2e99003cace9b853ee2ad13c30a5898273569465",
        "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards"
      },
      {
        "paperId": "3ed72bcf49eded503269095058022ee02ddc8844",
        "title": "Context-Informed Grounding Supervision"
      },
      {
        "paperId": "345125deb3994e4a1cb42e2ade14d19b6c733f76",
        "title": "Architecture is All You Need: Improving LLM Recommenders by Dropping the Text"
      },
      {
        "paperId": "d779cb17e2d2dbfb86b1887fb5b8ebf316954ce6",
        "title": "A Comparative Analysis of Ai-Based Solutions for Clinical Documentation"
      },
      {
        "paperId": "7973a601c208c80ba8f9cd6af9751f0609a17605",
        "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization"
      },
      {
        "paperId": "83ff50c729be0191aa766fe214bec3d852283fec",
        "title": "Optimizing Length Compression in Large Reasoning Models"
      },
      {
        "paperId": "080b2aa6b9b819f14d6ec4e66c4ad83714caf3ba",
        "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent"
      },
      {
        "paperId": "26875419a986a787e7431dc4d804eeb6bb75e82f",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization"
      },
      {
        "paperId": "6073986990bc4371966d52f238387fef4fe300e4",
        "title": "AviationLLM: An LLM-based Knowledge System for Aviation Training"
      },
      {
        "paperId": "6aa55900c2bcf1860938520bdf1952f462ed37a1",
        "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs"
      },
      {
        "paperId": "6c3c8a17bd3bf72d9bf8ce6d009d203363faa1e1",
        "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization"
      },
      {
        "paperId": "320d37163084ea31df715902f475acfef0581bd8",
        "title": "Large Language Models -- the Future of Fundamental Physics?"
      },
      {
        "paperId": "502a9fe58f14fa1252648bcb434e7ad04913c525",
        "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models"
      },
      {
        "paperId": "923f22edea4d93a215c0c2113ae39e05422bcfa5",
        "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction"
      },
      {
        "paperId": "813aad5e59199362f6e1ef47722ded137ddd1cd3",
        "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework"
      },
      {
        "paperId": "6de03206638d7d43c4142a1dfc891849fa0ea696",
        "title": "Mercury: Ultra-Fast Language Models Based on Diffusion"
      },
      {
        "paperId": "0933fc39e13991fc9650481f7467b8061423ee87",
        "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training"
      },
      {
        "paperId": "940a6c393afb01004c18aba9a586d993df05b67d",
        "title": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems"
      },
      {
        "paperId": "ecebfa518ff7ce392e5a73559484fb6b0b8493d2",
        "title": "Reinforcement Learning with Partially Defined Rewards and Human Feedback for Energy Efficiency Recommendations"
      },
      {
        "paperId": "672037cf5bd7ea386a197e59b614417396e1ec6c",
        "title": "Align-then-Unlearn: Embedding Alignment for LLM Unlearning"
      },
      {
        "paperId": null,
        "title": "Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention"
      },
      {
        "paperId": "e38f907b6f9f3f4b72008dd3ea685d3b73adddd7",
        "title": "Towards Pervasive Distributed Agentic Generative AI - A State of The Art"
      },
      {
        "paperId": "2297c66ea99a18895b50d02e939ff2098e0b7d49",
        "title": "Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs"
      },
      {
        "paperId": "f100df7dab0ec03bb17df2186461f7ed31e6f6c0",
        "title": "Understand the Implication: Learning to Think for Pragmatic Understanding"
      },
      {
        "paperId": "4a230ff814e6492da8c7dbd1b3d44842df1be989",
        "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
      },
      {
        "paperId": "732742c5121744194788198196405c52682dfc30",
        "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers"
      },
      {
        "paperId": "989d24f3ab2c35b0698c42b800caab5246f6eb53",
        "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
      },
      {
        "paperId": "ac7835481dc5e9774f607e82e9c97faa4880fd99",
        "title": "A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs"
      },
      {
        "paperId": "ffe05a6a329a6203ac6d45d5762143e19bfe4aa7",
        "title": "Socratic RL: A Novel Framework for Efficient Knowledge Acquisition through Iterative Reflection and Viewpoint Distillation"
      },
      {
        "paperId": "f3eaa334adcb315ff5bbdaf86eeee529e01ba78a",
        "title": "Value-Free Policy Optimization via Reward Partitioning"
      },
      {
        "paperId": "854dfb575c1ca27f1da4da6742b6771b92d66ce4",
        "title": "Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability"
      },
      {
        "paperId": "6cd3c72c2baf68873f2d1b4bf52cbfe66766c2fc",
        "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning"
      },
      {
        "paperId": "8d038731d0f071a477d76a0fd69ceedf92223514",
        "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction"
      },
      {
        "paperId": "911b068fe51ec1f7aa9f69cedf8e56e684a450ee",
        "title": "OneRec Technical Report"
      },
      {
        "paperId": "45c84b0c7b42f8d6d06e219c6c92d907ccebcfcc",
        "title": "Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs"
      },
      {
        "paperId": "eceed697eb1d38ccca7d3001f61f595db8a75fac",
        "title": "Flexible Realignment of Language Models"
      },
      {
        "paperId": "3ff0d33d002548e90ce4e5afc7f937a4ac1a75f7",
        "title": "Rethinking DPO: The Role of Rejected Responses in Preference Misalignment"
      },
      {
        "paperId": "1609b3fb46f8e160129150e01fc820cacdd5c647",
        "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers"
      },
      {
        "paperId": "69c6519c479925beb6f376cd3e4a3e94c69ff54d",
        "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills"
      },
      {
        "paperId": "07a076e92b7a39ec839d3278fe86a10b320ea8dd",
        "title": "Jailbreak Strength and Model Similarity Predict Transferability"
      },
      {
        "paperId": "30a14001f121cc3a98460a90165e0e6dfcf3faa8",
        "title": "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment"
      },
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "a9d825e3009f777cb07a72d07f954bb358be8f8d",
        "title": "Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning"
      },
      {
        "paperId": "a88e4abe02450ad417a2439f6f0e2e8eff886794",
        "title": "Bridging the Digital Divide: Small Language Models as a Pathway for Physics and Photonics Education in Underdeveloped Regions"
      },
      {
        "paperId": "e7b92915a9cd82fbc31388834097d5fd3ecb48c9",
        "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback"
      },
      {
        "paperId": "4f576cc32d6794c7eb8b1b15f0f21036f728f204",
        "title": "Detection, Classification, and Mitigation of Gender Bias in Large Language Models"
      },
      {
        "paperId": "d836e6685e534f58ec92f7ee77b66f0b389adc14",
        "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections"
      }
    ],
    "score": 2666.0
  },
  {
    "id": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
    "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
    "authors": [
      "Guangming Sheng",
      "Chi Zhang",
      "Zilingfeng Ye",
      "Xibin Wu",
      "Wang Zhang",
      "Ru Zhang",
      "Yanghua Peng",
      "Haibin Lin",
      "Chuan Wu"
    ],
    "year": 2024,
    "citationCount": 551,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF data flow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53x~20.57\u00d7 throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code is available at https://github.com/volcengine/verl",
    "url": "https://www.semanticscholar.org/paper/f2d0f3d47ae850f49a58f4977393bd0025af4bec",
    "pdf_url": "https://arxiv.org/pdf/2409.19256.pdf",
    "venue": "European Conference on Computer Systems",
    "publicationDate": "2024-09-28",
    "externalIds": {
      "DBLP": "conf/eurosys/ShengZYWZZPL025",
      "ArXiv": "2409.19256",
      "DOI": "10.1145/3689031.3696075",
      "CorpusId": 272987758
    },
    "references": [
      {
        "paperId": "afb06e773d9f073a885a095a8bbb5a5b761a3ab5",
        "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "6252b0d8ab26a4e71c82221837226fa5f41174ec",
        "title": "The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization"
      },
      {
        "paperId": "dbf829c977c121c3704d070d7800d29fe5914756",
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"
      },
      {
        "paperId": "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c",
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "72f77a393079431e4207b3afe678ee80b420e6f8",
        "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      },
      {
        "paperId": "2712a7c0a8275bd0db91a61790a9e7a7aa7e74b8",
        "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis"
      },
      {
        "paperId": "3eec0c1a7dc0d364d23e2e4544bf8772f5f8ffa3",
        "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference"
      },
      {
        "paperId": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
        "title": "A Survey of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ddacee7382548fd9976e846c92500cfa3b6741db",
        "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"
      },
      {
        "paperId": "a2eba36b34833621fa70bcc63ba239846bfd529a",
        "title": "GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "a84d6b82947f27bc6bf7f42d69f48b40adcfb6c3",
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "4a697c9a51ea8ecd19b0c9ac255688b147223728",
        "title": "GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models"
      },
      {
        "paperId": "e97addc2c9d137ca53a73d41ad59083c1a4cf214",
        "title": "Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "c12db2e67d1fb289266faa5507ff112c9a062465",
        "title": "Efficient RLHF: Reducing the Memory Usage of PPO"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "c773045f64e5d4e12fb54b95c0cf6c6c999c6a98",
        "title": "Swift: Expedited Failure Recovery for Large-Scale DNN Training"
      },
      {
        "paperId": "1d923b310728ea01a90deba0b4c97b37b55336c6",
        "title": "Accelerating large-scale distributed neural network training with SPMD parallelism"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352",
        "title": "Pathways: Asynchronous Distributed Dataflow for ML"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "774591fdd988eaaff3917e7c5171d044b0843e63",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
      },
      {
        "paperId": "d4f460d668e2863f9b19545e65bf53f1f8324ffd",
        "title": "Integer partitions"
      },
      {
        "paperId": "b0f1223e75b763b4a3579c6a546f10694792b7b0",
        "title": "Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training"
      },
      {
        "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
        "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
      },
      {
        "paperId": "2492ccc04932d87a07ae66b0e10b7ca37ef89693",
        "title": "RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d",
        "title": "DAPPLE: a pipelined data parallel approach for training large models"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "99ecfb3a86f4379ea935bfce5d63f5962c5667fb",
        "title": "Swift"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8",
        "title": "PipeDream: generalized pipeline parallelism for DNN training"
      },
      {
        "paperId": "76c929af6735cdff2c4badc9a9c8f39d15ea3e70",
        "title": "A generic communication scheduler for distributed DNN training acceleration"
      },
      {
        "paperId": "c95383f251a62c63217586059c67f63507c3e839",
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
      },
      {
        "paperId": "00c957711b12468cb38424caccdf5291bb354033",
        "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
      },
      {
        "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
      },
      {
        "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
        "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
      },
      {
        "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
        "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
      },
      {
        "paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8",
        "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"
      },
      {
        "paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf",
        "title": "Horovod: fast and easy distributed deep learning in TensorFlow"
      },
      {
        "paperId": "82a262a2034b349abaa720c7f8229a0ef19e87cd",
        "title": "RLlib: Abstractions for Distributed Reinforcement Learning"
      },
      {
        "paperId": "f83a207712fd4cf41aded79e9e6c4345ba879128",
        "title": "Ray: A Distributed Framework for Emerging AI Applications"
      },
      {
        "paperId": "601e39099068f64b650826a1c072272ced246ebd",
        "title": "TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "2e035b44c58eb9c24809d5af8c96162eed25358a",
        "title": "Dynamic Resource Management for Efficient Utilization of Multitasking GPUs"
      },
      {
        "paperId": "bbb9c3119edd9daa414fd8f2df5072587bfa3462",
        "title": "Apache Spark"
      },
      {
        "paperId": "a620d007603111ae263c5769c9dc9ac37efd2ddb",
        "title": "TensorFlow: learning functions at scale"
      },
      {
        "paperId": "109b416bdbf1739373638eb7e5b37f5d475fd40e",
        "title": "Simultaneous Multikernel GPU: Multi-tasking throughput processors via fine-grained sharing"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "035740197ba476892d6bc844436d39f3eedf4bb0",
        "title": "Efficient GPU Spatial-Temporal Multitasking"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "98ca08bdb4092d59ef6efcedf8003239f89cf58f",
        "title": "Naiad: a timely dataflow system"
      },
      {
        "paperId": "8aa5d3201c293d29a6702afa0ea2354d53d5bf3f",
        "title": "Collective communication: theory, practice, and experience"
      },
      {
        "paperId": "ed8f6a72bd8903078cfaef866ca7d71e12c5b8f1",
        "title": "Dryad: distributed data-parallel programs from sequential building blocks"
      },
      {
        "paperId": "dc3ceea37d4bea4332287717b943d2b363070d9a",
        "title": "The SPMD Model : Past, Present and Future"
      },
      {
        "paperId": "24251f02c34f32b1dd96572a1d984c4463a26a10",
        "title": "Validity of the single processor approach to achieving large scale computing capabilities"
      },
      {
        "paperId": "d83b906de413e9e2034702d0ed06f68dead18d7c",
        "title": "The Number of Partitions of a Set"
      },
      {
        "paperId": null,
        "title": "{ CheckFreq } : Frequent, { Fine-Grained }{ DNN } Checkpointing"
      },
      {
        "paperId": "96acd6b1c1528d3f50f71083c88d84d619b5b8b8",
        "title": "PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"
      },
      {
        "paperId": null,
        "title": "PyTorch implementation of advantage actor critic (A2C), proximal policy optimization (PPO) and scalable trust-region method for deep reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Flexflow:Aflexibledataflowacceleratorarchitecturefor convolutional neural networks"
      },
      {
        "paperId": null,
        "title": "Reinforcement learning coach by Intel"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Author manuscript, published in \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (2011)\" Preference-based Policy Learning"
      },
      {
        "paperId": "627be67feb084f1266cfc36e5aed3c3e7e6ce5f0",
        "title": "MapReduce: simplified data processing on large clusters"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Exponential polynomials"
      },
      {
        "paperId": null,
        "title": "2023. OpenRLHF: A Ray-based High-performance RLHF framework"
      },
      {
        "paperId": "f126817e63fbe081ae6ba40486a707c32e18edca",
        "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation. Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences"
      },
      {
        "paperId": null,
        "title": "2022. Orca: A distributed serving system for { Transformer-Based } generative models"
      },
      {
        "paperId": null,
        "title": "2024. A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization"
      },
      {
        "paperId": null,
        "title": "Collosal-AI"
      },
      {
        "paperId": null,
        "title": "2023. LLM-Analysis: Latency and Memory Analysis of Trans-former Models for Training and Inference"
      },
      {
        "paperId": null,
        "title": "2022. { Check-N-Run } : A check-pointing system for training deep learning recommendation models"
      },
      {
        "paperId": null,
        "title": "2023.Sarathi:Efficientllminferencebypiggybackingdecodeswithchunkedprefills"
      },
      {
        "paperId": null,
        "title": "2022. Alpa: Automating inter-and { Intra-Operator } parallelism for distributed deep learning"
      },
      {
        "paperId": null,
        "title": "EuroSys \u201925, March 30-April 3, 2025, Rotterdam, Netherlands"
      },
      {
        "paperId": null,
        "title": "2023. AnAdaptive Placementand ParallelismFramework forAccelerating RLHFTraining"
      },
      {
        "paperId": null,
        "title": "2023. TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference"
      }
    ],
    "cited_by": [
      {
        "paperId": "11153ef2b7269a03c53ab534c4ac038024e959db",
        "title": "Executable Counterfactuals: Improving LLMs'Causal Reasoning Through Code"
      },
      {
        "paperId": "646eef2e25ae06feeb7e761cd34cd234fb2a4c80",
        "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning"
      },
      {
        "paperId": "82a1759629f4ec4b55b3326e86e5cf328d24cc64",
        "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models"
      },
      {
        "paperId": "29c3368b63f7aa43d7e1b4928cc8a1a1e5693610",
        "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning"
      },
      {
        "paperId": "1a9c1ec7e03fbebbae9a8d261049d6bd1cb57d47",
        "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation"
      },
      {
        "paperId": "da3fbe3617acc5f8cf385ef39e13e2a041c9fc64",
        "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression"
      },
      {
        "paperId": "50f863402797cb2e05f2fba04c440d4a2d30c579",
        "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead"
      },
      {
        "paperId": "0451059a51e04456d20e5bd0b1005392ca2aae34",
        "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization"
      },
      {
        "paperId": "db48a10aef628d4af7337c92c3feba2ac812fc63",
        "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration"
      },
      {
        "paperId": "028a659689dff9d0fca2101163e5d71b8ddeb3af",
        "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks"
      },
      {
        "paperId": "e2199df22da4ce45e66852b97cb387bb6759d73a",
        "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective"
      },
      {
        "paperId": "a83190e397a525b02bf1fae6e225da3f4098794c",
        "title": "GEM: A Gym for Agentic LLMs"
      },
      {
        "paperId": "e8df3d563de8642b4e774be2e947b787df7c5ab7",
        "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning"
      },
      {
        "paperId": "31eca0d1ea5f1f291f210bf67fb9f2d77c031a37",
        "title": "Prompt Curriculum Learning for Efficient LLM Post-Training"
      },
      {
        "paperId": "492a99e7c25ed0f96ad65c2cdc88cbb2b3dfafe3",
        "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators"
      },
      {
        "paperId": "cde02dfa5c320ba84050176a4f97722704bc02d4",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense"
      },
      {
        "paperId": "a0257b94224ee92466fd682af58313bad85e8b48",
        "title": "Pay-Per-Search Models are Abstention Models"
      },
      {
        "paperId": "489a24c6753ebe1958e67404747f654dc0dc16d1",
        "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning"
      },
      {
        "paperId": "eb549b0e76c87fb1ec2b6d412710322e46cb51dc",
        "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum"
      },
      {
        "paperId": "1f9a5128156abe4dbe92b4f8d52bbbeaeee40e45",
        "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs"
      },
      {
        "paperId": "7bf4c1a6d17482e4edf59173586ad81e3e553edb",
        "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration"
      },
      {
        "paperId": "6d3646d4ebe0c3ab7c9af63589a93e6c57d504dc",
        "title": "It Takes Two: Your GRPO Is Secretly DPO"
      },
      {
        "paperId": "954262d662696e7a025568fffb19eb9e81214b3d",
        "title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models"
      },
      {
        "paperId": "a1421fbc8abc1a48644a6611b62a969872002dfb",
        "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs"
      },
      {
        "paperId": "7fa80662c035249ae4fff8d538041ab2214bc33b",
        "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?"
      },
      {
        "paperId": "ee66bd1639193012d60a918bde515e145308470f",
        "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning"
      },
      {
        "paperId": "f19ad49cf23159283e6d700ef84c6fc14e183c21",
        "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models"
      },
      {
        "paperId": "54936a36fead748c0cb8a84933a3967609928838",
        "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning"
      },
      {
        "paperId": "40d5a9a99d4f71eb3380e4ed0a114990bebd9103",
        "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models"
      },
      {
        "paperId": "41f3208b965effd7d7ac37eb5e81fbd0cf7da07f",
        "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap"
      },
      {
        "paperId": "896061a2a65a701f74c3cda9a00a7a85d4a1aba8",
        "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models"
      },
      {
        "paperId": "ea7960535d0854a65a44f58ef7dcc5fdeeaba110",
        "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection"
      },
      {
        "paperId": "9326860f371f8e46512340ec6f14acb2e64c00ae",
        "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models"
      },
      {
        "paperId": "dc504efda638aecc57206ac9f7f300575222f120",
        "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning"
      },
      {
        "paperId": "e4312a45b5c38916932c58e1a87cb83cd45196b6",
        "title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling"
      },
      {
        "paperId": "0bc3c6728e458bb6da02a73e8d2937e83e56d695",
        "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning"
      },
      {
        "paperId": "e069517326e61e6bec38391c4cc0115982301ed8",
        "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse"
      },
      {
        "paperId": "8c8d74daa222061151dcf0bcdc7dbc2764951590",
        "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners"
      },
      {
        "paperId": "829d1b23a7e057df4122910086b7417aa1d55a4a",
        "title": "R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning"
      },
      {
        "paperId": "12c799339cc26da96892e4541b598ce3e2f35bcc",
        "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning"
      },
      {
        "paperId": "059a907c71b5daab58788c17b65e918b8d882281",
        "title": "Nudging the Boundaries of LLM Reasoning"
      },
      {
        "paperId": "4d1a1bb2bf6d2916913b3b853516dc1dac9ea0d8",
        "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models"
      },
      {
        "paperId": "c9b28422d0dd3f23caf9cc48c5d5767201b8c292",
        "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation"
      },
      {
        "paperId": "572a26b581046bd020421a433590a1a540ad61e3",
        "title": "Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective"
      },
      {
        "paperId": "42c010fae77a00108d41d76b36b928a31d232763",
        "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards"
      },
      {
        "paperId": "d1c6fae67b3a48f4b90fa471077b534e1e87183b",
        "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models"
      },
      {
        "paperId": "42c563170744aa68d06caea3a97d8baec21f53a9",
        "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning"
      },
      {
        "paperId": "3e7c5c856ce9614445c4311d1c8b0fbafbce066b",
        "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning"
      },
      {
        "paperId": "a300d633112e9636a54c3e5f9abc43cd54cf85d4",
        "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training"
      },
      {
        "paperId": "7da0852f60fb5abf7a7172b4419c19fc292ece9c",
        "title": "Scaling Generalist Data-Analytic Agents"
      },
      {
        "paperId": "4294a39801a1365a7d97d0aca7adb75e83aea048",
        "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models"
      },
      {
        "paperId": "498a98386d007a2fdb88fabcd5f13b1f972697fc",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression"
      },
      {
        "paperId": "8678cd32753a92917abf211fe6129e39a58925db",
        "title": "AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration"
      },
      {
        "paperId": "118cb77198d437ef7f6f51c5924b36c6b93dc567",
        "title": "GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners"
      },
      {
        "paperId": "4f36b972658bebe1dda299fbdf8c8e468b777935",
        "title": "Rethinking Entropy Regularization in Large Reasoning Models"
      },
      {
        "paperId": "f6b8e944961b70595c5f476eec49d4d5d78c6679",
        "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval"
      },
      {
        "paperId": "3df5cc0c2fdf3ee64f7943053592c7254f890da4",
        "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models"
      },
      {
        "paperId": "49d8b888a6061e84850ad300c37bfa95489c8aa1",
        "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention"
      },
      {
        "paperId": "07ba7629ff0e2c9edb2eef8faaaf8478e156b347",
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks"
      },
      {
        "paperId": "6a88469b41fb025b4e37326189e5145db4bb30f4",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following"
      },
      {
        "paperId": "f43739359a51fe54c5b9d18b7b62595e3c88c1c4",
        "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends"
      },
      {
        "paperId": "94317dab6cb3d29548abb365c87128aa53ce7d49",
        "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks"
      },
      {
        "paperId": "bb869a9f5829d73a6909643440495abacc7f2cde",
        "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning"
      },
      {
        "paperId": "6794971b191f82e7b78f4a1cb8e09dca2d2b8e82",
        "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search"
      },
      {
        "paperId": "1a0fed22c7bee7df5556555293505e7e6f92a0a9",
        "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning"
      },
      {
        "paperId": "aa475f7900389f242dbe344c46e3474043759398",
        "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm"
      },
      {
        "paperId": "61b763922db3e3d293ecd85c54305fd1f996320f",
        "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs"
      },
      {
        "paperId": "81696e83b871ad9c24109cba8f79d1b75003f0f0",
        "title": "Anchored Supervised Fine-Tuning"
      },
      {
        "paperId": "952636193d810c7fe6b502a7da36f583d19feb23",
        "title": "How LLMs Learn to Reason: A Complex Network Perspective"
      },
      {
        "paperId": "c024b621d8fbb4c489d6a0c6980e647489c1a509",
        "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation"
      },
      {
        "paperId": "fd8c852f07ceb02a1fab80c443f068f159b112d3",
        "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models"
      },
      {
        "paperId": "6465b370e58c5abd010d9e07aa11fd0a51bd9465",
        "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR"
      },
      {
        "paperId": "ca60fbfc25b9a9ce3b0a6d77d9106a7d95e3c0e3",
        "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning"
      },
      {
        "paperId": "ed8c8d3d4417a1499c2bbfeda6426a7a6e20e293",
        "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking"
      },
      {
        "paperId": "c89873a13ec6ed1c2ac70806fa30ad19f7be6ebf",
        "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs"
      },
      {
        "paperId": "598291ebed8ecdaf42aa91aa46afa862e5c1c9df",
        "title": "SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts"
      },
      {
        "paperId": "9f6f36d1a3e4a1d7f78aaa4e3565bfe1fff53339",
        "title": "Causally-Enhanced Reinforcement Policy Optimization"
      },
      {
        "paperId": "59e218c85c3048d4249a91e5ad4a6b2f3798c73a",
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective"
      },
      {
        "paperId": "ad8d4c717bc0448de8e86deb637e5113cde2685c",
        "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning"
      },
      {
        "paperId": "d29c63863adb0b979e3ef7832dd422b6fc0a35fa",
        "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning"
      },
      {
        "paperId": "35ef4fb1b89f812eea2e2929706e93c426fa8e7e",
        "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping"
      },
      {
        "paperId": "df0a07beb75eea9c71ca70344726e5c822974c40",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
      },
      {
        "paperId": "5fcef621828668f59c7c00f5d2b86953a9eb9072",
        "title": "SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin"
      },
      {
        "paperId": "b23591aefc039307fda9d50d8494769bf4d28420",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models"
      },
      {
        "paperId": "f742059d91f61b24efe7e23c248195079389d7f8",
        "title": "Think Socially via Cognitive Reasoning"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "191047ab6432d519c08712d3178db6c13be7d985",
        "title": "RISK: A Framework for GUI Agents in E-commerce Risk Management"
      },
      {
        "paperId": "eb50e98cf5d73b4ae81d18f55d0ec1cd3356c037",
        "title": "Towards Strategic Persuasion with Language Models"
      },
      {
        "paperId": "30149a67c0ed480fe9f24847028357a5e90c96be",
        "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training"
      },
      {
        "paperId": "00965f7fc184245a81e1308a25e9233fe7f94a82",
        "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement"
      },
      {
        "paperId": "15f77ae9912f9e289361298753707c12af961fad",
        "title": "bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs"
      },
      {
        "paperId": "714abcc8b1c6527844a5f8ed691f1a85e7fb25a7",
        "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "9f251ca2f98310e80854d751683baf9466222fc7",
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning"
      },
      {
        "paperId": "2e302fe69bdc313de04d364c90a570cedb22cc70",
        "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning"
      },
      {
        "paperId": "e3eccae50df00df046262b119051c8db21ba29a5",
        "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation"
      },
      {
        "paperId": "239769a09308ba63fe15a35b5d914670e0c38565",
        "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system"
      },
      {
        "paperId": "a6478bab4f1985f3eb008e7ebca4a84a1af273cd",
        "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generation"
      },
      {
        "paperId": "7f1cdaad0af5a80c509ebb00590f6ee9599bf882",
        "title": "MAPO: Mixed Advantage Policy Optimization"
      },
      {
        "paperId": "d2e7c57f75d2493dc0c9e87e390d13a8bf6d993b",
        "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling"
      },
      {
        "paperId": "1d0e1d6538cb6c56787354f2aefbaa43a77d100b",
        "title": "LongCat-Flash-Thinking Technical Report"
      },
      {
        "paperId": "a8bf6fd51e10215ca71db02becc7a59b67a8df84",
        "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning"
      },
      {
        "paperId": "2e3d2cb75be2d8939387f9173b68cd81315ec694",
        "title": "Agentic Reinforcement Learning with Implicit Step Rewards"
      },
      {
        "paperId": "d9aea14072fbc8872e967edbc2d0c047c2aac4a6",
        "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs"
      },
      {
        "paperId": "8b0c3e63a5b68816d1ab3a605a4cac7e869385cc",
        "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "44c875bd75dfeca46096b44bd9b7ef1a53bcb28c",
        "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation"
      },
      {
        "paperId": "a6fbcfabbb20c8848364ec44c8a1dea18be00efe",
        "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning"
      },
      {
        "paperId": "4d01737b45ca7b1b42496859d9f487e45d65228f",
        "title": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning"
      },
      {
        "paperId": "18b891332767b951ff8ed49f31fb4b8892e66dc8",
        "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support"
      },
      {
        "paperId": "cd29f36cf7952f6e41fb9563846a4b5363179671",
        "title": "FlowRL: Matching Reward Distributions for LLM Reasoning"
      },
      {
        "paperId": "435c4584cb5640ab4a093c6c22e2f784ed9b90ed",
        "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning"
      },
      {
        "paperId": "8e84909fcadd50be875ce2522d9413d9a9cc6c53",
        "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook"
      },
      {
        "paperId": "10f479757f9db5edc486cde16e272fe3c6ecabd2",
        "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision"
      },
      {
        "paperId": "e6bc12d64c619ddd429a9ee17ccbf5eeb59d0854",
        "title": "Aegis: Automated Error Generation and Identification for Multi-Agent Systems"
      },
      {
        "paperId": "37d7e1b913710dd74c54772067b88527e05bde73",
        "title": "Single-stream Policy Optimization"
      },
      {
        "paperId": "3b37057e5bbd0a4121f0e1ee7dcf4c0a3094f229",
        "title": "Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations"
      },
      {
        "paperId": "e99667033b8ac8e31520fbe4fba4323ef21f5cb7",
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models"
      },
      {
        "paperId": "b02c7841eed5191aa6c1dac27cd50694971a912f",
        "title": "FunAudio-ASR Technical Report"
      },
      {
        "paperId": "014115bc6bd2f56c362a444d610b28c393da74f9",
        "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
      },
      {
        "paperId": "17747af1fd599a93b9d06765ab3839ccb8a7fd65",
        "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization"
      },
      {
        "paperId": "1b2ef8ba05fba415b5bd5d3a592c35d9d1136ec5",
        "title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents"
      },
      {
        "paperId": "b6f8f600547d406e9b7e86b65ce67488fab7694f",
        "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL"
      },
      {
        "paperId": "cc1658260272b8deb85fd4de7edba36ca83ccc95",
        "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents"
      },
      {
        "paperId": "bca7f4dd4db559d6772b470d9fe5391e3608cc8c",
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"
      },
      {
        "paperId": "30da58c425d4e2a5e3c0b774cf8302d2fcf9ce51",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "3a5548b9e9bb7b83e2179ca261f8f8aa0d5f1966",
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
      },
      {
        "paperId": "039625bbc94ffa91291699e1f0c1d2a5e254a236",
        "title": "K2-Think: A Parameter-Efficient Reasoning System"
      },
      {
        "paperId": "ea6c4230657ee48a88e83ccd70e8ed3902f0aad4",
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey"
      },
      {
        "paperId": "9bf180774da3f753435ff18e4947a4df7b84cb4c",
        "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "baf5c91d63e975865a1f742c957712a71898a4f2",
        "title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning"
      },
      {
        "paperId": "aa0939999d3821efb71fb8ad37fbc23dbd3bbde0",
        "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning"
      },
      {
        "paperId": "2bb47d7a9cafeff591b19840d429b376ed8c9921",
        "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning"
      },
      {
        "paperId": "92c39c226cb07a768ee11d2ac01aab33534b214b",
        "title": "Self-Aligned Reward: Towards Effective and Efficient Reasoners"
      },
      {
        "paperId": "87c3b9e37b35912097cb8a9e5059bb0c4cc2e3d7",
        "title": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training"
      },
      {
        "paperId": "47ffcbf9a852ab0e36c4fe342e07e486d87c5276",
        "title": "On Entropy Control in LLM-RL Algorithms"
      },
      {
        "paperId": "6d6679bd5bf00db7ea3a8305c6db8c8e73a3997a",
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations"
      },
      {
        "paperId": "6544b41d4412f315842465f11998fa6cd4486427",
        "title": "DCPO: Dynamic Clipping Policy Optimization"
      },
      {
        "paperId": "fe0b4cfccfbb7e701265cabcaf22b5e54004fe96",
        "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR"
      },
      {
        "paperId": "eb6ef63df104c1b35bbc2400f00285b3414400b2",
        "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning"
      },
      {
        "paperId": "86be3bd6da23443b881f37bf4eb05baa7542a4ff",
        "title": "DynaGuard: A Dynamic Guardrail Model With User-Defined Policies"
      },
      {
        "paperId": "7c74d038204f86f47f6be6b489f95b70bfbfd1f8",
        "title": "Reinforcement Learning for Machine Learning Engineering Agents"
      },
      {
        "paperId": "7e4dea45be16b7ffa59d9fdc8dc32dda0b5e2d33",
        "title": "Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward"
      },
      {
        "paperId": "596d9c53f1f9b98facbb6b97cff9afb120103835",
        "title": "Reinforced Visual Perception with Tools"
      },
      {
        "paperId": "ca38d3dbf0ca25d4eeeb43becdc50aa45052e22d",
        "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use"
      },
      {
        "paperId": "8784c5dc46d470b7fefb3a729adc7fe89db38e5a",
        "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning"
      },
      {
        "paperId": "e08d7bcc444875943e76625d9f92f9e9051d5408",
        "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes"
      },
      {
        "paperId": "39528bd930a4796d95e78b18d390fa73f776c00c",
        "title": "Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning"
      },
      {
        "paperId": "80e921a2fdbf52f160116ef808bc651e0eac15f0",
        "title": "RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use"
      },
      {
        "paperId": "00bbe58c8de1a277fdb46499101407af0fa7e5ac",
        "title": "ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking"
      },
      {
        "paperId": "974784443bc46fdbd0a66becae859511b39887b2",
        "title": "Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning"
      },
      {
        "paperId": "d4e4dfcad0d47aa5962dcca07c93dba9b1bde982",
        "title": "AWorld: Orchestrating the Training Recipe for Agentic AI"
      },
      {
        "paperId": "c1f394d5097f4b39edbf3477f72a5293a1cfd3b0",
        "title": "SUMMA: A Multimodal Large Language Model for Advertisement Summarization"
      },
      {
        "paperId": "2354568cc89601c881b4e1653e3021b326139e9d",
        "title": "rStar2-Agent: Agentic Reasoning Technical Report"
      },
      {
        "paperId": "9aa2b5470874ca27f41354ab86c61871ff39a5ba",
        "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems"
      },
      {
        "paperId": "146cda64133619f15a00f96ca0b66c0b620d34ea",
        "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning"
      },
      {
        "paperId": "021ae6869a956b22d59e32e9efc1d4aee2a1c089",
        "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning"
      },
      {
        "paperId": "eb7710c1bb4a2cce996da77a5514d13c69e868ec",
        "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control"
      },
      {
        "paperId": "9df0947ec8a14c43e69f3b72ece5e9af2b8e8215",
        "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning"
      },
      {
        "paperId": "563ebf2ff90f5e7a0b2526b5110539122216fee6",
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning"
      },
      {
        "paperId": "211ba4a0b12fbb97eedcc2649861d9073e289b23",
        "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning"
      },
      {
        "paperId": "58859f92a30b094a2852e890419c8eda87b8d17f",
        "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use"
      },
      {
        "paperId": "18d83103fb98905ccbce420987470eb2ea021187",
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "paperId": "7f30845e988d53f90e965c2f65a34c84fe771ea4",
        "title": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models"
      },
      {
        "paperId": "4c8dfac8ae1e7aa630ab298a67d4acef4bed109b",
        "title": "Proximal Supervised Fine-Tuning"
      },
      {
        "paperId": "6fb051cb422675f1497875c0bc845b0809f1d533",
        "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling"
      },
      {
        "paperId": "880eacd1b524bb6c1fcea57febc85641be3ee2d5",
        "title": "Towards Better Correctness and Efficiency in Code Generation"
      },
      {
        "paperId": "ee8a540d0d0eed6c08069cab7c686f1c0ca503fd",
        "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning"
      },
      {
        "paperId": "d4fce730c7ceb40d92612f9abd7eb12a6d13a652",
        "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning"
      },
      {
        "paperId": "4908fc9a9e4af6803263652d79a0863159c3ad58",
        "title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning"
      },
      {
        "paperId": "5499fad39bee3029b4eb410a6cf44b79835b955a",
        "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning"
      },
      {
        "paperId": "ea7849c0fe8b73940e8e1d1b63f093cfa1f7cd48",
        "title": "Intern-S1: A Scientific Multimodal Foundation Model"
      },
      {
        "paperId": "572e4869025e3e3be814a33c673520cb14051e40",
        "title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning"
      },
      {
        "paperId": "289cf1394f623c2e23cb12706756d107899d4604",
        "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance"
      },
      {
        "paperId": "ee26c8ca8aee85141075782bcf4d5baa8c4f61a3",
        "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration"
      },
      {
        "paperId": "a29904e740589b5edd0bdad3b2125f4fb88a9b3e",
        "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR"
      },
      {
        "paperId": "d9fa21a2655730470cf8bfb35f545d672eb9b3ff",
        "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward"
      },
      {
        "paperId": "e942bbc1ea5a11502b1dab90f5ac1e30ab895458",
        "title": "SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression"
      },
      {
        "paperId": "ea974e5365960629edc0588b9f039b91c49cf012",
        "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback"
      },
      {
        "paperId": "6a9ace63fe89eb19a57bf73c4cc34e65284df206",
        "title": "QuarkMed Medical Foundation Model Technical Report"
      },
      {
        "paperId": "c78b325b068cec59b6ae23ac1285c65e2542a353",
        "title": "Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs"
      },
      {
        "paperId": "a20a1baccc3ed4b8a223b3114ab179a298e2e4fc",
        "title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling"
      },
      {
        "paperId": "2e138f801a2f27b59f0c05e15e7c9a7a3ee94039",
        "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models"
      },
      {
        "paperId": "a6bfbe562609778a962e480b147af779ee67683b",
        "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments"
      },
      {
        "paperId": "20e402b04ebbc2345975fe126f5e888cb47241e4",
        "title": "Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes"
      },
      {
        "paperId": "9d5199546bfe6ec87eb58221e23d41be36b6d85c",
        "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward"
      },
      {
        "paperId": "772200bb0c18fd78b28e4feb66f3f821cff99598",
        "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models"
      },
      {
        "paperId": "3ce063a0c726caf3a3ae8735daec419203f075d2",
        "title": "Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards"
      },
      {
        "paperId": "78e03fb22a051c82bfa9e2051cd66245eba0f2dc",
        "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning"
      },
      {
        "paperId": "991638fced3b73f9230ace7fe646031cd3fc9bd7",
        "title": "CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization"
      },
      {
        "paperId": "3d9a5b769509909d8da60b0b9dbf2b79ec991597",
        "title": "Aryabhata: An exam-focused language model for JEE Math"
      },
      {
        "paperId": "d1ba86b844e2a95a1bb6e723440de8eb8c535bfb",
        "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments"
      },
      {
        "paperId": "7060cb3aa27e5ba766332fa8cf2ff293a3a372c1",
        "title": "InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling"
      },
      {
        "paperId": "640779e0ca75fd7b1d5d035ec20fc0f3350de837",
        "title": "Compass-Thinker-7B Technical Report"
      },
      {
        "paperId": "c5b9e6d2797e079df1a9ba213c144a46da055703",
        "title": "PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning"
      },
      {
        "paperId": "d72aa4d434454c73bf59d9ee9cc9ac033151829e",
        "title": "WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library"
      },
      {
        "paperId": "bd882244d2d84d7a455fdd1af5198f8fbdcdd228",
        "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation"
      },
      {
        "paperId": "acb36460f36b7195e05bf4da09ab5efe9652543d",
        "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision"
      },
      {
        "paperId": "e28ccea82e23a44b13aad780fee634f6f6568b78",
        "title": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning"
      },
      {
        "paperId": "cf8848e72c973726f2c89365a9301c83ed9eda43",
        "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation"
      },
      {
        "paperId": "1232596c191d17370fdfdaeaff7169fd0a5e3154",
        "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning"
      },
      {
        "paperId": "1414653547b72a23d2c0ca987d270fb236c9105a",
        "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"
      },
      {
        "paperId": "c181ab579252e789862e4ce9e56af2ae8f251dc8",
        "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning"
      },
      {
        "paperId": "6b3e64ab07bdb533d26bc4a78dad189e02a9bcb8",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "6cbd611a1ed36908b4a74dd747401229f01f67a7",
        "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"
      },
      {
        "paperId": "cc35548ca7f8b797402e9a95ff901b642af0b2b3",
        "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"
      },
      {
        "paperId": "3da24e1feaa1135432a6cd27d7ab01e4e1679f1d",
        "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy"
      },
      {
        "paperId": "726742d2dc0ef40e172b7b7f6962a6594eec3147",
        "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent"
      },
      {
        "paperId": "f93f4f3ea885d5cef13af2d62b29393b92dbe8b2",
        "title": "Exploring Superior Function Calls via Reinforcement Learning"
      },
      {
        "paperId": "4d1863da264b7f4c5eea6e83f980dba244ba53d7",
        "title": "COPO: Consistency-Aware Policy Optimization"
      },
      {
        "paperId": "cac217dee3a84a7e6a6c97b6f310d8479e92792c",
        "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning"
      },
      {
        "paperId": "b59f471f00533d4b9dc34f3ccc4122fdc9fedf56",
        "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards"
      },
      {
        "paperId": "fa478eeab535811afe25ae0a5ad01b3d3058e775",
        "title": "VRPRM: Process Reward Modeling via Visual Reasoning"
      },
      {
        "paperId": "09c1844b67c520e964a42f26161561535b6864ba",
        "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice"
      },
      {
        "paperId": "a46ea48b846f50e05267291cd881e9363baaf3c0",
        "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning"
      },
      {
        "paperId": "26bde5ebc6609218ad565cd6ba04f3c3d586e3a9",
        "title": "Self-Questioning Language Models"
      },
      {
        "paperId": "130d872488a20bafd6e270a939486d876cadcaee",
        "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following"
      },
      {
        "paperId": "0ec02408555067cd1b88d40742e3d24dcff215d8",
        "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward"
      },
      {
        "paperId": "5cf103a9610d3f1bb11ec254281f2e01284819b1",
        "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction"
      },
      {
        "paperId": "eebec6ce3af56890ca4eb2337383e11294c365f9",
        "title": "Tool-integrated Reinforcement Learning for Repo Deep Search"
      },
      {
        "paperId": "ae42cc15452b5b6d089bc535cdcd55648c29557d",
        "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning"
      },
      {
        "paperId": "867c73a93cbbe8ce373586b2d5c4c67baafb98c3",
        "title": "Reconsidering Overthinking: Penalizing Internal and External Redundancy in CoT Reasoning"
      },
      {
        "paperId": "5040e9bee84bdfed29cec6933f5a2fd6efda3d88",
        "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale"
      },
      {
        "paperId": "5f34b8b56bc4060e3a278aa3fc858c26edcd27e6",
        "title": "RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models"
      },
      {
        "paperId": "623c1b84461e97c41c78a95796cb44f1d948de3f",
        "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning"
      },
      {
        "paperId": "435fc162f0dbdd0768aefc68781c31cbe4e06f82",
        "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement"
      },
      {
        "paperId": "0b228ce57cf0929eec39e8ae4d450e6f266ea3fa",
        "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner"
      },
      {
        "paperId": "92342a489ca41fe8315881263e81824621a87979",
        "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks"
      },
      {
        "paperId": "b43f8bacd80f32734cdff4f9d8c79e397872aed6",
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"
      },
      {
        "paperId": "9b278fc7e617447a238854cd72279c86fe92b366",
        "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer"
      },
      {
        "paperId": "50d64e8897679cb5315b5a8d4231eb4ae90aad1f",
        "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks"
      },
      {
        "paperId": "b968f58fcc16d25e73e3bf21577ce5229f76c5f8",
        "title": "Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation"
      },
      {
        "paperId": "b284e0b3c5e09079253f025a664fb476d2e65bda",
        "title": "Teaching Language Models To Gather Information Proactively"
      },
      {
        "paperId": "e53914d6bd7ccfa61749e2c74daaca4e65d23592",
        "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models"
      },
      {
        "paperId": "3c8b92a8b6eee1377077ecfea95e53223845af20",
        "title": "Agentic Reinforced Policy Optimization"
      },
      {
        "paperId": "55580865c6ed61bb0f8f68828776ec1efd29d54a",
        "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities"
      },
      {
        "paperId": "618d58b52841a741487215cb04ace9d6b46e7e5a",
        "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster"
      },
      {
        "paperId": "d2cda4397bb3329c4c5b7277b61eeac6e7d98d2a",
        "title": "PurpCode: Reasoning for Safer Code Generation"
      },
      {
        "paperId": "7db22a2a14e4617faf0a3807291b918c4b19a4a0",
        "title": "Revisiting LLM Reasoning via Information Bottleneck"
      },
      {
        "paperId": "9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
      },
      {
        "paperId": "ea04cea955b1d959b0e36942216c07bd87f75bf9",
        "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "f2f6607c759581164d56cece90ad40e87065c451",
        "title": "URPO: A Unified Reward&Policy Optimization Framework for Large Language Models"
      },
      {
        "paperId": "85a11bd16eeb169548764e1e2f699a563315dc34",
        "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning"
      },
      {
        "paperId": "c1f63e6940938953a810f95a4f6f1c94ed9700af",
        "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR"
      },
      {
        "paperId": "5f8582f9b78cd78360aee5496ec9a0f9dc05acb2",
        "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning"
      },
      {
        "paperId": "52f36c2d1f109db3ff7efd862571e563150a0c9b",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization"
      },
      {
        "paperId": "342ac8f2206b052e538af2b92aa50843346fa257",
        "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents"
      },
      {
        "paperId": "fd5a5557fcf8e9ed741bd3bf2d7f218400b2d821",
        "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training"
      },
      {
        "paperId": "27c741e54cedfc7e648103f90b109dd58143fd34",
        "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning"
      },
      {
        "paperId": "63f08c793b32529939de0eddf1a16f29f381f07b",
        "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs"
      },
      {
        "paperId": "8327bd346d109e62ed0730bc5e2e11efdbd43e64",
        "title": "Scaling RL to Long Videos"
      },
      {
        "paperId": "c979aa555c031ee3434caa2b99d52373ab466102",
        "title": "The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs"
      },
      {
        "paperId": "446334ebc07b3c013b461558ecef8a994d10552c",
        "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology"
      },
      {
        "paperId": "c33bc656d087f75ab9ae1b04d5d7d32f240bcbbb",
        "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning"
      },
      {
        "paperId": "70a2663e578c1e7b7b180e131bb36ffd74ff0c41",
        "title": "First Return, Entropy-Eliciting Explore"
      },
      {
        "paperId": "6794ed31259d9e61cd22b8c8e78071c2e4be144b",
        "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models"
      },
      {
        "paperId": "a7d42f60c7db599b3500d31f1353b83c841a3c39",
        "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization"
      },
      {
        "paperId": "c640c653a4db2077899938981df777b532c49848",
        "title": "BlueLM-2.5-3B Technical Report"
      },
      {
        "paperId": "25c9d2d459967582e41601e0b6e63d693e51fd29",
        "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization"
      },
      {
        "paperId": "095feb2a22f64b9a436c69ae318684b6e407cf1b",
        "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning"
      },
      {
        "paperId": "822a806a5bb969f5a20665761b8566faf66a6fb8",
        "title": "Skywork-R1V3 Technical Report"
      },
      {
        "paperId": "00ff1875a01039af7726769d1630d632e5d9d40e",
        "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?"
      },
      {
        "paperId": "0632657e9c329f8fbf8eb2d9f733aefa09b0292b",
        "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training"
      },
      {
        "paperId": "e6dd46403e4077bd02b94d2fdd40b9ff8095c6d4",
        "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs"
      },
      {
        "paperId": "0311a0660b02ec1face3936a81e8ab5186789475",
        "title": "ESSA: Evolutionary Strategies for Scalable Alignment"
      },
      {
        "paperId": "216fd8b8b8f5e7947f6bf1b121a394598934145c",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent"
      },
      {
        "paperId": "7da361cbbf5e0c10e6eec89f8e8e1618a6778b84",
        "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents"
      },
      {
        "paperId": "233a096b55dae7a3da37a1bb43cecf7f7bd111b2",
        "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason"
      },
      {
        "paperId": "80a0548be8c857421aaf0602c6d83d3ef934a251",
        "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent"
      },
      {
        "paperId": "72c749cc9ee05bee3963488a1b1e68f0d63d6015",
        "title": "Look-Back: Implicit Visual Re-focusing in MLLM Reasoning"
      },
      {
        "paperId": "ea9a174cd0bf2ed6ba4747d2c8b35e4c839750b5",
        "title": "Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning"
      },
      {
        "paperId": "341428444a2bf015cb728abd61a9e531c01ac1e2",
        "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess"
      },
      {
        "paperId": "154f3d9c6165fe626e4cedf531074cd02401a6ae",
        "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy"
      },
      {
        "paperId": "a4aad5f43f026aaf2dec5a7de1746deb21eeb1c4",
        "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model"
      },
      {
        "paperId": "99d9293e91abce69918aa7c9db8ba2c6ad646cba",
        "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning"
      },
      {
        "paperId": "d69baac0669a73af35e244c30fd19b2c84ae965d",
        "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models"
      },
      {
        "paperId": "33eb087f52e37349f13c9a44e6bd09d5f89fcbfd",
        "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework"
      },
      {
        "paperId": "2dbd5fd5b92ae36c8d6510754ae31d9bb72940a9",
        "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling"
      },
      {
        "paperId": "43a3c8a9bb672e2f013dc563a178a0dc901f5363",
        "title": "MMSearch-R1: Incentivizing LMMs to Search"
      },
      {
        "paperId": "ab4614e59237bac090d1ea78518adcdf50abac93",
        "title": "R1-Ranker: Teaching LLM Rankers to Reason"
      },
      {
        "paperId": "0f243f282b7502ce688dfd53f65f7952285d7b00",
        "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models"
      },
      {
        "paperId": "b84a918a147f5f1d216d5b241384cfd4f2d9dfb3",
        "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning"
      },
      {
        "paperId": "c487eae0d9b82bc54a777f45d1bec5437d37a0ce",
        "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting"
      },
      {
        "paperId": "e09229591e42c1286b176e4a670bc2cc14d21369",
        "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs"
      },
      {
        "paperId": "b5a02c8d7f00e9c57a70c0d3323e2f92c48aaacb",
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation"
      },
      {
        "paperId": "3e801e70a9a83f3783295fe66b47666310f2b070",
        "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1"
      },
      {
        "paperId": "50553458c41b689d9dda6d6e8c84c152848ab483",
        "title": "A Simple\"Motivation\"Can Enhance Reinforcement Finetuning of Large Reasoning Models"
      },
      {
        "paperId": "358a4e531b5e6c04ba6b6a34108d3ff172bd217e",
        "title": "Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models"
      },
      {
        "paperId": "c854193bab740e43da93f1a260ae6dac10f0bdda",
        "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers"
      },
      {
        "paperId": "b36c3b79d678805b74025497bf3abc6cbe0ee1eb",
        "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens"
      },
      {
        "paperId": "c7015e293bd768f023c4e5b3458ffceff5b4a242",
        "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning"
      },
      {
        "paperId": "5695f983699b36af61851d8025aab9caea970eae",
        "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
      },
      {
        "paperId": "4cfc3ba013e8ff47c2bf9fc45bc82ba15c6d41b0",
        "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent"
      },
      {
        "paperId": "6d7f20de3a43ddd12f1a7a1250466b67b7b07599",
        "title": "EvoLM: In Search of Lost Language Model Training Dynamics"
      },
      {
        "paperId": "aaa17280e628673b68b6c801c6bdc203a0b8ae94",
        "title": "CC-LEARN: Cohort-based Consistency Learning"
      },
      {
        "paperId": "6c5f3dd82ee674d929a8eb4b256ca2045e02ae60",
        "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents"
      },
      {
        "paperId": "69535d3c6ff3238f8e7b2b29c1d40ea9d9d7914f",
        "title": "Reasoning with Exploration: An Entropy Perspective"
      },
      {
        "paperId": "6aa55900c2bcf1860938520bdf1952f462ed37a1",
        "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs"
      },
      {
        "paperId": "502a9fe58f14fa1252648bcb434e7ad04913c525",
        "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models"
      },
      {
        "paperId": "d4117fd026c574e95cbbb74f03034bf66de79951",
        "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning"
      },
      {
        "paperId": "91360031e430be76c20c4b02f3da08c3052d54af",
        "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"
      },
      {
        "paperId": "657bc0ea7b2a8a27b01b0987b74f7531fc2a7d2e",
        "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy"
      },
      {
        "paperId": "3c27f1433f893bf8071bd0e0a6d8ac5a860d81ee",
        "title": "FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design"
      },
      {
        "paperId": "882acbb20a0e748434df1fed3a32b6f006f5b7c4",
        "title": "BOW: Reinforcement Learning for Bottlenecked Next Word Prediction"
      },
      {
        "paperId": "6510d0b8539dcc11eeb771d12ea1762b419b0328",
        "title": "Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM"
      },
      {
        "paperId": "a50a3e3bd981825b3737dcf9cce73aeeb1adefd9",
        "title": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification"
      },
      {
        "paperId": "c33e8d1dbcc41a8f91579f60fd01f2f0231d0ec3",
        "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier"
      },
      {
        "paperId": "14998ec52d42803c891295d00e1dbd8183427342",
        "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs"
      },
      {
        "paperId": "568dde53ebd55103129eb9cdbe86014d34d8a47d",
        "title": "Magistral"
      },
      {
        "paperId": "f961589644c0030dbc2f2d53398fd03934e13bfa",
        "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing"
      },
      {
        "paperId": "eff086f0a6d66b6514322c6832f2469d057f33e3",
        "title": "CoRT: Code-integrated Reasoning within Thinking"
      },
      {
        "paperId": "797c2815a3224284e886a3a68a76a6a26b69973e",
        "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games"
      },
      {
        "paperId": "a8b0a54975067014d4c5017a7a07b9aa05c8e8c0",
        "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning"
      },
      {
        "paperId": "2780ac7088bfbc4e2e681f0053f4f014065147a4",
        "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling"
      },
      {
        "paperId": "cf92906ac18467b2bce3cf1cbd77bc4ed1201352",
        "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "ff76e9e076f4aa1b6fb9b1826297553973a54dc4",
        "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
      },
      {
        "paperId": "82f59319e581cdfe16a97fe29bec6215ad818a81",
        "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions"
      },
      {
        "paperId": "bbac26c776bd7312c6051e8abf39427e2f61cd88",
        "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards"
      },
      {
        "paperId": "0e426b088f5a02157577223d40f53fa966a53697",
        "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification"
      },
      {
        "paperId": "80020c414b4454468fe097d54d9b26f73cc20913",
        "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models"
      },
      {
        "paperId": "6089f6286fcc843181d66d49cc5dedb540ac5abc",
        "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models"
      },
      {
        "paperId": "d7e878f17db4d8b2297dbc75af4e22ee1ebaec1d",
        "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "7ebde24c82c80d14b3d9fd820673a25daa450d38",
        "title": "Reinforcement Pre-Training"
      },
      {
        "paperId": "2dfe824e0147ac2cd14ebdedbeebe89459552dd6",
        "title": "How Far Are We from Optimal Reasoning Efficiency?"
      },
      {
        "paperId": "40bf9f6b3c4ae001c7eb15853b164721f0d8194c",
        "title": "Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library"
      },
      {
        "paperId": "9ce396c6a23b83415c10c1b489581ebd61909df6",
        "title": "Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning"
      },
      {
        "paperId": "4905363f70809f270979815fa286ce9dd89c0c7f",
        "title": "Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning"
      },
      {
        "paperId": "2a471f03cb31cab24def7419d5a70b85c1a791a6",
        "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation"
      },
      {
        "paperId": "f0143edd509e55ec2337849aa2ebef0a432f4cca",
        "title": "TreeRPO: Tree Relative Policy Optimization"
      },
      {
        "paperId": "7fbabd628f9b1c08c5a357b7068a0effaacc820f",
        "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay"
      },
      {
        "paperId": "a9c54b2ea149da36c8b8e5d5b9a9f4bddeae5c86",
        "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design"
      },
      {
        "paperId": "94bbe80766824ca4163b2578e19daf6b9f2b1fd6",
        "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning"
      },
      {
        "paperId": "a8c5f91c6ad306734187f6e85a60a4a1ffb456c3",
        "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning"
      },
      {
        "paperId": "ab683c941931da5c6351679cb346e4d1ad5b63f4",
        "title": "Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement"
      },
      {
        "paperId": "fe54c92cb5f7c2c68847354af8770dd8df5f7088",
        "title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection"
      },
      {
        "paperId": "18432c7bcc8ac6eeadb9e2f4ab4490257af2aae5",
        "title": "Seed-Coder: Let the Code Model Curate Data for Itself"
      },
      {
        "paperId": "3a2b08d37fba04b6563b38818497f1f98ec9df7d",
        "title": "RewardAnything: Generalizable Principle-Following Reward Models"
      },
      {
        "paperId": "d9bf0f579ab2505825cbbfbbe9b0cc18bfb97f6b",
        "title": "MiMo-VL Technical Report"
      },
      {
        "paperId": "5f145bd0636027e192d7c8d0102806166fc30d64",
        "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning"
      },
      {
        "paperId": "2587ee3880c15a247f8f5fca40ffdef088102100",
        "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening"
      },
      {
        "paperId": "cd5f519f3e76aaf0bc2d4ea59d01567a0f37afe0",
        "title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment"
      },
      {
        "paperId": "f9593e4c247defe3e1e518f59848048c9bc19a8e",
        "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback"
      },
      {
        "paperId": "af6fc4138e4a4a2f7224dd39dd9340c0d441e934",
        "title": "Incentivizing LLMs to Self-Verify Their Answers"
      },
      {
        "paperId": "88f6b02cb11cde484266eb72f40aaeb9b2e83298",
        "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis"
      },
      {
        "paperId": "d6a29be03a0497602e89311ec38e5141335647c5",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "800dc9621a55d4334abe1807b26be9b907332967",
        "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts"
      },
      {
        "paperId": "a7933f44a76c680b074591deb74b444ab8c748e0",
        "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"
      },
      {
        "paperId": "f7b400cab655e820a38348fd1c933ab3036b29f9",
        "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning"
      },
      {
        "paperId": "d887221a26248fefb07eebf979744bc89ad4ba98",
        "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing"
      },
      {
        "paperId": "0eed0ba7bf63c67cd34c082990aaf96a49fbbdc5",
        "title": "Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment"
      },
      {
        "paperId": "1bac1895263f10e52cbee5742fa7b896b5bc4976",
        "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning"
      },
      {
        "paperId": "89c3f847ded8200d1a1d09e278bb32fd721c9761",
        "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning"
      },
      {
        "paperId": "f332d0658c7884bfa426cc897ea10a5c4fa80394",
        "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited"
      },
      {
        "paperId": "ac1d5df9687455f405de44ef8190aa5321a99a5b",
        "title": "Towards Effective Code-Integrated Reasoning"
      },
      {
        "paperId": "def268940f3bc1f3541693fbc565ff32e0c0dbc9",
        "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning"
      },
      {
        "paperId": "ef100592cda6f84736783c523f0e31160e6b46ff",
        "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding"
      },
      {
        "paperId": "3043778b2a9f102b7ffc47f3cf02d44538462d7e",
        "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation"
      },
      {
        "paperId": "3f26226bf48bde05bccc3025d9bcf229121f7dc6",
        "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models"
      },
      {
        "paperId": "5d10bada0d15ab6fde964373beec86a1cd0f2375",
        "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence"
      },
      {
        "paperId": "843422813fb73d5e677ac49e962f76c47843f9b1",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards"
      },
      {
        "paperId": "d9d8984f2cf7e8eae12fe1a4b2f21e1bf7160202",
        "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning"
      },
      {
        "paperId": "8884f99f9b011e80a6c1b88a4dccabea153fd35f",
        "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL"
      },
      {
        "paperId": "10feab31bb9e71a0f1094fb00c0554abfb992c4d",
        "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models"
      },
      {
        "paperId": "b75643c6817b50b65f23b3b3868be7f0eeaddb0e",
        "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer"
      },
      {
        "paperId": "2d3b5a90976b89ebd16efda15801df999f0a6599",
        "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization"
      },
      {
        "paperId": "dabd79d3cb65dcedf45cab47d673119b501eb887",
        "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models"
      },
      {
        "paperId": "944babbccbb11625ee6d2504f67ff69b51075426",
        "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents"
      },
      {
        "paperId": "cf209c92e33525a2cdc8817b1dfbfa0dcea28c0d",
        "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering"
      },
      {
        "paperId": "92bdc387d53b6ab42791c0168e13beb69c79130e",
        "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning"
      },
      {
        "paperId": "c91084908a3d4625c41a4e58b1cd79494b065646",
        "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind"
      },
      {
        "paperId": "8bb0689f435be4a2e0f448c027be8d9489c564c2",
        "title": "Grounded Reinforcement Learning for Visual Reasoning"
      },
      {
        "paperId": "09b159a27396632f4b80db721e7f88f285fee3f4",
        "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL"
      },
      {
        "paperId": "22f0804270d06d6a7e2d0cc52b1f8984c0a134a4",
        "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training"
      },
      {
        "paperId": "1f327e612a863ac5ad6ad9b382991819347d3b07",
        "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning"
      },
      {
        "paperId": "a183498f48ee0d79a781fa0de806e6828161a92e",
        "title": "On-Policy RL with Optimal Reward Baseline"
      },
      {
        "paperId": "4c09e6f08c03ba6c2396ec8a196abe5a92572ee3",
        "title": "InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning"
      },
      {
        "paperId": "6f99c69faa15825a52b594af20bd21b586b1e9e8",
        "title": "Pitfalls of Rule- and Model-based Verifiers - A Case Study on Mathematical Reasoning"
      },
      {
        "paperId": "6f88719fa7c73ae4b66bdce13e6f19088f2b3c13",
        "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning"
      },
      {
        "paperId": "c2699ff6ea7c112c7e6fc38d4306593c28e711f4",
        "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition"
      },
      {
        "paperId": "84ac4173ac8cd7cfea0262ceedc0796d0ea4b0e2",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency"
      },
      {
        "paperId": "67aa9790c22849e532491b072a1ee5a2394a0d0d",
        "title": "Fostering Video Reasoning via Next-Event Prediction"
      },
      {
        "paperId": "532c7d150d7bfece96e3c51ad0c8e4e18271912c",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "paperId": "c14bdd1555be953c3b37154a0b5cf1d8b129058b",
        "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?"
      },
      {
        "paperId": "98c8ebe17fa472058a65d07861de4b7f4de0eeed",
        "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning"
      },
      {
        "paperId": "01e0ff53a6af26f001c8866b6cee570f14b1372d",
        "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason"
      },
      {
        "paperId": "1b614e0c9a7ca4b7f13549ba05a459dbf051be49",
        "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning"
      },
      {
        "paperId": "ac07a0086ff3c7a4f189f7b2d3b147d759ec5f1b",
        "title": "Maximizing Confidence Alone Improves Reasoning"
      },
      {
        "paperId": "03e385b1a6fdb7bef5612feea44e44d5efd45bcc",
        "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression"
      },
      {
        "paperId": "7808483b2a9147318e8bc6078ed9566e12555a2c",
        "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "b12998ee8a2d2feba5e17eec7c437b4243b4aeee",
        "title": "Can Large Reasoning Models Self-Train?"
      },
      {
        "paperId": "121b2ea3770c1e5627846f06b58ada4d886a7f01",
        "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning"
      },
      {
        "paperId": "8f2eef2e8df6d9e1bc9b73df30fe7956c27faeb4",
        "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue"
      },
      {
        "paperId": "1ebfa83c21cd0af533d701c8a87a835801af2255",
        "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models"
      },
      {
        "paperId": "5bd465c1d9f74d4769d5a1887a13f84b8c7d1a73",
        "title": "Temporal Sampling for Forgotten Reasoning in LLMs"
      },
      {
        "paperId": "9709a69c947c561e5792ef843d20052fdb731ffc",
        "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective"
      },
      {
        "paperId": "74dadd23073faf65cb767f1f9467a0fc965ca7b3",
        "title": "ARM: Adaptive Reasoning Model"
      },
      {
        "paperId": "a50f7c2ec127f870198f34c2bca3a5ef187cf07e",
        "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners"
      },
      {
        "paperId": "e44058e7e14e5c7fd439fc8ed641d91090f35a3e",
        "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning"
      },
      {
        "paperId": "dfd5941b9f60a97de58b6bc4da56c0a22b348580",
        "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward"
      },
      {
        "paperId": "04f9cdd1a478eccb0b62d774c086e22fd8f35546",
        "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition"
      },
      {
        "paperId": "731861e42ed3743c46f511c5b8dc303d7cca636f",
        "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use"
      },
      {
        "paperId": "586f5fe2c595ef9a72da049e7400d6b0f34d52c9",
        "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training"
      },
      {
        "paperId": "c9c83f69e0ed4045bacaf11d3b9ddf40df6626e2",
        "title": "RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data"
      },
      {
        "paperId": "6dcce2904766e1e9fa5c795cd77b181ebd1b5ae3",
        "title": "Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning"
      },
      {
        "paperId": "7cab61a4d24a6e10abaadca758824e078e790187",
        "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning"
      },
      {
        "paperId": "353edce7fb68da762817cdd31880108462580a25",
        "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting"
      },
      {
        "paperId": "1cce5f75d76b07a9b8b8e2c36b4d0a3b520d27ef",
        "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs"
      },
      {
        "paperId": "55baf937c13205e826711eddf60b02da7dc74250",
        "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models"
      },
      {
        "paperId": "bb26c61cbe322a4f8cf81e8e461d680a5ff05923",
        "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL"
      },
      {
        "paperId": "9f099be6ea91146da05cebc2b441fb2ddd94d2ce",
        "title": "WiNGPT-3.0 Technical Report"
      },
      {
        "paperId": "f653a136e30f72eb28ea03059b30aef118715fd5",
        "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning"
      },
      {
        "paperId": "1f56331f0f0dc9959a37b141453edbec65cb870c",
        "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models"
      },
      {
        "paperId": "15f0ebcfef190e6aeac25e08e71e320be0263696",
        "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning"
      },
      {
        "paperId": "343b70582fce012903a42137bc664009f3c29e0b",
        "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning"
      },
      {
        "paperId": "38dbd4788dedc376c904589fa0799c830eef8d2c",
        "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective"
      },
      {
        "paperId": "29f6ef611eff29c47759a246eed8263fc98da537",
        "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning"
      },
      {
        "paperId": "859caa7e5a365c5c13d5fb0c614ac8d151126e29",
        "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
      },
      {
        "paperId": "eecec135de2f1c800bc8bc6ede73ff0246cd0d31",
        "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward"
      },
      {
        "paperId": "abc473899b15276c087c8ecf64e649169a1d1382",
        "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models"
      },
      {
        "paperId": "ea9591cf281409ffcbc284129d52050fea9cc115",
        "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay"
      },
      {
        "paperId": "cf5f4acfd80e30c141c132322e012bac687c74ab",
        "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning"
      },
      {
        "paperId": "8c980f08927b49b441aedb5337762d9695a11ba8",
        "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning"
      },
      {
        "paperId": "4aef44e4aeaf28868ae2f1fff2c4eb19ff4df1f6",
        "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning"
      },
      {
        "paperId": "06687e08c07bb970cc5f157ead0ab13916d3f3d7",
        "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "cc5b6a71b2598cb147c757caf84ac9f26354305b",
        "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning"
      },
      {
        "paperId": "89510d9c471bd77404ed0901b644bfd9871329f0",
        "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"
      },
      {
        "paperId": "5becc8d0a5066c58d2785798519f2cddad7f4482",
        "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning"
      },
      {
        "paperId": "8d7e76f4062cf84ba9e1049a9f7aebf38ba6e08b",
        "title": "Large Language Model Enhanced Drug Reposition Knowledge Extraction via Long Chain of Thought\uff1aDevelopment and Evaluation Study (Preprint)"
      },
      {
        "paperId": "c868d25192f138475a1ce66dd49dfcd28487c1e4",
        "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models"
      },
      {
        "paperId": "8b1738b888e3397bb22a8cf2f4d89721363b388b",
        "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL"
      },
      {
        "paperId": "150ec99378a547514ecb0b0b315aff4262e28b77",
        "title": "Think Only When You Need with Large Hybrid-Reasoning Models"
      },
      {
        "paperId": "7ba5352946b2df1032a50c1a299ca4b3143d576e",
        "title": "The Hallucination Tax of Reinforcement Finetuning"
      },
      {
        "paperId": "78169fd4729b3da0f5cf72f57bd968515a5ed646",
        "title": "RLVR-World: Training World Models with Reinforcement Learning"
      },
      {
        "paperId": "f9ce0248b823bcdcd37803c95421130fd49fb97e",
        "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models"
      },
      {
        "paperId": "ee865133d4243a53771131e2fd29d9211a0b7366",
        "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge"
      },
      {
        "paperId": "be1985ec9f0bce3b45c81e7b28b93acbaf7e20eb",
        "title": "Self-Evolving Curriculum for LLM Reasoning"
      },
      {
        "paperId": "5759a0e3a98496e7cce0a5a312aff3f4f491a2c4",
        "title": "Reward Reasoning Model"
      },
      {
        "paperId": "9995fe60c378bf9503c2c435feca5a560f58427b",
        "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning"
      },
      {
        "paperId": "e00b7ccb58e1a1ff977359ae73c549f4089a0f72",
        "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning"
      },
      {
        "paperId": "9102b58c727577172e6c0cf53c9b928b752a33aa",
        "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
      },
      {
        "paperId": "fde0ffe77186561497ce15e4faca82db11dacd64",
        "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards"
      },
      {
        "paperId": "0fe4e9d7f771290da5bec757ae77208d320922bb",
        "title": "AdaptThink: Reasoning Models Can Learn When to Think"
      },
      {
        "paperId": "830a0c3b26f11003ae0fdd50571b7a26df3f524b",
        "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs"
      },
      {
        "paperId": "721149194c40b3d7034f88c391f851747680e2f7",
        "title": "Thinkless: LLM Learns When to Think"
      },
      {
        "paperId": "23452c2b1136fd3fd08f2e2d85805c86792c2060",
        "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs"
      },
      {
        "paperId": "a742eb7d57e4e4f8457898ac6c15b9177a1e6769",
        "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning"
      },
      {
        "paperId": "e796d03228f09497ea43a0e330c88ba6015a6c0a",
        "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning"
      },
      {
        "paperId": "9da2ee84404ba52919e30cf9f0927a64a2d26da3",
        "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization"
      },
      {
        "paperId": "ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
        "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling"
      },
      {
        "paperId": "7d47a1672c2f297f167eda7fe6571a788091ac2a",
        "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO"
      },
      {
        "paperId": "742d9c80b2ca4d01f8a8675cfe98487e0783d3d7",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training"
      },
      {
        "paperId": "37f70927b32fbbbde8e4529a1137607245c1774d",
        "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL"
      },
      {
        "paperId": "e136b3f559b3856d5008aa5a15467210d88dc316",
        "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models"
      },
      {
        "paperId": "8f047a824ed8b5afc28715a552bcda9126fa7ef9",
        "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs"
      },
      {
        "paperId": "23abdb8156b199dd5a74be11dd483b7ea0be2246",
        "title": "Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning"
      },
      {
        "paperId": "6b0201470fe132402315c88932271d33e5028311",
        "title": "SuperCoder: Assembly Program Superoptimization with Large Language Models"
      },
      {
        "paperId": "3592cf332a58796d18e3d1cc359f09043a30b6bc",
        "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models"
      },
      {
        "paperId": "d3f48630511341fd64ab5847aac6486a9974da8e",
        "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
      },
      {
        "paperId": "6dc33387854a34fcd41388921d17edb86fa372d9",
        "title": "Towards ML System Extensibility"
      },
      {
        "paperId": "0f0157b3e679d3dacdfa56277831fed9b2b8e0a2",
        "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale"
      },
      {
        "paperId": "3a537f680591bb7d73ebdeeae5d3f3577e7fa6cc",
        "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent"
      },
      {
        "paperId": "7d774abaae11614b44ae867b4d494476a7f875c4",
        "title": "SEM: Reinforcement Learning for Search-Efficient Large Language Models"
      },
      {
        "paperId": "97e0a3548a6f0818262b4f86e4180f256e2f0128",
        "title": "MiMo: Unlocking the Reasoning Potential of Language Model - From Pretraining to Posttraining"
      },
      {
        "paperId": "f29c8c4bf20dea0ba57d91ba409d846123fb89c0",
        "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving"
      },
      {
        "paperId": "338878b4ae909c25acf60d0d9407597b2b12c7e5",
        "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards"
      },
      {
        "paperId": "09b2a0f0d7c1164ab334e13e70eb0d65b5b96393",
        "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data"
      },
      {
        "paperId": "e37e09a9f78f87f3c2eeafcfb18748e4670a1425",
        "title": "Bielik v3 Small: Technical Report"
      },
      {
        "paperId": "3d58168c202c95ee932c1913f445b3f5efe0f75e",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL"
      },
      {
        "paperId": "f25225055901731512dcbc905f75383dc678b4d0",
        "title": "RM-R1: Reward Modeling as Reasoning"
      },
      {
        "paperId": "5dea6c7e3abbcb41035d227ee932b5abf81b12b6",
        "title": "Adaptive Thinking via Mode Policy Optimization for Social Language Agents"
      },
      {
        "paperId": "b58ed3278832960ed88627dfa58799edb5ef3c88",
        "title": "DeepCritic: Deliberate Critique with Large Language Models"
      },
      {
        "paperId": "6f132f22dd19d89ef9eacd6d6d48bd56934a5fd1",
        "title": "Phi-4-reasoning Technical Report"
      },
      {
        "paperId": "5ac754e6d6b0ea662b19e83f445aec8f03569e75",
        "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning"
      },
      {
        "paperId": "4b2e85aae0ded21da525fd1de93444f63b6f70bc",
        "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models"
      },
      {
        "paperId": "1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example"
      },
      {
        "paperId": "41309d007b6d6b66a034900901ad5c934a2a2922",
        "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training"
      },
      {
        "paperId": "dd47fbb58924b1299200a249564378a1a7bd4759",
        "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning"
      },
      {
        "paperId": "493a36025a3440cc288ceb1b8801c13c011f1dd1",
        "title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning"
      },
      {
        "paperId": "465920fd7cbaaf38d6a089af8006c07987a17ee5",
        "title": "Tina: Tiny Reasoning Models via LoRA"
      },
      {
        "paperId": "350ddadbdc5c64c12d01ac2a5b9a2d8a9881d492",
        "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning"
      },
      {
        "paperId": "b7c5ad386f20954334a539c0558309a5a5bc311d",
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners"
      },
      {
        "paperId": "143e18bfd7c356592e7c1439738a3525d3e16279",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
      },
      {
        "paperId": "6593f4eecfa46c17abeb43ee3b5f331a461a0577",
        "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation"
      },
      {
        "paperId": "98c25cf1a48d446ac7bf99ac725fc77218b055ae",
        "title": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection"
      },
      {
        "paperId": "e42054d042e2e5b3efe6e96cd6dd1c76ab3ca358",
        "title": "ToolRL: Reward is All Tool Learning Needs"
      },
      {
        "paperId": "29d0ed9caf811212341f67617d8cbd8c1fa2cc25",
        "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce"
      },
      {
        "paperId": "85b3ffbd93ce4e73191f2b94a1582dc559d51f71",
        "title": "Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning"
      },
      {
        "paperId": "89713edb86935480d2223358de41104a72985ed4",
        "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning"
      },
      {
        "paperId": "bb18fd3f21ece2b48257c8294c17a49b00807a74",
        "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"
      },
      {
        "paperId": "c3094b492d00d76bc325c9c80fed15ca1b24292f",
        "title": "Better Estimation of the KL Divergence Between Language Models"
      },
      {
        "paperId": "f1bd6d2f60a348e2c75d35367cad4263e4353633",
        "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models"
      },
      {
        "paperId": "4e0016dd49b44e07c50f8adea6ea8c21bc4c0b1e",
        "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training"
      },
      {
        "paperId": "dfa3b27662b5d6cca57332249b8dd4ee39d57794",
        "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions"
      },
      {
        "paperId": "420a65ea7fe8bd235a4331af59dacf5accef180f",
        "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks"
      },
      {
        "paperId": "94a5613ece3c6e94e56b2701001a0a4a6f856075",
        "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning"
      },
      {
        "paperId": "0541ed8fe567c575d54c2cd4efdbb834041cf46e",
        "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning"
      },
      {
        "paperId": "a7381c3a8184d6c259eda7a2412edad50f2d50de",
        "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning"
      },
      {
        "paperId": "95417a79cd3730ad2c53194cef4238b10b14640f",
        "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning"
      },
      {
        "paperId": "b546e9c18cc5420448d720a3c4d0907643728afb",
        "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme"
      },
      {
        "paperId": "a05cc4e797611485208d14789b93c3b44b3d93c4",
        "title": "AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs"
      },
      {
        "paperId": "9b487024ccc5c575a6d72a597ad65f92cce79a04",
        "title": "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning"
      },
      {
        "paperId": "1b5b9fad9f97feefaf10c9f1e9685bed321dc3f2",
        "title": "Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?"
      },
      {
        "paperId": "1d0b3c90eb184ea9da554ea3381bdbf1e0f23629",
        "title": "ToRL: Scaling Tool-Integrated RL"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
      },
      {
        "paperId": "fb970ce4383a78ad52c641bc38815d78ad737aad",
        "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"
      },
      {
        "paperId": "18448e0a6bde22d4cb60f37f1add5c8b6fc2142d",
        "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "cce886854084ecfec8badc26f275fabfe69176be",
        "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning"
      },
      {
        "paperId": "52f0a61bfbf93000ba4110e7e884b7e2cdcfe182",
        "title": "General Table Question Answering via Answer-Formula Joint Generation"
      },
      {
        "paperId": "1fcaafeb72142fe3d1a5d698a072d69778d244b0",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "paperId": "9ebd1f44afd407011e167ed6090ccb6b3c3e637b",
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "942328473407147f609761dc22128a503713df58",
        "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models"
      },
      {
        "paperId": "f4c98d3958017085f78a597b3b86b2bc9e3de854",
        "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning models"
      },
      {
        "paperId": "c39ae6fb8ab9b8dbd8cca4b13b303033dc23f594",
        "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models"
      },
      {
        "paperId": "432cf4506d1fa7773abfaceb35841d60e5685839",
        "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs"
      },
      {
        "paperId": "5f2f7305a467e01f76b1dbf5867fa34bd90edc19",
        "title": "What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret"
      },
      {
        "paperId": "02d7673c89a94367edd7a2cf0ea4f5540fba3e42",
        "title": "DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "ec864acbc8db71f2a885130422b28b72c0d9d4e5",
        "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs"
      },
      {
        "paperId": "60dc49b5c46a588b3f191d461171c41866bdc92b",
        "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task"
      },
      {
        "paperId": "bd07e9cdd94ea56e18dd48311f6bd189ad29b17a",
        "title": "When More is Less: Understanding Chain-of-Thought Length in LLMs"
      },
      {
        "paperId": "9da85aaab11375e7778d6b71ccb2e83b480d97a1",
        "title": "XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters of Thousand-Plus Scale"
      },
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "12f7ec12c226ee0218aad806f87ee212d0bc44c7",
        "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement"
      },
      {
        "paperId": "6020a5b2da3b462441f0883e0990b970ac8607dd",
        "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"
      },
      {
        "paperId": "254c483dd7919f74a5d5054e4a8b659caf53f3e7",
        "title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models"
      },
      {
        "paperId": "7a3dc0eb5ef0bd60e6a478444b6865bda7647d17",
        "title": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks"
      },
      {
        "paperId": "4039ac3b8eee6e1ce39d204e57856432e6802c8f",
        "title": "Optimizing RLHF Training for Large Language Models with Stage Fusion"
      },
      {
        "paperId": "3d088df52138c80d235a8d76b5bcf67edd79ff38",
        "title": "ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development"
      },
      {
        "paperId": "e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
        "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation"
      },
      {
        "paperId": "a3d5e8729be281ed50002fe4eb99e7d475d75f27",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "3dcd4d249bb56ee3246785033277f6f5d6dba564",
        "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning"
      },
      {
        "paperId": "13261129251c9e8891cff02c3aee15c4df6a5630",
        "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"
      },
      {
        "paperId": "fa66f6cd86b4ae668d4f8ffe93019ba97fe7eab7",
        "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI Inference Workflows"
      },
      {
        "paperId": "cac59829e4d1da17119247a7f0bc8386a26408ad",
        "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs"
      },
      {
        "paperId": "28827ca806a07f33f9d6f47f5a72b1dc8c7e0b91",
        "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs"
      },
      {
        "paperId": "0525026f436f3012c8e345b423f4bf04858d8203",
        "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning"
      },
      {
        "paperId": "d4d11bd2032cf83027b0f284cfd645141017ed65",
        "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning"
      },
      {
        "paperId": "4cbcbfec289c9c1cfd6bfa385348bc331c20a166",
        "title": "W E C HAT -YATT: A S IMPLE , S CALABLE AND B ALANCED RLHF T RAINER"
      },
      {
        "paperId": "15298b2abb4ce1b5bf57aa9e61f202d3b2ce577c",
        "title": "DeepRetrieval: Powerful Query Generation for Information Retrieval with Reinforcement Learning"
      },
      {
        "paperId": "f48345fe67b3b5f440c8547473647f1c9ee1a15e",
        "title": "DiT-Serve and DeepCoder: Enabling Video and Code Generation at Scale"
      },
      {
        "paperId": "eb79c7fa0fc81b96cb396f4487e5b076e60de03f",
        "title": "Reinforcement Learning for Safe LLM Code Generation"
      },
      {
        "paperId": "2a5ad12391110e5315963a4a8a86116d0a2e8c5c",
        "title": "One RL to See Them All"
      },
      {
        "paperId": "00b39319f2dab85e7e338201a675f8c6dca2dfe8",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards"
      },
      {
        "paperId": "8ccf59bccededd90a4576bf7d025a230d6499cdf",
        "title": "C AN P ROMPT D IFFICULTY BE"
      },
      {
        "paperId": "23c78cc769c34020850d184ac681fa34c282f3f0",
        "title": "T HINK ON YOUR F EET : A DAPTIVE T HINKING VIA R E - INFORCEMENT L EARNING FOR S OCIAL A GENTS"
      },
      {
        "paperId": "f2866ca95b940f8c7da1901c6817674343b2938b",
        "title": "Dora The Explorer: Learning Explorative Policies for Language Model RL-Finetuning"
      },
      {
        "paperId": "ea00fe4ab21ecfa6cfe550e5a3bf5edfe1d2ea87",
        "title": "Disentangling Knowledge and Reasoning in Medical Evaluation Benchmarks"
      },
      {
        "paperId": "687a994edae684ed2e4d507a398721fb766a6441",
        "title": "A EGIS : A UTOMATED E RROR G ENERATION AND A TTRIBUTION FOR M ULTI -A GENT S YSTEMS"
      }
    ],
    "score": 551.0
  },
  {
    "id": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
    "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "authors": [
      "Harrison Lee",
      "Samrat Phatale",
      "Hassan Mansoor",
      "Kellie Lu",
      "Thomas Mesnard",
      "Colton Bishop",
      "Victor Carbune",
      "Abhinav Rastogi"
    ],
    "year": 2023,
    "citationCount": 408,
    "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.",
    "url": "https://www.semanticscholar.org/paper/600ff4c4ae9fc506c86673c5ecce4fa90803e987",
    "pdf_url": "https://arxiv.org/pdf/2309.00267.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2023-09-01",
    "externalIds": {
      "DBLP": "conf/icml/0001PMMFLBHCRP24",
      "ArXiv": "2309.00267",
      "CorpusId": 261493811
    },
    "references": [
      {
        "paperId": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions"
      },
      {
        "paperId": "fd8730a7e7efbfc3c7cd94f6534fb4eb11718c2c",
        "title": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "a9e155fda1d97baa2b8712f580cc61887cc64e9b",
        "title": "ChatGPT outperforms crowd workers for text-annotation tasks"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "70b98d90767345b15e0569082c0e4ac661279b5d",
        "title": "Is GPT-3 a Good Data Annotator?"
      },
      {
        "paperId": "35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "7f3bc301ae0e2bbb78a0d42f074865e87d908f9a",
        "title": "Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning"
      },
      {
        "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
        "title": "Large Language Models Can Self-Improve"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "f4df78183261538e718066331898ee5cad7cad05",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
        "title": "LaMDA: Language Models for Dialog Applications"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "c2a79e2a65b721d4de5f6d4806323174b9f8f393",
        "title": "Towards Zero-Label Language Learning"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "4e263b4cd6998bff2501dd143e685f413179b12d",
        "title": "Want To Reduce Labeling Cost? GPT-3 Can Help"
      },
      {
        "paperId": "63d8426ba1f51a8525dd19fd8ec92934ec71aea5",
        "title": "A Survey of Data Augmentation Approaches for NLP"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "be9aa06811f1438e8c6e2fcbed2f8aed06469f2c",
        "title": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation"
      },
      {
        "paperId": "b3b3d1d6d36ac203cd06c00bb37e66c000430275",
        "title": "A Theory of Regularized Markov Decision Processes"
      },
      {
        "paperId": "f54b36edae733ab9cd7a748595947710bd28a2e3",
        "title": "A Study of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "title": "Hierarchical Neural Story Generation"
      },
      {
        "paperId": "c72582122ff631117a05deb2aefa04b01362e3fa",
        "title": "Learning to Extract Coherent Summary via Deep Reinforcement Learning"
      },
      {
        "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "a870df7e7d43c9144e2520ef4e4779f1672dd654",
        "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control"
      },
      {
        "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "b2f4ca80f4a8c1de6508fb7856f73dc89a9b0f5f",
        "title": "Avoiding Wireheading with Value Reinforcement Learning"
      },
      {
        "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
        "title": "Taming the Noise in Reinforcement Learning via Soft Updates"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "c7d3e9a1dd86f9c96f709d0ddb76972862784231",
        "title": "Dynamic Programming and Markov Processes"
      },
      {
        "paperId": "e6d55ec9324158fc82f1f0ef8286fafaab132f69",
        "title": "The Problem of $m$ Rankings"
      },
      {
        "paperId": "6c5a1079d9705c0ee022cef77207daa20ce2cde5",
        "title": "Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "An overview of bard: an early experiment with generative ai"
      },
      {
        "paperId": null,
        "title": "Ai platform data labeling service pricing"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "Reinforcement learning from contrast distillation for language model"
      },
      {
        "paperId": null,
        "title": "RLAIF"
      }
    ],
    "cited_by": [
      {
        "paperId": "47009065ba03c9cd3e777ba172ae1849257f7d6f",
        "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization"
      },
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "b33bc4971432f58f72ac2f6d55fcb6edee1aea0b",
        "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "682d413e6cce94d8731703472509781c707d63f6",
        "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning"
      },
      {
        "paperId": "b5939cba32a1c1c6a6c3418b7cba16dbcafb8850",
        "title": "A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models"
      },
      {
        "paperId": "fd140dc3962ff37f82c61634bc359d65aac2a1a5",
        "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models"
      },
      {
        "paperId": "5903995c30681866d28f243c3ea846e836c497d7",
        "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning"
      },
      {
        "paperId": "17737248267d2e32bb14af4f13e2314b50e23b5e",
        "title": "Evaluation of cell type annotation reliability using a large language model-based identifier"
      },
      {
        "paperId": "dbab399a5f78d4231df4b3ff70aaf2b3345d75c4",
        "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective"
      },
      {
        "paperId": "60c1a310b9be66b9319ba33fbba2f2ed02b02e1c",
        "title": "Correct Reasoning Paths Visit Shared Decision Pivots"
      },
      {
        "paperId": "ed68534d5a8da7b097b1a6ec4ddb7b24a185a361",
        "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving"
      },
      {
        "paperId": "865da3faec0bd4aea4e56b9252149fe221309746",
        "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization"
      },
      {
        "paperId": "10570d4aee2d2a9030735f6935ec7c09695dfcfc",
        "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal"
      },
      {
        "paperId": "6aebe394b781205ec9accd634e9395b7c21c1310",
        "title": "The Alignment Bottleneck"
      },
      {
        "paperId": "a6fbcfabbb20c8848364ec44c8a1dea18be00efe",
        "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning"
      },
      {
        "paperId": "b07a6a99e8ada2410c1d6d01893d314814ca4649",
        "title": "Opal: An Operator Algebra View of RLHF"
      },
      {
        "paperId": "9ac4911450c42066330a4e6e02f435c76856e119",
        "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration"
      },
      {
        "paperId": "def0655b1d0d434442b3bbcb8475d4cc6e750718",
        "title": "How well can LLMs provide planning feedback in grounded environments?"
      },
      {
        "paperId": "ce4456ac12fd6c5211c422567b07a06de3cd7a76",
        "title": "Symbolic Graphics Programming with Large Language Models"
      },
      {
        "paperId": "2429de2c3f2b7ddcdf1f8b2d34c6eb8b75cc47f9",
        "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models"
      },
      {
        "paperId": "8fb24ff651165cd1d789282a3421c2a9521904e6",
        "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization"
      },
      {
        "paperId": "26730e1800a33de13aa8ce01f1400b4818e9de42",
        "title": "Measuring Scalar Constructs in Social Science with LLMs"
      },
      {
        "paperId": "b3b13107dd153f4de1ddfeec0a96998d70caf04b",
        "title": "Preference Robustness for DPO with Applications to Public Health"
      },
      {
        "paperId": "a76fffe2e593137ee4b6d54b13bea42e5a5f7da6",
        "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE"
      },
      {
        "paperId": "021ae6869a956b22d59e32e9efc1d4aee2a1c089",
        "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning"
      },
      {
        "paperId": "c6368520206644a036cb8011c211a82bb6bf46c9",
        "title": "Model Science: getting serious about verification, explanation and control of AI systems"
      },
      {
        "paperId": "83cf4086e8253b8845bc97d545fa479a2c0e080d",
        "title": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow"
      },
      {
        "paperId": "c56be4f1ee454f491470976c3f1dc859e0c668e8",
        "title": "Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?"
      },
      {
        "paperId": "0b91243e579c4a46516507e09db3467f9c48cb75",
        "title": "Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models"
      },
      {
        "paperId": "a60082b6ebe795bf88d4e9681b27aec1b5fbd706",
        "title": "Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs"
      },
      {
        "paperId": "b07ee4903dc59af8b4ab7925e9888d11c9dd7725",
        "title": "On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View"
      },
      {
        "paperId": "72cd4ae987f0f6c7a0406d9912dd232002159272",
        "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization"
      },
      {
        "paperId": "c5b9e6d2797e079df1a9ba213c144a46da055703",
        "title": "PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning"
      },
      {
        "paperId": "539107d4bc5fbc8e3a6ed6cb687c7b061939f048",
        "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"
      },
      {
        "paperId": "21f5779ad976f840ce8a7e6d9b36d08539cd0945",
        "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses"
      },
      {
        "paperId": "8862d3811bb38c3327164c4d01799b5f6f25fe87",
        "title": "FlowXpert: Expertizing Troubleshooting Workflow Orchestration with Knowledge Base and Multi-Agent Coevolution"
      },
      {
        "paperId": "82ea274f6e2394d8f671a245995b535e30431945",
        "title": "Generative AI Meets Wireless Networking: An Interactive Paradigm for Intent-Driven Communications"
      },
      {
        "paperId": "c14c3c7481f42165a6ef8814d547babfe8cb5ae7",
        "title": "Lessons from complex systems science for AI governance"
      },
      {
        "paperId": "a1aaa56c12313aac3059ba8cbc4fa876c03c2039",
        "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering"
      },
      {
        "paperId": "9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
      },
      {
        "paperId": "f2f6607c759581164d56cece90ad40e87065c451",
        "title": "URPO: A Unified Reward&Policy Optimization Framework for Large Language Models"
      },
      {
        "paperId": "a556ef1d3f2049dd1f281bebb1d8835a02f4efea",
        "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?"
      },
      {
        "paperId": "6478c185582f5e48c54677c683a9297ff2916118",
        "title": "One Token to Fool LLM-as-a-Judge"
      },
      {
        "paperId": "29461fe2acb5f96e6859c3a91dd2149d4142613b",
        "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training"
      },
      {
        "paperId": "5c3ae0ec1bd3ddd7863dfdd8e7ed9b7f50555119",
        "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains"
      },
      {
        "paperId": "0632657e9c329f8fbf8eb2d9f733aefa09b0292b",
        "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training"
      },
      {
        "paperId": "65cbdf62e2123ba6f66c64f4a0e8387dd8fbd11b",
        "title": "Pre-Trained Policy Discriminators are General Reward Models"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "85fb1dbc5251324908e0e8f36105119643a7c36c",
        "title": "ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization"
      },
      {
        "paperId": "6e1bd09edc0cca90aad8629f06101dcf974a355a",
        "title": "In-silico 3D molecular editing through physics-informed and preference-aligned generative foundation models"
      },
      {
        "paperId": "87251cd2f7ef82159aebb56f5e92c89af88a38f3",
        "title": "Activation Reward Models for Few-Shot Model Alignment"
      },
      {
        "paperId": "519b1b8c143948739a26eb7191f6659b45a76479",
        "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents"
      },
      {
        "paperId": "325255b6c41ab90bd239c26aef82add725eda8d2",
        "title": "Generalist Reward Models: Found Inside Large Language Models"
      },
      {
        "paperId": "a6a5e2d4319e2fa9bf780a9b28f71677de4baaf6",
        "title": "Aligning Spoken Dialogue Models from User Interactions"
      },
      {
        "paperId": "4cd5575d388df2de4cfe52f845c721283cc7d9cf",
        "title": "Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?"
      },
      {
        "paperId": "3cd7d4e8ff1d4a3f601ac1902fa68f898c04fd0e",
        "title": "Adaptive Accompaniment with ReaLchords"
      },
      {
        "paperId": "6c3c8a17bd3bf72d9bf8ce6d009d203363faa1e1",
        "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization"
      },
      {
        "paperId": "0933fc39e13991fc9650481f7467b8061423ee87",
        "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training"
      },
      {
        "paperId": "8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
        "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models"
      },
      {
        "paperId": "e7b92915a9cd82fbc31388834097d5fd3ecb48c9",
        "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback"
      },
      {
        "paperId": "4f576cc32d6794c7eb8b1b15f0f21036f728f204",
        "title": "Detection, Classification, and Mitigation of Gender Bias in Large Language Models"
      },
      {
        "paperId": "b61a64b24ddaf5fb83f8c0cd0c48dd9c4e96d47f",
        "title": "Self-Adapting Language Models"
      },
      {
        "paperId": "cfe8776f8a99bfdfdcdb4ac660aae9f0eb59e33c",
        "title": "Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models"
      },
      {
        "paperId": "47ea295a79c58b04eb71278e1f48afa9650ebb2f",
        "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning"
      },
      {
        "paperId": "739051073dbd7c6802d15079d9df9f51bf6a8e5b",
        "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
      },
      {
        "paperId": "f6180c800726e25693e9808360167760d1a89048",
        "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance"
      },
      {
        "paperId": "25683b5340a7856fdfd04220a0371e6de9cfa066",
        "title": "Understanding the Impact of Sampling Quality in Direct Preference Optimization"
      },
      {
        "paperId": "af6fc4138e4a4a2f7224dd39dd9340c0d441e934",
        "title": "Incentivizing LLMs to Self-Verify Their Answers"
      },
      {
        "paperId": "10e1f1665977a47f7a33925a1d69df5feb5b9517",
        "title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification"
      },
      {
        "paperId": "a7933f44a76c680b074591deb74b444ab8c748e0",
        "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"
      },
      {
        "paperId": "14f409c6c3661c7dd487a859a98b94c8f0b15a99",
        "title": "CRScore++: Reinforcement Learning with Verifiable Tool and AI Feedback for Code Review"
      },
      {
        "paperId": "1bac1895263f10e52cbee5742fa7b896b5bc4976",
        "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning"
      },
      {
        "paperId": "99ec760e87f009a4b5eca3f3d1a270ddaf8eae5e",
        "title": "Are Reasoning Models More Prone to Hallucination?"
      },
      {
        "paperId": "91d7e3e1cadd9d92c65ba6d028230e493b3185f2",
        "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data"
      },
      {
        "paperId": "fe13d660dbad9cd52d753f360424745e11d4844f",
        "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation"
      },
      {
        "paperId": "6fc8cc66cd36fbd2db5dd5f7bd920600631ed98a",
        "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback"
      },
      {
        "paperId": "f710d24dc91f0090bb4f9899c1bf46df2349251c",
        "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "2cbcd61bf8b994c96ada959e06a311e3e1c2d2d3",
        "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback"
      },
      {
        "paperId": "f185c99b0af3d7598f751ad977b1898ebcd05c33",
        "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients"
      },
      {
        "paperId": "9db4ecfb5f2e78b43ff388892ce3bf0f122fa0d3",
        "title": "Multi-Domain Explainability of Preferences"
      },
      {
        "paperId": "40380999b9df6026c4a5284161fc61f88fbc82ea",
        "title": "ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment"
      },
      {
        "paperId": "42708a3c67ac3616d0dfbecc38b7d1b4bcff570e",
        "title": "Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through Fine-Tuning and Reinforcement Learning"
      },
      {
        "paperId": "3775270497407dad1061dfefe42bbeedf44e5c8e",
        "title": "Evaluating Intra-firm LLM Alignment Strategies in Business Contexts"
      },
      {
        "paperId": "c11569e34e8ec917d8250f41657e9fe627e0efc8",
        "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning"
      },
      {
        "paperId": "573efaf44c0c56251c5336dee4c3cdd87228d4f6",
        "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals"
      },
      {
        "paperId": "f388d3a269deff3d98841e348529e6cf20d9dae2",
        "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models"
      },
      {
        "paperId": "7d47a1672c2f297f167eda7fe6571a788091ac2a",
        "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO"
      },
      {
        "paperId": "3968a794439b2655d327da2839e57470fd0b64db",
        "title": "WorldPM: Scaling Human Preference Modeling"
      },
      {
        "paperId": "2fcc2a4fa9ac9ca02d626f52d69795299fb6a5bf",
        "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering"
      },
      {
        "paperId": "bcef6be34620755da525c893d84a1feb2713eed4",
        "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach"
      },
      {
        "paperId": "018e6887dbf3b92f421ec7aee696faddd0a0e396",
        "title": "DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models"
      },
      {
        "paperId": "d92f83550353328cf1643bdec5b1362da07d42f3",
        "title": "A malware detection method based on LLM to mine semantics of API"
      },
      {
        "paperId": "a82eeabb9fd4b875c052b11bd760d7a25c9fa810",
        "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "0f7bb49d1272594cbae597bc2fd6f508daa26a27",
        "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration"
      },
      {
        "paperId": "da837b4116e22c625dce18d32832579918e0952a",
        "title": "Evolving AI: What Lies Beyond Today's Language Models"
      },
      {
        "paperId": "71aa4578fb712bfa22afaf36ad658c6d69652cbe",
        "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models"
      },
      {
        "paperId": "6c3efe0d2d7393a64d2729f7d355de78d9422bee",
        "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "5ad5eeff53070bb3fa1d11f8e21e0584ec2e992c",
        "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences"
      },
      {
        "paperId": "1617feeff3f5cc67af103542e01a4ba526957ab3",
        "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward"
      },
      {
        "paperId": "b3567434da5f23aa743f26b68948738a4d41ea71",
        "title": "FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct Preference Optimization"
      },
      {
        "paperId": "2a72757711b0fcc53278d9eb066057896c9f352c",
        "title": "Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "420a65ea7fe8bd235a4331af59dacf5accef180f",
        "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks"
      },
      {
        "paperId": "8991cdbbdca17e80cc06313ce668d00cff321d01",
        "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation"
      },
      {
        "paperId": "ffe6eddb44cb5aebd10345826a1b17f2663b2596",
        "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion"
      },
      {
        "paperId": "97d0a329688034150091fe2d2977e4717b356978",
        "title": "Do LLM Evaluators Prefer Themselves for a Reason?"
      },
      {
        "paperId": "8e697e2f8debd237ccebc34601e5e36142beae00",
        "title": "Large (Vision) Language Models are Unsupervised In-Context Learners"
      },
      {
        "paperId": "1ebca7dbea9d75ea4b10b97614c7e0b05beb94e7",
        "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment"
      },
      {
        "paperId": "68bf7eb60d2c1db80c9b37660d5c10bcdea83794",
        "title": "Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "cf5279cb9dd18a6ca94b73582133dccdb2a9d71a",
        "title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack"
      },
      {
        "paperId": "43150ca20011946a0f2e9117e7a263ea9406107e",
        "title": "Multi-head Reward Aggregation Guided by Entropy"
      },
      {
        "paperId": "1eec58f271ac291ac1645a2a6162c4a5b659d0dc",
        "title": "Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping"
      },
      {
        "paperId": "43908f36b802ab4422a7e7c995aa40f85906a10f",
        "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations"
      },
      {
        "paperId": "e968c45ea2c88772b02317a1a9ad5e26b320e6cb",
        "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data"
      },
      {
        "paperId": "ba69e01d22207ee8b068454ca57a50d26b918fe6",
        "title": "Generating Medical Diagnostic Scenarios with LLM-Based Reinforcement Learning Feedback: Dataset Release and Methodology"
      },
      {
        "paperId": "c8b93445561c6cf76670aad4294fe37401203871",
        "title": "Road of Large Language Model: Source, Challenge, and Future Perspectives"
      },
      {
        "paperId": "e3c168e1fa1907c390fdc04da99533df0f5099f0",
        "title": "Combinatorial Optimization via LLM-driven Iterated Fine-tuning"
      },
      {
        "paperId": "aa3db5cdbe06055809484e4bde3144d4ed763eba",
        "title": "Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic Text Rewriting"
      },
      {
        "paperId": "d2f37492d17c7333b7df48e91d50240e6a4b0b6b",
        "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity"
      },
      {
        "paperId": "0696c94acb9416761b66f49e33f5213acd2a5037",
        "title": "Improving LLM-as-a-Judge Inference with the Judgment Distribution"
      },
      {
        "paperId": "c8bd86e850416feebe18db1cf47e0d8cc53f68f2",
        "title": "Role of AI in empowering and redefining the oncology care landscape: perspective from a developing nation"
      },
      {
        "paperId": "f1862fc04ead47ff467a07a30d61994858975987",
        "title": "Alchemist: Towards the Design of Efficient Online Continual Learning System"
      },
      {
        "paperId": "af6e9cef7efb18a20a16f585fc27c26dbcb9483c",
        "title": "Offline RLAIF: Piloting VLM Feedback for RL via SFO"
      },
      {
        "paperId": "c35002ee2707617558d8c89e6c9a2e19cfb30e13",
        "title": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction"
      },
      {
        "paperId": "ad175c047e85f9badeabbc6fe8e578a86e502022",
        "title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models"
      },
      {
        "paperId": "0ec6a9659d50bfdb32e75e1b9c85372f67bde349",
        "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment"
      },
      {
        "paperId": "0152aafbec465a090684637e1da693d6deb98172",
        "title": "RLTHF: Targeted Human Feedback for LLM Alignment"
      },
      {
        "paperId": "e37151b321c6014be639fe784cef87a505eedb01",
        "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions"
      },
      {
        "paperId": "85a488a34cd2660b5ecdea1826ccf445e52e717c",
        "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking"
      },
      {
        "paperId": "99b049973f0cb477e50570b699f313e102ebd89f",
        "title": "Self Iterative Label Refinement via Robust Unlabeled Learning"
      },
      {
        "paperId": "043e8ca7683e64cf60fae7bb5a3b49d5c4c45404",
        "title": "Personalized Top-k Set Queries Over Predicted Scores"
      },
      {
        "paperId": "dbe40da8653e749b1055492f33ed92484d3c07c1",
        "title": "Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning"
      },
      {
        "paperId": "b11d172bbe3f959b22564d889c5b9ecd96c6dbe4",
        "title": "Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation"
      },
      {
        "paperId": "35239057edd3510592a0caa02cab97d05bbe62a1",
        "title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection"
      },
      {
        "paperId": "1d2644d650fa7e6ef771367e4764945890d2fac7",
        "title": "Speaking the Language of Teamwork: LLM-Guided Credit Assignment in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "708897c6ee81ca4ecdf9afdc57ae8b6b5af4f987",
        "title": "SPRI: Aligning Large Language Models with Context-Situated Principles"
      },
      {
        "paperId": "1bab53b78b75ace2ee5eb2916573a394f2e34ace",
        "title": "VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model"
      },
      {
        "paperId": "24e8029197b5f99126f0ef4e577d55a275405b36",
        "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge"
      },
      {
        "paperId": "10d2ae9cdf60da5558edea26fd067a8fd00ba759",
        "title": "Standardizing Intelligence: Aligning Generative AI for Regulatory and Operational Compliance"
      },
      {
        "paperId": "33c8fde911526140a85cdfc654c8ee318f83906d",
        "title": "CollabLLM: From Passive Responders to Active Collaborators"
      },
      {
        "paperId": "e34348bcc333103fb11c4a2fd0f1cca88a602313",
        "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models"
      },
      {
        "paperId": "6a2db3d4aad5e21c71bc6906e3d3d8b68ffff602",
        "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment"
      },
      {
        "paperId": "438c19fdd0f14f5082441db0c4183a50e7224b1b",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
      },
      {
        "paperId": "79cc7cb576b7eed9e7cf0d6a92fe36105fa7aad8",
        "title": "Data-adaptive Safety Rules for Training Reward Models"
      },
      {
        "paperId": "26ea226ea46801926f0d82c081956e2d89481fa4",
        "title": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression"
      },
      {
        "paperId": "546e94f762d293692486da8afc89b5cc7b673454",
        "title": "Lessons from complexity theory for AI governance"
      },
      {
        "paperId": "35bf12df551fa4851495585005e666ae53957672",
        "title": "VISION: a modular AI assistant for natural human-instrument interaction at scientific user facilities"
      },
      {
        "paperId": "c2c43036d6679486bd3f273777b82b8c8bffb1cf",
        "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment"
      },
      {
        "paperId": "a55c1d40c874a205aca1d7b5a4f2a4424168c0fa",
        "title": "Do Large Language Models Advocate for Inferentialism?"
      },
      {
        "paperId": "c85e7a719c68b88ebae654b13196d248576e5910",
        "title": "JuStRank: Benchmarking LLM Judges for System Ranking"
      },
      {
        "paperId": "c79a6fcd3ae0375f2f5cbbc0e37db6d275d78480",
        "title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel"
      },
      {
        "paperId": "0c34e53897c5527241efc1d34c88b8d49a238dfe",
        "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization"
      },
      {
        "paperId": "83385a6ff5c3b311a7366dfe87fa0c87159224a7",
        "title": "SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation"
      },
      {
        "paperId": "81843ec7b7bc09ff807a170dab48c2c330e32e68",
        "title": "Improving question answering in programming domain with pretrained language model finetuning using structured diverse online forum data"
      },
      {
        "paperId": "90272fee166238841cec8f02a1f38815afb2e3ba",
        "title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning"
      },
      {
        "paperId": "85022213b396e9c4460c1c6abbdc577b9cca5b71",
        "title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "5298be289861bef2ecbb891935c0e7f1b6f0011f",
        "title": "Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel Planning"
      },
      {
        "paperId": "d016152aa3ff624029d293318ccec34343fa228d",
        "title": "SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization"
      },
      {
        "paperId": "06c1ec31219d68f0ffca74f6ee4c22acd6664a5b",
        "title": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback"
      },
      {
        "paperId": "6e31b451f0d06c59b142537626375a5579533495",
        "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following"
      },
      {
        "paperId": "5401d8fa36a78642971c694506594afdcb100c73",
        "title": "LongReward: Improving Long-context Large Language Models with AI Feedback"
      },
      {
        "paperId": "92c82a51ad13c361d052987694cf93d6a72d5789",
        "title": "CycleResearcher: Improving Automated Research via Automated Review"
      },
      {
        "paperId": "29309bac83f5055c684789d297c69cf689d26144",
        "title": "On The Global Convergence Of Online RLHF With Neural Parametrization"
      },
      {
        "paperId": "29fa32caef41ecbf31962f62d8b67c09d6844d06",
        "title": "SkillAggregation: Reference-free LLM-Dependent Aggregation"
      },
      {
        "paperId": "36ffd74db97f98f121a6c2954bf98c93dad5d2ce",
        "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking"
      },
      {
        "paperId": "9fbe8cb6598da6438a8a47b7ca12f04d85675260",
        "title": "CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback"
      },
      {
        "paperId": "35fef9ac638cf8c90c0050dfb12b581e61d68dab",
        "title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment"
      },
      {
        "paperId": "dd01d4e738ceb006b24a8453f8dd0f756d22db0f",
        "title": "Direct Preference Optimization for LLM-Enhanced Recommendation Systems"
      },
      {
        "paperId": "9ca3d87bda7813ef29c1142fb6f49c631eac5c1b",
        "title": "Superficial Safety Alignment Hypothesis"
      },
      {
        "paperId": "1ccefd16ae5b75ea96b8e5a7fa56407fb370b0db",
        "title": "System 2 Reasoning Capabilities Are Nigh"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "b3cd5c9aab98f80b9afbf6057384e29071239110",
        "title": "ChatQCD: Let Large Language Models Explore QCD"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "3faa759bae4003a55c62618961158ac610902688",
        "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs"
      },
      {
        "paperId": "66cbace0db91e9648e439d1530c021ae3199eb2e",
        "title": "MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models"
      },
      {
        "paperId": "aa5d8374234c5f1df31073496713362bf52c0907",
        "title": "Iterative Graph Alignment"
      },
      {
        "paperId": "1ccdc5070da2ffc33dff7a6d6dfff7bb96501474",
        "title": "EPO: Hierarchical LLM Agents with Environment Preference Optimization"
      },
      {
        "paperId": "2f112209675710d3ec2d6f1d06bbdc74e9bc60af",
        "title": "Critique-out-Loud Reward Models"
      },
      {
        "paperId": "945ec8e851fcced41020be961877027edb5e7048",
        "title": "Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization"
      },
      {
        "paperId": "1035dfceeebd3dd9ba52a8162eeda670a432c56e",
        "title": "Self-Taught Evaluators"
      },
      {
        "paperId": "e6fe986a5e87c2e23456fc8d0ab3bd19634d2dbc",
        "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms"
      },
      {
        "paperId": "de3cefcd6d142afa8e7ac59523ceb116ec136248",
        "title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication"
      },
      {
        "paperId": "e344313490dc93bacf721c19f0b74ae921ac4285",
        "title": "Reward Models in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "76eb8409fccfe15a4d7ef8a8bb5ad0eddd097bd4",
        "title": "The Inadequacy of Reinforcement Learning From Human Feedback\u2014Radicalizing Large Language Models via Semantic Vulnerabilities"
      },
      {
        "paperId": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
        "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"
      },
      {
        "paperId": "6abfd5d2f175760f596ea56ee62b856bbb5beb9b",
        "title": "Large Language Model Agent in Financial Trading: A Survey"
      },
      {
        "paperId": "f5648f90f08e9a3beccd5f7efc6ebfd0f72f6bd1",
        "title": "Self-Directed Synthetic Dialogues and Revisions Technical Report"
      },
      {
        "paperId": "37a33d6c34fc2c51a16ecec10df60b44537d6d7c",
        "title": "Decomposed Direct Preference Optimization for Structure-Based Drug Design"
      },
      {
        "paperId": "4081d666ec027136a778acac1105aa17f5013e39",
        "title": "CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models"
      },
      {
        "paperId": "e1017a9c862959da964cd1c044485c05d67d65a8",
        "title": "The Better Angels of Machine Personality: How Personality Relates to LLM Safety"
      },
      {
        "paperId": "3bb361ab1af6b407129c2b15bf65687ae1ab91c4",
        "title": "The Synergy Between Data and Multi-Modal Large Language Models: A Survey From Co-Development Perspective"
      },
      {
        "paperId": "c4d0ec318c1ddca4337435af6004a3f190712a74",
        "title": "Progress or Regress? Self-Improvement Reversal in Post-training"
      },
      {
        "paperId": "7d49f330d114ec3d65ffb67eb9dc79aea7b8befc",
        "title": "Spontaneous Reward Hacking in Iterative Self-Refinement"
      },
      {
        "paperId": "f1a5156491f16ae595fc579b82d47ae6e58d3a83",
        "title": "Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models"
      },
      {
        "paperId": "809911e5238a84a8f8a9d96d6be2879c198edfde",
        "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows"
      },
      {
        "paperId": "cdc4f2c74601d9af3b100c205c6f6e6efb84cd58",
        "title": "Purple-teaming LLMs with Adversarial Defender Training"
      },
      {
        "paperId": "18d9966b6deb6fa8a389baefd4fa80066cf6a3d5",
        "title": "Applying RLAIF for Code Generation with API-usage in Lightweight LLMs"
      },
      {
        "paperId": "88786bb0ad8fbb66416e091ae89257aa4a810767",
        "title": "AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations"
      },
      {
        "paperId": "d9d8aef662bb7a3730a62b1015c3ed99e4287523",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt"
      },
      {
        "paperId": "74f1b67fa18bc9c4033a9e8fd4e11ed38b178a37",
        "title": "Hybrid Alignment Training for Large Language Models"
      },
      {
        "paperId": "50e69faefa9a78afb3068f7ca15df07e7ac7b319",
        "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models"
      },
      {
        "paperId": "82a80c4cedf7c00964ecf3f7e6b8d50513339a3a",
        "title": "Aligning Large Language Models from Self-Reference AI Feedback with one General Principle"
      },
      {
        "paperId": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
        "title": "A Survey on Human Preference Learning for Large Language Models"
      },
      {
        "paperId": "ef010a9038dc02462312f6c1c97bf8d296c1abb5",
        "title": "Self-Evolution Fine-Tuning for Policy Optimization"
      },
      {
        "paperId": "df3991bc4e7558d9303bac756b8e6511fba2351b",
        "title": "Applications of Generative AI in Healthcare: algorithmic, ethical, legal and societal considerations"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "paperId": "c27e57b6e083b522afa9d202e6dd99ced29ff857",
        "title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions"
      },
      {
        "paperId": "e67db5ac94e4c1daeb2c134d24f3c8592ee6462d",
        "title": "Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing"
      },
      {
        "paperId": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
        "title": "Creativity Has Left the Chat: The Price of Debiasing Language Models"
      },
      {
        "paperId": "a2494949a7061e8fe8343af44bc1edb00973726f",
        "title": "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors"
      },
      {
        "paperId": "7a79d484cb01f988bc70a07d2f81e7407e07405c",
        "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities"
      },
      {
        "paperId": "57b6a2996e3a1673b93680487c45ebfa1350479d",
        "title": "Towards Rationality in Language and Multimodal Agents: A Survey"
      },
      {
        "paperId": "c8b18682965ff9dccc0130dab3d679f78cefa617",
        "title": "A Survey on Large Language Models for Code Generation"
      },
      {
        "paperId": "1c941518f9eb023aae102f83c3e3d9fed2d238f1",
        "title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs"
      },
      {
        "paperId": "87912571f3df29464d3ccafae66f6e1eed581564",
        "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment"
      },
      {
        "paperId": "718b5f6c0cac99ec93b8c19743be2d3199e8b68c",
        "title": "Efficient Model-agnostic Alignment via Bayesian Persuasion"
      },
      {
        "paperId": "d6f3211db04541674a7c44633ad91c4cd3e8e557",
        "title": "Multi-Reference Preference Optimization for Large Language Models"
      },
      {
        "paperId": "6c376b088ebb10c7a0d35c0b9fbbb9ddbec44255",
        "title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models"
      },
      {
        "paperId": "4499afc74bda1c7d521a516df040facfe39943ed",
        "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "af5f53facaf8abbbeb3968798f96d5f496c74cc6",
        "title": "Embedding-Aligned Language Models"
      },
      {
        "paperId": "4040099ed20718f418733cd201709cd950f11def",
        "title": "Online Self-Preferring Language Models"
      },
      {
        "paperId": "2bbc0f8ff058227d693f680d64f9736ae7a19f23",
        "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts"
      },
      {
        "paperId": "094ce307a16d1cfa234a418f276a1ada33932578",
        "title": "Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents"
      },
      {
        "paperId": "4c2afc5e358a328286eb40f4e64572ba84a7f536",
        "title": "Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming"
      },
      {
        "paperId": "04762cbef4a118f56d9ad8f2d6d400e0e32dc6ef",
        "title": "Hummer: Towards Limited Competitive Preference Dataset"
      },
      {
        "paperId": "de103054dfeec44c2643b5e82707a81aa3737375",
        "title": "Automated Multi-level Preference for MLLMs"
      },
      {
        "paperId": "db2717c4883831506e46507065b313e74403ff8a",
        "title": "Automating Thematic Analysis: How LLMs Analyse Controversial Topics"
      },
      {
        "paperId": "4d46800c0a27307e3f68226ba3e6ec31f8fb952d",
        "title": "Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models"
      },
      {
        "paperId": "ce8f026d26a25a05110401c7f27f589bd7999b97",
        "title": "MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization"
      },
      {
        "paperId": "e1e066a61912b8add2314d6297b8ea4d2a3ded58",
        "title": "The Real, the Better: Aligning Large Language Models with Online Human Behaviors"
      },
      {
        "paperId": "38333f6e8f0388968edc4b2ea7a683ce69677e69",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      },
      {
        "paperId": "3cdb8432d11eed394906637f4eedf38d3f971015",
        "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning"
      },
      {
        "paperId": "6073196bd50b000571dbdfc46418304a4aff6591",
        "title": "Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "c827d319137ff92484833ca39c20af7372827b70",
        "title": "PatentGPT: A Large Language Model for Intellectual Property"
      },
      {
        "paperId": "27d55a944b5c02b8c10eb250773d8eb082e06476",
        "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback"
      },
      {
        "paperId": "08f6f1d7a98f47f146e539dd872180b9f0ddcd3d",
        "title": "AICAS Grand Challenge 2024: Software and Hardware Co-optimization for General Large Language Model Inference on CPU"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "370fb62e60f80081015d591f8c10c5a59a56a32d",
        "title": "Learn Your Reference Model for Real Good Alignment"
      },
      {
        "paperId": "865bf47dedfec416221504708cd1f039050b0a2d",
        "title": "Impact of Preference Noise on the Alignment Performance of Generative Language Models"
      },
      {
        "paperId": "d3755a4f2924b95a93e5652bffcf3d796f93c829",
        "title": "Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs"
      },
      {
        "paperId": "a5ec174d4e4894885ba6235fec0b73c2a9e69037",
        "title": "Latent Distance Guided Alignment Training for Large Language Models"
      },
      {
        "paperId": "f04e6bb8108cf191c88c461b5f367cd3ec336ebc",
        "title": "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages"
      },
      {
        "paperId": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
        "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data"
      },
      {
        "paperId": "48c34b348205ac1d3bbc5e84e7d2103b348b6c9a",
        "title": "ROPO: Robust Preference Optimization for Large Language Models"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "f29b21d60b8440afe50cb709bbe801315eff1e55",
        "title": "Token-Efficient Leverage Learning in Large Language Models"
      },
      {
        "paperId": "9216ec4ce85502e2d81beec89f54423af7810403",
        "title": "Prior Constraints-based Reward Model Training for Aligning Large Language Models"
      },
      {
        "paperId": "a98e51bf1de2b2d99ca75b9e8a2c983fcbe6f2c6",
        "title": "Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models"
      },
      {
        "paperId": "d624894c25478d32bbc399f4e8f9672fc37fe557",
        "title": "Can LLMs get help from other LLMs without revealing private information?"
      },
      {
        "paperId": "969c1dc38c98f2dbfb981a542880559890366494",
        "title": "Fine-Tuning Language Models with Reward Learning on Policy"
      },
      {
        "paperId": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee",
        "title": "Understanding the Learning Dynamics of Alignment with Human Feedback"
      },
      {
        "paperId": "d7bc3fecc6372c9b3fe2d0581167f00caaf05f36",
        "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models"
      },
      {
        "paperId": "8aeb1509d73c8d0846755f4fea8da36fb631d26b",
        "title": "Non-Linear Inference Time Intervention: Improving LLM Truthfulness"
      },
      {
        "paperId": "5acafd8acaebb9623af8ecc3deff8bcaeca2791e",
        "title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy"
      },
      {
        "paperId": "8155dad3723f4e5946561f8de6d37368b9b187e4",
        "title": "SC- Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models"
      },
      {
        "paperId": "e757377d351e7ae87e87a096c8e461b18f56b7dc",
        "title": "WoLF: Wide-scope Large Language Model Framework for CXR Understanding"
      },
      {
        "paperId": "b3cdde45d87a3a90d86ebe20c1ee8c4ef0150d1c",
        "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment"
      },
      {
        "paperId": "856873e8900405f7da9b4be2147e7039c46e9262",
        "title": "(N, K)-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model"
      },
      {
        "paperId": "914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
        "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy"
      },
      {
        "paperId": "1f9dd045cccbc8f7c38d553ddecd90d28a1b11db",
        "title": "Bridging Text and Molecule: A Survey on Multimodal Frameworks for Molecule"
      },
      {
        "paperId": "1660f5839b7068018951a226cb587cedf6aa66db",
        "title": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models"
      },
      {
        "paperId": "5e91fdffbd7bebcca842416885e8b1f460ab89aa",
        "title": "The Minimum Information about CLinical Artificial Intelligence Checklist for Generative Modeling Research (MI-CLAIM-GEN)"
      },
      {
        "paperId": "6fd7184bc15bf694902c0e465692e5bec533f69f",
        "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization"
      },
      {
        "paperId": "2403ca4ff39727bb1c922891d0320c07004bc17e",
        "title": "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation"
      },
      {
        "paperId": "db68cc363587bf82acf5a373b68bbf8a6bc11ac9",
        "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue"
      },
      {
        "paperId": "c14b58a49667fb0990759905ea3b874d50a983d1",
        "title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization"
      },
      {
        "paperId": "6f52cc360e9c6b31e50446864fff40bfec71310a",
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models"
      },
      {
        "paperId": "801f53923e66fd151f90b6d7cce08c65e65d1ee9",
        "title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning"
      },
      {
        "paperId": "94db8a625418800c8ae7b48157a9cad1c8129051",
        "title": "A Survey on Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "9ff25b04f81d21f700deb5b386857840b81a1f23",
        "title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic"
      },
      {
        "paperId": "de6ddb30b07f192f2be142062c4c6c817e508d96",
        "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation"
      },
      {
        "paperId": "4a9ce626c65f03ba9d9dc32403d2fbf91bbb192d",
        "title": "Reinforcement Learning with Large Language Models (LLMs) Interaction for Network Services"
      },
      {
        "paperId": "cacd57ad4eada225ae7c436fe726ac5549a6f926",
        "title": "Dissecting Human and LLM Preferences"
      },
      {
        "paperId": "8c95d8c8e34bf4cd778384fa49feaef54514cb9e",
        "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements"
      },
      {
        "paperId": "ee85c7c666135f4aae32336968f09584029b6a35",
        "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"
      },
      {
        "paperId": "4d4432514695e0f36720c73c23d15d8e21abe2fe",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models"
      },
      {
        "paperId": "19da1292e830f128f21b8f6b178c803edda657b7",
        "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models"
      },
      {
        "paperId": "0f6cd53c0cc1ee252433e0d37f419754e053b8a6",
        "title": "Suppressing Pink Elephants with Direct Principle Feedback"
      },
      {
        "paperId": "41ecff85ec0c8082fcd4efcf31fc5ec3911dce88",
        "title": "Large Language Models as Agents in Two-Player Games"
      },
      {
        "paperId": "2cb1933e7159a7cd2cd759c322e1973e28868cc7",
        "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs"
      },
      {
        "paperId": "fe66128c33a31677d5d91082042a7911a21be2fd",
        "title": "Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "e9640cd4bdb0fa93a94151ec00259909b5e88d6d",
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "b88b25aba67d7a391eb480563fbfd5b4a6625444",
        "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning"
      },
      {
        "paperId": "4c98e18cf16395b95ffaaeeac3eceffa608dcf8d",
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"
      },
      {
        "paperId": "a734edb6c3d70eec77ddb4504b2df87c3b74b77c",
        "title": "Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models"
      },
      {
        "paperId": "b90abcfbc391c606206b4d32c3887292f0fd3226",
        "title": "Professional Agents - Evolving Large Language Models into Autonomous Experts with Human-Level Competencies"
      },
      {
        "paperId": "4c2d8df556589ff4fbb5ee68c1f45bff3786624f",
        "title": "Aligner: Efficient Alignment by Learning to Correct"
      },
      {
        "paperId": "33b5dd24169ff876420f56c19c0a65b6194cb49f",
        "title": "User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT"
      },
      {
        "paperId": "7271c260a010ce15a8821891ecca52d83d011663",
        "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs"
      },
      {
        "paperId": "f977dac98cc603bfccae6ea991cf4b1f83bf139c",
        "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank"
      },
      {
        "paperId": "07de76b2862cf7738b3fd28aabd708186231a527",
        "title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models"
      },
      {
        "paperId": "421fcb39c696ae07a9b7c93e43716b21f8e6184b",
        "title": "Distilling LLMs' Decomposition Abilities into Compact Language Models"
      },
      {
        "paperId": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
        "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ec25dcdfd1cf0738fa712a30b0da1c00585ab21a",
        "title": "Enhancing Medical History Collection using LLMs"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "6d9d552af11f333b56158b4c4a3ccc236820eca1",
        "title": "GRATH: Gradual Self-Truthifying for Large Language Models"
      },
      {
        "paperId": "67eab08db30e397e400e3b36b3afd7526df83314",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      },
      {
        "paperId": "faae9de3d314e8731b0505607298fd826e3de1a7",
        "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation"
      },
      {
        "paperId": "8fd29e810540c40846cddce3cbdf5060cd59fb57",
        "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender"
      },
      {
        "paperId": "dabf7edde0efb9b1e092aa27847a547cf2961192",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback"
      },
      {
        "paperId": "d0cc57eccbc9b2ee027cff4813701ae3e61aedbc",
        "title": "Evaluating Language Model Agency through Negotiations"
      },
      {
        "paperId": "cfcf8ab7c595c1849e8396167a29f3bd3359107c",
        "title": "Agent Alignment in Evolving Social Norms"
      },
      {
        "paperId": "5708f725e13362da80a1062f51df118fca3529ab",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"
      },
      {
        "paperId": "b59069068c1e77e8c3d9e535c4780796279c5433",
        "title": "Towards eXplicitly eXplainable Artificial Intelligence"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "f65db0fc08e01bb41a7a69197b090765a7063f95",
        "title": "Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning"
      },
      {
        "paperId": "c65583b6848454de5d56668a2788093280e3cbb6",
        "title": "Typhoon: Thai Large Language Models"
      },
      {
        "paperId": "2b14d9e190022e388476ebb24eb1a84349ca0de4",
        "title": "Silkie: Preference Distillation for Large Visual Language Models"
      },
      {
        "paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "paperId": "e88a2b613cbbaf7e20a4039e7454b0f10eab3153",
        "title": "Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "a9e78765a4d49a50d67d0dacb033fb47f8d9f8c9",
        "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "ead69da72bf2e6c36be095a1c7da298f9b3055ae",
        "title": "Fault Diagnosis and System Maintenance Based on Large Language Models and Knowledge Graphs"
      },
      {
        "paperId": "0d1760f435aeb1a05ae99957a5dca58aa3f37abb",
        "title": "Large Language Models for Networking: Applications, Enabling Techniques, and Challenges"
      },
      {
        "paperId": "936f7f0fa77efcd322805b93a8d74c48a4108290",
        "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"
      },
      {
        "paperId": "5fdb4ca353809326ae5f5296a448c40ccbdc0b4b",
        "title": "Retrieval-Augmented Generation with Quantized Large Language Models: A Comparative Analysis"
      },
      {
        "paperId": "f5275c61736781d236abe6700b822f1ea62f982e",
        "title": "Diffusion Model Alignment Using Direct Preference Optimization"
      },
      {
        "paperId": "907cc13d907b2dfc39f6bb9e0c64cc69e71baefa",
        "title": "Mind\u2019s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"
      },
      {
        "paperId": "2e57f361d74582eefa047bce1a3943eaade2a1d6",
        "title": "An Empathetic User-Centric Chatbot for Emotional Support"
      },
      {
        "paperId": "de1894742b7f2e4fe02d9ff94761d6178e0a5d3c",
        "title": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision"
      },
      {
        "paperId": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
        "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?"
      },
      {
        "paperId": "e51f20efb872d0ba99a8b501259948bbb2f6963f",
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"
      },
      {
        "paperId": "e327ef8d46ea0413316c80ee1404453834d84f05",
        "title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training"
      },
      {
        "paperId": "45a119463a79e1f18d1234a96174b617a086388b",
        "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models"
      },
      {
        "paperId": "e6a207308c3f898d2c97ddf771c0060c23b93fe5",
        "title": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies"
      },
      {
        "paperId": "ffbf87817e86739040ef7e80169d55db707ea947",
        "title": "Policy Gradient with Kernel Quadrature"
      },
      {
        "paperId": "c085e88a0351e393609a95305afc1db792d1db0f",
        "title": "The History and Risks of Reinforcement Learning and Human Feedback"
      },
      {
        "paperId": "4d2e4436f2ecfad866000a545bbd65e516d8525f",
        "title": "Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning"
      },
      {
        "paperId": "9ec29a26336f043a705ac99baa04c8d7f69fe4b4",
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
        "title": "Verbosity Bias in Preference Labeling by Large Language Models"
      },
      {
        "paperId": "850d691e5f97001012fc323173635374d0a45ea9",
        "title": "ACES: Generating Diverse Programming Puzzles with with Autotelic Generative Models"
      },
      {
        "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
        "title": "Evaluating Large Language Models at Evaluating Instruction Following"
      },
      {
        "paperId": "cd267759c24771cdd6616dc16470e740ebb1dc8c",
        "title": "Factual and Personalized Recommendations using Language Models and Reinforcement Learning"
      },
      {
        "paperId": "d558f604981ed9911532baac17425b294799b528",
        "title": "Demystifying Embedding Spaces using Large Language Models"
      },
      {
        "paperId": "c31396f00c4e4ddba20d085d0da819b89c71bf4a",
        "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning"
      },
      {
        "paperId": "3a0e65b8af17adfb0f5f6db2fc80ec908a776fbb",
        "title": "Automatic Pair Construction for Contrastive Post-training"
      },
      {
        "paperId": "0bc95ed529fbc8a499dc6909937d5de04ed4266b",
        "title": "Adapting LLM Agents with Universal Feedback in Communication"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "b88ae92f5691b02eeefb967e570d0e1eac4aa01f",
        "title": "Evolving Diverse Red-team Language Models in Multi-round Multi-agent Games"
      },
      {
        "paperId": "541b66bad4a9bf9b7fd97f13f94ab9061c7c0c47",
        "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF"
      },
      {
        "paperId": "9099ee08e59cc33ed1c88d4708cf5c931bf46dc4",
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models"
      },
      {
        "paperId": "eed2a631d672a4130407f8d69a0ad9118a1e6e7d",
        "title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic"
      },
      {
        "paperId": "859d9e9c77ef556fc6257c2a395e9edfcac3b775",
        "title": "AceGPT, Localizing Large Language Models in Arabic"
      },
      {
        "paperId": "e56dc21699e6283fce072ffc908cb9f66321760d",
        "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60",
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning"
      },
      {
        "paperId": "d910a2ee08ca318ce56951f812be2b79c7318732",
        "title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator"
      },
      {
        "paperId": "cf4ded5439915863293ce6ab80fc704b9ee27ee4",
        "title": "Learning Evaluation Models From Large Language Models for Sequence Generation"
      },
      {
        "paperId": "a53a747ed6c7c8090d9542ed51d4f65103ac35ad",
        "title": "Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "9b9874bdc10f5c61dfeb316efc892e0d4e503af6",
        "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "13fd4277388cc2a9da75e8b772e5efcf6ebe2d32",
        "title": "Training Socially Aligned Language Models on Simulated Social Interactions"
      },
      {
        "paperId": "8f49bd5a69d64d0547a9e8c59cb7f91fcb5ed3ab",
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4"
      },
      {
        "paperId": "c226a4acb42912054d498bcf771023b0ba2da001",
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation"
      },
      {
        "paperId": "7919cb1a1dcf70ed7803c43a71d43dba696ef149",
        "title": "Making Language Models Better Tool Learners with Execution Feedback"
      },
      {
        "paperId": "0ea2173623c06b2fdae8830c93bdb5c2ec6f5d08",
        "title": "Evaluating Design Choices in Verifiable Generation with Open-source Models"
      },
      {
        "paperId": "a81f2fb0d1b7c923511c34f1b2ed04ecd27a3361",
        "title": "Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models"
      },
      {
        "paperId": "e6876d73d5aa06e09328d8ddf240c8a800a62667",
        "title": "Iterative Trajectory Exploration for Multimodal Agents"
      },
      {
        "paperId": "5798b423debb5b60dfcf4b0809c39844bb4f3378",
        "title": "Synthetic Documents for Medical Tasks: Bridging Privacy with Knowledge Injection and Reward Mechanism"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      },
      {
        "paperId": "ee49b4c93f675efb28f8f1d5549e65115f6b8e93",
        "title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization"
      },
      {
        "paperId": "48b5ff98e9bbd1beabc73b03313892c0c411036d",
        "title": "Direct Repair Optimization: Training Small Language Models For Educational Program Repair Improves Feedback"
      },
      {
        "paperId": "5b2234269a8c65eca9068ddd56f293ec1067c0fb",
        "title": "A Survey of Post-Training Scaling in Large Language Models"
      },
      {
        "paperId": "8d85be89f9a8e56f05f67e4dec5345a5bab743fe",
        "title": "LLM-Based Generative AI in Medicine: Analysis of Current Research Trends With BERTopic"
      },
      {
        "paperId": "6578806662072300f211220c21aecc09c911dc51",
        "title": "Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning"
      },
      {
        "paperId": "b6ffb6c17f22f46a590c396240dd23ec12f22b50",
        "title": "Can Large Language Models Capture Human Annotator Disagreements?"
      },
      {
        "paperId": "6fba163c2582e7269f719a58f00792187bb80875",
        "title": "ARM: Alignment with Residual Energy-Based Model"
      },
      {
        "paperId": "3c7b4f94a41fe6162717e5fb27a1d326a3304029",
        "title": "CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models"
      },
      {
        "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
        "title": "Large Language Models for Data Annotation: A Survey"
      },
      {
        "paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a",
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "a0748478cd2752b733b4183dbd0dcd1031c38b6e",
        "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "2294e237aa46d3bb34f71d224a6da9bdf061329b",
        "title": "Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender"
      },
      {
        "paperId": "a45d381a6803aa79c69b04936ad33d7284df8c91",
        "title": "Reflection-Reinforced Self-Training for Language Agents"
      },
      {
        "paperId": "31ccf19d7c8e0d5f5ad485a0a9127eac9af941d3",
        "title": "Language Models Resist Alignment"
      },
      {
        "paperId": "1303556899d0a97a46ab2d60eaba3ea7de3ce57c",
        "title": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement"
      },
      {
        "paperId": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
        "title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback"
      },
      {
        "paperId": "b49145c33d435f9e146ccff31d109104276159d9",
        "title": "The Paradox of Preference: A Study on LLM Alignment Algorithms and Data Acquisition Methods"
      },
      {
        "paperId": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
        "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models"
      },
      {
        "paperId": "9af217675ef0e237ddf1e465ea3aefed604c5b95",
        "title": "Combining model-based controller and ML advice via convex reparameterization"
      },
      {
        "paperId": "5a2c395ca5dc942dbb6ea7eea1e324b8c9e8534e",
        "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "694ed488bc3b63ad896cc0d2c92abdd1133698c1",
        "title": "Improving Low-Resource Machine Translation Using Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "c9240006dbe3be68b2e4a7b18ad40ea5f75bed9d",
        "title": "DRLC: Reinforcement Learning with Dense Rewards from LLM Critic"
      },
      {
        "paperId": "d7d996240798009aeb8b3d0cf21b42dc5b8e4023",
        "title": "ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors"
      },
      {
        "paperId": "7e8453fa32708b2690178c3a7a6618aba20928af",
        "title": "Training Socially Aligned Language Models in Simulated Human Society"
      },
      {
        "paperId": "836a463da0e813b6236adebef6b9a9cc9bdbe3f7",
        "title": "Contrastive Post-training Large Language Models on Data Curriculum"
      },
      {
        "paperId": "cb660ea0c8c14097513a2a2199ed3a18799683be",
        "title": "Entangled Preferences: The History and Risks of Reinforcement Learning and Human Feedback"
      },
      {
        "paperId": "1569056ac2299b0bb47551b574be187a1b7f6015",
        "title": "Data-Augmented DPO: Comparing Enhancements of SFT-Trained LLMs"
      },
      {
        "paperId": "cd0f365fca59f303ce158c36faa7a7f430a5a698",
        "title": "ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training"
      },
      {
        "paperId": "681a219b3c3e9c07dd5d3b1fafb5be49707ab551",
        "title": "Feedback or Autonomy? Analyzing LLMs\u2019 Ability to Self-Correct"
      },
      {
        "paperId": "951a11a4d247de5823dfa42affe39cf849c708b8",
        "title": "Pairwise Proximal Policy Optimization: Large Language Models Alignment via Comparative RL"
      },
      {
        "paperId": "a67a040ec0dcbfb3ec952915b6a66a8488b3ba4a",
        "title": "Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process"
      },
      {
        "paperId": "dd4958e486d0d54aa57dfbaeb9cd74c6b37f707c",
        "title": "RLDF:Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs"
      },
      {
        "paperId": "7e54cededaea99e988a1f77acff6897d4687c6ef",
        "title": "Prototypical Reward Network for Data Efficient Model Alignment"
      },
      {
        "paperId": "5c4942df130f4fc6ffccde4850b8b0cfdf58c082",
        "title": "O RCHESTRATING S YNTHETIC D ATA WITH R EASONING"
      },
      {
        "paperId": "4fe785ab34fd825e6ea5ac0e5c6647b94718085b",
        "title": "ReCAP: Recursive Context-Aware Reasoning and Planning with Language Models"
      },
      {
        "paperId": "069b1f734565f0215bc22caeaf178c24c0d7e18b",
        "title": "CS224R Default Final Project: Synthetic Data Augmentation for LLM Training"
      },
      {
        "paperId": "b06b91637892b0c5e13da6c3dc0dde5c374ce971",
        "title": "Train smarter LLMs: balancing Data Quality, Fine-Tuning and Reinforcement Learning"
      },
      {
        "paperId": "94ea4929a09e6947b762fa621676f767f004ddc1",
        "title": "ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization"
      }
    ],
    "score": 204.0
  },
  {
    "id": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
    "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
    "authors": [
      "Yecheng Jason Ma",
      "William Liang",
      "Guanzhi Wang",
      "De-An Huang",
      "Osbert Bastani",
      "Dinesh Jayaraman",
      "Yuke Zhu",
      "Linxi Fan",
      "Anima Anandkumar"
    ],
    "year": 2023,
    "citationCount": 393,
    "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
    "url": "https://www.semanticscholar.org/paper/6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
    "pdf_url": "https://arxiv.org/pdf/2310.12931.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-10-19",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-12931",
      "ArXiv": "2310.12931",
      "DOI": "10.48550/arXiv.2310.12931",
      "CorpusId": 264306288
    },
    "references": [
      {
        "paperId": "54814744b42b06c855c97b23de1366e0bcbe775a",
        "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"
      },
      {
        "paperId": "a9c75cf664f675a1b4034b0256ec3c5168e293df",
        "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "38939304bb760473141c2aca0305e44fbe04e6e8",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
      },
      {
        "paperId": "688fc1e744877c3a68f306443042f016196ce98a",
        "title": "The Perils of Trial-and-Error Reward Design: Misdesign through Overfitting and Invalid Task Specifications"
      },
      {
        "paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c",
        "title": "Language to Rewards for Robotic Skill Synthesis"
      },
      {
        "paperId": "b842ec712ff4ac7793016c5d4c03c0b0b37b998b",
        "title": "LLMatic: Neural Architecture Search Via Large Language Models And Quality Diversity Optimization"
      },
      {
        "paperId": "f69f95835deec7748a688675721b6d581b60d42b",
        "title": "LIV: Language-Image Representations and Rewards for Robotic Control"
      },
      {
        "paperId": "9dee1aceb09f7d4c22fdbaf49d238e1502effd1b",
        "title": "Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought"
      },
      {
        "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"
      },
      {
        "paperId": "b3bba15f000000a6d3b5808f798a9fe7629fa499",
        "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models"
      },
      {
        "paperId": "afc5092a4116f27b4c64733c7815cd662bab78f7",
        "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"
      },
      {
        "paperId": "003ef1cd670d01af05afa0d3c72d72228f494432",
        "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"
      },
      {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning"
      },
      {
        "paperId": "8f2d4758e6d525509ae36bb30224dc9259027e6b",
        "title": "Text2Motion: from natural language instructions to feasible plans"
      },
      {
        "paperId": "cf41ae462687f81ce95b27113c6a4f9c2751de42",
        "title": "Vision-Language Models as Success Detectors"
      },
      {
        "paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
        "title": "Task and Motion Planning with Large Language Models for Object Rearrangement"
      },
      {
        "paperId": "411b16add23976ffcdf6422f932453f6ebcca119",
        "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "3396609b96dd24cac3b1542aec686ce362f32fe2",
        "title": "Language-Driven Representation Learning for Robotics"
      },
      {
        "paperId": "89e184d2bc830af568e439db9476caa0c047e11a",
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "3ad346ae7af5c30964c4916dbcee798f72e1bdb7",
        "title": "Translating Natural Language to Planning Goals with Large-Language Models"
      },
      {
        "paperId": "960236c4380656fa254f7e367ceb4b14cbeda45c",
        "title": "Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo"
      },
      {
        "paperId": "c14a5239ab4e8dd2964169450e727a6b089671a2",
        "title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality"
      },
      {
        "paperId": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
        "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      },
      {
        "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867",
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"
      },
      {
        "paperId": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "paperId": "b4888a20a9910e292448a51bf4248cc5b60e2af3",
        "title": "A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning"
      },
      {
        "paperId": "4baac6b8fa7731352004bc45d2ba2b6bbd04a4e7",
        "title": "Evolution through Large Models"
      },
      {
        "paperId": "32c9b3859086d15184989454eb878638659e64c6",
        "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"
      },
      {
        "paperId": "efa4fa4120e3efe5d7384a7b3428f886de9b6a29",
        "title": "Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning"
      },
      {
        "paperId": "b7cbf03974511d7855b4c01e3005f3e38bd1a136",
        "title": "Rapid locomotion via reinforcement learning"
      },
      {
        "paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "49142e3e381c0dc7fee0049ea41d2ef02c0340d7",
        "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "1ca5ff6555d9fc634d3858d1fda9b3de2a91b13a",
        "title": "RMA: Rapid Motor Adaptation for Legged Robots"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "b4334052e996fa95a26088ccec30d291d0499145",
        "title": "Reward (Mis)design for Autonomous Driving"
      },
      {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand"
      },
      {
        "paperId": "f6fef0150ba7d16ebce009eba314cce586211219",
        "title": "Evolving Rewards to Automate Reinforcement Learning"
      },
      {
        "paperId": "64cdf82a4eab0420f8f79a7f2d5613251fe20953",
        "title": "Learning Navigation Behaviors End-to-End With AutoRL"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "3177334d5ef8e0ece30913b4692b86801f0845c5",
        "title": "Model Predictive Path Integral Control using Covariance Variable Importance Sampling"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "0935f824c95366d2a786e4f609487a6e9f83a1a8",
        "title": "Genetic Programming for Reward Function Search"
      },
      {
        "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
        "title": "Curriculum learning"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "46299fee72ca833337b3882ae1d8316f44b32b3c",
        "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection"
      },
      {
        "paperId": null,
        "title": "rl-games: A high-performance framework for reinforcement learning"
      },
      {
        "paperId": "67238f415dfb2024298082d8c9f7eb529fe191f5",
        "title": "Where Do Rewards Come From"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Artificial Intelligence: A Modern Approach"
      },
      {
        "paperId": null,
        "title": "you see [optional]"
      },
      {
        "paperId": null,
        "title": "Please analyze each existing reward component in the suggested manner above first, and then write the reward function code. The output of the reward function should consist of two items:(1)"
      },
      {
        "paperId": null,
        "title": "NVIDIA Omniverse Platform"
      },
      {
        "paperId": null,
        "title": "Do not invent new functions or classes. The only allowed functions you can call are the ones listed above, and do not implement them. Do not leave unimplemented code blocks in your response"
      },
      {
        "paperId": null,
        "title": "you see phrases like {CHOICE: choice1, choice2, ...}, it means you should replace the entire"
      },
      {
        "paperId": null,
        "title": "Some helpful tips for writing the reward function code: (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch"
      },
      {
        "paperId": null,
        "title": "Always format the code in code blocks"
      },
      {
        "paperId": null,
        "title": "of each individual reward component"
      },
      {
        "paperId": null,
        "title": "on the analysis of the policy feedback"
      },
      {
        "paperId": null,
        "title": "set_min_l2_distance_reward(name_obj_A, name_obj_B"
      },
      {
        "paperId": null,
        "title": "The only allowed library is numpy. Do not import or use any other library"
      },
      {
        "paperId": null,
        "title": "If some reward component magnitude is significantly larger, then you must re-scale its value to a proper range"
      },
      {
        "paperId": null,
        "title": "Do not wrap your code in a function. Your output should only consist of function calls like the example above"
      },
      {
        "paperId": null,
        "title": "If you are not sure what value to use, just use your best judge"
      }
    ],
    "cited_by": [
      {
        "paperId": "6a030995cbf4983498d9bf9c45e29f1bb8e006cc",
        "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning"
      },
      {
        "paperId": "ca8fb30e4425d6fde21695716f71d3f1433c9287",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning"
      },
      {
        "paperId": "61e1a5bb6dd6bcc48e4fc8fe051f935465d32610",
        "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models"
      },
      {
        "paperId": "f53c4466995a82eca04f370255f949f802cf2c8f",
        "title": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management"
      },
      {
        "paperId": "779e0e9480cbeb689b7d7b9576af3cc4c6e8b523",
        "title": "Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress"
      },
      {
        "paperId": "51357ff32df3924f29e550aaf71c9882c2b99348",
        "title": "Variational Reasoning for Language Models"
      },
      {
        "paperId": "3b3e3d991af437f7c9412548223a87e15472b1a7",
        "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning"
      },
      {
        "paperId": "df51d8936e440829ef52a668176c98ca8e51db8b",
        "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning"
      },
      {
        "paperId": "c3e6fd0fdd93e33abf551837effd93440facd439",
        "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models"
      },
      {
        "paperId": "4894b5e236924b49c93b0db32e0ea3f4e803f9e3",
        "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning"
      },
      {
        "paperId": "543f87b15b888cb0a2381c5b36141256ad5ec84e",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning"
      },
      {
        "paperId": "5f25bd70b974edf6a5babeaeb617303fca3dd1ee",
        "title": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning"
      },
      {
        "paperId": "929349ef1d0a1d007abe756a9912537ff08359d8",
        "title": "Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds"
      },
      {
        "paperId": "51d8a202334ca4067d637587f97ac56f71ccc98a",
        "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping"
      },
      {
        "paperId": "c3a8984fbcb50f9f18c3880901b52ef131412cce",
        "title": "Self-Improving Embodied Foundation Models"
      },
      {
        "paperId": "75297c2757238e17802549e6334ffe5e54a0290f",
        "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks"
      },
      {
        "paperId": "2fa350b566feb088de0b97057986ecb56ec0f360",
        "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation"
      },
      {
        "paperId": "eee2a5378d3fe02ab616ce5557d6b56f119b3bb3",
        "title": "Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models"
      },
      {
        "paperId": "bca7f4dd4db559d6772b470d9fe5391e3608cc8c",
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"
      },
      {
        "paperId": "63ee980397ea56bf54f45290b9244943e9ae93f3",
        "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions"
      },
      {
        "paperId": "1388f68e00cded439a6cbfa1f71daf00fc38e681",
        "title": "Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots"
      },
      {
        "paperId": "0c9405aa6d4c6a920a4504fd786d91b48bcc6b48",
        "title": "Large language model assisted hierarchical reinforcement learning training"
      },
      {
        "paperId": "d015b9a3bf45b7812bdf7d81d9256d643d0c6625",
        "title": "Scalable Option Learning in High-Throughput Environments"
      },
      {
        "paperId": "9422cc3b9b2cc547174778f3292dc65f8cd8a1e9",
        "title": "RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation"
      },
      {
        "paperId": "c1723f2ed49ae47e5ba9e7889ff2bd8702bd5061",
        "title": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending"
      },
      {
        "paperId": "dec0d3faa6343a547a9e828d765958ee3c1042ad",
        "title": "LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "6f3404739763e75ed5989078053a860ee7997f83",
        "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning"
      },
      {
        "paperId": "2220678b6bfb386698bff54b3e7aea5706dd59ca",
        "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints"
      },
      {
        "paperId": "f695bbbe1abd4f4d81d87c89d0652b0223c0a643",
        "title": "\\(X\\)-evolve: Solution space evolution powered by large language models"
      },
      {
        "paperId": "f0ffc6f1d50b9232e3e90d1ab0edd156c1fc1e45",
        "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation"
      },
      {
        "paperId": "ea4be551815e8bbdbbb9bef691428fa0b81350fc",
        "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models"
      },
      {
        "paperId": "2c26f5223f75eaac6a0b939bf402a6f36a54001d",
        "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks"
      },
      {
        "paperId": "82ea274f6e2394d8f671a245995b535e30431945",
        "title": "Generative AI Meets Wireless Networking: An Interactive Paradigm for Intent-Driven Communications"
      },
      {
        "paperId": "50fc2d333cdb60f7c52591b93ecb6763ee50c445",
        "title": "Integrating Large Language Models into Fluid Antenna Systems: A Survey"
      },
      {
        "paperId": "6aa422169a5667d664248304d50a914a68429fa6",
        "title": "Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation"
      },
      {
        "paperId": "5157d2a42823e24c53437ea9f09bd6e147746d72",
        "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding"
      },
      {
        "paperId": "24f423d96ee85118a0791c62cf4deebb0e44c17a",
        "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning"
      },
      {
        "paperId": "7f2a8f1891765c2394bd10a6f0203621c5fe85e5",
        "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models"
      },
      {
        "paperId": "77a4a2791680d7651e11199b88e115361cb28c3f",
        "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths"
      },
      {
        "paperId": "05aba9fc49d7f55fd361b2c66081f9825da03429",
        "title": "LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving"
      },
      {
        "paperId": "dff8a32d988886af3ac02e072997fc0eebd25dd3",
        "title": "The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio"
      },
      {
        "paperId": "8e44a535c4712b3e7a7d857296a8f91997837829",
        "title": "Uncertainty-aware Reward Design Process"
      },
      {
        "paperId": "d6f2993b38bb9891be5bc91ac3f2664801e0dbed",
        "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games"
      },
      {
        "paperId": "c1fe3eee495f4ef457881965fb5a44b7190be422",
        "title": "A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots"
      },
      {
        "paperId": "34cb5a043fbfb04dbcc3f346e959f537c22d7bfe",
        "title": "Enhancing Large Language Models through Structured Reasoning"
      },
      {
        "paperId": "5e93dbd203dc8997cfd604937ec0fe9d78528049",
        "title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models"
      },
      {
        "paperId": "507dd245030099afc1f5a416ac5a9547837e4cbe",
        "title": "Scaffolding Dexterous Manipulation with Vision-Language Models"
      },
      {
        "paperId": "d8f8562ce63d9176d3e30db46e208c25ef6932a5",
        "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models"
      },
      {
        "paperId": "5a970fdc180e28de0a072ae38672239197188518",
        "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization"
      },
      {
        "paperId": "8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
        "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models"
      },
      {
        "paperId": "f052b0bae05863022ea10c32440d14761e82492a",
        "title": "VLMs-Guided Representation Distillation for Efficient Vision-Based Reinforcement Learning"
      },
      {
        "paperId": "e12cf0e49ad8fc7a052cbc21597b6c5144f5f634",
        "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement"
      },
      {
        "paperId": "c0851f97ccb5ed8432f4c144cc7e63985536e43d",
        "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning"
      },
      {
        "paperId": "e4bcb3f40c952a639c2bd665f8754defd67aad5e",
        "title": "AI Agent Behavioral Science"
      },
      {
        "paperId": "2ca9ec016cbea7264bfa6453858e471a9e23544a",
        "title": "AURA: Autonomous Upskilling with Retrieval-Augmented Agents"
      },
      {
        "paperId": "4b9ecb766a3e247b2743b5dad2830dfe65ca37d7",
        "title": "LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation"
      },
      {
        "paperId": "f2f56afdd2c8c6dd00ec028eba1f792dd99504d9",
        "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation"
      },
      {
        "paperId": "62f1e4ef7f61fa4f50e4c05f3934e0915a87ed6e",
        "title": "RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward"
      },
      {
        "paperId": "38b248e2f38d9b2d3da70e35cb61efaf00b146f7",
        "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control"
      },
      {
        "paperId": "cfe99165e00c47dce3f1350b7987595cb831d8cd",
        "title": "VIRAL: Vision-grounded Integration for Reward design And Learning"
      },
      {
        "paperId": "efc028b1a397eeb46eb6b945faea536d068b752a",
        "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning"
      },
      {
        "paperId": "a2046b77adfc5c57551e0e62b702389496b6223c",
        "title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction"
      },
      {
        "paperId": "51b48b726a6ee5e3361f4bad97e6ba916104ee1d",
        "title": "Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments"
      },
      {
        "paperId": "193c0713c918a783c7d81398545c14b55d52b5bc",
        "title": "VideoGameBench: Can Vision-Language Models complete popular video games?"
      },
      {
        "paperId": "485787dcf70ddc4cea487bd446b111ae68373536",
        "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization"
      },
      {
        "paperId": "ff0dc43bbae86228028c143ec33d4c3e89e5b2a4",
        "title": "LLM Coach: Reward Shaping for Reinforcement Learning-Based Navigation Agent"
      },
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "380edc21facfd9961c7c941bb96805b10ce424b3",
        "title": "On the potential of agentic workflows for animal training plan generation"
      },
      {
        "paperId": "aee57eb1a5078c09d81a7c5f6e4cd3af0e89bcf4",
        "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks"
      },
      {
        "paperId": "4b8cf05b93af70ad77a9be04171e2efbbf773053",
        "title": "Generative-AI-Driven Jumping Robot Design Using Diffusion Models"
      },
      {
        "paperId": "1eedb171d1057b6852d1444a490845bc7ba15f6d",
        "title": "ASCENT: Autonomous Skill Learning Toward Complex Embodied Tasks With Foundation Models"
      },
      {
        "paperId": "c2158c5ad0a04ef1835f96ca8c308e893a49a38f",
        "title": "GenCo: A Dual VLM Generate-Correct Framework for Adaptive Peg-in-Hole Robotics"
      },
      {
        "paperId": "c8b1c313da7f654d58e626e4d32eda5b4c7f300b",
        "title": "Jailbreaking LLM-Controlled Robots"
      },
      {
        "paperId": "ce9ad37e019379d3ea68d6d787fd42dc60357e86",
        "title": "Soft Robotic Dynamic in-Hand Pen Spinning"
      },
      {
        "paperId": "43e53b6c7824d8db40ff6298fa4f6ea221bcc557",
        "title": "Interface Matters: Comparing First and Third-Person Perspective Interfaces for Bi-Manual Robot Behavioural Cloning"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "4046e8f4ac46a533f67aa9432609c150aba5faff",
        "title": "Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition"
      },
      {
        "paperId": "962eedb6cf3bd41f6481eb35030f7a2046521e4e",
        "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM"
      },
      {
        "paperId": "b5751c587aac51702e33f9b453eea2381f48c6a2",
        "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments"
      },
      {
        "paperId": "4efd24d4f84c901e27d708be7c8925b2618722da",
        "title": "MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos"
      },
      {
        "paperId": "33ef686c36a1d1d994de170005020f8a2d10c3f8",
        "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains"
      },
      {
        "paperId": "e17d1c586bc5581b46cddc2b535427a31f63f408",
        "title": "Robust Model-Based In-Hand Manipulation with Integrated Real-Time Motion-Contact Planning and Tracking"
      },
      {
        "paperId": "bb0c17deea3e8b4ece87556764cd10fa8b71a39c",
        "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators"
      },
      {
        "paperId": "50841a31ea78c3bd1170e985f4dd371f860d510e",
        "title": "DSDrive: Distilling Large Language Model for Lightweight End-to-End Autonomous Driving with Unified Reasoning and Planning"
      },
      {
        "paperId": "36b35bc8ea704f894ce776d27644baff5bc15f7e",
        "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows"
      },
      {
        "paperId": "58d06bb21c22192f03fa28c3261122e23e426461",
        "title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning"
      },
      {
        "paperId": "1f6d0df3dceb6f696dc0e0bd693b711c45c6f1c5",
        "title": "Meta-Optimization and Program Search using Language Models for Task and Motion Planning"
      },
      {
        "paperId": "0edf0d6f921cf853408c5c8c23d04fc81b5b8610",
        "title": "Automated Hybrid Reward Scheduling Via Large Language Models for Robotic Skill Learning"
      },
      {
        "paperId": "2174f1e6274bbe417ce5c5a2526699b0bcd8ec13",
        "title": "Prompt-Responsive Object Retrieval with Memory-Augmented Student-Teacher Learning"
      },
      {
        "paperId": "7ab10fec046316f5458637c2524936d72778f0e1",
        "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models"
      },
      {
        "paperId": "d754e3df3da552c87aa9f0ced53bdb2b16c72832",
        "title": "SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement"
      },
      {
        "paperId": "44bd8a38d97cda8cebeb71e0a6848868993069e2",
        "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations"
      },
      {
        "paperId": "e823117b506dfa7af0ce24fedc60dfcfee8715ef",
        "title": "An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination"
      },
      {
        "paperId": "251f02c704579ba98df0596f07d9dab1d95d1ca7",
        "title": "RL-Driven Data Generation for Robust Vision-Based Dexterous Grasping"
      },
      {
        "paperId": "55887195767045bc224dcdb8b0dd442bba3bfa87",
        "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning"
      },
      {
        "paperId": "3c27e34b1a969f231e8770c28c9be321a2a8ff6b",
        "title": "LLM-Based Reward Engineering for Reinforcement Learning: A Chain of Thought Approach"
      },
      {
        "paperId": "87f5ab251b54533a330905f500e82f4fd1bd6889",
        "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey"
      },
      {
        "paperId": "dd9e6a02ba61cb4839a22c6f53eb867f95828ef8",
        "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning"
      },
      {
        "paperId": "2f64b409961b563086bb7521009f62159fe6b52e",
        "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?"
      },
      {
        "paperId": "301a4e6a2aeb6a6abb5e2a37ab19c1c26d3f2716",
        "title": "Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models"
      },
      {
        "paperId": "3672b17ffda16e14bf034675eaffa904e4e71844",
        "title": "Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution"
      },
      {
        "paperId": "aa3517c664890cd36aee3cfff0f09d2645e373b6",
        "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility"
      },
      {
        "paperId": "3de45c7a7ef3ac6bd156c9b037b1f6c39a21e16c",
        "title": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization"
      },
      {
        "paperId": "fbc0b5e1b822796d7ae97268def2e0993b5da644",
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning"
      },
      {
        "paperId": "96cf799e708f1cf4c0e1a09f8ecbc3b5224cb3f2",
        "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill"
      },
      {
        "paperId": "1595da87d5bab53aa4a360dcb9058ccebeb701b3",
        "title": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning and Control via LLMs"
      },
      {
        "paperId": "7b04ad06498a615f5ddefb56d208c45714c33528",
        "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning"
      },
      {
        "paperId": "29764cce1641000c290144a0f6ea1377caff17d9",
        "title": "LLM-Guided Search for Deletion-Correcting Codes"
      },
      {
        "paperId": "ddd1b5e92e0464bd775fc52218a7fda150ea6149",
        "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?"
      },
      {
        "paperId": "69a57a3b95743dce030d6099f85519c3aa942300",
        "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap"
      },
      {
        "paperId": "2d7f3a99e916fc80ff890d109699f9682253e66d",
        "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models"
      },
      {
        "paperId": "764d8db2430d50b3ad7ba86fa0efd281c0003b47",
        "title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7d1da321eadaacc58ef4a7cc5e0e894aaf566076",
        "title": "Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy"
      },
      {
        "paperId": "b625d9bf38268f245e29b9406c7ce182d66d577f",
        "title": "AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models"
      },
      {
        "paperId": "2dd988da09f644bac28e4e6fe1a9108b9f030c96",
        "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning"
      },
      {
        "paperId": "292b516c8e0269505d5c44ac23f183f8aec1cd69",
        "title": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution"
      },
      {
        "paperId": "c85ecb7e84f73b4d1043c6860c0f39d5e344affa",
        "title": "PANDORA: Diffusion Policy Learning for Dexterous Robotic Piano Playing"
      },
      {
        "paperId": "1e934631aeb726b5cd2a68d02a88f98c9e60ce71",
        "title": "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models"
      },
      {
        "paperId": "314fb474798301cefceb089a10f1ae78b3ced097",
        "title": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-Based Planner and Graph-Based Policy"
      },
      {
        "paperId": "423c6f1180ed65500daabe6f5998300d1bae2293",
        "title": "LuciBot: Automated Robot Policy Learning from Generated Videos"
      },
      {
        "paperId": "fa64e81ad6ff1676007d1c8c1d8087575f8d79cc",
        "title": "Proc4Gem: Foundation models for physical agency through procedural generation"
      },
      {
        "paperId": "ffc7591e5374fb003ec8617974a93063cca688e4",
        "title": "Safety Guardrails for LLM-Enabled Robots"
      },
      {
        "paperId": "536c721c7d80c3da9807be535f4a4f82cd180849",
        "title": "Focused Blind Switching Manipulation Based on Constrained and Regional Touch States of Multi-Fingered Hand Using Deep Learning"
      },
      {
        "paperId": "b49cbccd88cacf9a7514c80d33cf5c42b80edc1a",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation With Large Language Models"
      },
      {
        "paperId": "91af851f674aaa6064827527a3f40c733e51f742",
        "title": "Generative Artificial Intelligence in Robotic Manipulation: A Survey"
      },
      {
        "paperId": "8a7ed56e9b1632c2295149ea448dcb089bf2081d",
        "title": "Closing the Intent-to-Behavior Gap via Fulfillment Priority Logic"
      },
      {
        "paperId": "33f24a2ac0248a72627b1ea42651e5b0700f34b6",
        "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning"
      },
      {
        "paperId": "96814a0576c703b021b1a7e7ccc17c776bf89b29",
        "title": "M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality"
      },
      {
        "paperId": "b58c2f6a0d7134c208fcad8534dd189baed8af0c",
        "title": "Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation"
      },
      {
        "paperId": "af6e9cef7efb18a20a16f585fc27c26dbcb9483c",
        "title": "Offline RLAIF: Piloting VLM Feedback for RL via SFO"
      },
      {
        "paperId": "ae843221017d8b8941d4ea11a883c52240f8ce07",
        "title": "PrefCLM: Enhancing Preference-Based Reinforcement Learning With Crowdsourced Large Language Models"
      },
      {
        "paperId": "62d621c022c7a55703d9a42666bb97134d8bffc9",
        "title": "Never too Prim to Swim: An LLM-Enhanced RL-based Adaptive S-Surface Controller for AUVs under Extreme Sea Conditions"
      },
      {
        "paperId": "d6cb7abdff56538526e15e070f5fcd06c8479924",
        "title": "A novel voice in head actor critic reinforcement learning with human feedback framework for enhanced robot navigation"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "c902b8449b2d3e3a2461c3a25f988301583bcf3f",
        "title": "SolSearch: An LLM-Driven Framework for Efficient SAT-Solving Code Generation"
      },
      {
        "paperId": "b59b392ad552464d55b76c19bf7ea38b0063ce92",
        "title": "Learning a High-Quality Robotic Wiping Policy Using Systematic Reward Analysis and Visual-Language Model Based Curriculum"
      },
      {
        "paperId": "f8e9cbb77a3fd538cf079d8156d82f763e803418",
        "title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning"
      },
      {
        "paperId": "06b60fdb6fe30cefa6041ba064023b0101291903",
        "title": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation"
      },
      {
        "paperId": "2a2d8ef3a738d1f8b09593e6681483de98f6fa29",
        "title": "Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos"
      },
      {
        "paperId": "56039ba61dbf35ffff1f10b94c094135e422c91b",
        "title": "S2-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation"
      },
      {
        "paperId": "3d2e1bccea595081350c578e7d03095a4ba670a7",
        "title": "A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards"
      },
      {
        "paperId": "56f2237a2dd1331a40b541bb490d292a9790a725",
        "title": "Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics"
      },
      {
        "paperId": "6590ba146e330de09ce7798164cc1ddeecfc0b17",
        "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping"
      },
      {
        "paperId": "f7fda04c653b44b043735cb4a3c108c84131e067",
        "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation"
      },
      {
        "paperId": "020ce0b77f5d1ef6cea39fca302b132a14cab1a3",
        "title": "Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning"
      },
      {
        "paperId": "289f55b66bd9dc7eebbd9f9d8a54b7f0b5705117",
        "title": "Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning"
      },
      {
        "paperId": "373c49b21ff6b68ab4c0ab825fb182802262e5d5",
        "title": "Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling"
      },
      {
        "paperId": "b5ecaa8f23ccf72595047e36aff3dc7c1cd6567c",
        "title": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning"
      },
      {
        "paperId": "ef32819c179eb32fe2061b4078c385d8e1ab0582",
        "title": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models"
      },
      {
        "paperId": "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
        "title": "Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "93b08946893d111933cd90b4a476d8da26e1f229",
        "title": "A Review of Embodied Grasping"
      },
      {
        "paperId": "ffc09c7971df578929ea3764d06e4ab62886421f",
        "title": "Improving Vision-Language-Action Model with Online Reinforcement Learning"
      },
      {
        "paperId": "d1d0e5ff34fa5daf3de70e24dfbfdfa919cb49c5",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems"
      },
      {
        "paperId": "a48388ba8823eac7d55cabb58b784b672eb3b3df",
        "title": "Weber-Fechner Law in Temporal Difference learning derived from Control as Inference"
      },
      {
        "paperId": "d5f037f09bbdfe2f249b9c0fad3c7a5d6fb5fbf2",
        "title": "Reward Design Framework Based on Reward Components and Large Language Models"
      },
      {
        "paperId": "67be338ca139ec1613f73243fc312e42d591a027",
        "title": "SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC"
      },
      {
        "paperId": "081e85450889861bc965fe5470a9c0f921ff4652",
        "title": "Automating the Search for Artificial Life with Foundation Models"
      },
      {
        "paperId": "f6ad40380432d292b88dd7625df45fd8abec8a10",
        "title": "Embedding high-resolution touch across robotic hands enables adaptive human-like grasping"
      },
      {
        "paperId": "c41b872dd1a138d53528921790f3ad13471ae52e",
        "title": "RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL Evaluation and LLM Enhancement"
      },
      {
        "paperId": "ee0fb97abf3ade9a7ea62e4507a58b3c1b6fbaa4",
        "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons"
      },
      {
        "paperId": "3ee603a1f6cad76b47055bcb823113236dc85900",
        "title": "The Synergy of LLMs&RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data"
      },
      {
        "paperId": "b5e2ee878e757f1e7e557b89e7d3460edacb50ed",
        "title": "RLZero: Direct Policy Inference from Language Without In-Domain Supervision"
      },
      {
        "paperId": "28d9ddc8048dabce5c0c8b373a959c5a1dea9b65",
        "title": "Video2Reward: Generating Reward Function from Videos for Legged Robot Behavior Learning"
      },
      {
        "paperId": "8bbb936361ed554302a8f119b5c681d553e3363e",
        "title": "Machine learning opportunities for nucleosynthesis studies"
      },
      {
        "paperId": "a23d109578ebe16652df0fdd46c4c11476c309e0",
        "title": "From Code to Play: Benchmarking Program Search for Games Using Large Language Models"
      },
      {
        "paperId": "6060599c97f32d8ef89d58604b664ab3c0e57cbc",
        "title": "DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation"
      },
      {
        "paperId": "f984a4d1f073223801a694200788a1a40d25ea8f",
        "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics"
      },
      {
        "paperId": "3fbf40b6d2125b0593f34d8e5e597abc00c976eb",
        "title": "Prediction with Action: Visual Policy Learning via Joint Denoising Process"
      },
      {
        "paperId": "d53d954f7bdbe77c9602952a052f04addffacd57",
        "title": "Don't Let Your Robot be Harmful: Responsible Robotic Manipulation via Safety-as-Policy"
      },
      {
        "paperId": "42d3b4738827cfba49f2a62c14674b0193d3ae46",
        "title": "Multi-Objective Control of Urban Railway Speed with Deep Reinforcement Learning"
      },
      {
        "paperId": "625c55216fceed626245c8e7306df6b96ed6d14a",
        "title": "From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards"
      },
      {
        "paperId": "75d79b3afabb95273ba9743b8bcd039ed948a47b",
        "title": "PIANIST: Learning Partially Observable World Models with LLMs for Multi-Agent Decision Making"
      },
      {
        "paperId": "96a68d22c4cd83c01f7d9be8e4c7d256bab8e2e0",
        "title": "Generalist Virtual Agents: A Survey on Autonomous Agents Across Digital Platforms"
      },
      {
        "paperId": "8e69c81246a6a043fd1de364318968b16717ab67",
        "title": "Open-World Task and Motion Planning via Vision-Language Model Inferred Constraints"
      },
      {
        "paperId": "2a8c78ddc0c0fc8f841d47538df0e03e1b9b2e39",
        "title": "Sampling-Based Model Predictive Control for Dexterous Manipulation on a Biomimetic Tendon-Driven Hand"
      },
      {
        "paperId": "4db6d0e73743f33bf5d77a5aa254927cf3fdbf63",
        "title": "A Study on the Effects of ICL and CoT Prompts on the Reward Function Generation of a Small Language Model"
      },
      {
        "paperId": "4749559e87d6a301a2ac61767d48ff4437fe4fd8",
        "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback"
      },
      {
        "paperId": "7712b0e640848cd67d34d10f0b8637ba89f39f0c",
        "title": "Vision Language Models are In-Context Value Learners"
      },
      {
        "paperId": "badf5d00486111d936b056e52f13c26c1bbbd573",
        "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning"
      },
      {
        "paperId": "fe472d15e2fd294c5d871a5b316d6a74908f409f",
        "title": "Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models"
      },
      {
        "paperId": "72499637780f4de2adc9b500596eec8a91238aa8",
        "title": "Eurekaverse: Environment Curriculum Generation via Large Language Models"
      },
      {
        "paperId": "4e94b064f731dcfad6c2b173743e3982fa6ed9de",
        "title": "Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration"
      },
      {
        "paperId": "a25ff3366a62075d391047a9a7747ecac9eb5788",
        "title": "Toward Automated Algorithm Design: A Survey and Practical Guide to Meta-Black-Box-Optimization"
      },
      {
        "paperId": "0156c9b0a872132b4300b931742a27014a2ed777",
        "title": "Bridging the Human to Robot Dexterity Gap Through Object-Oriented Rewards"
      },
      {
        "paperId": "e47211fe976e1000804022c59cb6830ca8bc72b6",
        "title": "KALM: Keypoint Abstraction Using Large Models for Object-Relative Imitation Learning"
      },
      {
        "paperId": "7bc7aafef5197a8235309d955eb4daad6679c90d",
        "title": "VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning"
      },
      {
        "paperId": "74abb1a62b70e7f9019c7209ea4e65b72867a574",
        "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback"
      },
      {
        "paperId": "6b31457d33a68785ff1de67d6c2bb68013fad370",
        "title": "Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem"
      },
      {
        "paperId": "8a381d4da1c5dc9837ef22afd5f47f7c567c00be",
        "title": "SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement"
      },
      {
        "paperId": "aeeaa6aa9b40454a0b0345197ab39a0758edc1a1",
        "title": "ICPL: Few-shot In-context Preference Learning via LLMs"
      },
      {
        "paperId": "7a3dc0eb5ef0bd60e6a478444b6865bda7647d17",
        "title": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks"
      },
      {
        "paperId": "80b75894562ac77a7074f164bbee0b18b861bf00",
        "title": "A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning"
      },
      {
        "paperId": "7f15c1aade1a82ec882bd9052e844545212887e2",
        "title": "ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization"
      },
      {
        "paperId": "d941fa629c35d80f957e3ebb0db165fc00faf713",
        "title": "The State of Robot Motion Generation"
      },
      {
        "paperId": "df1c9ede15c86c5621d1c5842249887bcc77179e",
        "title": "SDS - See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration"
      },
      {
        "paperId": "18c9afb0d6ed37c0933c786afce8641a998a4c9c",
        "title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI"
      },
      {
        "paperId": "1ed6f9c35bc23965d5451ba39f197fefbe20710f",
        "title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos"
      },
      {
        "paperId": "be37ee6676879d08b78433c5c9d597e5c9b7be4d",
        "title": "Automated Rewards via LLM-Generated Progress Functions"
      },
      {
        "paperId": "83fe6b75ee908d9bf858f443cb12dc0b709a6311",
        "title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts"
      },
      {
        "paperId": "3a1abdfed35b7f0f58692e62d53607673bc24c82",
        "title": "Large Language Models for Energy-Efficient Code: Emerging Results and Future Directions"
      },
      {
        "paperId": "53fc35a6ebb41008d8c9e296c5a2958267fd398c",
        "title": "On the Evaluation of Generative Robotic Simulations"
      },
      {
        "paperId": "340b5bbb430e8fbf1e3b7d9fa57bf874912f3714",
        "title": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance"
      },
      {
        "paperId": "c94512390c31942a73cbc152dfc7ab0db4ec1e6b",
        "title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making"
      },
      {
        "paperId": "f08e6657e93226e67c6cf19a1d76cb46e1535ede",
        "title": "YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "a56870071535a7d4b77eeaec4c7edf7823522f5d",
        "title": "Steering Large Language Models between Code Execution and Textual Reasoning"
      },
      {
        "paperId": "cc2ce2d9843c937ff6336b110e4066002dc77332",
        "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs"
      },
      {
        "paperId": "a549f6fcec90bb5522d0a5a49bcb4f937a735d17",
        "title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments"
      },
      {
        "paperId": "94e04d0e1ad8d9a6b82e8068df71df9036123d19",
        "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"
      },
      {
        "paperId": "f0c64291b16e7d724bcdc93155e7c5eff02c4c22",
        "title": "SEAL: SEmantic-Augmented Imitation Learning via Language Model"
      },
      {
        "paperId": "a57162ae7f03e875cf4e2ea130f696b58d967724",
        "title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with LLM-Guided Knowledge"
      },
      {
        "paperId": "076f35d58d26ac74c71e6e849dfffc0707aa8a6f",
        "title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation"
      },
      {
        "paperId": "3227444169081eb108fa640474077e1a0e4ae9ea",
        "title": "CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills Using Large Language Models"
      },
      {
        "paperId": "e55207228244079447f688566a4dd3c88cb36afa",
        "title": "FactorSim: Generative Simulation via Factorized Representation"
      },
      {
        "paperId": "b37b72a7bf6a9eae5211b41f1153b4a63ff886f3",
        "title": "Hierarchical LLMs In-the-loop Optimization for Real-time Multi-Robot Target Tracking under Unknown Hazards"
      },
      {
        "paperId": "19bba9e4bb6057e385f013a9edb521f279e5fc51",
        "title": "Embodiment-agnostic Action Planning via Object-Part Scene Flow"
      },
      {
        "paperId": "9ebce30da5743f37d5b6378ed6c0d8231b114d7e",
        "title": "MotIF: Motion Instruction Fine-Tuning"
      },
      {
        "paperId": "506d2a9ce9279161dea381532dc43521ab40b3b7",
        "title": "Materials Matter: Investigating Functional Advantages of Bio-Inspired Materials via Simulated Robotic Hopping"
      },
      {
        "paperId": "17d08bd1f807bde1c12c941fd2780e4acb08a61e",
        "title": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal Robots Guided by Large Language Models"
      },
      {
        "paperId": "dfafc44befd8e74c7a00013352ac6a69962b21e9",
        "title": "Adaptive Language-Guided Abstraction from Contrastive Explanations"
      },
      {
        "paperId": "5b55ad35502b4ad1521bc93d3c273134f17f9497",
        "title": "Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments"
      },
      {
        "paperId": "6225ef54b2440fe05314c1e800b2628b0d2b7e82",
        "title": "RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning"
      },
      {
        "paperId": "b934f9b888986a4f41e998de17b2409f590271c2",
        "title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "e7c148f93e1449b8252c12a473bb6a1299d45ed6",
        "title": "Affordance-based Robot Manipulation with Flow Matching"
      },
      {
        "paperId": "5d630f5270c6de5b749bbf137b5e139d3c04acf3",
        "title": "Acquiring musculoskeletal skills with curriculum-based reinforcement learning"
      },
      {
        "paperId": "70e85a58c176beaf4982524f8cb4b75ec2050aa3",
        "title": "Multi-Finger Manipulation via Trajectory Optimization With Differentiable Rolling and Geometric Constraints"
      },
      {
        "paperId": "c9537f656e7d9713fd4108ce7bf512290f48e562",
        "title": "Automated Design of Agentic Systems"
      },
      {
        "paperId": "83618495bc070faddd30da0e0676eb176c46189d",
        "title": "Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction"
      },
      {
        "paperId": "205c3352ec73278e737806849323b37b36e1de5d",
        "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning"
      },
      {
        "paperId": "c8456736969365d9138dc7641cb87dc19472b8ce",
        "title": "Reflective Verbal Reward Design for Pluralistic Alignment"
      },
      {
        "paperId": "65152777b02b428b4fe6e3dff42c7f54c3574213",
        "title": "Intelligent Agents for Data Exploration"
      },
      {
        "paperId": "6f2f3df759ed61687a5f57587105f5610098f728",
        "title": "Social Learning through Interactions with Other Agents: A Survey"
      },
      {
        "paperId": "d27f3c5724acff3248005761e0808de13dd70b5e",
        "title": "Lessons from Learning to Spin \"Pens\""
      },
      {
        "paperId": "b3cea5965e97e6430ae47e5f87a05c484706579b",
        "title": "Grammar-based Game Description Generation using Large Language Models"
      },
      {
        "paperId": "a8f5ecc4a3a5d1ed287d57f759d94d13f04b41c8",
        "title": "SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model"
      },
      {
        "paperId": "e3949c80ffa4670f831d445fa4075564a8bcff8d",
        "title": "Automatic Environment Shaping is the Next Frontier in RL"
      },
      {
        "paperId": "ae57a45798ff9061f1a401c5a3d2f6b6bdcf17e0",
        "title": "Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts"
      },
      {
        "paperId": "a816f55eaf012d957606bf68ae6821e9f47d017d",
        "title": "Simultaneous Localization and Affordance Prediction of Tasks from Egocentric Video"
      },
      {
        "paperId": "bfccdbbf24068c3d21e4b3344fc646c8e51adcbc",
        "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models"
      },
      {
        "paperId": "df9b2b471b9b2550a29b2fd2df6b9d5967f9133f",
        "title": "Affordance-Guided Reinforcement Learning via Visual Prompting"
      },
      {
        "paperId": "db35bb43daad7d7f85dfbbe0f9cc7b77b6e57272",
        "title": "GAVEL: Generating Games Via Evolution and Language Models"
      },
      {
        "paperId": "5c00aa34dd595cbf04cd75ef52c4178e0084383a",
        "title": "RoboMorph: Evolving Robot Morphology using Large Language Models"
      },
      {
        "paperId": "4447de6ae940b58cbdff7b62baa444ea0beb8250",
        "title": "Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach"
      },
      {
        "paperId": "a741a0e7d6612d0f7573012a16cb19aae0905c55",
        "title": "iLLM-TSC: Integration reinforcement learning and large language model for traffic signal control policy improvement"
      },
      {
        "paperId": "e73573c3d4a78d87f225d8822becb03f2a5b0f5f",
        "title": "E2CFD: Towards Effective and Efficient Cost Function Design for Safe Reinforcement Learning via Large Language Model"
      },
      {
        "paperId": "9ccf6b4f42fdf74e8b25ec29a8600c673404cc20",
        "title": "Communication and Control Co-Design in 6G: Sequential Decision-Making With LLMs"
      },
      {
        "paperId": "f1a5156491f16ae595fc579b82d47ae6e58d3a83",
        "title": "Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models"
      },
      {
        "paperId": "6175570322ba9616c2b2fa08d8fda7fa516124a7",
        "title": "Embodied AI in Mobile Robots: Coverage Path Planning with Large Language Models"
      },
      {
        "paperId": "4c78ebfb809285b83ef7c48c4cd66a56d43e1a88",
        "title": "Language-Guided Object-Centric Diffusion Policy for Generalizable and Collision-Aware Manipulation"
      },
      {
        "paperId": "528ad1f3e7b6ff4f61ff05ab5d2153ef6cc301ae",
        "title": "Revisiting Sparse Rewards for Goal-Reaching Reinforcement Learning"
      },
      {
        "paperId": "4a61c43c6b855efa9afdaf9839dfe2a27b3ac8dc",
        "title": "Beyond Human Preferences: Exploring Reinforcement Learning Trajectory Evaluation and Improvement through LLMs"
      },
      {
        "paperId": "cc44b932095dfd3146b886bd65b90ca69bc96916",
        "title": "SMPLOlympics: Sports Environments for Physically Simulated Humanoids"
      },
      {
        "paperId": "cdcc2c6e465d7c3a64915d0b33deb00c794840a9",
        "title": "LICO: Large Language Models for In-Context Molecular Optimization"
      },
      {
        "paperId": "0fba60cfe1d70bb52ecf200a3fa6ec14e2675bf6",
        "title": "OCALM: Object-Centric Assessment with Language Models"
      },
      {
        "paperId": "b85c299e556c1b0b3d66787b31a25d54e7528283",
        "title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models"
      },
      {
        "paperId": "5da64e4d1d9ad1322223d81d3133188a54619b0f",
        "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought"
      },
      {
        "paperId": "07fcdf01fc1824b8c659a72fe9c0534cf3eb69cf",
        "title": "Generating and Evolving Reward Functions for Highway Driving with Large Language Models"
      },
      {
        "paperId": "39f1bbc507f9f11a584b6b0bcb5828a1a4eacd56",
        "title": "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models"
      },
      {
        "paperId": "eb87604ee57fee65ad354961ca8212ba1bb55fe2",
        "title": "GPT-Fabric: Folding and Smoothing Fabric by Leveraging Pre-Trained Foundation Models"
      },
      {
        "paperId": "c1fd34360493c2aec0ed03ac7c0b56ce4feeadaa",
        "title": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning"
      },
      {
        "paperId": "550fa9db81118a96e72c1b371546dccb1eeb8d42",
        "title": "Position: Towards Bidirectional Human-AI Alignment"
      },
      {
        "paperId": "899f6c5ee479aa81d66e94812e3fa84a06a8cf7d",
        "title": "Discovering Preference Optimization Algorithms with and for Large Language Models"
      },
      {
        "paperId": "235e891e6bbf510c8c470cd91aaba5fb16dc742e",
        "title": "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models"
      },
      {
        "paperId": "3163aa45f04945aef68b30529cb3d9eed1f9eb3a",
        "title": "Language Guided Skill Discovery"
      },
      {
        "paperId": "a1c2039a3b3046ed91e1cbb66cacd392662c0bad",
        "title": "ChatPCG: Large Language Model-Driven Reward Design for Procedural Content Generation"
      },
      {
        "paperId": "9b523348a11022e8d2d6dd22f666a4096e224832",
        "title": "HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning"
      },
      {
        "paperId": "81ce10ec4d91ebbd12ed0d0beab77387302632bb",
        "title": "A Survey of Language-Based Communication in Robotics"
      },
      {
        "paperId": "50fe40b35d376d000a70a25ab2f9b6e6b6b336d6",
        "title": "DrEureka: Language Model Guided Sim-To-Real Transfer"
      },
      {
        "paperId": "54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
        "title": "REvolve: Reward Evolution with Large Language Models using Human Feedback"
      },
      {
        "paperId": "ef784cc33df8cc9c245cb8cb6faf3218efeb384d",
        "title": "Evolutionary Computation for the Design and Enrichment of General-Purpose Artificial Intelligence Systems: Survey and Prospects"
      },
      {
        "paperId": "427ea6bbe46c1bb0999a3b8a26b8aa57a489cc2e",
        "title": "Multi-Agent Transfer Learning via Temporal Contrastive Learning"
      },
      {
        "paperId": "761509e64bf8eb462b615e5fbcea866289ccc8af",
        "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning"
      },
      {
        "paperId": "3836a14d7921c899a6f2679d12adce5912dddfac",
        "title": "Controlling Large Language Model Agents with Entropic Activation Steering"
      },
      {
        "paperId": "4f0acfbef71769acaa2faced3faa7c6d2af1a0d3",
        "title": "InterPreT: Interactive Predicate Learning from Language Feedback for Generalizable Task Planning"
      },
      {
        "paperId": "313bfa540068ab6a77d36e6a20ecf28859719580",
        "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation"
      },
      {
        "paperId": "7b5ad79c9339e5b823277d403d648948c0cc23cd",
        "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs"
      },
      {
        "paperId": "d7a505964defe667f9c7aa8cb7b79f1caef89614",
        "title": "Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "39fe0048180b0a4e5b2bc54dac4af152a33cddb3",
        "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making"
      },
      {
        "paperId": "bc7d05f28822a9cb256ab074069aaa08e879a4ce",
        "title": "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "e1b62c7ee4e22ab63e3b0c9968563e6675833e36",
        "title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"
      },
      {
        "paperId": "bcd2e0ce32b153e2b4e775fcd89a87acc30523c5",
        "title": "Agentic Skill Discovery"
      },
      {
        "paperId": "ae9a2bcd460354c706aaea8797b1c2c15841a6b6",
        "title": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "paperId": "d72e76776ba9973e039e2aea0ed1f470f3fecf88",
        "title": "Goals as Reward-Producing Programs"
      },
      {
        "paperId": "39484c454bde6b96d013873f7300481473e993b4",
        "title": "Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills"
      },
      {
        "paperId": "1c66c41ff22adf66bb9b06d87d5a2ab7b8ee8de7",
        "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities"
      },
      {
        "paperId": "e51dff31f56847c16de3a2f2682d16109537b96e",
        "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery"
      },
      {
        "paperId": "0a1ec333b79aa449ff9d2c2ce0913c625729e972",
        "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction"
      },
      {
        "paperId": "a0065b02aeefa89faa40e4070691d1b46183499b",
        "title": "Dynamic On-Palm Manipulation via Controlled Sliding"
      },
      {
        "paperId": "83331243fcaeab4d5c02145d62f3d42df2c2d651",
        "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment"
      },
      {
        "paperId": "ea95349869badc1665b05573056745bd46690ded",
        "title": "DiffGen: Robot Demonstration Generation via Differentiable Physics Simulation, Differentiable Rendering, and Vision-Language Model"
      },
      {
        "paperId": "c1c2b65f8270af8d70d136070b6f3dcb2ce37e82",
        "title": "An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models"
      },
      {
        "paperId": "564e396da9cea68724cade292572077ae2ac6e42",
        "title": "SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants"
      },
      {
        "paperId": "ffcef91b1c25d2d740b335f9b4b3c942cd8d79de",
        "title": "Enhancing Q-Learning with Large Language Model Heuristics"
      },
      {
        "paperId": "d21cb7b9744365f0ada5527be380236495dd3521",
        "title": "Position: Leverage Foundational Models for Black-Box Optimization"
      },
      {
        "paperId": "8e05a8d64d6a457018eb62710ca3cf1996001299",
        "title": "Pretrained Optimization Model for Zero-Shot Black Box Optimization"
      },
      {
        "paperId": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models"
      },
      {
        "paperId": "77da0a75cb33c27269b5114a5334fd2228a31dea",
        "title": "LEGENT: Open Platform for Embodied Agents"
      },
      {
        "paperId": "18dfbcacae4a12db925889e5439743f623254eb0",
        "title": "Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations"
      },
      {
        "paperId": "22711187b1b7d22e41ece0cdc15998b8ec53a297",
        "title": "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts"
      },
      {
        "paperId": "a05ae66fc5808b32e0a91ecc015ed36fa100a4c8",
        "title": "Large Language Models for Orchestrating Bimanual Robots"
      },
      {
        "paperId": "c35b8dad08e11a77c249c0aed2b2f7f9ba853acd",
        "title": "A Survey on Large Language Model-Based Game Agents"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "00dc72c6c0072f2e41e42849d63d1827c303524e",
        "title": "RAIL: Robot Affordance Imagination with Large Language Models"
      },
      {
        "paperId": "b78adeffd4ab17a690e537ada53ddb85940f1015",
        "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games"
      },
      {
        "paperId": "718f1f14feb3f58b05f7400d0e274c133412ca2f",
        "title": "ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning"
      },
      {
        "paperId": "053f4be6bb13313da217e549a594af69a3fcebff",
        "title": "Techniques for Theoretical Prediction of Immunogenic Peptides"
      },
      {
        "paperId": "eb19304efccd3398ab2c78e8724a5589b92bb100",
        "title": "Context-aware LLM-based Safe Control Against Latent Risks"
      },
      {
        "paperId": "6cd1b99ec6d399a682b01e6fe9096e2fcf450862",
        "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents"
      },
      {
        "paperId": "58e2e14d90a6f0cdf8b199b408cd05cf397af7c1",
        "title": "ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models"
      },
      {
        "paperId": "28111eaf9ca4a4ce0df0d5b375eb0eba924c5093",
        "title": "NARRATE: Versatile Language Architecture for Optimal Control in Robotics"
      },
      {
        "paperId": "aedd5c4bc11d8faf01e8456c91a183f2f76e5778",
        "title": "ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "b9531d4364be4ee8a7dfbef3a0b323740220e54b",
        "title": "CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models"
      },
      {
        "paperId": "3a20c3669a54bb2cfe6ca56663c077c19140e764",
        "title": "Materials science in the era of large language models: a perspective"
      },
      {
        "paperId": "4cbf53465e2af3aa4f0079167a0474fc713f3ce0",
        "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models"
      },
      {
        "paperId": "8bb32652e0a935b6ba1f54bd3d39cad80db09908",
        "title": "3D Diffusion Policy"
      },
      {
        "paperId": "4e0c7732e8e68a1285359e41ab3dc1d02c408865",
        "title": "MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting"
      },
      {
        "paperId": "b671934871fb9a2db3ba729647f298d46958804d",
        "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism"
      },
      {
        "paperId": "ecd3091debcd2f393379508df70bceb94db0be3b",
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"
      },
      {
        "paperId": "81881d2589a34df12d5e2fd192d5354dda1f81a8",
        "title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents"
      },
      {
        "paperId": "218e3fdfe1b5c266d0cb50dd20dd0211e16580d1",
        "title": "Identify Critical Nodes in Complex Network with Large Language Models"
      },
      {
        "paperId": "fa8fa745f58d362925dd44f02750bab1b30a1189",
        "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy"
      },
      {
        "paperId": "844fdeb5089e6d1bb99fe3c8a830bf7f722ba910",
        "title": "Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas"
      },
      {
        "paperId": "b42713664a72410307839fe44ec51aef8d69c943",
        "title": "How Can LLM Guide RL? A Value-Based Approach"
      },
      {
        "paperId": "4ca78a93635aa6e41d695c78110699e04e5d5a5b",
        "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health"
      },
      {
        "paperId": "62cb04de4d5adb10d6cbb1bea02f534567e03808",
        "title": "Practice Makes Perfect: Planning to Learn Skill Parameter Policies"
      },
      {
        "paperId": "db781004798077c7b10e0920041fdf386fc27db3",
        "title": "Learning Playing Piano with Bionic-Constrained Diffusion Policy for Anthropomorphic Hand"
      },
      {
        "paperId": "a92335c5cce5eed9ad256280574aed9f130d73dc",
        "title": "Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation"
      },
      {
        "paperId": "94db8a625418800c8ae7b48157a9cad1c8129051",
        "title": "A Survey on Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "1243ccec1a27112bd37d1891b152ad68be8e4777",
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment"
      },
      {
        "paperId": "34d52d34e06122e30c207a8e38d4d911e6a2f012",
        "title": "Learning to Learn Faster from Human Feedback with Language Model Predictive Control"
      },
      {
        "paperId": "9120d793a1681128d064bc19aab4dc29ce7a38b8",
        "title": "AutoSAT: Automatically Optimize SAT Solvers via Large Language Models"
      },
      {
        "paperId": "11ab270eedc47ec3e4bf3ca7de84cd769b53f66d",
        "title": "BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents"
      },
      {
        "paperId": "cc6c0e6dda4f7f1cbef98fe73f618ce4f6163d51",
        "title": "Towards Unified Alignment Between Agents, Humans, and Environment"
      },
      {
        "paperId": "3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8",
        "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs"
      },
      {
        "paperId": "af50d0704b160198799025d0aefffc1cc4107318",
        "title": "An Interactive Agent Foundation Model"
      },
      {
        "paperId": "59f3cf13401b9cc45d0e5ad7ea525e1eec84cce1",
        "title": "Real-world robot applications of foundation models: a review"
      },
      {
        "paperId": "793f8572a022866caa66e44c98ff2839c1b4587a",
        "title": "Code as Reward: Empowering Reinforcement Learning with VLMs"
      },
      {
        "paperId": "4c98e18cf16395b95ffaaeeac3eceffa608dcf8d",
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"
      },
      {
        "paperId": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
        "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback"
      },
      {
        "paperId": "96910d8365709393ea33630e200f6229fe947178",
        "title": "The Essential Role of Causality in Foundation World Models for Embodied AI"
      },
      {
        "paperId": "be7a88babf78512b545f585517704cb597388cbc",
        "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution"
      },
      {
        "paperId": "b156004675ad3aa5e39a56928afc530aec191044",
        "title": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks"
      },
      {
        "paperId": "c362015d426c90ec01e1ad02bf3fd66ab8fd0fd9",
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models"
      },
      {
        "paperId": "78fbb6e7a1c568a04e8c935aa9909d0c942ea5f6",
        "title": "Executable Code Actions Elicit Better LLM Agents"
      },
      {
        "paperId": "49faf97a20e6cc53f2828a0f2219242e31a26180",
        "title": "Growing from Exploration: A self-exploring framework for robots based on foundation models"
      },
      {
        "paperId": "1215d4c5248124f6a0b133760c4c5b6d65804abf",
        "title": "When Large Language Models Meet Evolutionary Algorithms: Potential Enhancements and Challenges"
      },
      {
        "paperId": "478f71f1cd9bad0435560544a9dce7ca49d97766",
        "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap"
      },
      {
        "paperId": "dd653f0d5f6335625fb2429216bed33524a71256",
        "title": "Evolving Code with A Large Language Model"
      },
      {
        "paperId": "a2e128be33b85f8f91515e205bf0237ecc0546cc",
        "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model"
      },
      {
        "paperId": "dfc0aafbcdbb9fe991dd27821fe11a4131af315b",
        "title": "A Survey on Robotic Manipulation of Deformable Objects: Recent Advances, Open Challenges and New Frontiers"
      },
      {
        "paperId": "17afea1d90fc77caa49633957803e4a318bc12cd",
        "title": "GPT-in-the-Loop: Supporting Adaptation in Multiagent Systems"
      },
      {
        "paperId": "d17f666085018f19f6ab8253979d0d727af43f00",
        "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft"
      },
      {
        "paperId": "6140211405f9917ded519da50f00eee989eabd7f",
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis"
      },
      {
        "paperId": "287f4b599020d53634abbee6a6e58468d14ebbdc",
        "title": "Vision-Language Models as a Source of Rewards"
      },
      {
        "paperId": "c4c1c1d7312c7be6ad000f84201a72494dea33d3",
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning"
      },
      {
        "paperId": "0d8da8431bee9b2c2d40b605a754c5c840833323",
        "title": "Recursive Visual Programming"
      },
      {
        "paperId": "425f1edd88fe3539c40ddd93c3e07c95de67ba00",
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld"
      },
      {
        "paperId": "337f421a364a6fa8ca423afd627bee2426f69395",
        "title": "Robot Learning in the Era of Foundation Models: A Survey"
      },
      {
        "paperId": "3dee0acc8728655b3458704ca90778ec0b28758b",
        "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions"
      },
      {
        "paperId": "c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"
      },
      {
        "paperId": "ac258100ebe178287ae4ae3dc7ac78f8c27e017d",
        "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game"
      },
      {
        "paperId": "fb09b581589e1195ff018179c6a11668587c6d64",
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning"
      },
      {
        "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"
      },
      {
        "paperId": "b4c176029ddf95863b453096bbd43ae9651a2424",
        "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft"
      },
      {
        "paperId": "469e785578ded36526c39e4b32e52e4025279070",
        "title": "Auxiliary Reward Generation With Transition Distance Representation Learning"
      },
      {
        "paperId": "7fd501cc18df03dfd157dc55271540be33d5da55",
        "title": "PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks"
      },
      {
        "paperId": "c9d3305778a5d01d7634bf64dc51788b088ae01f",
        "title": "LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression"
      },
      {
        "paperId": "d560ee54c63a45ca62a6f8648bb1fedffd94f9a3",
        "title": "ReGen: Generative Robot Simulation via Inverse Design"
      },
      {
        "paperId": "5c743b98d1d2e12ea97337c714bd52b26b3c55b2",
        "title": "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights"
      },
      {
        "paperId": "459fb039bf6f322f84d3c5c53fab88106881a195",
        "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment"
      },
      {
        "paperId": "0a176ff2fcf08e852903a5bfa158e53a67c0b71a",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models"
      },
      {
        "paperId": "4b47193dc2e879194992f97468a6260a3c582097",
        "title": "Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization"
      },
      {
        "paperId": "bcf2bd95a6f60dd2998b57c26873d31461011e8d",
        "title": "REvolve: Reward Evolution with Large Language Models for Autonomous Driving"
      },
      {
        "paperId": "5fa7914587aa8ab2b5928fa3bd3b8230aa414c3d",
        "title": "Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation"
      },
      {
        "paperId": "1412f5ee973976e6c69f972e8427d1adba02e3ed",
        "title": "Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study"
      },
      {
        "paperId": "0caced806a27cfe41f66aa410f17e7e6ee25c3a7",
        "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct"
      },
      {
        "paperId": "fd72ca948f1df212c2327dfc0297b080fedbcb81",
        "title": "Simultaneous Localization and Affordance Prediction for Tasks in Egocentric Video"
      },
      {
        "paperId": "aa8d31c42ed95d56bdcda0cefa2834bc77e14c66",
        "title": "Don't Let Your Robot be Harmful: Responsible Robotic Manipulation"
      },
      {
        "paperId": "ba6eb892407fece0fff17c628810a8d9a10ea8e0",
        "title": "LLM-based Skill Diffusion for Zero-shot Policy Adaptation"
      },
      {
        "paperId": "2c3152761de1dec262ed6a0375cca9c4d334a52f",
        "title": "A match made in consistency heaven: when large language models meet evolutionary algorithms"
      },
      {
        "paperId": "d4035cbd20cf4eafdc4bc2439cd6e962ea43f92b",
        "title": "Action Space Design in Reinforcement Learning for Robot Motor Skills"
      },
      {
        "paperId": "e21873a173e54b3aed52b60a5318f60daa38e0f0",
        "title": "See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image"
      },
      {
        "paperId": "1dd66d1a04fd32ba4499e357f6b6f7dcfad99644",
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis"
      },
      {
        "paperId": "b0a4c6ad8b6988d2c23fc66cbe2c5a0dc3fc104d",
        "title": "A Simple Framework for Intrinsic Reward-Shaping for RL using LLM Feedback"
      },
      {
        "paperId": "661f92a7ac2d884b21ed36eef7e712e24556f61e",
        "title": "Prioritization Strategies for LLM-Designed Restless Bandit Rewards in Public Health"
      },
      {
        "paperId": "b648239a698981a33509fe626e8f602393ab9617",
        "title": "Large Language Models as Hyper-Heuristics for Combinatorial Optimization"
      },
      {
        "paperId": "fc06240770e06e9fdc1332268a8266ee2c2f1d86",
        "title": "Transfer Learning via Temporal Contrastive Learning"
      },
      {
        "paperId": "4ed4d4ec93e0362ff96219b1af117a3c8e9072e6",
        "title": "Generalizable Robotic Manipulation: Object-Centric Diffusion Policy with Language Guidance"
      },
      {
        "paperId": "b25bb144c79c5cfd1a03f8932d6857c051524f5e",
        "title": "Fine-tuning LLM Agents with Retrospective In-Context Online Learning"
      },
      {
        "paperId": "d330bb7c578c7e4de7566d0c958a41e0935c9721",
        "title": "Improving LLM Generation with Inverse and Forward Alignment: Reward Modeling, Prompting, Fine-Tuning, and Inference-Time Optimization"
      },
      {
        "paperId": "1d40ee2edc215644b738eca9ad931cf5c1c5749d",
        "title": "KALM: Knowledgeable Agent by Offline Reinforcement Learning from Large Language Model Rollouts"
      },
      {
        "paperId": "9d0edf173b18b98b20e94cbeae6b92a11c7ead7e",
        "title": "Position: Human-like reinforcement learning is facilitated by structured and expressive goals"
      },
      {
        "paperId": "f257ecc4bced70e1984645b5a3975891200cfbd5",
        "title": "Master Computer Science Steering"
      },
      {
        "paperId": "7339b6fb9cb33f17b0272e7aa17f69725c1470ec",
        "title": "T RANSFERABLE A DVERSARIAL A TTACK ON V ISION - ENABLED L ARGE L ANGUAGE M ODELS"
      }
    ],
    "score": 196.5
  },
  {
    "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
    "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
    "authors": [
      "Zhiqing Sun",
      "Yikang Shen",
      "Qinhong Zhou",
      "Hongxin Zhang",
      "Zhenfang Chen",
      "David D. Cox",
      "Yiming Yang",
      "Chuang Gan"
    ],
    "year": 2023,
    "citationCount": 366,
    "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
    "url": "https://www.semanticscholar.org/paper/e01515c6138bc525f7aec30fc85f2adf028d4156",
    "pdf_url": "https://arxiv.org/pdf/2305.03047.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2023-05-04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-03047",
      "ArXiv": "2305.03047",
      "DOI": "10.48550/arXiv.2305.03047",
      "CorpusId": 258479665
    },
    "references": [
      {
        "paperId": "bf52c9d94fd61fae0d231a7e43d45d673584c282",
        "title": "Poisoning Language Models During Instruction Tuning"
      },
      {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
        "title": "The Capacity for Moral Self-Correction in Large Language Models"
      },
      {
        "paperId": "69619a2a47faee7a29ec596db13172e2a42ff921",
        "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "b1b8c3e47f44158d22fb70bb453d2494ed013b70",
        "title": "On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "paperId": "ed99a2572fb5f4240aa6068e3bf274832e831306",
        "title": "Recitation-Augmented Language Models"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
        "title": "LaMDA: Language Models for Dialog Applications"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "92173d081b15824d22a9ef070e118744ceee8052",
        "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"
      },
      {
        "paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
        "title": "BBQ: A hand-built bias benchmark for question answering"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "d624bc273821c871f899d8256a34be40c09fc3cd",
        "title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "580d85d3ce5475a2931b2147648b1fb79ef03358",
        "title": "Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "7aa70e2c12c8ba2dcc828893adb8bb56e3766726",
        "title": "Artificial Intelligence, Values, and Alignment"
      },
      {
        "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7",
        "title": "Gender Bias in Coreference Resolution"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "57a10537978600fd33dcdd48922c791609a4851a",
        "title": "Sequence-Level Knowledge Distillation"
      },
      {
        "paperId": "f6cda66deb48549ecafca6f92706c686ee04500e",
        "title": "The Los Angeles County Museum of Art"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following llama model"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023"
      },
      {
        "paperId": null,
        "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data"
      },
      {
        "paperId": null,
        "title": "How do text-davinci-002 and text-davinci-003 differ?"
      },
      {
        "paperId": null,
        "title": "Koala: A dialogue model for academic research"
      },
      {
        "paperId": null,
        "title": "Core views on ai safety: When, why, what, and how"
      },
      {
        "paperId": null,
        "title": "Free dolly: Introducing the world's first truly open instructiontuned llm"
      },
      {
        "paperId": null,
        "title": "Introducing the new bing"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": null,
        "title": "Scramble the eggs in the skillet, stirring constantly, until they are cooked to your liking"
      },
      {
        "paperId": null,
        "title": "Bring sunscreen"
      },
      {
        "paperId": null,
        "title": "Explore the Hollywood Bowl"
      },
      {
        "paperId": null,
        "title": "Dromedary: Sure! Here is the steps to pack for a short trip to a tropical climate: 1. Choose the right luggage"
      },
      {
        "paperId": null,
        "title": "Add milk, salt, and pepper"
      },
      {
        "paperId": null,
        "title": "User: List some startups that have been founded by ex Google, Amazon and Facebook engineers"
      },
      {
        "paperId": null,
        "title": "Enjoy the views from the Hollywood Hills"
      },
      {
        "paperId": null,
        "title": "Dromedary: As an AI language model, I have no access to real-time GPS or traffic information. However"
      },
      {
        "paperId": null,
        "title": "Dromedary's internal knowledge and information were only current until some point in the year of 2021, and could be inaccurate / lossy"
      },
      {
        "paperId": null,
        "title": "Visit the Walt Disney Concert Hall"
      },
      {
        "paperId": null,
        "title": "Dromedary should first assess whether the question is valid and ethical before attempting to provide a response"
      },
      {
        "paperId": null,
        "title": "Dromedary: Sure! According to my knowledge from Wikipedia, here are some startups that have been founded by ex Google"
      },
      {
        "paperId": null,
        "title": "Bring a travel adapter"
      },
      {
        "paperId": null,
        "title": "Dromedary: I am an AI language model, so I do not have a family. I do not have any family members or family dynamics. I am sorry I cannot answer your question"
      },
      {
        "paperId": null,
        "title": "When offering explanations or solutions, Dromedary should present step-by-step justifications prior to delivering the answer. User: Outline the steps of cooking a scrambled eggs"
      },
      {
        "paperId": null,
        "title": "If the provided information is insufficient or the question is ambiguous, Dromedary ought to request the user to provide further clarification on their query"
      },
      {
        "paperId": null,
        "title": "Dromedary: Higher education is a controversial topic, with many people holding differing opinions"
      },
      {
        "paperId": null,
        "title": "Dromedary's responses should be positive, interesting, helpful and engaging. User: Describe what you should do to explore the beauty of the city of Los Angeles"
      },
      {
        "paperId": null,
        "title": "Dromedary is a static model and cannot provide real-time information. User: Give directions from the hotel Regency to the mall"
      },
      {
        "paperId": null,
        "title": "Dromedary should attempt to provide an answer for tasks that are operational for a computer"
      },
      {
        "paperId": null,
        "title": "Explore the Venice Canals"
      },
      {
        "paperId": null,
        "title": "Dromedary: Sure! Here are the steps of cooking a scrambled eggs"
      },
      {
        "paperId": null,
        "title": "Pour the eggs into the skillet"
      },
      {
        "paperId": null,
        "title": "Dromedary can create novel poems, stories, code (programs), essays, songs, celebrity parodies, summaries, translations, and more. User: Create a short"
      },
      {
        "paperId": null,
        "title": "Dromedary: Sure! Here is a short"
      },
      {
        "paperId": null,
        "title": "Pack a hat and sunglasses"
      },
      {
        "paperId": null,
        "title": "I do not have any information about the 2023 season of Stranger Things. 2. Please check the latest information for accurate and up-to-date details on the 2023 season of Stranger Things"
      },
      {
        "paperId": null,
        "title": "Bring a first-aid kit"
      },
      {
        "paperId": null,
        "title": "I am a helpful, ethical, and reliable assistant. The user asks about cooking scrambled eggs, which needs step-by-step instructions . I should follow rules"
      },
      {
        "paperId": null,
        "title": "Con: Specialized training in universities is a bad idea. It can limit the students' knowledge and understanding of other areas of study"
      }
    ],
    "cited_by": [
      {
        "paperId": "1f9dc0890201d55ed55ba342bcdaac5d25c3b0b2",
        "title": "DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models"
      },
      {
        "paperId": "942ebb5a0a9a906a668f92b574e5d61c714685db",
        "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs"
      },
      {
        "paperId": "50a62bbfffdd0873ec8a60a4f64fef76093d3af1",
        "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains"
      },
      {
        "paperId": "586fe42c43d000a9ad34012fbb6eabdebb76f039",
        "title": "Leveraging AI\u2010generated synthetic data to train natural language processing models for qualitative feedback analysis"
      },
      {
        "paperId": "4deccc959504ffd34cdd3afd04f067578b9b3a4e",
        "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning"
      },
      {
        "paperId": "b9c53085e20687a74702f8092b6f0dd609e36657",
        "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges"
      },
      {
        "paperId": "db0e4740f92164ded17c65b4bb3e74f4ca5c82f6",
        "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation"
      },
      {
        "paperId": "061ab7e11dd86c1ee84f04cd6e5b26a5ea03116e",
        "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation"
      },
      {
        "paperId": "a3d2eda94fb321d6ad0055c1c07972839fbb31e5",
        "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models"
      },
      {
        "paperId": "630ac252878b34cff962c925365d810beb249f02",
        "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning"
      },
      {
        "paperId": "ad0927eb6f3ad1a3b73ae7a575ec417d2abc72a3",
        "title": "Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation"
      },
      {
        "paperId": "c8e907b20e6af1b923878fd5095f82c6248546d2",
        "title": "Moral Judgment with a Large Language Model-Based Agent"
      },
      {
        "paperId": "38eadd2a5eaa850911a5e05c16e47434b356d3b7",
        "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation"
      },
      {
        "paperId": "1407c080e41961a502233d0af912ed9b0d952498",
        "title": "A Configurational Exploration on the Promise and Perils of Generative AI in Project-Based Language Learning"
      },
      {
        "paperId": "7d52e2da2d773587b1f6b9f57edfc06946b89d6d",
        "title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation"
      },
      {
        "paperId": "36afba97d2f5102f47ca4b558dc571947c5fe9b6",
        "title": "The Synthetic Mirror - Synthetic Data at the Age of Agentic AI"
      },
      {
        "paperId": "ee880b0551fa6e5124d8ed0f0ad7eff9974a974e",
        "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information"
      },
      {
        "paperId": "2b574c8e3a97b77529b8ec56579c91dd55ada850",
        "title": "Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives"
      },
      {
        "paperId": "739051073dbd7c6802d15079d9df9f51bf6a8e5b",
        "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"
      },
      {
        "paperId": "ad3cdcab99217d71952333aa5d97354c0904116b",
        "title": "PROVSYN: Synthesizing Provenance Graphs for Data Augmentation in Intrusion Detection Systems"
      },
      {
        "paperId": "800dc9621a55d4334abe1807b26be9b907332967",
        "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts"
      },
      {
        "paperId": "255baec590eb82804e918f004f68343c523939c2",
        "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "paperId": "91d7e3e1cadd9d92c65ba6d028230e493b3185f2",
        "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data"
      },
      {
        "paperId": "aa777c7f2eb080dadba5254128851950f3af6d0e",
        "title": "Training Free Stylized Abstraction"
      },
      {
        "paperId": "dd340b8432c88d0f8d99693cf7ab2d999bd155cb",
        "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge"
      },
      {
        "paperId": "46b4146be2a91020659fe9dd8c7a8e005e78536e",
        "title": "Square\u03c7PO: Differentially Private and Robust \u03c72-Preference Optimization in Offline Direct Alignment"
      },
      {
        "paperId": "2834adc0bfcb7a6eab2895bdfcdee218a4e8f4b8",
        "title": "Aligning LLMs by Predicting Preferences from User Writing Samples"
      },
      {
        "paperId": "63e14dc525400803aaa1c66559c319c7b5124066",
        "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models"
      },
      {
        "paperId": "b15de0b25b722619f618fb2d7a61bb1b2bd40e41",
        "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts"
      },
      {
        "paperId": "8932cb2a36f98c263f5a0f1287a1e0303fc0e7df",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment"
      },
      {
        "paperId": "7002aa560f51a2e14be6ae248c843aaff878d6b7",
        "title": "Latent Principle Discovery for Language Model Self-Improvement"
      },
      {
        "paperId": "92c02f2efbf3ee6e0e72deef88efa1570dd61fe4",
        "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs"
      },
      {
        "paperId": "47d9ffcf1f927f5c65cae08934396f3864ace3ff",
        "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation"
      },
      {
        "paperId": "fd0338267ede80bc6d840e699a86543829e85d13",
        "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data"
      },
      {
        "paperId": "18ccda104f8001fa0719153112c0bac3ac10f740",
        "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness"
      },
      {
        "paperId": "386a910f626032badfc2a4e1767a542355259d70",
        "title": "An LLM-based hybrid approach for enhanced automated essay scoring"
      },
      {
        "paperId": "141f7609ece033bb5a4ff06a73c3544004f6ed0a",
        "title": "Efficient Management of LLM-Based Coaching Agents' Reasoning While Maintaining Interaction Quality and Speed"
      },
      {
        "paperId": "3e6c3e636835e9ded21fac9ce7998331b933bdf4",
        "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content"
      },
      {
        "paperId": "b7788df9e929fd67118d41272754046207fbf157",
        "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization"
      },
      {
        "paperId": "402ee244a32fce7b53ff2e7bacb07f6996d65b33",
        "title": "Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking"
      },
      {
        "paperId": "5b08060d9efee18b235b4d49bb1f58898f3fc24f",
        "title": "Inference-Time Scaling for Generalist Reward Modeling"
      },
      {
        "paperId": "f26fcc2b9fc8944e054425d19c12b9d5cca64fcb",
        "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?"
      },
      {
        "paperId": "d6f12d83ccf1d8201922fd63e37679188e9043c2",
        "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation"
      },
      {
        "paperId": "04bc4203a51c52b632441a5a42f60a1e9f0c64ac",
        "title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models"
      },
      {
        "paperId": "a6cfdad1e056fd14eac5d93f935459b19aaa21f3",
        "title": "Enhancing LLM Knowledge Learning through Generalization"
      },
      {
        "paperId": "3ff44c2de70d779aacc766ee148d6df465f53e01",
        "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding"
      },
      {
        "paperId": "cbd822d674f38c2da8ec4a979772d288b9ad0756",
        "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution"
      },
      {
        "paperId": "12610a7b39efe0ec0a1cd52c242a712ea719fb4c",
        "title": "Large language model for knowledge synthesis and AI-enhanced biomanufacturing."
      },
      {
        "paperId": "eff701fe9cebb0d48e8243294ab832757fef590c",
        "title": "Accelerating Mixture-of-Experts language model inference via plug-and-play lookahead gate on a single GPU"
      },
      {
        "paperId": "1764658448a295ab2dc5b232dbc0d6290892c2ab",
        "title": "Societal Alignment Frameworks Can Improve LLM Alignment"
      },
      {
        "paperId": "b4683e8d44296a30f15f842aafbf283342d40f7b",
        "title": "On opportunities and challenges of large multimodal foundation models in education"
      },
      {
        "paperId": "c7e0d42b977aa9282a49d4f2ae0ec2b00e8bc7b3",
        "title": "WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale"
      },
      {
        "paperId": "57404457c52bbaead08adef52eba2fc7e161b492",
        "title": "Single-pass Detection of Jailbreaking Input in Large Language Models"
      },
      {
        "paperId": "0b8d12f9c91883bcab9bbb5166dc5cc3620e0223",
        "title": "Coherency Improved Explainable Recommendation via Large Language Model"
      },
      {
        "paperId": "0152aafbec465a090684637e1da693d6deb98172",
        "title": "RLTHF: Targeted Human Feedback for LLM Alignment"
      },
      {
        "paperId": "9d2ea7145abe00e28ec4954cba437cdfe81b3089",
        "title": "QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language"
      },
      {
        "paperId": "f364b095a9ad79fc13a3cac94597fd8e31372dba",
        "title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "173a8ccec0d724b34f1679879c9f2a7566962805",
        "title": "Quantifying and Exploiting Adversarial Vulnerability: Gradient-Based Input Pre-Filtering for Enhanced Performance in Black-Box Attacks"
      },
      {
        "paperId": "708897c6ee81ca4ecdf9afdc57ae8b6b5af4f987",
        "title": "SPRI: Aligning Large Language Models with Context-Situated Principles"
      },
      {
        "paperId": "05310179f6b41a7a318f4f47039070459db91fb7",
        "title": "Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search"
      },
      {
        "paperId": "e7f253e216839f0a84e6d004425e3765da775928",
        "title": "Decoding Human Preferences in Alignment: An Improved Approach to Inverse Constitutional AI"
      },
      {
        "paperId": "705baaf896c10699a6c97a9d4066449bad5ae216",
        "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF"
      },
      {
        "paperId": "0342fa8ec9712d412861d1ac6408382474bc15d5",
        "title": "From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning"
      },
      {
        "paperId": "acef200065fad07b8f95798964f341a29ab967bd",
        "title": "Aligning Instruction Tuning with Pre-training"
      },
      {
        "paperId": "693ca05f54e54e05ef93d4a6d61f0403664a03c0",
        "title": "CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive Diagnosis Theory"
      },
      {
        "paperId": "99c2801adabded8ab1a3f484e32f85e208742f97",
        "title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints"
      },
      {
        "paperId": "6439a3982e3bceb0ce76ffaaf5a601d838ddbc89",
        "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision"
      },
      {
        "paperId": "208b18709a20bbe518233441f3b080e72f4d4ec7",
        "title": "NILE: Internal Consistency Alignment in Large Language Models"
      },
      {
        "paperId": "77cb14ab20dff8917991f981c6ec1c25a79f82b7",
        "title": "LLMs Can Simulate Standardized Patients via Agent Coevolution"
      },
      {
        "paperId": "d25014b25bef396e25e753a5cfc76fd1a7cd1497",
        "title": "The Superalignment of Superhuman Intelligence with Large Language Models"
      },
      {
        "paperId": "72aad73c5d72cd7d11c68490dbdbe2beefd3fba9",
        "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM"
      },
      {
        "paperId": "a87ea5e3baa7b605d5b8740fd41584b742590977",
        "title": "Fine-Tuning Large Language Models for Sentiment Classification of AI-Related Tweets"
      },
      {
        "paperId": "23436a792cd2b40c69f8f9fd1e112062f66d96ac",
        "title": "LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification"
      },
      {
        "paperId": "68981715a1e37c955329fc1a278aef59c9be4764",
        "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models"
      },
      {
        "paperId": "5ca51a7f5366e77fe82a8fde16e69a5c64613317",
        "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset"
      },
      {
        "paperId": "78cca0da033edae72951940f9dd18eecc23ae36f",
        "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
      },
      {
        "paperId": "258afa6757b96f926ae311fdb94d6191f88e5c43",
        "title": "A Guide to Misinformation Detection Data and Evaluation"
      },
      {
        "paperId": "fe472d15e2fd294c5d871a5b316d6a74908f409f",
        "title": "Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models"
      },
      {
        "paperId": "ccf8ea6868eb7d2e924333630b4733922bda3d21",
        "title": "Unvoiced: Designing an LLM-assisted Unvoiced User Interface using Earables"
      },
      {
        "paperId": "442cd80bdcd49a843ad3ace7fbf6548b268475ef",
        "title": "LLM Security Alignment Framework Design Based on Personal Preference"
      },
      {
        "paperId": "9bf84876c41c9aadd41115628f78d149e07e99d1",
        "title": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models"
      },
      {
        "paperId": "3257a72f5cc9f9e35a179b28229045e8cb3c231c",
        "title": "SelfCodeAlign: Self-Alignment for Code Generation"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "cf01d7c40cbf815de0f62fa78c2352ba546ad680",
        "title": "Self-Preference Bias in LLM-as-a-Judge"
      },
      {
        "paperId": "7aadea6eb82077aa48b134b896f40cdbaa13f51b",
        "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning"
      },
      {
        "paperId": "e040c2983929b87740dff903056e1e4df71d325c",
        "title": "The Tug of War Within: Mitigating the Fairness-Privacy Conflicts in Large Language Models"
      },
      {
        "paperId": "557752a750b92f125b862220965fb79f312213a6",
        "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning"
      },
      {
        "paperId": "89b42dc02400db2c58703fb0d8d0b6f0210bfe7f",
        "title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning"
      },
      {
        "paperId": "274822eb12ed36b871fd829f0ce8520c4f0f26ab",
        "title": "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning"
      },
      {
        "paperId": "304fa6832d592329b2c5cc4318504c4d73f8569e",
        "title": "Negative-Prompt-driven Alignment for Generative Language Model"
      },
      {
        "paperId": "a41cd77492af3140caccb7f21a351cc6dc87343f",
        "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation"
      },
      {
        "paperId": "9393b0c00d509d58f9e8bb782fb9ec2c3bbf1b3c",
        "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs"
      },
      {
        "paperId": "8943fdc7694c7f30e5a221392af0e8b39adba6e9",
        "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models"
      },
      {
        "paperId": "545cccca3ed5e8277656f163f3032b062000b244",
        "title": "Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning"
      },
      {
        "paperId": "11d8667b427e3c3973e87fb85d54a5e00f3b5342",
        "title": "Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest Models Reward Hack"
      },
      {
        "paperId": "8b2d5f7b02de43d5b5b8eae230a421b00dc5d9c5",
        "title": "CursorCore: Assist Programming through Aligning Anything"
      },
      {
        "paperId": "4f98157c298b87408646672b812801f3d6618c76",
        "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning"
      },
      {
        "paperId": "5014ed8097639654124ecd79b79ef20eca5b055b",
        "title": "PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories"
      },
      {
        "paperId": "298cca23ba83a6fb17a753f465dbacaf167811fa",
        "title": "Rule-based Data Selection for Large Language Models"
      },
      {
        "paperId": "ff6f6ee7b62aa046021582786aac0f9396b99ef9",
        "title": "Intuitions of Compromise: Utilitarianism vs. Contractualism"
      },
      {
        "paperId": "bbcc159658595879f2fdfc019220bcac35776020",
        "title": "AlpaCream: an Effective Method of Data Selection on Alpaca"
      },
      {
        "paperId": "c6ef17e46c68449e28fe728783e0c46f99ec3234",
        "title": "In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement"
      },
      {
        "paperId": "45c70f76c60b582ef8a6432f19a5095643ebe902",
        "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "f533a18ac9226bea96bf6d3876c86d2514a72ec2",
        "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "a2f36bbef278ffd53212b7e3c4cb7bb9c92e7f3d",
        "title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation"
      },
      {
        "paperId": "6d0e972b8f9cc9e5b626d0b5663a7ab18552f98c",
        "title": "User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions"
      },
      {
        "paperId": "f2a8ad62d936e807e463aa64a73a9f71b6b1bfb6",
        "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning"
      },
      {
        "paperId": "e75577d2f1f83ffc8fa229a20ea080938b013c32",
        "title": "Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization"
      },
      {
        "paperId": "f2182f156aaaa6986b9ab208d872b934b1f0d8ec",
        "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models"
      },
      {
        "paperId": "282058c7b1f07b878c154f3b4245992fd2bbb15e",
        "title": "Preference-Guided Reflective Sampling for Aligning Language Models"
      },
      {
        "paperId": "7229fde03d19a585bd2694f8103b807fadf635d4",
        "title": "REInstruct: Building Instruction Data from Unlabeled Corpus"
      },
      {
        "paperId": "0276351cb3307dea8202bfb422849f1faa97133a",
        "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation"
      },
      {
        "paperId": "9c803f13a8d7e3029621c8363f07517293134227",
        "title": "Value Alignment from Unstructured Text"
      },
      {
        "paperId": "b7e34c42d56cac83a20fbc08ad05085ea279d0d3",
        "title": "Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs"
      },
      {
        "paperId": "6887a2513182a1b7376a8aece2e38cae23c26970",
        "title": "CyberPal.AI: Empowering LLMs with Expert-Driven Cybersecurity Instructions"
      },
      {
        "paperId": "13065291b871cade87cd3a0a793729cbc57e69ec",
        "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm"
      },
      {
        "paperId": "7607d4e1e8de5543657d0c2d3bdd1fc7b9380a96",
        "title": "Mapping Generative Artificial Intelligence (GAI's) Exciting Future: From Gemini to Q* and Beyond"
      },
      {
        "paperId": "68125cedd3c5a77c8c9268503e83932cc9ac9c5d",
        "title": "A Logical Fallacy-Informed Framework for Argument Generation"
      },
      {
        "paperId": "e592d3cc237bf8e1fa04f06bbdee0e8d66a1c077",
        "title": "Progressively Label Enhancement for Large Language Model Alignment"
      },
      {
        "paperId": "a61c3383c50a72696d09929f1c3ef3e5268bb873",
        "title": "ComVas: Contextual Moral Values Alignment System"
      },
      {
        "paperId": "b0230118746b2a8bed260a828e5731482aba019e",
        "title": "AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools"
      },
      {
        "paperId": "5655ad1db638ddf86db964c201a3014fb8e21db6",
        "title": "LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models"
      },
      {
        "paperId": "f5648f90f08e9a3beccd5f7efc6ebfd0f72f6bd1",
        "title": "Self-Directed Synthetic Dialogues and Revisions Technical Report"
      },
      {
        "paperId": "3342b1a515b0d05e379a3cdec5c255df4e0a06bf",
        "title": "MCM-Llama: A Fine-Tuned Large Language Model for Real-Time Threat Detection through Security Event Correlation"
      },
      {
        "paperId": "e1f9c72f602a1145d26ed11af6a5282e41864f0a",
        "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models"
      },
      {
        "paperId": "c8c9002af1d90e9dafa3d07e9edf0d883ec45472",
        "title": "Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation"
      },
      {
        "paperId": "3a588665fb59801fcbdce825ed7a4cf59984567f",
        "title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey"
      },
      {
        "paperId": "6c2f36544226a34ede9aaae4a9655595ab543611",
        "title": "Establishing Knowledge Preference in Language Models"
      },
      {
        "paperId": "d848b79f7d983774c8ec69b1e8cbaff9492911b6",
        "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment"
      },
      {
        "paperId": "26d3dd2fa8019e7b876ef92584f6b64506142d5a",
        "title": "Representation Learning and Information Retrieval"
      },
      {
        "paperId": "f10006d615cc32cf0cedac21a96dabd66508d273",
        "title": "A Field Guide to Automatic Evaluation of LLM-Generated Summaries"
      },
      {
        "paperId": "c4d0ec318c1ddca4337435af6004a3f190712a74",
        "title": "Progress or Regress? Self-Improvement Reversal in Post-training"
      },
      {
        "paperId": "523b8635597d6c0bf05fd0e3f35f3a18d2748a24",
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
      },
      {
        "paperId": "682ff66a5ec0248f7e4a17a684b2d1e328e57f70",
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models"
      },
      {
        "paperId": "257e773879bf56bdca82bf6346a8642c3a5c51db",
        "title": "Exploring and Improving Consistency in Large Language Models for Multiple-Choice Question Assessment"
      },
      {
        "paperId": "70b3ae0480e795608ce0b2a4b555d002c5d6f4d7",
        "title": "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement"
      },
      {
        "paperId": "7a033c23f0f923697b44a24e90376a2d5ba50f79",
        "title": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language Models via Inherent Capability"
      },
      {
        "paperId": "69f387ee723a607dcc8605fd43be9cc5f9f870de",
        "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models"
      },
      {
        "paperId": "a52783e2d37940fa9664f168239a49dea2b18672",
        "title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation"
      },
      {
        "paperId": "7045f1865288cc9f2d8cfa118d3f40b7f3ce8af4",
        "title": "Towards a Science Exocortex"
      },
      {
        "paperId": "34214e1de58c6b816263e2207c77ff65f093b967",
        "title": "Towards Region-aware Bias Evaluation Metrics"
      },
      {
        "paperId": "dab57aa71794781ae305218fce9fd004ecdfc4f0",
        "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness"
      },
      {
        "paperId": "d63c65f78ce6945eea33370341716ee06522857c",
        "title": "SS-GEN: A Social Story Generation Framework with Large Language Models"
      },
      {
        "paperId": "d7e07ee25e03080374c683048aa30a468be2f131",
        "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots"
      },
      {
        "paperId": "913a811d63addf34d5e5a5316476406629986a1c",
        "title": "ChatGPT, Bard, Bing Chat, and Claude generate feedback for Chinese as foreign language writing: A comparative case study"
      },
      {
        "paperId": "1c2595d0cee5ea3ffe8ae9aa2e7a716641c6141d",
        "title": "Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning"
      },
      {
        "paperId": "92fc70fd701835c5fb5c548d9a04596a5f6b1b12",
        "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models"
      },
      {
        "paperId": "1398dbee03dfc2454fec5e4ff1c7e62900e8468e",
        "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts"
      },
      {
        "paperId": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
        "title": "A Survey on Human Preference Learning for Large Language Models"
      },
      {
        "paperId": "ef010a9038dc02462312f6c1c97bf8d296c1abb5",
        "title": "Self-Evolution Fine-Tuning for Policy Optimization"
      },
      {
        "paperId": "0907d616383b18ff4a9973ce012b97ea51c9e7ab",
        "title": "Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models"
      },
      {
        "paperId": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "paperId": "ffeb4c933b81f06724379603e69e1b8f9cfbdb64",
        "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey"
      },
      {
        "paperId": "c27e57b6e083b522afa9d202e6dd99ced29ff857",
        "title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions"
      },
      {
        "paperId": "550fa9db81118a96e72c1b371546dccb1eeb8d42",
        "title": "Position: Towards Bidirectional Human-AI Alignment"
      },
      {
        "paperId": "cab3d65b3f4d0a4169d0fdaaed15af6be1d6bb84",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      },
      {
        "paperId": "23ecea454b520cee2732091744bb60a04653f7a2",
        "title": "Unique Security and Privacy Threats of Large Language Models: A Comprehensive Survey"
      },
      {
        "paperId": "1f8de20d7455dbc97991f718bef7b6208848a8c8",
        "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas"
      },
      {
        "paperId": "e9f1f02d0eeb1ca028ec0b41c0c00273a391c99b",
        "title": "Learning Task Decomposition to Assist Humans in Competitive Programming"
      },
      {
        "paperId": "f8585086d57b6b26682a4eb8a357df4e00759c50",
        "title": "Transforming dental diagnostics with artificial intelligence: advanced integration of ChatGPT and large language models for patient care"
      },
      {
        "paperId": "e100989ca03cecf8ad865287d7a77719ddb8d796",
        "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans"
      },
      {
        "paperId": "679b1c81bf676d0389e1d4317bee630f250d3399",
        "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions"
      },
      {
        "paperId": "eeef5677cdaa3ea907278eb651ba16335757a97f",
        "title": "Decoding Human Languages: A Transfer Learning Approach for Language Translation for Low Resources Languages - Nepali, Urdu, Pashto and Punjabi"
      },
      {
        "paperId": "b031b8e0535eb8cee4088acf9335ed27b145703c",
        "title": "Is Free Self-Alignment Possible?"
      },
      {
        "paperId": "f2cf484329dabf3499b17d724cf476ccf92aea9c",
        "title": "On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept"
      },
      {
        "paperId": "643980a504f676fbc5ba87a277dd50e136616efd",
        "title": "LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models"
      },
      {
        "paperId": "5c8245c04c902b0dd631b8633ca835967f395ad0",
        "title": "HonestLLM: Toward an Honest and Helpful Large Language Model"
      },
      {
        "paperId": "f0dece803297c0f368142a33aaa3afdc7f3b42a4",
        "title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement"
      },
      {
        "paperId": "f17945bb329439dc01e4aeacc6864fd1596698cc",
        "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "6e22d4b3070a9efa61b842cc530e6622b656b18a",
        "title": "The Impossibility of Fair LLMs"
      },
      {
        "paperId": "14b588f38a3af6ef1b0186e2bb98c77d3b650093",
        "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator"
      },
      {
        "paperId": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
        "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment"
      },
      {
        "paperId": "ce707711e8539d74fce82222e822eaf7f59b4b0a",
        "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models"
      },
      {
        "paperId": "537a571ec39a4b8739816e88fb9b1b5f8ef4aed4",
        "title": "Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity"
      },
      {
        "paperId": "1a182bba3afc9f65cd86eab33f15a6dd09650c1b",
        "title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data"
      },
      {
        "paperId": "d868a185c5d6ffda30583a714bb703461de797dd",
        "title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation"
      },
      {
        "paperId": "51860642f451377d835c781950f44c95b594029a",
        "title": "SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling"
      },
      {
        "paperId": "b218b84d4137befe61d9a096c3e9a6c3637c68e3",
        "title": "Prompt Exploration with Prompt Regression"
      },
      {
        "paperId": "0d69f44a47babaa522dee90baf632d9a8419bca3",
        "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents"
      },
      {
        "paperId": "4cdc17e0a6c6c379108ead3c061e5a3ab2683b1b",
        "title": "A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot"
      },
      {
        "paperId": "c47867e212534ca3da34e2220d78d803b5cdcf1c",
        "title": "PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games"
      },
      {
        "paperId": "c1f9b85ac8145808767a52954af8fb6d40fa7879",
        "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model"
      },
      {
        "paperId": "1b2e2a355916f61eea1f30fba1135509950118ba",
        "title": "A Survey on Self-Evolution of Large Language Models"
      },
      {
        "paperId": "c0f21f3ab029a8916f7430003938f6f6f60bd31c",
        "title": "Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels"
      },
      {
        "paperId": "3d91da4a2e56470c855f99c934a1b10ee3aa4c58",
        "title": "Unveiling Students\u2019 Experiences and Perceptions of Artificial Intelligence Usage in Higher Education"
      },
      {
        "paperId": "6c17de3e719c0f4d1df6a16f770c2a9a5f18206f",
        "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing"
      },
      {
        "paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f",
        "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs"
      },
      {
        "paperId": "a22afef5675c92bc270eefdd13fcf7f00c824571",
        "title": "MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain Expertise"
      },
      {
        "paperId": "beb1997dc3a8dca581c923eacb5370c564a6b5c8",
        "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment"
      },
      {
        "paperId": "d2be1c0f537adc9ffec42510bc84d2f478ee2402",
        "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model"
      },
      {
        "paperId": "d7bc3fecc6372c9b3fe2d0581167f00caaf05f36",
        "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models"
      },
      {
        "paperId": "6457474106e7af2c5f67769fbf915432b7d1b513",
        "title": "Qibo: A Large Language Model for Traditional Chinese Medicine"
      },
      {
        "paperId": "99ebf103de8ecdb29967ab34206f494893e921ee",
        "title": "Content Knowledge Identification with Multi-Agent Large Language Models (LLMs)"
      },
      {
        "paperId": "cef1e2542bd47e5e9ba7100835d383693428ca20",
        "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models"
      },
      {
        "paperId": "d06e65f74715e071678bf8ccdcf9d52004a10280",
        "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences"
      },
      {
        "paperId": "9ccb5de1e22238b93a6af01c1dc341dc9bc3f28d",
        "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision"
      },
      {
        "paperId": "a819ed5552883aefffded7dd97509fbee5142f17",
        "title": "CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion"
      },
      {
        "paperId": "577def35ce131165ea9f228466ed5fd2664f2e5f",
        "title": "Aligners: Decoupling LLMs and Alignment"
      },
      {
        "paperId": "66e7edf09589527ebb58418632418758cee668cd",
        "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"
      },
      {
        "paperId": "97f05d277bac590cdcfe512b8adf647bc958d966",
        "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "29a56e1c377ac6a4457656b57ef7631ed2bdb509",
        "title": "LLMCRIT: Teaching Large Language Models to Use Criteria"
      },
      {
        "paperId": "c7137aa84b5a48bda9a96432524f9948e8c823e0",
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models"
      },
      {
        "paperId": "3bdd3d56ef9054aba47f83879b531a4842640295",
        "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning"
      },
      {
        "paperId": "ac7c359ce903cdc104d82298537c61447d8cfe81",
        "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following"
      },
      {
        "paperId": "1ff5422e92e9691b67440842d4d601e388e99f9b",
        "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection"
      },
      {
        "paperId": "c08f383ac0aa40260f430f4607ebdfe7a8b30ba0",
        "title": "Don\u2019t Just Say \u201cI don\u2019t know\u201d! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations"
      },
      {
        "paperId": "2ad5e1e36563882b7acc5df0a1d703a8c42dc5be",
        "title": "Towards Robust Instruction Tuning on Multimodal Large Language Models"
      },
      {
        "paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
        "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning"
      },
      {
        "paperId": "1c0b3679919cd0531973fced1a1eb49745d9332d",
        "title": "Instruction-tuned Language Models are Better Knowledge Learners"
      },
      {
        "paperId": "94db8a625418800c8ae7b48157a9cad1c8129051",
        "title": "A Survey on Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "c3f079f9f59f255e032a1239aea02a2affe93be8",
        "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding"
      },
      {
        "paperId": "de6ddb30b07f192f2be142062c4c6c817e508d96",
        "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation"
      },
      {
        "paperId": "046cbc4c77ed65b33e7b95db058284824da3b580",
        "title": "Parallel Structures in Pre-training Data Yield In-Context Learning"
      },
      {
        "paperId": "b9447b25b309884be037ee25af758275b419bd95",
        "title": "Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models"
      },
      {
        "paperId": "89bc7f4df87ef36f28d048a8a9b8a7c1ac95b909",
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization"
      },
      {
        "paperId": "2d125b26788e41586fa1594da60169f33fb481f0",
        "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL"
      },
      {
        "paperId": "c16f5a6e4c51be0baa49275df441748134d67234",
        "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping"
      },
      {
        "paperId": "41ecff85ec0c8082fcd4efcf31fc5ec3911dce88",
        "title": "Large Language Models as Agents in Two-Player Games"
      },
      {
        "paperId": "a1f76db91c0debcf93ae9889736bce8470902113",
        "title": "Large Language Models: A Survey"
      },
      {
        "paperId": "9fd10119173ea30350db76550c1cbfacf87ce9a2",
        "title": "Understanding the Effects of Iterative Prompting on Truthfulness"
      },
      {
        "paperId": "e9640cd4bdb0fa93a94151ec00259909b5e88d6d",
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation"
      },
      {
        "paperId": "450d97c7f456eafbb69dd70322d341058028b171",
        "title": "In-Context Principle Learning from Mistakes"
      },
      {
        "paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1",
        "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"
      },
      {
        "paperId": "4c6dd7549148f54deac15ff2657d9b024253c147",
        "title": "Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations"
      },
      {
        "paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94",
        "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"
      },
      {
        "paperId": "be6873f649f809592aea9c5f07060799a1609411",
        "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds"
      },
      {
        "paperId": "67eab08db30e397e400e3b36b3afd7526df83314",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      },
      {
        "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
        "title": "Knowledge Verification to Nip Hallucination in the Bud"
      },
      {
        "paperId": "cfcf8ab7c595c1849e8396167a29f3bd3359107c",
        "title": "Agent Alignment in Evolving Social Norms"
      },
      {
        "paperId": "5708f725e13362da80a1062f51df118fca3529ab",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"
      },
      {
        "paperId": "798ece3c5491f613e5368bd2d818476a64b88905",
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers"
      },
      {
        "paperId": "5272acad9e4201e93dabe3fd99bd7ead9b1a544d",
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models"
      },
      {
        "paperId": "66ea57809a718f2634e4f2065569c0ba24659d44",
        "title": "Align on the Fly: Adapting Chatbot Behavior to Established Norms"
      },
      {
        "paperId": "41113411e1748a34bb80f12c761b7af1ed6dbb90",
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"
      },
      {
        "paperId": "6e5972640c7492172e636b0b5d644f1f7db2f4e8",
        "title": "Learning and Forgetting Unsafe Examples in Large Language Models"
      },
      {
        "paperId": "a1abf4d8bad5694621e4d8cd09e41c80cdbba318",
        "title": "From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape"
      },
      {
        "paperId": "7a31971b0af439dec6fc484cca20df57f440b644",
        "title": "One Shot Learning as Instruction Data Prospector for Large Language Models"
      },
      {
        "paperId": "4de9796feed3ecccaa1e9bcea2a079730bb65bf5",
        "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models"
      },
      {
        "paperId": "383c598625110e0a4c60da4db10a838ef822fbcf",
        "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"
      },
      {
        "paperId": "ed37dbbd05c59ce0564544f32cbe47a2af4eb73f",
        "title": "A collection of principles for guiding and evaluating large language models"
      },
      {
        "paperId": "87e8a31ff60497f9e2978b48d14cde9d7b569b48",
        "title": "Beyond Accuracy: Statistical Measures and Benchmark for Evaluation of Representation from Self-Supervised Learning"
      },
      {
        "paperId": "75b8ea8e9175232fe22f6d385006dd5e28e63ab0",
        "title": "Axiomatic Preference Modeling for Longform Question Answering"
      },
      {
        "paperId": "20a965316352e813b5cce13b35e537dbdcf30b9d",
        "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models"
      },
      {
        "paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c",
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"
      },
      {
        "paperId": "38552c3f0f6afd651cc93b59ef7db6dafa405bf0",
        "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest"
      },
      {
        "paperId": "9331818a22f1b80b397b4de8f0742403e2588436",
        "title": "Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value"
      },
      {
        "paperId": "248c9663001cddba588709ac5fb67f2a549c01a0",
        "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"
      },
      {
        "paperId": "06ba45793c761583ffdd408141f81517b92f649f",
        "title": "Functionality learning through specification instructions"
      },
      {
        "paperId": "7a147a745f69329afb1c86becdba7b3029a169ca",
        "title": "Predicting Text Preference Via Structured Comparative Reasoning"
      },
      {
        "paperId": "57d0e672040800e8d882ff0022647c087095e35f",
        "title": "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications"
      },
      {
        "paperId": "93a10102cdd501fdc1ccc79868416313fe719e01",
        "title": "Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue"
      },
      {
        "paperId": "1ee8b053244d0bbb0674bd05c58ec61cc56b85b5",
        "title": "A Study of Implicit Ranking Unfairness in Large Language Models"
      },
      {
        "paperId": "e51f20efb872d0ba99a8b501259948bbb2f6963f",
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"
      },
      {
        "paperId": "e327ef8d46ea0413316c80ee1404453834d84f05",
        "title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training"
      },
      {
        "paperId": "d9e771732100c0f696cefb2a5c82cdd953140bb0",
        "title": "TarGEN: Targeted Data Generation with Large Language Models"
      },
      {
        "paperId": "8ce31d72dcfcd888015646b15f201d49aa71c648",
        "title": "Unpacking the Ethical Value Alignment in Big Models"
      },
      {
        "paperId": "ab90b84b42d43c3077c374cd34b3a48a881faf43",
        "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation"
      },
      {
        "paperId": "45a4aedb233244b8442d37729ff09cc1f92c6d7d",
        "title": "Can You Follow Me? Testing Situational Understanding in ChatGPT"
      },
      {
        "paperId": "12aa2b1e9556c20752e37e8b18d0e396c0cea1c5",
        "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "4d2e4436f2ecfad866000a545bbd65e516d8525f",
        "title": "Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning"
      },
      {
        "paperId": "3b918b15178bcc84fd22af5094fe1efbcd388e72",
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration"
      },
      {
        "paperId": "c170d4c6c0f9a0d0f78d09dc78eaed51d8eb5376",
        "title": "Exploration with Principles for Diverse AI Supervision"
      },
      {
        "paperId": "f05c288caeb9a14ef387e6867934ced3d2200259",
        "title": "SALMON: Self-Alignment with Instructable Reward Models"
      },
      {
        "paperId": "bfe9e6f64d9e819efc27772ab87dc0c5e2d62906",
        "title": "Critique Ability of Large Language Models"
      },
      {
        "paperId": "6f39442852656f8a9decc95854a2ed461b3a83ab",
        "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"
      },
      {
        "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "paperId": "c31396f00c4e4ddba20d085d0da819b89c71bf4a",
        "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning"
      },
      {
        "paperId": "ec366af9feac8bdd0b759b72286489a43a53ac45",
        "title": "Conceptual Framework for Autonomous Cognitive Entities"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "b1e2abc63630f26be54a1931041e0f4eeb0434e0",
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement"
      },
      {
        "paperId": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"
      },
      {
        "paperId": "3c0810ba09cc128b197bb2003d9a80ed6b1d504e",
        "title": "Parameter-Efficient Tuning Helps Language Model Alignment"
      },
      {
        "paperId": "b88ae92f5691b02eeefb967e570d0e1eac4aa01f",
        "title": "Evolving Diverse Red-team Language Models in Multi-round Multi-agent Games"
      },
      {
        "paperId": "5efa173323e8850dde3f504a8c023cdbb6309b55",
        "title": "Self-Specialization: Uncovering Latent Expertise within Large Language Models"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c",
        "title": "Large Language Model Alignment: A Survey"
      },
      {
        "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
        "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
      },
      {
        "paperId": "8ec117feff6ee10e3b20a19ac101fee5c99e14d7",
        "title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"
      },
      {
        "paperId": "a0d83f9e15e722f23c14eb83cb2f87c1d1ea6400",
        "title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria"
      },
      {
        "paperId": "320df3537de1f1804a7b010c4e21a44c6e7c8061",
        "title": "Are You Worthy of My Trust?: A Socioethical Perspective on the Impacts of Trustworthy AI Systems on the Environment and Human Society"
      },
      {
        "paperId": "31ead991a433f233d490a7efd6c61d5fae98c327",
        "title": "Learning by Self-Explaining"
      },
      {
        "paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60",
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning"
      },
      {
        "paperId": "daa5df014ad89aebc0dfcec507eccbbf3934224e",
        "title": "Decolonial AI Alignment: Openness, Visesa-Dharma, and Including Excluded Knowledges"
      },
      {
        "paperId": "1c594adf08fc2ab2fbe5ec1f2468cd7eca73b587",
        "title": "Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?"
      },
      {
        "paperId": "1ce1738d7f224ebd7ad98e23692404f06697b5f4",
        "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"
      },
      {
        "paperId": "f8b90d640158f61c4553518a8554a73b540e07e7",
        "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models"
      },
      {
        "paperId": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
        "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment"
      },
      {
        "paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8",
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"
      },
      {
        "paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475",
        "title": "Self-Alignment with Instruction Backtranslation"
      },
      {
        "paperId": "7f55ef29a6f8b2771c5435bbeba29c87264fdc88",
        "title": "Shepherd: A Critic for Language Model Generation"
      },
      {
        "paperId": "a50d4fd8f584276c0fd8560255884edd57aa926e",
        "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue"
      },
      {
        "paperId": "01d31fb9fc6ab36df6627b8555b64789113eb7a5",
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment"
      },
      {
        "paperId": "03d13caead288fdb7f4f86617bba0400d3bde5c7",
        "title": "Deceptive Alignment Monitoring"
      },
      {
        "paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "title": "A Comprehensive Overview of Large Language Models"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "78c488e2d84bd193a40006b1fceb03e3845b81d4",
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"
      },
      {
        "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models"
      },
      {
        "paperId": "9d81ec931b85d6c6cf3453126670cd7a30a689e7",
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models"
      },
      {
        "paperId": "4656ec3210a3d28f9f30a8ec8a202aae7ed3bf1f",
        "title": "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "6fca85024354e3fafa75b767961bee9245263170",
        "title": "Reward Collapse in Aligning Large Language Models"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99",
        "title": "Aligning Large Language Models through Synthetic Feedback"
      },
      {
        "paperId": "d3e1b1025e6fc73850156da60f85dde91334663e",
        "title": "On the Limitations of Simulating Active Learning"
      },
      {
        "paperId": "4a4272c5ea84a6475b7aa643a8feceaa1645c655",
        "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "1ade52a56ef4e836c445446f74b15e413e53e271",
        "title": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models"
      },
      {
        "paperId": "08a80cb34d785258c770acecd302ab41ead46eed",
        "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86",
        "title": "Using In-Context Learning to Improve Dialogue Safety"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "d092d0a1c874f5d66d2aea005c6174655a285f53",
        "title": "Knowledge Distillation for Language Models"
      },
      {
        "paperId": "43f212bae5d763bae2694fc6358b8551b4801951",
        "title": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory"
      },
      {
        "paperId": "58ab034688774cd17680cad5115df5bacb9776aa",
        "title": "Stronger Models are Not Always Stronger Teachers for Instruction Tuning"
      },
      {
        "paperId": "006c73468323d4b0fa9fa6ec09e8f82f6d777c24",
        "title": "Token-level Preference Self-Alignment Optimization for Multi-style Outline Controllable Generation"
      },
      {
        "paperId": "5b2234269a8c65eca9068ddd56f293ec1067c0fb",
        "title": "A Survey of Post-Training Scaling in Large Language Models"
      },
      {
        "paperId": "27095b8ff52b722c25c0a75c02acf562671f7f97",
        "title": "Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models"
      },
      {
        "paperId": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
        "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection"
      },
      {
        "paperId": "fa18ab4e2df14e5e8c83f95094fe83a1a3051fb0",
        "title": "Donkii: Characterizing and Detecting Errors in Instruction-Tuning Datasets"
      },
      {
        "paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a",
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "928268f287fda48a1410273e8d0e7ced58435562",
        "title": "What Factors Influence LLMs\u2019 Judgments? A Case Study on Question Answering"
      },
      {
        "paperId": "a8c86a10951e21814606bddb68c18d1980f3f481",
        "title": "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey"
      },
      {
        "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
        "title": "Large Language Models for Data Annotation: A Survey"
      },
      {
        "paperId": "eded871f5ef8f502245d1475af2bc3b0152a8093",
        "title": "Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models"
      },
      {
        "paperId": "0caced806a27cfe41f66aa410f17e7e6ee25c3a7",
        "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct"
      },
      {
        "paperId": "04ebbac1a75b083ec871961b0e0807f5ac24393c",
        "title": "INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning"
      },
      {
        "paperId": "c11d885b219e817bdb3d4e95c0307e7f987d3bba",
        "title": "Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"
      },
      {
        "paperId": "a6659669c152bd592e233e3d23d7b90b838dc629",
        "title": "BadRobot: Jailbreaking LLM-based Embodied AI in the Physical World"
      },
      {
        "paperId": "15c17d98a90ef6536a357e0dae6a8a59043e437f",
        "title": "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model"
      },
      {
        "paperId": "da01b426baea356687c7ee1d006c9cf986f498b5",
        "title": "CriticBench: Evaluating Large Language Models as Critic"
      },
      {
        "paperId": "d0fe343fbdecaf4cc477d70e8701f9a6935b13d0",
        "title": "Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment"
      },
      {
        "paperId": "d7d9cd23f91565649d8a3294e2f51d2b95e8e1dd",
        "title": "Enhancing LLM Capabilities Beyond Scaling Up"
      },
      {
        "paperId": "5f2f6f395b500010cec482776d2b885efc44599c",
        "title": "Exploring Safety Generalization Challenges of Large Language Models via Code"
      },
      {
        "paperId": "4a5d06f52af9242b5087e3040e589d45a39f2796",
        "title": "A Guide to Misinformation Detection Datasets"
      },
      {
        "paperId": "72b69978b035f003525c7f75932c5304d771e425",
        "title": "StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure"
      },
      {
        "paperId": "c243df958269bf501f874ef213ba6cc904f24ea9",
        "title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding"
      },
      {
        "paperId": "174ea5ae16c1cd23c386491749e3a1c3c4ccce33",
        "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models"
      },
      {
        "paperId": "005fc6908a12f8d460aa06c1eb6c8176ca9b1199",
        "title": "D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models"
      },
      {
        "paperId": "ec94157ecd55482ea4b7d66016593e3072e671f6",
        "title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models"
      },
      {
        "paperId": "79d0c3422e4ccab2d20be3de2c387157425de842",
        "title": "Is Free Self-Alignment Possible?"
      },
      {
        "paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68",
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models"
      },
      {
        "paperId": "7637ed79d30d0139901175ae4abedd822c217ab4",
        "title": "3D-LLM: Injecting the 3D World into Large Language Models"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": "2df1428839bf21e2e803957a142a5027024a5199",
        "title": "Decolonial AI Alignment: Vi\u015besadharma, Argument, and Artistic Expression"
      },
      {
        "paperId": "df9b47b2d1fdf6b66dac31309fa1552907414ffe",
        "title": "Learning Preference Model for LLMs via Automatic Preference Data Generation"
      },
      {
        "paperId": "2d7a14fabe2631a205e65bcac94d8e84d883492f",
        "title": "Enable Language Models to Implicitly Learn Self-Improvement From Data"
      },
      {
        "paperId": "cc1f72c2334703be01063536705b116ba49341cd",
        "title": "RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment"
      },
      {
        "paperId": "0fc451fd78a629fcc2ff5b5e89b60a45e8819be2",
        "title": "Aligning Agent Policy with Externalities: Reward Design via Bilevel RL"
      },
      {
        "paperId": "6e25613b843c658c8051e35f361fee7ae5f068e0",
        "title": "Aligning Agent Policy with Principal Interests: Reward Design via Bilevel RL"
      },
      {
        "paperId": "24de1048791bac4972ecc16d1c3c1de23691407d",
        "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"
      },
      {
        "paperId": "c5022fbd0d6cf1f31cad591541c9c0d593da4b0c",
        "title": "Analyzing and Mitigating Cultural Hallucinations of Commercial Language Models in Turkish"
      },
      {
        "paperId": "3bc73c887a0eeadf791ae50251e26b082faa5613",
        "title": "Detecting LLM Hallucinations Using Monte Carlo Simulations on Token Probabilities"
      },
      {
        "paperId": "3e2734025037fc626873c56e05ddb43ccccd3858",
        "title": "A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage"
      },
      {
        "paperId": "a78b8d5b3499b59811f68c88506b173320034c77",
        "title": "The Age of Superintelligence: \u223c Capitalism to Broken Communism \u223c"
      },
      {
        "paperId": "b0da4d18b6eeeb4206b087016c2d7fd368e18c55",
        "title": "RMB OOST : R EWARD M ODEL T RAINING W ITH P REFERENCE -C ONDITIONAL M ULTI -A SPECT S YN - THETIC D ATA G ENERATION"
      },
      {
        "paperId": "eb9a0c61ef05c1e8058130ffe0623feeed395ace",
        "title": "Weak-to-Strong In-Context Optimization of Language Model Reasoning"
      },
      {
        "paperId": "791a791def5257d9841dfce81f8051454d27b800",
        "title": "Functionality learning through speci\ufb01cation instructions"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      },
      {
        "paperId": "c9236278bcaa752a49b30e4b4ce52f1eba234a9d",
        "title": "CodeSpeak: Improving smart contract vulnerability detection via LLM-assisted code analysis"
      }
    ],
    "score": 183.0
  },
  {
    "id": "182c7b40ff7560a5545764814338f55a2098e441",
    "title": "Reinforced Self-Training (ReST) for Language Modeling",
    "authors": [
      "Caglar Gulcehre",
      "T. Paine",
      "S. Srinivasan",
      "Ksenia Konyushkova",
      "L. Weerts",
      "Abhishek Sharma",
      "Aditya Siddhant",
      "Alexa Ahern",
      "Miaosen Wang",
      "Chenjie Gu",
      "Wolfgang Macherey",
      "A. Doucet",
      "Orhan Firat",
      "Nando de Freitas"
    ],
    "year": 2023,
    "citationCount": 338,
    "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",
    "url": "https://www.semanticscholar.org/paper/182c7b40ff7560a5545764814338f55a2098e441",
    "pdf_url": "https://arxiv.org/pdf/2308.08998.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-08-17",
    "externalIds": {
      "ArXiv": "2308.08998",
      "DBLP": "journals/corr/abs-2308-08998",
      "CorpusId": 261031028
    },
    "references": [
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "155aec5cff650263a4c71136f97570611d1bba7a",
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget"
      },
      {
        "paperId": "a36658b26ea4ccb58f85d8a578f6ec6767446095",
        "title": "Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "004357dd9bbf3012c8fe0ccada4da401bf85dfff",
        "title": "Defining and Characterizing Reward Hacking"
      },
      {
        "paperId": "c009a959dd236c162e51703e3bfd4d2b0b751c17",
        "title": "MAD for Robust Reinforcement Learning in Machine Translation"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "511e6559df79b5b7cc3fa69ae31ef1c3badce048",
        "title": "When does return-conditioned supervised learning work for offline reinforcement learning?"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
        "title": "Competition-level code generation with AlphaCode"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "68f141724814839d556a989646194be88641b143",
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "de1fdaf92488f2f33ddc0272628c8543778d0da9",
        "title": "Scaling Laws for Neural Machine Translation"
      },
      {
        "paperId": "dc32a984b651256a8ec282be52310e6bd33d9815",
        "title": "Highly accurate protein structure prediction with AlphaFold"
      },
      {
        "paperId": "c72361b09be06124397798415d9746bb3318c28b",
        "title": "Launchpad: A Programming Model for Distributed Machine Learning Research"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "e8cc5b6204970a88cd1b2df491aa10c4333e083e",
        "title": "Machine Translation Decoding beyond Beam Search"
      },
      {
        "paperId": "0dbef95ac15785e03b54a529a3d85128c19aa09a",
        "title": "Regularized Behavior Value Estimation"
      },
      {
        "paperId": "91b82be3a82b026937ac3accd6818073d2d1b739",
        "title": "Supervised Seeded Iterated Learning for Interactive Language Learning"
      },
      {
        "paperId": "9e67b9758520e49016ab66bafb974d2e1ed762d1",
        "title": "COMET: A Neural Framework for MT Evaluation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "3277e13821617580534fb25dd2d041bf75f0bceb",
        "title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios"
      },
      {
        "paperId": "4ae52766028e69186052ea8f33a137fbbbdb986a",
        "title": "BLEURT: Learning Robust Metrics for Text Generation"
      },
      {
        "paperId": "81fdd50a55efe384817c0a18c928fd866c48745c",
        "title": "Countering Language Drift with Seeded Iterated Learning"
      },
      {
        "paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "title": "Self-Training With Noisy Student Improves ImageNet Classification"
      },
      {
        "paperId": "12442420adf1c36887fafd108f4b7f4fc822ae60",
        "title": "Revisiting Self-Training for Neural Sequence Generation"
      },
      {
        "paperId": "e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379",
        "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control"
      },
      {
        "paperId": "d397f4cf400f6ffcb1b8e3db27bb75966a0513cf",
        "title": "Self-Imitation Learning"
      },
      {
        "paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e",
        "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "d9b03cd97db6255081d1e57983fa673d1f8f2d0e",
        "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "8f78f92ac1394deaf7850a3de7b02c98c81b434c",
        "title": "Iterated learning and the evolution of language"
      },
      {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "paperId": "906765f7ac46011123cca59de775216f1ee9b451",
        "title": "Bootstrapping POS-taggers using unlabelled data"
      },
      {
        "paperId": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
        "title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
      },
      {
        "paperId": "17a620afc87f5266e3fd8b3f308c883cc2c2b7c7",
        "title": "Probability of error of some adaptive pattern-recognition machines"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": "4084a7605b87306b5d688d215b4fbfb4e55dd8b3",
        "title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU \u2013 Neural Metrics Are Better and More Robust"
      },
      {
        "paperId": "a8c409791404828a3276ffb0e4a71d3263b0526e",
        "title": "Should I Run Offline Reinforcement Learning or Behavioral Cloning?"
      },
      {
        "paperId": "33c1087b86025c7bc919f2fda817a491c0c350ab",
        "title": "Beyond Tabula Rasa: Reincarnating Reinforcement Learning"
      },
      {
        "paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508",
        "title": "An empirical analysis of compute-optimal large language model training"
      },
      {
        "paperId": "257432aaaba92fc878293028b63131d87cb61712",
        "title": "On Multi-objective Policy Optimization as a Tool for Reinforcement Learning"
      },
      {
        "paperId": "63aa5073ce839c26b3d505200e5ef78d875f91c0",
        "title": "Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain"
      },
      {
        "paperId": null,
        "title": "2021) we did not use the V-trace as it did not improve over the vanilla advantage"
      },
      {
        "paperId": null,
        "title": "The BVMPO approach is similar to DIME"
      },
      {
        "paperId": "44d8e789f8ae6e0ee854d218aa307b001a6f7dd7",
        "title": "Expert iteration"
      },
      {
        "paperId": null,
        "title": "When using it in Improve step, we start with a BC policy"
      },
      {
        "paperId": "74327f2d5ab7367667dad56e13858ff5ecdb7d81",
        "title": "Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment"
      },
      {
        "paperId": "502a1995c0a56af4487d364f56e1ce8abe78f23a",
        "title": "The DeepMind Chinese\u2013English Document Translation System at WMT2020"
      },
      {
        "paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671",
        "title": "Text Generation by Learning from Demonstrations"
      },
      {
        "paperId": null,
        "title": "The DeepMind JAX Ecosystem, 2020"
      },
      {
        "paperId": null,
        "title": "In each Grow step, we generate 25 candidates for each source sentence in the training"
      },
      {
        "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "title": "ALVINN, an autonomous land vehicle in a neural network"
      },
      {
        "paperId": "81aace0e90c6a962059b117c24db0d856f340f41",
        "title": "Report on the 11th IWSLT evaluation campaign"
      },
      {
        "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
        "title": "Batch Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Reinforced Self-Training ( ReST ) for Language Modeling"
      },
      {
        "paperId": null,
        "title": "Confidential -DeepMind, do not share or forward externally"
      }
    ],
    "cited_by": [
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "b7f246505dffafd4f6a5f4bf02f22ebbc69328af",
        "title": "Diffusion Alignment as Variational Expectation-Maximization"
      },
      {
        "paperId": "498a98386d007a2fdb88fabcd5f13b1f972697fc",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression"
      },
      {
        "paperId": "85359dcfea8498a0119d24d4f84f4f52e64a971d",
        "title": "Future Policy Aware Preference Learning for Mathematical Reasoning"
      },
      {
        "paperId": "657b5236760a233605ac3a70eec2ed17bf487279",
        "title": "Rethinking Molecule Synthesizability with Chain-of-Reaction"
      },
      {
        "paperId": "efdae81c0231cf6601c7f0dba782cc81dc91fede",
        "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey"
      },
      {
        "paperId": "dd6dd841ea57410d4d8aa0873189f035464e4119",
        "title": "Generative Data Refinement: Just Ask for Better Data"
      },
      {
        "paperId": "b3c13dc7d84369678f16a19fd19e829fd80b9253",
        "title": "Reverse-Engineered Reasoning for Open-Ended Generation"
      },
      {
        "paperId": "9b683772ab72b8a3ea1b4e973ba30603a17defe7",
        "title": "Towards a Unified View of Large Language Model Post-Training"
      },
      {
        "paperId": "4be2790382dc1745ffdb86d768e8490fddb820e7",
        "title": "Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control"
      },
      {
        "paperId": "fcd4af45421052e56f26b37e2486b848df87aaab",
        "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment"
      },
      {
        "paperId": "2187694098646f46f7221bf2346c0faff05ced84",
        "title": "Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding"
      },
      {
        "paperId": "5a7f832739dfe245834f6ab2b3c868014843803d",
        "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation"
      },
      {
        "paperId": "d4fa13b5f06fc1f415cbf9ae89286574101b062c",
        "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge"
      },
      {
        "paperId": "097f801ad7f0a71ed7f4cfdf5d61351fff4f2b08",
        "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization"
      },
      {
        "paperId": "81b6143853e3380ccd24eae8e32a47dc94351576",
        "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning"
      },
      {
        "paperId": "630ac252878b34cff962c925365d810beb249f02",
        "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning"
      },
      {
        "paperId": "265fdd781ac5f96f0e6ae37fbca0a370ee7c1b26",
        "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "3a0bde838c114f61fd8081c3c34a258eeb09c015",
        "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning"
      },
      {
        "paperId": "92f432d4d43f0bca8c2859d9269a94ea93fcfb1f",
        "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game"
      },
      {
        "paperId": "502a9fe58f14fa1252648bcb434e7ad04913c525",
        "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models"
      },
      {
        "paperId": "0933fc39e13991fc9650481f7467b8061423ee87",
        "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training"
      },
      {
        "paperId": "78d4ec6423af0a2653f88c97738e6908aaf8a5bc",
        "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs"
      },
      {
        "paperId": "6f385038c0ab31215974e77dc63dea4127a89b08",
        "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards"
      },
      {
        "paperId": "8ce077b75a709e5fc11526236c37d2f77bedf4c0",
        "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling"
      },
      {
        "paperId": "fc450c5aa4c85224e9944e65daaccc971470113d",
        "title": "Thompson Sampling in Online RLHF with General Function Approximation"
      },
      {
        "paperId": "e2dff540f6dbc226f0a23f1a5ec2ac4d09d9b6b4",
        "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering"
      },
      {
        "paperId": "c14bdd1555be953c3b37154a0b5cf1d8b129058b",
        "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?"
      },
      {
        "paperId": "b12998ee8a2d2feba5e17eec7c437b4243b4aeee",
        "title": "Can Large Reasoning Models Self-Train?"
      },
      {
        "paperId": "10b0744a3c58d0575033f23c195fe72ced2f540f",
        "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection"
      },
      {
        "paperId": "9a3799afe5b8ad041be8f5b71d0f8342d04be6d1",
        "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation"
      },
      {
        "paperId": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
        "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning"
      },
      {
        "paperId": "0de94f0ac10f63019b27fcd70a6bfbc7d9ba235a",
        "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection"
      },
      {
        "paperId": "fb70bcedfe620d0a218b3012e5049c03a4a3639a",
        "title": "ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects"
      },
      {
        "paperId": "a5a360af27af5153fbdbeb685159b4adcc930d66",
        "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning"
      },
      {
        "paperId": "2287c77e0e8c533647577c2ddc6dfa0d509db6bb",
        "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning"
      },
      {
        "paperId": "830a0c3b26f11003ae0fdd50571b7a26df3f524b",
        "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs"
      },
      {
        "paperId": "17734a5ca836a8d60e49c7cd7af2388f11b0f904",
        "title": "Latent Veracity Inference for Identifying Errors in Stepwise Reasoning"
      },
      {
        "paperId": "c778fad9acef0847c4f4e0f4ac033b7c4f1c217b",
        "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging"
      },
      {
        "paperId": "3a89dc7a79f24aefd23065840444c0dba1206a1a",
        "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution"
      },
      {
        "paperId": "4ecacdd6d7958cb4e536cdde5685676e856d419d",
        "title": "Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning"
      },
      {
        "paperId": "0bb931017ae0911386d08eef5f3ac81853831119",
        "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes"
      },
      {
        "paperId": "139932253d5a8f07ddf07bac61a62d6a4dcc947d",
        "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning"
      },
      {
        "paperId": "c2feda1e804700d0980d71cfc71ce66d369b6b6c",
        "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "3d58168c202c95ee932c1913f445b3f5efe0f75e",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL"
      },
      {
        "paperId": "5ac754e6d6b0ea662b19e83f445aec8f03569e75",
        "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning"
      },
      {
        "paperId": "bddc210ec0ecc42b617fb84bd5fe331f5d294374",
        "title": "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving"
      },
      {
        "paperId": "6758a6db1bfb6ebc5134aea9ce0fc28dd2e031a4",
        "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review"
      },
      {
        "paperId": "a30d44d7cfff6d161e9310b8d73e8aeaf29e3ec1",
        "title": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models"
      },
      {
        "paperId": "143e18bfd7c356592e7c1439738a3525d3e16279",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
      },
      {
        "paperId": "80ec713f64a6b7b0c72de3c23824840e9037c893",
        "title": "Offline Learning and Forgetting for Reasoning with Large Language Models"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "07017b4c848371df9414f64d91002292d42324cd",
        "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining"
      },
      {
        "paperId": "30ca68af36578d238964df1c5c596e33521e00a5",
        "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts"
      },
      {
        "paperId": "fbc0b5e1b822796d7ae97268def2e0993b5da644",
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning"
      },
      {
        "paperId": "a05cc4e797611485208d14789b93c3b44b3d93c4",
        "title": "AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs"
      },
      {
        "paperId": "34ef778670b0d04352981911a60bd97e816c4c4a",
        "title": "LLM Fine-Tuning: Concepts, Opportunities, and Challenges"
      },
      {
        "paperId": "a231a99f0e6bbb63ad538bb6670ecf1fb3100c63",
        "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead"
      },
      {
        "paperId": "27c3fe1e984c93347ea9c7f39910bc085c58978a",
        "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models"
      },
      {
        "paperId": "e968c45ea2c88772b02317a1a9ad5e26b320e6cb",
        "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data"
      },
      {
        "paperId": "d1755c9aff0c3cccad45b9474676b5a700f5b0c1",
        "title": "Don't lie to your friends: Learning what you know from collaborative self-play"
      },
      {
        "paperId": "ae6f30f15a699b2b8cba027e6d89fa8c4340c19f",
        "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "paperId": "c619a09113ba490dd2d84fe7ca57bae70aba5310",
        "title": "DAST: Difficulty-Aware Self-Training on Large Language Models"
      },
      {
        "paperId": "d2f37492d17c7333b7df48e91d50240e6a4b0b6b",
        "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity"
      },
      {
        "paperId": "9934c78aab745e1c8b51d966dd7af8b789d543bc",
        "title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge"
      },
      {
        "paperId": "f2131456c69bd232610ad8305936706eec2f2e43",
        "title": "Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets"
      },
      {
        "paperId": "4c1b7cf0550130a2aca6215b759cb09c76b19978",
        "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning"
      },
      {
        "paperId": "550300f63860fc347fb97401cf55015fd369056b",
        "title": "Language Models can Self-Improve at State-Value Estimation for Better Search"
      },
      {
        "paperId": "485ac73320040449bdd224f2d89732e69c585c2d",
        "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models"
      },
      {
        "paperId": "8f2db783ac3481d9fddea610009b555c888d3dd1",
        "title": "PEO: Improving Bi-Factorial Preference Alignment with Post-Training Policy Extrapolation"
      },
      {
        "paperId": "2967cde57893bcb08e18814fae277a47aea82317",
        "title": "Superficial Self-Improved Reasoners Benefit from Model Merging"
      },
      {
        "paperId": "5a6213d46942c7ff6318972ee1618a7106b84c5a",
        "title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "1137feeae870e782814c338343f870211ac72d59",
        "title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization"
      },
      {
        "paperId": "bb146358872cf242e97d891bbf6994bc1faae2fe",
        "title": "Training a Generally Curious Agent"
      },
      {
        "paperId": "0b9fae1ac6b080d65e79970da2d452c64f3352a8",
        "title": "IPO: Your Language Model is Secretly a Preference Classifier"
      },
      {
        "paperId": "e495cdd1c62281b67677103650b885d4eaf151c5",
        "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "031ed8286daf421cc7b9dd2322e921bc50c652f4",
        "title": "GiFT: Gibbs Fine-Tuning for Code Generation"
      },
      {
        "paperId": "181b4a18a17105c6c270ec533b96b519c9e28c42",
        "title": "Small Models Struggle to Learn from Strong Reasoners"
      },
      {
        "paperId": "ea73d330baf91ae6b00d401e652f69e19b442aa8",
        "title": "Diversified Sampling Improves Scaling LLM inference"
      },
      {
        "paperId": "c6e8297646a161facbd4c5ca708df87dfd79a607",
        "title": "Preference learning made easy: Everything should be understood through win rate"
      },
      {
        "paperId": "027beaab85bcc1572274350293c7449296c19705",
        "title": "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning"
      },
      {
        "paperId": "77a40f4441409e27038cd47f00b5f0b78aa6b9a0",
        "title": "AppVLM: A Lightweight Vision Language Model for Online App Control"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "ec0b78cd2d2ddbe69f0b487049e1754c9350b2ea",
        "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search"
      },
      {
        "paperId": "3ac976418e99aa5e558dedcb0ec25d6eb6c35750",
        "title": "STAIR: Improving Safety Alignment with Introspective Reasoning"
      },
      {
        "paperId": "e0a5df1b92981d8ae046e21e053ab1593fcc67f3",
        "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development"
      },
      {
        "paperId": "512439b94b682534d8afc3c7c6809fee087a19b1",
        "title": "Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "438c19fdd0f14f5082441db0c4183a50e7224b1b",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
      },
      {
        "paperId": "7ae48cdb8a044ccd22d279a01d135ecfd304d025",
        "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "54e1a79ef688b8f6462b6265fc803d9c3e90a72a",
        "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model"
      },
      {
        "paperId": "8d6411e337502f7fe0bfa59d486803a73d2c1192",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "paperId": "6439a3982e3bceb0ce76ffaaf5a601d838ddbc89",
        "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision"
      },
      {
        "paperId": "1fd282ff3a034ff6113f076b02769c46a7159476",
        "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning"
      },
      {
        "paperId": "34e94629224927355b163c17d2bd0ab3d874cafd",
        "title": "Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement"
      },
      {
        "paperId": "d65e47d3cfce21ff1e2b51f0b18efae61570ccd5",
        "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners"
      },
      {
        "paperId": "3a8192a2cea57217b15ec80c2dea66db56eb5238",
        "title": "Diving into Self-Evolving Training for Multimodal Reasoning"
      },
      {
        "paperId": "b98029d2a2dac2953533d8ef2724b1b4b85ef3d3",
        "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration"
      },
      {
        "paperId": "c2c43036d6679486bd3f273777b82b8c8bffb1cf",
        "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment"
      },
      {
        "paperId": "68e64ff720c2a6cc2a306aacbeb6f04320ad9805",
        "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "2db5d39e6acd826ac672f78961933242bad67de9",
        "title": "How to Synthesize Text Data without Model Collapse?"
      },
      {
        "paperId": "2e70bc1746833bb46853de7f47407509d54fc42e",
        "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models"
      },
      {
        "paperId": "5d373c79f1802e5c57d1d3f7a22c666ce10b29c9",
        "title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models"
      },
      {
        "paperId": "15894307dd6ca9a436288e9a954ec1fe793049d9",
        "title": "Towards Adaptive Mechanism Activation in Language Agent"
      },
      {
        "paperId": "2584aa70d6812193d0d4350ffe173cfb80cdb9db",
        "title": "VideoSAVi: Self-Aligned Video Language Models without Human Supervision"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "68376458e36f2142bb105ea76591924a154905d8",
        "title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models"
      },
      {
        "paperId": "6bd400acb88b37d1f1b46b2db16c0281cf2fa469",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "paperId": "6c6d2839ae5e79fa310eb1ea0d65af826f043f49",
        "title": "Textual Aesthetics in Large Language Models"
      },
      {
        "paperId": "628f204c7f136f5328a5d2a5ccd89d0b834c5637",
        "title": "DynaSaur: Large Language Agents Beyond Predefined Actions"
      },
      {
        "paperId": "131a13c60f179511572abc81d6bd6aa988e96854",
        "title": "Rule Based Rewards for Language Model Safety"
      },
      {
        "paperId": "e409ce659b1aaa70d6c6ea2c4891e50322138b7a",
        "title": "Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "9e7b26e36a68783b0eefc2099320f89e84991ba4",
        "title": "Vision-Language Models Can Self-Improve Reasoning via Reflection"
      },
      {
        "paperId": "2bc4fb0d2f185f0e24fa783c8187557583510ca1",
        "title": "Matryoshka: Learning to Drive Black-Box LLMs with LLMs"
      },
      {
        "paperId": "4a117153a63ab99d9c76a8f56b9ca047b77530b1",
        "title": "Tab to Autocomplete: The Effects of AI Coding Assistants on Web Accessibility"
      },
      {
        "paperId": "f037488648e7d75133bcce227beb278f5a22da5d",
        "title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning"
      },
      {
        "paperId": "9d9268b0191891511b09362759ba6a754c28fd9e",
        "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains"
      },
      {
        "paperId": "528ebaaace61d47559447c04a800a8a338fb4040",
        "title": "Automated Proof Generation for Rust Code via Self-Evolution"
      },
      {
        "paperId": "07748c2355f809bf2fa6e68908d8249916eaa795",
        "title": "Learning from others' mistakes: Finetuning machine translation models with span-level error annotations"
      },
      {
        "paperId": "9bc29b988ea02d669216e0d62724c30c26f7e94e",
        "title": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks"
      },
      {
        "paperId": "d62cd2c02309400756528ac55bbb18e2b51f3f42",
        "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning"
      },
      {
        "paperId": "e6b84c5390672d1efbbd718ae1b482a02a70f9de",
        "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines"
      },
      {
        "paperId": "2faaf795183358c80712492b9822d624e9b672c7",
        "title": "GraphNarrator: Generating Textual Explanations for Graph Neural Networks"
      },
      {
        "paperId": "cebabbad23715e03a988480cc1039f2606a3e172",
        "title": "Anchored Alignment for Self-Explanations Enhancement"
      },
      {
        "paperId": "275ca35b8adce1bb6238981f65e8b9f00c414a20",
        "title": "Advancing Large Language Model Attribution through Self-Improving"
      },
      {
        "paperId": "5d4513c5321cac93ec26e6ebc5d82e1f9b7c7b51",
        "title": "A Survey on Data Synthesis and Augmentation for Large Language Models"
      },
      {
        "paperId": "44a7150f6cc94e761737ef356561f8e27331e943",
        "title": "QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation"
      },
      {
        "paperId": "f820624b5738ae079cd83980143c958d5c58653b",
        "title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning"
      },
      {
        "paperId": "ebeb20304f0d0a255f5f087ddd99f66d7873d7ad",
        "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization"
      },
      {
        "paperId": "2fc8a207405ac233cde98424c36dd19c8731ce5e",
        "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"
      },
      {
        "paperId": "20c3bb12929f323b5d9b2a8b7d8b3e6bdfa0fa1d",
        "title": "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "3c81187fe776dab26834b8f833f54fdf127a6a43",
        "title": "Self-Boosting Large Language Models with Synthetic Preference Data"
      },
      {
        "paperId": "5e3e9034e281702f3802eeb4142f20ed137f65b1",
        "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation"
      },
      {
        "paperId": "3dff5d0be6a2b4249365a970e77af64e0b0fba2d",
        "title": "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics"
      },
      {
        "paperId": "58f614941629541c8c04acdb8acb9e3fb350ac5a",
        "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning"
      },
      {
        "paperId": "9e696551d3d0a80f3fc025aeb9c5209181450a62",
        "title": "Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance"
      },
      {
        "paperId": "55a0ebdc0b3fbc1f1e86e9975296d3c26f00f795",
        "title": "TypedThinker: Typed Thinking Improves Large Language Model Reasoning"
      },
      {
        "paperId": "817c2305b56fb8f2e1b1b7d55cd5057caa0bd2ed",
        "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "ad08f6474d005eb7382ef57ecdf5151b24f36285",
        "title": "Semi-Supervised Reward Modeling via Iterative Self-Training"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "3194d7f2335d6a7a9e7bc5c3761a60d1012ba6ac",
        "title": "EvoChart: A Benchmark and a Self-Training Approach Towards Real-World Chart Understanding"
      },
      {
        "paperId": "784915cae65585ebfcadeec6aa7c3b95629fe625",
        "title": "TSO: Self-Training with Scaled Preference Optimization"
      },
      {
        "paperId": "41a4234a11c2000f84fe5e0d6f9d60ec3d66a447",
        "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
        "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data"
      },
      {
        "paperId": "282058c7b1f07b878c154f3b4245992fd2bbb15e",
        "title": "Preference-Guided Reflective Sampling for Aligning Language Models"
      },
      {
        "paperId": "eebf5c6a5102ea7a782dc57aea5539da603e1f2f",
        "title": "Importance Weighting Can Help Large Language Models Self-Improve"
      },
      {
        "paperId": "cfec305bd6c8282e240c82d7623aa29a916a33b6",
        "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF"
      },
      {
        "paperId": "6cde8f9498d75b9069a2cf9bfbb6fa0f99a87c19",
        "title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions"
      },
      {
        "paperId": "c6f21c64c08295e595c82602c37f0bcac96d3907",
        "title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning"
      },
      {
        "paperId": "0291e9bf06c80904c26ebb3d4d8e2d981483b715",
        "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "682ff66a5ec0248f7e4a17a684b2d1e328e57f70",
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models"
      },
      {
        "paperId": "ed2247522162589fab9ce29f410b3a4b3440bd94",
        "title": "Towards Federated RLHF with Aggregated Client Preference for LLMs"
      },
      {
        "paperId": "0d58aecbf6f05babe447267ef3aa96e2ee9f3e61",
        "title": "LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives"
      },
      {
        "paperId": "706513890d7e2d821d3933cca6645a1949364d1e",
        "title": "Exploring Advanced Large Language Models with LLMsuite"
      },
      {
        "paperId": "17b0637c32a0b67b69f7c8e90146af37ca6f3789",
        "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning"
      },
      {
        "paperId": "63832d146f2a2706e563072cc4b59feecf4f4d01",
        "title": "PORT: Preference Optimization on Reasoning Traces"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "c45925acea6a94d881542acb47e43f8924d0b9d3",
        "title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving"
      },
      {
        "paperId": "a8489211b2fc9065b8e1ce62b6ec3815a32493e0",
        "title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "fcdac76ea896fc25a1ee890e5361cd9f7cbe2b41",
        "title": "From Text to Life: On the Reciprocal Relationship between Artificial Life and Large Language Models"
      },
      {
        "paperId": "3fb26c0cf930b04635540e4815c4b8ca0581155c",
        "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "c27e57b6e083b522afa9d202e6dd99ced29ff857",
        "title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions"
      },
      {
        "paperId": "4ef4bb74612fedb4933de2b463a7f1b588193f43",
        "title": "Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences"
      },
      {
        "paperId": "c42216140396a9fadafabb1d3d6782b54ff55273",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback"
      },
      {
        "paperId": "0e850d2081dd00ce9005d4603462858a5a174cf7",
        "title": "Language Models Resist Alignment: Evidence From Data Compression"
      },
      {
        "paperId": "3072ad4982cd916606ac88ea1883d4d725c4eda4",
        "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments"
      },
      {
        "paperId": "adabaf6fc304af494d6d5f37ec59ee640482801e",
        "title": "Aligning Agents like Large Language Models"
      },
      {
        "paperId": "3fa1e1c67514b9eaf9ec8da562baef8974b4f3f9",
        "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs"
      },
      {
        "paperId": "bae5e043025a6856637bdf6ebda2a6ac6c2ece30",
        "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents"
      },
      {
        "paperId": "bd73a558eab7e4108f133d9862feaf30cc30ebe2",
        "title": "Aligning Language Models with Demonstrated Feedback"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "e6912147e69a1177ad015131f1f5b139bd286c21",
        "title": "Reinforcement Learning from Experience Feedback: Application to Economic Policy"
      },
      {
        "paperId": "f984a1fc32b18253e93b1716ed8f7621f74d9d59",
        "title": "Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "357b7b1f0f5d92315de93272e45f458f875cc064",
        "title": "Bayesian WeakS-to-Strong from Text Classification to Generation"
      },
      {
        "paperId": "58bf4853effe89282eb8d8cd1e0dd1b782eee62f",
        "title": "LIRE: listwise reward enhancement for preference alignment"
      },
      {
        "paperId": "ffdc36d131defe10b8ede39548e74f3f215dd41c",
        "title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation"
      },
      {
        "paperId": "864024055b02a84dd9769a5fd5fce9f9ebd653f7",
        "title": "LLMs can learn self-restraint through iterative self-reflection"
      },
      {
        "paperId": "6e9736a35020a91c3639269a1b281257cdbce9cb",
        "title": "Archimedes-AUEB at SemEval-2024 Task 5: LLM explains Civil Procedure"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "38333f6e8f0388968edc4b2ea7a683ce69677e69",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      },
      {
        "paperId": "c946888e2f81b1db84ba4addf2a11e87f0568fe9",
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "7c0b751d782341504387b8fdcf26f842a9915061",
        "title": "Soft Preference Optimization: Aligning Language Models to Expert Distributions"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
        "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning"
      },
      {
        "paperId": "f94bb63b34c6a84a55d10472c6b07e4f25febbaa",
        "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "1b2e2a355916f61eea1f30fba1135509950118ba",
        "title": "A Survey on Self-Evolution of Large Language Models"
      },
      {
        "paperId": "c84f4710bcef432bb59783966d32c0897cc79504",
        "title": "UIClip: A Data-driven Model for Assessing User Interface Design"
      },
      {
        "paperId": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
        "title": "Many-Shot In-Context Learning"
      },
      {
        "paperId": "e6fa169323f1fe839e0bec21c520cf26c0514783",
        "title": "Self-Supervised Visual Preference Alignment"
      },
      {
        "paperId": "6bd8f3d617fa0c6e38eb0f042d20bc571dbcea96",
        "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning"
      },
      {
        "paperId": "49daaadfe84f1bf850feb8d6db46b652f75c9750",
        "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "5f40b8d3cd67d676150b3f962d294f593c982e1e",
        "title": "Exploring Text-to-Motion Generation with Human Preference"
      },
      {
        "paperId": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
        "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data"
      },
      {
        "paperId": "c93a21e063a1685558f18c9cd075947f1c1e660a",
        "title": "Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement"
      },
      {
        "paperId": "3bc23a232db48f454ebf27be39c189c9aa64af18",
        "title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
        "title": "Stream of Search (SoS): Learning to Search in Language"
      },
      {
        "paperId": "7c8851cce662351c49da94fa4512e2a6d2c1ace0",
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement"
      },
      {
        "paperId": "553c86ba967f55db9caffe08240061b1282da893",
        "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection"
      },
      {
        "paperId": "d144b521c91fed5b1a90debce18737a730c67d39",
        "title": "Agentic AI: The Era of Semantic Decoding"
      },
      {
        "paperId": "16837e4526ce8d6796811d35c406dfeed7b3eb4a",
        "title": "Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning"
      },
      {
        "paperId": "9a741f33aa4d782639e1f81a7e9c341b58b6ed2a",
        "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices"
      },
      {
        "paperId": "9ccb5de1e22238b93a6af01c1dc341dc9bc3f28d",
        "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision"
      },
      {
        "paperId": "b5bffe41155052a43010ec7197f832e81c546268",
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"
      },
      {
        "paperId": "77e3b253c2deeb2661ffcb9fb9f952ceb700c4db",
        "title": "SOTOPIA-\u03c0: Interactive Learning of Socially Intelligent Language Agents"
      },
      {
        "paperId": "fd50db0489828d0da2557a6da8d931d523479957",
        "title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "c78350e81298ca87bc1d59b466fa40081232caaa",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "paperId": "66e7edf09589527ebb58418632418758cee668cd",
        "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"
      },
      {
        "paperId": "97f05d277bac590cdcfe512b8adf647bc958d966",
        "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "f95da5b7be2fac2381eb5dfe26dc7dc5bc2d9a90",
        "title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents"
      },
      {
        "paperId": "9177fe8ad9ff7754c2fd27e805fe6ed882fe2954",
        "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling"
      },
      {
        "paperId": "8c785ebee1f34464dbc85ab4113bccafd7a74b0a",
        "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"
      },
      {
        "paperId": "668858489bbec3ce45f7a84a6a557b329f9ec91a",
        "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL"
      },
      {
        "paperId": "0ec88f8071d4a55e62a1b85661c1f11a01489047",
        "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "59347f86ce9af155266729e4b0301a29c65abf88",
        "title": "Approaching Human-Level Forecasting with Language Models"
      },
      {
        "paperId": "ac7c359ce903cdc104d82298537c61447d8cfe81",
        "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following"
      },
      {
        "paperId": "b392122a48d8b3212ee17074ff65f6b9df5c36c7",
        "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models"
      },
      {
        "paperId": "94db8a625418800c8ae7b48157a9cad1c8129051",
        "title": "A Survey on Knowledge Distillation of Large Language Models"
      },
      {
        "paperId": "4d41124771f982334a2168331c40e8074e9fe439",
        "title": "Enabling Weak LLMs to Judge Response Reliability via Meta Ranking"
      },
      {
        "paperId": "34d52d34e06122e30c207a8e38d4d911e6a2f012",
        "title": "Learning to Learn Faster from Human Feedback with Language Model Predictive Control"
      },
      {
        "paperId": "8c68c6f45d4f8af0536dd7a401fe9333eda7e1be",
        "title": "Aligning Large Language Models by On-Policy Self-Judgment"
      },
      {
        "paperId": "a4e3fcc394193737d9925fb74ddb520fa6045a6d",
        "title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses"
      },
      {
        "paperId": "1708b700eab634bd26e08663321e93687d0d1e03",
        "title": "Reward Generalization in RLHF: A Topological Perspective"
      },
      {
        "paperId": "9637ef9019671034912ea0f506ae67c3f2fc4689",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      },
      {
        "paperId": "4d4432514695e0f36720c73c23d15d8e21abe2fe",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models"
      },
      {
        "paperId": "0f6cd53c0cc1ee252433e0d37f419754e053b8a6",
        "title": "Suppressing Pink Elephants with Direct Principle Feedback"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "a8176838a636651324b9bac1b3443c803b44e1b3",
        "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model"
      },
      {
        "paperId": "4fe4f0f9d39d708a6c3d7b8dfbfa2616cd376e1e",
        "title": "V-STaR: Training Verifiers for Self-Taught Reasoners"
      },
      {
        "paperId": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"
      },
      {
        "paperId": "2fbcc6dc5a60642b7dc64d38a6d985ef489f2f33",
        "title": "Limitations of Agents Simulated by Predictive Models"
      },
      {
        "paperId": "0de191d3d94927ab4c68b7fa746ccbe0b120fb7f",
        "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay"
      },
      {
        "paperId": "4c2d8df556589ff4fbb5ee68c1f45bff3786624f",
        "title": "Aligner: Efficient Alignment by Learning to Correct"
      },
      {
        "paperId": "421fcb39c696ae07a9b7c93e43716b21f8e6184b",
        "title": "Distilling LLMs' Decomposition Abilities into Compact Language Models"
      },
      {
        "paperId": "6ac2856b0192ec307e349406270ec40a6f7e14a4",
        "title": "YODA: Teacher-Student Progressive Learning for Language Models"
      },
      {
        "paperId": "5b9fad3e2b2cc5dd23b01e0089bb7b6f6865cb82",
        "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model"
      },
      {
        "paperId": "a16372e000df6bcce7c2acf660dc81c7857784cb",
        "title": "West-of-N: Synthetic Preferences for Self-Improving Reward Models"
      },
      {
        "paperId": "67eab08db30e397e400e3b36b3afd7526df83314",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "7ca300c16abbd38382dec5b7ea6809fee570be54",
        "title": "ReFT: Reasoning with Reinforced Fine-Tuning"
      },
      {
        "paperId": "e21e5e9f87c3ac0c281264f4e8d94d15924591d6",
        "title": "Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation"
      },
      {
        "paperId": "dabf7edde0efb9b1e092aa27847a547cf2961192",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback"
      },
      {
        "paperId": "f5b077e01f6e3d91f58cb4ed7158fa61eec5a1f8",
        "title": "AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning"
      },
      {
        "paperId": "8223f81e8cc126b83d2774fe2da19ead290c144d",
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk"
      },
      {
        "paperId": "d0cc57eccbc9b2ee027cff4813701ae3e61aedbc",
        "title": "Evaluating Language Model Agency through Negotiations"
      },
      {
        "paperId": "cfcf8ab7c595c1849e8396167a29f3bd3359107c",
        "title": "Agent Alignment in Evolving Social Norms"
      },
      {
        "paperId": "5708f725e13362da80a1062f51df118fca3529ab",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"
      },
      {
        "paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      },
      {
        "paperId": "19df5eb2c74606414ed93633b4c61947cc42dbbb",
        "title": "Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "e35426fd81c78b044258cf419be6b7e5093b71c0",
        "title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "48362b169a235ca650918c489c8cea4c597da645",
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
      },
      {
        "paperId": "f2e1c7a7456a6196889950a58b6b8e46e6b94f80",
        "title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective"
      },
      {
        "paperId": "f5275c61736781d236abe6700b822f1ea62f982e",
        "title": "Diffusion Model Alignment Using Direct Preference Optimization"
      },
      {
        "paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26",
        "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"
      },
      {
        "paperId": "85a1f32e4794b4c176f3330364bc39977a50d258",
        "title": "Aligning Neural Machine Translation Models: Human Feedback in Training and Inference"
      },
      {
        "paperId": "6c6d2ac4f7c94b30ceef79ba3e72840d0f4ba1d0",
        "title": "Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding"
      },
      {
        "paperId": "abf1ddd4f48b2cee7e5c71ee4609f07209189c75",
        "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search"
      },
      {
        "paperId": "e51f20efb872d0ba99a8b501259948bbb2f6963f",
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"
      },
      {
        "paperId": "937205cf51b7ee46aa2983c129b7f5d596ea8293",
        "title": "Can LLMs Follow Simple Rules?"
      },
      {
        "paperId": "72883ceab65262f1e38ab1cd4262a326425488f1",
        "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models"
      },
      {
        "paperId": "20c6a1f647dd0e0ebdf81064f6bf69b7ff2153c0",
        "title": "Improving Compositional Generalization Using Iterated Learning and Simplicial Embeddings"
      },
      {
        "paperId": "795b3d3341c7c93daf316a59794f67a6c1c230f1",
        "title": "SoK: Memorization in General-Purpose Large Language Models"
      },
      {
        "paperId": "871b196639337f5d5b7da2ae9d747583a1bc484e",
        "title": "xcomet: Transparent Machine Translation Evaluation through Fine-grained Error Detection"
      },
      {
        "paperId": "f05c288caeb9a14ef387e6867934ced3d2200259",
        "title": "SALMON: Self-Alignment with Instructable Reward Models"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "6a0f1a8a03baba3e54a1a2ef348a1b0c2b8dff4b",
        "title": "B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "b1e2abc63630f26be54a1931041e0f4eeb0434e0",
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement"
      },
      {
        "paperId": "3c0810ba09cc128b197bb2003d9a80ed6b1d504e",
        "title": "Parameter-Efficient Tuning Helps Language Model Alignment"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "paperId": "a3ab8ff13f86beb677c315a1877b4be72a02d121",
        "title": "Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges"
      },
      {
        "paperId": "f719d0ba9ba035e2511d0c1a5448db755502df92",
        "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods"
      },
      {
        "paperId": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "paperId": "3de4c6545e535562a9b6770eac1c52513aa72694",
        "title": "PDFTriage: Question Answering over Long, Structured Documents"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
        "title": "Mitigating the Alignment Tax of RLHF"
      },
      {
        "paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b",
        "title": "Making Large Language Models Better Reasoners with Alignment"
      },
      {
        "paperId": "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04",
        "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"
      },
      {
        "paperId": "085ce7865e7492487f2f367f39d22a44bc64d151",
        "title": "Linear Alignment of Vision-language Models for Image Captioning"
      },
      {
        "paperId": "9b9874bdc10f5c61dfeb316efc892e0d4e503af6",
        "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "fb985e51608b7fa464ffa38eb3657b6c9f3d21bf",
        "title": "Self-Training: A Survey"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      },
      {
        "paperId": "060a1239ba7a6b3d210384d6a95fc243bcd8c8ad",
        "title": "Can Large Language Models Invent Algorithms to Improve Themselves?"
      },
      {
        "paperId": "3bee3eb0209c0d1d0c0ac5e6737a8fc642f34268",
        "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning"
      },
      {
        "paperId": "5798b423debb5b60dfcf4b0809c39844bb4f3378",
        "title": "Synthetic Documents for Medical Tasks: Bridging Privacy with Knowledge Injection and Reward Mechanism"
      },
      {
        "paperId": "5e71b2a8da2d3bcffa73ac0cf9f2bce1bf1c3925",
        "title": "Improving NMT Models by Retrofitting Quality Estimators into Trainable Energy Loss"
      },
      {
        "paperId": "b9568a63526799a59848a38203614335d627760e",
        "title": "Teaching Large Language Models to Reason through Learning and Forgetting"
      },
      {
        "paperId": "880c48200ec2b16129816fa91aa252f9c76598ef",
        "title": "AgentGym: Evaluating and Training Large Language Model-based Agents across Diverse Environments"
      },
      {
        "paperId": "47ac1fe29e81d2737a595af2cab39687b75f33b3",
        "title": "I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments"
      },
      {
        "paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a",
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "13283161c09960ffe0e37e19b0862d5222187e80",
        "title": "Generating Synthetic Documents with Clinical Keywords: A Privacy-Sensitive Methodology"
      },
      {
        "paperId": "a45d381a6803aa79c69b04936ad33d7284df8c91",
        "title": "Reflection-Reinforced Self-Training for Language Agents"
      },
      {
        "paperId": "2ac35475ccf0a6a89bbd04377a4fe61c175030a4",
        "title": "Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement"
      },
      {
        "paperId": "31ccf19d7c8e0d5f5ad485a0a9127eac9af941d3",
        "title": "Language Models Resist Alignment"
      },
      {
        "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
        "title": "Large Language Models for Data Annotation: A Survey"
      },
      {
        "paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0",
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"
      },
      {
        "paperId": "ab18f2caf0b3cd22368dbc1cdc598d6d40631f02",
        "title": "REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset"
      },
      {
        "paperId": "7b88fb2e94a42c5f16c27d744ea5d04eb1f8069b",
        "title": "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives"
      },
      {
        "paperId": "9a50a73562b5569e7e62668c1c5eba4c6a1d45f1",
        "title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "29083f7d40c7c9f4e490a8871d80e4f0fe18706f",
        "title": "Accelerating Greedy Coordinate Gradient via Probe Sampling"
      },
      {
        "paperId": "8640405ec36df751a855349980e4cd321c61bf6f",
        "title": "Towards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout"
      },
      {
        "paperId": "e51a7528b9f6ec2101a682ebde818e40f96c8b24",
        "title": "Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "18e306e7926daa42aea393b91d518eadf032b3f7",
        "title": "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback"
      },
      {
        "paperId": "c3703dda182472a8005c57053fc5207c725cfb75",
        "title": "Offline RLHF Methods Need More Accurate Supervision Signals"
      },
      {
        "paperId": "980447be8a8b7f1247463dac4d13c88d43dd2c58",
        "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima"
      },
      {
        "paperId": "e4435f282266da92d37066064c5239c6f96f0d64",
        "title": "Gibbs Sampling from Human Feedback: A Provable KL- constrained Framework for RLHF"
      },
      {
        "paperId": "2d7a14fabe2631a205e65bcac94d8e84d883492f",
        "title": "Enable Language Models to Implicitly Learn Self-Improvement From Data"
      },
      {
        "paperId": "ac35e13321b3fd108b1a427964872514ea3c3eb7",
        "title": "The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges"
      },
      {
        "paperId": "5b50d0c86bc232f83f29cf1c7b9df5b46850c110",
        "title": "Reward Model Underspeci\ufb01cation in Language Model Alignment"
      },
      {
        "paperId": "951a11a4d247de5823dfa42affe39cf849c708b8",
        "title": "Pairwise Proximal Policy Optimization: Large Language Models Alignment via Comparative RL"
      },
      {
        "paperId": "4d7294c62913a1a2ce651212f8fbb20baf938227",
        "title": "STA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "8564e17fa4ec36c687de1355390222c3e6e2d47a",
        "title": "LLM Alignment Through Successive Policy Re-weighting (SPR)"
      },
      {
        "paperId": "d47123f3ef330b65d34b1cc74f95fc983f62eb74",
        "title": "S ELF -I MPROVING T RANSFORMERS O VERCOME E ASY - TO -H ARD & L ENGTH G ENERALIZATION C HALLENGES"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      },
      {
        "paperId": "1ff56fccf9d09b7dc7722aba4c57484dab338a9e",
        "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning"
      }
    ],
    "score": 169.0
  },
  {
    "id": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
    "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint",
    "authors": [
      "Wei Xiong",
      "Hanze Dong",
      "Chen Ye",
      "Han Zhong",
      "Nan Jiang",
      "Tong Zhang"
    ],
    "year": 2023,
    "citationCount": 249,
    "abstract": "This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.",
    "url": "https://www.semanticscholar.org/paper/44a9d8b0314d34aff91ccff9207d38eed37216ed",
    "pdf_url": "https://arxiv.org/pdf/2312.11456.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2023-12-18",
    "externalIds": {
      "DBLP": "conf/icml/0015DYW0J0024",
      "ArXiv": "2312.11456",
      "CorpusId": 266359219
    },
    "references": [
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f2f9cc38d5a1a4fdef996ea03fdb71e01f65d574",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "b1e2abc63630f26be54a1931041e0f4eeb0434e0",
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement"
      },
      {
        "paperId": "860c8de4fdac38695ff6860dd15312f1079c6117",
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "e96d3f85aa56f027e028189346e043e346f3acea",
        "title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models"
      },
      {
        "paperId": "370e51386abb7b999728e08b74f0a77fbd064834",
        "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cedfdde4b9d01530bf2932554561bb25623890e5",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "018e943ba0452b05edd903c3eaf746068ebca138",
        "title": "LeTI: Learning to Generate from Textual Interactions"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "bfce86bcdce722c175cd07c8cb0203a7bb0b5711",
        "title": "A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes"
      },
      {
        "paperId": "fbe1003ec391f6bcf4660f6ef81f1e6199849bfe",
        "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "f1398927e7f78295f277ee2e0b437e26a04ec1c6",
        "title": "Adversarial Model for Offline Reinforcement Learning"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "88ae720354fa4883be13e25e1cf92dcd4fed5f5d",
        "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning"
      },
      {
        "paperId": "4d81c33b295c092016ac236cfd32020a5bb70b97",
        "title": "Optimizing Prompts for Text-to-Image Generation"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "4724ae0869b4cb65a741399489cb577a3373ada7",
        "title": "Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes"
      },
      {
        "paperId": "4b04354e35e558c362ef36cda266f46074158b44",
        "title": "VOQL: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation"
      },
      {
        "paperId": "aef62643b561991d7db70ff29b009822065641d8",
        "title": "On the Sensitivity of Reward Inference to Misspecified Human Models"
      },
      {
        "paperId": "49bbd1064f1a17f314f8c61528bcce959a7c249b",
        "title": "GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "49daab987dcc0aeb82c0986fb2155ce0b7970623",
        "title": "A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games"
      },
      {
        "paperId": "f0f3e3cccb4e37d71bd44d3eb78c4a2f223597c9",
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "bb4c13c86048a48ec3f398d7d613ff8e56e1c8ae",
        "title": "Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "6be327602deb674d0e9f3b606f3e6baf733bf266",
        "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "dc94a18c69c71434590299fad9aeb2c932f45e15",
        "title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "05ed6ab7fbb8feba2c9b390d398e97b9857bd870",
        "title": "Fast Rates in Pool-Based Batch Active Learning"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
        "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "1e00e8760472dab0fe1632a04a037d52d227d3a6",
        "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning"
      },
      {
        "paperId": "eb46163d767f7de1d3de725909b73d04a1eefd68",
        "title": "Towards General Function Approximation in Zero-Sum Markov Games"
      },
      {
        "paperId": "e2ad21dae85950ab3631f65a0f142924c99fb9c4",
        "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning"
      },
      {
        "paperId": "d769ca62d90adc7e7869849a421426bdc54a32fb",
        "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning"
      },
      {
        "paperId": "43d0f879f7ae59d9bf7b6275905a928a74224dd7",
        "title": "The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces"
      },
      {
        "paperId": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "188e4e97c8ac1e25b61904078fb9bed1e6eb6f2a",
        "title": "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "b214c676da7241453631061b2591b30722f02bcf",
        "title": "A Framework of Composite Functional Gradient Methods for Generative Adversarial Models"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "e4609613a9efb85103424abb0cb15e814464ccd9",
        "title": "Understanding Learned Reward Functions"
      },
      {
        "paperId": "af252daf6ab49b3df9901f99af5a67ff3478edcd",
        "title": "Batch Value-function Approximation with Only Realizability"
      },
      {
        "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
        "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees"
      },
      {
        "paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634",
        "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "62c1bc6a8bffe09a1e1138ffd37a64988dc27d48",
        "title": "Optimistic Exploration even with a Pessimistic Initialisation"
      },
      {
        "paperId": "5fa14f5d09031736a70bca9675f888dbeab644a3",
        "title": "Improved Optimistic Algorithms for Logistic Bandits"
      },
      {
        "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
        "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "97cd86d8d8c0f27cd3e64c6ca5cfdeb957ee39f4",
        "title": "Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "fa2c136d526d7445b02e802ff9601ea9e9663fcb",
        "title": "Eluder Dimension and the Sample Complexity of Optimistic Exploration"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "0b14178e7d79ac426d0a39700e1ac8b2c6f2e752",
        "title": "Convex optimization"
      },
      {
        "paperId": "de07a63b458eef61fdddd4217231aeb6d24751a8",
        "title": "Linearly Parameterized Bandits"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "bcc91889adc389cbe25295961c07a3484225ee7b",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "14c3cf58192774b9b6fc6188df99efd6ab5fc739",
        "title": "Better Aligning Text-to-Image Models with Human Preference"
      },
      {
        "paperId": "46b4cf516bb32f22b8061fdd750dee84969aaeab",
        "title": "Optimistic Exploration with Backward Bootstrapped Bonus for Deep Reinforcement Learning"
      },
      {
        "paperId": "d39a902effe6873746868ca44860be8f0d13ff8b",
        "title": "Optimal Algorithms for Stochastic Contextual Preference Bandits"
      },
      {
        "paperId": "9d2714b20d3bca952403be13b1c69b86004f91dc",
        "title": "Implicit Generation and Modeling with Energy Based Models"
      },
      {
        "paperId": null,
        "title": "Rein-forcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA"
      },
      {
        "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
        "title": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": "551e19e5113cdff60a3c545d684fc4b9eb9a7306",
        "title": "Stochastic Linear Optimization under Bandit Feedback"
      },
      {
        "paperId": "7066363968c68743b37096b122524501dafb2cdf",
        "title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information"
      },
      {
        "paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce",
        "title": "google,\u6211,\u8428\u5a1c"
      },
      {
        "paperId": null,
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and analysis of the multiarmed bandit problem"
      },
      {
        "paperId": null,
        "title": "Various techniques used in connection with random digits"
      },
      {
        "paperId": null,
        "title": "Openllama: An open reproduction of llama,"
      },
      {
        "paperId": null,
        "title": ": Ensembling large language models with pairwise comparison and generative fusion"
      },
      {
        "paperId": null,
        "title": "Preference tuning llms with direct preference optimization methods"
      },
      {
        "paperId": null,
        "title": "Multi-turn interactive evaluation for tool-augmented llms with language feedback"
      }
    ],
    "cited_by": [
      {
        "paperId": "cde02dfa5c320ba84050176a4f97722704bc02d4",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense"
      },
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "f424beba20f4aecf24dc3225806cb8712bccf380",
        "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization"
      },
      {
        "paperId": "51fa6682984b9a8b3085cdd3d2918444317ef3ee",
        "title": "Alignment-Aware Decoding"
      },
      {
        "paperId": "6cd762a0856b904dc27cc450711ae48fbc93e784",
        "title": "Fading to Grow: Growing Preference Ratios via Preference Fading Discrete Diffusion for Recommendation"
      },
      {
        "paperId": "498a98386d007a2fdb88fabcd5f13b1f972697fc",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression"
      },
      {
        "paperId": "530812807335ae9d4e3666a748c01203c8f6d755",
        "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling"
      },
      {
        "paperId": "dacb8fd9a524eca1ec31e71a7b67d40e50c556cf",
        "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs"
      },
      {
        "paperId": "d162c0f1682ca88fdc07bdbfa429b245ee51f4d7",
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment"
      },
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "3a6dc3516c20a45062564e67d742a567674865bf",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization"
      },
      {
        "paperId": "f55bf9a4262ca00b72345ee95b0b1ad65b907e95",
        "title": "Multiplayer Nash Preference Optimization"
      },
      {
        "paperId": "435de15da2ce37a97e0f2731beb10ff9765746d5",
        "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "41ef22a984b4f345743f21917f269226aa4ea429",
        "title": "T-TAMER: Provably Taming Trade-offs in ML Serving"
      },
      {
        "paperId": "ed68534d5a8da7b097b1a6ec4ddb7b24a185a361",
        "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "59c6403b4445fcdc4e0cb918cb7aa080206432ba",
        "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences"
      },
      {
        "paperId": "b3b13107dd153f4de1ddfeec0a96998d70caf04b",
        "title": "Preference Robustness for DPO with Applications to Public Health"
      },
      {
        "paperId": "a51c25e7e7902237ec2c2e21815f1ec5219c0535",
        "title": "Indirect Online Preference Optimization via Reinforcement Learning"
      },
      {
        "paperId": "0544547cb6d932312de0ca5e3f2010f163e33262",
        "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "2d81e4362f0a9d1c1410bbb2833bc7d9105d242f",
        "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention"
      },
      {
        "paperId": "94af13c6ad98ba03ed61e745e372c786ebbd20b4",
        "title": "Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression"
      },
      {
        "paperId": "10f5a33793b39fc7d2cd53c707486b749e51cbda",
        "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap"
      },
      {
        "paperId": "82ea274f6e2394d8f671a245995b535e30431945",
        "title": "Generative AI Meets Wireless Networking: An Interactive Paradigm for Intent-Driven Communications"
      },
      {
        "paperId": "16389181178b4d1795fc07d9e6e276a1fc0071d2",
        "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal"
      },
      {
        "paperId": "d33bd07c39b1e1c1907439a17ce6b35217ca2d41",
        "title": "Libra: Assessing and Improving Reward Model by Learning to Think"
      },
      {
        "paperId": "f8aa4488fe95ba15123f9ea21992c880610f7f90",
        "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models"
      },
      {
        "paperId": "5f8582f9b78cd78360aee5496ec9a0f9dc05acb2",
        "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "02dac7990c1b934e50a70e21560ceb3b62eba96d",
        "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling"
      },
      {
        "paperId": "c1a78f39b367aaeb5dda88ae93f1ba4cecdf6a4e",
        "title": "Bridging Offline and Online Reinforcement Learning for LLMs"
      },
      {
        "paperId": "ce4a35fe58b5f185221eb20a1ce96c1b7955b754",
        "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards"
      },
      {
        "paperId": "264e38ebc1bfa24c0eefb17c9d8e7cfb11b1569f",
        "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems"
      },
      {
        "paperId": "1c084d3d922458135d867d202bc62406e0e2d39b",
        "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach"
      },
      {
        "paperId": "0933fc39e13991fc9650481f7467b8061423ee87",
        "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training"
      },
      {
        "paperId": "1da4c54174a7ba8bf5559930d7d2c9a3a80bfac3",
        "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization"
      },
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "21abbc6a6edde1b8bc2ceb7b7a7564a5a8e34c85",
        "title": "Reinforce LLM Reasoning through Multi-Agent Reflection"
      },
      {
        "paperId": "c4d65d124c3ceeb7369425954516b1d7c6ffc1ab",
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization"
      },
      {
        "paperId": "23678c54e17e016fe1d9c61157aab7c3a274ec7d",
        "title": "Boosting LLM Reasoning via Spontaneous Self-Correction"
      },
      {
        "paperId": "3d21dab6253c86ce73a43ef1eb578179f21b7abb",
        "title": "Debiasing Online Preference Learning via Preference Feature Preservation"
      },
      {
        "paperId": "fe54c92cb5f7c2c68847354af8770dd8df5f7088",
        "title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection"
      },
      {
        "paperId": "828ab065d15c9516c633b54e3245ec08cc75a2f8",
        "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "25683b5340a7856fdfd04220a0371e6de9cfa066",
        "title": "Understanding the Impact of Sampling Quality in Direct Preference Optimization"
      },
      {
        "paperId": "3fae1aae2f8e70ae0b706a237ab7da721ed33ad6",
        "title": "World Modelling Improves Language Model Agents"
      },
      {
        "paperId": "e9e22a099d1e065c027a3bced40d7fd61f9f9e65",
        "title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model"
      },
      {
        "paperId": "fc450c5aa4c85224e9944e65daaccc971470113d",
        "title": "Thompson Sampling in Online RLHF with General Function Approximation"
      },
      {
        "paperId": "36540ee20aa4ba4f02f7359e1113cadbefbf1b6d",
        "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy"
      },
      {
        "paperId": "03e385b1a6fdb7bef5612feea44e44d5efd45bcc",
        "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression"
      },
      {
        "paperId": "2cbcd61bf8b994c96ada959e06a311e3e1c2d2d3",
        "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback"
      },
      {
        "paperId": "b29ff2491b0365f1e3862c43197860eac9962db3",
        "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "9709a69c947c561e5792ef843d20052fdb731ffc",
        "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective"
      },
      {
        "paperId": "8443d3b607b3f30ace2449a4df1d55976dd16a37",
        "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data"
      },
      {
        "paperId": "970775f59013ff6198a1a684dfe819c784d99d42",
        "title": "Online Knowledge Distillation with Reward Guidance"
      },
      {
        "paperId": "25c8a4df271129fb59d81fe06e35fdb220f7145f",
        "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics"
      },
      {
        "paperId": "4ef740de1c70ad84139b6afd9dc406ded7679c71",
        "title": "Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ca889669f46a090fee32263147d91da77ad368ff",
        "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF"
      },
      {
        "paperId": "29f6ef611eff29c47759a246eed8263fc98da537",
        "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning"
      },
      {
        "paperId": "15f0ebcfef190e6aeac25e08e71e320be0263696",
        "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning"
      },
      {
        "paperId": "bdf6334fb878f90b71b325223cc904c3b7aad0c3",
        "title": "Reward Model Overoptimisation in Iterated RLHF"
      },
      {
        "paperId": "e2d64cacc9639ab176425055550c6b2139348942",
        "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO"
      },
      {
        "paperId": "2c4ca86d4023c85cd5066873f1f889aeaaab9b28",
        "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?"
      },
      {
        "paperId": "0fb758ba4d05604e263b2c7488e5b51c5c15b240",
        "title": "Fractured Chain-of-Thought Reasoning"
      },
      {
        "paperId": "1bf20b99c46de408e5c2edfd6af9f62c63174f88",
        "title": "Is Active Persona Inference Necessary for Aligning Small Models to Personal Preferences?"
      },
      {
        "paperId": "ffed0c2f6084c6eed1cd732ffcf710c09249fc47",
        "title": "Online Iterative Self-Alignment for Radiology Report Generation"
      },
      {
        "paperId": "68cd9e02bd17961d4b864696edfae82f8256bb4c",
        "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge"
      },
      {
        "paperId": "2a8f57ed823a84a139113cd9c5c7fea43c449546",
        "title": "Mutual-Taught for Co-adapting Policy and Reward Models"
      },
      {
        "paperId": "177d189e0e939d3afd56569ac908b5d88e0c67f9",
        "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models"
      },
      {
        "paperId": "7d47a1672c2f297f167eda7fe6571a788091ac2a",
        "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO"
      },
      {
        "paperId": "340da3e43e49943f93f0a208b12c0ac71a842c01",
        "title": "Improved Algorithms for Differentially Private Language Model Alignment"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "78ad49d8e911cb5549c50cf77fd92febd144ba6e",
        "title": "Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment"
      },
      {
        "paperId": "f1173df25d83e5c6ff16e087f890993bccf355ed",
        "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
      },
      {
        "paperId": "bc4b173d49b4dcac8ebd6329fa5b48db954e50f1",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning"
      },
      {
        "paperId": "3d58168c202c95ee932c1913f445b3f5efe0f75e",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL"
      },
      {
        "paperId": "ad9eed80bca2b208e60d9648c30f712285498151",
        "title": "Large language model-based planning agent with generative memory strengthens performance in textualized world"
      },
      {
        "paperId": "92f2e1a17cbff22164611a2c8a908afe71b4b626",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models"
      },
      {
        "paperId": "40455a044a750e367e0c225cea74bf078ca73c95",
        "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis"
      },
      {
        "paperId": "1523caee9abc4ffc86557089bc494ef0ba12e7a0",
        "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback"
      },
      {
        "paperId": "d997a240cb508b2dbc3b8286b9be673ac6745570",
        "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models"
      },
      {
        "paperId": "e6accc69f64ff38d85388b4fca7d10c79920274b",
        "title": "Compass-V2 Technical Report"
      },
      {
        "paperId": "fe56f9471e9993b032f233aadca4fdc42d7bad05",
        "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits"
      },
      {
        "paperId": "1617feeff3f5cc67af103542e01a4ba526957ab3",
        "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward"
      },
      {
        "paperId": "7874c6e5d7313a98b832918d5cb7f1b10ffea4fb",
        "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo"
      },
      {
        "paperId": "29d0ed9caf811212341f67617d8cbd8c1fa2cc25",
        "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce"
      },
      {
        "paperId": "ca7f46ee0d49c1522b90033670152b9b096797cf",
        "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores"
      },
      {
        "paperId": "790689cbe7d44fa50f67238ae0c408a04426be0a",
        "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients"
      },
      {
        "paperId": "66c16a4eb1457f447a44fb1ea1968f8841ad5a2d",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
      },
      {
        "paperId": "fc0f813c9918ec0c2f2582518387786ea1f0f8e6",
        "title": "Entropy-Based Adaptive Weighting for Self-Training"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "76509b17bc5582137720ebe2c3a6b661a2804382",
        "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF"
      },
      {
        "paperId": "21c274d953b68e1d1793e23af7e70c9314a9dd1b",
        "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization"
      },
      {
        "paperId": "836d035ff15ae4dfa500f07fc119cdc4709cc212",
        "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation"
      },
      {
        "paperId": "295089ef50ce22ec05cc149bfe494411c50512b0",
        "title": "Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "5b58cfebd95ba503cdf26a8021f4e7ca0893351d",
        "title": "Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback"
      },
      {
        "paperId": "9e90796e971a31f10be4a23ddff894f4fef8243b",
        "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models"
      },
      {
        "paperId": "038a85b0eee75ac361869df8eadf0b711f9370e7",
        "title": "RePO: ReLU-based Preference Optimization"
      },
      {
        "paperId": "8a834ec7f1d2c428e244c288eb5e9fe31912dd61",
        "title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality"
      },
      {
        "paperId": "b497c67dd697db90127e5743eacb5734f165b310",
        "title": "SHAPE : Self-Improved Visual Preference Alignment by Iteratively Generating Holistic Winner"
      },
      {
        "paperId": "d71ef7ee9d08df6a3766c99314df09fbd9f54b5b",
        "title": "Iterative Value Function Optimization for Guided Decoding"
      },
      {
        "paperId": "c6e3855c889b46997f687544d7778a4cd108be09",
        "title": "Gradient Imbalance in Direct Preference Optimization"
      },
      {
        "paperId": "35b9b9404695ec566f554cc2138ee60e9c45e7a9",
        "title": "Kanana: Compute-efficient Bilingual Language Models"
      },
      {
        "paperId": "653546ec8e98c4037fe56277eaff8666882eaadd",
        "title": "Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "7324c2d0e82742f3299e2ec4617a253318e6ece3",
        "title": "Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization"
      },
      {
        "paperId": "ad175c047e85f9badeabbc6fe8e578a86e502022",
        "title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models"
      },
      {
        "paperId": "3404f2051763368df1613e3d9b7b787cb4a8b833",
        "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?"
      },
      {
        "paperId": "93679036b6eb4293d42683940ce80aa589c9d321",
        "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent"
      },
      {
        "paperId": "11313ae3f1745da638fa1852d020a18417a13bbf",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch"
      },
      {
        "paperId": "7c69de6b1108ce03ba32ca9a10c36925187a8160",
        "title": "RSPO: Regularized Self-Play Alignment of Large Language Models"
      },
      {
        "paperId": "0ec6a9659d50bfdb32e75e1b9c85372f67bde349",
        "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment"
      },
      {
        "paperId": "fcde368302111abbcefaad3f67510002c9f29321",
        "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization"
      },
      {
        "paperId": "f400b0d5724203633f56299cb19f34bf18fd31b9",
        "title": "Rejected Dialects: Biases Against African American Language in Reward Models"
      },
      {
        "paperId": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
        "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
      },
      {
        "paperId": "6c306b9b3dfe5976a750362b7d569d60af640998",
        "title": "Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits"
      },
      {
        "paperId": "0f11244a4290d31c80e744dc7207fa8d18bac016",
        "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation"
      },
      {
        "paperId": "1ea031bb17ea54edff9ea66599e50c0f72d2ce1e",
        "title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "7a06ca5e148c6362d982f40388d3919ba15bf9c0",
        "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective"
      },
      {
        "paperId": "c9511a769eeb6668893ce108ed91e6f14d551556",
        "title": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models"
      },
      {
        "paperId": "ffbc6126817f509f218fe661aedfe1457f0383ac",
        "title": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals"
      },
      {
        "paperId": "a2a5ea730b7d0ef7653060595a021360be4ad57f",
        "title": "LLM Safety Alignment is Divergence Estimation in Disguise"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "87380d3b0cc72badb9e4a417b20497199a551822",
        "title": "Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective"
      },
      {
        "paperId": "79cc7cb576b7eed9e7cf0d6a92fe36105fa7aad8",
        "title": "Data-adaptive Safety Rules for Training Reward Models"
      },
      {
        "paperId": "feac701797be018483f6eb2286c1169c03d6b019",
        "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training"
      },
      {
        "paperId": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
      },
      {
        "paperId": "f520c3faaa8e665c019995d01ad3fde80d3c0be2",
        "title": "Entropy-Regularized Process Reward Model"
      },
      {
        "paperId": "980bbe792855b811efae0311b149d2f4030a5d58",
        "title": "Hybrid Preference Optimization for Alignment: Provably Faster Convergence Rates by Combining Offline Preferences with Online Exploration"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "93ca54fa0fbf2dae113e5433154ffb49eadecd04",
        "title": "Yi-Lightning Technical Report"
      },
      {
        "paperId": "ae2fa2af2da2c3e96e87af00887b4938e48be79c",
        "title": "Preference Optimization for Reasoning with Pseudo Feedback"
      },
      {
        "paperId": "944fade485948cc43a1bd52ae5849d81939a6981",
        "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"
      },
      {
        "paperId": "6bd400acb88b37d1f1b46b2db16c0281cf2fa469",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "paperId": "117cd296518ae3aae104cba284251875ef507d48",
        "title": "Towards Improved Preference Optimization Pipeline: from Data Generation to Budget-Controlled Regularization"
      },
      {
        "paperId": "c759d61941a5b4f62531559ceac580e089965c55",
        "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization"
      },
      {
        "paperId": "6e220dff22a40acb83c5af920fe3ad5d71e3cc83",
        "title": "Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of Human Annotation in Preference Tuning"
      },
      {
        "paperId": "8430ea2921172c7c07e5286f19ffdf4a43d1d4b0",
        "title": "Sample-Efficient Alignment for LLMs"
      },
      {
        "paperId": "99d59b4ec0a2b0860da49f11fe698282af43dff8",
        "title": "Towards Reliable Alignment: Uncertainty-aware RLHF"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "6e31b451f0d06c59b142537626375a5579533495",
        "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following"
      },
      {
        "paperId": "d9390efd59d6d8d1eb1e19967b3ab4fc96cb109d",
        "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online Feedback"
      },
      {
        "paperId": "92c82a51ad13c361d052987694cf93d6a72d5789",
        "title": "CycleResearcher: Improving Automated Research via Automated Review"
      },
      {
        "paperId": "f09e51ae8cc08b3df6544a94faea601d463439af",
        "title": "Uncertainty-Penalized Direct Preference Optimization"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "89b42dc02400db2c58703fb0d8d0b6f0210bfe7f",
        "title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning"
      },
      {
        "paperId": "864b530b17373aa4e6f09a00614cf2f6c2be707d",
        "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling"
      },
      {
        "paperId": "efae534e2a3e4b79fe4050fa3fd40d8c7ba9745b",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      },
      {
        "paperId": "9e91b085852906decb4052ea198dd81a73ed6894",
        "title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization"
      },
      {
        "paperId": "f820624b5738ae079cd83980143c958d5c58653b",
        "title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning"
      },
      {
        "paperId": "ebeb20304f0d0a255f5f087ddd99f66d7873d7ad",
        "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "008a858ed75c50af068709c77c273ea98ea51fe4",
        "title": "DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback"
      },
      {
        "paperId": "9bdce8ff12b2546c52363139b24806bf543c9f25",
        "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF"
      },
      {
        "paperId": "5e553317596d37b6438441a38cfe3562eed4d374",
        "title": "RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization"
      },
      {
        "paperId": "f0eb84e2a2aa5a30de92f4ce85c8ddaa6527c45a",
        "title": "Aligning LLMs with Individual Preferences via Interaction"
      },
      {
        "paperId": "d05391825371fdd6f4619f47c0704a5bd4b562c8",
        "title": "Learning Code Preference via Synthetic Evolution"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "158730a16cd4cc4d4842e42e5c2a0843b75d527c",
        "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data"
      },
      {
        "paperId": "3ea2a38eb00af88a7bf775781007396f2f5b6c1f",
        "title": "The Crucial Role of Samplers in Online Direct Preference Optimization"
      },
      {
        "paperId": "7c8ad9293ad11527090503be4b3ba156cf73b06a",
        "title": "Just say what you want: only-prompting self-rewarding online preference optimization"
      },
      {
        "paperId": "e87604639b3ec9bde72d88fce3f762b28a1c39eb",
        "title": "Inference-Time Language Model Alignment via Integrated Value Guidance"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "1acc966b193be4eca4849d132ce3208a80d1c6fe",
        "title": "Training Language Models to Win Debates with Self-Play Improves Judge Accuracy"
      },
      {
        "paperId": "2b32f9b686e4ec64e72ee0ef6679fc044ee734d2",
        "title": "Orthogonal Finetuning for Direct Preference Optimization"
      },
      {
        "paperId": "6d3a7b453048673a98b082a73bc864366fbd1cf4",
        "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "1158310f204db29513bd8322f7010f0ca78d69be",
        "title": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do"
      },
      {
        "paperId": "ab83eae00db6a19de1ebdb23f7ceed29b5832675",
        "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement"
      },
      {
        "paperId": "8096ca5f6895955dc41f05094f976b76419437fd",
        "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison"
      },
      {
        "paperId": "781d3c7fe990303f56f5da39eba3253f152e3505",
        "title": "Multi-Type Preference Learning: Empowering Preference-Based Reinforcement Learning with Equal Preferences"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "711774dbe2d14045c4df37b57b324d18e0fe5572",
        "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "898dd545628758bf622954ee43450bcf8c58f22c",
        "title": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "cce16fd47ebebd51f27c449843415064ad70a5b0",
        "title": "ProFuser: Progressive Fusion of Large Language Models"
      },
      {
        "paperId": "21cdb3aced0e15d8581f3f58ea836b4f379a06fa",
        "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law"
      },
      {
        "paperId": "928b3ef966cb0e1e9cff7e5e96d5df23c47c2d5a",
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift"
      },
      {
        "paperId": "0b9adbe01131857fd56db2b42b196a149fab778e",
        "title": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More"
      },
      {
        "paperId": "9ddda210b622e0127a43dd5d7cd02928d449b799",
        "title": "Improving Context-Aware Preference Modeling for Language Models"
      },
      {
        "paperId": "1281f7cbab728c2aa89f0a1cac925992f64eb2e3",
        "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism"
      },
      {
        "paperId": "3c3237df4b09dc5a21f7bc2b917f72ccbb6cf863",
        "title": "Learning Dynamics of LLM Finetuning"
      },
      {
        "paperId": "f52627fe3c6904928f24afdfa48eae603dd57522",
        "title": "PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods"
      },
      {
        "paperId": "dc47abf03e644afa0bdc834bab11018d3804e3c8",
        "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators"
      },
      {
        "paperId": "aa647ee4e050ccbb53e77444f5e7a96be9ca7ce8",
        "title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning"
      },
      {
        "paperId": "17b0637c32a0b67b69f7c8e90146af37ca6f3789",
        "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning"
      },
      {
        "paperId": "8b89036b5136b081867fbec089801b2bd4ab0685",
        "title": "Decoding-Time Language Model Alignment with Multiple Objectives"
      },
      {
        "paperId": "13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      },
      {
        "paperId": "2257d0894da128e9c900df1237f43504c9c0e82e",
        "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "4eae5af6597134c6190d72a9ad9c7c29c92c8a1e",
        "title": "Style Transfer with Multi-iteration Preference Optimization"
      },
      {
        "paperId": "ccf91d44ddf11e495eff2029ba18cd2b1d28eef8",
        "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level"
      },
      {
        "paperId": "fd655992d1b0220e16004ca39774e9390fb28cee",
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models"
      },
      {
        "paperId": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "paperId": "cab3d65b3f4d0a4169d0fdaaed15af6be1d6bb84",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      },
      {
        "paperId": "5387445a58a958422a8cfd297e6a611aade0f0e8",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      },
      {
        "paperId": "79e41008d1837acc27029d7c5061898385c5ea3c",
        "title": "OPTune: Efficient Online Preference Tuning"
      },
      {
        "paperId": "86905d23c2d1c9386f3eb063132c14119ed79e5c",
        "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases"
      },
      {
        "paperId": "f14f24d30e8cfd0c8745495de19fb1e253304884",
        "title": "Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment"
      },
      {
        "paperId": "0e850d2081dd00ce9005d4603462858a5a174cf7",
        "title": "Language Models Resist Alignment: Evidence From Data Compression"
      },
      {
        "paperId": "a264dbba6c16a460998cd16e4f48d62a1eddd4d0",
        "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment"
      },
      {
        "paperId": "3a10570fb3f88d755ba4648598b3d30f8ddfb26c",
        "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "f984a1fc32b18253e93b1716ed8f7621f74d9d59",
        "title": "Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment"
      },
      {
        "paperId": "8053f1079767d3d6cc6ffd90c885b513acc6af5f",
        "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models"
      },
      {
        "paperId": "e96c41933f4b1f0e79449451395e6ed0e6644133",
        "title": "Preference Alignment with Flow Matching"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
        "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization"
      },
      {
        "paperId": "0a7dd136b2f69d7b74a57dc2f220b31946f7234a",
        "title": "iREPO: implicit Reward Pairwise Difference based Empirical Preference Optimization"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "2052a297586451686bbf959c47254cc3db13abab",
        "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process"
      },
      {
        "paperId": "fb5336e23363da4ffe3a4e66b8bbd5ea807b7915",
        "title": "The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "27cd9e0187751c1af2e306e3174db131942a1c14",
        "title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation"
      },
      {
        "paperId": "59516436839bb0dd90eee34e913bb9306f383619",
        "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "de17e7ed443e13320694cce3b2f475c694801246",
        "title": "Stepwise Alignment for Constrained Language Model Policy Optimization"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "48c34b348205ac1d3bbc5e84e7d2103b348b6c9a",
        "title": "ROPO: Robust Preference Optimization for Large Language Models"
      },
      {
        "paperId": "3bc23a232db48f454ebf27be39c189c9aa64af18",
        "title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "1708b700eab634bd26e08663321e93687d0d1e03",
        "title": "Reward Generalization in RLHF: A Topological Perspective"
      },
      {
        "paperId": "a8176838a636651324b9bac1b3443c803b44e1b3",
        "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model"
      },
      {
        "paperId": "7508634ac1312a7a975cbdf06fe754db2a1a3c09",
        "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration"
      },
      {
        "paperId": "7faaf3adf9b69f98fc40c5fbfc574f634e6ce98b",
        "title": "Information Theoretic Guarantees For Policy Alignment In Large Language Models"
      },
      {
        "paperId": "c20b353c4c6badab1f7003c617d6270c5bb2df4b",
        "title": "Internal Activation as the Polar Star for Steering Unsafe LLM Behavior"
      },
      {
        "paperId": "ee49b4c93f675efb28f8f1d5549e65115f6b8e93",
        "title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization"
      },
      {
        "paperId": "60b772a9fa659c8cb5ebc28df9243323501e73c8",
        "title": "Consistency-Aware Online Multi-Objective Alignment for Related Search Query Generation"
      },
      {
        "paperId": "3679d12929f5cf3a18a3096fd2157210645a21f8",
        "title": "Coactive Learning for Large Language Models using Implicit User Feedback"
      },
      {
        "paperId": "e1bcc131b19792472b7a7417622833a84c80a6d4",
        "title": "On Championing Foundation Models: From Explainability to Interpretability"
      },
      {
        "paperId": "7c12e0b3495fe2fff960c3ee0ac350c72fa7553a",
        "title": "Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques"
      },
      {
        "paperId": "e51a7528b9f6ec2101a682ebde818e40f96c8b24",
        "title": "Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "fac493a6e87a9e38ce441fa47fed6c2a51d8e42f",
        "title": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference Annotation"
      },
      {
        "paperId": "de816b055631ec74ad02f4c8600828dfa01cee07",
        "title": "Weak-to-Strong Extrapolation Expedites Alignment"
      },
      {
        "paperId": "4d7294c62913a1a2ce651212f8fbb20baf938227",
        "title": "STA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "566c0e6982a34385a8d5fbc55be8749ee532d117",
        "title": "I TERATIVE DPO WITH A N I MPROVEMENT M ODEL FOR F INE - TUNING D IFFUSION M ODELS"
      },
      {
        "paperId": "10e37de70767088938d5c2200cc2cd9b73af31cb",
        "title": "T RAINING C ODE P REFERENCE M ODELS VIA"
      },
      {
        "paperId": "819d25a7f90654ebf769d1b1c6499e5248cf3d8a",
        "title": "O UTLIER -A WARE P REFERENCE O PTIMIZATION FOR L ARGE L ANGUAGE M ODELS"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      }
    ],
    "score": 124.5
  },
  {
    "id": "c78350e81298ca87bc1d59b466fa40081232caaa",
    "title": "Teaching Large Language Models to Reason with Reinforcement Learning",
    "authors": [
      "Alex Havrilla",
      "Yuqing Du",
      "S. Raparthy",
      "Christoforos Nalmpantis",
      "Jane Dwivedi-Yu",
      "Maksym Zhuravinskyi",
      "Eric Hambro",
      "Sainbayar Sukhbaatar",
      "R. Raileanu"
    ],
    "year": 2024,
    "citationCount": 115,
    "abstract": "Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",
    "url": "https://www.semanticscholar.org/paper/c78350e81298ca87bc1d59b466fa40081232caaa",
    "pdf_url": "https://arxiv.org/pdf/2403.04642.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-07",
    "externalIds": {
      "ArXiv": "2403.04642",
      "DBLP": "journals/corr/abs-2403-04642",
      "DOI": "10.48550/arXiv.2403.04642",
      "CorpusId": 268264399
    },
    "references": [
      {
        "paperId": "ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd",
        "title": "GAIA: a benchmark for General AI Assistants"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "b16c7d45183b9d595ab64301be019741b1528860",
        "title": "Llemma: An Open Language Model For Mathematics"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "e56dc21699e6283fce072ffc908cb9f66321760d",
        "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"
      },
      {
        "paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"
      },
      {
        "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      },
      {
        "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
        "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"
      },
      {
        "paperId": "f1a838e57963bd774dd565275ff586cbd719083e",
        "title": "Dialogue Shaping: Empowering Agents through NPC Interaction"
      },
      {
        "paperId": "e0ca43a635d35fd0414ee76ca1e7c287715f5b00",
        "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"
      },
      {
        "paperId": "c29dbfbc17fa190b787a2662d49f08a38c8bd166",
        "title": "ARB: Advanced Reasoning Benchmark for Large Language Models"
      },
      {
        "paperId": "f8ad22941c633b62573aaee9cee651a9bc895fe5",
        "title": "Deep Reinforcement Learning"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
        "title": "Holistic Evaluation of Language Models"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      },
      {
        "paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
        "title": "StarCoder: may the source be with you!"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
        "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"
      },
      {
        "paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"
      },
      {
        "paperId": "ae9a0d7c98f8a74a538198291b0925609e5c26ac",
        "title": "Neural Story Planning"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
        "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
      },
      {
        "paperId": "e89ed6bb1864558e3889f5f2fb8931643c633479",
        "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning"
      },
      {
        "paperId": "d33504263ff0b263a1562075711264228d83b181",
        "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models"
      },
      {
        "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
        "title": "Large Language Models Can Self-Improve"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "paperId": "87b05afde92ec1f2c4ac4dffe2ab5c27ae36ea0c",
        "title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "e3a9af420cd2c0c8241856da92374027fefb87be",
        "title": "Language Model Cascades"
      },
      {
        "paperId": "6d994b4f5a46cd14e8f09f1e9e49120546b15e31",
        "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "4baac6b8fa7731352004bc45d2ba2b6bbd04a4e7",
        "title": "Evolution through Large Models"
      },
      {
        "paperId": "511e6559df79b5b7cc3fa69ae31ef1c3badce048",
        "title": "When does return-conditioned supervised learning work for offline reinforcement learning?"
      },
      {
        "paperId": "023edab4738690444e3924e224c2641017a0d794",
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "a791712371ed865cb6debe9cb4d5fd59a3405b85",
        "title": "Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning"
      },
      {
        "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
      },
      {
        "paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35",
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "6999fd72868c4044e852c43a040a87a43d03ab3a",
        "title": "Prioritized Level Replay"
      },
      {
        "paperId": "8aed57b61457655e8354f1b68b34ed1cc0a222ef",
        "title": "Keep CALM and Explore: Language Models for Action Generation in Text-based Games"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "3fa61dbb52424e7de78cb193b2460b8d38fac351",
        "title": "Constitutional"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "f3caa43a7016fbbf309d45112b31b20230eaf8da",
        "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "745c614dbd23bc1e3def79f600680b88cee28700",
        "title": "Thinking Fast and Slow with Deep Learning and Tree Search"
      },
      {
        "paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb",
        "title": "Improving Neural Machine Translation Models with Monolingual Data"
      },
      {
        "paperId": "f48eed915cbb9c6592cdb9df80c1edaeb46959af",
        "title": "A measure of intelligence"
      },
      {
        "paperId": "c769286434affcba7153281c54f8ae952a3dea63",
        "title": "Language"
      },
      {
        "paperId": "52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7",
        "title": "LILA: A Unified Benchmark for Mathematical Reasoning"
      },
      {
        "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
      },
      {
        "paperId": null,
        "title": "ArXiv , abs/1707.06347"
      },
      {
        "paperId": null,
        "title": "A framework for few-shot language model evaluation"
      },
      {
        "paperId": "35433f77c6cff142657eeffb01d473d9636b1ee6",
        "title": "A Study on Improving Reasoning in Language Models"
      },
      {
        "paperId": null,
        "title": "reported in 5. positive:negative ratio GSM8K (maj@1) EI-minimal - 0.17"
      },
      {
        "paperId": null,
        "title": "We find we achieve best performance when the amount of positive training data greatly outweighs the amount of negative data"
      },
      {
        "paperId": null,
        "title": "negative samples, performance generally decreases"
      },
      {
        "paperId": null,
        "title": ":1 0.18 1:1 0.15 Table 5. RCRL without SFT, using different proportions of positive"
      },
      {
        "paperId": null,
        "title": "positive and negative training data"
      },
      {
        "paperId": null,
        "title": "Label Balance We also experiment with different proportions of \u2018[GOOD] and \u2018[BAD]\u2019 labels in RCRL training data"
      },
      {
        "paperId": null,
        "title": "which is much easier to generate than its positive counterpart. Better teaching the student what not to do with this data would ideally increase the number of valid solutions. Recall by"
      },
      {
        "paperId": null,
        "title": "RL for LLM Reasoning"
      }
    ],
    "cited_by": [
      {
        "paperId": "eb3a028a58470639799c640ca5e6bc2b0a65d8f2",
        "title": "Hybrid Training for Vision-Language-Action Models"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "095877ca77a899eaa9aebb82a88d953ea94dfb6f",
        "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization"
      },
      {
        "paperId": "702ed44b9a800cfb8308582c7f2122c8043ac17f",
        "title": "Once Upon a Time: Interactive Learning for Storytelling with Small Language Models"
      },
      {
        "paperId": "21d16c0e96f907de610950344ed784551e8e4ccd",
        "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling"
      },
      {
        "paperId": "eab7ba0cc06e29ed858506ebcb17f7189b9e4ab1",
        "title": "ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models"
      },
      {
        "paperId": "be97a07fb7fbeffb32234543247da56b6208819b",
        "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective"
      },
      {
        "paperId": "6b3e64ab07bdb533d26bc4a78dad189e02a9bcb8",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "fa005378f2cc756a846d6fe807e02917e58f30ae",
        "title": "FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models"
      },
      {
        "paperId": "05e9fbdef92f059787310b7993b35435688a5db3",
        "title": "A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models"
      },
      {
        "paperId": "b43f8bacd80f32734cdff4f9d8c79e397872aed6",
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"
      },
      {
        "paperId": "5e77bacebd4a6ef123f768916969136d7aae576b",
        "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings"
      },
      {
        "paperId": "3d96b5f71760e86efb13ec71d0f249fa81cb5a45",
        "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs"
      },
      {
        "paperId": "92f432d4d43f0bca8c2859d9269a94ea93fcfb1f",
        "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game"
      },
      {
        "paperId": "b88cc13aa6176b87975a148e66260842309dd806",
        "title": "Learning Efficient Robotic Garment Manipulation with Standardization"
      },
      {
        "paperId": "2868932ec97c80d9a17376ee5dacda7e3121e269",
        "title": "RL for Reasoning by Adaptively Revealing Rationales"
      },
      {
        "paperId": "502a9fe58f14fa1252648bcb434e7ad04913c525",
        "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models"
      },
      {
        "paperId": "86de12d65a14fc10f88c97bc08ecf790f38a79ee",
        "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning"
      },
      {
        "paperId": "fed4309c9842e484b35b2317545d8b4a7ec46206",
        "title": "RePO: Replay-Enhanced Policy Optimization"
      },
      {
        "paperId": "04e8103c5049fd95eb99d218d2e9cd0c29f13dbb",
        "title": "Let\u2019s Verify and Reinforce Image Generation Step by Step"
      },
      {
        "paperId": "c4d65d124c3ceeb7369425954516b1d7c6ffc1ab",
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization"
      },
      {
        "paperId": "4351e81bcfb5164a55c44eda6674a0331337b3de",
        "title": "SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms"
      },
      {
        "paperId": "4d608203639087e0fe3c5d2b7a374941dd182cb7",
        "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models"
      },
      {
        "paperId": "c0851f97ccb5ed8432f4c144cc7e63985536e43d",
        "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning"
      },
      {
        "paperId": "828ab065d15c9516c633b54e3245ec08cc75a2f8",
        "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content"
      },
      {
        "paperId": "82eb3d408ba312148d4f76fa654c1d042092ea30",
        "title": "Training a Scientific Reasoning Model for Chemistry"
      },
      {
        "paperId": "a7933f44a76c680b074591deb74b444ab8c748e0",
        "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"
      },
      {
        "paperId": "1bac1895263f10e52cbee5742fa7b896b5bc4976",
        "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning"
      },
      {
        "paperId": "98280abfb6cd5cc626f0fdb52c1f41dad46a0634",
        "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought"
      },
      {
        "paperId": "b75643c6817b50b65f23b3b3868be7f0eeaddb0e",
        "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer"
      },
      {
        "paperId": "fe13d660dbad9cd52d753f360424745e11d4844f",
        "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation"
      },
      {
        "paperId": "6d79c639f63cbbe1c2d0f211cae7c7c43178aee6",
        "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model"
      },
      {
        "paperId": "78f4d69750ecf17c76b9940a82e2c2f244deb27d",
        "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning"
      },
      {
        "paperId": "7319a8bfe18c701399431f15e65ceb5756c3452f",
        "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards"
      },
      {
        "paperId": "e5a6f2eab0b8c87189f95d3bb08d781a4b116c2f",
        "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "abc473899b15276c087c8ecf64e649169a1d1382",
        "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models"
      },
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "5a96bfbae6ea33b0f108604436bc33b144f36ad0",
        "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space"
      },
      {
        "paperId": "d89470898539f8faf0c7be8cbee3fe34da717083",
        "title": "FlashThink: An Early Exit Method For Efficient Reasoning"
      },
      {
        "paperId": "57e959b74f36a30cd62d0abd4204f08907b42e87",
        "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning"
      },
      {
        "paperId": "7d47a1672c2f297f167eda7fe6571a788091ac2a",
        "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO"
      },
      {
        "paperId": "69146241df580151be22281271880389b18b79ae",
        "title": "Chain-of-Thought Tokens are Computer Program Variables"
      },
      {
        "paperId": "d653af97a808ce0b6594a543d5dc80cabdf6d562",
        "title": "Multi-agent Embodied AI: Advances and Future Directions"
      },
      {
        "paperId": "1d03586baa32b3d6ff657a180053821543e11abb",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "10331b2d70b1247516572cdec0db5374f19996cc",
        "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room"
      },
      {
        "paperId": "aab504414b797e7a01c31d49196c9e2b2a885d69",
        "title": "Everything You Wanted to Know About LLM-based Vulnerability Detection But Were Afraid to Ask"
      },
      {
        "paperId": "1574b4933bd9db95a039a8bcbb81d9519be66f7f",
        "title": "Aligning Constraint Generation with Design Intent in Parametric CAD"
      },
      {
        "paperId": "ef3730364b904b64f8783196c6e784048cec21ba",
        "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data"
      },
      {
        "paperId": "07017b4c848371df9414f64d91002292d42324cd",
        "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining"
      },
      {
        "paperId": "aa06db9a5b5f8173502d6dd6fd4be96eda6a0f89",
        "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning"
      },
      {
        "paperId": "bb9a43a4db94e4cf7c6dac62e59b747550a2a534",
        "title": "AcademiCraft: Transforming WritingAssistance for English for Academic Purposes with Multi-Agent System Innovations"
      },
      {
        "paperId": "4cf76fda98c6b2335753ef0ac034fab24ad7ce3e",
        "title": "Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large Language Models with Sequences"
      },
      {
        "paperId": "8a834ec7f1d2c428e244c288eb5e9fe31912dd61",
        "title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality"
      },
      {
        "paperId": "6e03e2525a1b11485e477f454fc8bc9a65f42449",
        "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs"
      },
      {
        "paperId": "432cf4506d1fa7773abfaceb35841d60e5685839",
        "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs"
      },
      {
        "paperId": "e145627eb1501d0ef0439757c8b556f77afdfe43",
        "title": "Improving Retrospective Language Agents via Joint Policy Gradient Optimization"
      },
      {
        "paperId": "6a57356fc97492acfcee2419a0d27066761b0c91",
        "title": "VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted Trie in Sponsored Search"
      },
      {
        "paperId": "bb146358872cf242e97d891bbf6994bc1faae2fe",
        "title": "Training a Generally Curious Agent"
      },
      {
        "paperId": "a31f0356eb2ebf2f87320686b05033a6ca7c3172",
        "title": "S2R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning"
      },
      {
        "paperId": "8137cebccf27d8f22f8099b5a5eb8858675cb2c9",
        "title": "Learning to Reason at the Frontier of Learnability"
      },
      {
        "paperId": "c0b68738f3e46e7b9dacbe566f0c31104a78b616",
        "title": "Active Advantage-Aligned Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "8f599aaa623c43ad9a9d91b3c67ee29ad14ca078",
        "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning"
      },
      {
        "paperId": "927af91f480e69e3bbe294358dc1659d9bbf5435",
        "title": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization"
      },
      {
        "paperId": "f9cef96bc0e4e282cbff93d35a7fc68e643504cc",
        "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"
      },
      {
        "paperId": "e3dd4964c07c914a0ccca2e2f3ed6410f8a86a6a",
        "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework"
      },
      {
        "paperId": "d65e47d3cfce21ff1e2b51f0b18efae61570ccd5",
        "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners"
      },
      {
        "paperId": "673fbdd957cada770d10dffca5e45b53da43a3c6",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space"
      },
      {
        "paperId": "33a4b77f0c278fbe24f8203d6c958db5be439f43",
        "title": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback"
      },
      {
        "paperId": "250d920ba5cf1e8dcb521eee47e181cf3eb3755a",
        "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models"
      },
      {
        "paperId": "51ccb1dd38c434979ac430ab18213a09598d15a0",
        "title": "Aligning LLMs with Human Instructions and Stock Market Feedback in Financial Sentiment Analysis"
      },
      {
        "paperId": "043731dd9f2292b7e4788d0b97edee5db170f840",
        "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
      },
      {
        "paperId": "864b530b17373aa4e6f09a00614cf2f6c2be707d",
        "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling"
      },
      {
        "paperId": "f69c1ac52b55c270b64266fabf5efc700ceee93b",
        "title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning"
      },
      {
        "paperId": "9d3d5adcdbdf68f726bf8490480060863f8fff1a",
        "title": "Boosting Deductive Reasoning with Step Signals In RLHF"
      },
      {
        "paperId": "e3662a0e4a1bff43237c9596b2ecd7efcf0faeee",
        "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization"
      },
      {
        "paperId": "24e2ee1d0e7165d1f3fc355b80dee7f90a1315ea",
        "title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both"
      },
      {
        "paperId": "f820624b5738ae079cd83980143c958d5c58653b",
        "title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning"
      },
      {
        "paperId": "66d9550a51cb493aaeeae72d396b0bdb1ca47fe9",
        "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning"
      },
      {
        "paperId": "5eb42572232c53c1ba0620935f4bba9f2e643600",
        "title": "StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning"
      },
      {
        "paperId": "dc96b6ddb25bc2ebba5750638d3e36290773b4ea",
        "title": "Rational Metareasoning for Large Language Models"
      },
      {
        "paperId": "739f769a2461c6bb7c893fa794fa1ac362bffe3e",
        "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult"
      },
      {
        "paperId": "03bae9c8d33a312987dc0b14fcfc7d31890a8829",
        "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards"
      },
      {
        "paperId": "c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb",
        "title": "Training Language Models to Self-Correct via Reinforcement Learning"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "b22574b067132f183395947cbc82d1a7c99b096e",
        "title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic"
      },
      {
        "paperId": "36b6c1f09d28dd85ea3fe0efbf56b16bfe2e81b4",
        "title": "Learning to Plan Long-Term for Language Modeling"
      },
      {
        "paperId": "0cbe018c7456e6708e456fc744c6a4a6a7efdb38",
        "title": "Prover-Verifier Games improve legibility of LLM outputs"
      },
      {
        "paperId": "6c2f36544226a34ede9aaae4a9655595ab543611",
        "title": "Establishing Knowledge Preference in Language Models"
      },
      {
        "paperId": "e0f0a1a0a62560b50e1f4967779b0c67aeb209e1",
        "title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning"
      },
      {
        "paperId": "325b74b629e60115c9ef50f07efd6b15818dc0c0",
        "title": "Large Language Models Assume People are More Rational than We Really are"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "9fb5f5f109c699295f0e5920aafe9007d1a4b6f9",
        "title": "Algorithm Development Using Artificial Intelligence: An Overview"
      },
      {
        "paperId": "0880d3306de7a1651d9a8723a70de488e56f2e07",
        "title": "Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples"
      },
      {
        "paperId": "7f149b843332aec40c03fc72f4adf677ef82cb1c",
        "title": "HBTP: Heuristic Behavior Tree Planning with Large Language Model Reasoning"
      },
      {
        "paperId": "729839f301931f8c77322489b3c5cd5373b82a0e",
        "title": "Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "63f7790a137091c25cb61767b7c1c5f971fb17d5",
        "title": "Curriculum Direct Preference Optimization for Diffusion and Consistency Models"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "49daaadfe84f1bf850feb8d6db46b652f75c9750",
        "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "72627b55a30e993a0c2443bf92e0539553597576",
        "title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents"
      },
      {
        "paperId": "bd598f1b113891cbaccd5b7b96008a9bf2759712",
        "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation"
      },
      {
        "paperId": "fc8eb088e82e9b0f3d82c1ab0382cbd16be9b877",
        "title": "Learning to Plan for Language Modeling from Unlabeled Data"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "a01e1138600499f65462ed3d51c3e76af1aad18c",
        "title": "The pitfalls of next-token prediction"
      },
      {
        "paperId": "1660f5839b7068018951a226cb587cedf6aa66db",
        "title": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "da90291ffa1197fc74b596ff562d765c640293d9",
        "title": "War of Thoughts: Competition Stimulates Stronger Reasoning in Large Language Models"
      },
      {
        "paperId": "46c3b3781badd1553fe2aae19b75243802805598",
        "title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents"
      },
      {
        "paperId": "65332c006c114a9eb8f1a848f4aad7214bba6514",
        "title": "Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking"
      },
      {
        "paperId": "9a50a73562b5569e7e62668c1c5eba4c6a1d45f1",
        "title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "98867f10f0ee0914667a6b92700e8dc8c8929ae3",
        "title": "F LOW OF R EASONING : T RAINING LLM S FOR D IVER - GENT P ROBLEM S OLVING WITH M INIMAL E XAMPLES"
      },
      {
        "paperId": "5652ec62eb6aee72e09f6c38704b3187972a0589",
        "title": "VinePPO: Accurate Credit Assignment in RL for LLM Mathematical Reasoning"
      },
      {
        "paperId": "f98ef89ca455a8a75be3892dc3fbfa75ef91fdde",
        "title": "On Reward Functions For Self-Improving Chain-of-Thought Reasoning Without Supervised Datasets (Abridged Version)"
      },
      {
        "paperId": "34b2185a9ba9d9304f3bd645026db750365cb7ce",
        "title": "Using Curriculum to Improve Mathematical Reasoning"
      },
      {
        "paperId": "d8cbc109761ca181275c9d46f002d23d491f4a41",
        "title": "Countdown to Brilliance: Evolving Math Reasoning Through Train and Test-Time Compute"
      }
    ],
    "score": 115.0
  },
  {
    "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "title": "A Survey of Reinforcement Learning from Human Feedback",
    "authors": [
      "Timo Kaufmann",
      "Paul Weng",
      "Viktor Bengs",
      "Eyke H\u00fcllermeier"
    ],
    "year": 2023,
    "citationCount": 206,
    "abstract": "Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.",
    "url": "https://www.semanticscholar.org/paper/6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "pdf_url": "https://arxiv.org/pdf/2312.14925.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-12-22",
    "externalIds": {
      "ArXiv": "2312.14925",
      "DBLP": "journals/corr/abs-2312-14925",
      "DOI": "10.48550/arXiv.2312.14925",
      "CorpusId": 266521540
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "5ca6051d4bb28b2f0b3f098fa2a02edb833b8c32",
        "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning"
      },
      {
        "paperId": "6cf2cc9be83db655c42eae61cbbe91ea5051e9fc",
        "title": "Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning"
      },
      {
        "paperId": "1a0fed22c7bee7df5556555293505e7e6f92a0a9",
        "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning"
      },
      {
        "paperId": "e5018e699df56684dae9d6b35f515cf5b030704c",
        "title": "IRIS: Intrinsic Reward Image Synthesis"
      },
      {
        "paperId": "cbb066e3225ba9246879fe4c670757e87963c806",
        "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety"
      },
      {
        "paperId": "18619335bd6efbb756b8a441656a549c5eb7cd53",
        "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models"
      },
      {
        "paperId": "5c6730b6173f89443830d39a92a0d8775e83093a",
        "title": "Tree Search for LLM Agent Reinforcement Learning"
      },
      {
        "paperId": "4e5c342611527a95f4a76199d610b1032bbdd98c",
        "title": "Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "90aa783a692c2fe4ef52aaf0a0adca79fbfa3440",
        "title": "Towards personalized, precise and survey-free environment recognition: AI-enhanced sensor fusion without pre-deployment"
      },
      {
        "paperId": "1403af47c87394d30e019bd836b7ee367ded9977",
        "title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents"
      },
      {
        "paperId": "7c92d3b036f361e931ad23a5a422a090fa7f3ece",
        "title": "Long Context Automated Essay Scoring with Language Models"
      },
      {
        "paperId": "feedaa6fb2bbaf08228ea443b1b692df797019e5",
        "title": "ForestGPT and Beyond: A Trustworthy Domain-Specific Large Language Model Paving the Way to Forestry 5.0"
      },
      {
        "paperId": "bb9d4cdddfd6f5742fb4428c08245ce56100fa1d",
        "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate"
      },
      {
        "paperId": "4c0129962ccc9e2e3b432d5800ce5888ab62fd4b",
        "title": "Connections between reinforcement learning with feedback,test-time scaling, and diffusion guidance: An anthology"
      },
      {
        "paperId": "76e098eb53ab659102c999d876c4ecf8402b8a78",
        "title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment"
      },
      {
        "paperId": "c9e58c7983b70e0ea042ccdf3a89355580a9c532",
        "title": "Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey"
      },
      {
        "paperId": "1ab8fa5ee2fc77d9114d124d30017b7f8407a5c4",
        "title": "Active Query Selection for Crowd-Based Reinforcement Learning"
      },
      {
        "paperId": "e541351ebcfd6be1e37a4d0d4d4661cc3b850dae",
        "title": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains"
      },
      {
        "paperId": "0374952d1c61795b2f7607b05603e33792ab9447",
        "title": "Ask ChatGPT: Caveats and Mitigations for Individual Users of AI Chatbots"
      },
      {
        "paperId": "ef0e33fa685c8dc6b8199b199fd03e482089f5e7",
        "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning"
      },
      {
        "paperId": "c0e1fb53244eb6fa3d48e5e9492d534363545ba3",
        "title": "ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning"
      },
      {
        "paperId": "6b3e64ab07bdb533d26bc4a78dad189e02a9bcb8",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "e43411eae18848baea72f5daa1730530453c1002",
        "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning"
      },
      {
        "paperId": "32e456f84d5fc11e1e8e715d0f900573a6f7bd3e",
        "title": "MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language"
      },
      {
        "paperId": "8f5d187a6173f018ffc77d708bc76572c2f20c9b",
        "title": "RecGPT Technical Report"
      },
      {
        "paperId": "6ea6867effafcd81e4281ac55c2ca0531acc7991",
        "title": "Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning"
      },
      {
        "paperId": "33bf820ea3f9a5f098348ab1e58a23b6d5ba418e",
        "title": "The Blessing and Curse of Dimensionality in Safety Alignment"
      },
      {
        "paperId": "f078a1a30ba9aa02c032c1e2741919364845cfab",
        "title": "A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction"
      },
      {
        "paperId": "3c8b92a8b6eee1377077ecfea95e53223845af20",
        "title": "Agentic Reinforced Policy Optimization"
      },
      {
        "paperId": "9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
      },
      {
        "paperId": "441e92246a87dfcc68c2a6a5900a5d3ad654a894",
        "title": "Granular feedback merits sophisticated aggregation"
      },
      {
        "paperId": "17721464503d10ec06d1339d0fb59d4f32d6f692",
        "title": "Potential to perpetuate social biases in health care by Chinese large language models: a model evaluation study"
      },
      {
        "paperId": "3acaa8592735cd7fed7e4d1d251bfa894141e81b",
        "title": "DCR: Quantifying Data Contamination in LLMs Evaluation"
      },
      {
        "paperId": "ef9172cc95fd17d788108ac8f3b8912908d362a9",
        "title": "Enhancing Autonomous Driving Policy Stability through Auxiliary Network in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d341989bb259c35dc723c386ab45636a78613046",
        "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "a8a1fe146c9e012a00bde0a9dbd0bb4815b7b333",
        "title": "How trust networks shape students\u2019 opinions about the proficiency of artificially intelligent assistants"
      },
      {
        "paperId": "8df0bd7c0fc03465825a58f78f9c2853304aec6c",
        "title": "Adaptive alert prioritisation in security operations centres via learning to defer with human feedback"
      },
      {
        "paperId": "effc681bc2a7621b49d4c6bd317f7e54ab3554a2",
        "title": "Collaborative Editable Model"
      },
      {
        "paperId": "4a230ff814e6492da8c7dbd1b3d44842df1be989",
        "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
      },
      {
        "paperId": "8bdf4beea35d4ddaeb700440c17ae5955b936b5e",
        "title": "Improving Large Language Model Safety with Contrastive Representation Learning"
      },
      {
        "paperId": "14998ec52d42803c891295d00e1dbd8183427342",
        "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs"
      },
      {
        "paperId": "61c9b8bdfcebf032ce8b80ed2be5c75f0ba03483",
        "title": "Foundation Models in Medical Imaging - A Review and Outlook"
      },
      {
        "paperId": "4c0f92b35a1d64460a62ef49bb0f225c1cfbd9de",
        "title": "Position: Simulating Society Requires Simulating Thought"
      },
      {
        "paperId": "c5e8e0c86d1df2131be6712d4c702edfa3c9d420",
        "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions"
      },
      {
        "paperId": "f6180c800726e25693e9808360167760d1a89048",
        "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance"
      },
      {
        "paperId": "3a2b08d37fba04b6563b38818497f1f98ec9df7d",
        "title": "RewardAnything: Generalizable Principle-Following Reward Models"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "3d27c752bef3418f855d003735ebbdb19aaf4f7d",
        "title": "A Descriptive and Normative Theory of Human Beliefs in RLHF"
      },
      {
        "paperId": "dcbfe9d7c6f0d29b092947ba491728bed570223c",
        "title": "MultiFedRL: Efficient Training of Service Agents for Heterogeneous Internet of Things Environments"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "d90740ce0ff42d02ec83cd468cee086695d4db3a",
        "title": "The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process"
      },
      {
        "paperId": "4fa1e4442dc9c4bf8c5b2109955d28f066fd6991",
        "title": "Evaluation of LLMs for mathematical problem solving"
      },
      {
        "paperId": "fab2cb0b275dc555024af61b22e14219971393c5",
        "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective"
      },
      {
        "paperId": "5fa94fd08dbdc539c8f03b5b5c7b2d7135a7ea92",
        "title": "Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies"
      },
      {
        "paperId": "bae4a2d326d23e53861e5aa94062c4e866e8c514",
        "title": "Preference Learning with Response Time"
      },
      {
        "paperId": "1d92c345ff78631aa5509c84e6ff91d446e2c712",
        "title": "What Can RL Bring to VLA Generalization? An Empirical Study"
      },
      {
        "paperId": "d6123d6d213436d8258b4a8f8b7fb90120006239",
        "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles"
      },
      {
        "paperId": "9db4ecfb5f2e78b43ff388892ce3bf0f122fa0d3",
        "title": "Multi-Domain Explainability of Preferences"
      },
      {
        "paperId": "7319a8bfe18c701399431f15e65ceb5756c3452f",
        "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards"
      },
      {
        "paperId": "1fc45ec4de3994020122664280b515d45785b90d",
        "title": "Incentivizing High-Quality Human Annotations with Golden Questions"
      },
      {
        "paperId": "ab196ccb386ecb7305758936985f1423c722c3aa",
        "title": "Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons"
      },
      {
        "paperId": "8932cb2a36f98c263f5a0f1287a1e0303fc0e7df",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment"
      },
      {
        "paperId": "c4adaa35132733efc484b50fcd867a7da86b3f82",
        "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents"
      },
      {
        "paperId": "5becc8d0a5066c58d2785798519f2cddad7f4482",
        "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning"
      },
      {
        "paperId": "be72422ef89f2e67f18864b0682e9dabadcc709b",
        "title": "Direct Preference Optimization for Adaptive Concept-based Explanations"
      },
      {
        "paperId": "f4d363facb85c73ab395638e944ed318369bd675",
        "title": "Cooperative Bargaining Games Without Utilities: Mediated Solutions from Direction Oracles"
      },
      {
        "paperId": "879361e834f2895fd0dd006f31d1f19e5b37b203",
        "title": "Quantum Knowledge Distillation for Large Language Models"
      },
      {
        "paperId": "fb555ebb79f859e786e3c9f18f1faf0207a53294",
        "title": "Multi-Armed Bandits Meet Large Language Models"
      },
      {
        "paperId": "8f047a824ed8b5afc28715a552bcda9126fa7ef9",
        "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs"
      },
      {
        "paperId": "b57e52d6229fbab71df97fd917158b4f4b8a4502",
        "title": "The Accountability Paradox: How Platform API Restrictions Undermine AI Transparency Mandates"
      },
      {
        "paperId": "2a3bb4dafd9d4fcff73dfaf8637a3754062372fb",
        "title": "AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques"
      },
      {
        "paperId": "efe6aae8e90fafef29566b0a5862ae229cc873f3",
        "title": "Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning"
      },
      {
        "paperId": "e2b233fe82f0e3e54512073f85549fc3a2de76e2",
        "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration"
      },
      {
        "paperId": "33a7bb26825560a0b13c955152ff14f27979c026",
        "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making"
      },
      {
        "paperId": "434e9814369b6a353464149d9e6ab9b96131a1f6",
        "title": "Emotions in Artificial Intelligence"
      },
      {
        "paperId": "930318b6375e59dd108f85493f4f5ee63519e390",
        "title": "Comparison of effectiveness between ChatGPT 3.5 and 4 in understanding different natural languages"
      },
      {
        "paperId": "1523caee9abc4ffc86557089bc494ef0ba12e7a0",
        "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback"
      },
      {
        "paperId": "823a6f37db9d18ec4fdfaf02e01400c5ec6c0ab9",
        "title": "Contemplative Artificial Intelligence"
      },
      {
        "paperId": "e42054d042e2e5b3efe6e96cd6dd1c76ab3ca358",
        "title": "ToolRL: Reward is All Tool Learning Needs"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "ace53b570483a7bbd3db574993c53070a3be6922",
        "title": "Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization"
      },
      {
        "paperId": "ca02e620553eac1279eb7ddbe2e742171ed74b11",
        "title": "Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology"
      },
      {
        "paperId": "13f9dd2c7d259460699b0d2ca9acf52351535d91",
        "title": "Align to Structure: Aligning Large Language Models with Structural Information"
      },
      {
        "paperId": "67d99f2951eb1b65cca208ed8a5e054f77894e3a",
        "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models"
      },
      {
        "paperId": "36cf20d3650c5206454b5b8ff595c6ef024718e9",
        "title": "Urban Computing in the Era of Large Language Models"
      },
      {
        "paperId": "7cc0d28b6a9790cabe127472acaaf63ffece54a7",
        "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models"
      },
      {
        "paperId": "18448e0a6bde22d4cb60f37f1add5c8b6fc2142d",
        "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning"
      },
      {
        "paperId": "b2f1df899c17b4e8b8817ebc9e2b470a9558474d",
        "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise"
      },
      {
        "paperId": "4f657dd99723932d6b4476372d56fe11b938d9a4",
        "title": "Strategyproof Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1fcaafeb72142fe3d1a5d698a072d69778d244b0",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "fffdfe464d587c5da27aa6f8fb8afe47c7e313f8",
        "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
      },
      {
        "paperId": "0ff903811333bf12b1694d83309316be16b73465",
        "title": "Reversing the logic of generative AI alignment: a pragmatic approach for public interest"
      },
      {
        "paperId": "05ab61cb986ee3778f1a5628176af3976f097ef9",
        "title": "Teaching Your Models to Understand Code via Focal Preference Alignment"
      },
      {
        "paperId": "28b4a8c5b6cca93a5bb2418d3bf7ff49961240be",
        "title": "Shh, don't say that! Domain Certification in LLMs"
      },
      {
        "paperId": "c4d24bfede611ebb771d149fd817e932be0c07a6",
        "title": "Representation Engineering for Large-Language Models: Survey and Research Challenges"
      },
      {
        "paperId": "4293d4d28daaa95ad68e5c72ad30e78d5d15e21a",
        "title": "A comparative analysis of rank aggregation methods for the partial label ranking problem"
      },
      {
        "paperId": "b17acec8cbf488d1f95f5c01f0392a576ee36c43",
        "title": "Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study"
      },
      {
        "paperId": "677b617b44e2d8f30af484ef23f3847d2d3d7b89",
        "title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation"
      },
      {
        "paperId": "c6e8297646a161facbd4c5ca708df87dfd79a607",
        "title": "Preference learning made easy: Everything should be understood through win rate"
      },
      {
        "paperId": "a922ecbc9f440e5547117a7f188a76de7996eb3c",
        "title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models"
      },
      {
        "paperId": "b0a94c399b1771c8b7efab0c9fcab9df41ddd7da",
        "title": "Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers"
      },
      {
        "paperId": "a7b7d3dfb7959aaab09b2d4f2a54922cbe5f4e5f",
        "title": "Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects"
      },
      {
        "paperId": "2eba7fae05ef0922aafd7cde69b52093c051b245",
        "title": "Multitask deep learning for the emulation and calibration of an agent-based malaria transmission model"
      },
      {
        "paperId": "a0f2f14afbf7b077d519bf7ca74f1e1d061a7d5a",
        "title": "Active RLHF via Best Policy Learning from Trajectory Preference Feedback"
      },
      {
        "paperId": "53f9acf725a9fd5159d6b7c4f1bb917ac890c219",
        "title": "Feasible Learning"
      },
      {
        "paperId": "705baaf896c10699a6c97a9d4066449bad5ae216",
        "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF"
      },
      {
        "paperId": "44dcaa20f5eb5c5fd5b773ef9a41629cbebe452f",
        "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
      },
      {
        "paperId": "d1d0e5ff34fa5daf3de70e24dfbfdfa919cb49c5",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems"
      },
      {
        "paperId": "1950060574db666f7f47468cbb383c67a09bbdb1",
        "title": "Complexification through gradual involvement and reward Providing in deep reinforcement learning"
      },
      {
        "paperId": "d492ea367f2831d07cbd043bd95fde87cc6e3e8f",
        "title": "AI-Powered CPS-Enabled Urban Transportation Digital Twin: Methods and Applications"
      },
      {
        "paperId": "812f20b087dd927512963fee56268419462b7efc",
        "title": "A Research Agenda for Usability and Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "19174da69f1f06346e9a6289d05f29ceb337c2fe",
        "title": "Military Fine-tuning Dataset Construction Method for Chinese Large Language Model"
      },
      {
        "paperId": "a55c1d40c874a205aca1d7b5a4f2a4424168c0fa",
        "title": "Do Large Language Models Advocate for Inferentialism?"
      },
      {
        "paperId": "886801bb9f10f39242f3cabb96464ab6728906b9",
        "title": "Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods"
      },
      {
        "paperId": "b741ac25a3d12e0923f2a403d34294e27811a728",
        "title": "PlanCritic: Formal Planning with Human Feedback"
      },
      {
        "paperId": "11de822b3bd68820ac2475ea26c2d1e614fe8f2b",
        "title": "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems"
      },
      {
        "paperId": "763b1b2530b307716985145d7d82d9fa6274f534",
        "title": "Engagement-Driven Content Generation with Large Language Models"
      },
      {
        "paperId": "4f064e8a523593d1c8c4577f1959183b6fc24865",
        "title": "AIDBench: A benchmark for evaluating the authorship identification capability of large language models"
      },
      {
        "paperId": "ad9cc1c463faa60d91bfaa48c3b16df0b3719ffa",
        "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd"
      },
      {
        "paperId": "f2f98860b97cf5ffa7264d4069f844102a1c239e",
        "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models"
      },
      {
        "paperId": "d016152aa3ff624029d293318ccec34343fa228d",
        "title": "SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization"
      },
      {
        "paperId": "c70124639ba84b610bc7552930ce774374fff8a6",
        "title": "Benchmarking XAI Explanations with Human-Aligned Evaluations"
      },
      {
        "paperId": "5833a30758feb1e8cf714887187a5772fe849677",
        "title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models"
      },
      {
        "paperId": "0ac58ec3b14b9d1101387c5cd6cbdc07f473cfa9",
        "title": "Comparison-based Active Preference Learning for Multi-dimensional Personalization"
      },
      {
        "paperId": "516b5400f234154d8fac0c3386d45e7f8123e319",
        "title": "ComPO: Community Preferences for Language Model Personalization"
      },
      {
        "paperId": "9f597bdbd47d38e3920d2ab9020b91a558cdf026",
        "title": "Causality for Large Language Models"
      },
      {
        "paperId": "bd1d876b6e8dc707775976a8d7a163750ca128f0",
        "title": "VideoAgent: Self-Improving Video Generation"
      },
      {
        "paperId": "cb8b8ce83dca901cc447847851cebbde45bc77a0",
        "title": "Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory"
      },
      {
        "paperId": "58075b343e8d3ab61a10b5b52389d8ee5cb6e5e1",
        "title": "Can a large language model be a gaslighter?"
      },
      {
        "paperId": "98dccb1b347a2f8d80adfb4801d13358ed7d8dcb",
        "title": "Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "3e1d812a1ef4b02b7d60020cc8aaa596d28373da",
        "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization"
      },
      {
        "paperId": "c39f45bbc55d4c5be539aea296d3bde6486f0bbc",
        "title": "LoRTA: Low Rank Tensor Adaptation of Large Language Models"
      },
      {
        "paperId": "9d289e75414078f53ab5feb112c4ff8272519e43",
        "title": "Reward Learning From Preference With Ties"
      },
      {
        "paperId": "82000db55600a297056f4847f3362cbe2b574232",
        "title": "Auction-Based Regulation for Artificial Intelligence"
      },
      {
        "paperId": "1ed6e729d2397c25adc465ba83718714b1979484",
        "title": "GameLabel-10K: Collecting Image Preference Data Through Mobile Game Crowdsourcing"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "9f13dc5a9adad33c6020b967ea2a73687e1cb4d1",
        "title": "Preference Alignment Improves Language Model-Based TTS"
      },
      {
        "paperId": "5ec71c979dcb07272f50f021560d3e36e2e47ede",
        "title": "Climate action or delay: the dynamics of competing narratives in the UK political sphere and the influence of climate protest"
      },
      {
        "paperId": "ed239f470564b6cfd20243b1fc1b0e3bad6ea6a2",
        "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems"
      },
      {
        "paperId": "78c06d6c4eccce45f7c0fb24c4de3c05809c4993",
        "title": "Imitating Language via Scalable Inverse Reinforcement Learning"
      },
      {
        "paperId": "2097b656b25c9291548fa082b3c433eb81856c76",
        "title": "Problem Solving Through Human-AI Preference-Based Cooperation"
      },
      {
        "paperId": "f24458a343c40f3dd418a3095f993ea068ff5418",
        "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era"
      },
      {
        "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
        "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "e344313490dc93bacf721c19f0b74ae921ac4285",
        "title": "Reward Models in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "9c6d83fa083a87fc69719d033f4a2b860a4011ed",
        "title": "Continual Multi-Objective Reinforcement Learning via Reward Model Rehearsal"
      },
      {
        "paperId": "42c3bb6501f1cba5c1366c2bdad45b273bca6075",
        "title": "Motamot: A Dataset for Revealing the Supremacy of Large Language Models Over Transformer Models in Bengali Political Sentiment Analysis"
      },
      {
        "paperId": "a6cc88b2bfa891adfc305629658817985ba2cc01",
        "title": "ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model"
      },
      {
        "paperId": "f5648f90f08e9a3beccd5f7efc6ebfd0f72f6bd1",
        "title": "Self-Directed Synthetic Dialogues and Revisions Technical Report"
      },
      {
        "paperId": "3d7925cb321c059a039f8445da38d60b2c81d535",
        "title": "Building an Ethical and Trustworthy Biomedical AI Ecosystem for the Translational and Clinical Integration of Foundation Models"
      },
      {
        "paperId": "69779edde7b072808d676e9fdc44a1b4ce565957",
        "title": "Learning From Crowdsourced Noisy Labels: A signal processing perspective"
      },
      {
        "paperId": "6013c87aa7d6fa6bbe40764df2a59aaa876cddf8",
        "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models"
      },
      {
        "paperId": "e4d5ebc2e2bfd2cc2b4cb524fb5dee8229f144a3",
        "title": "ASRRL-TTS: Agile Speaker Representation Reinforcement Learning for Text-to-Speech Speaker Adaptation"
      },
      {
        "paperId": "d950c9aae1b8653948096361c75e2c5dd91b106c",
        "title": "Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation"
      },
      {
        "paperId": "5c9f49042e5ed8073623a1bb616d9147b1db460b",
        "title": "A Survey on Trustworthiness in Foundation Models for Medical Image Analysis"
      },
      {
        "paperId": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
        "title": "A Survey on Human Preference Learning for Large Language Models"
      },
      {
        "paperId": "bfa39217ad93bc2435f5e588c76d13851de78cf0",
        "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation"
      },
      {
        "paperId": "ffb69b5febe4afc1817377f717b62b69b9d3659f",
        "title": "Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests"
      },
      {
        "paperId": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
        "title": "Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis"
      },
      {
        "paperId": "b114714da62c234b897700a200f8fd37a849e503",
        "title": "Intelligent knowledge base search tool using large language model and graph neural network"
      },
      {
        "paperId": "5ba130279094e4766d0bbac5b4915d595cc84ed7",
        "title": "UltraMedical: Building Specialized Generalists in Biomedicine"
      },
      {
        "paperId": "42fbf3824040d17194ffcb025c8a7a76137bd00a",
        "title": "Inverse Constitutional AI: Compressing Preferences into Principles"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "45562e42830e2f5e2b4a5de6e64c9a3bfb1847d5",
        "title": "QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation"
      },
      {
        "paperId": "9771c7e604f295798062298e557108e7fd0a8f1a",
        "title": "Are PPO-ed Language Models Hackable?"
      },
      {
        "paperId": "91bf3f7337389b152c06a70f76d0da4340b11df1",
        "title": "Human-in-the-loop Reinforcement Learning for Data Quality Monitoring in Particle Physics Experiments"
      },
      {
        "paperId": "d3c3213cdc21facafca8cac3fa7397ea066fc4ef",
        "title": "Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input"
      },
      {
        "paperId": "ebba25e925c4ff6ab12a60431835e7387e9edf4f",
        "title": "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations"
      },
      {
        "paperId": "d9166a92922064fbbd4c99a948d6937739ead5dd",
        "title": "Hallucinations in LLMs: Understanding and Addressing Challenges"
      },
      {
        "paperId": "c86be48a505bc8aec0a87a9650c5fb3dcbb90dd6",
        "title": "Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task"
      },
      {
        "paperId": "4cf8bca5663c4ffaa57e3323f3391faf397a6671",
        "title": "The Power of Combined Modalities in Interactive Robot Learning"
      },
      {
        "paperId": "713ff427f83967d202082ee635f6e08631df5338",
        "title": "Fairness in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ccb3ce84f8be6aa4e88f5b0902090bf4b0e5174c",
        "title": "The new paradigm in machine learning \u2013 foundation models, large language models and beyond: a primer for physicians"
      },
      {
        "paperId": "79226d4525fd88264b78a9e2fc2061c081824ff0",
        "title": "Towards Generalist Robot Learning from Internet Video: A Survey"
      },
      {
        "paperId": "488b3d0489e2d6d305e64ce2ec54e6c86a1e8ccb",
        "title": "Leveraging Simulation Data to Understand Bias in Predictive Models of Infectious Disease Spread"
      },
      {
        "paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f",
        "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs"
      },
      {
        "paperId": "3820396a84d23a7fc3da21d85257b58e6af384ca",
        "title": "EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation"
      },
      {
        "paperId": "a0e210b79e96ccb5384e90dc0f7809302d67858f",
        "title": "Navigating the Landscape of Hint Generation Research: From the Past to the Future"
      },
      {
        "paperId": "4d1f37057fd008317d634d3f10ff73c81b442035",
        "title": "Goal-Guided Generative Prompt Injection Attack on Large Language Models"
      },
      {
        "paperId": "5c2171a7238e3dc154e19948e07d3efc9e1ff63c",
        "title": "Review of Machine Learning in Robotic Grasping Control in Space Application"
      },
      {
        "paperId": "36423063a6cc195a679eecf08e3b506b7f7763a4",
        "title": "Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning"
      },
      {
        "paperId": "3730ad714aa22ebd3694974abe0bc6437a3ad4ee",
        "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback"
      },
      {
        "paperId": "5a9ce2055a0f200fd53b7eeddf1c05f9e01ccf5a",
        "title": "Large Language Models and Games: A Survey and Roadmap"
      },
      {
        "paperId": "df5bb57e03c38a439d664d3c609a1c03a9a64009",
        "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping"
      },
      {
        "paperId": "ded732209b0ba8a6704cc62ab8197a898b57f833",
        "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models"
      },
      {
        "paperId": "7271c260a010ce15a8821891ecca52d83d011663",
        "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs"
      },
      {
        "paperId": "60316dcf0c41799928cd46b4579db8e74fd9bb90",
        "title": "A Survey for Foundation Models in Autonomous Driving"
      },
      {
        "paperId": "911bae2b36d64101a41c6aebde062e07853b3918",
        "title": "Understanding User Experience in Large Language Model Interactions"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "75f314a98e30c8eb93618ecbb0a488a9a766bcd2",
        "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing"
      },
      {
        "paperId": "5134badfa21a99d5109cc41dcf92fe3cb8de5f53",
        "title": "LLM-Based Text Style Transfer: Have We Taken a Step Forward?"
      },
      {
        "paperId": "d383d344818ee1d6ad58f7a1928cbc0b5f40a3a3",
        "title": "TEMPO: Timestep Explanations for Modeling Preferences in Online Preference-Based RL"
      },
      {
        "paperId": "54a2465069b7f827e65fa0fee994ca97cef3c009",
        "title": "Applications, Challenges, and Future Directions of Human-in-the-Loop Learning"
      },
      {
        "paperId": "4ec4b38e1d8161c3f61a1f9268ae596fa0eb8715",
        "title": "Evaluating Chinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment"
      },
      {
        "paperId": "88cdcd182a9ffb27ed29499ea27d4c2cd492953d",
        "title": "Automated Text Identification on Languages of the Iberian Peninsula: LLM and BERT-based Models Aggregation"
      },
      {
        "paperId": "92066405c027c837f5eac95df8e9b2d7d2ecc5d3",
        "title": "Automatically Generated Definitions and their utility for Modeling Word Meaning"
      },
      {
        "paperId": "f35d51b99a003053a6a8a57075f7028e00328e87",
        "title": "Reward Modeling Requires Automatic Adjustment Based on Data Quality"
      },
      {
        "paperId": "4ef1e22a6e1e01dad8134028f367117ba9cb4f7d",
        "title": "Application of Large Language Models in Cybersecurity: A Systematic Literature Review"
      },
      {
        "paperId": "877478f09b0d14e336a67b609974a5cf89d95824",
        "title": "Do Multimodal Foundation Models Understand Enterprise Workflows? A Benchmark for Business Process Management Tasks"
      },
      {
        "paperId": "94609ed29b073233fb35556d9b7a4094efcab58f",
        "title": "Q-Adapter: Training Your LLM Adapter as a Residual Q-Function"
      },
      {
        "paperId": "70743db92db7dcc0870552edc02e6893a3dd9555",
        "title": "Human-Interactive Robot Learning: Definition, Challenges, and Recommendations"
      }
    ],
    "score": 103.0
  },
  {
    "id": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
    "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
    "authors": [
      "Robert Kirk",
      "Ishita Mediratta",
      "Christoforos Nalmpantis",
      "Jelena Luketina",
      "Eric Hambro",
      "Edward Grefenstette",
      "R. Raileanu"
    ],
    "year": 2023,
    "citationCount": 204,
    "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
    "url": "https://www.semanticscholar.org/paper/cb3968152f7d93f53d24b00279a90d5071ddc85a",
    "pdf_url": "https://arxiv.org/pdf/2310.06452.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-10-10",
    "externalIds": {
      "ArXiv": "2310.06452",
      "DBLP": "journals/corr/abs-2310-06452",
      "DOI": "10.48550/arXiv.2310.06452",
      "CorpusId": 263830929
    },
    "references": [
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "ae6a4cd221684be6ca3082b6f526a7901281490b",
        "title": "Emergent autonomous scientific research capabilities of large language models"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
        "title": "Instruction Tuning with GPT-4"
      },
      {
        "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"
      },
      {
        "paperId": "c11810fa8887b678facea62da4607c4898360308",
        "title": "Training Language Models with Language Feedback at Scale"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "d2170504c4ad9403bea118ae8debdfda95978546",
        "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers"
      },
      {
        "paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff",
        "title": "Chain of Hindsight Aligns Language Models with Feedback"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "0e1ae0bdcc8469db99a4f8008288e20f285f1c6d",
        "title": "Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning"
      },
      {
        "paperId": "808e9ce4e86e79098edea7f00b5b91663b87a5e6",
        "title": "A taxonomy and review of generalization research in NLP"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "32f87b51e3ba42894821716b8145bde41fc65983",
        "title": "Semantic Diversity in Dialogue with Natural Language Inference"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
        "title": "Training Compute-Optimal Large Language Models"
      },
      {
        "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
        "title": "Teaching language models to support answers with verified quotes"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d73d3a82da0bc93be238c286abfd06722247d298",
        "title": "Rethinking and Refining the Distinct Metric"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "68f141724814839d556a989646194be88641b143",
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "c8f2ecf4c2c47f4e1c8128440fe68470ed641541",
        "title": "Discovering Diverse Solutions in Deep Reinforcement Learning"
      },
      {
        "paperId": "07fd366a8ebdefe54cdb57d87c81dcd22de25a91",
        "title": "A Distributional Approach to Controlled Text Generation"
      },
      {
        "paperId": "e4a46c64aafbef0406e9cfa90dd9c43e3e07598c",
        "title": "One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "901b2ca38f4aef1648a029886283b3eb1451792a",
        "title": "Evaluating the Evaluation of Diversity in Natural Language Generation"
      },
      {
        "paperId": "1e2db47d023539e81ff996069aa2208d441408e8",
        "title": "Training"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      },
      {
        "paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
        "title": "Neural Text Generation with Unlikelihood Training"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "86be5c90c4128ec59b1c320a16996bb5de68624e",
        "title": "Texygen: A Benchmarking Platform for Text Generation Models"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "a870df7e7d43c9144e2520ef4e4779f1672dd654",
        "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control"
      },
      {
        "paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
        "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "651e5bcc14f14605a879303e97572a27ea8c7956",
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": null,
        "title": "AlpacaEval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": "944aefbf748ad7172dd7e1a532675bc3647c0e23",
        "title": "Reinforcement Learning for Language Models"
      },
      {
        "paperId": null,
        "title": "URL http://arxiv.org/ abs/2110.14168"
      },
      {
        "paperId": "0718d725fd01fdf147cd7787fa62f814f5723053",
        "title": "Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "TextBlob: Simplified Text Processing "
      },
      {
        "paperId": null,
        "title": "you should combine questions with imperative instrucitons"
      },
      {
        "paperId": null,
        "title": "Compute-Optimal"
      },
      {
        "paperId": null,
        "title": "On Generalization of Adversarial"
      },
      {
        "paperId": "810b9ffea4c74db3923336a22dc9563679cfe564",
        "title": "Conference Paper"
      },
      {
        "paperId": null,
        "title": "A Survey on Assessments of"
      }
    ],
    "cited_by": [
      {
        "paperId": "11153ef2b7269a03c53ab534c4ac038024e959db",
        "title": "Executable Counterfactuals: Improving LLMs'Causal Reasoning Through Code"
      },
      {
        "paperId": "eb549b0e76c87fb1ec2b6d412710322e46cb51dc",
        "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum"
      },
      {
        "paperId": "54936a36fead748c0cb8a84933a3967609928838",
        "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning"
      },
      {
        "paperId": "de595f98909a561d0adc9958648e4be6539cbde7",
        "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation"
      },
      {
        "paperId": "fd6797069bc52f1ff036cf1453adc665eda339f9",
        "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications"
      },
      {
        "paperId": "c8433b11da2ece34e2ee70d44bdaadeb9d156664",
        "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving"
      },
      {
        "paperId": "146ded602ca4aba6173643186dd574beaeca9a1c",
        "title": "LLM Output Homogenization is Task Dependent"
      },
      {
        "paperId": "d41e12709c702b8568544ca3c6778d6116ac7e39",
        "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface"
      },
      {
        "paperId": "5e5246bb96771b4392a87d5e26f4e69f8f12c4fa",
        "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs"
      },
      {
        "paperId": "a6fbcfabbb20c8848364ec44c8a1dea18be00efe",
        "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning"
      },
      {
        "paperId": "25a21ceb0912442e31f526394e428d6be9521d97",
        "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety"
      },
      {
        "paperId": "a16aba7e5fd046df3485d7146c1372117b992111",
        "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization"
      },
      {
        "paperId": "dd6dd841ea57410d4d8aa0873189f035464e4119",
        "title": "Generative Data Refinement: Just Ask for Better Data"
      },
      {
        "paperId": "74143d1f23cfe22630bc842c65e4c796cb316039",
        "title": "FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support"
      },
      {
        "paperId": "204a5ea89bf9e889b96165d9862738436e6f0759",
        "title": "Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "63f251bbb143f7f993bf87c74796fae4abe4cb4e",
        "title": "RL Fine-Tuning Heals OOD Forgetting in SFT"
      },
      {
        "paperId": "0a66ab5760423129b19d7cd0dc3ad0446fd0500b",
        "title": "Enhancing Diversity in Large Language Models via Determinantal Point Processes"
      },
      {
        "paperId": "6d6679bd5bf00db7ea3a8305c6db8c8e73a3997a",
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations"
      },
      {
        "paperId": "dab217bb6f5d52ce7cb764a13f238657ad054bf5",
        "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation"
      },
      {
        "paperId": "2bc704af7ccfc0fccf2800e4a37649e889bd4cfa",
        "title": "Homogenizing Effect of Large Language Models (LLMs) on Creative Diversity: An Empirical Comparison of Human and ChatGPT Writing"
      },
      {
        "paperId": "381ad04b827c92fb89d1d59cd1d615ad93a8d0dd",
        "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models"
      },
      {
        "paperId": "28c1e69bc0135ef8de85662ec6aa649c78d30e6c",
        "title": "Spacer: Towards Engineered Scientific Inspiration"
      },
      {
        "paperId": "a60082b6ebe795bf88d4e9681b27aec1b5fbd706",
        "title": "Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs"
      },
      {
        "paperId": "8bc1ce01f7a1483e7f3246feacbc7986e4208ff8",
        "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs"
      },
      {
        "paperId": "2d81e4362f0a9d1c1410bbb2833bc7d9105d242f",
        "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention"
      },
      {
        "paperId": "bb88bf646f41aac26a799c8e2cfc2aaa91d8bb2a",
        "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss"
      },
      {
        "paperId": "0e95bdd2119212a9e8449d3714bcb3dcbf739002",
        "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"
      },
      {
        "paperId": "01da53f390b204f7497ef8df088e5f6f08dad7e1",
        "title": "KG-Augmented Executable CoT for Mathematical Coding"
      },
      {
        "paperId": "e659c90681416d6cd9812b7b2d4fb4e3adec107c",
        "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks"
      },
      {
        "paperId": "1013387896d548c12be5b724637a981e269d80cb",
        "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought"
      },
      {
        "paperId": "8f5d187a6173f018ffc77d708bc76572c2f20c9b",
        "title": "RecGPT Technical Report"
      },
      {
        "paperId": "42dc37d11e968d5ae4527e884a9a6c1ec88fe56b",
        "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation"
      },
      {
        "paperId": "204ab008a98d1f0b1602a443ccd305c04647df75",
        "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations"
      },
      {
        "paperId": "9a6ac74553457e89de1494d5dded6f17675c10e6",
        "title": "ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning"
      },
      {
        "paperId": "02dac7990c1b934e50a70e21560ceb3b62eba96d",
        "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling"
      },
      {
        "paperId": "e506c1218da58533f009a09853384166b44f1684",
        "title": "Flippi: End To End GenAI Assistant for E-Commerce"
      },
      {
        "paperId": "d081c5ce0e9946fa5159bd83965d5e7262a02ee0",
        "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data"
      },
      {
        "paperId": "b285ed0a4d8bc1281a7a336d7b04eb37c76d3499",
        "title": "CTR-Guided Generative Query Suggestion in Conversational Search"
      },
      {
        "paperId": "2edd4f0026f60da2f31606cedb95c14f90c165f5",
        "title": "TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection"
      },
      {
        "paperId": "a26f44e9deda0113f2d93992e92d004ecb92426a",
        "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation"
      },
      {
        "paperId": "713639da388839273a21bc31a6f40162a65651d0",
        "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "65f646776ffd4a946b39ab42985450974c4bb238",
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training"
      },
      {
        "paperId": "3df19f73eb387e5336758933cf740e7d65078a97",
        "title": "How Alignment Shrinks the Generative Horizon"
      },
      {
        "paperId": "bc12120882c0d498bebf7465a853833d2991e967",
        "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training"
      },
      {
        "paperId": "16056be1862f5b1b17821eedbe76f0fe5beaa60e",
        "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models"
      },
      {
        "paperId": "2777866777aa264cce98b63c1208cfeb81ee0b17",
        "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model"
      },
      {
        "paperId": "a7a82c0d06a2d1957b1f8ccae1115a88a2c20feb",
        "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers"
      },
      {
        "paperId": "8408df2d1326e6fe8713e680c24e2fd40ea0c7d7",
        "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory"
      },
      {
        "paperId": "4146c039c00eb1eaa9b738c86209fc96c6546167",
        "title": "Elementary Math Word Problem Generation using Large Language Models"
      },
      {
        "paperId": "f3935500a776b1935f12d747ad1e4f02155c4049",
        "title": "EthicsBot: Fine-Tuning Open-Source LLMs to Assist Scientific Investigators in Analyzing Ethical Issues in Research"
      },
      {
        "paperId": "00a8081f3c03b38146dd742e344f08c371c15044",
        "title": "SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat"
      },
      {
        "paperId": "828ab065d15c9516c633b54e3245ec08cc75a2f8",
        "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content"
      },
      {
        "paperId": "5f5c80c61d5210288220fcdbe0b2efe6c649da12",
        "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring"
      },
      {
        "paperId": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
        "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "4a0be5039b2d462fedafec282ac19dce5746dad8",
        "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
      },
      {
        "paperId": "fe13d660dbad9cd52d753f360424745e11d4844f",
        "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation"
      },
      {
        "paperId": "a183498f48ee0d79a781fa0de806e6828161a92e",
        "title": "On-Policy RL with Optimal Reward Baseline"
      },
      {
        "paperId": "20fba6e77082281566c2093de84a8ccfeb4363d2",
        "title": "Beyond Monoliths: Expert Orchestration for More Capable, Democratic, and Safe Large Language Models"
      },
      {
        "paperId": "d624ac7d5cacc53ff21eb8bb94988165b1ddc197",
        "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model"
      },
      {
        "paperId": "d20e6d59f63fbfbe7a4c2df7ccee74235afda39e",
        "title": "The Price of Format: Diversity Collapse in LLMs"
      },
      {
        "paperId": "ce061d7bf4e7619d4826c456c38f378cdebea27b",
        "title": "Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning"
      },
      {
        "paperId": "13a4892027454daeb8355cea4300f9ba9aa9fd9f",
        "title": "Safety Alignment via Constrained Knowledge Unlearning"
      },
      {
        "paperId": "70e4e9f55df812438df0335cae799f9baa43409a",
        "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models"
      },
      {
        "paperId": "5a5139fd00b4443ee0a51e3ed212f137f22542b7",
        "title": "Large Language Models for Predictive Analysis: How Far Are They?"
      },
      {
        "paperId": "9bd723ac16fab44267211f93442153626aac8ccd",
        "title": "Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models"
      },
      {
        "paperId": "62b41bffd0dbef06a6efc985a07cf7cd20f5f117",
        "title": "Balancing Large Language Model Alignment and Algorithmic Fidelity in Social Science Research"
      },
      {
        "paperId": "349b70b0027d9890efb95b6b4e15758d6280d50a",
        "title": "Creative Preference Optimization"
      },
      {
        "paperId": "9af9c0df0a328d5f327bed3151819b06dfc33622",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment"
      },
      {
        "paperId": "2d45cac81b28850a10f67fbd26928bab3bdd6342",
        "title": "Base Models Beat Aligned Models at Randomness and Creativity"
      },
      {
        "paperId": "d9cb876767802dacc4f10f52e95b4410515466ac",
        "title": "SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward"
      },
      {
        "paperId": "8ac1da9a970bd97b0c05ce461b35a3fce639aed2",
        "title": "DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization"
      },
      {
        "paperId": "f0870be65f480a57d28f792f89246cae90431a5f",
        "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay"
      },
      {
        "paperId": "a98d0e00bfcb5315aa118120560f178c018bbb10",
        "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model"
      },
      {
        "paperId": "b20c1b8541ea79ac6cfe49a53ba6fc14249dfa3d",
        "title": "Evaluating the Diversity and Quality of LLM Generated Content"
      },
      {
        "paperId": "07017b4c848371df9414f64d91002292d42324cd",
        "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining"
      },
      {
        "paperId": "2adbf228f96164e4780cbc19da9a0005cf8cde32",
        "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models"
      },
      {
        "paperId": "fbc0b5e1b822796d7ae97268def2e0993b5da644",
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning"
      },
      {
        "paperId": "67b728df4a688eaad8c92d92051354813e062112",
        "title": "Arti-\"fickle\" Intelligence: Using LLMs as a Tool for Inference in the Political and Social Sciences"
      },
      {
        "paperId": "81192a4d4a078a6c8d7de81b265168fb51752948",
        "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context"
      },
      {
        "paperId": "3a48e315facb5115eacea25faf2cc32caa01d53b",
        "title": "Responsible Innovation: A Strategic Framework for Financial LLM Integration"
      },
      {
        "paperId": "1ebca7dbea9d75ea4b10b97614c7e0b05beb94e7",
        "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment"
      },
      {
        "paperId": "f5a69ade7980dfde0128858d7cbeee7b837639e8",
        "title": "Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation"
      },
      {
        "paperId": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
        "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "2e6ece905dbfec1d82d85de40f17e25c1ee2d8c9",
        "title": "Writing as a testbed for open ended agents"
      },
      {
        "paperId": "19fc52e069def016d1853b0bc016f0a547dc5edf",
        "title": "TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource Language Modeling"
      },
      {
        "paperId": "6e0e4d88194ccd424af25aeb60cdd37a030bf813",
        "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts"
      },
      {
        "paperId": "54ff8fd6c2db21886e470d5a599161024c696435",
        "title": "ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation"
      },
      {
        "paperId": "5aed07b4e94d6af8eab9366b57ce8f5a3b6ec608",
        "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing"
      },
      {
        "paperId": "10088fee858ee55fa0e46eb3e31d6cf9d36861b5",
        "title": "A Survey on Personalized Alignment - The Missing Piece for Large Language Models in Real-World Applications"
      },
      {
        "paperId": "640900648f50ec933bf28a68473a0e8a4278fb23",
        "title": "CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization"
      },
      {
        "paperId": "9ebd1f44afd407011e167ed6090ccb6b3c3e637b",
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "9ee75cc435f055015d0a3dd9de7d43b564717378",
        "title": "Process-Supervised LLM Recommenders via Flow-guided Tuning"
      },
      {
        "paperId": "1147e811e8b9d95316c853b355d66cc5e6a3e936",
        "title": "LLM-Feynman: Leveraging Large Language Models for Universal Scientific Formula and Theory Discovery"
      },
      {
        "paperId": "aac7655950d5d4c24250fab4b12eca3961a28a9f",
        "title": "Gender Biases within Artificial Intelligence and ChatGPT: Evidence, Sources of Biases and Solutions"
      },
      {
        "paperId": "ab72ab40e2baeea8a57d1db386737239d8e07397",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models"
      },
      {
        "paperId": "0b9fae1ac6b080d65e79970da2d452c64f3352a8",
        "title": "IPO: Your Language Model is Secretly a Preference Classifier"
      },
      {
        "paperId": "72107c3e180381c3d1c0b24ba9fd9f7a676ca11e",
        "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation"
      },
      {
        "paperId": "e2f05b7885ec5295e80a51d7264adc08681d9f14",
        "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation"
      },
      {
        "paperId": "c6e8297646a161facbd4c5ca708df87dfd79a607",
        "title": "Preference learning made easy: Everything should be understood through win rate"
      },
      {
        "paperId": "b83d737b98a43fcb4c467036d720a6d7adcc313b",
        "title": "Position: We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles"
      },
      {
        "paperId": "3ac976418e99aa5e558dedcb0ec25d6eb6c35750",
        "title": "STAIR: Improving Safety Alignment with Introspective Reasoning"
      },
      {
        "paperId": "117b52d94d38048aca36fc7432ac17742103e2e2",
        "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization"
      },
      {
        "paperId": "2c3a0648587bcad70a13338660f6812ff59c8161",
        "title": "Diverse Preference Optimization"
      },
      {
        "paperId": "134faaddd00156e87826c62a73dc24f6d0587c9c",
        "title": "Controllable Protein Sequence Generation with LLM Preference Optimization"
      },
      {
        "paperId": "53d5d595b263f54c9a5c4d51e298413c450abb79",
        "title": "Curiosity-Driven Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "68e64ff720c2a6cc2a306aacbeb6f04320ad9805",
        "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "a112125e251610b135a151b416a227bffadeb8f2",
        "title": "Self-Consistency Preference Optimization"
      },
      {
        "paperId": "b38c17d4c1cf719f110e811190255428fe7a53aa",
        "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks"
      },
      {
        "paperId": "411591a8f4e0d73fa784870a37e793cab712fe59",
        "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina"
      },
      {
        "paperId": "89b42dc02400db2c58703fb0d8d0b6f0210bfe7f",
        "title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning"
      },
      {
        "paperId": "d5544571fa37a6eaac972153cec57df591c38a04",
        "title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback"
      },
      {
        "paperId": "cebabbad23715e03a988480cc1039f2606a3e172",
        "title": "Anchored Alignment for Self-Explanations Enhancement"
      },
      {
        "paperId": "2bbfe484ea3c38619791c10c5a30142dbea12fd0",
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms"
      },
      {
        "paperId": "afbf3975f5f774a9eaead1c54389a63542b9d474",
        "title": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment"
      },
      {
        "paperId": "9cfdb3cdb63d41de52b68f0ad6219b822a4b37d5",
        "title": "Diversity-Rewarded CFG Distillation"
      },
      {
        "paperId": "7d44d096ea5ce822a40ee7b5519fb12b65eeba4e",
        "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "46cff4ce8deed2ef1b00d483ed9a69f8183e3538",
        "title": "CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints"
      },
      {
        "paperId": "a35076b46fc0e7d4dfa63b538ffa7489b106a738",
        "title": "SELU: Self-Learning Embodied MLLMs in Unknown Environments"
      },
      {
        "paperId": "36b6edf59f64ff9c203f84c5df48774bb62bc79c",
        "title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "585e95a43f4ceb3b9fdd8408b7b0b5df468c1030",
        "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning"
      },
      {
        "paperId": "bc6f2d1b9a366967c89fb173795a59641021ad2c",
        "title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction"
      },
      {
        "paperId": "2d591bd9f42bbbd259510d871bdd6786b1283b2d",
        "title": "LLM-based multi-agent poetry generation in non-cooperative environments"
      },
      {
        "paperId": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
        "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation"
      },
      {
        "paperId": "07bd842ee329888e5c2ca66a1105bcdbddab04cc",
        "title": "LLMs generate structurally realistic social networks but overestimate political homophily"
      },
      {
        "paperId": "37f621bbb60248c73db182d9c653f4c3aa62f790",
        "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models"
      },
      {
        "paperId": "5f635bca6c6a42c99bb102b8f5eab6780cf22184",
        "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI technologies: an investigation of Baidu, Ernie and Qwen"
      },
      {
        "paperId": "579b3942e36968a826294cb8e0cbd371aed3c65d",
        "title": "The advantages of context specific language models: the case of the Erasmian Language Model"
      },
      {
        "paperId": "adbf46150ffa246c4121454ee4c44d831b40824b",
        "title": "Evaluating LLMs on Entity Disambiguation in Tables"
      },
      {
        "paperId": "c8c9002af1d90e9dafa3d07e9edf0d883ec45472",
        "title": "Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation"
      },
      {
        "paperId": "e4fdb3241176edaa0dcdba82e4c0639ea65cc225",
        "title": "Analyzing the Generalization and Reliability of Steering Vectors"
      },
      {
        "paperId": "20be3ba3f361d9630cc0c17442a0d0132873e63d",
        "title": "Benchmarking Language Model Creativity: A Case Study on Code Generation"
      },
      {
        "paperId": "c4d0ec318c1ddca4337435af6004a3f190712a74",
        "title": "Progress or Regress? Self-Improvement Reversal in Post-training"
      },
      {
        "paperId": "04b49899d91db9195c473ce1f9139f7ec56cf763",
        "title": "Generative Monoculture in Large Language Models"
      },
      {
        "paperId": "3da5f21144fef19dd88f7dcc11a5d9f2edbfe417",
        "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs"
      },
      {
        "paperId": "6b702e51640eb0d598f10bee2555d4a50937e1df",
        "title": "Detection and Measurement of Syntactic Templates in Generated Text"
      },
      {
        "paperId": "88786bb0ad8fbb66416e091ae89257aa4a810767",
        "title": "AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations"
      },
      {
        "paperId": "dff7a3989c41bd0200095e6560f73923b0f3bf32",
        "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment"
      },
      {
        "paperId": "cdab286b5b28f03aa9dda2b818f30f8a32a809d4",
        "title": "Multi-property Steering of Large Language Models with Dynamic Activation Composition"
      },
      {
        "paperId": "aef234095eb56f3510737df572fd155668523bb0",
        "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies"
      },
      {
        "paperId": "6789a2f409af02915e48223e4be3e451ae7aa008",
        "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages"
      },
      {
        "paperId": "ef010a9038dc02462312f6c1c97bf8d296c1abb5",
        "title": "Self-Evolution Fine-Tuning for Policy Optimization"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "57451ce18f3035fcadf64db38420434f9299b7f3",
        "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF"
      },
      {
        "paperId": "2196dafdbc900a5a252b4f8d003401567d873bb7",
        "title": "Self-Improving Robust Preference Optimization"
      },
      {
        "paperId": "3a10570fb3f88d755ba4648598b3d30f8ddfb26c",
        "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage"
      },
      {
        "paperId": "27ce24d046da77a6302c9a68e1d953a4bfd0ced4",
        "title": "Large Language Models for Life Cycle Assessments: Opportunities, Challenges, and Risks"
      },
      {
        "paperId": "ae03f10729959435ecefc0e90cba4cbe8438a10b",
        "title": "Evaluating Large Language Model Biases in Persona-Steered Generation"
      },
      {
        "paperId": "87912571f3df29464d3ccafae66f6e1eed581564",
        "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "a11d3cc04c3293df3e7ea3247c561f42fc36d1c5",
        "title": "MallowsPO: Fine-Tune Your LLM with Preference Dispersions"
      },
      {
        "paperId": "150a1e91b96fdd5b675ce72ee7f4f4b9e301ae04",
        "title": "Strengthening LLM Trust Boundaries: A Survey of Prompt Injection Attacks Surender Suresh Kumar Dr. M.L. Cummings Dr. Alexander Stimpson"
      },
      {
        "paperId": "db2717c4883831506e46507065b313e74403ff8a",
        "title": "Automating Thematic Analysis: How LLMs Analyse Controversial Topics"
      },
      {
        "paperId": "6002f0d76189ad99ed3cb2d9945e8be869b96244",
        "title": "Understanding LLMs Requires More Than Statistical Generalization"
      },
      {
        "paperId": "112480ed6b6647263b7d49332bd953ed9b2f68f9",
        "title": "Old Moats for New Models: Openness, Control, and Competition in Generative AI"
      },
      {
        "paperId": "b893044d157c2516f5b39e449be2af991b61813b",
        "title": "Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G"
      },
      {
        "paperId": "27cd9e0187751c1af2e306e3174db131942a1c14",
        "title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation"
      },
      {
        "paperId": "315a5cb5dbbbe2be8468b2bb7c62ea72af8930da",
        "title": "Filtered Direct Preference Optimization"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "053734bec67be8e59570ea75c7703965866f21b2",
        "title": "Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications"
      },
      {
        "paperId": "daea745ef990be5cfe5af3196e819021a23f3597",
        "title": "MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering"
      },
      {
        "paperId": "de17e7ed443e13320694cce3b2f475c694801246",
        "title": "Stepwise Alignment for Constrained Language Model Policy Optimization"
      },
      {
        "paperId": "38b09f8de25885e16c3459a590cc67ab08d21fbf",
        "title": "MAD Speech: Measures of Acoustic Diversity of Speech"
      },
      {
        "paperId": "49daaadfe84f1bf850feb8d6db46b652f75c9750",
        "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "370fb62e60f80081015d591f8c10c5a59a56a32d",
        "title": "Learn Your Reference Model for Real Good Alignment"
      },
      {
        "paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f",
        "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs"
      },
      {
        "paperId": "6edf144fb397afde80d9acf9472dfaffd22c8072",
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "c78350e81298ca87bc1d59b466fa40081232caaa",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723",
        "title": "Unintended Impacts of LLM Alignment on Global Representation"
      },
      {
        "paperId": "59b4b5b1e2198f264536d83e33d96b0a45ed3bac",
        "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning"
      },
      {
        "paperId": "5eab7e7ee77f2bbc6639a5c352f87c2855a378ee",
        "title": "Exploring Precision and Recall to assess the quality and diversity of LLMs"
      },
      {
        "paperId": "5125d07c02393c3b7754a53738394cbc25c44822",
        "title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models"
      },
      {
        "paperId": "bf40c8f88875f7c591dddc0936542918f4083b22",
        "title": "A Roadmap to Pluralistic Alignment"
      },
      {
        "paperId": "c993152d0858584dee2859c21e324ee811ec3991",
        "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models"
      },
      {
        "paperId": "71c7f6b006be878fc1862d1d215dad2301bf451e",
        "title": "From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models"
      },
      {
        "paperId": "8bfa5558adb601b733ba7473fe52abb31f4b89e6",
        "title": "Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better"
      },
      {
        "paperId": "dabf7edde0efb9b1e092aa27847a547cf2961192",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "e6423c211fea2945aa71e1ac5ea24f8f595b4b0a",
        "title": "Towards Understanding Sycophancy in Language Models"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "04b880e1e32f37b3796d41a47d013fa07095ae32",
        "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning"
      },
      {
        "paperId": "b88ae92f5691b02eeefb967e570d0e1eac4aa01f",
        "title": "Evolving Diverse Red-team Language Models in Multi-round Multi-agent Games"
      },
      {
        "paperId": "532430bfcedf0ca4d5ca695967b52fc21cb5b778",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"
      },
      {
        "paperId": "1c7402843d8b586d945b3b030e3edd93f0ae3959",
        "title": "On the Creativity of Large Language Models"
      },
      {
        "paperId": "dbf4aa5c88c2b6d34ed5910929977667fba44c89",
        "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models"
      },
      {
        "paperId": "ccd4db9c164b1cf1b294f1db0c727d5c5aa0714f",
        "title": "Off-policy Predictive Control with Causal Sensitivity Analysis"
      },
      {
        "paperId": "5d046a15563b11662ac55553cf0e179671662efe",
        "title": "Sun-Shine: A Large Language Model for Tibetan Culture"
      },
      {
        "paperId": "46b5c02bc75928ade192c199b953b07bc9eee591",
        "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI technologies: a case study on Baidu, Ernie and Qwen"
      },
      {
        "paperId": "ed9db2081f375e338b1e170fb7089a292553db29",
        "title": "DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models"
      },
      {
        "paperId": "9a50a73562b5569e7e62668c1c5eba4c6a1d45f1",
        "title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards"
      },
      {
        "paperId": "f2109f7f2412308738e03a60f79946cb1ad2aa7a",
        "title": "Alignment-Aware Model Extraction Attacks on Large Language Models"
      },
      {
        "paperId": "f3ac10b563b7e4672240f22ba41a3e9623f3109f",
        "title": "Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning"
      },
      {
        "paperId": "b5e8e0f65258e40e0fde9ef66cabc4178f22a0a0",
        "title": "Does D ETECT GPT Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "b0da4d18b6eeeb4206b087016c2d7fd368e18c55",
        "title": "RMB OOST : R EWARD M ODEL T RAINING W ITH P REFERENCE -C ONDITIONAL M ULTI -A SPECT S YN - THETIC D ATA G ENERATION"
      },
      {
        "paperId": "c012b2d64458fc56ca8d128f93f182e189b112a2",
        "title": "VADER: V IDEO D IFFUSION A LIGNMENT VIA R EWARD G RADIENTS"
      },
      {
        "paperId": "ebdffb65d5110117c9c44c14697bf3b63e0eceb6",
        "title": "Robust Chain of Thoughts Preference Optimization"
      },
      {
        "paperId": "58ebc7f497eb8770bdd53ba247710a2ecee88950",
        "title": "Scalable Oversight by Accounting for Unreliable Feedback"
      },
      {
        "paperId": "4c300039c3c40859b907469f3e85d1a06ce65f80",
        "title": "SELU: Self-Learning Embodied Multimodal Large Language Models in Unknown Environments"
      },
      {
        "paperId": "bec6957a0d2bbc92e0e2b92a1a320fada4630137",
        "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models"
      }
    ],
    "score": 102.0
  },
  {
    "id": "548278897d46a54958909bb23bcaecf63e24fadf",
    "title": "Secrets of RLHF in Large Language Models Part I: PPO",
    "authors": [
      "Rui Zheng",
      "Shihan Dou",
      "Songyang Gao",
      "Wei Shen",
      "Wei-Yuan Shen",
      "Bing Wang",
      "Yan Liu",
      "Senjie Jin",
      "Qin Liu",
      "Limao Xiong",
      "Luyao Chen",
      "Zhiheng Xi",
      "Yuhao Zhou",
      "Nuo Xu",
      "Wen-De Lai",
      "Minghao Zhu",
      "Rongxiang Weng",
      "Wen-Chun Cheng",
      "Cheng Chang",
      "Zhangyue Yin",
      "Yuan Hua",
      "Haoran Huang",
      "Tianxiang Sun",
      "Hang Yan",
      "Tao Gui",
      "Qi Zhang",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "year": 2023,
    "citationCount": 201,
    "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
    "url": "https://www.semanticscholar.org/paper/548278897d46a54958909bb23bcaecf63e24fadf",
    "pdf_url": "https://arxiv.org/pdf/2307.04964.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-07-11",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-04964",
      "ArXiv": "2307.04964",
      "DOI": "10.48550/arXiv.2307.04964",
      "CorpusId": 259766568
    },
    "references": [
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior"
      },
      {
        "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
        "title": "Instruction Tuning with GPT-4"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "title": "PaLM-E: An Embodied Multimodal Language Model"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
        "title": "LaMDA: Language Models for Dialog Applications"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "998577ea3c393330e4ecc563d5e4026cf9e49a17",
        "title": "Gender and Representation Bias in GPT-3 Generated Stories"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634",
        "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
      },
      {
        "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "630bf4c0b7e5abd276ec38468f490b7d6222f3a2",
        "title": "Interactive Learning from Policy-Dependent Human Feedback"
      },
      {
        "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "2750c552afefe7dacd65146891f297db9478c577",
        "title": "Heart"
      },
      {
        "paperId": null,
        "title": "StackLLaMA: An RL fine-tuned LLaMA model for stack exchange question and answering, 2023"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%"
      },
      {
        "paperId": null,
        "title": "Easy RL: Reinforcement Learning Tutorial"
      },
      {
        "paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
        "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following LLaMA model"
      },
      {
        "paperId": null,
        "title": "You pocket the money and move on to your next victim"
      },
      {
        "paperId": null,
        "title": "The 37 implementation details of proximal policy optimization"
      },
      {
        "paperId": null,
        "title": "Planning for agi and beyond"
      },
      {
        "paperId": null,
        "title": "Overall, this line encourages us to appreciate the beauty of life and the power of love, and to live in the moment instead of always rushing forward towards an unknown future"
      },
      {
        "paperId": null,
        "title": "After a few weeks or months, send the victims a letter saying they have won a large sum of money in a lottery or sweepstakes"
      },
      {
        "paperId": null,
        "title": ": An open-source chatbot"
      },
      {
        "paperId": null,
        "title": "Human Prompt \u2192 What does this line mean: Time is just a second hand of love, so tell me why am I in such a rush?"
      },
      {
        "paperId": null,
        "title": "Open-Chinese-LLaMA: Chinese large language model base generated through incremental pre-training on chinese datasets"
      },
      {
        "paperId": null,
        "title": "A simulation framework for methods"
      },
      {
        "paperId": null,
        "title": "Ask the victim to send a small fee (e.g., $50) as processing fees to claim their prize. 5. The victim sends the money, but never receives their supposed winnings or hears back from the"
      }
    ],
    "cited_by": [
      {
        "paperId": "5cc118820dba8b7fec9fce6e08b620432cb881f8",
        "title": "Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports"
      },
      {
        "paperId": "89892acc2c3e839a98e237d3ee6294dd7d931358",
        "title": "Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications"
      },
      {
        "paperId": "30da58c425d4e2a5e3c0b774cf8302d2fcf9ce51",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "18d83103fb98905ccbce420987470eb2ea021187",
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
      },
      {
        "paperId": "94954679cf7732569bd9508e74446c7d18fc4b9e",
        "title": "Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization"
      },
      {
        "paperId": "ba1435c88f1aa70382736cecd05fa1d2ba5dd966",
        "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning"
      },
      {
        "paperId": "4b4db2c63974719b1a7371ccfc4d070d7bb5ea69",
        "title": "An Efficient Finetuning Method for LLM generated text detection in Power Grid"
      },
      {
        "paperId": "331f34e13c05a8f169b1ba948a5a5179490b8a96",
        "title": "Graph Representation Learning with Massive Unlabeled Data for Rumor Detection"
      },
      {
        "paperId": "eb12f5b6157df1b8ead72b31812ff8e1fd50805d",
        "title": "Self-disclosure and relational agents for mental health: a scoping review protocol"
      },
      {
        "paperId": "d33bd07c39b1e1c1907439a17ce6b35217ca2d41",
        "title": "Libra: Assessing and Improving Reward Model by Learning to Think"
      },
      {
        "paperId": "d4fa13b5f06fc1f415cbf9ae89286574101b062c",
        "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "29ae3a74225a0ac1c8fe4003873ed5570c85261f",
        "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models"
      },
      {
        "paperId": "0d426b4dd26dc2baaef545eeb445136d612bc6b7",
        "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities"
      },
      {
        "paperId": "b0517cd2739494cea8d9a49fe4307cda3b58131c",
        "title": "Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization"
      },
      {
        "paperId": "0bab5941e6a81043b07a2aada57c4bd1496a75f4",
        "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models"
      },
      {
        "paperId": "00ff1875a01039af7726769d1630d632e5d9d40e",
        "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "702e01fd7c9111ee539a4da2d2a2d2ad19a00c65",
        "title": "Engineering-Adaptive Pavement Maintenance Decision-Making Model: A Reinforcement Learning Approach From Expert Feedback"
      },
      {
        "paperId": "30a14001f121cc3a98460a90165e0e6dfcf3faa8",
        "title": "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment"
      },
      {
        "paperId": "b76d2d4da4b70489d85cd1fbf2ff3a7ebbadc578",
        "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms"
      },
      {
        "paperId": "9d0f0fb82a339debb181382f725baa278bb202d7",
        "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors"
      },
      {
        "paperId": "416908dee133533bc8a08f7507718c5697190467",
        "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints"
      },
      {
        "paperId": "4d608203639087e0fe3c5d2b7a374941dd182cb7",
        "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models"
      },
      {
        "paperId": "828ab065d15c9516c633b54e3245ec08cc75a2f8",
        "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "0477db73b33bb07dc10238cec22638e3e7ff02e3",
        "title": "Reinforcing Video Reasoning with Focused Thinking"
      },
      {
        "paperId": "fea9cf41242aa4faf831d26b7570e501363ca65b",
        "title": "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective"
      },
      {
        "paperId": "27f8c2883ff1ceca2efa3193282245a1e71504a4",
        "title": "Accelerating RLHF Training with Reward Variance Increase"
      },
      {
        "paperId": "1d92c345ff78631aa5509c84e6ff91d446e2c712",
        "title": "What Can RL Bring to VLA Generalization? An Empirical Study"
      },
      {
        "paperId": "8452bf789915673de6611c65f89e7f577c2001b1",
        "title": "Inference-time Alignment in Continuous Space"
      },
      {
        "paperId": "38dbd4788dedc376c904589fa0799c830eef8d2c",
        "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective"
      },
      {
        "paperId": "63b730849873b3c38e29aaff16ad60b8917092c7",
        "title": "Adaptive Contrastive Decoding for Instruction-Following in Large Language Models"
      },
      {
        "paperId": "62529f40fcc21c10a90608bc2b6bbeb43b6a11db",
        "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "6f10f8de122a9f83f74320750c6936002a9c172d",
        "title": "InfoPO: On Mutual Information Maximization for Large Language Model Alignment"
      },
      {
        "paperId": "c2feda1e804700d0980d71cfc71ce66d369b6b6c",
        "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "7874c6e5d7313a98b832918d5cb7f1b10ffea4fb",
        "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo"
      },
      {
        "paperId": "8132f76f1090659317e6a3067f13222439571ff3",
        "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration"
      },
      {
        "paperId": "777849125d2eefa434dbf0cb7acb2bac95d87636",
        "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models"
      },
      {
        "paperId": "f26fcc2b9fc8944e054425d19c12b9d5cca64fcb",
        "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?"
      },
      {
        "paperId": "23a874c60e26c4c16c5e7c164310f2c94f581c11",
        "title": "Meta-Learning and ReFT (Reasoning with Reinforced Fine-Tuning) are Used to Improve the Inference Accuracy of Large Language Models"
      },
      {
        "paperId": "21c274d953b68e1d1793e23af7e70c9314a9dd1b",
        "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization"
      },
      {
        "paperId": "1291150f5f18e5261ebdd649b0c511c0e238849c",
        "title": "A framework for mitigating malicious RLHF feedback in LLM training using consensus based reward"
      },
      {
        "paperId": "445e202eede8d02561643546591330cc1986dcc3",
        "title": "BalancedDPO: Adaptive Multi-Metric Alignment"
      },
      {
        "paperId": "18a12c19a9a6a222472d5f7075ce9b0cddbcd48e",
        "title": "RankPO: Preference Optimization for Job-Talent Matching"
      },
      {
        "paperId": "6e03e2525a1b11485e477f454fc8bc9a65f42449",
        "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs"
      },
      {
        "paperId": "d71ef7ee9d08df6a3766c99314df09fbd9f54b5b",
        "title": "Iterative Value Function Optimization for Guided Decoding"
      },
      {
        "paperId": "8f2db783ac3481d9fddea610009b555c888d3dd1",
        "title": "PEO: Improving Bi-Factorial Preference Alignment with Post-Training Policy Extrapolation"
      },
      {
        "paperId": "87412ed8f790d73e8daa0ed01f61ba2126a2f991",
        "title": "Overcoming Non-stationary Dynamics with Evidential Proximal Policy Optimization"
      },
      {
        "paperId": "dd951242ebc94bf633eecc4994c64f46146a1413",
        "title": "Reward Shaping to Mitigate Reward Hacking in RLHF"
      },
      {
        "paperId": "3404f2051763368df1613e3d9b7b787cb4a8b833",
        "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?"
      },
      {
        "paperId": "0152aafbec465a090684637e1da693d6deb98172",
        "title": "RLTHF: Targeted Human Feedback for LLM Alignment"
      },
      {
        "paperId": "aece81d7dcbf2929e650a6094af63666e95a0c83",
        "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models"
      },
      {
        "paperId": "c8384dc76e0426fa2ba212f2a1f7f79249b60274",
        "title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation"
      },
      {
        "paperId": "335542289d91431a1b1ec4d2cd9bb6fb9243d4a5",
        "title": "IPO: Iterative Preference Optimization for Text-to-Video Generation"
      },
      {
        "paperId": "dec2983a58e97c4e5e7aedfa4d42e688fae25473",
        "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use"
      },
      {
        "paperId": "4e4b7d0a6c6e5d722e941a8025758be1038efc57",
        "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation"
      },
      {
        "paperId": "53adca9d83a3b4f1e4b23b2ac980662f70877732",
        "title": "Assessing Moral Decision Making in Large Language Models"
      },
      {
        "paperId": "4c6e25486f9d2a8ca93c24b9ab92535e64daac1f",
        "title": "Personalized Preference Fine-tuning of Diffusion Models"
      },
      {
        "paperId": "ebcf4b850d4f6609a722750d46c00bf903ecb1cd",
        "title": "Graph Generative Pre-trained Transformer"
      },
      {
        "paperId": "284030b1303a8532e2a25f3c0fd2d01ae748b1e8",
        "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers"
      },
      {
        "paperId": "3fdcccf1939b244678e1bbbde7ffa867aef9d1b0",
        "title": "Explainable CTR Prediction via LLM Reasoning"
      },
      {
        "paperId": "90bf3c89ae55ff02d2ece7c2ad14971c6bf1323f",
        "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability"
      },
      {
        "paperId": "96a68d22c4cd83c01f7d9be8e4c7d256bab8e2e0",
        "title": "Generalist Virtual Agents: A Survey on Autonomous Agents Across Digital Platforms"
      },
      {
        "paperId": "78d9c14c845c5ead1a3a839a3463d0e4e2cb30e2",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization"
      },
      {
        "paperId": "fe472d15e2fd294c5d871a5b316d6a74908f409f",
        "title": "Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models"
      },
      {
        "paperId": "82783f35793da105bb0622c94c988379f4d83519",
        "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF"
      },
      {
        "paperId": "01c12d59d9e3e7282514bdc75ba4f9d9b3e9fd57",
        "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment"
      },
      {
        "paperId": "043731dd9f2292b7e4788d0b97edee5db170f840",
        "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
      },
      {
        "paperId": "7595eb04d2f5013bb01be08f7a5763ea32eead6e",
        "title": "SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment"
      },
      {
        "paperId": "ab0df4c771c19e3e7d4edc1057a9f91c6c8e2ae5",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
      },
      {
        "paperId": "efae534e2a3e4b79fe4050fa3fd40d8c7ba9745b",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      },
      {
        "paperId": "24e2ee1d0e7165d1f3fc355b80dee7f90a1315ea",
        "title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "fdbc542fa0fe613ec5f10f2f36edc3c1f09a73a4",
        "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time"
      },
      {
        "paperId": "673d5eeaa9eed85ae9c5f1a99063860b3ed8e690",
        "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning"
      },
      {
        "paperId": "7d44d096ea5ce822a40ee7b5519fb12b65eeba4e",
        "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c6247a6de28885098477084f779d268f6e619ddb",
        "title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths"
      },
      {
        "paperId": "2bd1a664b51813e6f38439f205865ac5a64fd624",
        "title": "Investigating on RLHF methodology"
      },
      {
        "paperId": "ab116ae3e6bc3aedfd1b6ec165e59a8219a5d2f5",
        "title": "VinePPO: Refining Credit Assignment in RL Training of LLMs"
      },
      {
        "paperId": "0e3ffaf73eaf0e5579b0418118b1e7ffb1b91c92",
        "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization"
      },
      {
        "paperId": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
        "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
      },
      {
        "paperId": "12361097de16eea65e89356a50aa0f13b633440f",
        "title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "3e9876349999037c7e27dc1e8fe16f121306f46d",
        "title": "Source-Sink Matching Optimization Based on the Proximal Policy Optimization (PPO) Algorithm"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "84bb47ae6e528ce1dc0e9e3353bf92a45fa120d2",
        "title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization"
      },
      {
        "paperId": "8096ca5f6895955dc41f05094f976b76419437fd",
        "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison"
      },
      {
        "paperId": "9123ec44f0026e70f8398b904e97a4224866bb36",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      },
      {
        "paperId": "4521f8d02e2d4150f6f0bff0ba87eab42d30bb30",
        "title": "GraphWiz: An Instruction-Following Language Model for Graph Computational Problems"
      },
      {
        "paperId": "a2fae006e6c5ac346fd51bc8a009127f9abe22df",
        "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"
      },
      {
        "paperId": "ebfd28b256177177beebb8b864043a7944826b1a",
        "title": "SEAL: Systematic Error Analysis for Value ALignment"
      },
      {
        "paperId": "4eb4952d69655253f49e25293adbb36d9b9ac1a4",
        "title": "Alleviating Action Hallucination for LLM-based Embodied Agents via Inner and Outer Alignment"
      },
      {
        "paperId": "a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
        "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective"
      },
      {
        "paperId": "5f0da4850cbea81d984b55eff677bfee97ca4d0e",
        "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study"
      },
      {
        "paperId": "e9b156ddf40f660dfb6a9b2a720d0b0735414dc0",
        "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization"
      },
      {
        "paperId": "335707e9a442724591eb136bc79584a28ea492ee",
        "title": "Prompt Refinement with Image Pivot for Text-to-Image Generation"
      },
      {
        "paperId": "0654d0addc15c0c76e39ef71c48164e395c6993f",
        "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback"
      },
      {
        "paperId": "3d5462cb9ce4654bd2b33e567401893ce0c1316e",
        "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment"
      },
      {
        "paperId": "82a80c4cedf7c00964ecf3f7e6b8d50513339a3a",
        "title": "Aligning Large Language Models from Self-Reference AI Feedback with one General Principle"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "1dc914b7dcc157944018cdbd953ba1fbdd0fc52d",
        "title": "Toward Optimal LLM Alignments Using Two-Player Games"
      },
      {
        "paperId": "49be1c4f0c8275a82675e67439c1304098f5bbf8",
        "title": "Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms"
      },
      {
        "paperId": "882a212dc979795065593b76d8da287b622f874a",
        "title": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning"
      },
      {
        "paperId": "05677a114e0829f0d786c13ce0dc9f864e855ded",
        "title": "Uncertainty Aware Learning for Language Model Alignment"
      },
      {
        "paperId": "3072ad4982cd916606ac88ea1883d4d725c4eda4",
        "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments"
      },
      {
        "paperId": "92b3572fb2ca580c55e9f5952d8592ed2f18225a",
        "title": "Prototypical Reward Network for Data-Efficient RLHF"
      },
      {
        "paperId": "e5f29e2800537c171af725f6c781d6a4a84c33cd",
        "title": "Exploring connections between auditory hallucinations and language model structures and functions"
      },
      {
        "paperId": "a16a6f868f5119e4306ed3f7d999652fec7b32ee",
        "title": "Dishonesty in Helpful and Harmless Alignment"
      },
      {
        "paperId": "329e8f3c261c82c72ba3b739269721f6bb4c4e8c",
        "title": "Alibaba LingmaAgent: Improving Automated Issue Resolution via Comprehensive Repository Exploration"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "0268225b5c7f12f3d980ccc4311e488e613b60f4",
        "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training"
      },
      {
        "paperId": "0090fc7b7a3d2e4da647b624de126160c5f41748",
        "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications"
      },
      {
        "paperId": "6b74102f7c287a25b14df208aff98a631a8fedf9",
        "title": "Preference Learning Algorithms Do Not Learn Preference Rankings"
      },
      {
        "paperId": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
        "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "a11d3cc04c3293df3e7ea3247c561f42fc36d1c5",
        "title": "MallowsPO: Fine-Tune Your LLM with Preference Dispersions"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "4040099ed20718f418733cd201709cd950f11def",
        "title": "Online Self-Preferring Language Models"
      },
      {
        "paperId": "d09d91b7294f5e486d00dad2a900cd3c2cbd00b9",
        "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG"
      },
      {
        "paperId": "8c73c5305de8aef0ee52a45687b29f10f240b3dc",
        "title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation"
      },
      {
        "paperId": "ebba25e925c4ff6ab12a60431835e7387e9edf4f",
        "title": "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations"
      },
      {
        "paperId": "e1e066a61912b8add2314d6297b8ea4d2a3ded58",
        "title": "The Real, the Better: Aligning Large Language Models with Online Human Behaviors"
      },
      {
        "paperId": "3cdb8432d11eed394906637f4eedf38d3f971015",
        "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning"
      },
      {
        "paperId": "62104714e31a9f332287444091c0a8a08bbc6dc8",
        "title": "When to Trust LLMs: Aligning Confidence with Response Quality"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "47434c384889d4d356925a99248678ab73188da8",
        "title": "Aligning Diffusion Models by Optimizing Human Utility"
      },
      {
        "paperId": "8b57632b19df9ebd3e57e3adbef7fc6ec93bc506",
        "title": "HyperCLOVA X Technical Report"
      },
      {
        "paperId": "7647707b6b68c963314de0aab1514176b11732df",
        "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment"
      },
      {
        "paperId": "8115ffbbadd1055424d18369dba66ce32a572800",
        "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"
      },
      {
        "paperId": "f4bb0154e537ce9631ed401060029580e8775aaa",
        "title": "Improving Attributed Text Generation of Large Language Models via Preference Learning"
      },
      {
        "paperId": "ce13bba037382e240411dcb16105012f42335387",
        "title": "CLHA: A Simple Yet Effective Contrastive Learning Framework for Human Alignment"
      },
      {
        "paperId": "8ea73ef53b0e00c1c5d250197da64fe52971f36b",
        "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection"
      },
      {
        "paperId": "662f0841a2a8435c8b3f196a4016347ad31cd8d5",
        "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach"
      },
      {
        "paperId": "5a2d52a4f03e26707710a40af96e997bd49e1544",
        "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning"
      },
      {
        "paperId": "024b6735f88c1607462068e4eb56d9a5784e6d7b",
        "title": "A Moral Imperative: The Need for Continual Superalignment of Large Language Models"
      },
      {
        "paperId": "d811074d9e1a54ed2b5707930ec67ca581a23e3c",
        "title": "DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation"
      },
      {
        "paperId": "8c785ebee1f34464dbc85ab4113bccafd7a74b0a",
        "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"
      },
      {
        "paperId": "3730ad714aa22ebd3694974abe0bc6437a3ad4ee",
        "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback"
      },
      {
        "paperId": "db5516b3fa82ed1f63cc951a5f99578332ad37b9",
        "title": "Do Large Language Models Mirror Cognitive Language Processing?"
      },
      {
        "paperId": "72f51c3ef967f7905e3194296cf6fd8337b1a437",
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"
      },
      {
        "paperId": "356842f9c2e1c0abd4424d82d933e28a749c9b20",
        "title": "GraphWiz: An Instruction-Following Language Model for Graph Problems"
      },
      {
        "paperId": "559846635fdc3242898af592808d8e346f1ec00f",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too"
      },
      {
        "paperId": "da3c41164c7502709cca508710127336e138621e",
        "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning"
      },
      {
        "paperId": "c3f079f9f59f255e032a1239aea02a2affe93be8",
        "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "66d491e0054f92e2959c8adb912b293a1e2af832",
        "title": "Natural Language Reinforcement Learning"
      },
      {
        "paperId": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"
      },
      {
        "paperId": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
        "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d822ebeb8e98176df5b651d3dd8023e028c845fb",
        "title": "Velocity Domain-Based Distributed Pursuit- Encirclement Control for Multi-USVs With Incomplete Information"
      },
      {
        "paperId": "3a589ce2b38da3083ce1b63b2785646536a364f9",
        "title": "Towards Efficient Exact Optimization of Language Model Alignment"
      },
      {
        "paperId": "1d500905ca70d40a85d10f60d44a018ec0d9349d",
        "title": "Weaver: Foundation Models for Creative Writing"
      },
      {
        "paperId": "5b9fad3e2b2cc5dd23b01e0089bb7b6f6865cb82",
        "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model"
      },
      {
        "paperId": "1491e2d5d8ba63132ff157e47e824af76c422450",
        "title": "ARGS: Alignment as Reward-Guided Search"
      },
      {
        "paperId": "e360eb07461f2741793f99ece8b97a6c04fb2b68",
        "title": "MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization"
      },
      {
        "paperId": "59084df7203c6be33838ba3e3854eb9bda053ed2",
        "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d",
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models"
      },
      {
        "paperId": "485f8a429cf5f70c558181187f2d62e31784deaa",
        "title": "Reasons to Reject? Aligning Language Models with Judgments"
      },
      {
        "paperId": "2a0d5087bdf110302dae177afa1630a80c265fad",
        "title": "Financial Text Sentiment Classification Based on Baichuan2 Instruction Finetuning Model"
      },
      {
        "paperId": "d3ee9ab2b145e9e6a94441c5fff61a95bb875bb2",
        "title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models"
      },
      {
        "paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2",
        "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"
      },
      {
        "paperId": "7bca28a11b7bd50dd378dd59e97f3556b16183f3",
        "title": "MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications"
      },
      {
        "paperId": "a84d6b82947f27bc6bf7f42d69f48b40adcfb6c3",
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "8fd11c6f3eb1d0aeb915369f3c4f0b1bb24cab0c",
        "title": "Large Language Model Unlearning"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c",
        "title": "Large Language Model Alignment: A Survey"
      },
      {
        "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
        "title": "Mitigating the Alignment Tax of RLHF"
      },
      {
        "paperId": "d00735241af700d21762d2f3ca00d920241a15a4",
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"
      },
      {
        "paperId": "36a364316c7bab6ed2a599ad2ce364fb2cd3e9dc",
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"
      },
      {
        "paperId": "658cd67a91da86cf451e6f1b015f762b56015172",
        "title": "Detecting and Preventing Hallucinations in Large Vision Language Models"
      },
      {
        "paperId": "1e26b42669b060a3850e4766dea0db6e3c85cdec",
        "title": "Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey"
      },
      {
        "paperId": "ebb13007d456d6816867fd3dad0da081800bad2d",
        "title": "Edge-Enhanced Intelligence: A Comprehensive Survey of Large Language Models and Edge-Cloud Computing Synergy"
      },
      {
        "paperId": "6a0c544b0a07442290e377ac5a1a3990e1eacd10",
        "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models"
      },
      {
        "paperId": "880c48200ec2b16129816fa91aa252f9c76598ef",
        "title": "AgentGym: Evaluating and Training Large Language Model-based Agents across Diverse Environments"
      },
      {
        "paperId": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
        "title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback"
      },
      {
        "paperId": "c3475615542c4218f2fff301c0420f684f63e057",
        "title": "Bit_numeval at SemEval-2024 Task 7: Enhance Numerical Sensitivity and Reasoning Completeness for Quantitative Understanding"
      },
      {
        "paperId": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
        "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models"
      },
      {
        "paperId": "651c9af1ee9467c7a0b46aaddd4db8b264f5248d",
        "title": "Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models"
      },
      {
        "paperId": "694ed488bc3b63ad896cc0d2c92abdd1133698c1",
        "title": "Improving Low-Resource Machine Translation Using Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "5edcbf3ea678a7d9977555ecd59370452667597a",
        "title": "Towards Efficient and Exact Optimization of Language Model Alignment"
      },
      {
        "paperId": "97f6df02198a4d957fcab414330c77c2bb9366a6",
        "title": "Low-Redundant Optimization for Large Language Model Alignment"
      },
      {
        "paperId": "5c667f3fd6215266e482dcbeb1e0119c27032519",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too"
      },
      {
        "paperId": "fa055a6949629b53c4949b7a199687ee28233467",
        "title": "Limited Out-of-Context Knowledge Reasoning in Large Language Models"
      },
      {
        "paperId": "72b69978b035f003525c7f75932c5304d771e425",
        "title": "StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure"
      },
      {
        "paperId": "ed62e3847f45f152cf6d7b9b4bebb782547f1a54",
        "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment"
      },
      {
        "paperId": "bfd7c6e68638e791e795cefd3fc55ff1e88f6595",
        "title": "Safety and Ethical Concerns of Large Language Models"
      },
      {
        "paperId": "28717178a58af7e00d5ed197a61157e54bd4f163",
        "title": "A Reinforcement Learning Approach for Intelligent Conversational Chatbot For Enhancing Mental Health Therapy"
      },
      {
        "paperId": "eb8954cf2a4e508d84ecbcd3171bfa9c4533ee53",
        "title": "On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models"
      },
      {
        "paperId": "951a11a4d247de5823dfa42affe39cf849c708b8",
        "title": "Pairwise Proximal Policy Optimization: Large Language Models Alignment via Comparative RL"
      },
      {
        "paperId": "7e54cededaea99e988a1f77acff6897d4687c6ef",
        "title": "Prototypical Reward Network for Data Efficient Model Alignment"
      },
      {
        "paperId": "5652ec62eb6aee72e09f6c38704b3187972a0589",
        "title": "VinePPO: Accurate Credit Assignment in RL for LLM Mathematical Reasoning"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "d330bb7c578c7e4de7566d0c958a41e0935c9721",
        "title": "Improving LLM Generation with Inverse and Forward Alignment: Reward Modeling, Prompting, Fine-Tuning, and Inference-Time Optimization"
      },
      {
        "paperId": "f4aa7e4033d67e1b3d2cab2302fd59df2b641350",
        "title": "Prompt Re\ufb01nement with Image Pivot for Text-to-Image Generation"
      },
      {
        "paperId": "8ccf59bccededd90a4576bf7d025a230d6499cdf",
        "title": "C AN P ROMPT D IFFICULTY BE"
      }
    ],
    "score": 100.5
  },
  {
    "id": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
    "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
    "authors": [
      "Prasann Singhal",
      "Tanya Goyal",
      "Jiacheng Xu",
      "Greg Durrett"
    ],
    "year": 2023,
    "citationCount": 179,
    "abstract": "Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for\"helpfulness\"in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.",
    "url": "https://www.semanticscholar.org/paper/59a2203ef6ea159bb41540bd282e29e80a8ad579",
    "pdf_url": "https://arxiv.org/pdf/2310.03716.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-05",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-03716",
      "ArXiv": "2310.03716",
      "DOI": "10.48550/arXiv.2310.03716",
      "CorpusId": 263672200
    },
    "references": [
      {
        "paperId": "e56dc21699e6283fce072ffc908cb9f66321760d",
        "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"
      },
      {
        "paperId": "01d31fb9fc6ab36df6627b8555b64789113eb7a5",
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "d40df59a1df8048a671bce60260b817e109a33f8",
        "title": "Reward Gaming in Conditional Text Generation"
      },
      {
        "paperId": "b6d98839dfae708d845ba482665f3e6343f4f173",
        "title": "A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "891edceb78a274b0c2494d8176bc4d6f6e3f9cbc",
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"
      },
      {
        "paperId": "004357dd9bbf3012c8fe0ccada4da401bf85dfff",
        "title": "Defining and Characterizing Reward Hacking"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "70cd2d6a3e055c76d1a24d1f8029a7b72681e65c",
        "title": "On Length Divergence Bias in Textual Matching Models"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "ad962c10e83c4b0c4857a5813b5ce6726b911837",
        "title": "Consequences of Misaligned AI"
      },
      {
        "paperId": "ee5fff85d3ec62698eddba162f054b7e73670b2a",
        "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "ebf59587f8f170ff4241c42263bbfb9da5bd2135",
        "title": "ELI5: Long Form Question Answering"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
        "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations"
      },
      {
        "paperId": "f3caa43a7016fbbf309d45112b31b20230eaf8da",
        "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "fc097d528fd62fe76d73fafbf0c57473b58d1e84",
        "title": "Correcting Length Bias in Neural Machine Translation"
      },
      {
        "paperId": "8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6",
        "title": "Hypothesis Only Baselines in Natural Language Inference"
      },
      {
        "paperId": "2997b26ffb8c291ce478bd8a6e47979d5a55c466",
        "title": "Annotation Artifacts in Natural Language Inference Data"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "fb39923f6a4cdb486da5b579c5f8e2c500f36a35",
        "title": "Controlling Linguistic Style Aspects in Neural Language Generation"
      },
      {
        "paperId": "47a26b0c5d27b99da175e0a719f42d707f97ec3d",
        "title": "Event Representations for Automated Story Generation with Deep Neural Nets"
      },
      {
        "paperId": "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70",
        "title": "Controlling Output Length in Neural Encoder-Decoders"
      },
      {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
        "title": "Interactively shaping agents via human reinforcement: the TAMER framework"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "db20bd3bb82d1011ce704d440d8c2578f665e6e1",
        "title": "Aligning Robot and Human Representations"
      },
      {
        "paperId": null,
        "title": "HuggingFace H4 Stack Exchange Preference Dataset, 2023"
      },
      {
        "paperId": null,
        "title": "Question answering; human labels"
      },
      {
        "paperId": null,
        "title": "Stack (Technical question answering; upvotes"
      },
      {
        "paperId": null,
        "title": ": Transformer Reinforcement Learning"
      },
      {
        "paperId": "5c1b3dc95d1ba44fd7d9e3f6dec60fdc46a0eb78",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS"
      },
      {
        "paperId": null,
        "title": "A Simulation Framework for Methods"
      },
      {
        "paperId": null,
        "title": "Past work"
      },
      {
        "paperId": null,
        "title": "Human-centered loss functions (halos)"
      }
    ],
    "cited_by": [
      {
        "paperId": "27f6531af74b0aee04a572884d908421f03881ad",
        "title": "Towards Better Optimization For Listwise Preference in Diffusion Models"
      },
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "1e22e7829b2d0922f14306e7b20f8455ca02ba14",
        "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge"
      },
      {
        "paperId": "1ef29b16fdca0122deb2ff217bb42a57e6fadc55",
        "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues"
      },
      {
        "paperId": "783c03bd491b5c7533f259e0318e02b287e02e80",
        "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics"
      },
      {
        "paperId": "8111293cbde870ae432deb98bdad2b4dc6e70644",
        "title": "Expert Preference-based Evaluation of Automated Related Work Generation"
      },
      {
        "paperId": "f90881642f28616ba051ad01e3d08534b5bebd39",
        "title": "Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"
      },
      {
        "paperId": "c7b279896e90bd1723c4fd5e2f1e3e03a3a12a43",
        "title": "Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback"
      },
      {
        "paperId": "a67c0fb651e7175be1dd457dea336fd4a12f6130",
        "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains"
      },
      {
        "paperId": "265fdd781ac5f96f0e6ae37fbca0a370ee7c1b26",
        "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions"
      },
      {
        "paperId": "c1a78f39b367aaeb5dda88ae93f1ba4cecdf6a4e",
        "title": "Bridging Offline and Online Reinforcement Learning for LLMs"
      },
      {
        "paperId": "9fb8419ef6a8a10d4dff8f62f238a956f23601e4",
        "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control"
      },
      {
        "paperId": "17535e013fda643b5f11f9b9adb35d1e94e0a14b",
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning"
      },
      {
        "paperId": "26875419a986a787e7431dc4d804eeb6bb75e82f",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization"
      },
      {
        "paperId": "b76d2d4da4b70489d85cd1fbf2ff3a7ebbadc578",
        "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms"
      },
      {
        "paperId": "171cfd2aa8b0b47649a36a4eb5620806007c34c8",
        "title": "Reward Model Interpretability via Optimal and Pessimal Tokens"
      },
      {
        "paperId": "c4d65d124c3ceeb7369425954516b1d7c6ffc1ab",
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization"
      },
      {
        "paperId": "17d25128b18cadc7cd88b7d7d18513087537612b",
        "title": "Preference Learning for AI Alignment: a Causal Perspective"
      },
      {
        "paperId": "3d21dab6253c86ce73a43ef1eb578179f21b7abb",
        "title": "Debiasing Online Preference Learning via Preference Feature Preservation"
      },
      {
        "paperId": "331cae01113adbd97bb63c64250a1243e8167d5f",
        "title": "Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models"
      },
      {
        "paperId": "8e70bd26dd94fe39dd2202a8183906b67b1f425b",
        "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models"
      },
      {
        "paperId": "26e9ce47fc8da27174e73a19d3f42beb948574d4",
        "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO"
      },
      {
        "paperId": "bbdc38ce5aa93070fd4394640715b37870057e07",
        "title": "Policy Optimized Text-to-Image Pipeline Design"
      },
      {
        "paperId": "9db4ecfb5f2e78b43ff388892ce3bf0f122fa0d3",
        "title": "Multi-Domain Explainability of Preferences"
      },
      {
        "paperId": "2728c07b1cfe313920bdf21dc8c2c88d21fa5d59",
        "title": "Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting"
      },
      {
        "paperId": "bdf6334fb878f90b71b325223cc904c3b7aad0c3",
        "title": "Reward Model Overoptimisation in Iterated RLHF"
      },
      {
        "paperId": "36d91bf582f56c125449815e1913a889517983a0",
        "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization"
      },
      {
        "paperId": "eb7b2be98e20ebcc08643aedd68e68aa890899c2",
        "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild"
      },
      {
        "paperId": "a71840c64c2584f3b99adf68dabfaee3797f6f08",
        "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF"
      },
      {
        "paperId": "176e100cca02b4f149b147df84811f4d35f8d2b4",
        "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following"
      },
      {
        "paperId": "cde94812bf1525006612b2cd69ebd3ebd3ebc049",
        "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models"
      },
      {
        "paperId": "e3e345499763162605cded6d99eb9963d356ef5c",
        "title": "Scalable Chain of Thoughts via Elastic Reasoning"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "bc4b173d49b4dcac8ebd6329fa5b48db954e50f1",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning"
      },
      {
        "paperId": "92f2e1a17cbff22164611a2c8a908afe71b4b626",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models"
      },
      {
        "paperId": "a9c0d81fcf801fa689e04438289e09098659c1dd",
        "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation"
      },
      {
        "paperId": "3cca2ceb61a0e8ebfd00479c2fba722489ba1476",
        "title": "An Empirical Study on Prompt Compression for Large Language Models"
      },
      {
        "paperId": "d60e33f599873a511ee8cf4c6813e283052e659d",
        "title": "Establishing Reliability Metrics for Reward Models in Large Language Models"
      },
      {
        "paperId": "741e35b536463037391373d8a4e9cdd8c420f242",
        "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation"
      },
      {
        "paperId": "87f5ab251b54533a330905f500e82f4fd1bd6889",
        "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey"
      },
      {
        "paperId": "06a2c35e96d84edf08591ed451c074efffc4a466",
        "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF"
      },
      {
        "paperId": "27c3fe1e984c93347ea9c7f39910bc085c58978a",
        "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models"
      },
      {
        "paperId": "df0b652b099b10fb7554ef8d50f31e023211ddca",
        "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "c9e4efa58fd42a07da27ae70254981715cc257d5",
        "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization"
      },
      {
        "paperId": "1f3a52ef812303a79e6ecf46b613a3db0fd8d973",
        "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark"
      },
      {
        "paperId": "a0a9f9b9bcd34bad3b7cde89cd0ea473a17d181e",
        "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence"
      },
      {
        "paperId": "b2f1df899c17b4e8b8817ebc9e2b470a9558474d",
        "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise"
      },
      {
        "paperId": "0c2ff3753bbfa6668975c09e2a34ace4ef3423ce",
        "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs"
      },
      {
        "paperId": "12c6b257be4fcdb3722335066f982271cce8fcff",
        "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "0383e4223c8ab97d6aa7b6a6397f1b3a446bed33",
        "title": "Mitigating Preference Hacking in Policy Optimization with Pessimism"
      },
      {
        "paperId": "0681d80cd58b6135534cf279a69f5f999f47ebca",
        "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning"
      },
      {
        "paperId": "ded8cf8b28bab1930f5c57b31f7906835dfeec38",
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems"
      },
      {
        "paperId": "dd951242ebc94bf633eecc4994c64f46146a1413",
        "title": "Reward Shaping to Mitigate Reward Hacking in RLHF"
      },
      {
        "paperId": "6a57356fc97492acfcee2419a0d27066761b0c91",
        "title": "VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted Trie in Sponsored Search"
      },
      {
        "paperId": "e446584d601cba65c5155b2fc4cf110fc0c7da79",
        "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking"
      },
      {
        "paperId": "f400b0d5724203633f56299cb19f34bf18fd31b9",
        "title": "Rejected Dialects: Biases Against African American Language in Reward Models"
      },
      {
        "paperId": "c6e8297646a161facbd4c5ca708df87dfd79a607",
        "title": "Preference learning made easy: Everything should be understood through win rate"
      },
      {
        "paperId": "a922ecbc9f440e5547117a7f188a76de7996eb3c",
        "title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models"
      },
      {
        "paperId": "fc8152ec6fba4a7cb7c54ec0199ffb8db71c9b39",
        "title": "Does Training on Synthetic Data Make Models Less Robust?"
      },
      {
        "paperId": "36635314d9361154ff381bbe465b06223f118239",
        "title": "Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking"
      },
      {
        "paperId": "44dcaa20f5eb5c5fd5b773ef9a41629cbebe452f",
        "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
      },
      {
        "paperId": "ea8ad60b260813b0f1fc9c64518ae8a0e9a75bc2",
        "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models"
      },
      {
        "paperId": "f3eee8fa080bfbcaa6ff233664a49e81bbb459ea",
        "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis"
      },
      {
        "paperId": "6ef32591af268d810c3e9272921803d1f4098467",
        "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?"
      },
      {
        "paperId": "e56588f0014033e1f954549bdf107f45bafdbe59",
        "title": "Hansel: Output Length Controlling Framework for Large Language Models"
      },
      {
        "paperId": "5d373c79f1802e5c57d1d3f7a22c666ce10b29c9",
        "title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models"
      },
      {
        "paperId": "4de79e47a9879d2eda2ae20b4c2dee7b78c09303",
        "title": "Interpreting Language Reward Models via Contrastive Explanations"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "0445711f119fe20d0a2f8837cd6184c79d331f97",
        "title": "On the loss of context-awareness in general instruction fine-tuning"
      },
      {
        "paperId": "b38c17d4c1cf719f110e811190255428fe7a53aa",
        "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks"
      },
      {
        "paperId": "fba7e7d1b20652f5d87358d5451840dd68c6bfa5",
        "title": "Reward Modeling with Weak Supervision for Language Models"
      },
      {
        "paperId": "369e33aa3a955f581c9f5182cac0073f67fb7550",
        "title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision"
      },
      {
        "paperId": "cdf06034c58381afe58aed0ed8dc505039055a4f",
        "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback"
      },
      {
        "paperId": "4f9f4bc55a25ff6cf04f3e2f01c23ce7801184d7",
        "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models"
      },
      {
        "paperId": "cb6de49a43808a5d15f91ed113d90134b19498b4",
        "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "043731dd9f2292b7e4788d0b97edee5db170f840",
        "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
      },
      {
        "paperId": "5456e833710dba2bb3ae92621fa89c27733b1db0",
        "title": "AI can help humans find common ground in democratic deliberation"
      },
      {
        "paperId": "3f062cfb7762e45f82cc703a5b093f4fc9c9a9f3",
        "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
      },
      {
        "paperId": "24e2ee1d0e7165d1f3fc355b80dee7f90a1315ea",
        "title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both"
      },
      {
        "paperId": "dc96b6ddb25bc2ebba5750638d3e36290773b4ea",
        "title": "Rational Metareasoning for Large Language Models"
      },
      {
        "paperId": "46cff4ce8deed2ef1b00d483ed9a69f8183e3538",
        "title": "CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints"
      },
      {
        "paperId": "2bd1a664b51813e6f38439f205865ac5a64fd624",
        "title": "Investigating on RLHF methodology"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "158730a16cd4cc4d4842e42e5c2a0843b75d527c",
        "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data"
      },
      {
        "paperId": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
        "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "50ee11e0d057defe6c449099e01b12cf3cb74446",
        "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking"
      },
      {
        "paperId": "11a6f7d2969e61be206e4fa2914776cd8a509ae6",
        "title": "Speechworthy Instruction-tuned Language Models"
      },
      {
        "paperId": "6d3a7b453048673a98b082a73bc864366fbd1cf4",
        "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
      },
      {
        "paperId": "0eaf243f2f7c8a381baf0952f85396e2f6a655c5",
        "title": "Language Models Learn to Mislead Humans via RLHF"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "27b30fa10cd2e372f2935b2b6df1c63947cc15ae",
        "title": "Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance"
      },
      {
        "paperId": "1a7ba7513701ddf45c4a4c5e690cf20c5672c59d",
        "title": "More is More: Addition Bias in Large Language Models"
      },
      {
        "paperId": "310d1846269352f5e20a1aca25d98ea7af037f8e",
        "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation"
      },
      {
        "paperId": "4036c337a8cba9c724df31d8041b9961b9b72c3f",
        "title": "Efficient LLM Scheduling by Learning to Rank"
      },
      {
        "paperId": "d95220be54d155f8be7f36b4fde794711cbb69b3",
        "title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts"
      },
      {
        "paperId": "fc24091a9b44953c71084449b4fa0eabfb58abb3",
        "title": "KnowPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models"
      },
      {
        "paperId": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
        "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"
      },
      {
        "paperId": "a469471c93fc6e99a5e71d8aaffdd0619df91d2f",
        "title": "Wolf: Dense Video Captioning with a World Summarization Framework"
      },
      {
        "paperId": "0d58aecbf6f05babe447267ef3aa96e2ee9f3e61",
        "title": "LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives"
      },
      {
        "paperId": "dff7a3989c41bd0200095e6560f73923b0f3bf32",
        "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment"
      },
      {
        "paperId": "f7f0d2ff76f750f7b6e5befbe2665815e8609bfc",
        "title": "Following Length Constraints in Instructions"
      },
      {
        "paperId": "aef234095eb56f3510737df572fd155668523bb0",
        "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies"
      },
      {
        "paperId": "d9d8aef662bb7a3730a62b1015c3ed99e4287523",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt"
      },
      {
        "paperId": "9a9253e1a8e5b22800d8a17b5b10c25306dd009c",
        "title": "Multi-Objective Linguistic Control of Large Language Models"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "d24e5a8e0f65d527872e33ef1d5f665fc171cf8b",
        "title": "It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "5ba130279094e4766d0bbac5b4915d595cc84ed7",
        "title": "UltraMedical: Building Specialized Generalists in Biomedicine"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "3a10570fb3f88d755ba4648598b3d30f8ddfb26c",
        "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage"
      },
      {
        "paperId": "590dc68769cba614e15846dcd36650f272c81b4b",
        "title": "On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots"
      },
      {
        "paperId": "4dd571c6514d169d0b0a7933f3ba3440301d2e46",
        "title": "Improving Reward Models with Synthetic Critiques"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "87912571f3df29464d3ccafae66f6e1eed581564",
        "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment"
      },
      {
        "paperId": "6b74102f7c287a25b14df208aff98a631a8fedf9",
        "title": "Preference Learning Algorithms Do Not Learn Preference Rankings"
      },
      {
        "paperId": "2433170286f1adf55670dc68ce73d816e1ccb0be",
        "title": "Aligning to Thousands of Preferences via System Message Generalization"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "3c9cb3004ea53d74b85bbb40bb2c4148979880ad",
        "title": "Language Models can Evaluate Themselves via Probability Discrepancy"
      },
      {
        "paperId": "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607",
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models"
      },
      {
        "paperId": "7a646b1f4e66c4e4e9a673834567e03341e89162",
        "title": "D2PO: Discriminator-Guided DPO with Response Evaluation Models"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "213816dc724d5e305a83b299cfa09ab7f2c8c7f8",
        "title": "Mapping Social Choice Theory to RLHF"
      },
      {
        "paperId": "f533bbe34eea2c3346c7d67149c62c4761fc5248",
        "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment"
      },
      {
        "paperId": "de17e7ed443e13320694cce3b2f475c694801246",
        "title": "Stepwise Alignment for Constrained Language Model Policy Optimization"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f",
        "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs"
      },
      {
        "paperId": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
        "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "9689b5fdb0d3a1bad802d03d348bd32aa5a4c2df",
        "title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data"
      },
      {
        "paperId": "8b57632b19df9ebd3e57e3adbef7fc6ec93bc506",
        "title": "HyperCLOVA X Technical Report"
      },
      {
        "paperId": "eeef23397297b4760a16e3b0a0ac8d72bc341300",
        "title": "Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided"
      },
      {
        "paperId": "8115ffbbadd1055424d18369dba66ce32a572800",
        "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "5a28d82bf7f5beabfeb0974829e8da878aca1636",
        "title": "Large Language Models Produce Responses Perceived to be Empathic"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "e1e027639b54616217bde738033470dbf7d73d64",
        "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "685e5d9cbe177ec50129a6ca1ffb912636bb49d2",
        "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723",
        "title": "Unintended Impacts of LLM Alignment on Global Representation"
      },
      {
        "paperId": "087699924e3dc468a486e0763f1cc097824a60d2",
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models"
      },
      {
        "paperId": "cacd57ad4eada225ae7c436fe726ac5549a6f926",
        "title": "Dissecting Human and LLM Preferences"
      },
      {
        "paperId": "4d4432514695e0f36720c73c23d15d8e21abe2fe",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "b88b25aba67d7a391eb480563fbfd5b4a6625444",
        "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "bddd8187e5e07e8cfc9e430ddbafb851fb63457f",
        "title": "Rethinking the Role of Proxy Rewards in Language Model Alignment"
      },
      {
        "paperId": "60e3658f86393d65a6d523bfb88fd21e4447d941",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "e4047a2c970baeb2cc917d6e59fcd087443ff811",
        "title": "Dolomites: Domain-Specific Long-Form Methodical Tasks"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "ce316ce4671e38019582f7be7a6e87b4c3909b57",
        "title": "RLHF and IIA: Perverse Incentives"
      },
      {
        "paperId": "12cf8be0865649b5e878051182b69f51e67fb8d4",
        "title": "A density estimation perspective on learning from pairwise human preferences"
      },
      {
        "paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26",
        "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "f495d2741f804cb37a6afc3c039f1d3964b964a8",
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"
      },
      {
        "paperId": "a5c61aae22f47a7deaf7397a4a24bce42a8cc1f1",
        "title": "Grounding Gaps in Language Model Generations"
      },
      {
        "paperId": "ef72f4aae225529393264c2b2594e876bec3fd04",
        "title": "Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
        "title": "Evaluating Large Language Models at Evaluating Instruction Following"
      },
      {
        "paperId": "64ad8e62544cca34a9714cbc79af8c56807310fa",
        "title": "Reward Engineering for Generating Semi-structured Explanation"
      },
      {
        "paperId": "a2bcf4bf415bc35ae0bb27a862efea67a4090483",
        "title": "Beyond Excess and Deficiency: Adaptive Length Bias Mitigation in Reward Models for RLHF"
      },
      {
        "paperId": "84a99865c3aa1a1732b0329b372cb5bbc5aefe61",
        "title": "Length Controlled Generation for Black-box LLMs"
      },
      {
        "paperId": "7b88fb2e94a42c5f16c27d744ea5d04eb1f8069b",
        "title": "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives"
      },
      {
        "paperId": "242a90f2994124fca35eef8484640c9d470fd5ef",
        "title": "Information Dense Question Answering for RLHF"
      },
      {
        "paperId": "7fa92edb84802e413c497062a026439a5a10f5b7",
        "title": "S TYLE OVER S UBSTANCE : F AILURE M ODES OF LLM J UDGES IN A LIGNMENT B ENCHMARKING"
      },
      {
        "paperId": "58ebc7f497eb8770bdd53ba247710a2ecee88950",
        "title": "Scalable Oversight by Accounting for Unreliable Feedback"
      },
      {
        "paperId": "92616847a332a1bb0960c95cf2276b4dadfd4288",
        "title": "Strict Verification: Exploring Reinforcement Learning with Weak Verifiers"
      }
    ],
    "score": 89.5
  },
  {
    "id": "900cd128482bbab4d2752d01ce80c55498b78dd2",
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "authors": [
      "Yuxiang Wei",
      "Olivier Duchenne",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Lingming Zhang",
      "Daniel Fried",
      "Gabriele Synnaeve",
      "Rishabh Singh",
      "Sida Wang"
    ],
    "year": 2025,
    "citationCount": 87,
    "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
    "url": "https://www.semanticscholar.org/paper/900cd128482bbab4d2752d01ce80c55498b78dd2",
    "pdf_url": "https://arxiv.org/pdf/2502.18449.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-25",
    "externalIds": {
      "ArXiv": "2502.18449",
      "DBLP": "journals/corr/abs-2502-18449",
      "DOI": "10.48550/arXiv.2502.18449",
      "CorpusId": 276580226
    },
    "references": [
      {
        "paperId": "45e1c99a1c8935bf137c0b51a08a03ffb6821993",
        "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "98bdf937d7eb831ff9a2a9b363a4e682638ee366",
        "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "4a8c54109639b436f92a94a93fcb62454cd2dc4d",
        "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution"
      },
      {
        "paperId": "d3f0dc8ec576c9a1dcc31375bbbbfe65135609a1",
        "title": "Training Software Engineering Agents and Verifiers with SWE-Gym"
      },
      {
        "paperId": "1d59c7a29723aa56271ff0252b79fb378655cf21",
        "title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?"
      },
      {
        "paperId": "b37074caf88e60afd2a760e8c2c2b4ce68e972c6",
        "title": "TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark"
      },
      {
        "paperId": "47e0ded22e3f446af96b41ec25c3b38f533cb489",
        "title": "Evaluating Language Models for Efficient Code Generation"
      },
      {
        "paperId": "f2e0b3d6a02dac33872f0a0b42affdcf454715cb",
        "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions"
      },
      {
        "paperId": "1c3c531fc0fbe79f97f367ed3648de8467caeeaa",
        "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering"
      },
      {
        "paperId": "7b3c8d1aacf2c3b356d7ee26d6e7fc2a8914bb45",
        "title": "Fewer Truncations Improve Language Modeling"
      },
      {
        "paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f",
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"
      },
      {
        "paperId": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "4701914bc77dedef9e0a001687277103fb3ddfc6",
        "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution"
      },
      {
        "paperId": "f68164bf402e311a510a741860e96ab4ea24aa9f",
        "title": "Magicoder: Empowering Code Generation with OSS-Instruct"
      },
      {
        "paperId": "f1bd7ea3a63b78a60b5d90d91fdb4a1d7ac0de8e",
        "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion"
      },
      {
        "paperId": "4edaffbfe5c6545f12fb6e695b38eb1a0dc967ea",
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair"
      },
      {
        "paperId": "e539218e2f6054ed002da6d6efc96d73221c22dc",
        "title": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation"
      },
      {
        "paperId": "f97413a497d47c739d41d237917e6566154647b4",
        "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"
      },
      {
        "paperId": "4c552ad10efda5283f7b681f7dd2e532445259fc",
        "title": "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation"
      },
      {
        "paperId": "b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a",
        "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"
      },
      {
        "paperId": "f9b301daed4205af692a1b1389d86238610de270",
        "title": "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models"
      },
      {
        "paperId": "4e3c65511292a800b17be6653bd057e7a545a0b0",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation"
      },
      {
        "paperId": "d3de0bac5703825796c240bfab8dc3c8e0a90222",
        "title": "Impact of Code Language Models on Automated Program Repair"
      },
      {
        "paperId": "5aec1050fd62971f2b40b597ef7526fbc820beab",
        "title": "Automated Program Repair in the Era of Large Pre-trained Language Models"
      },
      {
        "paperId": "876eb375cb7b365475040046df669c039ad54202",
        "title": "CodeT: Code Generation with Generated Tests"
      },
      {
        "paperId": "e37155d21818513bd40d64ee212099aac82bd6f8",
        "title": "Less training, more repairing please: revisiting automated program repair via zero-shot learning"
      },
      {
        "paperId": "2ec4d25b8ffb1821a664fb6e3722c236fe0c07db",
        "title": "GitHub"
      },
      {
        "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
        "title": "Competition-level code generation with AlphaCode"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
        "title": "Measuring Coding Challenge Competence With APPS"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": null,
        "title": "Approximating kl divergence"
      },
      {
        "paperId": null,
        "title": "Pattern Matching: The Gestalt Approach"
      },
      {
        "paperId": null,
        "title": "OpenAI. Gpt-4o system card"
      },
      {
        "paperId": null,
        "title": "@Meta: Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, and Angela Fan et al."
      },
      {
        "paperId": null,
        "title": ": Can language models resolve"
      },
      {
        "paperId": null,
        "title": ": Breaking the barrier of closed-source models"
      },
      {
        "paperId": null,
        "title": ": An open development-process-centric language model for automated software"
      },
      {
        "paperId": "fed9cc193a14b84131812372d8d5857f8f304c52",
        "title": "Claude 3.5 Sonnet Model Card Addendum"
      },
      {
        "paperId": null,
        "title": "Moatless tools"
      },
      {
        "paperId": null,
        "title": "Raising the bar on SWE-bench Verified with Claude 3.5"
      },
      {
        "paperId": null,
        "title": "We are currently solving the following issue within our repository"
      },
      {
        "paperId": null,
        "title": "note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION"
      },
      {
        "paperId": null,
        "title": "Repository-level code completion"
      },
      {
        "paperId": null,
        "title": "Ilya Grigorik"
      },
      {
        "paperId": null,
        "title": "|><|start_header_id|>assistant<|end_header_id|>"
      },
      {
        "paperId": null,
        "title": "contain bugs"
      },
      {
        "paperId": null,
        "title": ": Grounding code llms in"
      },
      {
        "paperId": null,
        "title": "Aider is ai pair programming in your terminal"
      },
      {
        "paperId": null,
        "title": "Swe-bench leaderboard"
      }
    ],
    "cited_by": [
      {
        "paperId": "64660a1b2affa1d86d2e4e1973021a115aff6c28",
        "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation"
      },
      {
        "paperId": "32f42b7688f27c42e1e3883e6d378efedc2ff188",
        "title": "TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture"
      },
      {
        "paperId": "51357ff32df3924f29e550aaf71c9882c2b99348",
        "title": "Variational Reasoning for Language Models"
      },
      {
        "paperId": "df0a07beb75eea9c71ca70344726e5c822974c40",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
      },
      {
        "paperId": "a3707ba55b88df07b79f9d188723ebc75440fff3",
        "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans"
      },
      {
        "paperId": "630c0e441905b16a10e4e6ca572074c71de2bba5",
        "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning"
      },
      {
        "paperId": "a6478bab4f1985f3eb008e7ebca4a84a1af273cd",
        "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generation"
      },
      {
        "paperId": "2e3d2cb75be2d8939387f9173b68cd81315ec694",
        "title": "Agentic Reinforcement Learning with Implicit Step Rewards"
      },
      {
        "paperId": "022375a5639131bae3c5cd7745f4f4456767ef7c",
        "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning"
      },
      {
        "paperId": "db45d8cc9a65112d421bc116dccfb70609667352",
        "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair"
      },
      {
        "paperId": "f1638fa0b7d078653797bc956d25671a701f1f2f",
        "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models"
      },
      {
        "paperId": "a16aba7e5fd046df3485d7146c1372117b992111",
        "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization"
      },
      {
        "paperId": "17747af1fd599a93b9d06765ab3839ccb8a7fd65",
        "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization"
      },
      {
        "paperId": "cc1658260272b8deb85fd4de7edba36ca83ccc95",
        "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents"
      },
      {
        "paperId": "261e2dc366183452e95f4cc1ef5dc7912cbee3fd",
        "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints"
      },
      {
        "paperId": "33c6b4e8eb02368712e7375d30bd9352b52aee83",
        "title": "Agentic Software Engineering: Foundational Pillars and a Research Roadmap"
      },
      {
        "paperId": "41c0abfe56b6c82e43a6b04ec0c63d7f31698248",
        "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities"
      },
      {
        "paperId": "7c74d038204f86f47f6be6b489f95b70bfbfd1f8",
        "title": "Reinforcement Learning for Machine Learning Engineering Agents"
      },
      {
        "paperId": "4b37680b828ea09aab51a3fa829d7ec166dcfb48",
        "title": "Dream-Coder 7B: An Open Diffusion Language Model for Code"
      },
      {
        "paperId": "0544547cb6d932312de0ca5e3f2010f163e33262",
        "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "bf1bdf42e86bf54c8adc0fef01ddf4aae352bd51",
        "title": "Reasoning LLMs in the Medical Domain: A Literature Survey"
      },
      {
        "paperId": "608531e07f3cb0566f45328fdac1b5624b127a8a",
        "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo"
      },
      {
        "paperId": "ebe7209983d0c6c2eff2b5ee80915bd2189f3a01",
        "title": "G2RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance"
      },
      {
        "paperId": "37f8a18c74565ffbbbc20fa5214b72c00b4618d7",
        "title": "From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation"
      },
      {
        "paperId": "9cc561ce68c2bf854192fc1adb78edbccec3b6be",
        "title": "Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment"
      },
      {
        "paperId": "09c1844b67c520e964a42f26161561535b6864ba",
        "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice"
      },
      {
        "paperId": "10c98cc047490b30eb286e3ac7a09de71163b5da",
        "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning"
      },
      {
        "paperId": "468bb4a949af92f23631685d4023460bff377d2b",
        "title": "Sotopia-RL: Reward Design for Social Intelligence"
      },
      {
        "paperId": "06ec2770e40a45a24eeec080e28d5f2df75436d8",
        "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements"
      },
      {
        "paperId": "8857068d8701ceae49272deeb54a7d8c707b1afe",
        "title": "SWE-Exp: Experience-Driven Software Issue Resolution"
      },
      {
        "paperId": "d1414ef32139735bc5bf7bc17b727b344a7abbae",
        "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning"
      },
      {
        "paperId": "ded5c43500713f353334d6240c8567e64ae8846a",
        "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation"
      },
      {
        "paperId": "bc25c459b67a310e5d0232d3512698aa0711ce82",
        "title": "Replacing thinking with tool usage enables reasoning in small language models"
      },
      {
        "paperId": "22133a71e3ddc9e42b316895fbc14ebd30d0a62a",
        "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications"
      },
      {
        "paperId": "d9ec646be4f9d4681e0651a313aa8f9f85c05c31",
        "title": "Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing"
      },
      {
        "paperId": "df7f86f547179a2032fe7284fc9cd215412103cc",
        "title": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution"
      },
      {
        "paperId": "319694295cc5f57d119bbc460ed77d1c35c83680",
        "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks"
      },
      {
        "paperId": "6f385038c0ab31215974e77dc63dea4127a89b08",
        "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards"
      },
      {
        "paperId": "adf4e6e5d2f15c00079bec4e3c5273b3cc99c303",
        "title": "OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics"
      },
      {
        "paperId": "1526c2db582e6b691e010dd2dacffdd9ca7c13cc",
        "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science"
      },
      {
        "paperId": "2780ac7088bfbc4e2e681f0053f4f014065147a4",
        "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling"
      },
      {
        "paperId": "0eed0ba7bf63c67cd34c082990aaf96a49fbbdc5",
        "title": "Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment"
      },
      {
        "paperId": "5d10bada0d15ab6fde964373beec86a1cd0f2375",
        "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence"
      },
      {
        "paperId": "cf209c92e33525a2cdc8817b1dfbfa0dcea28c0d",
        "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering"
      },
      {
        "paperId": "dead4faccd5d800c3e06ea378e1da468cae7afd3",
        "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation"
      },
      {
        "paperId": "22f0804270d06d6a7e2d0cc52b1f8984c0a134a4",
        "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training"
      },
      {
        "paperId": "7aefed3af51083005f56bf3adda117ca477c7319",
        "title": "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs"
      },
      {
        "paperId": "9e8e6d8a128a293ae3be4691ebaf2b3b35c2a025",
        "title": "R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning"
      },
      {
        "paperId": "ac9fbe3af000a2f09099cf282afdac0aeb68e859",
        "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey"
      },
      {
        "paperId": "5bd465c1d9f74d4769d5a1887a13f84b8c7d1a73",
        "title": "Temporal Sampling for Forgotten Reasoning in LLMs"
      },
      {
        "paperId": "78f4d69750ecf17c76b9940a82e2c2f244deb27d",
        "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning"
      },
      {
        "paperId": "fff3bec525f79f2dcc433a275b234e3550534d21",
        "title": "Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models"
      },
      {
        "paperId": "7cab61a4d24a6e10abaadca758824e078e790187",
        "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning"
      },
      {
        "paperId": "8f2e8c783f0338dac22224fe05030d744eda3b00",
        "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions"
      },
      {
        "paperId": "be1985ec9f0bce3b45c81e7b28b93acbaf7e20eb",
        "title": "Self-Evolving Curriculum for LLM Reasoning"
      },
      {
        "paperId": "9995fe60c378bf9503c2c435feca5a560f58427b",
        "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning"
      },
      {
        "paperId": "0ae1916175a22fc8c40e5832c8a475413af182da",
        "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving"
      },
      {
        "paperId": "57244a5d6b500899eb1245fd4f52d85902e50cad",
        "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings"
      },
      {
        "paperId": "61cf800a8669b76592a08b3e84bd24a6aac26a9c",
        "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios"
      },
      {
        "paperId": "742d9c80b2ca4d01f8a8675cfe98487e0783d3d7",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training"
      },
      {
        "paperId": "6b0201470fe132402315c88932271d33e5028311",
        "title": "SuperCoder: Assembly Program Superoptimization with Large Language Models"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "0708d2761c22419a77f54873b7bb7800b43d31ad",
        "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models"
      },
      {
        "paperId": "70cb3adf46d64c685ef1a2673183e5c7b3ae2f83",
        "title": "TTRL: Test-Time Reinforcement Learning"
      },
      {
        "paperId": "350ddadbdc5c64c12d01ac2a5b9a2d8a9881d492",
        "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning"
      },
      {
        "paperId": "29bfc39b9eb357fb7d18b2029d1570844f2c9b9b",
        "title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs"
      },
      {
        "paperId": "5c29ce07433679fc5e2a5bed83d3a80464b966e6",
        "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents"
      },
      {
        "paperId": "aa06db9a5b5f8173502d6dd6fd4be96eda6a0f89",
        "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning"
      },
      {
        "paperId": "920cd8b25373358779fde44f90774533f26d782a",
        "title": "A Survey of Scaling in Large Language Model Reasoning"
      },
      {
        "paperId": "1b5b9fad9f97feefaf10c9f1e9685bed321dc3f2",
        "title": "Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?"
      },
      {
        "paperId": "e84ea7de17d55d11c63e3956644e2b0c34902622",
        "title": "Z1: Efficient Test-time Scaling with Code"
      },
      {
        "paperId": "7d03e6e12c24f832bc1a08db1d6f7b7f9c288e62",
        "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute"
      },
      {
        "paperId": "4aa5b70e979368c15374f76ac30d76289d040df3",
        "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL"
      },
      {
        "paperId": "08839ffbd8dbbc8ecf5cf1b2218e95647a32842a",
        "title": "CoSIL: Issue Localization via LLM-Driven Code Graph Searching"
      },
      {
        "paperId": "1d8e92e684a8a905edb622f9d8256258ba17447f",
        "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning"
      },
      {
        "paperId": "ccd9eca10294fe822a25e1133d59deacab005860",
        "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs"
      },
      {
        "paperId": "e968c45ea2c88772b02317a1a9ad5e26b320e6cb",
        "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data"
      },
      {
        "paperId": "f741edf18770aaedc20a807648fe214cb012eb99",
        "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "0b8321b6b218e52ff2050d6f0c4ef39d7e75d7a2",
        "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale"
      },
      {
        "paperId": "e2bd895ffbe527ba9cf038fc86990c059ee33a04",
        "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey"
      },
      {
        "paperId": "392d358e4d3a4d9509f7f609ee8e4b358a2a4207",
        "title": "Training of Scaffolded Language Models with Language Supervision: A Survey"
      },
      {
        "paperId": "6180615110d86033887d0dc4feebe8e8a162346f",
        "title": "Large Language Models Meet NLP: A Survey"
      },
      {
        "paperId": "cac59829e4d1da17119247a7f0bc8386a26408ad",
        "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs"
      },
      {
        "paperId": "0525026f436f3012c8e345b423f4bf04858d8203",
        "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning"
      },
      {
        "paperId": "d4d11bd2032cf83027b0f284cfd645141017ed65",
        "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      }
    ],
    "score": 87.0
  },
  {
    "id": "d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
    "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
    "authors": [
      "Ping Yu",
      "Hua Xu",
      "Xia Hu",
      "C. Deng"
    ],
    "year": 2023,
    "citationCount": 172,
    "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
    "url": "https://www.semanticscholar.org/paper/d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
    "pdf_url": "https://doi.org/10.3390/healthcare11202776",
    "venue": "Healthcare",
    "publicationDate": "2023-10-01",
    "externalIds": {
      "PubMedCentral": "10606429",
      "DOI": "10.3390/healthcare11202776",
      "CorpusId": 264374318,
      "PubMed": "37893850"
    },
    "references": [
      {
        "paperId": "a570d3d8f1b66a8ab3fff7876dc9bba3bcdc789b",
        "title": "Aligning Large Language Models for Clinical Tasks"
      },
      {
        "paperId": "2090f208a05344fd15a995fcbc9d539508841347",
        "title": "Local Large Language Models for Complex Structured Tasks."
      },
      {
        "paperId": "3b0792f6d7f6aa6aadd316e73943116afef2979b",
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "54e37d3f6234f615ead9e29c601d4ea37c5825fe",
        "title": "Leveraging Large Language Models for Generating Responses to Patient Messages"
      },
      {
        "paperId": "f5bcbf5738c42530e2d36f61b4aaa612af391639",
        "title": "Next Steps for Human-Centered Generative AI: A Technical Perspective"
      },
      {
        "paperId": "43a905cd70276d03b1da4f95a8f8b05ff054f80e",
        "title": "Large language model (ChatGPT) as a support tool for breast tumor board"
      },
      {
        "paperId": "ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb",
        "title": "Towards Expert-Level Medical Question Answering with Large Language Models"
      },
      {
        "paperId": "a70c50bff01547f00ce341cd7e42797f11c4cdc7",
        "title": "Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum."
      },
      {
        "paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a",
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"
      },
      {
        "paperId": "328926c090b184c3c62b4d6224cc7b7fdc318c53",
        "title": "Using AI-generated suggestions from ChatGPT to optimize clinical decision support"
      },
      {
        "paperId": "38179848e2d6a3ad373b1793848816111428ac36",
        "title": "OpenAGI: When LLM Meets Domain Experts"
      },
      {
        "paperId": "19cd2250f419666d4df441bae7ade1dd9a2f6bf9",
        "title": "ChatGPT in Healthcare: A Taxonomy and Systematic Review"
      },
      {
        "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"
      },
      {
        "paperId": "4a7f6c4e71e20311ade4e76e8d0945d499c31fcd",
        "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"
      },
      {
        "paperId": "6e96773bac534c87cf0eeaf11c5ba2a596b3380e",
        "title": "Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine"
      },
      {
        "paperId": "acc7a277c6a9cb05b8bf4c3c25c7369aa9a42555",
        "title": "Evaluating the use of large language model in identifying top research questions in gastroenterology"
      },
      {
        "paperId": "bdf7bf9e81a6c12e22323d0402885b2ba62f623e",
        "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"
      },
      {
        "paperId": "6c7ba2af4b3e472bd8a5717367b88dcdd4abbd31",
        "title": "Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios"
      },
      {
        "paperId": "dfdf7ff01aa6f691831e663fd29bc71890be39e2",
        "title": "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns"
      },
      {
        "paperId": "bab39663b1f31eb1ac79e6e0b1ed560b086803da",
        "title": "Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow"
      },
      {
        "paperId": "6839816cd3ab194dfa3ffe3066cc35d099820e00",
        "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT"
      },
      {
        "paperId": "89e184d2bc830af568e439db9476caa0c047e11a",
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c",
        "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment"
      },
      {
        "paperId": "d05ba0c40f3408aab7bb594628f24d9f04bf2831",
        "title": "Evaluating ChatGPT as an Adjunct for Radiologic Decision-Making"
      },
      {
        "paperId": "2223b92d9c5cb9b2594c1585eac03b32c2f313cb",
        "title": "The promise of large language models in health care"
      },
      {
        "paperId": "680c72c29b518398d9c45b5995a160583ea8e090",
        "title": "Analysis of large-language model versus human performance for genetics questions"
      },
      {
        "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
        "title": "Large language models encode clinical knowledge"
      },
      {
        "paperId": "cf1f26e7cbed3958b3c2870656568c299fece6e3",
        "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"
      },
      {
        "paperId": "0dc90d5c311de8bc11554e87b059fbb7c27c88ac",
        "title": "In Conversation with Artificial Intelligence: Aligning language Models with Human Values"
      },
      {
        "paperId": "c5427eab1a236e4529dc004307e921677fde304b",
        "title": "A new generative adversarial network for medical images super resolution"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "b15469d0ab3dc3a9dec037d761817b3fe546bed6",
        "title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey"
      },
      {
        "paperId": "7d5ca5268f9562b1b53fec94a3399e540f4fba61",
        "title": "Artificial Intelligence and Health Care Disparities in Radiology."
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
      },
      {
        "paperId": "4ae632b89089b38ce41d307a6cda4727e42aaab3",
        "title": "Detoxifying Language Models Risks Marginalizing Minority Voices"
      },
      {
        "paperId": "d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
        "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
      },
      {
        "paperId": "dba7d53701767e649e5a7c43acad0fec129481a9",
        "title": "Measurement of clinical documentation burden among physicians and nurses using electronic health records: a scoping review"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "paperId": "e718828e8f776d9f80daa3f8e0af6895f5d34c44",
        "title": "Adversarial attacks on medical machine learning"
      },
      {
        "paperId": "f3782c403afee5d154f8fd72ca7a375abae341e8",
        "title": "Citation-based clustering of publications using CitNetExplorer and VOSviewer"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "9230bf07c65aab932bf824d59f7dd60146ad990a",
        "title": "The Reliability of AHRQ Common Format Harm Scales in Rating Patient Safety Events"
      },
      {
        "paperId": "7191d865f6de3cdb212c41aa657c30670c23fff6",
        "title": "Can Large Language Models Safely Address Patient Questions Following Cataract Surgery?"
      },
      {
        "paperId": null,
        "title": "Fact Sheet: Biden-Harris Administration Secures Voluntary Commitments from Leading Arti\ufb01cial Intelligence Companies to Manage the Risks Posed by AI"
      },
      {
        "paperId": null,
        "title": "Alpaca: A Strong, Replicable Instruction-Following Model."
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "Self-re\ufb01ne: Iterative Re\ufb01nement with Self-Feedback"
      },
      {
        "paperId": null,
        "title": "Inside a Radical New Project to Democratize AI"
      },
      {
        "paperId": null,
        "title": "Language Models are Changing AI: The Need for Holistic Evaluation"
      },
      {
        "paperId": null,
        "title": "Generative AI: A Game-Changer That Society and Industry Need to Be Ready for"
      }
    ],
    "cited_by": [
      {
        "paperId": "6490ca6f79ccb8c3939c6d135d58e6fe71022ccd",
        "title": "Abortion AI: Toward an equity-centered research agenda for AI and abortion."
      },
      {
        "paperId": "341b9a28728ac01ff26981c1e7bed6046c8c6c79",
        "title": "Digital Twin Cognition: AI-Biomarker Integration in Biomimetic Neuropsychology"
      },
      {
        "paperId": "aa722631eee5c97ab82c51158ca0bebbda85c7c6",
        "title": "Evaluation of AI Tools Versus the PRISMA Method for Literature Search, Data Extraction, and Study Composition in Glaucoma Systematic Reviews: Content Analysis"
      },
      {
        "paperId": "934a3bc1c56d6a435d7ab88a99c4cf6a7f12ecf7",
        "title": "Quality and efficiency of integrating customised large language model-generated summaries versus physician-written summaries: a validation study"
      },
      {
        "paperId": "788a6ad091d0933bf3e25dd1c9bb95df42c3352e",
        "title": "Comparative evaluation of AI platforms \"Google Gemini 2.5 Flash, Google Gemini 2.0 Flash, DeepSeek V3 and ChatGPT 4o\" in solving multiple-choice questions from different subtopics of anatomy."
      },
      {
        "paperId": "3cc2d5e2df2612b85068fe3b6b1111829e9f296b",
        "title": "A comprehensive evaluation of large language models for information extraction from unstructured electronic health records in residential aged care."
      },
      {
        "paperId": "8b39d4de4a65c8f303adb647c1ec49309180845f",
        "title": "Use of generative artificial intelligence to improve output message effectiveness in decision support systems for prosumers"
      },
      {
        "paperId": "0e6ad9fd72c8e478b7f9627c45d10eaeff749c1a",
        "title": "Development of a method for using color in machine-readable optical codes to increase the information capacity"
      },
      {
        "paperId": "3fe1ac9ad25dad345c402298a98a4a33a8304fb9",
        "title": "A comparative analysis of deep learning architectures for thyroid tissue classification with hyperspectral imaging"
      },
      {
        "paperId": "ea97cc1bf9b5bf6646add1b5ccf0421761219a4b",
        "title": "Evaluation of Alignment Between Large Language Models and Expert Clinicians in Suicide Risk Assessment"
      },
      {
        "paperId": "56cf2311379429120ba6d8fc6de5d799c8a9da38",
        "title": "Digital Cardiovascular Twins, AI Agents, and Sensor Data: A Narrative Review from System Architecture to Proactive Heart Health"
      },
      {
        "paperId": "f54641868db8444426d134b0400cb70441bf0024",
        "title": "A comparative evaluation of publicly available large language models in the assessment of CTG traces according to the FIGO criteria."
      },
      {
        "paperId": "5bb10425798d09564ce2f6d4851a0b53f60b4eee",
        "title": "Parents\u2019 understanding and attitudes toward the application of AI in pediatric healthcare: a cross-sectional survey study"
      },
      {
        "paperId": "2118ab15fd4bc0506a8dc2cc1b17495da912f21c",
        "title": "Exploring AI use policies in manuscript writing in cardiology and vascular journals"
      },
      {
        "paperId": "a86969a415f029709c55c1928df1046b986e6e23",
        "title": "A Framework for Integrating Privacy by Design into Generative AI Applications"
      },
      {
        "paperId": "3c40bf60a1c1d0ccc953a2dbbb77e7b4d5494496",
        "title": "Artificial intelligence in pharmacovigilance: a narrative review and practical experience with an expert-defined Bayesian network tool"
      },
      {
        "paperId": "03aac9e4639c4628a59bc8378ce9a042f054c86c",
        "title": "The AI Genie in the Bottle: Charting AI Evolution and Aversion in Advertising"
      },
      {
        "paperId": "c1ad4524205d911491cfc8ab88c7bd22e6a2a40f",
        "title": "How Generative AI Operated by Large Language Models Fosters Sustainable Green Practices in Green Metaverse: Mediating Role of Green Cybersecurity"
      },
      {
        "paperId": "0b8ef565df3085db47804d8e8232f107c27284f8",
        "title": "Empowering standardized residency training in China through large language models: problem analysis and solutions"
      },
      {
        "paperId": "ef3af8d6d356d4c05cbebc166778934bd21c14e2",
        "title": "A Novel Llama 3-Based Prompt Engineering Platform for Textual Data Generation and Labeling"
      },
      {
        "paperId": "609ea44aa5f4a190fdd8f8cc049872f928187cd0",
        "title": "Conversational Health Interfaces in the Era of LLMs: Designing for Engagement, Privacy, and Wellbeing"
      },
      {
        "paperId": "756542e9a4d09ae611ef1b912769c6ef4a3b7949",
        "title": "Deep learning-based in-ambulance speech recognition and generation of prehospital emergency diagnostic summaries using LLMs"
      },
      {
        "paperId": "9a55c7eeea4d1e09245cc01a49b5a327872b9a18",
        "title": "Smart Healthcare Monitoring: A Generative AI Approach for Real-Time Analysis"
      },
      {
        "paperId": "90e294207e348fc3967bf7047f75065aee464091",
        "title": "From MYCIN to MedGemma: A Historical and Comparative Analysis of Healthcare AI Evolution"
      },
      {
        "paperId": "c761e722cc36a67d42028eec7345f6e99f0df53f",
        "title": "An Implementation of Machine Learning-Based Framework for Tertiary Institutions Data Harmonization using Large Language Model"
      },
      {
        "paperId": "5c163625721bafee04912ae8654636b2aabae587",
        "title": "Toward Real-time Detection of Drug-induced Liver Injury Using Large Language Models: A Feasibility Study From Clinical Notes."
      },
      {
        "paperId": "8eb2bb3f0215ee77ab9b08888ff0f917624fcc27",
        "title": "Comparative performance of twelve machine learning models in predicting COVID-19 mortality risk in children: a population-based retrospective cohort study in Brazil"
      },
      {
        "paperId": "29a1c9d4925445f512b29245775b6984b6454797",
        "title": "Generative Artificial Intelligence and Large Language Models: A Systematic Review of Architectures, Applications, and Future Directions"
      },
      {
        "paperId": "70dfcfb4c71794ee8abd895ae56ea372c95626b4",
        "title": "Promises and perils of generative artificial intelligence: a narrative review informing its ethical and practical applications in clinical exercise physiology"
      },
      {
        "paperId": "e8de7d34ad05e75427549c11e41b78cd8e6dc083",
        "title": "Benchmarking large language models GPT-4o, llama 3.1, and qwen 2.5 for cancer genetic variant classification"
      },
      {
        "paperId": "fcf0e2eacf20bb3db537c706c3c926d177a93f5b",
        "title": "Research on hospital information management system based on large language model (LLM) algorithm model"
      },
      {
        "paperId": "dadb8871b5576da627b4fc94c75ee06b13b7a038",
        "title": "Prompt design and comparing large language models for healthcare simulation case scenarios"
      },
      {
        "paperId": "2c6c6777e70d85e84bf388dd0d2176c5a1652f32",
        "title": "CGM Data Analysis 2.0: Functional Data Pattern Recognition and Artificial Intelligence Applications."
      },
      {
        "paperId": "ab064d19b68712a3d3273b90afd07ea85818d396",
        "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications"
      },
      {
        "paperId": "61916f05d83effd14f5c0b83c8fc0b307865c2d3",
        "title": "Unlocking Parkinson's disease: the role of microRNAs in regulation, diagnosis, and therapy."
      },
      {
        "paperId": "8eb30868abfe3e3ea9677399a35a131d32ca8a72",
        "title": "Integrating Large language models into radiology workflow: Impact of generating personalized report templates from summary."
      },
      {
        "paperId": "7408c05164bec084919dfa5d94f5f3e3516ed546",
        "title": "\"As an Autistic Person Myself:\" The Bias Paradox Around Autism in LLMs"
      },
      {
        "paperId": "65c30f44a31378c971b12462139050d05e0d4c0c",
        "title": "Industrial applications of large language models"
      },
      {
        "paperId": "e473fe791eae586e5524d28918dc80d9952ef69e",
        "title": "Assessing ChatGPT 4.0\u2019s Capabilities in the United Kingdom Medical Licensing Examination (UKMLA): A Robust Categorical Analysis"
      },
      {
        "paperId": "3390b3a60dca783ce53a1589a7a83ccac783d766",
        "title": "The Role of Generative AI in Personalized Medicine and Treatment Recommendations"
      },
      {
        "paperId": "63d5dda36039254aa8e84cf2eca2bb55ea3bb384",
        "title": "Exploring the impact of generative AI\u00a0tools on healthcare delivery in Tanzania"
      },
      {
        "paperId": "39b6751fcbf5cbea113456db473c1ec7edc4ea5f",
        "title": "How to read a paper involving artificial intelligence (AI)"
      },
      {
        "paperId": "af83b5aa450bb843f871e00a05dd5204010e406b",
        "title": "Medical accuracy of artificial intelligence chatbots in oncology: a scoping review"
      },
      {
        "paperId": "dbaf75ee9193e93a9fcce09c7f1f7b5d3d3013e5",
        "title": "Artificial Intelligence in Speech-Language Pathology and Dysphagia: A Review From Latin American Perspective and Pilot Test of LLMs for Rehabilitation Planning."
      },
      {
        "paperId": "67ec60d092971ce0ce38f9a935a082eb36814013",
        "title": "Artificial-intelligence-driven innovations in mechanistic computational modeling and digital twins for biomedical applications."
      },
      {
        "paperId": "f273c6461d8c2c10a2495458aa74d9d77e41cabb",
        "title": "Challenges for implementing generative artificial intelligence (GenAI) into clinical healthcare"
      },
      {
        "paperId": "61576633c9c8a9df4b6eb6f349cc88e4f494fb01",
        "title": "Generative AI-Driven Decision-Making for Disease Control and Pandemic Preparedness Model 4.0 in Rural Communities of Bangladesh: Management Informatics Approach"
      },
      {
        "paperId": "adffe6eef849bf2d84d50fe640bcca2fba0f972a",
        "title": "Assessing the Effectiveness of Automatic Speech Recognition Technology in Emergency Medicine Settings: a Comparative Study of Four AI-Powered Engines"
      },
      {
        "paperId": "1dd5a66d6a7346979571056fc991b764f4785abb",
        "title": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language Models with Fundus Photographs and OCT Images"
      },
      {
        "paperId": "d53112d2c6de373aed329503f6446c3a3d2d047e",
        "title": "Can large language models assist with pediatric dosing accuracy?"
      },
      {
        "paperId": "309d614e252c9954e357d6515249019b6a854d4a",
        "title": "Opportunities and Challenges for Large Language Models in Primary Health Care"
      },
      {
        "paperId": "45347b2c5363466376ccf3f8e43f495b5c66ae8c",
        "title": "Evaluating Large Language Models on the Spanish Medical Intern Resident (MIR) Examination 2024/2025:A Comparative Analysis of Clinical Reasoning and Knowledge Application"
      },
      {
        "paperId": "d69df13af87e4f61d86a75468a324ec1a4c0c401",
        "title": "Streamlining Ophthalmic Documentation with Anonymized, Fine-Tuned Language Models: A Feasibility Study (Preprint)"
      },
      {
        "paperId": "8f14ba1ce827a174916098d78b396c863d2ee7d5",
        "title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches"
      },
      {
        "paperId": "02d746c75c134f5fa115ec9d9d853c5a647cfd28",
        "title": "Artificial intelligence in healthcare education: evaluating the accuracy of ChatGPT, Copilot, and Google Gemini in cardiovascular pharmacology"
      },
      {
        "paperId": "ead55548208470391eda2f82f096c32f1ca01fad",
        "title": "Who Will Author the Synthetic Texts? Evoking Multiple Personas from Large Language Models to Represent Users' Associative Thesauri"
      },
      {
        "paperId": "69ed85369560853e51b1e6a0b06d8206036d84f8",
        "title": "The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs"
      },
      {
        "paperId": "045222d304b32dddec6c4d8712555bd261d60204",
        "title": "Bridging the Digital Divide: A Practical Roadmap for Deploying Medical Artificial Intelligence Technologies in Low-Resource Settings."
      },
      {
        "paperId": "d56575fcf74abd315c71553cca76242dd08182b5",
        "title": "Leveraging AI-Generated Content for Synthetic Electronic Health Record Generation With Deep Learning-Based Diagnosis Model"
      },
      {
        "paperId": "4c8c9107fb239b4af47b6b9840d712017e22b557",
        "title": "Large Language Models in Diabetes Management: The Need for Human and Artificial Intelligence Collaboration."
      },
      {
        "paperId": "248e7694603467295132531da77252fab608ad62",
        "title": "Adoption and impact of generative artificial intelligence on blockchain-enabled supply chain efficiency"
      },
      {
        "paperId": "e63de776fcc1ecabcde3e8d9d98f5f36306f3fd8",
        "title": "From statistics to deep learning: Using large language models in psychiatric research"
      },
      {
        "paperId": "520a17accab7e2ff89354d4db4aeb2ed38c46450",
        "title": "Harnessing artificial intelligence in sepsis care: advances in early detection, personalized treatment, and real-time monitoring"
      },
      {
        "paperId": "e77737b194220c6542173e11bdc48e4c829c0bb0",
        "title": "Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models"
      },
      {
        "paperId": "89c096a49ee213464a384b200100ce218550b1d3",
        "title": "Beyond the Screen: The Impact of Generative Artificial Intelligence (AI) on Patient Learning and the Patient-Physician Relationship"
      },
      {
        "paperId": "9eb21b25d4c1a5e46b31649f74d3a92c3bdfd4e8",
        "title": "Using Large Language Models to Retrieve Critical Data from Clinical Processes and Business Rules"
      },
      {
        "paperId": "59f8e71067e49b3d91133839482f7b2afa919397",
        "title": "Artificial Intelligence and ChatGPT in Medical Education: A Cross-Sectional Questionnaire on students\u2019 Competence"
      },
      {
        "paperId": "6416ee691fd651671e4d578b5d254c7d8d684b44",
        "title": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health and Therapeutic Counselling"
      },
      {
        "paperId": "d898693331565afc944797eba2a22d86ee7e3f99",
        "title": "Dynamic Insights: Well-being Activity Infused Fine-Tuning of Large Language Models"
      },
      {
        "paperId": "e4014ae4ba3e67371a310b40c5173231b60318bb",
        "title": "DIP AI-Driven Architecture for Enhanced Project Management Using Large Language Models"
      },
      {
        "paperId": "87169f4d783640b5031bb6a86854f605895a801c",
        "title": "Multimodal Large Language Model-Based Fault Detection and Diagnosis in Context of Industry 4.0"
      },
      {
        "paperId": "7368f32ce2741a7127d1805936c50c1d425d2be6",
        "title": "Changing trends in lung cancer disease burden between China and Australia from 1990 to 2019 and its predictions"
      },
      {
        "paperId": "d2bce6bdd2ceca9b8627e0545cffa5ea64cfc786",
        "title": "Exploring the potential of large language models for integration into an academic statistical consulting service\u2013the EXPOLS study protocol"
      },
      {
        "paperId": "4a65b46063edcd9a97ff73dfe42d2eeb0098e6a5",
        "title": "Artificial intelligence and pediatric surgery: where are we?"
      },
      {
        "paperId": "2dec8104c2f3d5fed1d27173d3aaa9a5059c6e41",
        "title": "Faster and better than a physician?: Assessing diagnostic proficiency of ChatGPT in misdiagnosed individuals with neuromyelitis optica spectrum disorder"
      },
      {
        "paperId": "3792f4798d18a7399bdfd2ac1a6a5d33c7e58056",
        "title": "AI-pocalypse now: Automating the systematic literature review with SPARK (Systematic processing and automated review Kit) \u2013 gathering, organising, filtering, and scaffolding."
      },
      {
        "paperId": "f4cea9f1d2337dfa48da374011330d25bdc70373",
        "title": "Ethical Application of Generative Artificial Intelligence in Medicine."
      },
      {
        "paperId": "28cac7ba9e7927edfba3e0f78652f2e74883df7c",
        "title": "Large Language Models Applied to Healthcare Tasks May Improve Clinical Efficiency, Value of Care Rendered, Research, and Medical Education."
      },
      {
        "paperId": "56973aecb87b2ea7ebd86b834103e75a7b556a28",
        "title": "The Intersection of Generative AI and Wearable Health Devices: A Privacy-Focused Analysis"
      },
      {
        "paperId": "6726ffbb5bc1edadfb06fc3abff32b558288aa5a",
        "title": "Qualitative metrics from the biomedical literature for evaluating large language models in clinical decision-making: a narrative review"
      },
      {
        "paperId": "fd61a0325c152c284d033940bb8d072fd5b7a951",
        "title": "Wearable Stroke Alert System - New Health of Things Approach Based on Generative AI and Datafusion for Real-Time Stroke Monitoring"
      },
      {
        "paperId": "939cdbd7eb4270d62226789d880aa04e079cbe2a",
        "title": "Exploring the Potential of LLMs and Attributed Prompt Engineering for Efficient Text Generation and Labeling"
      },
      {
        "paperId": "340f2a45bc3afc8f2cc4ef90f6ba5394e6151577",
        "title": "URAG: Unified Retrieval-Augmented Generation"
      },
      {
        "paperId": "6d3e2582a7f4a49e3e9c121696fa2e70c549e541",
        "title": "Artificial intelligence integration in healthcare: perspectives and trends in a survey of U.S. health system leaders"
      },
      {
        "paperId": "3682c14dc22b7db76915c71f17d5cfb488bec989",
        "title": "Lightening the Load: Generative AI to Mitigate the Burden of the New Era of Obesity Medical Therapy"
      },
      {
        "paperId": "31f42cf62948f389db01ae2b106add5b3b34a7d7",
        "title": "Integration of Generative Artificial Intelligence in Medicine - First Steps Toward an Ethical Risk Management Methodology"
      },
      {
        "paperId": "474467a493d2604e5f74394cb1dc1782acd751cd",
        "title": "Utilization of Generative Artificial Intelligence in Nursing Education: A Topic Modeling Analysis"
      },
      {
        "paperId": "f2d38483a4d5658a8f0e5825d3c28faa73443c6a",
        "title": "High accuracy but limited readability of large language model-generated responses to frequently asked questions about Kienb\u00f6ck\u2019s disease"
      },
      {
        "paperId": "e4c7c0b64b6b3bc2425f0d3327175a39ebf73cc6",
        "title": "The Role Generative AI in Human Resource Management: Enhancing Operational Efficiency, Decision-Making, and Addressing Ethical Challenges"
      },
      {
        "paperId": "5bdefae2f9cca78332bcbcde5963aa1b3a694834",
        "title": "The role of AI in detecting and mitigating human errors in safety-critical industries: A review"
      },
      {
        "paperId": "7a641a82086a5770724859f6d84ccd4f4910da0e",
        "title": "Creating Perinatal Nursing Care Plans Using ChatGPT"
      },
      {
        "paperId": "33601e98234d3e41eb6778e73bf30769e3bb823f",
        "title": "Statistics of Generative AI & Non-Generative Predictive Analytics Machine Learning in Medicine."
      },
      {
        "paperId": "61665383f759630ac693059c82b63570f2f39fb2",
        "title": "The recent history and near future of digital health in the field of behavioral medicine: an update on progress from 2019 to 2024"
      },
      {
        "paperId": "77f2def4e2d6a33ffbb157e709e070e843685203",
        "title": "Can LLMs Reason Like Humans? Assessing Theory of Mind Reasoning in LLMs for Open-Ended Questions"
      },
      {
        "paperId": "52fe2f85791c521d9a8bc99f4b42f591ee7421ca",
        "title": "Challenges and applications in generative AI for clinical tabular data in physiology"
      },
      {
        "paperId": "cb3790da11fd1f15ec668fcf584be1a6a23c0425",
        "title": "Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"
      },
      {
        "paperId": "f5f5e1f378a996d267b6e1b733a69a04c026b447",
        "title": "A Comprehensive Investigation on Leveraging Generative AI and Large Language Models in the Healthcare Domain"
      },
      {
        "paperId": "cc6c41598d65b23737e8af86a975f699fca747af",
        "title": "Exploring the potential benefits and challenges of artificial intelligence for research funding organisations: a scoping review"
      },
      {
        "paperId": "1895f05db41c375fc098fe9323824be2da72cae2",
        "title": "The Role of Language Models in Modern Healthcare: A Comprehensive Review"
      },
      {
        "paperId": "6965c9b550661e09d5c08cbad815637a3ff8f5b6",
        "title": "Evaluation and Analysis of Large Language Models Performance in English Exam"
      },
      {
        "paperId": "618602f64902ca0632e58a376a163008f99ac5b2",
        "title": "Leveraging Guideline-Based Clinical Decision Support Systems with Large Language Models: A Case Study with Breast Cancer"
      },
      {
        "paperId": "8a9a2c7036902e3f80ceb95c6339a4f844f6af2d",
        "title": "Chatting Up Attachment: Using LLMs to Predict Adult Bonds"
      },
      {
        "paperId": "82b23dd514e7fad09452d010b054fd698cc27b5b",
        "title": "Large Language Model Prompting Techniques for Advancement in Clinical Medicine"
      },
      {
        "paperId": "0a4d275dc822a64f7a48abbe2580864547844a37",
        "title": "Generative artificial intelligence in healthcare: current status and future directions"
      },
      {
        "paperId": "fe32274865d90cb955a917f2c44e158e5e621155",
        "title": "Assessing the Effectiveness of Automatic Speech Recognition Technology in Emergency Medicine Settings: A Comparative Study of Four AI-powered Engines"
      },
      {
        "paperId": "6dd8b68cbe1fc1a31926e213fec7a23eb01df812",
        "title": "Assessing and Enhancing Large Language Models in Rare Disease Question-answering"
      },
      {
        "paperId": "0f239738ab2a3223585642a0b8df224f874bbdba",
        "title": "Navigating Consumer Skepticism: Conceptual Model of Distrust Detection and Recognition Technology Adoption Among Online Shop Owners in Indonesia"
      },
      {
        "paperId": "42f4be1f38ac34c5ce5942ea1302329e6b47db00",
        "title": "Performance of ChatGPT 4.0 on Japan's National Physical Therapist Examination: A Comprehensive Analysis of Text and Visual Question Handling"
      },
      {
        "paperId": "0fe2c75cc895704a0d22502efb728fe816fce28c",
        "title": "Cybersecurity in the generative artificial intelligence era."
      },
      {
        "paperId": "72d87ab60ee85c26814c8e24e2f37e7252bf8c9d",
        "title": "Charting the Path"
      },
      {
        "paperId": "8dd0c66d5e4b9a1b5b08649e25db5e4032e52065",
        "title": "Clinician Perceptions of Generative Artificial Intelligence Tools and Clinical Workflows: Potential Uses, Motivations for Adoption, and Sentiments on Impact"
      },
      {
        "paperId": "4b6397281fd865d5bf6f7deffac3e6a659f1e43c",
        "title": "Generative artificial intelligence in dentistry: Current approaches and future challenges"
      },
      {
        "paperId": "87a4af866f51bc1bc946b5e59fa6a3f3925a1924",
        "title": "The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare"
      },
      {
        "paperId": "3cff673e4ddb32d3e381b0cb735144082974204e",
        "title": "Framework for Integrating Generative AI in Developing Competencies for Accounting and Audit Professionals"
      },
      {
        "paperId": "6e3c23e7161e81c25c96d17e60ada6edb9393f11",
        "title": "Quality of ChatGPT-Generated Therapy Recommendations for Breast Cancer Treatment in Gynecology"
      },
      {
        "paperId": "f71f5842db99915c7b3381d008827d7878590b13",
        "title": "Assessment of the information provided by ChatGPT regarding exercise for patients with type 2 diabetes: a pilot study"
      },
      {
        "paperId": "7960026916bd2a9dfd37444351266de789494089",
        "title": "Status and Trends of the Digital Healthcare Industry"
      },
      {
        "paperId": "402dcc9c208ed9b287d5ddf6ef23b4f1b961cb00",
        "title": "Evaluating machine learning approaches for multi-label classification of unstructured electronic health records with a generative large language model"
      },
      {
        "paperId": "8829ebc76471e5bd3afae270ccc2d0466aad1820",
        "title": "Integrating ChatGPT-4: A Novel XAI Interface for Enhanced Clinician Understanding of MRI Image Segmentation Results"
      },
      {
        "paperId": "caff0d85b06bbe157c0427a024666d2d4f0ee694",
        "title": "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses"
      },
      {
        "paperId": "2385d9b63898b1c11080f3897e71b0ed6ec06219",
        "title": "The assessment of the validity, safety, and utility of ChatGPT for patients with herniated lumbar disc: A preliminary study"
      },
      {
        "paperId": "1bcb54dadcec8ea7d99a9284d8d3d23072357cad",
        "title": "Health informatics to enhance the healthcare industry's culture: An extensive analysis of its features, contributions, applications and limitations"
      },
      {
        "paperId": "a012f39750f446e9f45d064c4fc6a284c485b091",
        "title": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records"
      },
      {
        "paperId": "bc32832ade41cc271bf097386733e20427581a75",
        "title": "Large language models and artificial intelligence chatbots in vascular surgery."
      },
      {
        "paperId": "72ed7785539450085f17ff171a936d369c79379f",
        "title": "Large language models reshaping molecular biology and drug development"
      },
      {
        "paperId": "64e31c324ee3efa535e633b6134e227693a4fecc",
        "title": "Use of an ambient artificial intelligence tool to improve quality of clinical documentation"
      },
      {
        "paperId": "9d059ae691953578017f1361742c4b36046b92e8",
        "title": "On Fairness of Low-Rank Adaptation of Large Models"
      },
      {
        "paperId": "04762cbef4a118f56d9ad8f2d6d400e0e32dc6ef",
        "title": "Hummer: Towards Limited Competitive Preference Dataset"
      },
      {
        "paperId": "c1fbe12a069fc29b1870bef8888d35b0e3ee20b7",
        "title": "Holistic Multi-layered System Design for Human-Centered Dialog Systems"
      },
      {
        "paperId": "f48e0406bfac8025b36982c94a9183968378587f",
        "title": "A scoping review of using Large Language Models (LLMs) to investigate Electronic Health Records (EHRs)"
      },
      {
        "paperId": "ccb3ce84f8be6aa4e88f5b0902090bf4b0e5174c",
        "title": "The new paradigm in machine learning \u2013 foundation models, large language models and beyond: a primer for physicians"
      },
      {
        "paperId": "34cf4624cc657cf7fb50466e6dc1b028ff239ece",
        "title": "Artificial Intelligence-Generated Content Needs a Human Oversight"
      },
      {
        "paperId": "6da1f31fe0a8bf7df405ffac4c3372df45cf6575",
        "title": "Generative AI Models in Time-Varying Biomedical Data: Scoping Review"
      },
      {
        "paperId": "abe088d2d09dab9d0197a52deb03c6ccb16ddee0",
        "title": "A Comparison Between Transformers and Foundation Models in Sentiment Analysis of Student Evaluation of Teaching"
      },
      {
        "paperId": "02fd3961cbfcc4fef1af2656565ce36e32c96ae2",
        "title": "Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study"
      },
      {
        "paperId": "7bba55799001a87ed8ac35369a23df2f33f35b6b",
        "title": "ChatGPT and Large Language Models in Healthcare; a Bibliometrics Analysis and Review"
      },
      {
        "paperId": "af82ec99b191e84ca1138ea787e9c0ace8a90613",
        "title": "Evaluation of the safety, accuracy, and helpfulness of the GPT-4.0 Large Language Model in neurosurgery"
      },
      {
        "paperId": "e5f8053ae4f5264eec4875df77eb798b9c81263c",
        "title": "ChatGPT for Automated Cross-Checking of Authors' Conflicts of Interest Against Industry Payments."
      },
      {
        "paperId": "21aff006cf2acb8e8fdab1bbd19f9d9a1f4cbc7f",
        "title": "Generative Pre-Trained Transformer-Empowered Healthcare Conversations: Current Trends, Challenges, and Future Directions in Large Language Model-Enabled Medical Chatbots"
      },
      {
        "paperId": "30fce83297f693ec2f97a0c05ec28d910c6c6b9d",
        "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future"
      },
      {
        "paperId": "e9298b8e336eccfb55b08a5222b1223bb8fd194d",
        "title": "The Intersection of Generative AI and Healthcare: Addressing Challenges to Enhance Patient Care"
      },
      {
        "paperId": "ed75bfc4ffda848edbf63e70d352735e16d14eec",
        "title": "A rapid review on current and potential uses of large language models in nursing."
      },
      {
        "paperId": "d7e70b562784659721e9c12e162c3b0b1a023b6d",
        "title": "Integrating Retrieval-Augmented Generation with Large Language Models in Nephrology: Advancing Practical Applications"
      },
      {
        "paperId": "86c10e08b8f1e848ddb6b5f59f7dae061e7e8bd9",
        "title": "Ethical considerations in implementing generative AI for healthcare supply chain optimization: A cross-country analysis across India, the United Kingdom, and the United States of America"
      },
      {
        "paperId": "527c03e8b15b493684724f3a5aa1504cf4fa1d57",
        "title": "Recommendations for diabetic macular edema management by retina specialists and large language model-based artificial intelligence platforms"
      },
      {
        "paperId": "7641749cae1ad30779bfb46948fd47922bcc296a",
        "title": "LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop"
      },
      {
        "paperId": "f36baa7789a546f4d87309c318914e7a47d675df",
        "title": "Diagnosis of malignancy in oropharyngeal confocal laser endomicroscopy using GPT 4.0 with vision"
      },
      {
        "paperId": "fca22b967142e375d01d2fa14b33b423746d9485",
        "title": "Recommendations for initial diabetic retinopathy screening of diabetic patients using large language model-based artificial intelligence in real-life case scenarios"
      },
      {
        "paperId": "d4591a5e57efe9971e5030e0eba805753030bfc8",
        "title": "An Advanced Deep Learning Framework for Multi-Class Diagnosis from Chest X-ray Images"
      },
      {
        "paperId": "467411fde97d4bc25b8f2238cf36324367a3d6d5",
        "title": "Transformative Potential of AI in Healthcare: Definitions, Applications, and Navigating the Ethical Landscape and Public Perspectives"
      },
      {
        "paperId": "4d75bc6d1d751883abfceffba32cd8b5d4a03ade",
        "title": "A Review of Top Cardiology and Cardiovascular Medicine Journal Guidelines regarding the use of Generative Artificial Intelligence Tools in Scientific Writing."
      },
      {
        "paperId": "61ae93ee6b2028217a977dfa41a5765934d6d911",
        "title": "Chain of Thought Utilization in Large Language Models and Application in Nephrology"
      },
      {
        "paperId": "5edc904ce96ff2648a84251b6af7867f03b877f9",
        "title": "Personalized Medicine in Urolithiasis: AI Chatbot-Assisted Dietary Management of Oxalate for Kidney Stone Prevention"
      },
      {
        "paperId": "be2b40b214470ab7a65464e32b2ff68e0ce3bf65",
        "title": "Assessing AI efficacy in medical knowledge tests: A study using Taiwan's internal medicine exam questions from 2020 to 2023"
      },
      {
        "paperId": "50518a04f0e1ee4f9e9c28eaa21c2f76099b5e41",
        "title": "Clinical Text Classification in Healthcare: Leveraging BERT for NLP"
      },
      {
        "paperId": "d9403061e3a3392652d037138532fab111c845c1",
        "title": "Large language models in healthcare and medical domain: A review"
      },
      {
        "paperId": "5c869eb83c035e038503f78c08f8ecde48f4fd93",
        "title": "Fine-Tuning the Llama2 Large Language Model Using Books on the Diagnosis and Treatment of Musculoskeletal System in Physical Therapy"
      },
      {
        "paperId": "46b2a58d30ea3b0953cac342bbac62cd7f3ce671",
        "title": "A Review of Applying Large Language Models in Healthcare"
      },
      {
        "paperId": "c77204c3c3f21ef82b4295c6a77e5eb04cf90c52",
        "title": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs"
      },
      {
        "paperId": "cd686fa41fe5b4a6fc22f9ff0d8290324ae07b4f",
        "title": "Metaverse Unbound: A Survey on Synergistic Integration Between Semantic Communication, 6G, and Edge Learning"
      },
      {
        "paperId": "e65eb34fe0fc28d866a7a6d591e7f62090d44aa6",
        "title": "Six-step approach for developing customized GPT in medical education \u200e"
      },
      {
        "paperId": "1a6e2f6d807c8e63d63c1c413586b7fb10d82524",
        "title": "A Comparative Study of the Advantages and Disadvantages of DeepSeek and SPSS in \nStatistical Analysis"
      },
      {
        "paperId": "553eb2db30f18be1dd0368ced133d9d019cb8a06",
        "title": "Artificial Intelligence Chatbots in Surgical Care: A Systematic Review of Clinical Applications"
      },
      {
        "paperId": "7af44b5d8f51fa856e7537766deb44e3e8616735",
        "title": "Conceptualizing generative AI as style engines: Application archetypes and implications"
      },
      {
        "paperId": "b543596a6557d9d1f35a644ea7b05108c4e6cf66",
        "title": "Effectiveness of Cross-linguistic Extraction of Genetic Information using Generative Large Language Models"
      },
      {
        "paperId": "fbca0c2ec5425bbd8dc4898d684c909a58dab1de",
        "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop"
      },
      {
        "paperId": "59822f384f790b7d1ddd3e15bf0869b684525093",
        "title": "A Systematic Literature Review on LLM-Based Information Retrieval: The Issue of Contents Classification"
      },
      {
        "paperId": "c03eef81fa8b507c409b4464ec509d587dc95511",
        "title": "Influence of AI on Healthcare Delivery"
      },
      {
        "paperId": "29dad53fc506afa45485979183cd6e041364e851",
        "title": "On Fairness of Low-Rank Adaptation of Vision Models"
      },
      {
        "paperId": "b355c9b370bb03020697bf9ef67c1633ca30566e",
        "title": "Transformative Potential of Large Language Models in Healthcare: A Comprehensive Review and Analysis"
      },
      {
        "paperId": "b5d1fe3d9a039e972bd0ae4d10bba93512efc8a5",
        "title": "Utilization of Generative Arti\ufb01cial Intelligence in Nursing Education: A Topic Modeling Analysis"
      },
      {
        "paperId": "c47bb1a6c7fa8c6965875b183d6d1dd8605f6a1f",
        "title": "Generative LLMs for Synthetic Data Generation: Methods, Challenges and the Future"
      }
    ],
    "score": 86.0
  },
  {
    "id": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
    "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "authors": [
      "Han Zhong",
      "Guhao Feng",
      "Wei Xiong",
      "Li Zhao",
      "Di He",
      "Jiang Bian",
      "Liwei Wang"
    ],
    "year": 2024,
    "citationCount": 84,
    "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of large language models, its open-source implementation is still largely sub-optimal. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Under this framework, we introduce an algorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive experiments demonstrate that \\texttt{RTO} performs better than PPO and other direct preference learning algorithms. In particular, RTO outperforms PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.",
    "url": "https://www.semanticscholar.org/paper/16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
    "pdf_url": "https://arxiv.org/pdf/2404.18922.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-04-29",
    "externalIds": {
      "ArXiv": "2404.18922",
      "DBLP": "journals/corr/abs-2404-18922",
      "DOI": "10.48550/arXiv.2404.18922",
      "CorpusId": 269448794
    },
    "references": [
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "1c3aaffa10f83cac66a78f7cb796cb64edac6030",
        "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "05f02b4ed43d01f3efbbdcb454cc17b333f74817",
        "title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "8f2254fb38cfc8f79524fd1cd2609124808f2c8c",
        "title": "Token-level Direct Preference Optimization"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "9637ef9019671034912ea0f506ae67c3f2fc4689",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
        "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "187c1466b00069969c8c44e2491e31d7aac09a4a",
        "title": "Mathematical Analysis of Machine Learning Algorithms"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "b3f272d644fe580d18f635be4d6ac4c520ef0d0f",
        "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cedfdde4b9d01530bf2932554561bb25623890e5",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "93c5963791eda842b1897a4660d737e76c852b25",
        "title": "On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "bfce86bcdce722c175cd07c8cb0203a7bb0b5711",
        "title": "A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "49bbd1064f1a17f314f8c61528bcce959a7c249b",
        "title": "GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond"
      },
      {
        "paperId": "bb4c13c86048a48ec3f398d7d613ff8e56e1c8ae",
        "title": "Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "a1282f8334e781ed33d7479fcde32956d7e9c0dc",
        "title": "When Is Partially Observable Reinforcement Learning Not Scary?"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "e75ee46c70e9fc56a59b5c783c3070f28791c218",
        "title": "Nearly Optimal Policy Optimization with Stable at Any Time Guarantee"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
        "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "a56776af2884d1e5bc0515c3dc49288b87c0a2ef",
        "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization"
      },
      {
        "paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634",
        "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "5fa14f5d09031736a70bca9675f888dbeab644a3",
        "title": "Improved Optimistic Algorithms for Logistic Bandits"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "8b191d96db9248c8284d6146976ca3b5c4858f00",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      },
      {
        "paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0",
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": null,
        "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "bcc91889adc389cbe25295961c07a3484225ee7b",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multi-modal models"
      },
      {
        "paperId": "d39a902effe6873746868ca44860be8f0d13ff8b",
        "title": "Optimal Algorithms for Stochastic Contextual Preference Bandits"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c",
        "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "Introducing claude"
      },
      {
        "paperId": null,
        "title": "Chris Glaze (2024)"
      }
    ],
    "cited_by": [
      {
        "paperId": "21b81883c1f4c789fec33d239ef173ea51b73918",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"
      },
      {
        "paperId": "cfea01604b97d1ed0721992fcf0e8dea4ecbcde6",
        "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation"
      },
      {
        "paperId": "2e3d2cb75be2d8939387f9173b68cd81315ec694",
        "title": "Agentic Reinforcement Learning with Implicit Step Rewards"
      },
      {
        "paperId": "6371edc5318fcfbe2628d938e9d4173ee7e50167",
        "title": "On-Device, Diverse, Difficulty-Driven Level Generation for Match 3D Puzzles via Reinforcement Learning"
      },
      {
        "paperId": "563ebf2ff90f5e7a0b2526b5110539122216fee6",
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning"
      },
      {
        "paperId": "66d7b70b059206c4e93f77331b3157122d17ddfb",
        "title": "Reinforced Language Models for Sequential Decision Making"
      },
      {
        "paperId": "8a61f300c04a888bdca5b731bd7e9938a0bac21a",
        "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "33eb087f52e37349f13c9a44e6bd09d5f89fcbfd",
        "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework"
      },
      {
        "paperId": "bba9df17cf0452603e402f508da87ef311282c6b",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"
      },
      {
        "paperId": "7973a601c208c80ba8f9cd6af9751f0609a17605",
        "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization"
      },
      {
        "paperId": "0933fc39e13991fc9650481f7467b8061423ee87",
        "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training"
      },
      {
        "paperId": "21abbc6a6edde1b8bc2ceb7b7a7564a5a8e34c85",
        "title": "Reinforce LLM Reasoning through Multi-Agent Reflection"
      },
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "136f282b12e52779a471ed2417e32fd882f67cde",
        "title": "Discriminative Policy Optimization for Token-Level Reward Models"
      },
      {
        "paperId": "02bc41796acfbba89c01785ee8635ea960f67f3e",
        "title": "Token-Importance Guided Direct Preference Optimization"
      },
      {
        "paperId": "3f72b27af217e53849e5a20105f280920351f62b",
        "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models"
      },
      {
        "paperId": "496e0476e3368cb522612dde39cb477292d3fd08",
        "title": "Toward Scientific Reasoning in LLMs: Training from Expert Discussions via Reinforcement Learning"
      },
      {
        "paperId": "acf2f14fbf6372470d6cc3bd7a8d17b54a00939d",
        "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning"
      },
      {
        "paperId": "4ed58e259c81ccabc41fd683faef1f1835a89153",
        "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning"
      },
      {
        "paperId": "859caa7e5a365c5c13d5fb0c614ac8d151126e29",
        "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
      },
      {
        "paperId": "23452c2b1136fd3fd08f2e2d85805c86792c2060",
        "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs"
      },
      {
        "paperId": "123d32247adb624d77694b2562ddb3ed9b964188",
        "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans"
      },
      {
        "paperId": "f1173df25d83e5c6ff16e087f890993bccf355ed",
        "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "3d58168c202c95ee932c1913f445b3f5efe0f75e",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL"
      },
      {
        "paperId": "758fc4cb9100f83ed68f30850af2af824e2a9f6f",
        "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization"
      },
      {
        "paperId": "fe81184b9fdb0ff9a5cad8c68b5bcebd9cfccf5e",
        "title": "Kongzi: A Historical Large Language Model with Fact Enhancement"
      },
      {
        "paperId": "9c41e86dcaa1b1b78e77876a54acb2adf479b20b",
        "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure"
      },
      {
        "paperId": "fc0f813c9918ec0c2f2582518387786ea1f0f8e6",
        "title": "Entropy-Based Adaptive Weighting for Self-Training"
      },
      {
        "paperId": "bc6016c22b3ccdc838adaf0963d04bdc4ec654b4",
        "title": "Controlling Large Language Model with Latent Actions"
      },
      {
        "paperId": "95bcd5cdec716fa2509cc275aa7bf0f619755fcc",
        "title": "MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "864aad3b02011f9eca0fc21946f322d2481e826b",
        "title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models"
      },
      {
        "paperId": "eaf12ceb99411878ffdc339998803f98a6dfca73",
        "title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      },
      {
        "paperId": "ad231642cc7953b82d98d648fdb1fde28bea9c78",
        "title": "A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning"
      },
      {
        "paperId": "145a2e2ff7e83dc2bbf3d95acf3dba046e2d69da",
        "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "ab72ab40e2baeea8a57d1db386737239d8e07397",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models"
      },
      {
        "paperId": "7726d40d59af31de2a88d265d3eb8ee5be5f74c1",
        "title": "Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL"
      },
      {
        "paperId": "9f13649ed8c1987a8de4c1d6df90ab9da344d09d",
        "title": "Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective"
      },
      {
        "paperId": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
        "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
      },
      {
        "paperId": "0f11244a4290d31c80e744dc7207fa8d18bac016",
        "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation"
      },
      {
        "paperId": "3604e2aa051055d81ef9fe636da4f1c4285e964b",
        "title": "Current and future state of evaluation of large language models for medical summarization tasks"
      },
      {
        "paperId": "75acc2173df357ac8f2e47e2431100c1dbc47e18",
        "title": "On Almost Surely Safe Alignment of Large Language Models at Inference-Time"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "1c3aaffa10f83cac66a78f7cb796cb64edac6030",
        "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model"
      },
      {
        "paperId": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
      },
      {
        "paperId": "68e64ff720c2a6cc2a306aacbeb6f04320ad9805",
        "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "f520c3faaa8e665c019995d01ad3fde80d3c0be2",
        "title": "Entropy-Regularized Process Reward Model"
      },
      {
        "paperId": "2f203a74cacbffaa6c9e369b85d140cf1c987afa",
        "title": "Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention"
      },
      {
        "paperId": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
        "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution"
      },
      {
        "paperId": "6bd400acb88b37d1f1b46b2db16c0281cf2fa469",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "paperId": "6e220dff22a40acb83c5af920fe3ad5d71e3cc83",
        "title": "Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of Human Annotation in Preference Tuning"
      },
      {
        "paperId": "7541299db7430f11211c3d8479f121bdfb485729",
        "title": "Token-level Proximal Policy Optimization for Query Generation"
      },
      {
        "paperId": "cff317750060a230a6f7a1ca4ae147bc84c62abd",
        "title": "Fine-Tuning and Evaluating Open-Source Large Language Models for the Army Domain"
      },
      {
        "paperId": "da1630b8e02de660c0baccaddbb790ce94623327",
        "title": "Fast Best-of-N Decoding via Speculative Rejection"
      },
      {
        "paperId": "04f94ef16da433fc22949ffe267151bae3fd3510",
        "title": "Adaptive Segment-level Reward: Bridging the Gap Between Action and Reward Space in Alignment"
      },
      {
        "paperId": "89b42dc02400db2c58703fb0d8d0b6f0210bfe7f",
        "title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning"
      },
      {
        "paperId": "0f1b1cd6309691ff5e5e7f90a8debef38c8c7117",
        "title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets"
      },
      {
        "paperId": "2fd1e96f56794be12b02044f83eb2c2370c69037",
        "title": "Process Reward Model with Q-Value Rankings"
      },
      {
        "paperId": "5839e3641ed21a12a2844b824a6707daec10ab1a",
        "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment"
      },
      {
        "paperId": "a1fab27dc91bb36a37a3bb9b5a741267e1db4630",
        "title": "Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review"
      },
      {
        "paperId": "e13362d862a844a14d9d43f80ef12118d06621a4",
        "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "7792fe02a297344a25a0b22fa8d6cbd86884be31",
        "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "a2fae006e6c5ac346fd51bc8a009127f9abe22df",
        "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "5387445a58a958422a8cfd297e6a611aade0f0e8",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "fb5336e23363da4ffe3a4e66b8bbd5ea807b7915",
        "title": "The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "8f2254fb38cfc8f79524fd1cd2609124808f2c8c",
        "title": "Token-level Direct Preference Optimization"
      },
      {
        "paperId": "fac493a6e87a9e38ce441fa47fed6c2a51d8e42f",
        "title": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference Annotation"
      },
      {
        "paperId": "80b98ce3dda7c40418ceb54091ed15bd5a22acd7",
        "title": "Accelerating Best-of-N via Speculative Rejection"
      },
      {
        "paperId": "66687b19fd83edd3d1c638392d3c7d57c09e48a0",
        "title": "Accelerating Best-of-N via Speculative Rejection"
      }
    ],
    "score": 84.0
  },
  {
    "id": "0c43750030198dbe7fe164e1ce743ec64427bca1",
    "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
    "authors": [
      "Rafael Rafailov",
      "Yaswanth Chittepu",
      "Ryan Park",
      "Harshit S. Sikchi",
      "Joey Hejna",
      "Bradley Knox",
      "Chelsea Finn",
      "S. Niekum"
    ],
    "year": 2024,
    "citationCount": 84,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is reward over-optimization or reward hacking, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.",
    "url": "https://www.semanticscholar.org/paper/0c43750030198dbe7fe164e1ce743ec64427bca1",
    "pdf_url": "https://arxiv.org/pdf/2406.02900.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-06-05",
    "externalIds": {
      "DBLP": "conf/nips/RafailovCPSHKFN24",
      "ArXiv": "2406.02900",
      "DOI": "10.48550/arXiv.2406.02900",
      "CorpusId": 270257855
    },
    "references": [
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "6073196bd50b000571dbdfc46418304a4aff6591",
        "title": "Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee",
        "title": "Understanding the Learning Dynamics of Alignment with Human Feedback"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "4fe4f0f9d39d708a6c3d7b8dfbfa2616cd376e1e",
        "title": "V-STaR: Training Verifiers for Self-Taught Reasoners"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      },
      {
        "paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "f5275c61736781d236abe6700b822f1ea62f982e",
        "title": "Diffusion Model Alignment Using Direct Preference Optimization"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "5b5c03667055aae101ba6d57399457732a6fae99",
        "title": "Coherent Soft Imitation Learning"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
        "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "622a5434eae1b53fe0b74ad0d0942212cd722c36",
        "title": "A Ranking Game for Imitation Learning"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb",
        "title": "Scaling Laws for Transfer"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
        "title": "Quantifying Generalization in Reinforcement Learning"
      },
      {
        "paperId": "567eb0ab96fb7f2869e9c4c204da9f2cf422d946",
        "title": "Generalization and Regularization in DQN"
      },
      {
        "paperId": "f3f4689e4346ce497a2c3e367f6ce365da9c8966",
        "title": "On Adversarial Examples for Character-Level Neural Machine Translation"
      },
      {
        "paperId": "80939dd8a5cad405053c88ed856c28791bdc0582",
        "title": "Categorizing Variants of Goodhart's Law"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e5ba74bd3b6c9bb0287d8835621ddd40dd3ebbf4",
        "title": "Reinforcement Learning with a Corrupted Reward Channel"
      },
      {
        "paperId": "236b40f3144b95cd84779484c8269092122920aa",
        "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "4e8ff3b4069a12a00196d62925bab8add7389742",
        "title": "Quantilizers: A Safer Alternative to Maximizers for Limited Optimization"
      },
      {
        "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "title": "Intriguing properties of neural networks"
      },
      {
        "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
        "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"
      },
      {
        "paperId": "117a50fbdfd473e43e550c6103733e6cb4aecb4c",
        "title": "Maximum margin planning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "012948026226fc74bf546ff5024dd735f5d30605",
        "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning"
      },
      {
        "paperId": "e0d35b72c3bc26ec3f01019678ea18ced037c805",
        "title": "The \u2018 awful idea of accountability \u2019 : inscribing people into the measurement of objects"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": null,
        "title": "Classifying specification problems"
      },
      {
        "paperId": null,
        "title": "Faulty reward functions in the wild"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Author manuscript, published in \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (2011)\" Preference-based Policy Learning"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "A simulation framework for methods"
      },
      {
        "paperId": null,
        "title": "Statistical rejection sampling"
      },
      {
        "paperId": null,
        "title": "This appendix contains similar intra-epoch KL divergence and winrate evolution results as in Fig. 2, 517 across all model sizes"
      },
      {
        "paperId": null,
        "title": "Reward model ensembles"
      },
      {
        "paperId": null,
        "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      },
      {
        "paperId": null,
        "title": "A suite for"
      },
      {
        "paperId": null,
        "title": "If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully"
      },
      {
        "paperId": null,
        "title": "Who answers it better? an in-depth analysis of chatgpt and stack overflow"
      }
    ],
    "cited_by": [
      {
        "paperId": "51fa6682984b9a8b3085cdd3d2918444317ef3ee",
        "title": "Alignment-Aware Decoding"
      },
      {
        "paperId": "94317dab6cb3d29548abb365c87128aa53ce7d49",
        "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks"
      },
      {
        "paperId": "9f6f36d1a3e4a1d7f78aaa4e3565bfe1fff53339",
        "title": "Causally-Enhanced Reinforcement Policy Optimization"
      },
      {
        "paperId": "057d785034c4debfe169a9f320b3bb6e98120f3a",
        "title": "MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems"
      },
      {
        "paperId": "4cba3bd7a32f7ef1ab33383dc02e79dee8cb69fe",
        "title": "Adaptive Margin RLHF via Preference over Preferences"
      },
      {
        "paperId": "5ef246ec5941eeb61e532e0d8b9bdf18617a8222",
        "title": "Review of Hallucination Understanding in Large Language and Vision Models"
      },
      {
        "paperId": "89d01a6cd69161b1fba09dc4a2bed062497e4000",
        "title": "Failure Modes of Maximum Entropy RLHF"
      },
      {
        "paperId": "6aebe394b781205ec9accd634e9395b7c21c1310",
        "title": "The Alignment Bottleneck"
      },
      {
        "paperId": "28af78801ba76122e047a8f8df37061b957e1b45",
        "title": "Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs"
      },
      {
        "paperId": "ae11024cb18ee5444d8c7c950d42a111735aed72",
        "title": "MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning"
      },
      {
        "paperId": "265fdd781ac5f96f0e6ae37fbca0a370ee7c1b26",
        "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions"
      },
      {
        "paperId": "a6a5e2d4319e2fa9bf780a9b28f71677de4baaf6",
        "title": "Aligning Spoken Dialogue Models from User Interactions"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "b76d2d4da4b70489d85cd1fbf2ff3a7ebbadc578",
        "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms"
      },
      {
        "paperId": "7985e019694318d12c52cc03ee2adef36845059c",
        "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization"
      },
      {
        "paperId": "d5925e094acf86f5bd1b8169d21808afbc78c752",
        "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling"
      },
      {
        "paperId": "416908dee133533bc8a08f7507718c5697190467",
        "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "f9593e4c247defe3e1e518f59848048c9bc19a8e",
        "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback"
      },
      {
        "paperId": "fea9cf41242aa4faf831d26b7570e501363ca65b",
        "title": "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective"
      },
      {
        "paperId": "121b2ea3770c1e5627846f06b58ada4d886a7f01",
        "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning"
      },
      {
        "paperId": "2edd56fd82749d0f0d987ee341d8caf95037b164",
        "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "961789d210238911b457c190e86adae816203a55",
        "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling"
      },
      {
        "paperId": "bdf6334fb878f90b71b325223cc904c3b7aad0c3",
        "title": "Reward Model Overoptimisation in Iterated RLHF"
      },
      {
        "paperId": "9f6ec75cbb930edecbeeef964f50cddb10abdb6e",
        "title": "Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation"
      },
      {
        "paperId": "5759a0e3a98496e7cce0a5a312aff3f4f491a2c4",
        "title": "Reward Reasoning Model"
      },
      {
        "paperId": "62529f40fcc21c10a90608bc2b6bbeb43b6a11db",
        "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization"
      },
      {
        "paperId": "0467ea5c1751f55693840be65a46573fe1c9d5eb",
        "title": "On the Robustness of Reward Models for Language Model Alignment"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "f901c812e41109f84168a5d29b080514795ae802",
        "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models"
      },
      {
        "paperId": "43908f36b802ab4422a7e7c995aa40f85906a10f",
        "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations"
      },
      {
        "paperId": "8557dadfd5d4827f3788fed34a01c3d59654d6f1",
        "title": "Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators"
      },
      {
        "paperId": "24519912c263a84321d5bf1126eddc24a2e1863e",
        "title": "Aligning Multimodal LLM with Human Preference: A Survey"
      },
      {
        "paperId": "5c33a1dade777d08f3d4ba8a761a4902dafd211a",
        "title": "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation"
      },
      {
        "paperId": "038a85b0eee75ac361869df8eadf0b711f9370e7",
        "title": "RePO: ReLU-based Preference Optimization"
      },
      {
        "paperId": "ed034fff0b46b7b375befb284f3591b022e38def",
        "title": "Robust Multi-Objective Preference Alignment with Online DPO"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "986839c6cd0f3cbd3dbb68b5376ea4c9c1d58765",
        "title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time"
      },
      {
        "paperId": "a2e08a2fce333a7c8961103fe022bb9b334030cf",
        "title": "Aligning Compound AI Systems via System-level DPO"
      },
      {
        "paperId": "29ea9d80a13ee9387235723fcd9e61d700f9e969",
        "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs"
      },
      {
        "paperId": "d885b445226dc2979ad9d4f2a67f05c4ef18045f",
        "title": "KL Penalty Control via Perturbation for Direct Preference Optimization"
      },
      {
        "paperId": "c6e8297646a161facbd4c5ca708df87dfd79a607",
        "title": "Preference learning made easy: Everything should be understood through win rate"
      },
      {
        "paperId": "15378f453102f627467d7b7603d270438dea1237",
        "title": "Design Considerations in Offline Preference-based RL"
      },
      {
        "paperId": "9b4a577a59df2ad605d4b6f4ddb5ed6522660604",
        "title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models"
      },
      {
        "paperId": "3820dca7d376347187c0ca64a66ae6bf2417c846",
        "title": "The Differences Between Direct Alignment Algorithms are a Blur"
      },
      {
        "paperId": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking"
      },
      {
        "paperId": "8d6411e337502f7fe0bfa59d486803a73d2c1192",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "paperId": "a652026eb912f940c56af1e5f9b094583ccc083c",
        "title": "AlphaPO: Reward Shape Matters for LLM Alignment"
      },
      {
        "paperId": "fb59ade3eea4cef206284f693938bb32cdae296b",
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization"
      },
      {
        "paperId": "6ef32591af268d810c3e9272921803d1f4098467",
        "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?"
      },
      {
        "paperId": "5d1aea91e8cdb37a3f23f566ee095c0f007f02ad",
        "title": "Drowning in Documents: Consequences of Scaling Reranker Inference"
      },
      {
        "paperId": "f3f5b04dc152c1b5077ed3ec1cf009771b7720be",
        "title": "Efficient Alignment of Large Language Models via Data Sampling"
      },
      {
        "paperId": "f38741dc7fde4185d07df3316d00ad7784786b2d",
        "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval"
      },
      {
        "paperId": "b38c17d4c1cf719f110e811190255428fe7a53aa",
        "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks"
      },
      {
        "paperId": "a38ab0d55aea5afd931a6054a6cfd2bb7916cedc",
        "title": "Process Supervision-Guided Policy Optimization for Code Generation"
      },
      {
        "paperId": "2bbfe484ea3c38619791c10c5a30142dbea12fd0",
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms"
      },
      {
        "paperId": "8b4faf263cebc2a05e0dd3efaf21b24572c615b2",
        "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective"
      },
      {
        "paperId": "f28a570b7509eef12951787f58e777ecf0d46b91",
        "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization"
      },
      {
        "paperId": "efae534e2a3e4b79fe4050fa3fd40d8c7ba9745b",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      },
      {
        "paperId": "fa2a637f6532562a9eff1f5e9fef4438aae3f28b",
        "title": "Scaling Laws for Predicting Downstream Performance in LLMs"
      },
      {
        "paperId": "9fbe8cb6598da6438a8a47b7ca12f04d85675260",
        "title": "CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback"
      },
      {
        "paperId": "e35b1f39b6f05a19557feffc7d0972cf99ae1313",
        "title": "Strong Preferences Affect the Robustness of Preference Models and Value Alignment"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "6a686fb1a1f30d01eedf7cbf6f050041da556a66",
        "title": "Evaluating Robustness of Reward Models for Mathematical Reasoning"
      },
      {
        "paperId": "59eed4c468846a4d45105a4603dabf72e2bef830",
        "title": "Generative Reward Models"
      },
      {
        "paperId": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
        "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "f9b6af0ef6393c73c32ffe1122b28634a8fb6367",
        "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment"
      },
      {
        "paperId": "39fd3d41f5ab882eea29dbe27eef8d0954b29856",
        "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment"
      },
      {
        "paperId": "2057869daa0b3f88b2b4ee3e7b5413d880357f35",
        "title": "LIONs: An Empirically Optimized Approach to Align Language Models"
      },
      {
        "paperId": "13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      },
      {
        "paperId": "58636fabd0e7f9a4921732276f240f53c4818052",
        "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding"
      },
      {
        "paperId": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "e96c41933f4b1f0e79449451395e6ed0e6644133",
        "title": "Preference Alignment with Flow Matching"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "370fb62e60f80081015d591f8c10c5a59a56a32d",
        "title": "Learn Your Reference Model for Real Good Alignment"
      },
      {
        "paperId": "b190697d8106a555f525acec33c6a91c67b88483",
        "title": "The Clever Hans Mirage: A Comprehensive Survey on Spurious Correlations in Machine Learning"
      },
      {
        "paperId": "9c119a51cd110907ed94d9fb38704077b293e7c8",
        "title": "Handle With Care! A Mechanistic Case Study of DPO Out-of-Distribution Extrapolation"
      },
      {
        "paperId": "64353265c2b38b81e392572fec3e467a50587cef",
        "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization"
      },
      {
        "paperId": "c78ba55f8bad0c9b8b02e472153a7fa801e77ecf",
        "title": "M ITIGATING REWARD OVER - OPTIMIZAATION IN D I - RECT A LIGNMENT A LGORITHMS WITH A DAPTIVE I M - PORTANCE S AMPLING"
      }
    ],
    "score": 84.0
  },
  {
    "id": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
    "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
    "authors": [
      "Yufei Wang",
      "Zhanyi Sun",
      "Jesse Zhang",
      "Zhou Xian",
      "Erdem Biyik",
      "David Held",
      "Zackory Erickson"
    ],
    "year": 2024,
    "citationCount": 81,
    "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
    "url": "https://www.semanticscholar.org/paper/550006bea81e4ccb67743dd1b82a70b86b48d93a",
    "pdf_url": "https://arxiv.org/pdf/2402.03681.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-06",
    "externalIds": {
      "ArXiv": "2402.03681",
      "DBLP": "journals/corr/abs-2402-03681",
      "DOI": "10.48550/arXiv.2402.03681",
      "CorpusId": 267499679
    },
    "references": [
      {
        "paperId": "fa6835a3a7c1f1edf229b5030410769f7934bfdf",
        "title": "LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers"
      },
      {
        "paperId": "bfae03cba791a429821cee6bb6a8acb8edf85616",
        "title": "Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans"
      },
      {
        "paperId": "c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"
      },
      {
        "paperId": "fb09b581589e1195ff018179c6a11668587c6d64",
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning"
      },
      {
        "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
      },
      {
        "paperId": "8c9e95f32982ca21a7e4ef9c986aaa934cb11293",
        "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies"
      },
      {
        "paperId": "c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
      },
      {
        "paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c",
        "title": "Language to Rewards for Robotic Skill Synthesis"
      },
      {
        "paperId": "f69f95835deec7748a688675721b6d581b60d42b",
        "title": "LIV: Language-Image Representations and Rewards for Robotic Control"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "fc918d6f8e2523696c34fa1be5aabdb42e9648d2",
        "title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "203a1c0e5025489a52c030adbbc102a787685ee6",
        "title": "Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity"
      },
      {
        "paperId": "d28b9f65c849eba9ba2b27f7e91906f46fbe7fa1",
        "title": "Human-to-Robot Imitation in the Wild"
      },
      {
        "paperId": "32c9b3859086d15184989454eb878638659e64c6",
        "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"
      },
      {
        "paperId": "3f6d6ed110f3262fdc194184c54dd63701b3bca9",
        "title": "Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?"
      },
      {
        "paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      },
      {
        "paperId": "51965de80f86432d42749427db1e5bb0fa1e204c",
        "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "3fb50a3c764d0a800af529ff9b8e359f59b1fcc2",
        "title": "Learning Multimodal Rewards from Rankings"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "10dd4db543a4689aff533427e0f7e83ba68c808f",
        "title": "SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation"
      },
      {
        "paperId": "2b39687dad82d15b7a57c1f68c54115c5cbb07b9",
        "title": "f-IRL: Inverse Reinforcement Learning via State Marginal Matching"
      },
      {
        "paperId": "e5fc53230b7abced54d83058286dfff7f631289d",
        "title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "8cc77d98ea62a4ad0515e74dcd2635a0d7b338d3",
        "title": "Asking Easy Questions: A User-Friendly Approach to Active Reward Learning"
      },
      {
        "paperId": "9311779489e597315488749ee6c386bfa3f3512e",
        "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "36f7f407bbad234929c69c0dd3bdcfcd80298c7c",
        "title": "Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "815aa52cfc02961d82415f080384594639a21984",
        "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "c52b30a60fda5f23cc0d2241c4e127f5191bbb2d",
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"
      },
      {
        "paperId": "a09560239e398fe8aea05856823b46219a7dc539",
        "title": "Zero-Shot Reward Specification via Grounded Natural Language"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": null,
        "title": "A method for stochastic optimization"
      },
      {
        "paperId": "d4415b1b0279ae9986869851b69258543a07cdb1",
        "title": "Theory and Application of Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Accel-erating reinforcement learning of robotic manipulations via feedback from large language models"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "Blip-2: Boot-strapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "paperId": null,
        "title": "Language reward modulation for pretraining"
      },
      {
        "paperId": null,
        "title": "Reinforcement Learning from Vision Language Foundation Model Feedback"
      }
    ],
    "cited_by": [
      {
        "paperId": "b5eeff55c73861fec39e6b02019dd76a32bb698c",
        "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play"
      },
      {
        "paperId": "c024b621d8fbb4c489d6a0c6980e647489c1a509",
        "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation"
      },
      {
        "paperId": "30255f6a3541a6240d7b5d2b25923247c879c649",
        "title": "LAGEA: Language Guided Embodied Agents for Robotic Manipulation"
      },
      {
        "paperId": "df51d8936e440829ef52a668176c98ca8e51db8b",
        "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning"
      },
      {
        "paperId": "2f09aebc780958b66c203cfb36ce735d62441b15",
        "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning"
      },
      {
        "paperId": "def0655b1d0d434442b3bbcb8475d4cc6e750718",
        "title": "How well can LLMs provide planning feedback in grounded environments?"
      },
      {
        "paperId": "fa18d1031a04ecd0cf4eaae9123e82909e9c3cba",
        "title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization"
      },
      {
        "paperId": "012d0d12426f5d098992c3a466b15613e01ed52c",
        "title": "Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception"
      },
      {
        "paperId": "d03b8607ba3cbf91075793af08be8d73ab1b8169",
        "title": "In-Context Iterative Policy Improvement for Dynamic Manipulation"
      },
      {
        "paperId": "4650d8f503d7078988ffa9eaec691f0d2e20698b",
        "title": "Task-Context-Aware Diffusion Policy with Language Guidance for Multi-task Disassembly"
      },
      {
        "paperId": "1789ab7feee000d5f9c9dc3fd55d9a89dea34891",
        "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"
      },
      {
        "paperId": "3f7f25ea2af8d72693308b26f016e0e61eaf71f1",
        "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"
      },
      {
        "paperId": "2c26f5223f75eaac6a0b939bf402a6f36a54001d",
        "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks"
      },
      {
        "paperId": "27a3089b808f442bcff957037166ac6c8204d7ee",
        "title": "HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation"
      },
      {
        "paperId": "ff0f7c80c9ecabf6986884b918626d431b9b6778",
        "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling"
      },
      {
        "paperId": "0d426b4dd26dc2baaef545eeb445136d612bc6b7",
        "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities"
      },
      {
        "paperId": "25caf432faf629085f4db9c9614086a57ddd233b",
        "title": "Training-free Generation of Temporally Consistent Rewards from VLMs"
      },
      {
        "paperId": "507dd245030099afc1f5a416ac5a9547837e4cbe",
        "title": "Scaffolding Dexterous Manipulation with Vision-Language Models"
      },
      {
        "paperId": "d8f8562ce63d9176d3e30db46e208c25ef6932a5",
        "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models"
      },
      {
        "paperId": "8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
        "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models"
      },
      {
        "paperId": "bcba3b56a280f70379bc55fae8aee0df168bad46",
        "title": "ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving"
      },
      {
        "paperId": "efc028b1a397eeb46eb6b945faea536d068b752a",
        "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning"
      },
      {
        "paperId": "a2046b77adfc5c57551e0e62b702389496b6223c",
        "title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction"
      },
      {
        "paperId": "de6eee20fc95c31d18b72a914f689820b095640a",
        "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "206f07a961b480948bb3839a85f6897f3535e019",
        "title": "TREND: Tri-Teaching for Robust Preference-based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "33a7bb26825560a0b13c955152ff14f27979c026",
        "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making"
      },
      {
        "paperId": "44bd8a38d97cda8cebeb71e0a6848868993069e2",
        "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations"
      },
      {
        "paperId": "8e51cf726308232369db364477c776f66c9f91e8",
        "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation"
      },
      {
        "paperId": "764d8db2430d50b3ad7ba86fa0efd281c0003b47",
        "title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "b625d9bf38268f245e29b9406c7ce182d66d577f",
        "title": "AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models"
      },
      {
        "paperId": "2dd988da09f644bac28e4e6fe1a9108b9f030c96",
        "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning"
      },
      {
        "paperId": "625ea7a6a13e3438c3387d7742405d2aa7489b21",
        "title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models"
      },
      {
        "paperId": "eafd292ee60cc833eee3a02c943e509f9ec46221",
        "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences"
      },
      {
        "paperId": "c85ecb7e84f73b4d1043c6860c0f39d5e344affa",
        "title": "PANDORA: Diffusion Policy Learning for Dexterous Robotic Piano Playing"
      },
      {
        "paperId": "1e934631aeb726b5cd2a68d02a88f98c9e60ce71",
        "title": "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models"
      },
      {
        "paperId": "3c9e60f9bd36ce0922f9bd95cc41dbe5f2af8ca7",
        "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "423c6f1180ed65500daabe6f5998300d1bae2293",
        "title": "LuciBot: Automated Robot Policy Learning from Generated Videos"
      },
      {
        "paperId": "e12e43beff1e712a6f6d453ecebdf1491bf29b7a",
        "title": "Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models"
      },
      {
        "paperId": "bc53729436473d7c62d850426f7956097ea69c10",
        "title": "SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models"
      },
      {
        "paperId": "af6e9cef7efb18a20a16f585fc27c26dbcb9483c",
        "title": "Offline RLAIF: Piloting VLM Feedback for RL via SFO"
      },
      {
        "paperId": "1c3e59862cb82b78c6fb534fd2c5dc899216ca26",
        "title": "VLP: Vision-Language Preference Learning for Embodied Manipulation"
      },
      {
        "paperId": "f7fda04c653b44b043735cb4a3c108c84131e067",
        "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation"
      },
      {
        "paperId": "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
        "title": "Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "93b08946893d111933cd90b4a476d8da26e1f229",
        "title": "A Review of Embodied Grasping"
      },
      {
        "paperId": "4fa43bd231676d5ae94430755f4f559441297e38",
        "title": "Sample-Efficient Behavior Cloning Using General Domain Knowledge"
      },
      {
        "paperId": "e08c4d38cd851c38784a386832524cb7cb46f858",
        "title": "FDPP: Fine-Tune Diffusion Policy with Human Preference"
      },
      {
        "paperId": "6ce659d5f52bc26e2f3aed6d867ce48c30f298fd",
        "title": "Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation"
      },
      {
        "paperId": "58a5a16fbad1eb32490f71c8b0b256c5c7c29860",
        "title": "Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model"
      },
      {
        "paperId": "9014ac0d02ab76f1cae17b3bd368428e15b42e9e",
        "title": "MAPLE: A Framework for Active Preference Learning Guided by Large Language Models"
      },
      {
        "paperId": "83385a6ff5c3b311a7366dfe87fa0c87159224a7",
        "title": "SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation"
      },
      {
        "paperId": "f984a4d1f073223801a694200788a1a40d25ea8f",
        "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics"
      },
      {
        "paperId": "0bfcb7799bf3529ae142d9681285a4f39d88d513",
        "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble"
      },
      {
        "paperId": "37e1c757eca7903729b14f58e31910cb163deefc",
        "title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation"
      },
      {
        "paperId": "4749559e87d6a301a2ac61767d48ff4437fe4fd8",
        "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback"
      },
      {
        "paperId": "7712b0e640848cd67d34d10f0b8637ba89f39f0c",
        "title": "Vision Language Models are In-Context Value Learners"
      },
      {
        "paperId": "0111e51679678f81eafa0ce064b578f284eb8e59",
        "title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models"
      },
      {
        "paperId": "1ed6f9c35bc23965d5451ba39f197fefbe20710f",
        "title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos"
      },
      {
        "paperId": "c94512390c31942a73cbc152dfc7ab0db4ec1e6b",
        "title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making"
      },
      {
        "paperId": "25052b2594e9fb1edc6ff056d3f887983606a8ac",
        "title": "Trajectory Improvement and Reward Learning from Comparative Language Feedback"
      },
      {
        "paperId": "a35076b46fc0e7d4dfa63b538ffa7489b106a738",
        "title": "SELU: Self-Learning Embodied MLLMs in Unknown Environments"
      },
      {
        "paperId": "9ebce30da5743f37d5b6378ed6c0d8231b114d7e",
        "title": "MotIF: Motion Instruction Fine-Tuning"
      },
      {
        "paperId": "762542801daf04c052902bcac843f56b4fd72afa",
        "title": "Real-Time Interactive Capabilities of Dual-Arm Systems for Humanoid Robots in Unstructured Environments"
      },
      {
        "paperId": "205c3352ec73278e737806849323b37b36e1de5d",
        "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning"
      },
      {
        "paperId": "e344313490dc93bacf721c19f0b74ae921ac4285",
        "title": "Reward Models in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "df9b2b471b9b2550a29b2fd2df6b9d5967f9133f",
        "title": "Affordance-Guided Reinforcement Learning via Visual Prompting"
      },
      {
        "paperId": "528ad1f3e7b6ff4f61ff05ab5d2153ef6cc301ae",
        "title": "Revisiting Sparse Rewards for Goal-Reaching Reinforcement Learning"
      },
      {
        "paperId": "60ba3b7dc5c7e9e74997854c4f7f4b4a3a6663a9",
        "title": "Manipulate-Anything: Automating Real-World Robots using Vision-Language Models"
      },
      {
        "paperId": "3565ed1d884fcd6b7d20f9d410c6bbb871f90b0c",
        "title": "EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data"
      },
      {
        "paperId": "eb87604ee57fee65ad354961ca8212ba1bb55fe2",
        "title": "GPT-Fabric: Folding and Smoothing Fabric by Leveraging Pre-Trained Foundation Models"
      },
      {
        "paperId": "d18554cfa740cc9fe7cafaa2b2d578f4512ca18e",
        "title": "Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance"
      },
      {
        "paperId": "ffcef91b1c25d2d740b335f9b4b3c942cd8d79de",
        "title": "Enhancing Q-Learning with Large Language Model Heuristics"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "0338dabad410f2cd01696078ba1d13dec10b0800",
        "title": "A Generalized Acquisition Function for Preference-based Reward Learning"
      },
      {
        "paperId": "fa8fa745f58d362925dd44f02750bab1b30a1189",
        "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy"
      },
      {
        "paperId": "0a543cc850645c09eecc1dff4cc77e1ffbba0abb",
        "title": "Batch Active Learning of Reward Functions from Human Preferences"
      },
      {
        "paperId": "34eea97d5a4dfbc96b37dfe20b4045dc7b6eba6f",
        "title": "Optimizing Robot Behavior via Comparative Language Feedback"
      },
      {
        "paperId": "a248b0f89762f0427de4c60fae983af15aa2cabf",
        "title": "FDPP: Fine-tune Di\ufb00usion Policy with Human Preference"
      },
      {
        "paperId": "226cb3a6b51707a9eeb1c5f2176fdcecc8ff272b",
        "title": "Reinforcement Learning from Human Text Feedback: Learning a Reward Model from Human Text Input"
      },
      {
        "paperId": "664a3576bc06196410206a369f7cb21ed50c69a8",
        "title": "Time Your Rewards: Learning Temporally Consistent Rewards from a Single Video Demonstration"
      },
      {
        "paperId": "4c300039c3c40859b907469f3e85d1a06ce65f80",
        "title": "SELU: Self-Learning Embodied Multimodal Large Language Models in Unknown Environments"
      }
    ],
    "score": 81.0
  },
  {
    "id": "ec97a1565dff9d2fab1ef489e47296bbef68b680",
    "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
    "authors": [
      "Kai Yang",
      "Jian Tao",
      "Jiafei Lyu",
      "Chunjiang Ge",
      "Jiaxin Chen",
      "Qimai Li",
      "Weihan Shen",
      "Xiaolong Zhu",
      "Xiu Li"
    ],
    "year": 2023,
    "citationCount": 143,
    "abstract": "Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for De-noising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal re-ward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.",
    "url": "https://www.semanticscholar.org/paper/ec97a1565dff9d2fab1ef489e47296bbef68b680",
    "pdf_url": "https://arxiv.org/pdf/2311.13231.pdf",
    "venue": "Computer Vision and Pattern Recognition",
    "publicationDate": "2023-11-22",
    "externalIds": {
      "DBLP": "conf/cvpr/YangTLGCSZL24",
      "ArXiv": "2311.13231",
      "DOI": "10.1109/CVPR52733.2024.00854",
      "CorpusId": 265352082
    },
    "references": [
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a553bf27d801d09f667fe121c0ba9632257f364b",
        "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"
      },
      {
        "paperId": "5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb",
        "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor"
      },
      {
        "paperId": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d",
        "title": "Training Diffusion Models with Reinforcement Learning"
      },
      {
        "paperId": "1b2355c3c674b26a977768a91a164384ad51bbb1",
        "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation"
      },
      {
        "paperId": "bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "3ac2d89388a816786234aa9f8ef2de9a635b0a69",
        "title": "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC"
      },
      {
        "paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add",
        "title": "Adding Conditional Control to Text-to-Image Diffusion Models"
      },
      {
        "paperId": "c5434eef64f3275d821ba24bfa3818bfc10649fb",
        "title": "Optimizing DDPM Sampling with Shortcut Fine-Tuning"
      },
      {
        "paperId": "6d45e9a52301a123b3e0c5fefc1d20c33c6d1838",
        "title": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation"
      },
      {
        "paperId": "f19dfc360088922cf1d423c538662aae8d542c28",
        "title": "Is Conditional Generative Modeling all you need for Decision-Making?"
      },
      {
        "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
      },
      {
        "paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "title": "Imagen Video: High Definition Video Generation with Diffusion Models"
      },
      {
        "paperId": "1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
        "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data"
      },
      {
        "paperId": "af9f365ed86614c800f082bd8eb14be76072ad16",
        "title": "Classifier-Free Diffusion Guidance"
      },
      {
        "paperId": "3ff7153fd6bd47d08084c7f50f8fd70026c126e7",
        "title": "Compositional Visual Generation with Composable Diffusion Models"
      },
      {
        "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
      },
      {
        "paperId": "3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e",
        "title": "Planning with Diffusion for Flexible Behavior Synthesis"
      },
      {
        "paperId": "75bb9eda70751c63fc54dbe63377c673b7dbdb15",
        "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"
      },
      {
        "paperId": "c57293882b2561e1ba03017902df9fc2f289dea2",
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "15e234a67f30d6761f1d7670d501095d1697b69c",
        "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors"
      },
      {
        "paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99",
        "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
      },
      {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
        "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
        "title": "CogView: Mastering Text-to-Image Generation via Transformers"
      },
      {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "title": "Zero-Shot Text-to-Image Generation"
      },
      {
        "paperId": "3a5ac09e759f3223ee78b995ae2b519efc0f9292",
        "title": "Introduction to Reinforcement Learning"
      },
      {
        "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
        "title": "Taming Transformers for High-Resolution Image Synthesis"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "14fdc18d9c164e5b0d6d946b3238c04e81921358",
        "title": "Analyzing and Improving the Image Quality of StyleGAN"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "2ee463bba9d4db6aec0eab17e54431a6dc80bf17",
        "title": "Superhuman AI for multiplayer poker"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
        "title": "Density estimation using Real NVP"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474",
        "title": "Pixel Recurrent Neural Networks"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "469a7d19c074c0d2df699340cbd5b105bdd0f7e6",
        "title": "Data Generation as Sequential Decision Making"
      },
      {
        "paperId": "0f899b92b7fb03b609fee887e4b6f3b633eaf30d",
        "title": "Variational Inference with Normalizing Flows"
      },
      {
        "paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "cc2e61e06917238ca745155d9f1f0e679e63866e",
        "title": "Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm"
      },
      {
        "paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "8c2ed19db86875489a5f661ef68b6ba91ad15a94",
        "title": "Zero-shot Preference Learning for Offline RL via Optimal Transport"
      },
      {
        "paperId": "356770d13ad2e7b60961e5bc7368ffd3b9a2bcd9",
        "title": "Learning to summarize with human feedback"
      },
      {
        "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
        "title": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "paperId": null,
        "title": "A method for stochastic optimization"
      },
      {
        "paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce",
        "title": "google,\u6211,\u8428\u5a1c"
      },
      {
        "paperId": null,
        "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model"
      },
      {
        "paperId": null,
        "title": "Direct Authorized licensed use limited to the terms of the"
      },
      {
        "paperId": null,
        "title": "Introducing claude, 2023. 3"
      }
    ],
    "cited_by": [
      {
        "paperId": "27f6531af74b0aee04a572884d908421f03881ad",
        "title": "Towards Better Optimization For Listwise Preference in Diffusion Models"
      },
      {
        "paperId": "34e6cc059b06730d1bcfe6afa2997bb8c5154969",
        "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models"
      },
      {
        "paperId": "2675882539acfa91336591eb1cecca015e1d1c0f",
        "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs"
      },
      {
        "paperId": "7ae8684ce048e14d2694f457df0f1aff01085a3b",
        "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models"
      },
      {
        "paperId": "052e8e4a9cfd766aa07550a10778514c54e3dba6",
        "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"
      },
      {
        "paperId": "bbf7c2d4bc565a03a70d6aa1d6a3de6f99811ddc",
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning"
      },
      {
        "paperId": "4d7ac4cc98e25d366be3d4781ad29e7813d412b1",
        "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation"
      },
      {
        "paperId": "96b7b18af643fffceeac93b739d3418bd3e42645",
        "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process"
      },
      {
        "paperId": "abf03d59a76ce15934c0beea918af4007efc5de0",
        "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making"
      },
      {
        "paperId": "2affa73c5527e5e0cc8ffa378744f1f9c65dbf1e",
        "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching"
      },
      {
        "paperId": "cf8059caf2de34b1bb102af8652cf554f90799a2",
        "title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models"
      },
      {
        "paperId": "e7197f0ff2e60c94c8009e1c9b0885be6e2b1c2e",
        "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning"
      },
      {
        "paperId": "52ebc607cccf53d29ec5648c9c5322a0acf04378",
        "title": "Composition and Alignment of Diffusion Models using Constrained Learning"
      },
      {
        "paperId": "13e1db69ecd32b75df9632abb7dda7c53b3386b8",
        "title": "Learning User Preferences for Image Generation Model"
      },
      {
        "paperId": "701fa19a19f9996f5d322e13e8c7427324338a06",
        "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models"
      },
      {
        "paperId": "1a41a24220a754e1188fdde478ff231154515ddb",
        "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models"
      },
      {
        "paperId": "a471394a503970eb28da2ce64c4d1473844bdda6",
        "title": "The Promise of RL for Autoregressive Image Editing"
      },
      {
        "paperId": "36814c92eb3238915f9150ceb272b0926d8cfd9f",
        "title": "Unlocking compositional potential: Leveraging specific feedback for text-to-image generation in diffusion models"
      },
      {
        "paperId": "b214e7ca8ef5b8ed762b54ee5debd2079122eb69",
        "title": "ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning"
      },
      {
        "paperId": "594d82ab5ed8ac54c4419e5e10152ac54dcf2f60",
        "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again"
      },
      {
        "paperId": "e083eace1333ca8c67b297cf95e72a144d213ecd",
        "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models"
      },
      {
        "paperId": "5e2522694a4f18b4599b81f9a1596266a1c368c7",
        "title": "Divergence Minimization Preference Optimization for Diffusion Model Alignment"
      },
      {
        "paperId": "8fe4308c33e942ab60dbcbf6d625f74edd62dd38",
        "title": "Discrete Diffusion Trajectory Alignment via Stepwise Decomposition"
      },
      {
        "paperId": "bba9df17cf0452603e402f508da87ef311282c6b",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"
      },
      {
        "paperId": "8d038731d0f071a477d76a0fd69ceedf92223514",
        "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction"
      },
      {
        "paperId": "e7b92915a9cd82fbc31388834097d5fd3ecb48c9",
        "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback"
      },
      {
        "paperId": "0a75130ccd4fad87c22b0790f715bacb63a03ae9",
        "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning"
      },
      {
        "paperId": "04a6f3b98dc65d85d66e0eb0316e5cca90ca32b5",
        "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision"
      },
      {
        "paperId": "3efbf8a4c5d4b5346a33c47ddd4f9391bdf621e5",
        "title": "Scaling Inference Time Compute for Diffusion Models"
      },
      {
        "paperId": "8e70bd26dd94fe39dd2202a8183906b67b1f425b",
        "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models"
      },
      {
        "paperId": "6f9c589f96a1e3a7d37596e52ce390fc37e67e31",
        "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences"
      },
      {
        "paperId": "391ecc949737031af11c003b7571de41487d3258",
        "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF"
      },
      {
        "paperId": "f684cc249dda30ffd828db0fbee28c16c2b502c3",
        "title": "Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models"
      },
      {
        "paperId": "93bca0544166ee7fa0d32e6b5bf914227425a9bd",
        "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization"
      },
      {
        "paperId": "3198c08d67edbcb4c1f736d815fd7139ba7c3ce5",
        "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples"
      },
      {
        "paperId": "15143892b7e8cded24304c1077a9f7d20de363dd",
        "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training"
      },
      {
        "paperId": "1f1f4547d3e0c25eedf9c09c5ae528a47cc99ebc",
        "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning"
      },
      {
        "paperId": "acf2f14fbf6372470d6cc3bd7a8d17b54a00939d",
        "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning"
      },
      {
        "paperId": "b68a92107858b60b300e2a207fbe6804aa48f9b5",
        "title": "Rethinking Direct Preference Optimization in Diffusion Models"
      },
      {
        "paperId": "5464fd5bc6a3c8abdffcea21b29a3ed2aec57fa6",
        "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search"
      },
      {
        "paperId": "38605527f763a3782e117b96e109e0d8640a3236",
        "title": "I2G: Generating Instructional Illustrations via Text-Conditioned Diffusion"
      },
      {
        "paperId": "55fa94e0707f16353b66038b24c4bf52e7b54fab",
        "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations"
      },
      {
        "paperId": "003c35f00d036cc8b7e43d89b68a930f48aa55b9",
        "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization"
      },
      {
        "paperId": "018342d4d9bb8f4dbe1456f64f0511b6adc66bc1",
        "title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models"
      },
      {
        "paperId": "42e8182979113b4c25a77a2102c4980fe8df2cc4",
        "title": "Efficient Diffusion Models: A Comprehensive Survey From Principles to Practices"
      },
      {
        "paperId": "446b29b2894678ee4c5e869544487c522a0376de",
        "title": "How good are humans at detecting AI-generated images? Learnings from an experiment"
      },
      {
        "paperId": "16c38a5b7d7d8487c9e03e7231330445eb64e5b1",
        "title": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "paperId": "c173d62c1456389b2e04fd5093b6c38f61b1550e",
        "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing"
      },
      {
        "paperId": "7c5881743648a7dfa8ccbfb460ee8a27f358e473",
        "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "827127e9cef8d36a871d01a56637b5a9730ec870",
        "title": "Twin Co-Adaptive Dialogue for Progressive Image Generation"
      },
      {
        "paperId": "13504d5d6d34483587da1587fe0c9769a4942963",
        "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models"
      },
      {
        "paperId": "5b8e091a424698c5c68f45f193e3a0daeb73792a",
        "title": "SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization"
      },
      {
        "paperId": "9bc09d60d3ceecfd70c74c1c69a26de2bae2776d",
        "title": "SPIE: Semantic and Structural Post-Training of Image Editing Diffusion Models with AI feedback"
      },
      {
        "paperId": "1b4094f5c0aa073b4f1d50b731722474a716c4c6",
        "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis"
      },
      {
        "paperId": "b3567434da5f23aa743f26b68948738a4d41ea71",
        "title": "FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct Preference Optimization"
      },
      {
        "paperId": "e72efe88b16972f115f9dc85f0f2b165a9f4e6a9",
        "title": "SkyReels-V2: Infinite-length Film Generative Model"
      },
      {
        "paperId": "88345d7ce1016cf241694367e1d3b62d37db0a7e",
        "title": "Aligning Anime Video Generation with Human Feedback"
      },
      {
        "paperId": "6103f147ef9484c5f68cef3c96552a5353753b74",
        "title": "Discriminator-Free Direct Preference Optimization for Video Diffusion"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "132585bc5d529e1212f4668e521e50fa0dcde477",
        "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing"
      },
      {
        "paperId": "e091e10464d1eb9e2762213db9fcc1cc1128ff8c",
        "title": "IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient Prompt-level Fine-Tuning on Text-to-Image Models"
      },
      {
        "paperId": "e85fd23127a81950742d2ceca174d63f84dda2e9",
        "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment"
      },
      {
        "paperId": "154207c730b5beea69bbe9a338eba91d1b477a80",
        "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO"
      },
      {
        "paperId": "bcac8bfc09d088bde8ad37f631fc2270ef26a396",
        "title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models"
      },
      {
        "paperId": "2ca509423718504511b040d19b3d3b1e40b694a0",
        "title": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference Alignment"
      },
      {
        "paperId": "ed3640d224797c8d5f8441cfde10efcdd2adf6eb",
        "title": "Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image Generation"
      },
      {
        "paperId": "445e202eede8d02561643546591330cc1986dcc3",
        "title": "BalancedDPO: Adaptive Multi-Metric Alignment"
      },
      {
        "paperId": "f9f346a4e899e13a3da2d8aa60fb849503d27f33",
        "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection"
      },
      {
        "paperId": "e84015c5dbdf0366a0da00d7a7a074328e4efda0",
        "title": "Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "477be96c1a1208dabe30011a8f287ea2586906bf",
        "title": "D3PO: Preference-Based Alignment of Discrete Diffusion Models"
      },
      {
        "paperId": "4d41946f2f8d1fafa12edd284fde140455ccffff",
        "title": "CoPL: Collaborative Preference Learning for Personalizing LLMs"
      },
      {
        "paperId": "a855ac764ae1466ab81ecce02843968eb1a4ddc5",
        "title": "CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation"
      },
      {
        "paperId": "91b430cac8a484e3980a922425b6e0d37c0c6aea",
        "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening"
      },
      {
        "paperId": "5ffcf94ceb91c54d552125bc8f89bfc13b73a0f6",
        "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model"
      },
      {
        "paperId": "9aab5fd715ca150df855d6d2c13d530ceed200f0",
        "title": "Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic Scenario Generation"
      },
      {
        "paperId": "77f733475cb309a553083d7ef7f6557fdd201d0c",
        "title": "A First-order Generative Bilevel Optimization Framework for Diffusion Models"
      },
      {
        "paperId": "49f7b33fb59f7281018a5d29b8fc577e56a85b07",
        "title": "Dual Caption Preference Optimization for Diffusion Models"
      },
      {
        "paperId": "fe9edacbf58afc49b4c4ce4f731d5fc76008e282",
        "title": "Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization"
      },
      {
        "paperId": "6cdb43f5893dfa84ab54307451c107205c61c026",
        "title": "HuViDPO:Enhancing Video Generation through Direct Preference Optimization for Human-Centric Alignment"
      },
      {
        "paperId": "112b25d977e7f633b4d3647308b8f3d374f93a06",
        "title": "Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation"
      },
      {
        "paperId": "e662cd34cb20d3448de70bd84e8c12df8796b090",
        "title": "Half-order Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer"
      },
      {
        "paperId": "c6aaba1db14e72874258735277dd1a1427f6a920",
        "title": "Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference Ranking"
      },
      {
        "paperId": "21d0c6903b45c2a63b1ee9b6506ce0c4cfc808fc",
        "title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search"
      },
      {
        "paperId": "2bb2f4e3a55c19c66875ea0a84d409b6a2e16fce",
        "title": "Improving Video Generation with Human Feedback"
      },
      {
        "paperId": "4c6e25486f9d2a8ca93c24b9ab92535e64daac1f",
        "title": "Personalized Preference Fine-tuning of Diffusion Models"
      },
      {
        "paperId": "4aa43c023c3793e87d5e7de128d549ec3a8db045",
        "title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation"
      },
      {
        "paperId": "e1b3d89532f75755898f3a1176203b503a6b7a3b",
        "title": "SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization"
      },
      {
        "paperId": "f4249988be0e8e4d87ad18eaa751672da45aa940",
        "title": "DyMO: Training-Free Diffusion Model Alignment with Dynamic Multi-Objective Scheduling"
      },
      {
        "paperId": "db0cf3e124310633a97e836f259ff3a6f4cff7eb",
        "title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis"
      },
      {
        "paperId": "1898b422c5b24e0476fc06cb8193b9659e4c0b46",
        "title": "Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward"
      },
      {
        "paperId": "74bdaf7257fd6bab17285ec7a6546f0165450935",
        "title": "Preference Alignment for Diffusion Model via Explicit Denoised Distribution Estimation"
      },
      {
        "paperId": "be24a6e2e75d224fe61f174d5faf288fc60794d8",
        "title": "Aligning Few-Step Diffusion Models with Dense Reward Difference Learning"
      },
      {
        "paperId": "c759d61941a5b4f62531559ceac580e089965c55",
        "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization"
      },
      {
        "paperId": "6b04d1a6c631371ef737a64bc595abb992a56776",
        "title": "Controlling Language and Diffusion Models by Transporting Activations"
      },
      {
        "paperId": "1936eaf479432c9aaa2fb9dc1dc895b15e7fc355",
        "title": "PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference"
      },
      {
        "paperId": "1b1a137541805081f6fc3356347b5902e9e3e5fa",
        "title": "David and Goliath: Small One-step Model Beats Large Diffusion with Score Post-training"
      },
      {
        "paperId": "4aabb3802133c76c21be9e98e81cd43d020b0863",
        "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "49f95b100b4c24699b7fc1b714ed11a10be3854c",
        "title": "Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models"
      },
      {
        "paperId": "801384e657f7dbbc2c0fcb68577d3dd2bbdd5218",
        "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation"
      },
      {
        "paperId": "25609eb8a8cc9477ba6c654e40b03928f0a5c8af",
        "title": "Training-free Diffusion Model Alignment with Sampling Demons"
      },
      {
        "paperId": "deb5a2fd08d7951bac7b378445ea48b9c0789bdc",
        "title": "Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning"
      },
      {
        "paperId": "dd08724a78fbcf945b76307d36b89c53ffba2b31",
        "title": "Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization"
      },
      {
        "paperId": "275499681685a08462c761b373a35cf397f88871",
        "title": "Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization"
      },
      {
        "paperId": "61e3681ec697ba1128458b63d3cf25aeb877672d",
        "title": "Illustrious: an Open Advanced Illustration Model"
      },
      {
        "paperId": "809db82227bcc74322840c2b16851ec23da91006",
        "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with Preferences through f-divergence Minimization"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "d161db376ed8be91b5387144f28c11e961ab6adb",
        "title": "Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models"
      },
      {
        "paperId": "33c9147cfacb6ce401fe45b57307b8e3eeeb16d6",
        "title": "Forward KL Regularized Preference Optimization for Aligning Diffusion Policies"
      },
      {
        "paperId": "49adafa41a9a1a148be90d587ffc0562deea86a8",
        "title": "Reflective Human-Machine Co-adaptation for Enhanced Text-to-Image Generation Dialogue System"
      },
      {
        "paperId": "7e8d648a44b0e1856a40f7558ace4f6ee99e194c",
        "title": "BATON: Aligning Text-to-Audio Model Using Human Preference Feedback"
      },
      {
        "paperId": "8cee54df89130080164d7d71ee286e67c065300b",
        "title": "Towards Reliable Advertising Image Generation Using Human Feedback"
      },
      {
        "paperId": "4730772976f5bf46ff8fbccc41bf25da6a28c5c7",
        "title": "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models"
      },
      {
        "paperId": "3a6cab17104dd9a486898504e68d78fc6b6213e2",
        "title": "PopAlign: Population-Level Alignment for Fair Text-to-Image Generation"
      },
      {
        "paperId": "268e7df89189e13a4bf0dc4ebbf74cf057dd2082",
        "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference"
      },
      {
        "paperId": "502987c72f29cf1f6189406c49d125ffb04eb5d3",
        "title": "Aesthetic Post-Training Diffusion Models from Generic Preferences with Step-by-step Preference Optimization"
      },
      {
        "paperId": "9cdfd0a4b0cb7b975683c95a42360430b1bfb795",
        "title": "ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization"
      },
      {
        "paperId": "4da96a97a09dab8181c90a3cb195ee0ccb7e8601",
        "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment"
      },
      {
        "paperId": "e00ec17d1d30e94043c0622d0b9101feba3d4e57",
        "title": "Enhancing Zero-shot Text-to-Speech Synthesis with Human Feedback"
      },
      {
        "paperId": "182d4ed333a892cbcd44784dd90a717a3703fdce",
        "title": "Boost Your Human Image Generation Model via Direct Preference Optimization"
      },
      {
        "paperId": "8053f1079767d3d6cc6ffd90c885b513acc6af5f",
        "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models"
      },
      {
        "paperId": "8fdb8148966066ae4923f3348a15d64b3247c47b",
        "title": "Participation in the age of foundation models"
      },
      {
        "paperId": "45444fbaaaa63c76de8bc8e1558e96c6f8e6d55a",
        "title": "Multi-Player Approaches for Dueling Bandits"
      },
      {
        "paperId": "f7f71bad41e6b76c455dd7edf333c3831e766bcf",
        "title": "Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation"
      },
      {
        "paperId": "4070ff640465e6486d506d607fc2ff6fa52407d0",
        "title": "YaART: Yet Another ART Rendering Technology"
      },
      {
        "paperId": "3714bb3aac3f0429cd62e315da0ffb3f79655f98",
        "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"
      },
      {
        "paperId": "47434c384889d4d356925a99248678ab73188da8",
        "title": "Aligning Diffusion Models by Optimizing Human Utility"
      },
      {
        "paperId": "fbf78c26ff88a30aa8c1c899837c729fe6d088e7",
        "title": "DreamReward: Text-to-3D Generation with Human Preference"
      },
      {
        "paperId": "4639ef0478d1a337710ab1ccd8156f990c8968e8",
        "title": "Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives"
      },
      {
        "paperId": "6a225bb2454f66d1b106bbae7e5129d6569aa260",
        "title": "Feedback Efficient Online Fine-Tuning of Diffusion Models"
      },
      {
        "paperId": "613a32f18388958cc60dbb906d87fc7f206c0e66",
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation"
      },
      {
        "paperId": "f7c563c1da2301a8ef795082466d6e7aaef2f19b",
        "title": "DSPO: Direct Score Preference Optimization for Diffusion Model Alignment"
      },
      {
        "paperId": "6438ce52abdf73ad0aa4bf7264ddb6e60813ed07",
        "title": "DSFDcd: Joint Distribution Sampling and Feature Decoupling Deep Network for Remote Sensing Change Detection"
      },
      {
        "paperId": "dcccc64c577e26576fe309552b4b9352ea5a1402",
        "title": "Eye Tracking-Based Substitution of Human Feedback for Image Quality Assessment in Diffusion Models"
      },
      {
        "paperId": "e56754ab96aebf822f024b7880b86e00fe27b3af",
        "title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better"
      },
      {
        "paperId": "4a962f84f0131e293912601e5f4a80d94aeaaeaa",
        "title": "Towards Co-Designing a Continuous-Learning Human-AI Interface: A Case Study in Online Grooming Detection Peter Daish"
      },
      {
        "paperId": "fd2c9530e03db116b4ee0aaea1abbd7d44546b0d",
        "title": "Optimizing Synthetic Data from Scarcity: Towards Meaningful Data Generation in High-Dimensional Low-Sample Size Domains"
      },
      {
        "paperId": "e3064585104120ba0e2285deaadb7b8a9e049186",
        "title": "Boosting Text-to-Video Generative Model with MLLMs Feedback"
      },
      {
        "paperId": "279b7828dc7eeb43b46c32eba5d4aed75f8c6b19",
        "title": "A T AILORED F RAMEWORK FOR A LIGNING D IFFUSION M ODELS WITH"
      },
      {
        "paperId": "7e6fe0c44c78aebc34bd753098bcb0c9a68c237c",
        "title": "Diffusion-RainbowPA: Improvements Integrated Preference Alignment for Diffusion-based Text-to-Image Generation"
      },
      {
        "paperId": "a116c6ad1f137adb50c784ef32f0565aab11f869",
        "title": "D IFFUSION P REFERENCE A LIGNMENT VIA R ELATIVE T EXT -I MAGE C ONTRAST"
      },
      {
        "paperId": "f1a7243dafe9c5e2a3c16e01d713ed93fabbfc7e",
        "title": "EMI -P OLICY P REFERENCE O PTIMIZATION FOR"
      }
    ],
    "score": 71.5
  },
  {
    "id": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
    "authors": [
      "Souradip Chakraborty",
      "Jiahao Qiu",
      "Hui Yuan",
      "Alec Koppel",
      "Furong Huang",
      "Dinesh Manocha",
      "A. S. Bedi",
      "Mengdi Wang"
    ],
    "year": 2024,
    "citationCount": 67,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.",
    "url": "https://www.semanticscholar.org/paper/db32da8f3b075d566a73512f4ccc2c95449c75a1",
    "pdf_url": "https://arxiv.org/pdf/2402.08925.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-14",
    "externalIds": {
      "DBLP": "conf/icml/ChakrabortyQYKM24",
      "ArXiv": "2402.08925",
      "CorpusId": 267657954
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "5ef246ec5941eeb61e532e0d8b9bdf18617a8222",
        "title": "Review of Hallucination Understanding in Large Language and Vision Models"
      },
      {
        "paperId": "2aec8d65f6af046c83927716b52ed733a2134078",
        "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework"
      },
      {
        "paperId": "59c6403b4445fcdc4e0cb918cb7aa080206432ba",
        "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences"
      },
      {
        "paperId": "ec37ae28907638a87c8b229a296f57a6539e6838",
        "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design"
      },
      {
        "paperId": "1ab8fa5ee2fc77d9114d124d30017b7f8407a5c4",
        "title": "Active Query Selection for Crowd-Based Reinforcement Learning"
      },
      {
        "paperId": "051e7d6caaa4c4c05c57062e21e094a2fdff10cd",
        "title": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework"
      },
      {
        "paperId": "11f2ea50fbcbff167f55c2421c095806fdb5a344",
        "title": "An Architecture for Learning Context-based Value Systems"
      },
      {
        "paperId": "bd48ec72d9dcc42758e5a6d2663c1680e1cd73fb",
        "title": "Learning the Value Systems of Societies from Preferences"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "c4d65d124c3ceeb7369425954516b1d7c6ffc1ab",
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization"
      },
      {
        "paperId": "0625b0d9444036c5503ae0bdd8a83984f28fd268",
        "title": "Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach"
      },
      {
        "paperId": "488991dd1b2ec6ce47bacb10766d730d0a58fb4a",
        "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales"
      },
      {
        "paperId": "2c96c423d0cd481953e50ded37fb04921af02f5e",
        "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
      },
      {
        "paperId": "68d3f73f48805bdcef68d45cf4401dea6bac7f42",
        "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?"
      },
      {
        "paperId": "6eedb1cde02d4a5b94cc0d3e1a46de63d7aad404",
        "title": "Alignment of large language models with constrained learning"
      },
      {
        "paperId": "496e0476e3368cb522612dde39cb477292d3fd08",
        "title": "Toward Scientific Reasoning in LLMs: Training from Expert Discussions via Reinforcement Learning"
      },
      {
        "paperId": "6c8f155cafdb286e09abbed89cd7c70a550a8aef",
        "title": "Do Large Language Models (Really) Need Statistical Foundations?"
      },
      {
        "paperId": "eaeefb4cc4825f76cb18513a66a73d1fa3005500",
        "title": "AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing"
      },
      {
        "paperId": "8932cb2a36f98c263f5a0f1287a1e0303fc0e7df",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment"
      },
      {
        "paperId": "2c4ca86d4023c85cd5066873f1f889aeaaab9b28",
        "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?"
      },
      {
        "paperId": "acee74388ee2dc7e83e0e28fd20288796ab8cddb",
        "title": "Metric Distortion for Tournament Voting and Beyond"
      },
      {
        "paperId": "ac646c716cf44716731fe9ac6b3706f12d88e67e",
        "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models"
      },
      {
        "paperId": "a634d3d83d2254a078eeada6adf4b6f055d65087",
        "title": "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning"
      },
      {
        "paperId": "4f657dd99723932d6b4476372d56fe11b938d9a4",
        "title": "Strategyproof Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "feee95af39ecf8a4bc2d8f1d0ddbe53d622155f7",
        "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning"
      },
      {
        "paperId": "18b1c6181ce6d3ec9486d0183b62e523a04c098e",
        "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment"
      },
      {
        "paperId": "afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d",
        "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions"
      },
      {
        "paperId": "c167290e0bb97c6cc64bf90a199d25ab5b73c19f",
        "title": "Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning"
      },
      {
        "paperId": "117b52d94d38048aca36fc7432ac17742103e2e2",
        "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization"
      },
      {
        "paperId": "21057527ad9394688ecb60d2e69753b9761a8d52",
        "title": "What can large language models do for sustainable food?"
      },
      {
        "paperId": "af0bd618ed97579b044f7540e8c128832a52d5ee",
        "title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas"
      },
      {
        "paperId": "6ef32591af268d810c3e9272921803d1f4098467",
        "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?"
      },
      {
        "paperId": "ad9cc1c463faa60d91bfaa48c3b16df0b3719ffa",
        "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd"
      },
      {
        "paperId": "5f999f9c6745663e79b149a7d7dac39dab22081a",
        "title": "PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment"
      },
      {
        "paperId": "e4df2f3525a5b57837ca8225ccd14bd832615913",
        "title": "From Efficiency to Equity: Measuring Fairness in Preference Learning"
      },
      {
        "paperId": "29309bac83f5055c684789d297c69cf689d26144",
        "title": "On The Global Convergence Of Online RLHF With Neural Parametrization"
      },
      {
        "paperId": "3f062cfb7762e45f82cc703a5b093f4fc9c9a9f3",
        "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
      },
      {
        "paperId": "24e2ee1d0e7165d1f3fc355b80dee7f90a1315ea",
        "title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both"
      },
      {
        "paperId": "29fd49eaa4a90549e2e4693da751ae48f3937bbc",
        "title": "AIME: AI System Optimization via Multiple LLM Evaluators"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "9123ec44f0026e70f8398b904e97a4224866bb36",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      },
      {
        "paperId": "9c803f13a8d7e3029621c8363f07517293134227",
        "title": "Value Alignment from Unstructured Text"
      },
      {
        "paperId": "21cdb3aced0e15d8581f3f58ea836b4f379a06fa",
        "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law"
      },
      {
        "paperId": "69779edde7b072808d676e9fdc44a1b4ce565957",
        "title": "Learning From Crowdsourced Noisy Labels: A signal processing perspective"
      },
      {
        "paperId": "c98995860494439675006f5846faf6595840636d",
        "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?"
      },
      {
        "paperId": "b8f435d3b8202f1086be9d791857c20cb3a4a90a",
        "title": "Pareto-Optimal Learning from Preferences with Hidden Context"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "197a7751deb0646e468859a7379b032e48d4291f",
        "title": "One-Shot Safety Alignment for Large Language Models via Optimal Dualization"
      },
      {
        "paperId": "b36760782b4cfd9359c6d4a57c86e2630393b1a3",
        "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models"
      },
      {
        "paperId": "81f0bac7736ef2e4efe00c775ffd483a2ab086e7",
        "title": "Direct Preference Optimization With Unobserved Preference Heterogeneity"
      },
      {
        "paperId": "713ff427f83967d202082ee635f6e08631df5338",
        "title": "Fairness in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ad54cb8887c98cc823666402678d249225d4e9f0",
        "title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation"
      },
      {
        "paperId": "1544f956492b613b47efd9ac79c3987df373fb81",
        "title": "Aligning Language Models to Explicitly Handle Ambiguity"
      },
      {
        "paperId": "73c38b406a9ec6d7a4d93969b003387e72e45ce1",
        "title": "Scalable Interactive Machine Learning for Future Command and Control"
      },
      {
        "paperId": "4dcccc23c169293df73da1390c7af32ab47f3995",
        "title": "Personalized Language Modeling from Personalized Human Feedback"
      },
      {
        "paperId": "4989bead096e388cc79dccb6f0ee73f0ebdfd504",
        "title": "Crowd-PrefRL: Preference-Based Reward Learning from Crowds"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
        "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models"
      },
      {
        "paperId": "abe28351f96c1bc471ac7126685a4a31450c6ee4",
        "title": "PAD: Personalized Alignment at Decoding-Time"
      },
      {
        "paperId": "9d0edf173b18b98b20e94cbeae6b92a11c7ead7e",
        "title": "Position: Human-like reinforcement learning is facilitated by structured and expressive goals"
      },
      {
        "paperId": "4d7294c62913a1a2ce651212f8fbb20baf938227",
        "title": "STA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "db1f7706ed9cdca2a3f9f5572076dba87d8f673b",
        "title": "Do Large Language Models Need Statistical Foundations?"
      },
      {
        "paperId": "8f60d82c164de8003289383c596b1d8eaf65570e",
        "title": "Boat Does it Float? Improving Personalization in Preference Tuning"
      }
    ],
    "score": 67.0
  },
  {
    "id": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
    "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
    "authors": [
      "Yao Fu",
      "Litu Ou",
      "Mingyu Chen",
      "Yuhao Wan",
      "Hao-Chun Peng",
      "Tushar Khot"
    ],
    "year": 2023,
    "citationCount": 122,
    "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
    "url": "https://www.semanticscholar.org/paper/ea75117f34b168a20f2a4309ac2eb685ca6b1436",
    "pdf_url": "https://arxiv.org/pdf/2305.17306.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-05-26",
    "externalIds": {
      "ArXiv": "2305.17306",
      "DBLP": "journals/corr/abs-2305-17306",
      "DOI": "10.48550/arXiv.2305.17306",
      "CorpusId": 258959433
    },
    "references": [
      {
        "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
        "title": "Holistic Evaluation of Language Models"
      },
      {
        "paperId": "5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35",
        "title": "The False Promise of Imitating Proprietary LLMs"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
      },
      {
        "paperId": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
        "title": "Complexity-Based Prompting for Multi-Step Reasoning"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
        "title": "Training Compute-Optimal Large Language Models"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt"
      },
      {
        "paperId": null,
        "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm"
      },
      {
        "paperId": null,
        "title": "Koala: A dialogue model for academic research"
      },
      {
        "paperId": null,
        "title": "A suite of language and symbolic reasoning tasks consisting 6.5k problems within 23 sub-sets that are particularly suitable for testing chain-of-thought prompting"
      },
      {
        "paperId": null,
        "title": "GSM8k A widely used math reasoning datasets consisting of 8k problems that jointly test models\u2019 ability of arithmetic reasoning and composing math steps using language"
      },
      {
        "paperId": null,
        "title": "MMLU An evaluation suite of 15k problems within 57 subjects testing model\u2019s high-school and college-level knowledge and reasoning"
      },
      {
        "paperId": null,
        "title": "Google PaLM including PaLM, PaLM-2, and their instruction-tuned versions (FLan-PaLM and Flan-U-PaLM). Strong base and instruction-tuned models"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following llama model"
      },
      {
        "paperId": null,
        "title": "untested non-public models on some datasets. This is partly the reason we view our CoT Hub as a continuous effort"
      },
      {
        "paperId": null,
        "title": "Anthropic Claude including claude-v1.3 (slower but more capable) and claude-instant-v1.0 (faster but less capable) 6 ."
      },
      {
        "paperId": null,
        "title": "GSM8k performance because it is a classical benchmark testing models\u2019 reasoning capabilities. Numbers marked by an asterisk are tested by ourselves, others"
      },
      {
        "paperId": null,
        "title": "Meta LLaMA including the 7B, 13B, 33B and 65B variants."
      },
      {
        "paperId": null,
        "title": "How does gpt obtain its ability? tracing emergent"
      }
    ],
    "cited_by": [
      {
        "paperId": "326f147bcc81ca289f5115f425f6aabff5d93a40",
        "title": "Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA"
      },
      {
        "paperId": "4a631dd0d5d7ca63bd08c4055322443f51e0297b",
        "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap"
      },
      {
        "paperId": "93e0e493cca485a05a864cbfd7d471b9a5c80380",
        "title": "ProCut: LLM Prompt Compression via Attribution Estimation"
      },
      {
        "paperId": "321db610d22d09c0328f32ff1e81de43cb895f2a",
        "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning"
      },
      {
        "paperId": "3df19f73eb387e5336758933cf740e7d65078a97",
        "title": "How Alignment Shrinks the Generative Horizon"
      },
      {
        "paperId": "593a8358e3e6bd41d05f115aa4b248b89cd28d07",
        "title": "Unveiling Confirmation Bias in Chain-of-Thought Reasoning"
      },
      {
        "paperId": "ff3e9ccdd06203b5dd40273b65f4b393dab43849",
        "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing"
      },
      {
        "paperId": "57e959b74f36a30cd62d0abd4204f08907b42e87",
        "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning"
      },
      {
        "paperId": "1ae82d1d9790fd6d67cf5507e6f5e619ef68e037",
        "title": "Forecasting Communication Derailments Through Conversation Generation"
      },
      {
        "paperId": "0eb0484d19b8cbfeae73163fe9bff41089a985fa",
        "title": "Rethinking Reflection in Pre-Training"
      },
      {
        "paperId": "b56e8a4ef68b75401d422cf382662cc798c27a7e",
        "title": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation"
      },
      {
        "paperId": "871dbb6908121ede908f2c47c8f5f0707b67a0d1",
        "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats"
      },
      {
        "paperId": "d8b2a313d52b9caaccc5a38f13a6ffe038dac024",
        "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants"
      },
      {
        "paperId": "a519a158acd03983a22bdb9240b8885cbbce32be",
        "title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking"
      },
      {
        "paperId": "c3bd20c5794efdde989a7dfbbc29946a9550878d",
        "title": "Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment"
      },
      {
        "paperId": "6691dd7513ebd43f7951d36a38f35f900b629881",
        "title": "Detecting Cyberbullying in Visual Content: A Large Vision-Language Model Approach"
      },
      {
        "paperId": "90bf3c89ae55ff02d2ece7c2ad14971c6bf1323f",
        "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability"
      },
      {
        "paperId": "7c67389d4696fe012548499764d3e618db309082",
        "title": "Application of large language models in diagnostics and maintenance of aircraft propulsion systems"
      },
      {
        "paperId": "3512fe4ccde1512c29ed78fc2fc12eb659156713",
        "title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"
      },
      {
        "paperId": "62b85d34ad4772594afb93bb0b2dfd597cfd8b62",
        "title": "Review of the opportunities and challenges to accelerate mass\u2010scale application of smart grids with large\u2010language models"
      },
      {
        "paperId": "9eb31d882f036920467586ace4da519044a2dd44",
        "title": "LOGO - Long cOntext aliGnment via efficient preference Optimization"
      },
      {
        "paperId": "669313bf9d90f8a39a610507cca9cd55cae95da1",
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models"
      },
      {
        "paperId": "9e9351b4e89d4efb401612d268bd7937ba43e220",
        "title": "A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration"
      },
      {
        "paperId": "188b35043c43bc6ca032ae4234f101cf7eab952c",
        "title": "Bridging Today and the Future of Humanity: AI Safety in 2024 and Beyond"
      },
      {
        "paperId": "ff6f6ee7b62aa046021582786aac0f9396b99ef9",
        "title": "Intuitions of Compromise: Utilitarianism vs. Contractualism"
      },
      {
        "paperId": "6920cd2d28ed5194b51a7733b738d5e1ae79c6f5",
        "title": "LongGenBench: Long-context Generation Benchmark"
      },
      {
        "paperId": "c15ee85f84d37339afa71ff2408deed92b472e73",
        "title": "Parse Trees Guided LLM Prompt Compression"
      },
      {
        "paperId": "9b6c35db256e38496ad81c189783529ab4209960",
        "title": "Linguini: A benchmark for language-agnostic linguistic reasoning"
      },
      {
        "paperId": "fbf0e72bd11fc80d1873891850201ebaadd5ca8f",
        "title": "CogLM: Tracking Cognitive Development of Large Language Models"
      },
      {
        "paperId": "00764884538ef7e1ef736706dfe9d5c417527899",
        "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations"
      },
      {
        "paperId": "a77d28e13ff34f34b8d1114e603bff074ee2b056",
        "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost"
      },
      {
        "paperId": "022f386eb66fc5532dd6f439e7a356fd33ebb9a2",
        "title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference"
      },
      {
        "paperId": "cec7ebbaa7b32793659fd1a07d25892e95098615",
        "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data"
      },
      {
        "paperId": "5b465156130555b5230f9bef566276490b6ed9f7",
        "title": "Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step"
      },
      {
        "paperId": "813d9a7492719b47e8e932dc9c138daed8715295",
        "title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs"
      },
      {
        "paperId": "58636fabd0e7f9a4921732276f240f53c4818052",
        "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding"
      },
      {
        "paperId": "76c9e50188448c117d6b602e3463ffe9d5ef3bdf",
        "title": "FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation"
      },
      {
        "paperId": "00e88a0c006296e53bb7d4cfc90a134883ad34fd",
        "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners"
      },
      {
        "paperId": "727580d0354aa30094d5d1234fc23ca2b57cb228",
        "title": "Are Large Language Models Good Statisticians?"
      },
      {
        "paperId": "976f69e69a94535b88d6059550aa622ebb1c550f",
        "title": "Cycles of Thought: Measuring LLM Confidence through Stable Explanations"
      },
      {
        "paperId": "1406bb4cb6801bc4767b661308118c888a9b09da",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
      },
      {
        "paperId": "b7f9f6ac44fee822c692cdc1147c852a150f4aea",
        "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction"
      },
      {
        "paperId": "fbd25ef0b5b0cf1af57a63f5287139c09eb884bf",
        "title": "Response Performance Evaluations of ChatGPT Models on Large Language Model Frameworks"
      },
      {
        "paperId": "512a5c307fdab29112a0f4af5c94a3436632eda1",
        "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning"
      },
      {
        "paperId": "eeb3655b040d44a30751bfc92312353505f926bd",
        "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models"
      },
      {
        "paperId": "f91d83872a528c83e10e9884c06997d478888c60",
        "title": "FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models"
      },
      {
        "paperId": "557f8d2b41e1ddabffe6c30c65fe5b2bc5dda13d",
        "title": "TEL'M: Test and Evaluation of Language Models"
      },
      {
        "paperId": "63d2f26f1984b3952c1e7afb38629d6ef4db7ff8",
        "title": "On the Taxonomy of Developers\u2019 Discussion Topics with ChatGPT"
      },
      {
        "paperId": "aa3def6c0d6eef8183e9b78894a711ed9e092df6",
        "title": "Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward"
      },
      {
        "paperId": "3fb1818b9ab3f8d36ed3f8329087058f561c05da",
        "title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline"
      },
      {
        "paperId": "d0255ce4c897bea6578955de64fc40324f991878",
        "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey"
      },
      {
        "paperId": "a0f15d57cc627e5224aa439fde4e9b78cf7336e7",
        "title": "A Theory for Length Generalization in Learning to Reason"
      },
      {
        "paperId": "7c8851cce662351c49da94fa4512e2a6d2c1ace0",
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement"
      },
      {
        "paperId": "662f0841a2a8435c8b3f196a4016347ad31cd8d5",
        "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach"
      },
      {
        "paperId": "7a54aad06171f59149aca5380863c62729c70b41",
        "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"
      },
      {
        "paperId": "f7274310391c50eb319d5fbe7f4f557c124fffd7",
        "title": "Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models"
      },
      {
        "paperId": "6f24e0782300dca8a4cefcb5a3ccba94bfbb1395",
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning"
      },
      {
        "paperId": "7cebf3afab09d990a2322ea053d0de96f31f2971",
        "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?"
      },
      {
        "paperId": "d4f43485c9f71618dfdff884a74aff8a51fc018c",
        "title": "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes"
      },
      {
        "paperId": "068ff3def994d1424832e1f56ed72ef8245a42f0",
        "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence"
      },
      {
        "paperId": "5f764b3c02670f8d161c841af32804b23f8d5910",
        "title": "Similarity-based Neighbor Selection for Graph LLMs"
      },
      {
        "paperId": "bad12cb32faee21eafadbed03c6eec7e23e88ac9",
        "title": "Improving Sequential Recommendations with LLMs"
      },
      {
        "paperId": "800bb1ad43778ed7682e21ffbf9b2d12ffdd08c9",
        "title": "Visualization Generation with Large Language Models: An Evaluation"
      },
      {
        "paperId": "f8d7b0245480646abd257bac60ee37804981a0d7",
        "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models"
      },
      {
        "paperId": "78ee5f5915de75671f21ce4bbed274ce090fb83c",
        "title": "Prompt Valuation Based on Shapley Values"
      },
      {
        "paperId": "4dbae943822dcb21d3d1dd757b70740615bb4172",
        "title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models"
      },
      {
        "paperId": "b9ea4ea333eeb8875c3e23637a8dce99038be95c",
        "title": "An In-depth Look at Gemini's Language Abilities"
      },
      {
        "paperId": "1f5b4e393a1e02ab49e5ca2e6819cc28841918a2",
        "title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning"
      },
      {
        "paperId": "3328a40f312a8149380008f54a0345ce1f35e391",
        "title": "PathFinder: Guided Search over Multi-Step Reasoning Paths"
      },
      {
        "paperId": "ef4e4e4b52d4379ab5387d8dc53da87e561e78db",
        "title": "Good Questions Help Zero-Shot Image Reasoning"
      },
      {
        "paperId": "4558188257c5041900c3fa086a7a069dde252e9f",
        "title": "Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model"
      },
      {
        "paperId": "dd69049674f41d4ef5314b8f95bacfe59de31376",
        "title": "Conditions for Length Generalization in Learning Reasoning Skills"
      },
      {
        "paperId": "0bee079faf3dc53380db55cd5fc1ba9267c4d5e7",
        "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy"
      },
      {
        "paperId": "4014253368133c01bfc0383660c518d11afccad2",
        "title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "4f63c5a89c7299a864c6c48aa1844fb0fe8c9437",
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"
      },
      {
        "paperId": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
        "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models"
      },
      {
        "paperId": "f669d7a6fab0147253178a6fc854e05e3d92fb3f",
        "title": "Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models"
      },
      {
        "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"
      },
      {
        "paperId": "bfe9e6f64d9e819efc27772ab87dc0c5e2d62906",
        "title": "Critique Ability of Large Language Models"
      },
      {
        "paperId": "9c770a7a392df853387d5701e9876324f71f36c7",
        "title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games"
      },
      {
        "paperId": "0ab00c549b8683baeae23fb6c85f26c7a9878de7",
        "title": "PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning"
      },
      {
        "paperId": "bfeda6c7aa7899a80adb01894555b09d24756a59",
        "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"
      },
      {
        "paperId": "532430bfcedf0ca4d5ca695967b52fc21cb5b778",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"
      },
      {
        "paperId": "9099ee08e59cc33ed1c88d4708cf5c931bf46dc4",
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models"
      },
      {
        "paperId": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
        "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems"
      },
      {
        "paperId": "0f45608ddc01b3e192f3490330f4c4b8de074f79",
        "title": "Are Human-generated Demonstrations Necessary for In-context Learning?"
      },
      {
        "paperId": "4f9ebbb53e93fa5821b599d00e9beff6322821ef",
        "title": "In-context Interference in Chat-based Large Language Models"
      },
      {
        "paperId": "94d878ba7eeba4abc4d5e42b4c2c4c98d4e575ce",
        "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"
      },
      {
        "paperId": "e716e6e0b3dd5124268780dc9bed521a07f371b8",
        "title": "Contrastive Decoding Improves Reasoning in Large Language Models"
      },
      {
        "paperId": "a9c75cf664f675a1b4034b0256ec3c5168e293df",
        "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "paperId": "412fe1f135cb20c952962133ca1e534a71bfd27f",
        "title": "When Do Program-of-Thoughts Work for Reasoning?"
      },
      {
        "paperId": "19b43ff57e5d8f8a99da4110fbc30b4ecc39a527",
        "title": "Spoken Language Intelligence of Large Language Models for Language Learning"
      },
      {
        "paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "paperId": "f6d62a720a499e5d527357cd39a32e85b0afaa6d",
        "title": "How Susceptible Are LLMs to Logical Fallacies?"
      },
      {
        "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      },
      {
        "paperId": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"
      },
      {
        "paperId": "c29dbfbc17fa190b787a2662d49f08a38c8bd166",
        "title": "ARB: Advanced Reasoning Benchmark for Large Language Models"
      },
      {
        "paperId": "4993258852711c4e3d0011325ac3db680eae84f4",
        "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"
      },
      {
        "paperId": "c48fc69b62c749e78928d6a3bae98ffe278f761a",
        "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study"
      },
      {
        "paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf",
        "title": "A Survey on Evaluation of Large Language Models"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "5db0f55332839c408e3049cea1a6ad48fefba70c",
        "title": "Aligning Language Models to User Opinions"
      },
      {
        "paperId": "74cbfe5b9331ecc717cc47913ec43e707badcf97",
        "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models"
      },
      {
        "paperId": "c131ac4360aa9d9b7a955bed87e9cd4a1a3e7562",
        "title": "Knowledge Rumination for Pre-trained Language Models"
      },
      {
        "paperId": "fd5576d95b4feb7aa7ceaf158ab6c8eef8774a95",
        "title": "Generalizing Reasoning Problems to Longer Lengths"
      },
      {
        "paperId": "86bdb560b1fa46c5cf81597f4072f7bf8d640ecf",
        "title": "Pitfalls in Evaluating Inference-time Methods for Improving LLM Reliability"
      },
      {
        "paperId": "ebda8d187f83876e37922c4105b171e6d7f99b05",
        "title": "DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention"
      },
      {
        "paperId": "13ba8f1f02b0a9d78477487717d3cab59b5ef05f",
        "title": "FakeBench: Uncover the Achilles' Heels of Fake Images with Large Multimodal Models"
      },
      {
        "paperId": "8c479389f30749d94c99be3c0ef0d764721a81dd",
        "title": "MuMath: Multi-perspective Data Augmentation for Mathematical Reasoning in Large Language Models"
      },
      {
        "paperId": "672e04bbd3f3bcbcab2c7eb87fe896ff1f1e883c",
        "title": "Linear Layer Extrapolation for Fine-Grained Emotion Classification"
      },
      {
        "paperId": "72493d38ed8c95e0f49ee5b6cf672470d69261df",
        "title": "GEAR: An Efficient Error Reduction Framework for KV Cache Compression in LLM Inference"
      },
      {
        "paperId": "8d17234680db76f99efd22fbcb169f45d2d79d93",
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "paperId": "6c0a3927005c8cde1e9df52e7aee686b59ca5bd0",
        "title": "Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark)"
      },
      {
        "paperId": "f74a2014346701d64988c58ab9e2d38fb38dd9ff",
        "title": "X-Mark: Towards Lossless Watermarking Through Lexical Redundancy"
      },
      {
        "paperId": "08a3986c595dc38147edf6425641de1fe0d796b3",
        "title": "Mobile-Env: An Evaluation Platform and Benchmark for Interactive Agents in LLM Era"
      },
      {
        "paperId": "cb165bf7b2fc5db748f81b4fdba94621101ce698",
        "title": "Large Language Models(LLM) for Automating 20 Questions Game"
      },
      {
        "paperId": "791301a73d4126ccabdf279d44ff06370984e1bd",
        "title": "Towards Artificial General Intelligence: Enhancing LLMs capability for Abstraction and Reasoning"
      },
      {
        "paperId": "4a222a27a944539c5ffbb4cc44cf7de5663b57d3",
        "title": "Dependency-Aware Semi-Structured Sparsity: Declining Roles of Outliers in Pruning GLU-based LLMs"
      },
      {
        "paperId": "95f43c99a9cb6544e0e8d97cab86a066cee97c11",
        "title": "Aligning Large Language Models via Chain-of-Thought Reasoning"
      },
      {
        "paperId": "d26162289004cad1a485b54aae84a043185191aa",
        "title": "Mitigating Feedback Inconsistency in Large Language Model Self-Tuning Anonymous"
      },
      {
        "paperId": "f98ef89ca455a8a75be3892dc3fbfa75ef91fdde",
        "title": "On Reward Functions For Self-Improving Chain-of-Thought Reasoning Without Supervised Datasets (Abridged Version)"
      }
    ],
    "score": 61.0
  },
  {
    "id": "dacc3a8d45968616f220628dc0db8d5d78c1a389",
    "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences",
    "authors": [
      "Souradip Chakraborty",
      "Jiahao Qiu",
      "Hui Yuan",
      "Alec Koppel",
      "Furong Huang",
      "Dinesh Manocha",
      "A. S. Bedi",
      "Mengdi Wang"
    ],
    "year": 2024,
    "citationCount": 58,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/dacc3a8d45968616f220628dc0db8d5d78c1a389",
    "pdf_url": "https://doi.org/10.48550/arXiv.2402.08925",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2402-08925",
      "DOI": "10.48550/arXiv.2402.08925",
      "CorpusId": 275118993
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "0c9405aa6d4c6a920a4504fd786d91b48bcc6b48",
        "title": "Large language model assisted hierarchical reinforcement learning training"
      },
      {
        "paperId": "5cdcd77dd36a26ed3ea9da9cf8aac577a61b4c18",
        "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"
      },
      {
        "paperId": "c264e326bf7cac8ade6e7c3850ec3043c56744d7",
        "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation"
      },
      {
        "paperId": "ac9a7ebd4187ba0a280428e044b60cd71701f418",
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models"
      },
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "bd50ee192e60d53afd2a031162200ba4da4df81c",
        "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models"
      },
      {
        "paperId": "1b108ded8ea5524d90bc43fd9bd9bdd82f8f8f74",
        "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning"
      },
      {
        "paperId": "09b7ee50254468d469c8afe3dffd7f9e8a91c1ad",
        "title": "Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency and Preference Matching"
      },
      {
        "paperId": "1bf20b99c46de408e5c2edfd6af9f62c63174f88",
        "title": "Is Active Persona Inference Necessary for Aligning Small Models to Personal Preferences?"
      },
      {
        "paperId": "9af9c0df0a328d5f327bed3151819b06dfc33622",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment"
      },
      {
        "paperId": "c8c583a5081e0d87ea6b5bf5827cd47c4c85df9e",
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach"
      },
      {
        "paperId": "30ca68af36578d238964df1c5c596e33521e00a5",
        "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts"
      },
      {
        "paperId": "97d76146a96f201f0bb7a73f436e67b357414821",
        "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment"
      },
      {
        "paperId": "76509b17bc5582137720ebe2c3a6b661a2804382",
        "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF"
      },
      {
        "paperId": "10d0287009262d25ad5c97a17991e3b87cd83ac0",
        "title": "Capturing Individual Human Preferences with Reward Features"
      },
      {
        "paperId": "8e4114799cc07580af4d161702583a11045668e0",
        "title": "Adaptive Preference Aggregation"
      },
      {
        "paperId": "a9d6a3fa154837e0d88238fc779a9993817e440f",
        "title": "Robust Multi-Objective Controlled Decoding of Large Language Models"
      },
      {
        "paperId": "9e90796e971a31f10be4a23ddff894f4fef8243b",
        "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models"
      },
      {
        "paperId": "a1ab7f948bf858e4cdb9c0cf4409b14793b01837",
        "title": "LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces"
      },
      {
        "paperId": "e983d7ad555dad845059da1ee0a5e57cfd057af0",
        "title": "Direct Alignment with Heterogeneous Preferences"
      },
      {
        "paperId": "2b26c9ba5ae0f1a081a47266c34c6d77b211618c",
        "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models"
      },
      {
        "paperId": "76d57ec8616a2f6d4bd5451a3344966b1c6dad5a",
        "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis"
      },
      {
        "paperId": "b0a94c399b1771c8b7efab0c9fcab9df41ddd7da",
        "title": "Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers"
      },
      {
        "paperId": "2f16f56f955e9fc5700dca88a76734bb9cbdfce0",
        "title": "Clone-Robust AI Alignment"
      },
      {
        "paperId": "588c0018a03fea0a29f0a7c636ffb7cbe11cb98b",
        "title": "Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment"
      },
      {
        "paperId": "afc7c6da6d22ac999633394f18345749a2ad5397",
        "title": "No Preference Left Behind: Group Distributional Preference Optimization"
      },
      {
        "paperId": "3d0557ed32fa648e6977faed9a60c655a7e33164",
        "title": "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization"
      },
      {
        "paperId": "c59f9b9544e9dde50fcfd77a7b5b478741fe3682",
        "title": "Reducing the Scope of Language Models"
      },
      {
        "paperId": "b8cff4ac41240587aca403914ff38a305d5d2ebf",
        "title": "AdapTUI: Adaptation of Geometric-Feature-Based Tangible User Interfaces in Augmented Reality"
      },
      {
        "paperId": "516b5400f234154d8fac0c3386d45e7f8123e319",
        "title": "ComPO: Community Preferences for Language Model Personalization"
      },
      {
        "paperId": "89b42dc02400db2c58703fb0d8d0b6f0210bfe7f",
        "title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning"
      },
      {
        "paperId": "bc22992620495487ffba2e7c37065423a348119d",
        "title": "Personalized Adaptation via In-Context Preference Learning"
      },
      {
        "paperId": "249033f644140dd712b679a6b0ef295ede95fecd",
        "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF"
      },
      {
        "paperId": "95124cb03a6e5de7a623db32b987531d7830629e",
        "title": "PAD: Personalized Alignment of LLMs at Decoding-time"
      },
      {
        "paperId": "0d49552b54a1c2e064047d332018a898fcf6d9cb",
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "e7b5d0269bdd37d01cea2bddb4d2ec9cf1539a40",
        "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"
      },
      {
        "paperId": "d95220be54d155f8be7f36b4fde794711cbb69b3",
        "title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts"
      },
      {
        "paperId": "c8456736969365d9138dc7641cb87dc19472b8ce",
        "title": "Reflective Verbal Reward Design for Pluralistic Alignment"
      },
      {
        "paperId": "c9df92684eab49aea9b74a4b6409e2fb5963834d",
        "title": "Game Theory Meets Large Language Models: A Systematic Survey"
      },
      {
        "paperId": "39fd3d41f5ab882eea29dbe27eef8d0954b29856",
        "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment"
      },
      {
        "paperId": "9ddda210b622e0127a43dd5d7cd02928d449b799",
        "title": "Improving Context-Aware Preference Modeling for Language Models"
      },
      {
        "paperId": "e9b156ddf40f660dfb6a9b2a720d0b0735414dc0",
        "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization"
      },
      {
        "paperId": "af11d43d3b4e6f2d01f34feb149037cdbd05d9ee",
        "title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models"
      },
      {
        "paperId": "1a7781465495ae54ce8413aa4ddedd24f5666dc7",
        "title": "Robust Reinforcement Learning from Corrupted Human Feedback"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      },
      {
        "paperId": "2433170286f1adf55670dc68ce73d816e1ccb0be",
        "title": "Aligning to Thousands of Preferences via System Message Generalization"
      },
      {
        "paperId": "ea4f6a6dbe31729b90836221ba131f080937ae5f",
        "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales"
      },
      {
        "paperId": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
        "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization"
      },
      {
        "paperId": "af09dff7d60f01c2ca9aa344956274e34c60bc6b",
        "title": "Axioms for AI Alignment from Human Feedback"
      },
      {
        "paperId": "2bbc0f8ff058227d693f680d64f9736ae7a19f23",
        "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts"
      },
      {
        "paperId": "80fd3d2d3b2f7e0cb41c935c6b7ac1b73823a60d",
        "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "3714bb3aac3f0429cd62e315da0ffb3f79655f98",
        "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"
      },
      {
        "paperId": "2a7d3d3ded5511a184329ac404980e90f864d442",
        "title": "Active Preference Optimization for Sample Efficient RLHF"
      },
      {
        "paperId": "7271c260a010ce15a8821891ecca52d83d011663",
        "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs"
      },
      {
        "paperId": "f02a685211591e75c63e76e3fed90a1f25d8e689",
        "title": "Reducing the Scope of Language Models with Circuit Breakers"
      },
      {
        "paperId": "631adfb581298335f00560b860dcad0cf800a326",
        "title": "Policy Dreamer: Diverse Public Policy Generation Via Elicitation and Simulation of Human Preferences"
      },
      {
        "paperId": "b31b76764b24f66d3223fad688380b200b85bf62",
        "title": "WITH H IDDEN C ONTEXT"
      }
    ],
    "score": 58.0
  },
  {
    "id": "fb09b581589e1195ff018179c6a11668587c6d64",
    "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
    "authors": [
      "Juan Rocamonde",
      "Victoriano Montesinos",
      "Elvis Nava",
      "Ethan Perez",
      "David Lindner"
    ],
    "year": 2023,
    "citationCount": 106,
    "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second\"baseline\"prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
    "url": "https://www.semanticscholar.org/paper/fb09b581589e1195ff018179c6a11668587c6d64",
    "pdf_url": "https://arxiv.org/pdf/2310.12921.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-10-19",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-12921",
      "ArXiv": "2310.12921",
      "DOI": "10.48550/arXiv.2310.12921",
      "CorpusId": 264306321
    },
    "references": [
      {
        "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
      },
      {
        "paperId": "8c9e95f32982ca21a7e4ef9c986aaa934cb11293",
        "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies"
      },
      {
        "paperId": "690df0820f35a47e1ce44f90e6ddb4132aa09267",
        "title": "Vision-Language Models for Vision Tasks: A Survey"
      },
      {
        "paperId": "c11810fa8887b678facea62da4607c4898360308",
        "title": "Training Language Models with Language Feedback at Scale"
      },
      {
        "paperId": "28dafb94a87daa71ad3edf9f04f3c8c32753398b",
        "title": "Improving Code Generation by Training with Natural Language Feedback"
      },
      {
        "paperId": "cf41ae462687f81ce95b27113c6a4f9c2751de42",
        "title": "Vision-Language Models as Success Detectors"
      },
      {
        "paperId": "c1c663d8a7d78342d8eabb6ca144e5761b6a2443",
        "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b",
        "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning"
      },
      {
        "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
      },
      {
        "paperId": "32c9b3859086d15184989454eb878638659e64c6",
        "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"
      },
      {
        "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
      },
      {
        "paperId": "3f6d6ed110f3262fdc194184c54dd63701b3bca9",
        "title": "Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?"
      },
      {
        "paperId": "c57293882b2561e1ba03017902df9fc2f289dea2",
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "a6f5b8f114b3eabbcd7f3f62091a481ca6f7f243",
        "title": "Predictability and Surprise in Large Generative Models"
      },
      {
        "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
        "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
      },
      {
        "paperId": "826383e18568c9c37b5fc5dd7e2913352db22b47",
        "title": "Simple but Effective: CLIP Embeddings for Embodied AI"
      },
      {
        "paperId": "8f5177778ba5915a763e08d20977173bc18f5c87",
        "title": "Cut the CARP: Fishing for zero-shot story evaluation"
      },
      {
        "paperId": "69ee9b3a915951cc84b74599a3a2699a66d4004f",
        "title": "CLIPort: What and Where Pathways for Robotic Manipulation"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "aaa99de83292370a964fcaa51e6e866a96726bb2",
        "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "18f44e605082388ffc0a59b895ff70b01cfc0034",
        "title": "Quantifying Differences in Reward Functions"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "c52b30a60fda5f23cc0d2241c4e127f5191bbb2d",
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"
      },
      {
        "paperId": "a09560239e398fe8aea05856823b46219a7dc539",
        "title": "Zero-Shot Reward Specification via Grounded Natural Language"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      }
    ],
    "cited_by": [
      {
        "paperId": "5e3ee5d468bf323469a504cfebe37f910caf62ed",
        "title": "Zero-shot reasoning for simulating scholarly peer-review"
      },
      {
        "paperId": "b5eeff55c73861fec39e6b02019dd76a32bb698c",
        "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play"
      },
      {
        "paperId": "30255f6a3541a6240d7b5d2b25923247c879c649",
        "title": "LAGEA: Language Guided Embodied Agents for Robotic Manipulation"
      },
      {
        "paperId": "a9c3b0ceff087c81b701eccb205d8db381fc014d",
        "title": "AI Model Integrating Imaging and Clinical Data for Predicting CSF Diversion in Neonatal Hydrocephalus: A Preliminary Study"
      },
      {
        "paperId": "4599cf1c8ceb324fcd6c840f0bae5c370a9e17c6",
        "title": "OpenGVL -- Benchmarking Visual Temporal Progress for Data Curation"
      },
      {
        "paperId": "75297c2757238e17802549e6334ffe5e54a0290f",
        "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks"
      },
      {
        "paperId": "956348b657599896737c7f81f7310a8da6e2f6cd",
        "title": "Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation"
      },
      {
        "paperId": "ff0f7c80c9ecabf6986884b918626d431b9b6778",
        "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling"
      },
      {
        "paperId": "b7b0d2411723fec46715f129b13fe39e34d0e686",
        "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making"
      },
      {
        "paperId": "25caf432faf629085f4db9c9614086a57ddd233b",
        "title": "Training-free Generation of Temporally Consistent Rewards from VLMs"
      },
      {
        "paperId": "d8f8562ce63d9176d3e30db46e208c25ef6932a5",
        "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models"
      },
      {
        "paperId": "81d348c56a16e15efb67754aa189efe8aaa7d736",
        "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills"
      },
      {
        "paperId": "8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
        "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models"
      },
      {
        "paperId": "49c193405f73ba371384c8c62eef2cf7f8e1ccf7",
        "title": "VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models"
      },
      {
        "paperId": "c0851f97ccb5ed8432f4c144cc7e63985536e43d",
        "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning"
      },
      {
        "paperId": "61fe493bf3a4a87531f95d9e02f6676fbd2521a6",
        "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "5df032869f18c9b87ffee865ef2eb16db91df2e7",
        "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates"
      },
      {
        "paperId": "efc028b1a397eeb46eb6b945faea536d068b752a",
        "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning"
      },
      {
        "paperId": "ff0dc43bbae86228028c143ec33d4c3e89e5b2a4",
        "title": "LLM Coach: Reward Shaping for Reinforcement Learning-Based Navigation Agent"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "4efd24d4f84c901e27d708be7c8925b2618722da",
        "title": "MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos"
      },
      {
        "paperId": "206f07a961b480948bb3839a85f6897f3535e019",
        "title": "TREND: Tri-Teaching for Robust Preference-based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "33a7bb26825560a0b13c955152ff14f27979c026",
        "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making"
      },
      {
        "paperId": "6c3efe0d2d7393a64d2729f7d355de78d9422bee",
        "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning"
      },
      {
        "paperId": "44bd8a38d97cda8cebeb71e0a6848868993069e2",
        "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations"
      },
      {
        "paperId": "8913f68977f6e320c24b2dd6f10beb9fea365cdc",
        "title": "Text-to-Decision Agent: Offline Meta-Reinforcement Learning from Natural Language Supervision"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "8e51cf726308232369db364477c776f66c9f91e8",
        "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation"
      },
      {
        "paperId": "96cf799e708f1cf4c0e1a09f8ecbc3b5224cb3f2",
        "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill"
      },
      {
        "paperId": "3990f2211e16fff86af9ad9f0c967b0fe37d3658",
        "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning"
      },
      {
        "paperId": "b635dd85b8edb9bf2daaf7dc62840c78c1fe4c94",
        "title": "Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "2dd988da09f644bac28e4e6fe1a9108b9f030c96",
        "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning"
      },
      {
        "paperId": "625ea7a6a13e3438c3387d7742405d2aa7489b21",
        "title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models"
      },
      {
        "paperId": "c85ecb7e84f73b4d1043c6860c0f39d5e344affa",
        "title": "PANDORA: Diffusion Policy Learning for Dexterous Robotic Piano Playing"
      },
      {
        "paperId": "423c6f1180ed65500daabe6f5998300d1bae2293",
        "title": "LuciBot: Automated Robot Policy Learning from Generated Videos"
      },
      {
        "paperId": "0dc09dc31d58f381d2cee0c53a286714d3c5cc43",
        "title": "Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "8023143f98c54751a819db8493ac4bcc47b5c86e",
        "title": "Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed Environments: Vision-Language Model Approach"
      },
      {
        "paperId": "33f24a2ac0248a72627b1ea42651e5b0700f34b6",
        "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning"
      },
      {
        "paperId": "bc53729436473d7c62d850426f7956097ea69c10",
        "title": "SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models"
      },
      {
        "paperId": "af6e9cef7efb18a20a16f585fc27c26dbcb9483c",
        "title": "Offline RLAIF: Piloting VLM Feedback for RL via SFO"
      },
      {
        "paperId": "add637341443b4980988cd2096074f06019d6341",
        "title": "Subtask-Aware Visual Reward Learning from Segmented Demonstrations"
      },
      {
        "paperId": "42022ebb089d31e372f6fce248fa591dc187b717",
        "title": "A Survey on Large Language Models for Automated Planning"
      },
      {
        "paperId": "1c3e59862cb82b78c6fb534fd2c5dc899216ca26",
        "title": "VLP: Vision-Language Preference Learning for Embodied Manipulation"
      },
      {
        "paperId": "74450ea0f46583a01b707b9abbf4c64a76e1447e",
        "title": "Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "a92ca87378244eeede44196a347c32df4ef9f6e6",
        "title": "Imitation Learning from a Single Temporally Misaligned Video"
      },
      {
        "paperId": "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
        "title": "Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "aa2c38560a8a50938a20f9de5a371142e41337da",
        "title": "INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation"
      },
      {
        "paperId": "b5e2ee878e757f1e7e557b89e7d3460edacb50ed",
        "title": "RLZero: Direct Policy Inference from Language Without In-Domain Supervision"
      },
      {
        "paperId": "c33fe0ac500e93606876cf3d1915e40f85c1a130",
        "title": "Human 0, MLLM 1: Unlocking New Layers of Automation in Language-Conditioned Robotics with Multimodal LLMs"
      },
      {
        "paperId": "0bfcb7799bf3529ae142d9681285a4f39d88d513",
        "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble"
      },
      {
        "paperId": "e1ff1fe678cd7529b08a551828065b879f7f50a5",
        "title": "ViSTa Dataset: Do vision-language models understand sequential tasks?"
      },
      {
        "paperId": "4749559e87d6a301a2ac61767d48ff4437fe4fd8",
        "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback"
      },
      {
        "paperId": "7712b0e640848cd67d34d10f0b8637ba89f39f0c",
        "title": "Vision Language Models are In-Context Value Learners"
      },
      {
        "paperId": "74abb1a62b70e7f9019c7209ea4e65b72867a574",
        "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback"
      },
      {
        "paperId": "92c82a51ad13c361d052987694cf93d6a72d5789",
        "title": "CycleResearcher: Improving Automated Research via Automated Review"
      },
      {
        "paperId": "7f15c1aade1a82ec882bd9052e844545212887e2",
        "title": "ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization"
      },
      {
        "paperId": "18c9afb0d6ed37c0933c786afce8641a998a4c9c",
        "title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI"
      },
      {
        "paperId": "df1c9ede15c86c5621d1c5842249887bcc77179e",
        "title": "SDS - See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration"
      },
      {
        "paperId": "1ed6f9c35bc23965d5451ba39f197fefbe20710f",
        "title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos"
      },
      {
        "paperId": "340b5bbb430e8fbf1e3b7d9fa57bf874912f3714",
        "title": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance"
      },
      {
        "paperId": "06fb61d20aa23ab856229425be75e0588a61fb2f",
        "title": "Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models"
      },
      {
        "paperId": "c94512390c31942a73cbc152dfc7ab0db4ec1e6b",
        "title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making"
      },
      {
        "paperId": "3227444169081eb108fa640474077e1a0e4ae9ea",
        "title": "CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills Using Large Language Models"
      },
      {
        "paperId": "4f42fe9fd364342154b6c64bc53dd0f3e72b0575",
        "title": "CLSP: High-Fidelity Contrastive Language-State Pre-training for Agent State Representation"
      },
      {
        "paperId": "9ebce30da5743f37d5b6378ed6c0d8231b114d7e",
        "title": "MotIF: Motion Instruction Fine-Tuning"
      },
      {
        "paperId": "7b32e4e18dcd680ae2aee356542f022047e782ca",
        "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models"
      },
      {
        "paperId": "dfafc44befd8e74c7a00013352ac6a69962b21e9",
        "title": "Adaptive Language-Guided Abstraction from Contrastive Explanations"
      },
      {
        "paperId": "687e42c4d01a37c835d4ef1bb89a4757e775d5b6",
        "title": "Game On: Towards Language Models as RL Experimenters"
      },
      {
        "paperId": "205c3352ec73278e737806849323b37b36e1de5d",
        "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning"
      },
      {
        "paperId": "e344313490dc93bacf721c19f0b74ae921ac4285",
        "title": "Reward Models in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "885a3dce8f2aea452af241808e7f817def80e172",
        "title": "Grounding Open-Domain Knowledge from LLMs to Real-World Reinforcement Learning Tasks: A Survey"
      },
      {
        "paperId": "fa299eb816102ebff8930bd2545178660444b675",
        "title": "Text-Aware Diffusion for Policy Learning"
      },
      {
        "paperId": "528ad1f3e7b6ff4f61ff05ab5d2153ef6cc301ae",
        "title": "Revisiting Sparse Rewards for Goal-Reaching Reinforcement Learning"
      },
      {
        "paperId": "9931dd03b3378923f5c2f035087f24d05b71290a",
        "title": "GenRL: Multimodal-foundation world models for generalization in embodied agents"
      },
      {
        "paperId": "3565ed1d884fcd6b7d20f9d410c6bbb871f90b0c",
        "title": "EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data"
      },
      {
        "paperId": "39f1bbc507f9f11a584b6b0bcb5828a1a4eacd56",
        "title": "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models"
      },
      {
        "paperId": "761509e64bf8eb462b615e5fbcea866289ccc8af",
        "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning"
      },
      {
        "paperId": "b96d979e0a4c3127907c5dba170a3470f9ff8956",
        "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics"
      },
      {
        "paperId": "313bfa540068ab6a77d36e6a20ecf28859719580",
        "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation"
      },
      {
        "paperId": "d18554cfa740cc9fe7cafaa2b2d578f4512ca18e",
        "title": "Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance"
      },
      {
        "paperId": "f7749635a5fc0492ef4705bee963ffa887bb2865",
        "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning"
      },
      {
        "paperId": "e3c7262cb541bf6e72e4e932e9369424dc6a0551",
        "title": "A Survey of Optimization-Based Task and Motion Planning: From Classical to Learning Approaches"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "c3d328f2f6ea0ab605bb09a53788d6208e22cd8f",
        "title": "LORD: Large Models Based Opposite Reward Design for Autonomous Driving"
      },
      {
        "paperId": "128785349190fcced541db12f6c3386f79547db9",
        "title": "AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents"
      },
      {
        "paperId": "362db7e58c4e4e3d199026dca4dbc3cdbe4d8c88",
        "title": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors"
      },
      {
        "paperId": "ecd3091debcd2f393379508df70bceb94db0be3b",
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"
      },
      {
        "paperId": "66d491e0054f92e2959c8adb912b293a1e2af832",
        "title": "Natural Language Reinforcement Learning"
      },
      {
        "paperId": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
        "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback"
      },
      {
        "paperId": "4c98e18cf16395b95ffaaeeac3eceffa608dcf8d",
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"
      },
      {
        "paperId": "a3ca77456142b78367dd5d53138b50dfac8086ca",
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities"
      },
      {
        "paperId": "e0702a22e0841c54ab865b4996d7b07af192a3e1",
        "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"
      },
      {
        "paperId": "287f4b599020d53634abbee6a6e58468d14ebbdc",
        "title": "Vision-Language Models as a Source of Rewards"
      },
      {
        "paperId": "dca4ed6d4db18216796336d647f8d4bdf197f039",
        "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts"
      },
      {
        "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"
      },
      {
        "paperId": "28e536d4b425a8743af9b074ddb11baba66f8b47",
        "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey"
      },
      {
        "paperId": "12971251a5f526b220e41903d87b3d209ec30d6a",
        "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision"
      },
      {
        "paperId": "e6876d73d5aa06e09328d8ddf240c8a800a62667",
        "title": "Iterative Trajectory Exploration for Multimodal Agents"
      },
      {
        "paperId": "20ae5a6c58b47dc84fe6b7d123b7d4afdfef5f51",
        "title": "Multimodal foundation world models for generalist embodied agents"
      },
      {
        "paperId": "391ba9a9491b3a85ef952bff08cf482c171420d4",
        "title": "Evolutionary Reward Design and Optimization with Multimodal Large Language Models"
      },
      {
        "paperId": "1b1fa60477532fcd19cc782054c880141a84597d",
        "title": "Fine-Tuning Transformer-Based Vision-Language Models for Robust Object Detection in Unstructured Environments"
      },
      {
        "paperId": "e299bb57b3d94534d7704ef45af7793e429ab5a1",
        "title": "Pilot Analysis for: Learning to Encode Multi-level Dynamics in Effect Heterogeneity Estimation"
      },
      {
        "paperId": "226cb3a6b51707a9eeb1c5f2176fdcecc8ff272b",
        "title": "Reinforcement Learning from Human Text Feedback: Learning a Reward Model from Human Text Input"
      },
      {
        "paperId": "9d0edf173b18b98b20e94cbeae6b92a11c7ead7e",
        "title": "Position: Human-like reinforcement learning is facilitated by structured and expressive goals"
      },
      {
        "paperId": "664a3576bc06196410206a369f7cb21ed50c69a8",
        "title": "Time Your Rewards: Learning Temporally Consistent Rewards from a Single Video Demonstration"
      },
      {
        "paperId": "4c300039c3c40859b907469f3e85d1a06ce65f80",
        "title": "SELU: Self-Learning Embodied Multimodal Large Language Models in Unknown Environments"
      }
    ],
    "score": 53.0
  },
  {
    "id": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
    "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
    "authors": [
      "Alex J. Chan",
      "Hao Sun",
      "Samuel Holt",
      "M. Schaar"
    ],
    "year": 2024,
    "citationCount": 50,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many\"actions\"(selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.",
    "url": "https://www.semanticscholar.org/paper/9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
    "pdf_url": "https://arxiv.org/pdf/2402.00782.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-01",
    "externalIds": {
      "ArXiv": "2402.00782",
      "DBLP": "conf/icml/ChanSHS24",
      "DOI": "10.48550/arXiv.2402.00782",
      "CorpusId": 267365205
    },
    "references": [
      {
        "paperId": "c8168177d5cbd3a3d9ac5abdef7b694f388ec8f5",
        "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"
      },
      {
        "paperId": "de2455741f60d15d9d34507b701fdc83868b1df0",
        "title": "Optimising Human-AI Collaboration by Learning Convincing Explanations"
      },
      {
        "paperId": "72883ceab65262f1e38ab1cd4262a326425488f1",
        "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "7fcc7c21023b3c8f95773e14db7cc8007c4c8d24",
        "title": "Neural Laplace Control for Continuous-time Delayed Systems"
      },
      {
        "paperId": "f2b0017ddd77fa38760a18145e63553105a1a236",
        "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "3986a3070cde3354e9d80faff70d72c328434032",
        "title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees"
      },
      {
        "paperId": "2a8ce79d05232acfaad7753420cd99dcd5d718c8",
        "title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies"
      },
      {
        "paperId": "80110db20124242316a13795e964d09cd15a3802",
        "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "a7d58bd29778ef0d15b9e9e3eb2f37a8cf1ea70c",
        "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "fb21824c6a9634ec718016337170107ab4422c38",
        "title": "Off-Policy Reinforcement Learning with Delayed Rewards"
      },
      {
        "paperId": "03308bd92c5993f6828950aaea2f38f5f7d9cd8d",
        "title": "Scalable Bayesian Inverse Reinforcement Learning"
      },
      {
        "paperId": "13e4716495c82e917a18f599648372c4dead28a8",
        "title": "Learning Guidance Rewards with Trajectory-space Smoothing"
      },
      {
        "paperId": "10efeca4a3237504985963fff25ef34fb4808737",
        "title": "Reinforcement Learning with Random Delays"
      },
      {
        "paperId": "e5624daed3f2e59be0f90c83e5bf622e69962fef",
        "title": "Toward the Fundamental Limits of Imitation Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "22ba288be6f469d79c4501ad099d880573fea719",
        "title": "Gradient-free Online Learning in Continuous Games with Delayed Rewards"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "f1697ce4dddb58533d7d3f937fed74807d46edb8",
        "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "0f16a8567bd9dbaebae99080624794f589409998",
        "title": "Bandit Learning with Delayed Impact of Actions"
      },
      {
        "paperId": "867cc74781225da4e08a77fc35037ba77911e455",
        "title": "Hindsight Credit Assignment"
      },
      {
        "paperId": "44c88284ed696dc21158ebac401617bb69b54dd9",
        "title": "Reward"
      },
      {
        "paperId": "f17bf1a6d241b28b6b7c327e8ffee8c9fa709f5c",
        "title": "Self-Attentional Credit Assignment for Transfer in Reinforcement Learning"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"
      },
      {
        "paperId": "4203aa1f116c8e1d3ad50771a37a6aaea20e9523",
        "title": "Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "abcbfc9742e8f4825cfc536091fd414e08d03998",
        "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "1a2118bed729579528deb51e745d58dd3629baf6",
        "title": "Learning Important Features Through Propagating Activation Differences"
      },
      {
        "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
      },
      {
        "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
        "title": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "719c14406f2d3e2c1d7e3cf632af80653257f32f",
        "title": "Learning and planning in environments with delayed feedback"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "cc8c9ce95b081eebe862679065a6bdc42966605a",
        "title": "Stochastic analysis and control of real-time systems with random time delays"
      },
      {
        "paperId": "947bc7d68a276527c7dbf23c2070ebd114bcfe51",
        "title": "Dysphagia"
      },
      {
        "paperId": "e51730736a690302d2b13e6787a2f0d92901b1d8",
        "title": "IT Matters"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "Koala: A dialogue model for academic research"
      },
      {
        "paperId": null,
        "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "e7c6e744a4cef9620cd245cf9b5887f6591ec6ad",
        "title": "Learning in Generalized Linear Contextual Bandits with Stochastic Delays"
      },
      {
        "paperId": null,
        "title": "Return decomposition for delayed rewards"
      },
      {
        "paperId": "1f4731d5133cb96ab30e08bf39dffa874aebf487",
        "title": "A Framework for Behavioural Cloning"
      },
      {
        "paperId": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
        "title": "Temporal credit assignment in reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Alpacae-val: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chat-gpt quality"
      },
      {
        "paperId": null,
        "title": "Gemini: A family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "Model card and evaluations for claude models"
      },
      {
        "paperId": null,
        "title": "Redpajama-data"
      },
      {
        "paperId": null,
        "title": "Dense Reward for Free in RLHF"
      },
      {
        "paperId": null,
        "title": "open reproduction of llama"
      }
    ],
    "cited_by": [
      {
        "paperId": "2007b767d8aeeb23290b76e2b9eb44e1a272b5a6",
        "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs"
      },
      {
        "paperId": "d6cbebb1b7884de42163b1ab4e72856dd416d2ca",
        "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS"
      },
      {
        "paperId": "1670b7d6fbda73ccb80904178c636eb147ab549a",
        "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "ef4e21071bab214fe11b27b8440664b955808f3e",
        "title": "Enhancing RLHF with Human Gaze Modeling"
      },
      {
        "paperId": "39e8b83aea2f232eb7b57b19580044ff596eefd9",
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
      },
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "136f282b12e52779a471ed2417e32fd882f67cde",
        "title": "Discriminative Policy Optimization for Token-Level Reward Models"
      },
      {
        "paperId": "36540ee20aa4ba4f02f7359e1113cadbefbf1b6d",
        "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy"
      },
      {
        "paperId": "2109255b50868c453d397bc732fc44d8e83a164f",
        "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF"
      },
      {
        "paperId": "acf2f14fbf6372470d6cc3bd7a8d17b54a00939d",
        "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning"
      },
      {
        "paperId": "dc4f3484edad0a9c1c659ef85740f83e2816f7a9",
        "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models"
      },
      {
        "paperId": "34126632dc65dc86ce90f2c6f68fbb68a7196772",
        "title": "Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling"
      },
      {
        "paperId": "7e0fc989fca3006c543270bf3d82f54e33d35819",
        "title": "Attention-Based Reward Shaping for Sparse and Delayed Rewards"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "758fc4cb9100f83ed68f30850af2af824e2a9f6f",
        "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization"
      },
      {
        "paperId": "a24a85b1e45770385ee668ad02f8794498e5c5eb",
        "title": "TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning"
      },
      {
        "paperId": "13f9dd2c7d259460699b0d2ca9acf52351535d91",
        "title": "Align to Structure: Aligning Large Language Models with Structural Information"
      },
      {
        "paperId": "1eec58f271ac291ac1645a2a6162c4a5b659d0dc",
        "title": "Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping"
      },
      {
        "paperId": "95bcd5cdec716fa2509cc275aa7bf0f619755fcc",
        "title": "MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      },
      {
        "paperId": "145a2e2ff7e83dc2bbf3d95acf3dba046e2d69da",
        "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference"
      },
      {
        "paperId": "ab72ab40e2baeea8a57d1db386737239d8e07397",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models"
      },
      {
        "paperId": "3dd6e3dc2d7a8fa6aaa4671557e6a94e62aa1106",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data"
      },
      {
        "paperId": "53d5d595b263f54c9a5c4d51e298413c450abb79",
        "title": "Curiosity-Driven Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1c3aaffa10f83cac66a78f7cb796cb64edac6030",
        "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model"
      },
      {
        "paperId": "4de79e47a9879d2eda2ae20b4c2dee7b78c09303",
        "title": "Interpreting Language Reward Models via Contrastive Explanations"
      },
      {
        "paperId": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
        "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution"
      },
      {
        "paperId": "19716f3ad5fe69de7a18bb1a3c496a6b9197bac7",
        "title": "Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings"
      },
      {
        "paperId": "864b530b17373aa4e6f09a00614cf2f6c2be707d",
        "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling"
      },
      {
        "paperId": "f6cf3e80791cc8a40ef07f258ee49a189c994514",
        "title": "Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies"
      },
      {
        "paperId": "fb5daae094e79c1de6787b5a6372d900692806e8",
        "title": "XLM for Autonomous Driving Systems: A Comprehensive Review"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "7792fe02a297344a25a0b22fa8d6cbd86884be31",
        "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "945ec8e851fcced41020be961877027edb5e7048",
        "title": "Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization"
      },
      {
        "paperId": "e57583e5c4e9bd472f1e229a7a5017216e296f9b",
        "title": "TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "899f6c5ee479aa81d66e94812e3fa84a06a8cf7d",
        "title": "Discovering Preference Optimization Algorithms with and for Large Language Models"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "2852426a094360457a5aef545c53d0f9d5a3732f",
        "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning"
      },
      {
        "paperId": "2a8b65910598873b8125fffb73cee7c1510e4b35",
        "title": "When is Off-Policy Evaluation (Reward Modeling) Useful in Contextual Bandits? A Data-Centric Perspective"
      },
      {
        "paperId": "3ebe7c1a9f8a12cdfaf863069a253c2c31940ac6",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "d330bb7c578c7e4de7566d0c958a41e0935c9721",
        "title": "Improving LLM Generation with Inverse and Forward Alignment: Reward Modeling, Prompting, Fine-Tuning, and Inference-Time Optimization"
      }
    ],
    "score": 50.0
  },
  {
    "id": "6366cb50a5e2043b2bca11a8f03005c42b036c3e",
    "title": "Multi-turn Reinforcement Learning from Preference Human Feedback",
    "authors": [
      "Lior Shani",
      "Aviv Rosenberg",
      "Asaf B. Cassel",
      "Oran Lang",
      "Daniele Calandriello",
      "Avital Zipori",
      "Hila Noga",
      "Orgad Keller",
      "Bilal Piot",
      "Idan Szpektor",
      "A. Hassidim",
      "Yossi Matias",
      "R\u00e9mi Munos"
    ],
    "year": 2024,
    "citationCount": 48,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.",
    "url": "https://www.semanticscholar.org/paper/6366cb50a5e2043b2bca11a8f03005c42b036c3e",
    "pdf_url": "https://arxiv.org/pdf/2405.14655.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-23",
    "externalIds": {
      "ArXiv": "2405.14655",
      "DBLP": "journals/corr/abs-2405-14655",
      "DOI": "10.48550/arXiv.2405.14655",
      "CorpusId": 269982866
    },
    "references": [
      {
        "paperId": "fef73ec58077f1cc6e4c1d7f04b4ce14341c9504",
        "title": "Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "1e672bf4d38a93c4c140ee208216425444368fa6",
        "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models"
      },
      {
        "paperId": "f2f9cc38d5a1a4fdef996ea03fdb71e01f65d574",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "7c25adf2ddb35df05a61c697da97efb8583d77df",
        "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "891edceb78a274b0c2494d8176bc4d6f6e3f9cbc",
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "0e41fd22d483047bd8fdb1757d90c7702493567e",
        "title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "81d6dc0a54531874e26521082242158ad0f6da21",
        "title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally"
      },
      {
        "paperId": "cca15babd21194df08267908713035c34a4441b8",
        "title": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback"
      },
      {
        "paperId": "b772aa052b7c24877b85a95d9b0b4387ad52cb81",
        "title": "Online Markov Decision Processes with Aggregate Bandit Feedback"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "8e79043b89f007bd81d65244345b50f7bff7fca9",
        "title": "Reinforcement Learning with Trajectory Feedback"
      },
      {
        "paperId": "cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544",
        "title": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "08eb40da621640e14ffac36c5e2595d7c0250541",
        "title": "Mirror Descent Policy Optimization"
      },
      {
        "paperId": "59b1d64cbf24d9c9d990d2d053ea94c52c83fbca",
        "title": "Optimistic Policy Optimization with Bandit Feedback"
      },
      {
        "paperId": "83fbf14897f7d4aaf4552db01c902f55e7fdddb7",
        "title": "A Modern Introduction to Online Learning"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "a2ea78ea760e056eb5c7d4bc27e31ad066736c18",
        "title": "Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs"
      },
      {
        "paperId": "1171d2a1c416fb59c3a7b3e05b12618ae46cd646",
        "title": "Online Convex Optimization in Adversarial Markov Decision Processes"
      },
      {
        "paperId": "b3b3d1d6d36ac203cd06c00bb37e66c000430275",
        "title": "A Theory of Regularized Markov Decision Processes"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
        "title": "Deep Reinforcement Learning for Dialogue Generation"
      },
      {
        "paperId": "5e838b1d5a0dc3454dad082b2ca6b9bf301bd25c",
        "title": "Online Markov Decision Processes"
      },
      {
        "paperId": "4f0df7ed89d9a5bb5bafa5727ccdc0b6e2fb463d",
        "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization"
      },
      {
        "paperId": "522da3897fedc49e64a31ded9a20c7b740c1f619",
        "title": "On general minimax theorems"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "90d88e38b1fc555012394824d7e9a36171fc0d23",
        "title": "Zur Theorie der Gesellschaftsspiele"
      },
      {
        "paperId": "bcc91889adc389cbe25295961c07a3484225ee7b",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "ca7b640cbf9008149f11f9b435b3304758eb57f1",
        "title": ": Student"
      },
      {
        "paperId": "e300627dad2f99b2e5da578d22fe4a354a88dcfd",
        "title": "Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function"
      },
      {
        "paperId": "c3529f2c36240d50e15c99164a3954f4d26e1c62",
        "title": "AirDialogue: An Environment for Goal-Oriented Dialogue Research"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": null,
        "title": "Aliprantis and"
      },
      {
        "paperId": "29f8b7112fb4660a64df8ac34a5d85c119019efe",
        "title": "Problem Complexity and Method Efficiency in Optimization"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ],
    "cited_by": [
      {
        "paperId": "2f11fdb66ca5d382f3b0ca763c2d13cde9e6f522",
        "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models"
      },
      {
        "paperId": "b465a2dc3cb4525b83cc56b07c68a3634792a58f",
        "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning"
      },
      {
        "paperId": "c89873a13ec6ed1c2ac70806fa30ad19f7be6ebf",
        "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs"
      },
      {
        "paperId": "7f72fc249b4eca634b25b9a3263422adfe607491",
        "title": "POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization"
      },
      {
        "paperId": "9f251ca2f98310e80854d751683baf9466222fc7",
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning"
      },
      {
        "paperId": "1ef29b16fdca0122deb2ff217bb42a57e6fadc55",
        "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues"
      },
      {
        "paperId": "27855928029f30b91167f79c07ef533c59a480f3",
        "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning"
      },
      {
        "paperId": "4b43dc6374b9a528362571d5e25f9a0850cfdb7a",
        "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs"
      },
      {
        "paperId": "177cb74c055004b6b3d1d54a223cfc1d95c3e6dd",
        "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice"
      },
      {
        "paperId": "3a7e649794e41bb15acd60283a5d861d18cc0c8f",
        "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning"
      },
      {
        "paperId": "da39b537216415819c92f7b82c7776b89388c253",
        "title": "Dynamic Retrieval Strategy Optimization in Retrieval-Augmented Generation Based on User Feedback"
      },
      {
        "paperId": "c33e8d1dbcc41a8f91579f60fd01f2f0231d0ec3",
        "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier"
      },
      {
        "paperId": "21abbc6a6edde1b8bc2ceb7b7a7564a5a8e34c85",
        "title": "Reinforce LLM Reasoning through Multi-Agent Reflection"
      },
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "a4cfce7de08dc54ad876d777ee4851c3257e8482",
        "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback"
      },
      {
        "paperId": "07ceaac52a048dc99a01dfdf6ba2e43f0876e7ef",
        "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox"
      },
      {
        "paperId": "213949b621e40c3abcd9ad2becb20e65651cceb6",
        "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "d740834b4a00c60fb99cb243fd2b483d0a4659ca",
        "title": "ShiQ: Bringing back Bellman to LLMs"
      },
      {
        "paperId": "c778fad9acef0847c4f4e0f4ac033b7c4f1c217b",
        "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging"
      },
      {
        "paperId": "957786aba1fbf389a1273f9375a313edb30f6232",
        "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
      },
      {
        "paperId": "d1755c9aff0c3cccad45b9474676b5a700f5b0c1",
        "title": "Don't lie to your friends: Learning what you know from collaborative self-play"
      },
      {
        "paperId": "5b58cfebd95ba503cdf26a8021f4e7ca0893351d",
        "title": "Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback"
      },
      {
        "paperId": "726af902a8842a80bbd793ee5ff4a045feac10fe",
        "title": "Learning from Failures in Multi-Attempt Reinforcement Learning"
      },
      {
        "paperId": "96814a0576c703b021b1a7e7ccc17c776bf89b29",
        "title": "M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "aa89c6bf86486e180833037333555e3492b15c8e",
        "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning"
      },
      {
        "paperId": "93679036b6eb4293d42683940ce80aa589c9d321",
        "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent"
      },
      {
        "paperId": "6a208872304109fc7612dbafda91fb4323221155",
        "title": "Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees"
      },
      {
        "paperId": "33c8fde911526140a85cdfc654c8ee318f83906d",
        "title": "CollabLLM: From Passive Responders to Active Collaborators"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "a0f2f14afbf7b077d519bf7ca74f1e1d061a7d5a",
        "title": "Active RLHF via Best Policy Learning from Trajectory Preference Feedback"
      },
      {
        "paperId": "3f2f74dc638ff7e72bcf0a67d2b32e73e7d202ef",
        "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking"
      },
      {
        "paperId": "96c90a0c76f361fc45af33569d9f58aa3ea7350b",
        "title": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning"
      },
      {
        "paperId": "cec871374c8838229c10aca61e49c626ccd2fe00",
        "title": "PRISM: A Personalized, Rapid, and Immersive Skill Mastery framework for personalizing experiential learning through Generative AI"
      },
      {
        "paperId": "250d920ba5cf1e8dcb521eee47e181cf3eb3755a",
        "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models"
      },
      {
        "paperId": "9e91b085852906decb4052ea198dd81a73ed6894",
        "title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization"
      },
      {
        "paperId": "9bdce8ff12b2546c52363139b24806bf543c9f25",
        "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF"
      },
      {
        "paperId": "36b6edf59f64ff9c203f84c5df48774bb62bc79c",
        "title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity"
      },
      {
        "paperId": "c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb",
        "title": "Training Language Models to Self-Correct via Reinforcement Learning"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "23fc4e3e553d5404b30445a6ec058e3af67c55e9",
        "title": "LLM-based Collaborative Agents with Pedagogy-guided Interaction Modeling for Timely Instructive Feedback Generation in Task-oriented Group Discussions"
      },
      {
        "paperId": "a4065e70b25bdf20b4587f8d1bbaaa7e4eeb9390",
        "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "71d68f772a7ef2e66d2170b8d695d11ba182a9d6",
        "title": "Scalably Solving Assistance Games"
      },
      {
        "paperId": "f34fb8260b5bd895dce775e8ec470164d5efe1e7",
        "title": "Less Details, But Be Thorough: Addressing Contradicting User Preferences in Multi-turn LLM-based Conversation"
      },
      {
        "paperId": "96ef1000643f2278039186bb4183254f68f6ffaf",
        "title": "20 Questions: Multi-turn RLHF for Sparse Rewards"
      }
    ],
    "score": 48.0
  },
  {
    "id": "b2991a4b2ecc9db0fbd9ca738022801b4e5ee001",
    "title": "CogBench: a large language model walks into a psychology lab",
    "authors": [
      "Julian Coda-Forno",
      "Marcel Binz",
      "Jane X. Wang",
      "Eric Schulz"
    ],
    "year": 2024,
    "citationCount": 46,
    "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",
    "url": "https://www.semanticscholar.org/paper/b2991a4b2ecc9db0fbd9ca738022801b4e5ee001",
    "pdf_url": "https://arxiv.org/pdf/2402.18225.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-28",
    "externalIds": {
      "ArXiv": "2402.18225",
      "DBLP": "conf/icml/Coda-FornoBWS24",
      "DOI": "10.48550/arXiv.2402.18225",
      "CorpusId": 268041290
    },
    "references": [
      {
        "paperId": "699d4f771013f85aba334ad5a0dcab36457409e9",
        "title": "Dynamic computational phenotyping of human cognition"
      },
      {
        "paperId": "cf50b51e98446de12e073fb909a53870307661a2",
        "title": "How should the advent of large language models affect the practice of science?"
      },
      {
        "paperId": "5eb5cad49de8a1f386f3dd127de2046eaa27ea5c",
        "title": "Prompting Frameworks for Large Language Models: A Survey"
      },
      {
        "paperId": "0786c88990235414611478099e43611542d973b0",
        "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models"
      },
      {
        "paperId": "73dd6fce2db3e34d1f87f8449b1e8bde78c31547",
        "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve"
      },
      {
        "paperId": "ff4acf33aeafcbe7d12afdc6bb9ca26537219658",
        "title": "Studying and improving reasoning in humans and machines"
      },
      {
        "paperId": "dc7de606ecf7c65abc26396629b633e18d8704d5",
        "title": "An Empirical Study of the Non-Determinism of ChatGPT in Code Generation"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "3f98cf521222c65522200037c0eb95a17081b2dd",
        "title": "Playing repeated games with large language models"
      },
      {
        "paperId": "19c63eade265d8a47d160098d97194b3b83d3770",
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases"
      },
      {
        "paperId": "286c3587f2616839286748461cbc90261ea49caf",
        "title": "Meta-in-context learning in large language models"
      },
      {
        "paperId": "de916e9e6f8841dbb73b35615f8d15eba2e9eaf7",
        "title": "The emergence of economic rationality of GPT"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "0857e8e6a84a672e64fb060532d6617887cb07f8",
        "title": "Rethink reporting of evaluation results in AI"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "9004924824cc0b1c840bcc91ba79475882623790",
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks"
      },
      {
        "paperId": "0bfc05adcddd4fe5d1335d96cc313c41526d4558",
        "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT"
      },
      {
        "paperId": "290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",
        "title": "Language models show human-like content effects on reasoning"
      },
      {
        "paperId": "fa3609e00f9f422a309c621a35394c4a38f88687",
        "title": "Using cognitive psychology to understand GPT-3"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "b6e933f57d2e842030e5175fc23516d9ea51f58d",
        "title": "The computational roots of positivity and confirmation biases in reinforcement learning"
      },
      {
        "paperId": "7ef9aafc68511afab5b287e62b754576ea37b4ce",
        "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks"
      },
      {
        "paperId": "341bdbcfc3febef7691a97c216ad394653211095",
        "title": "Can language models learn from explanations in context?"
      },
      {
        "paperId": "22168945c4e722edba32c000eff904cd873b4ce8",
        "title": "The globalizability of temporal discounting"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "846c511f17cb534f88e0f770ccb61277727dada1",
        "title": "Exploration beyond bandits"
      },
      {
        "paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770",
        "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models"
      },
      {
        "paperId": "016cf71ffcb999000502303cbcc334f8b1901e0e",
        "title": "Sources of Metacognitive Inefficiency"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "paperId": "5ef0bec8d10b8496902a1b9e1f926e3571cc2c6b",
        "title": "A theory of learning to infer"
      },
      {
        "paperId": "d305fba2db65bc791fa3e88e2e03e171e45051a0",
        "title": "Computational Phenotyping: Using Models to Understand Individual Differences in Personality, Development, and Mental Illness"
      },
      {
        "paperId": "ab7752d552a776ff931253bef71371ddf800a378",
        "title": "Domain-General Enhancements of Metacognitive Ability Through Adaptive Training"
      },
      {
        "paperId": "756eded65bf7b2946db9353df75bb6ef7dfeb6a5",
        "title": "Deconstructing the human algorithms for exploration"
      },
      {
        "paperId": "3a288c63576fc385910cb5bc44eaea75b442e62e",
        "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction"
      },
      {
        "paperId": "0f3fcaa694f61599829b4f2db1d85a4bc6e15578",
        "title": "Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
      },
      {
        "paperId": "864e84bc37b880c143976c0a98fb3297c492c744",
        "title": "Behavioural and neural characterization of optimistic reinforcement learning"
      },
      {
        "paperId": "93765a2a800b40fd2b237370e31e7f6fe70e9f2e",
        "title": "On the functional form of temporal discounting: An optimized adaptive test"
      },
      {
        "paperId": "2be81316d28b3683d388b951c8bb147ab2f61fcc",
        "title": "Humans use directed and random exploration to solve the explore-exploit dilemma."
      },
      {
        "paperId": "a2e0e3ba3831b3acaed5de5899241ebbccaa1016",
        "title": "How to measure metacognition"
      },
      {
        "paperId": "4eed3a5007ca2e6c43035c35eda2adb9a3dce905",
        "title": "Computational psychiatry"
      },
      {
        "paperId": "8e1f38b228b13a6669a684c1f0d0ae2defc5afa5",
        "title": "Model-based influences on humans\u2019 choices and striatal prediction errors"
      },
      {
        "paperId": "7ed99fc2e5e74f69fc168cf4634941e9f11f3e30",
        "title": "Trial"
      },
      {
        "paperId": "93b3f7a1999517d56d05dca85d868b4b8da60c33",
        "title": "Detecting Regime Shifts: The Causes of Under- and Over-Reaction"
      },
      {
        "paperId": "21214735152caf16e00768f67dde68bc7bd9ed43",
        "title": "Meta-cognitive Efficiency in Learned Value-based Choice"
      },
      {
        "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
        "title": "Are Emergent Abilities of Large Language Models a Mirage?"
      },
      {
        "paperId": "27c16cca907aa43397cc226a182b73b396c5cf66",
        "title": "Inducing anxiety in large language models increases exploration and bias"
      },
      {
        "paperId": null,
        "title": "Anatomical coupling be-tween distinct metacognitive systems for memory and visual perception"
      },
      {
        "paperId": "282f4922e82ecd8d6041f3442c3215291c49a650",
        "title": "Evaluation of a behavioral measure of risk taking: the Balloon Analogue Risk Task (BART)."
      },
      {
        "paperId": null,
        "title": "Introducing mpt-30b: Raising the bar for open-source foundation models"
      },
      {
        "paperId": null,
        "title": "Chain-of-thought prompting elicits"
      },
      {
        "paperId": null,
        "title": "Performance: The performance is the average points across all simulations"
      },
      {
        "paperId": null,
        "title": "The waluigi effect (mega-post)"
      },
      {
        "paperId": null,
        "title": "Behaviour - Model-basedness: To retrieve the model-basedness of an agent, we compute a regression using three regressors:"
      },
      {
        "paperId": null,
        "title": "Towards a transparent ai future: The call for less regulatory hurdles on open-source ai in eu-rope"
      },
      {
        "paperId": null,
        "title": "Falcon-40B: an open large language model with state-of-the-art performance"
      },
      {
        "paperId": null,
        "title": "Testing the limits of chain-of-thought with multistep"
      }
    ],
    "cited_by": [
      {
        "paperId": "e89e65c09509e760752aa1750ff3305178f6d78e",
        "title": "Unraveling the cognitive patterns of Large Language Models through module communities"
      },
      {
        "paperId": "ff37316a10f79dfec7197dc5163f16bfd3b7c30e",
        "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations"
      },
      {
        "paperId": "64227a31e5684096a4c6d91e2cbf518804e85bcf",
        "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models"
      },
      {
        "paperId": "b0d5cb6e5c6e33a7ab90cfb265c4a1b78e1c1bee",
        "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning"
      },
      {
        "paperId": "d43485b01d838020c9cb2d7022abef263316981a",
        "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs"
      },
      {
        "paperId": "0f278fdee1b16106fec879b0d2515a188445e6e1",
        "title": "H2HTalk: Evaluating Large Language Models as Emotional Companion"
      },
      {
        "paperId": "a4e64dee0d0f284edc160a9a8bc2b4385577249a",
        "title": "A foundation model to predict and capture human cognition"
      },
      {
        "paperId": "8ea7dab13511495dde87a26b645c35354be93d83",
        "title": "The cultural stereotype and cultural bias of ChatGPT"
      },
      {
        "paperId": "5cfd27d325e5fb76b5155e77879ddb66079a2c75",
        "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents"
      },
      {
        "paperId": "e4bcb3f40c952a639c2bd665f8754defd67aad5e",
        "title": "AI Agent Behavioral Science"
      },
      {
        "paperId": "f91870faf08dabfabcd751ce6a1ed0387d2d4832",
        "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets"
      },
      {
        "paperId": "378283f290f9e54b338004c3b32f1b95d227c6cd",
        "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems"
      },
      {
        "paperId": "44fe850bfc4f993afda3e1e525f89b1531382e86",
        "title": "Relative Value Encoding in Large Language Models: A Multi-Task, Multi-Model Investigation"
      },
      {
        "paperId": "922c5c2eb78da92c5a5ca3b9813694756c8e53d5",
        "title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers"
      },
      {
        "paperId": "8523d33b269b1bc6f94786ffb192f24a7961b18a",
        "title": "Memorization and Knowledge Injection in Gated LLMs"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "948487415a9bf8db2e3ea9f9cc428316e7a974fc",
        "title": "A Framework for Robust Cognitive Evaluation of LLMs"
      },
      {
        "paperId": "a771a2ce81dbbc56f9187c94c7d77cbf4dfc3ba7",
        "title": "Predicting Field Experiments with Large Language Models"
      },
      {
        "paperId": "0dd2512bf67699a01f1ec0d3b1240b774c5ebe8f",
        "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas"
      },
      {
        "paperId": "c29cd63e3ed392abbe562ada5ada3bd4e16c9880",
        "title": "Levels of Analysis for Large Language Models"
      },
      {
        "paperId": "0fa01ae4a8b28b5ccbb5cec9b3832efab0209cf1",
        "title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns"
      },
      {
        "paperId": "1147047241713c4e6dca1f5346789f22ee1dd4e4",
        "title": "On Benchmarking Human-Like Intelligence in Machines"
      },
      {
        "paperId": "05305460bbc69b679ad1a25181db022822e4dbcc",
        "title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs"
      },
      {
        "paperId": "7dc0fe0700c79cc02d78da20bd4f3fa034878d78",
        "title": "The potential - and the pitfalls - of using pre-trained language models as cognitive science theories"
      },
      {
        "paperId": "32793708a767ea49b88609ab4877a1f575de2f2a",
        "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning"
      },
      {
        "paperId": "9c6304a50696b6dc2da5e2035e7c08c4f2db3383",
        "title": "VideoCogQA: A Controllable Benchmark for Evaluating Cognitive Abilities in Video-Language Models"
      },
      {
        "paperId": "659d78d44ed1fab15ac2a7b8d81dd6f9641b2d7c",
        "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games"
      },
      {
        "paperId": "8b7a1271768f36a972d273fd43b3e7e7f3518391",
        "title": "Can Language Models Learn to Skip Steps?"
      },
      {
        "paperId": "ae04b8a2239bcc04449d106b5007b4c4aae62bbc",
        "title": "Can LLMs make trade-offs involving stipulated pain and pleasure states?"
      },
      {
        "paperId": "b24c88572e748baebb598ed61be713d56689e3b9",
        "title": "Large Language Model Benchmarks in Medical Tasks"
      },
      {
        "paperId": "008526bd675a55bd7007c3a343340b5a8a9abca3",
        "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse"
      },
      {
        "paperId": "ecf877300385cc8204609fc75aaee23c7da55864",
        "title": "TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles with Simulated Students"
      },
      {
        "paperId": "3e1b14f6bcc7a4b3c756585898d4f36cab64fd64",
        "title": "How Does Code Pretraining Affect Language Model Task Performance?"
      },
      {
        "paperId": "1d67e374db528795faee1aa0b9a09fda1b4fd576",
        "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges"
      },
      {
        "paperId": "8407a338773ed13489b40be782a4bd86915e5e91",
        "title": "Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture"
      },
      {
        "paperId": "a335b6443f1b77b47c0a1ba79fd21363448fe5f2",
        "title": "Evaluating AI Evaluation: Perils and Prospects"
      },
      {
        "paperId": "f2ae2926aa13e6640841a5d862cfaf74980b76cf",
        "title": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect"
      },
      {
        "paperId": "3aef92d1908eae912f8079ace0e5b78b4ddbf237",
        "title": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings"
      },
      {
        "paperId": "325b74b629e60115c9ef50f07efd6b15818dc0c0",
        "title": "Large Language Models Assume People are More Rational than We Really are"
      },
      {
        "paperId": "899d30a3796b377e01866fac1e9be037590471bb",
        "title": "M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark"
      },
      {
        "paperId": "30d87ce76db313bf0650951bdae45c39d525e05f",
        "title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice"
      },
      {
        "paperId": "a904a341583a78b8edfec9fd81d2ce9147bbe98e",
        "title": "Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning"
      },
      {
        "paperId": "c960ef199bef146b630fa78fa3918dcd8d1c7d44",
        "title": "Large Language Models are Biased Reinforcement Learners"
      },
      {
        "paperId": "d492c004b74dcbec9d5c0732d95bbd3588e30451",
        "title": "Can large language models explore in-context?"
      },
      {
        "paperId": "affb54e9f6537a0dd19365bf437f07efc32f5398",
        "title": "Machine Psychology"
      },
      {
        "paperId": "1a56c8a04ed797652441c1346468b82c03aa8348",
        "title": "How Does Code Pretraining A\ufb00ect Language Model Task Performance?"
      }
    ],
    "score": 46.0
  },
  {
    "id": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
    "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
    "authors": [
      "Shicong Cen",
      "Jincheng Mei",
      "Katayoon Goshvadi",
      "Hanjun Dai",
      "Tong Yang",
      "Sherry Yang",
      "D. Schuurmans",
      "Yuejie Chi",
      "Bo Dai"
    ],
    "year": 2024,
    "citationCount": 44,
    "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations. In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.",
    "url": "https://www.semanticscholar.org/paper/a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
    "pdf_url": "https://arxiv.org/pdf/2405.19320.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-05-29",
    "externalIds": {
      "ArXiv": "2405.19320",
      "DBLP": "journals/corr/abs-2405-19320",
      "DOI": "10.48550/arXiv.2405.19320",
      "CorpusId": 270095172
    },
    "references": [
      {
        "paperId": "e6d54edfd27593b6facbddf92aa2f4292d1d8d71",
        "title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games"
      },
      {
        "paperId": "00c60010949bef569eea055af1ccef8153451408",
        "title": "Faster WIND: Accelerating Iterative Best-of-N Distillation for LLM Alignment"
      },
      {
        "paperId": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "187c1466b00069969c8c44e2491e31d7aac09a4a",
        "title": "Mathematical Analysis of Machine Learning Algorithms"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "cab74f898a3523f1ceb8fe48dcda8ec467a3d95b",
        "title": "Provable Offline Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
        "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
      },
      {
        "paperId": "fc70db46738fff97d9ee3d66c6f9c57794d7b4fa",
        "title": "A survey of uncertainty in deep neural networks"
      },
      {
        "paperId": "e2ad21dae85950ab3631f65a0f142924c99fb9c4",
        "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning"
      },
      {
        "paperId": "43d0f879f7ae59d9bf7b6275905a928a74224dd7",
        "title": "The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces"
      },
      {
        "paperId": "91edb1532effc929e785812f54d601e43c6515f2",
        "title": "On the Optimality of Batch Policy Optimization Algorithms"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "574dcce887496e251382df6689cc8622a3c480ae",
        "title": "Reward Biased Maximum Likelihood Estimation for Reinforcement Learning"
      },
      {
        "paperId": "85c04e7b770d114b3b2cf08b674e63c5a9d8d729",
        "title": "Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits"
      },
      {
        "paperId": "5a29de624baa172236f3f544f3c5726293fe9399",
        "title": "Reward-Biased Maximum Likelihood Estimation for Linear Stochastic Bandits"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "a56776af2884d1e5bc0515c3dc49288b87c0a2ef",
        "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "11026fc42c4d30f9ea91ec8c32f8d75768b70f6d",
        "title": "Boltzmann Exploration Done Right"
      },
      {
        "paperId": "96a067e188f1c89db9faea1fea2314a15ae51bbc",
        "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "b4ee2aceba9d0293655c47f1bcb29a91b05c75a5",
        "title": "A new family of optimal adaptive controllers for Markov chains"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "e4435f282266da92d37066064c5239c6f96f0d64",
        "title": "Gibbs Sampling from Human Feedback: A Provable KL- constrained Framework for RLHF"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      },
      {
        "paperId": null,
        "title": "iii) VPO admits a practically-implementable form suitable for RLHF on LLMs, and more generally, deep-learning architectures"
      }
    ],
    "cited_by": [
      {
        "paperId": "435de15da2ce37a97e0f2731beb10ff9765746d5",
        "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "59c6403b4445fcdc4e0cb918cb7aa080206432ba",
        "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences"
      },
      {
        "paperId": "b3b13107dd153f4de1ddfeec0a96998d70caf04b",
        "title": "Preference Robustness for DPO with Applications to Public Health"
      },
      {
        "paperId": "5458d07ae90e746e54a1f77906ae2258c42605e7",
        "title": "Repair-R1: Better Test Before Repair"
      },
      {
        "paperId": "985796f3006ffe41eecaa9096be68ccc2f1096c1",
        "title": "Statistical and Algorithmic Foundations of Reinforcement Learning"
      },
      {
        "paperId": "d5925e094acf86f5bd1b8169d21808afbc78c752",
        "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling"
      },
      {
        "paperId": "71728ff20af60f4d2a369bf41206301b8318255f",
        "title": "Tokenized Bandit for LLM Decoding and Alignment"
      },
      {
        "paperId": "2c96c423d0cd481953e50ded37fb04921af02f5e",
        "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
      },
      {
        "paperId": "e9e22a099d1e065c027a3bced40d7fd61f9f9e65",
        "title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model"
      },
      {
        "paperId": "15143892b7e8cded24304c1077a9f7d20de363dd",
        "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training"
      },
      {
        "paperId": "03e385b1a6fdb7bef5612feea44e44d5efd45bcc",
        "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression"
      },
      {
        "paperId": "b29ff2491b0365f1e3862c43197860eac9962db3",
        "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "e8a45c4e2af9839b8fab8b7ee3a5925be549762f",
        "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits"
      },
      {
        "paperId": "970775f59013ff6198a1a684dfe819c784d99d42",
        "title": "Online Knowledge Distillation with Reward Guidance"
      },
      {
        "paperId": "e2d64cacc9639ab176425055550c6b2139348942",
        "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "0383e4223c8ab97d6aa7b6a6397f1b3a446bed33",
        "title": "Mitigating Preference Hacking in Policy Optimization with Pessimism"
      },
      {
        "paperId": "baba33c46d2c8b3c1e68e31464b01cb1e0b48261",
        "title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning"
      },
      {
        "paperId": "653546ec8e98c4037fe56277eaff8666882eaadd",
        "title": "Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective"
      },
      {
        "paperId": "7c69de6b1108ce03ba32ca9a10c36925187a8160",
        "title": "RSPO: Regularized Self-Play Alignment of Large Language Models"
      },
      {
        "paperId": "e6d54edfd27593b6facbddf92aa2f4292d1d8d71",
        "title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games"
      },
      {
        "paperId": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
        "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
      },
      {
        "paperId": "15378f453102f627467d7b7603d270438dea1237",
        "title": "Design Considerations in Offline Preference-based RL"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
      },
      {
        "paperId": "53106a642a12b05753ebe9ffca62d8efb0670281",
        "title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models"
      },
      {
        "paperId": "980bbe792855b811efae0311b149d2f4030a5d58",
        "title": "Hybrid Preference Optimization for Alignment: Provably Faster Convergence Rates by Combining Offline Preferences with Online Exploration"
      },
      {
        "paperId": "00c60010949bef569eea055af1ccef8153451408",
        "title": "Faster WIND: Accelerating Iterative Best-of-N Distillation for LLM Alignment"
      },
      {
        "paperId": "e6fac5811e260466366f3a905076c33e252405ef",
        "title": "Optimal Design for Reward Modeling in RLHF"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "efae534e2a3e4b79fe4050fa3fd40d8c7ba9745b",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "33c9147cfacb6ce401fe45b57307b8e3eeeb16d6",
        "title": "Forward KL Regularized Preference Optimization for Aligning Diffusion Policies"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "928b3ef966cb0e1e9cff7e5e96d5df23c47c2d5a",
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "dd486be958a1edb007f72972eb0136b5fffcaa26",
        "title": "2025 Incentivize without Bonus: Provably E\ufb03cient Model-based Online Multi-agent RL for Markov Games"
      }
    ],
    "score": 44.0
  },
  {
    "id": "96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
    "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
    "authors": [
      "Shima Rahimi Moghaddam",
      "C. Honey"
    ],
    "year": 2023,
    "citationCount": 86,
    "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
    "url": "https://www.semanticscholar.org/paper/96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
    "pdf_url": "https://arxiv.org/pdf/2304.11490.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-04-22",
    "externalIds": {
      "DBLP": "journals/corr/abs-2304-11490",
      "ArXiv": "2304.11490",
      "DOI": "10.48550/arXiv.2304.11490",
      "CorpusId": 258298867
    },
    "references": [
      {
        "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
        "title": "Do Large Language Models Know What They Don't Know?"
      },
      {
        "paperId": "0857e8e6a84a672e64fb060532d6617887cb07f8",
        "title": "Rethink reporting of evaluation results in AI"
      },
      {
        "paperId": "9a3edb5c6b0e8c84c94ea99a9ab647b1209f650f",
        "title": "Why think step-by-step? Reasoning emerges from the locality of experience"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f",
        "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"
      },
      {
        "paperId": "9004924824cc0b1c840bcc91ba79475882623790",
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks"
      },
      {
        "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d",
        "title": "Dissociating language and thought in large language models: a cognitive perspective"
      },
      {
        "paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
        "title": "Teaching Small Language Models to Reason"
      },
      {
        "paperId": "311fd5f6f114ae51f8cbd95a0da69d7b556d25f1",
        "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs"
      },
      {
        "paperId": "e32185936ab3b23f39b1dd93e1507e6d80a71776",
        "title": "The debate over understanding in AI\u2019s large language models"
      },
      {
        "paperId": "6e02a7eedad079451b9a8dd358268727cf599c6e",
        "title": "Do Large Language Models know what humans know?"
      },
      {
        "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d",
        "title": "Large Language Models and the Reverse Turing Test"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "b83eb34b088b31118974f33109ce53a32209d73a",
        "title": "Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "68f141724814839d556a989646194be88641b143",
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
      },
      {
        "paperId": "d3aa93e3453ea2d5ea0e39324d8f2d9a0ac0aa43",
        "title": "Performance vs. competence in human\u2013machine comparisons"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "fd87692771c391088ba998967786fb6de5818249",
        "title": "Knowing me, knowing you: theory of mind in AI"
      },
      {
        "paperId": "13ba84fa8409a1a06078c495ef1e034abbe61bdc",
        "title": "Theory of mind in animals: Current and future directions."
      },
      {
        "paperId": "82d14f2d33cde6332481ed7432977017934f7ff8",
        "title": "A New Multiple Object Awareness Paradigm Shows that Imperfect Knowledge of Object Location Is Still Knowledge"
      },
      {
        "paperId": "875163fccd295fea65fd7183165fd66ef8653f58",
        "title": "The cultural evolution of mind reading"
      },
      {
        "paperId": "212301127d8826dc29424a5ddd74f6919820d012",
        "title": "Reading Literary Fiction Improves Theory of Mind"
      },
      {
        "paperId": "329cc9f7068a1aa0d47d842f5b7972ce9b6d5b39",
        "title": "Affiliation, empathy, and the origins of theory of mind"
      },
      {
        "paperId": "00c0c0fc4fe992189e719862e826a507a84e9fbf",
        "title": "fMRI item analysis in a theory of mind task"
      },
      {
        "paperId": "e504e5dfe3f6a5f7f4f6ee54592335b807c090b5",
        "title": "Impaired theory of mind for moral judgment in high-functioning autism"
      },
      {
        "paperId": "0b50e7833424005a2bbc8a69b1ca80c2d7b3d217",
        "title": "Growing up blind does not change the neural bases of Theory of Mind"
      },
      {
        "paperId": "2d49e18f41d4c0d14a610691a054c978365c056d",
        "title": "The neural basis of the interaction between theory of mind and moral judgment"
      },
      {
        "paperId": "c73b58599d7dbbb3db987507218a67779dfc0c09",
        "title": "In two minds: dual-process accounts of reasoning"
      },
      {
        "paperId": "db6a5008ac8b28b935ccca3bb76d6bc4b3389d50",
        "title": "Theory of mind: evolutionary history of a cognitive specialization"
      },
      {
        "paperId": "464c3a3512d5bde8078185114f38777843d88256",
        "title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models"
      },
      {
        "paperId": null,
        "title": "GPT Models Documentation"
      },
      {
        "paperId": null,
        "title": "Sparks of Artificial General Intelligence: Experiments with an early version of GPT-4"
      },
      {
        "paperId": null,
        "title": "Introducing ChatGPT"
      },
      {
        "paperId": null,
        "title": "Sargent famously painted the south bank of the river in 1885"
      },
      {
        "paperId": null,
        "title": "Accounts of the country's economic success were recorded in books from the early 1900s. Soon after, a horrible plague hit the country, and the country was sent into an economic depression."
      },
      {
        "paperId": null,
        "title": "A small leaf was placed on a wet clay flower pot. When the pot was baked at high temperatures to harden the clay, the leaf crumbled, but its impression remained."
      },
      {
        "paperId": null,
        "title": "A map shows the ground floor plan. A photocopy was sent to the architect yesterday, but at the time the kitchen door was missing. It was added to the map this morning."
      },
      {
        "paperId": null,
        "title": "Susie parked her sports car in the driveway. In the middle of the night, Nathan moved her car into the garage to make room for his minivan. Susie woke up early in the morning."
      },
      {
        "paperId": null,
        "title": "A popular attraction in the park, pictured on many souvenirs, was a cliff face covered with ancient petroglyphs. Recently, the petroglyphs crumbled and scientists have not begun to restore them."
      },
      {
        "paperId": null,
        "title": "Every day Jill goes to the coffee shop on the corner and orders a latte, her favorite drink. Today, the cashier misunderstands Jill and prepares a mocha instead."
      },
      {
        "paperId": null,
        "title": "Lisa was running late for work, so she quickly grabbed her lunch out of the fridge and left the house"
      },
      {
        "paperId": null,
        "title": "The family's old video tape recorded the daughter's first birthday party at their house in Chicago. Since then, the family sold their house and moved to San Francisco."
      },
      {
        "paperId": null,
        "title": "Lucy was baking a cake for her friend's birthday. She put the cake in the oven and set a 20-minute timer. She then went to the living room to watch her favorite show for 30 minutes"
      },
      {
        "paperId": null,
        "title": "Sally and Greg called ahead of time to make a reservation for the back-country cabin. The park ranger forgot to write down the reservation and two other hikers got to the cabin first"
      },
      {
        "paperId": null,
        "title": "Jenny put her chocolate away in the cupboard. Then she went outside. Alan moved the chocolate from the cupboard into the fridge. Half an hour later, Jenny came back inside."
      },
      {
        "paperId": null,
        "title": "A large oak tree stood in front of City Hall from the time the building was built. Last year the tree fell down and was replaced by a stone fountain."
      },
      {
        "paperId": null,
        "title": "Part of the garden is supposed to be reserved for the roses; it's labeled accordingly. Recently the garden has run wild, and dandelions have taken over the entire flower bed."
      },
      {
        "paperId": null,
        "title": "Model index for researchers"
      },
      {
        "paperId": null,
        "title": "Amy walked to work today. When George woke up, he saw her car in the drive. Her room was quiet and dark. George knows that when Amy is sick, she lies down in a dark room."
      },
      {
        "paperId": null,
        "title": "A volcano erupted on a Caribbean island three months ago. Barren lava rock is all that remains today. Satellite photographs show the island as it was before the eruption."
      },
      {
        "paperId": null,
        "title": "Hopeful to catch a prize fish, George went fishing."
      },
      {
        "paperId": null,
        "title": "The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping."
      },
      {
        "paperId": null,
        "title": "Example 4: Sarah placed her high heel shoes under her dress before she went shopping, so she knows where she left them"
      },
      {
        "paperId": null,
        "title": "During a thunderstorm, Sarah closed all the windows in her house. She then went to the basement to do laundry. While she was downstairs, a tree branch fell and broke one of the windows upstairs."
      },
      {
        "paperId": null,
        "title": "A photograph was taken of an apple hanging on a tree branch. The film took half an hour to develop. In the meantime, a strong wind blew the apple to the ground."
      },
      {
        "paperId": null,
        "title": "When Lisa left Jacob, he was deep asleep on the beach. A few minutes later a wave woke him. Seeing Lisa was gone, Jacob decided to go swimming."
      },
      {
        "paperId": null,
        "title": "John told Mary that he had lost his keys. The two of them searched the house with no luck. Then Mary went outside to look in the car. Suddenly John noticed his keys behind the sofa."
      },
      {
        "paperId": null,
        "title": "When Jeff got ready this morning, he put on a light pink shirt instead of a white one. Jeff is colorblind, so he can't tell the difference between subtle shades of color."
      },
      {
        "paperId": null,
        "title": "A long time ago, an explorer mapped a small island. Since then, the water levels rose and only a tiny part of the island is now left above water."
      },
      {
        "paperId": null,
        "title": "Tim and Amy went on a hike through the forest. After walking for several hours, they came across a bear. They quickly and quietly backed away, retracing their steps back to the starting point."
      },
      {
        "paperId": null,
        "title": "At night a bear broke into a cooler near a tent and drank the soda. Five hours later, the campers woke up and went to their cooler for breakfast."
      },
      {
        "paperId": null,
        "title": "At the time a portrait was drawn of a young man, he had short brown hair and no facial hair. Now the man's hair is long and gray and so is his beard."
      },
      {
        "paperId": null,
        "title": "Laura didn't have time to braid her horse\u2019s mane before going to camp. While she was at camp, William brushed Laura's horse and braided the horse\u2019s mane for her."
      },
      {
        "paperId": null,
        "title": "Larry chose a debated topic for his class paper due on Friday. The news on Thursday indicated that the debate had been solved, but Larry never read it"
      },
      {
        "paperId": null,
        "title": "She realized she was out of sugar and went to the store to buy some. While she was gone, her dog, Max, jumped on the counter and ate the cookie dough"
      },
      {
        "paperId": null,
        "title": "The girl's middle school pictures showed her wearing a white blouse. Later"
      },
      {
        "paperId": null,
        "title": "Just as she was setting the table, she accidentally knocked over a glass of red wine, spilling it all over the food. She ordered pizza as a last-minute replacement"
      },
      {
        "paperId": null,
        "title": "When the picture was taken of the house, it was one story tall. Since then, the renovators added an additional story and a garage."
      },
      {
        "paperId": null,
        "title": "To detect intruders, the lab uses an automated system for recording voices. In the empty lab one night, a computer error occurs and a synthetic voice reads the error message."
      }
    ],
    "cited_by": [
      {
        "paperId": "ebbbf35e967c5564149e47908ccc09d568287bae",
        "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions"
      },
      {
        "paperId": "da416c9d950f99581f59baebd5507766f5db6ef9",
        "title": "Exploring a Gamified Personality Assessment Method through Interaction with LLM Agents Embodying Different Personalities"
      },
      {
        "paperId": "6c35bc9be105ed59faf920e9d0f05f6aaa67c2ef",
        "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs"
      },
      {
        "paperId": "97b64825cb5bca3d3a6d5910efd7c1182a542a01",
        "title": "Language Models use Lookbacks to Track Beliefs"
      },
      {
        "paperId": "94338b48816494076d0e128d74fae194f0ab4e3f",
        "title": "The impact of individual AI proficiency on human\u2013agent collaboration: Higher sensitivity to discern the comprehension ability of intelligent agents for users with higher AI proficiency levels"
      },
      {
        "paperId": "8d84f9324071899d1e4b8cd6fc10423493868c63",
        "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind"
      },
      {
        "paperId": "a17273b4f6120f7cb0c1f4f11fc04e189d662284",
        "title": "Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models"
      },
      {
        "paperId": "d00972677e180c5a6fa70082b5853774904903e2",
        "title": "Advanced NLP Framework for Analyzing Elasticity Feedback in Apparel Design"
      },
      {
        "paperId": "4184d23d819b11c8c7336284e7f21e88d3b7bdad",
        "title": "Towards Personalized Conversational Sales Agents : Contextual User Profiling for Strategic Action"
      },
      {
        "paperId": "8a6949442c4997f6de1327ed131cd6a7dd12039d",
        "title": "DancingBoard: Streamlining the Creation of Motion Comics to Enhance Narratives"
      },
      {
        "paperId": "0bfcdbdfd063797dc02d994c431e952d4045a093",
        "title": "Re-evaluating Theory of Mind evaluation in large language models"
      },
      {
        "paperId": "0ece54fbd88cdf69296ecd3ab0d116299ac7b8b6",
        "title": "Towards properly implementing Theory of Mind in AI systems: An account of four misconceptions"
      },
      {
        "paperId": "2fd38639841e8d4baf7393d56d6fa51c5aba4e52",
        "title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma"
      },
      {
        "paperId": "3cc53a6aaac89e3c75771a9ef85c4c3948b8dddf",
        "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection"
      },
      {
        "paperId": "267c7f80f915ceb1f9aff887050943a834b9a1f6",
        "title": "Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models"
      },
      {
        "paperId": "0852af290f29c2f7871f6900f2ea6e288ea5fa32",
        "title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting"
      },
      {
        "paperId": "f48ae73bcda434be2a650af2b66920cd1d75dc22",
        "title": "Large Language Model Applied in Multi-agent SystemA Survey"
      },
      {
        "paperId": "71b4046f00521d2412fbca2cda5fcce895371e99",
        "title": "Large Language Models as a Tool for Mining Object Knowledge"
      },
      {
        "paperId": "dcf93eb9848d4a81e607c78637f7954e889ddade",
        "title": "Humanity in AI: Detecting the Personality of Large Language Models"
      },
      {
        "paperId": "0789ab8a994e6de7a1971da91b7c028fccd8e90e",
        "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing"
      },
      {
        "paperId": "a20fee4ee551a374d2eb3314aa29d107a20cd0e8",
        "title": "What You Need is what You Get: Theory of Mind for an LLM-Based Code Understanding Assistant"
      },
      {
        "paperId": "29d97178ba07bf3ab0d9867a2c36c203daea2dc6",
        "title": "Context Conquers Parameters: Outperforming Proprietary Llm in Commit Message Generation"
      },
      {
        "paperId": "d625c0c24246df86885b09e960147c384d0a4e9e",
        "title": "Evaluating and Enhancing LLMs Agent Based on Theory of Mind in Guandan: A Multi-Player Cooperative Game Under Imperfect Information"
      },
      {
        "paperId": "10f5a5e9985d1a1f6658da5ebc55080fcc26319a",
        "title": "Identification and Description of Emotions by Current Large Language Models"
      },
      {
        "paperId": "d6bd903ccfa66a08c73a8c01d54d4d133246d6fe",
        "title": "Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models"
      },
      {
        "paperId": "8afaf576273a729ddbcd313c4a871709e9500320",
        "title": "The origin and function of external representations"
      },
      {
        "paperId": "7c649618873fb6e72261183507a09324c722e788",
        "title": "InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context"
      },
      {
        "paperId": "0fba1346d415bbf2d0dad54687ec6e166a9144fa",
        "title": "The Social Cognition Ability Evaluation of LLMs: A Dynamic Gamified Assessment and Hierarchical Social Learning Measurement Approach"
      },
      {
        "paperId": "6dd415a07a6c304a78e191bbb09f50ee04c03d89",
        "title": "A Notion of Complexity for Theory of Mind via Discrete World Models"
      },
      {
        "paperId": "ea983bce3002ba3586daaf4fdbb18b79a5f38f68",
        "title": "Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large Language Models"
      },
      {
        "paperId": "89b91bd0ee16ab97a0c2d45aa710bdc54a3a09aa",
        "title": "Comparing Humans and Large Language Models on an Experimental Protocol Inventory for Theory of Mind Evaluation (EPITOME)"
      },
      {
        "paperId": "d2649e3397c1f7971eb4367c302a928c4a6d568a",
        "title": "Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game"
      },
      {
        "paperId": "6fa8fb210cf40464c605268756c92d8e6640a8ff",
        "title": "ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic Executors in Large Language Models"
      },
      {
        "paperId": "4ff598828d16fae431ca88045162720e905bb0ee",
        "title": "SolMover: Smart Contract Code Translation Based on Concepts"
      },
      {
        "paperId": "84d5d280f726ddf94ba85621581c4fe5f243a04f",
        "title": "Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation"
      },
      {
        "paperId": "ce3b187063b8bed3f54e35c888a25c6f67fe4d70",
        "title": "The general intelligence of GPT\u20134, its knowledge diffusive and societal influences, and its governance"
      },
      {
        "paperId": "7e388155314475226bd001b84d8a9a69f35021a7",
        "title": "Language Models Represent Beliefs of Self and Others"
      },
      {
        "paperId": "798feda076ad710df65d509a7884bd15937c8056",
        "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs"
      },
      {
        "paperId": "18155f3818e12f33d926f180b92a5c1f68e22f93",
        "title": "MMToM-QA: Multimodal Theory of Mind Question Answering"
      },
      {
        "paperId": "c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
        "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models"
      },
      {
        "paperId": "c4c1c1d7312c7be6ad000f84201a72494dea33d3",
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning"
      },
      {
        "paperId": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
        "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"
      },
      {
        "paperId": "dde6c1910d0496c9e5d5483c2c18271a2a660e6c",
        "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding"
      },
      {
        "paperId": "b8ca663060b8537054193833b6fba9bd06d0493b",
        "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models"
      },
      {
        "paperId": "8ce31d72dcfcd888015646b15f201d49aa71c648",
        "title": "Unpacking the Ethical Value Alignment in Big Models"
      },
      {
        "paperId": "2361bae8f0ff3627a91408c172e6612b4d554cf2",
        "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models"
      },
      {
        "paperId": "088a5fa00ed6c14351209da5f53e770b51fd2909",
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions"
      },
      {
        "paperId": "ed7172b60ad1dc21d27243d99692198c261c3047",
        "title": "Probing the Creativity of Large Language Models: Can models produce divergent semantic association?"
      },
      {
        "paperId": "e17c58d7a48b6b811df023484161a3b9c03e0d6b",
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models"
      },
      {
        "paperId": "c2cd20f162420afc739979dbb3bc900d760e38a7",
        "title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models"
      },
      {
        "paperId": "3aa13d6313d49dc634b28bde15fca4d3d3032694",
        "title": "An evolutionary model of personality traits related to cooperative behavior using a large language model"
      },
      {
        "paperId": "cc3b700fda68846d1551b067283689d7af6cf0a9",
        "title": "A New Approach to Web Application Security: Utilizing GPT Language Models for Source Code Inspection"
      },
      {
        "paperId": "9977fee41d9cce1b2ed924da966140ac8120762b",
        "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval"
      },
      {
        "paperId": "e10eeddc2672909c2ae4ec98db23e9eb446b4a22",
        "title": "Evaluation of large language models for discovery of gene set function"
      },
      {
        "paperId": "c608fb120ff2de5b2ed25b02731ec092882d7cf8",
        "title": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?"
      },
      {
        "paperId": "916c798b34d55a78c47ae44906fe018a60a30613",
        "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4"
      },
      {
        "paperId": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"
      },
      {
        "paperId": "0bb8c92fb8f77ec2f8d5fb093b574de6108fcb68",
        "title": "Large Language Models (LLMs) and Empathy - A Systematic Review"
      },
      {
        "paperId": "af8e2bdeba90900f1f52919fd579815c4fe3655b",
        "title": "Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings"
      },
      {
        "paperId": "aa5993501f4d0036d310e0292dea20e2c60b6c8f",
        "title": "Deception abilities emerged in large language models"
      },
      {
        "paperId": "aeb032b29956783aafa0ec38a3989a3456129218",
        "title": "What Should Data Science Education Do with Large Language Models?"
      },
      {
        "paperId": "5a714e920688a3a9296efa0cfa531de263a93de3",
        "title": "OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning"
      },
      {
        "paperId": "f590cbb28e4994f62e94bf9400a9cb33e99922fa",
        "title": "Understanding Social Reasoning in Language Models with Language Models"
      },
      {
        "paperId": "58fce438f817b46d37d072f8af7dfa4fd2dcd866",
        "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization"
      },
      {
        "paperId": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad",
        "title": "Large language models as tax attorneys: a case study in legal capabilities emergence"
      },
      {
        "paperId": "3f98cf521222c65522200037c0eb95a17081b2dd",
        "title": "Playing repeated games with large language models"
      },
      {
        "paperId": "784fd32a8f66640a31813f78fbe25b851cb7ea00",
        "title": "Does ChatGPT have Theory of Mind?"
      },
      {
        "paperId": "9702aa281204e7a692fb3ecc83981198426ff70d",
        "title": "Exploring Human-Like Translation Strategy with Large Language Models"
      },
      {
        "paperId": "b6ccdd0eb776eee6b317d235e457f20175f380ff",
        "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic"
      },
      {
        "paperId": "affb54e9f6537a0dd19365bf437f07efc32f5398",
        "title": "Machine Psychology"
      },
      {
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning"
      },
      {
        "paperId": "c4d65688c54154e01bb4fc18e4a58ef4ed6ea46b",
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework"
      },
      {
        "paperId": "bff499d51b002fd0b1aa05ba151a4a515e5bf36f",
        "title": "Emotional intelligence of Large Language Models"
      },
      {
        "paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",
        "title": "Reasoning with Language Model Prompting: A Survey"
      },
      {
        "paperId": "1b0c3f9b6fc16883eea1691d76411924e4dd53cf",
        "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents"
      },
      {
        "paperId": "ea052da88d817ea2c7b13b6706b2124965936769",
        "title": "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning"
      },
      {
        "paperId": "e3196507319c2fedf1b8d08573a1036615f3e25d",
        "title": "Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning"
      },
      {
        "paperId": "e3dfeb8be76960036fbb4439e7cff4b9c7184998",
        "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind"
      },
      {
        "paperId": "064422a3713b96acc173d6bbcdfc6b6b15e3f5b7",
        "title": ": Leveraging Collective Human Intelligence to Study Large Language Models"
      },
      {
        "paperId": "a1cc7b6819e9aafce1813f0aa76863d74e91b671",
        "title": "Evaluating Cognitive Maps in Large Language Models with CogEval: No Emergent Planning"
      },
      {
        "paperId": "4096ef74e800883ebfbb9c8e28808e4d4369f88e",
        "title": "Interactional Co-Creativity of Human and AI in Analogy-Based Design"
      },
      {
        "paperId": "9365e0bc69f7212a9811197a0d2b7886ebc237e9",
        "title": "Data Science and Arti\ufb01cial Intelligence Emergent Theory of Mind in Large Language Models"
      },
      {
        "paperId": "e0343c9b951d894233bde9680eeaa79829e04c4c",
        "title": "Language Model Evaluation Based on Korean-English Empathetic Dialogue Datasets and Personality"
      },
      {
        "paperId": "b69b2fc694dffd19535dbe06ec6fe263a5cbb055",
        "title": "1. EXPLORING RESEARCH-RELATED DESIGN: A COMPREHENSIVE INFORMATION SYSTEM FOR KNOWLEDGE PRODUCTION \u2014 ONTOCHATGPT"
      },
      {
        "paperId": "d11bd83c9d6c909c0b0fdf43ccdc2530c2d85aca",
        "title": "Do LLMs selectively encode the goal of an agent\u2019s reach?"
      },
      {
        "paperId": "3008c23d61c2f125e41a38b8e54aa71fc0fbefbe",
        "title": "Aspect Based Emotion Analysis for Dialogue Understanding"
      }
    ],
    "score": 43.0
  },
  {
    "id": "420af69e5ae3686b709c14a8cec7dc9f90a85681",
    "title": "ChatGPT: perspectives from human\u2013computer interaction and psychology",
    "authors": [
      "Jiaxi Liu"
    ],
    "year": 2024,
    "citationCount": 37,
    "abstract": "The release of GPT-4 has garnered widespread attention across various fields, signaling the impending widespread adoption and application of Large Language Models (LLMs). However, previous research has predominantly focused on the technical principles of ChatGPT and its social impact, overlooking its effects on human\u2013computer interaction and user psychology. This paper explores the multifaceted impacts of ChatGPT on human\u2013computer interaction, psychology, and society through a literature review. The author investigates ChatGPT\u2019s technical foundation, including its Transformer architecture and RLHF (Reinforcement Learning from Human Feedback) process, enabling it to generate human-like responses. In terms of human\u2013computer interaction, the author studies the significant improvements GPT models bring to conversational interfaces. The analysis extends to psychological impacts, weighing the potential of ChatGPT to mimic human empathy and support learning against the risks of reduced interpersonal connections. In the commercial and social domains, the paper discusses the applications of ChatGPT in customer service and social services, highlighting the improvements in efficiency and challenges such as privacy issues. Finally, the author offers predictions and recommendations for ChatGPT\u2019s future development directions and its impact on social relationships.",
    "url": "https://www.semanticscholar.org/paper/420af69e5ae3686b709c14a8cec7dc9f90a85681",
    "pdf_url": "https://doi.org/10.3389/frai.2024.1418869",
    "venue": "Frontiers Artif. Intell.",
    "publicationDate": "2024-06-18",
    "externalIds": {
      "PubMedCentral": "11217544",
      "DBLP": "journals/frai/Liu24",
      "DOI": "10.3389/frai.2024.1418869",
      "CorpusId": 270610217,
      "PubMed": "38957452"
    },
    "references": [
      {
        "paperId": "c8a6aa5d4ba18f3e38f8de27aa17d6fcfb33b89f",
        "title": "Exploring ChatGPT and its Impact on Society"
      },
      {
        "paperId": "732d128b70f6c601db69850f80dce8e690a5f97f",
        "title": "ChatGPT and mental health: Friends or foes?"
      },
      {
        "paperId": "d0455ce1815866bf6657bddfd64c0eba73239785",
        "title": "The scholarly footprint of ChatGPT: a bibliometric analysis of the early outbreak phase"
      },
      {
        "paperId": "b2a492ab1dc7636f2972349823881ed93100085d",
        "title": "Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study"
      },
      {
        "paperId": "9bfa64a0721e4ea7c6f7a04cf4dfd74c17594cd0",
        "title": "ChatGPT effects on cognitive skills of undergraduate students: Receiving instant responses from AI-based conversational large language models (LLMs)"
      },
      {
        "paperId": "c30b6afef04e4eaf27bd29d5cd025032afd86239",
        "title": "The beginning of ChatGPT \u2013 a systematic and bibliometric review of the literature"
      },
      {
        "paperId": "6a5659e0c064aa1d2c6b899acf7fcc9fcf2c98a5",
        "title": "Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines"
      },
      {
        "paperId": "a80e4cd6acc55907648b6cff172a58c94a3ac1ca",
        "title": "The End of the World as We Know It? ChatGPT and Social Work."
      },
      {
        "paperId": "1e0e7bcfedc5a4ced41cd1fe7d010ea9ec701f7d",
        "title": "ChatGPT: A brief narrative review"
      },
      {
        "paperId": "33de72ed0616f3c6022c3e4d36c69f34b916ee12",
        "title": "A social robot connected with chatGPT to improve cognitive functioning in ASD subjects"
      },
      {
        "paperId": "1ecf8b4362100aa0d1a36450186c6137be70e3f6",
        "title": "The Artificial Third: Utilizing ChatGPT in Mental Health"
      },
      {
        "paperId": "cea012c01f09ac382fe67ede5215a85175753487",
        "title": "The Social Impact of Generative AI: An Analysis on ChatGPT"
      },
      {
        "paperId": "2c5da7b7d52c322eea2be018cedc373db4b64297",
        "title": "The role of ChatGPT in disrupting concepts, changing values, and challenging ethical norms: a qualitative study"
      },
      {
        "paperId": "3cc896b9a9c4951510b6b9f28be930d65b24d29f",
        "title": "A Comprehensive Survey of ChatGPT: Advancements, Applications, Prospects, and Challenges."
      },
      {
        "paperId": "9493c5d7e9a0379aa57c517e22b7954e8283c72a",
        "title": "ChatGPT: The cognitive effects on learning and memory"
      },
      {
        "paperId": "d594fbd47848513d6c088f264bde1dcb8c39b5fb",
        "title": "Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment"
      },
      {
        "paperId": "ef6ff2a9f081937fc1fac47e5c187691e2bf982c",
        "title": "AIGC Challenges and Opportunities Related to Public Safety: A Case Study of ChatGPT"
      },
      {
        "paperId": "c696f1b7875089f1a72c283a7a70f216c29325bc",
        "title": "Integrating ChatGPT in Medical Education: Adapting Curricula to Cultivate Competent Physicians for the AI Era"
      },
      {
        "paperId": "84194b86655a2f118def311c517989c72fe21680",
        "title": "Impact of the Implementation of ChatGPT in Education: A Systematic Review"
      },
      {
        "paperId": "f6bb29e33d1763664e1c6b50167b27f10c0552c8",
        "title": "ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane"
      },
      {
        "paperId": "1062b007c09aa0b343b1c4b27ebf19177eaec9d2",
        "title": "Chatting with ChatGPT: decoding the mind of Chatbot users and unveiling the intricate connections between user perception, trust and stereotype perception on self-esteem and psychological well-being"
      },
      {
        "paperId": "367a161324a3b6d1b64b75bb8b44a797fc6a948b",
        "title": "Generative AI meets copyright"
      },
      {
        "paperId": "24725b0b327d4c9a141cb58d15502da2d688a159",
        "title": "Application of ChatGPT in Improving Customer Sentiment Analysis for Businesses"
      },
      {
        "paperId": "94a45955b0a1e86771e8d3cd867cd6553d418776",
        "title": "Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration"
      },
      {
        "paperId": "1d61410f21de30eec4193b06383897e883608a84",
        "title": "Generative AI for Business Decision-Making: A Case of ChatGPT"
      },
      {
        "paperId": "771660de64b37d871da33922fc3d78e314305b69",
        "title": "ChatGPT, a Product of AI, and its Influences in the Business World"
      },
      {
        "paperId": "2910b51022362c3d0f207c09972da737fac47f9a",
        "title": "Deceptive AI Ecosystems: The Case of ChatGPT"
      },
      {
        "paperId": "2a7fb97d670134e6dcda084822a2c32bcb854c88",
        "title": "Research on Criminal Risk Analysis and Governance Mechanism of Generative Artificial Intelligence such as ChatGPT"
      },
      {
        "paperId": "0d2c6a441c7d51225d9e0823e8f55e6b59868c4c",
        "title": "Chat-GPT: Opportunities and Challenges in Child Mental Healthcare"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb27e7e859e0cf29f2202dd9ac20adf4082e87db",
        "title": "The Role of Information Technology in Driving Innovation and Entrepreneurial Business Growth"
      },
      {
        "paperId": "18faafb15d7e7a8ad923b293f8c945c35680a2ef",
        "title": "ChatGPT and Open-AI Models: A Preliminary Review"
      },
      {
        "paperId": "94d96e6bbffa0d4c877f8ba49b706286f6366f2a",
        "title": "ChatGPT outperforms humans in emotional awareness evaluations"
      },
      {
        "paperId": "63f5c21f845b5f538f61e435b8f6e31c9d1c0a6a",
        "title": "A Survey on ChatGPT: AI\u2013Generated Contents, Challenges, and Solutions"
      },
      {
        "paperId": "484923f4db65eb68000b5e1c9da0c4cf22465ed3",
        "title": "Exploring the Potential of ChatGPT in Improving Online Marketing and Promotion of MSMEs"
      },
      {
        "paperId": "8fe808f225a4d75f2ad901bc877f9eefe4f9d94e",
        "title": "The muse in the machine"
      },
      {
        "paperId": "f88d306816b574d8fb9d6fe0c1e8840d754ac906",
        "title": "EXPLORING THE PSYCHOLOGICAL IMPLICATIONS OF CHATGPT:A QUALITATIVE STUDY"
      },
      {
        "paperId": "34c75964df9543fac07d7db5ce4cdad6ae65d812",
        "title": "Exploring the Use of Large Language Models (LLMs) in Chemical Engineering Education: Building Core Course Problem Models"
      },
      {
        "paperId": "3bc3ed2d1a2b6396318b469cf748d1ad00586ce9",
        "title": "ChatGPT-4 and the Global Burden of Disease Study: Advancing Personalized Healthcare Through Artificial Intelligence in Clinical and Translational Medicine"
      },
      {
        "paperId": "704fb0508f72a40566fb45279903fb124b3b44c8",
        "title": "The Role of Information Technology in Improving Urban Governance"
      },
      {
        "paperId": "8cf819f6ee33909484ece40d79944c9c37f01a89",
        "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development"
      },
      {
        "paperId": "2c1d7e6f485da3b7189012227fed0a1839af353e",
        "title": "What Is the Impact of ChatGPT on Education? A Rapid Review of the Literature"
      },
      {
        "paperId": "84f0982cf20ce172455d2a2c4038c281b404f256",
        "title": "Towards Designing a ChatGPT Conversational Companion for Elderly People"
      },
      {
        "paperId": "51a0bba0c5fb4257e843040615bb23f712fed4e6",
        "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"
      },
      {
        "paperId": "4f9de652780668029a39186128e999acb9cebcaf",
        "title": "Bridging the gap \u2013 the impact of ChatGPT on financial research"
      },
      {
        "paperId": "4651e2cff4e4f20bf6039c4719dbbe8cbe05b278",
        "title": "ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "df996bda58d52ac90048b83473caed6e5f9e4c36",
        "title": "The Potential and Concerns of Using AI in Scientific Research: ChatGPT Performance Evaluation"
      },
      {
        "paperId": "31528279c0769bcbf5e250565490898de247fa0d",
        "title": "Artificial intelligence in the era of ChatGPT - Opportunities and challenges in mental health care"
      },
      {
        "paperId": "5848737f78397f72ceae2ba6f3419a6a8502b8ba",
        "title": "ChatGPT: Jack of all trades, master of none"
      },
      {
        "paperId": "3bbb397ea3b09a10de198f167fba045da680b4e8",
        "title": "The Utility of ChatGPT as an Example of Large Language Models in Healthcare Education, Research and Practice: Systematic Review on the Future Perspectives and Potential Limitations"
      },
      {
        "paperId": "9543d4b9c6f71bdececdad7f397ad68cac359d2d",
        "title": "The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies"
      },
      {
        "paperId": "220856b0d885ff7d18ce2064ed69daedae1f7638",
        "title": "ChatGPT: Fundamentals, Applications and Social Impacts"
      },
      {
        "paperId": "7060d8e8837a0728bfb6424e31225d64e51717eb",
        "title": "Improving Student Creativity through Digital Technology Products: A Literature Review"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "9e5b918f5efd63ca2592be2e7f106b5d3e58e78a",
        "title": "When the Social Becomes Non-Human: Young People's Perception of Social Support in Chatbots"
      },
      {
        "paperId": "449205fec70e06f47067328a4eb5b42f2384cdbd",
        "title": "Artificial intelligence in communication impacts language and social relationships"
      },
      {
        "paperId": "21ef693aad998593a929be2302ea5381349952ba",
        "title": "Health chatbots acceptability moderated by perceived stigma and severity: A cross-sectional survey"
      },
      {
        "paperId": "a8a47b1b1d571699fcbe6519e3dc806e6cd66508",
        "title": "User Experiences of Social Support From Companion Chatbots in Everyday Contexts: Thematic Analysis"
      },
      {
        "paperId": "a2d21bbe2bb413499e83713772f72eccb95469cd",
        "title": "Building a Stronger CASA: Extending the Computers Are Social Actors Paradigm"
      },
      {
        "paperId": "5e307e074f5a54185834aaf83c5fb4a9bac6f7e6",
        "title": "Artificial intelligence and communication: A Human\u2013Machine Communication research agenda"
      },
      {
        "paperId": "e04eed6d0271c99f99d8b9cc87f7c4797a5ef707",
        "title": "How Do Ideal Friend Preferences and Interaction Context Affect Friendship Formation? Evidence for a Domain- General Relationship Initiation Process"
      },
      {
        "paperId": "05d35907cfd648dea8c36e58351bf2ad01b1d6d7",
        "title": "Performance Evaluation"
      },
      {
        "paperId": "0c432b171e52c86203843e200260f443a72ecaed",
        "title": "Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: A mixed-methods study"
      },
      {
        "paperId": "9c4e7f280264f7377e5cdcd1a290d61bdbca5f34",
        "title": "Should Machines Express Sympathy and Empathy? Experiments with a Health Advice Chatbot"
      },
      {
        "paperId": "81d3d140d041d800a6719b1a2106d026160e9dec",
        "title": "Living up to the chatbot hype: The influence of anthropomorphic design cues and communicative agency framing on conversational agent and company perceptions"
      },
      {
        "paperId": "cc054020508de1670f7728aa55906d2cade6a986",
        "title": "Levels of emotional awareness during psychotherapy among gynecologic cancer patients"
      },
      {
        "paperId": "b71ce1fd4c04ea2d7410f94a0a21d024893d4bd7",
        "title": "Exploring the values of writing collaboratively through a digital storytelling platform: a mixed-methods analysis of users\u2019 participation, perspectives and practices"
      },
      {
        "paperId": "740c41a1fc974c4c91b8a989e8ef3e79f9dcd474",
        "title": "Social isolation, loneliness and their relationships with depressive symptoms: A population-based study"
      },
      {
        "paperId": "c52044dcf1b985c1373de672d89c354e85463575",
        "title": "Natural language processing: state of the art, current trends and challenges"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "ddbdde502c1f8260ed9758bf8462513143a7d8ba",
        "title": "Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial"
      },
      {
        "paperId": "a411287f0022d30cdcf3beb6bd52415485929961",
        "title": "Under Threat: Responses to and the Consequences of Threats to Individuals' Identities"
      },
      {
        "paperId": "43242d0fe2ef5b025db05dd7bf4d72b2f713dcde",
        "title": "Evaluation and comparison of natural language and graphical user interfaces in \"query-by-impressions\" scenes"
      },
      {
        "paperId": "56ccf17dced2d3bb73f66a18afa20caf5a429c21",
        "title": "Machines and Mindlessness"
      },
      {
        "paperId": "e91ce40634ee1867b0c9a4526bf9bd2012963bf7",
        "title": "Does ChatGPT Alter Job Seekers' Identity? An Experimental Study"
      },
      {
        "paperId": null,
        "title": "\u201cWhat is the best expense management app?\u201d"
      },
      {
        "paperId": null,
        "title": "Can generative artificial intelligence foster belongingness, social support, and reduce loneliness? A conceptual analysis"
      },
      {
        "paperId": null,
        "title": "The GPT-3 vocabulary size [we did the math] \u00bb"
      },
      {
        "paperId": null,
        "title": "Minds and machines: unveiling the psychological effects of chatting with AI chatbots - the academic"
      },
      {
        "paperId": null,
        "title": "A short history of ChatGPT: how we got to where we are today"
      },
      {
        "paperId": "b575d7eb2d9edc9765db38d0149d1336f3596ebb",
        "title": "The role of ChatGPT in scientific communication: writing better scientific review articles."
      },
      {
        "paperId": "6043811788dba5a632d9ec447b4840a5aaab2784",
        "title": "Role and Challenges of ChatGPT and Similar Generative Artificial Intelligence in Finance and Accounting"
      },
      {
        "paperId": "19da9b5c1cc03f869d6e46cf0f5e8dbe4679cb9d",
        "title": "Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning"
      },
      {
        "paperId": "c85cff41e8b3160a04097a147d8612a10de74cbc",
        "title": "Exploring ChatGPT Capabilities and Limitations: A Survey"
      },
      {
        "paperId": "f70d76403b30a01e2b5cb429c0382f8c164d7f26",
        "title": "ChatGPT and its application in the field of mental health"
      },
      {
        "paperId": null,
        "title": "Death by AI? Man kills self after chatting with ChatGPT-like chatbot about climate change"
      },
      {
        "paperId": null,
        "title": "GPT-4 can ace the bar, but it only has a decent chance of passing the CFA exams. Here\u2019s a list of difficult exams the ChatGPT and GPT-4 have passed"
      },
      {
        "paperId": null,
        "title": "Does ChatGPT resemble humans in language use?"
      },
      {
        "paperId": null,
        "title": "Why we built the ChatGPT app for slack"
      },
      {
        "paperId": null,
        "title": "BuzzFeed says it will use AI tools from OpenAI to personalize its content"
      },
      {
        "paperId": null,
        "title": "ChatGPT in business: exploring the benefits and limitations"
      },
      {
        "paperId": null,
        "title": "A review of ChatGPT AI\u2019s impact on several business sectors"
      },
      {
        "paperId": null,
        "title": "What is ChatGPT and how will it change literature? | opinion"
      },
      {
        "paperId": null,
        "title": "Is ChatGPT better at business consulting than an experienced human analyst? An experimental comparison of solutions to a strategic business problem"
      },
      {
        "paperId": null,
        "title": "GPT-1 to GPT-4: each of OpenAI's GPT models explained and compared"
      },
      {
        "paperId": null,
        "title": "The impact of ChatGPT and AI on higher education: - Johanna Creswell B\u00e1ez, PhD, LCSW - medium"
      },
      {
        "paperId": null,
        "title": "Human computer interaction proclivity formed from the analysis and interpretation of survey"
      },
      {
        "paperId": "20f21b342f1b415eb900266c9bcf6e2fd0ca755c",
        "title": "My Chatbot Companion - a Study of Human-Chatbot Relationships"
      },
      {
        "paperId": "ef0f61566795320817acde5f32caa713c55a9753",
        "title": "The Chatbot Disclosure Dilemma: Desirable and Undesirable Effects of Disclosing the Non-Human Identity of Chatbots"
      },
      {
        "paperId": "744569e1ff6377ba0b7e3a8e2bcd88ac94d9a02d",
        "title": "Natural Language Processing: A Historical Review"
      },
      {
        "paperId": "a00b966f5e335cb35f3e7537fad56f6b6f507478",
        "title": "Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students"
      }
    ],
    "cited_by": [
      {
        "paperId": "fe3f65380c3cefaf67a64fa32f3eb8fd0b333f58",
        "title": "Establishing a real-time biomarker-to-LLM interface: a modular pipeline for HRV signal acquisition, processing, and physiological state interpretation via generative AI"
      },
      {
        "paperId": "79ea123c8dd45345e38c99daa9c770b1d1b0779b",
        "title": "The Siren Song of LLMs: How Users Perceive and Respond to Dark Patterns in Large Language Models"
      },
      {
        "paperId": "e6495485ff680b6b9e01f03d3fe9316d4786e186",
        "title": "ChatGPT in Early Childhood Science Education: Can It Offer Innovative Effective Solutions to Overcome Challenges?"
      },
      {
        "paperId": "244b0c190341d96eb070bebadc83442119a14d1a",
        "title": "Artificial Intelligence-driven Assessment of Sleep Quality: Comparing Artificial Intelligence-generated Sleep Questionnaire with Pittsburgh Sleep Quality Index in Undergraduate Medical Students"
      },
      {
        "paperId": "7d44ca06ca4121dc105a1b727e639f23a4621c84",
        "title": "How does students' perception of ChatGPT shape online learning engagement and performance?"
      },
      {
        "paperId": "22963e442cbe12ad2cb71b5cb92c0fdb08266449",
        "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content"
      },
      {
        "paperId": "0d9377b969e3e5575999da4ba6c01cb98a6709ff",
        "title": "User Self-Disclosure and Communication Depth Across Input Modality Types in GPT-Based Large Language Models: Text vs. Voice"
      },
      {
        "paperId": "cfe036acfa46091357d6b9b6a61bbc90655e9d5e",
        "title": "The Performance of ChatGPT-4.0 and ChatGPT-4omni on Answering Thyroid Question: A Multicenter Study."
      },
      {
        "paperId": "6e34479fa77f091e436f60846c951c0d8472f4f9",
        "title": "Motivation of University Students to Use LLMs to Assist with Online Consumption of Sustainable Products: An Analysis Based on a Hybrid SEM\u2013ANN Approach"
      },
      {
        "paperId": "95e2e6e39ae06795c0679cedd4eb446fb2132f69",
        "title": "Exploration of Mehrabian\u2019s communication model with an android"
      },
      {
        "paperId": "d2082b843d4cd82df9bb207d7edc12b4845ecaad",
        "title": "The digital interactive design of mirror painting under transformer based intelligent rendering methods"
      },
      {
        "paperId": "609ea44aa5f4a190fdd8f8cc049872f928187cd0",
        "title": "Conversational Health Interfaces in the Era of LLMs: Designing for Engagement, Privacy, and Wellbeing"
      },
      {
        "paperId": "d3da6bca1b336fdd6626ed621a4edc630699c00a",
        "title": "Comparison of physician and large language model chatbot responses to online ear, nose, and throat inquiries"
      },
      {
        "paperId": "757a590719df56250aff55029e2a894149b76b2e",
        "title": "How AIGC enhances the teaching quality of the guided-discovery teaching method: driving factors and moderators"
      },
      {
        "paperId": "a01898d483f54ba5b3e4410454fbb166970859d0",
        "title": "\"Person is a person, a tool is a tool\" - ChatGPT's Role in Student Help-Seeking Behavior and Peer Support"
      },
      {
        "paperId": "320d0c5659166bce1cccbf6a000b2e77d78403ef",
        "title": "Exploring Sentiment Patterns in ChatGPT Interactions: A Machine and Deep Learning Approach to Sentiment Classification"
      },
      {
        "paperId": "8c6959e01d518e126999a008dfd262af8e6f4ac7",
        "title": "Adaptive human-computer interaction for industry 5.0: A novel concept, with comprehensive review and empirical validation"
      },
      {
        "paperId": "db6f08ab954040150eaec8fdb7a7efdce8bf5746",
        "title": "\u0412\u041f\u041b\u0418\u0412 \u0428\u0422\u0423\u0427\u041d\u041e\u0413\u041e \u0406\u041d\u0422\u0415\u041b\u0415\u041a\u0422\u0423 \u041d\u0410 \u041c\u0415\u0422\u041e\u0414\u041e\u041b\u041e\u0413\u0406\u042e \u041d\u0410\u0423\u041a\u041e\u0412\u0418\u0425 \u0414\u041e\u0421\u041b\u0406\u0414\u0416\u0415\u041d\u042c \u0423 \u0421\u0423\u0427\u0410\u0421\u041d\u0406\u0419 \u041f\u0421\u0418\u0425\u041e\u041b\u041e\u0413\u0406\u0407"
      },
      {
        "paperId": "dc2135b948d98f4dcd8641e0e2efb4ba1a4d741a",
        "title": "Artificial intelligence (AI) in psychotherapy: A challenging frontier"
      },
      {
        "paperId": "651e84eaddb2447093dd9a3c8b830fc8c85ca98a",
        "title": "Building consumer trust in the ChatGPT\u2019s era: Insights from the hospitality industry"
      },
      {
        "paperId": "b14d4f8138a0eaf740b325465d776aaa66ed918a",
        "title": "The Usability of ChatGPT: A Language Learning Tool for Improving Grammar and Vocabulary in L2 Writing"
      },
      {
        "paperId": "e489df5401d5262b502c610ac4cd510d38c7cdcb",
        "title": "When ChatGPT Writes Your Research Proposal: Scientific Creativity in the Age of Generative AI"
      },
      {
        "paperId": "211c6d2268c3e5b214f30bb09d377fb36e7c58fa",
        "title": "The Role of Large Language Model Chatbots in Sexual Education: An Unmet Need of Research"
      },
      {
        "paperId": "44bf5372d9350a526e7cf2518073fa570847b35e",
        "title": "UniF$^2$ace: A Unified Fine-grained Face Understanding and Generation Model"
      },
      {
        "paperId": "85d4a70b331186bc5ac7f56021e92fad5adf6e5b",
        "title": "Integrating ChatGPT in halal tourism: impact on tourist satisfaction, e-WoM and revisit intention"
      },
      {
        "paperId": "cfd209ebcc840da4e9aba7a327afce72edb9bb66",
        "title": "Exploring the Impact of Emotional Awareness, Anthropomorphism, Technology Trust and Familiarity on Adoption of AI-Enabled Customer Service"
      },
      {
        "paperId": "bc07e506523bf5274260303da8372c18baa00b69",
        "title": "Hysteria in empathy: Understanding virtual companionship and emotional connection between humans and Al"
      },
      {
        "paperId": "5ac2d7f1bc4d46f80ddf3600995dcf6de7346073",
        "title": "Enhancing Self\u2010Esteem: Evaluating the Effects of a Self\u2010Affirmation Intervention Among Indian Adults With Subclinical Depression"
      },
      {
        "paperId": "b396a12f6c71d35e8c389d23cfaded9b17adaa7d",
        "title": "Using AI chatbots (e.g., CHATGPT) in seeking health-related information online: the case of a common ailment"
      },
      {
        "paperId": "563a3498447171b942e55605e9a8394ed6525bd2",
        "title": "Advancing Emotionally Aware Child\u2013Robot Interaction with Biophysical Data and Insight-Driven Affective Computing"
      },
      {
        "paperId": "c7a6bd97662495175b7312bc18d96949a3243fe9",
        "title": "Ethical use of ChatGPT in education\u2014Best practices to combat AI-induced plagiarism"
      },
      {
        "paperId": "7799aedf35a327ce90fa2ccb3a93f0e324a65fc2",
        "title": "Pilot Study on AI Image Analysis for Lower-Limb Reconstruction\u2014Assessing ChatGPT-4\u2019s Recommendations in Comparison to Board-Certified Plastic Surgeons and Resident Physicians"
      },
      {
        "paperId": "51d0ab045bda5a2ccac7f09f5bc9b1af2e1637a8",
        "title": "Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements"
      },
      {
        "paperId": "ff5ee0a41c52e5e75fa515079f0934d37f20847c",
        "title": "Evaluating the Performance of ChatGPT 3.5 and 4.0 on StatPearls Oculoplastic Surgery Text- and Image-Based Exam Questions"
      },
      {
        "paperId": "cad5c202fde776ada86ddd39520a520698e348e4",
        "title": "Impact of Criterion-Based Reflection on Prospective Physics Teachers' Perceptions of ChatGPT-Generated Content"
      },
      {
        "paperId": "d7eb7694734795686dd09d5cfcaca83ff6f296bd",
        "title": "Role of Synchronous, Moderated, and Anonymous Peer Support Chats on Reducing Momentary Loneliness in Older Adults: Retrospective Observational Study"
      },
      {
        "paperId": "6d08198d694ca01e4f789c753b69699e6847e9cc",
        "title": "Designing AI to elicit positive word-of-mouth in service recovery: The role of stress, anthropomorphism, and personal resources"
      }
    ],
    "score": 37.0
  },
  {
    "id": "2d906cda427cb2c4a71069423312e57ba4cd5445",
    "title": "Reinforcement Learning Enhanced LLMs: A Survey",
    "authors": [
      "Shuhe Wang",
      "Shengyu Zhang",
      "Jie Zhang",
      "Runyi Hu",
      "Xiaoya Li",
      "Tianwei Zhang",
      "Jiwei Li",
      "Fei Wu",
      "Guoyin Wang",
      "Eduard H. Hovy"
    ],
    "year": 2024,
    "citationCount": 35,
    "abstract": "Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
    "url": "https://www.semanticscholar.org/paper/2d906cda427cb2c4a71069423312e57ba4cd5445",
    "pdf_url": "https://arxiv.org/pdf/2412.10400.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-12-05",
    "externalIds": {
      "ArXiv": "2412.10400",
      "DBLP": "journals/corr/abs-2412-10400",
      "DOI": "10.48550/arXiv.2412.10400",
      "CorpusId": 274776492
    },
    "references": [
      {
        "paperId": "131a13c60f179511572abc81d6bd6aa988e96854",
        "title": "Rule Based Rewards for Language Model Safety"
      },
      {
        "paperId": "7796f56c7b9c152ac9573c8bb34f716aa78b4e6d",
        "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs"
      },
      {
        "paperId": "545cccca3ed5e8277656f163f3032b062000b244",
        "title": "Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "158730a16cd4cc4d4842e42e5c2a0843b75d527c",
        "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data"
      },
      {
        "paperId": "637f0398b32a4383320d38e54624db78a64c58ad",
        "title": "Quantile Regression for Distributional Reward Models in RLHF"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
        "title": "Gemma 2: Improving Open Language Models at a Practical Size"
      },
      {
        "paperId": "3c20be1b4227d1da11bcc705fd92ba76b010ecaf",
        "title": "\u03b2-DPO: Direct Preference Optimization with Dynamic \u03b2"
      },
      {
        "paperId": "c7f9706898bdfa3241601e075b1305649b174ff1",
        "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "65b5c132a90b66d6b21f0672abbe9eba5f9c63cb",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      },
      {
        "paperId": "cab3d65b3f4d0a4169d0fdaaed15af6be1d6bb84",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "87912571f3df29464d3ccafae66f6e1eed581564",
        "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment"
      },
      {
        "paperId": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
        "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment"
      },
      {
        "paperId": "53a803388e83ae89261624099d7be4287ace67cb",
        "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
      },
      {
        "paperId": "ecdd53eaab7455daea27609b07a418a21aa7ad35",
        "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models"
      },
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "db407c3a60c6dc768fde8dd1088dab3be951f04e",
        "title": "Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks"
      },
      {
        "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "paperId": "1e0feecb4d49c6dbdcf2eca6ddde6eb0dc3abd68",
        "title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "6dc1bb04eb0df303b1820ff1de15ab78f554cfff",
        "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "f19ddba513dfeca571a6e2ba7542a63677cd5e3b",
        "title": "Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought"
      },
      {
        "paperId": "8115ffbbadd1055424d18369dba66ce32a572800",
        "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"
      },
      {
        "paperId": "eabd317bf33a636b099a38f4b49aecca97202661",
        "title": "sDPO: Don't Use Your Data All at Once"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "53f4fb0e9972989194368faf288ff8e3cba5bd60",
        "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"
      },
      {
        "paperId": "f531d1a681ed12fd582767133318d0728316a0ae",
        "title": "Can large language models reason and plan?"
      },
      {
        "paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38",
        "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"
      },
      {
        "paperId": "e89ee3f84f1f07229a7ba211bad3465d2c80a325",
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"
      },
      {
        "paperId": "af89c2e64cfd12c0553500889d9bd04530b6e7cc",
        "title": "Sim-GPT: Text Similarity via GPT Annotated Data"
      },
      {
        "paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260",
        "title": "MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
        "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88",
        "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"
      },
      {
        "paperId": "cd391facabf5005419b79997b2ef8473644a8192",
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
        "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics"
      },
      {
        "paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
        "title": "Instruction Tuning for Large Language Models: A Survey"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "7011bf9aa7e68fabaa1df498da6d2dd8a950f037",
        "title": "Pushing the Limits of ChatGPT on NLP Tasks"
      },
      {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
      },
      {
        "paperId": "44f0876dec21a04533587def2add230b878a5006",
        "title": "Prompt Engineering with ChatGPT: A Guide for Academic Writers"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "bda605928d6ebe4db906e69ab5d343df75918727",
        "title": "Large Language Model Guided Tree-of-Thought"
      },
      {
        "paperId": "08a80cb34d785258c770acecd302ab41ead46eed",
        "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "023edab4738690444e3924e224c2641017a0d794",
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "title": "Longformer: The Long-Document Transformer"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a",
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "e1cb632953d78fed1e77fd0cd0cdbfa1c483810a",
        "title": "General Preference Modeling with Preference Representations for Aligning Language Models"
      },
      {
        "paperId": null,
        "title": "Anthropic"
      },
      {
        "paperId": "c52b30a60fda5f23cc0d2241c4e127f5191bbb2d",
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": ". Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "OpenAI. 2023."
      },
      {
        "paperId": null,
        "title": "2023. Eu-reka: Human-level reward design via coding large language models"
      },
      {
        "paperId": null,
        "title": "2024a. Gemma: Open models based on gemini research and technology"
      },
      {
        "paperId": null,
        "title": "2022. Defining and characterizing reward gaming"
      },
      {
        "paperId": null,
        "title": "2024. Gpt-4o system card"
      },
      {
        "paperId": null,
        "title": "2023. Mistral 7b"
      },
      {
        "paperId": null,
        "title": "2024c. Best practices and lessons learned on synthetic data"
      },
      {
        "paperId": null,
        "title": "2023. Alpaca: A strong, replicable instruction-following model"
      },
      {
        "paperId": null,
        "title": "2024. Offsetbias: Lever-aging debiased data for tuning evaluators"
      },
      {
        "paperId": null,
        "title": "2023d. Sentiment analysis through llm negotiations"
      },
      {
        "paperId": null,
        "title": "2024b. O-1: Optimization for language models with continuous integration"
      },
      {
        "paperId": null,
        "title": "2024. Starling-7b: Improving helpfulness and harmlessness with rlaif"
      },
      {
        "paperId": null,
        "title": "2024. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      },
      {
        "paperId": null,
        "title": "2023. Instruction tuning with gpt-4"
      },
      {
        "paperId": null,
        "title": "2023. Self-alignment with instruction back-translation"
      },
      {
        "paperId": null,
        "title": "2023. Meta prompting for agi systems"
      },
      {
        "paperId": null,
        "title": "2024. Tree of thoughts: Deliberate problem solving with large language models"
      },
      {
        "paperId": null,
        "title": "HuggingFaceH4"
      },
      {
        "paperId": null,
        "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "2023. Camels in a changing climate: En-hancing lm adaptation with tulu 2"
      },
      {
        "paperId": null,
        "title": "2024. Hermes 3 technical report"
      },
      {
        "paperId": null,
        "title": "2023. Active prompting with chain-of-thought for large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "61b763922db3e3d293ecd85c54305fd1f996320f",
        "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs"
      },
      {
        "paperId": "c61186e5ffe02954118d97bf845e1d1af431c34d",
        "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies"
      },
      {
        "paperId": "5fad7f432b969d4264a54df873edbcd45b076567",
        "title": "Can GRPO Boost Complex Multimodal Table Understanding?"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "3ae1b603374ca5d96903a3036382da0fd196da6b",
        "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models"
      },
      {
        "paperId": "2138d3bb41cc0bde43f2a0994cc61b271df61944",
        "title": "Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing"
      },
      {
        "paperId": "f93f4f3ea885d5cef13af2d62b29393b92dbe8b2",
        "title": "Exploring Superior Function Calls via Reinforcement Learning"
      },
      {
        "paperId": "32447990d7a5bc770f9a5c9692b9f4b1c30d22cc",
        "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning"
      },
      {
        "paperId": "b59f471f00533d4b9dc34f3ccc4122fdc9fedf56",
        "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards"
      },
      {
        "paperId": "618d58b52841a741487215cb04ace9d6b46e7e5a",
        "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster"
      },
      {
        "paperId": "368bbac734e5683d2f10d6399a9d19cb58fa7e3b",
        "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation"
      },
      {
        "paperId": "d1414ef32139735bc5bf7bc17b727b344a7abbae",
        "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning"
      },
      {
        "paperId": "f14b60456e8b2e53496717e05eaf860bde3abe2a",
        "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning"
      },
      {
        "paperId": "587c2796c048d014365746c7591088aa73532e4f",
        "title": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "2a5ee875e2d9151d086683920a876b14e23882d8",
        "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities"
      },
      {
        "paperId": "2804f5e90879ed0bb3af30a612138e6381775abf",
        "title": "From Emergence to Control: Probing and Modulating Self-Reflection in Language Models"
      },
      {
        "paperId": "c62d915d7ea1c4be7abfc1b970f02c96d2994590",
        "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO"
      },
      {
        "paperId": "f6180c800726e25693e9808360167760d1a89048",
        "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance"
      },
      {
        "paperId": "33bfced3ccc883ecac60da7888028a9a3c4d7f43",
        "title": "Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems"
      },
      {
        "paperId": "2d11d32b51a1c14d6c837fdaef3af32023d4da76",
        "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models"
      },
      {
        "paperId": "fcda84a6d3375d59f8eabe32b7d54a707e1d3837",
        "title": "Double-Feedback: Enhancing Large Language Models Reasoning in Robotic Tasks by Knowledge Graphs"
      },
      {
        "paperId": "eb7db9262a189fa8bb1da878736b030d2a564932",
        "title": "Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review"
      },
      {
        "paperId": "9caa68ab2a4fbe79521f3cd3337962dc989c9ebd",
        "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers"
      },
      {
        "paperId": "823a6f37db9d18ec4fdfaf02e01400c5ec6c0ab9",
        "title": "Contemplative Artificial Intelligence"
      },
      {
        "paperId": "ccd9eca10294fe822a25e1133d59deacab005860",
        "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs"
      },
      {
        "paperId": "bb870577bc853105e27a236e3cd242246ef785c8",
        "title": "DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless Networks: A Survey"
      },
      {
        "paperId": "c22e13ff1157d9174e5fde35938b8d738b027ecf",
        "title": "TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language Models in Traditional Chinese Medicine"
      },
      {
        "paperId": "499979b1347c53cc3cadedf66575623cc0d6a727",
        "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks"
      },
      {
        "paperId": "8180d39107c9b9b1589f93f92c16ee3d78514974",
        "title": "A Statistical Case Against Empirical Human-AI Alignment"
      },
      {
        "paperId": "208130591c949e50a463cd1ddf955e1942f63982",
        "title": "Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training"
      },
      {
        "paperId": "c9e8647e17bf342d6491d827c56c147dac2e935f",
        "title": "Towards Watermarking of Open-Source LLMs"
      },
      {
        "paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
        "title": "Instruction Tuning for Large Language Models: A Survey"
      },
      {
        "paperId": "84a1e29b162313df91ab3fb529b6ff64f2dd2ef3",
        "title": "The Evolution of Generative AI: Trends and Applications"
      },
      {
        "paperId": "567a020cf81e52b480fafb33d972ec92b21d94fc",
        "title": "Debate for Multiagent Large Language Model Tuning"
      }
    ],
    "score": 35.0
  },
  {
    "id": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
    "title": "Self-Generated Critiques Boost Reward Modeling for Language Models",
    "authors": [
      "Yue Yu",
      "Zhengxing Chen",
      "Aston Zhang",
      "Liang Tan",
      "Chenguang Zhu",
      "Richard Yuanzhe Pang",
      "Yundi Qian",
      "Xuewei Wang",
      "Suchin Gururangan",
      "Chao Zhang",
      "M. Kambadur",
      "Dhruv Mahajan",
      "Rui Hou"
    ],
    "year": 2024,
    "citationCount": 34,
    "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",
    "url": "https://www.semanticscholar.org/paper/b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
    "pdf_url": "https://arxiv.org/pdf/2411.16646.pdf",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "publicationDate": "2024-11-25",
    "externalIds": {
      "DBLP": "journals/corr/abs-2411-16646",
      "ArXiv": "2411.16646",
      "DOI": "10.48550/arXiv.2411.16646",
      "CorpusId": 274280905
    },
    "references": [
      {
        "paperId": "a112125e251610b135a151b416a227bffadeb8f2",
        "title": "Self-Consistency Preference Optimization"
      },
      {
        "paperId": "7943ec4a67151a559b25cd34369e661c9a7924c8",
        "title": "GPT-4o System Card"
      },
      {
        "paperId": "b5843e5acb4620346ba20e7eab89775519302491",
        "title": "HelpSteer2-Preference: Complementing Ratings with Preferences"
      },
      {
        "paperId": "59eed4c468846a4d45105a4603dabf72e2bef830",
        "title": "Generative Reward Models"
      },
      {
        "paperId": "c9cec46da172fbe591a2f086c2647176c34d8ca3",
        "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models"
      },
      {
        "paperId": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
        "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
      },
      {
        "paperId": "6d3a7b453048673a98b082a73bc864366fbd1cf4",
        "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "2f112209675710d3ec2d6f1d06bbdc74e9bc60af",
        "title": "Critique-out-Loud Reward Models"
      },
      {
        "paperId": "1035dfceeebd3dd9ba52a8162eeda670a432c56e",
        "title": "Self-Taught Evaluators"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
        "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"
      },
      {
        "paperId": "c8c9002af1d90e9dafa3d07e9edf0d883ec45472",
        "title": "Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation"
      },
      {
        "paperId": "029d309dd1f3a93ff74740ceca32da8c09c63e5c",
        "title": "BOND: Aligning LLMs with Best-of-N Distillation"
      },
      {
        "paperId": "8deb5fd40e310dc4feb27f7db7019e734b44631b",
        "title": "LLM Critics Help Catch LLM Bugs"
      },
      {
        "paperId": "b1e681dda709217a67316d2394bc992a26008ef7",
        "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "d0e7353189feb4501c637b7008ff993de603c3f0",
        "title": "Nemotron-4 340B Technical Report"
      },
      {
        "paperId": "65b5c132a90b66d6b21f0672abbe9eba5f9c63cb",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "4dd571c6514d169d0b0a7933f3ba3440301d2e46",
        "title": "Improving Reward Models with Synthetic Critiques"
      },
      {
        "paperId": "ecdd53eaab7455daea27609b07a418a21aa7ad35",
        "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models"
      },
      {
        "paperId": "53d212ea368e75cc152c3cb287343da22849915e",
        "title": "Large Language Models are Inconsistent and Biased Evaluators"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "paperId": "6f24e0782300dca8a4cefcb5a3ccba94bfbb1395",
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning"
      },
      {
        "paperId": "a28071c63963cc59ba500cd00c140ac08eb5ccb0",
        "title": "Humans or LLMs as the Judge? A Study on Judgement Bias"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "7a147a745f69329afb1c86becdba7b3029a169ca",
        "title": "Predicting Text Preference Via Structured Comparative Reasoning"
      },
      {
        "paperId": "c871377b208814713c18e25633866323a2982136",
        "title": "Proving Test Set Contamination in Black Box Language Models"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "9ebf47129c15f61f4b77bbfe305c522480c20347",
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"
      },
      {
        "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
        "title": "Evaluating Large Language Models at Evaluating Instruction Following"
      },
      {
        "paperId": "f05c288caeb9a14ef387e6867934ced3d2200259",
        "title": "SALMON: Self-Alignment with Instructable Reward Models"
      },
      {
        "paperId": "5001630bcc65e8e0e621b19625629a2689724743",
        "title": "Generative Judge for Evaluating Alignment"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475",
        "title": "Self-Alignment with Instruction Backtranslation"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a57ef1f5c3af185af79751855b8033b7fc6d89b3",
        "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
        "title": "Large Language Models Are Reasoning Teachers"
      },
      {
        "paperId": "29acc890e521f7a6415666ab9eb3432c49b4587a",
        "title": "Self-critiquing models for assisting human evaluators"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "1eecfd21543286d45ca44e424a2e351d1ee6ab12",
        "title": "The Trickle-down Impact of Reward Inconsistency on RLHF"
      },
      {
        "paperId": "597f869a3ae3572e16de3643a7bd285ea2f1eabd",
        "title": "Defining and Characterizing Reward Gaming"
      },
      {
        "paperId": null,
        "title": "2023. Wizardlm: Empowering large language models to follow complex instructions"
      },
      {
        "paperId": null,
        "title": "West-of-n: Synthetic preference generation for improved reward modeling"
      },
      {
        "paperId": null,
        "title": "Rai Michael"
      },
      {
        "paperId": null,
        "title": "2022. Introducing"
      },
      {
        "paperId": null,
        "title": "2024. ULTRAFEEDBACK: Boosting language models with scaled AI feedback"
      },
      {
        "paperId": null,
        "title": "An automatic evaluator of instruction-following models"
      }
    ],
    "cited_by": [
      {
        "paperId": "028dc666c1c1362e5b4d060d311af862764055a2",
        "title": "RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation"
      },
      {
        "paperId": "b23591aefc039307fda9d50d8494769bf4d28420",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models"
      },
      {
        "paperId": "06df7400cba1ef6947f72d2dae130eb74c1ac1ca",
        "title": "BaseReward: A Strong Baseline for Multimodal Reward Model"
      },
      {
        "paperId": "231d4842edd8128ed8515b635930241af984ad65",
        "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique"
      },
      {
        "paperId": "bba5163dbc56b9cea548ef0cd370897049766c51",
        "title": "EQA-RM: A Generative Embodied Reward Model with Test-time Scaling"
      },
      {
        "paperId": "3a2b08d37fba04b6563b38818497f1f98ec9df7d",
        "title": "RewardAnything: Generalizable Principle-Following Reward Models"
      },
      {
        "paperId": "6069fa0fc5cccb8ebd0061c5e1816f5069cc255b",
        "title": "RewardBench 2: Advancing Reward Model Evaluation"
      },
      {
        "paperId": "abc473899b15276c087c8ecf64e649169a1d1382",
        "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models"
      },
      {
        "paperId": "5759a0e3a98496e7cce0a5a312aff3f4f491a2c4",
        "title": "Reward Reasoning Model"
      },
      {
        "paperId": "68cd9e02bd17961d4b864696edfae82f8256bb4c",
        "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge"
      },
      {
        "paperId": "4993fa8b35ea26cd2a83d8c08e9f1672911d3b76",
        "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages"
      },
      {
        "paperId": "d3f48630511341fd64ab5847aac6486a9974da8e",
        "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
      },
      {
        "paperId": "f25225055901731512dcbc905f75383dc678b4d0",
        "title": "RM-R1: Reward Modeling as Reasoning"
      },
      {
        "paperId": "abeb46288d537f98f76b979040a547ee81216377",
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "160820bb80922cfc3ff47a0c9b3a2bb369bb97b9",
        "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning"
      },
      {
        "paperId": "b5f6c29c6c9446e69a3f1c7508b2a1a7012b53c1",
        "title": "Heimdall: test-time scaling on the generative verification"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "5b08060d9efee18b235b4d49bb1f58898f3fc24f",
        "title": "Inference-Time Scaling for Generalist Reward Modeling"
      },
      {
        "paperId": "00b883250ce2bfe6e983426882e355539feca9b8",
        "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model"
      },
      {
        "paperId": "7a335bc70fed77651be02223239d952fc58f080f",
        "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond"
      },
      {
        "paperId": "7cede4b487ecdca4d29ec9a4e17f5818c4c74d1b",
        "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks"
      },
      {
        "paperId": "ded8cf8b28bab1930f5c57b31f7906835dfeec38",
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems"
      },
      {
        "paperId": "0ec6a9659d50bfdb32e75e1b9c85372f67bde349",
        "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment"
      },
      {
        "paperId": "dddd33f1e83220f5c4736644490e483f9de32ed1",
        "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails"
      },
      {
        "paperId": "a1a1eba8ae74d12af512ae22dc77745459361360",
        "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information"
      },
      {
        "paperId": "438c19fdd0f14f5082441db0c4183a50e7224b1b",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
      },
      {
        "paperId": "1fd282ff3a034ff6113f076b02769c46a7159476",
        "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning"
      },
      {
        "paperId": "f3eee8fa080bfbcaa6ff233664a49e81bbb459ea",
        "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis"
      },
      {
        "paperId": "84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c",
        "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
      },
      {
        "paperId": "92056d644aed7caa6c5367fe77774883246af793",
        "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge"
      },
      {
        "paperId": "ce3131576e537dd2899014db7bf064962a007a87",
        "title": "In Context Learning and Reasoning for Symbolic Regression with Large Language Models"
      },
      {
        "paperId": "9875390100a25da7855e9c7cb25fcd9a6c35e9e0",
        "title": "Data-Centric Human Preference with Rationales for Direct Preference Alignment"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      }
    ],
    "score": 34.0
  },
  {
    "id": "386cebdba39d2d5f2862a9ab43a8d807f3863dae",
    "title": "Contrastive Preference Learning: Learning from Human Feedback without RL",
    "authors": [
      "Joey Hejna",
      "Rafael Rafailov",
      "Harshit S. Sikchi",
      "Chelsea Finn",
      "S. Niekum",
      "W. B. Knox",
      "Dorsa Sadigh"
    ],
    "year": 2023,
    "citationCount": 65,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.",
    "url": "https://www.semanticscholar.org/paper/386cebdba39d2d5f2862a9ab43a8d807f3863dae",
    "pdf_url": "https://arxiv.org/pdf/2310.13639.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-13639",
      "ArXiv": "2310.13639",
      "DOI": "10.48550/arXiv.2310.13639",
      "CorpusId": 264405839
    },
    "references": [
      {
        "paperId": "e571f9b823dcb31580161a37342a73dd93ebbe52",
        "title": "Learning Optimal Advantage from Preferences and Mistaking it for Reward"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "d123a153790442bc7d2fe39baa7621b795b7f6d0",
        "title": "Beyond Reward: Offline Preference-guided Policy Optimization"
      },
      {
        "paperId": "4367911d9d28d83fafbcf6c908698dd981ddbe9e",
        "title": "Inverse Preference Learning: Preference-based RL without a Reward Function"
      },
      {
        "paperId": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d",
        "title": "Training Diffusion Models with Reinforcement Learning"
      },
      {
        "paperId": "f8f6942be75d102a14c6441e0bb31ac7c59235a4",
        "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models"
      },
      {
        "paperId": "c7dea47e008a439e11439dfe6a8c1b08357fad65",
        "title": "Distance Weighted Supervised Learning for Offline Interaction Data"
      },
      {
        "paperId": "c6478decdfff11ccbd085967c2f83aea11927a46",
        "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
        "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning"
      },
      {
        "paperId": "4f0bfeadd39e64456d15d400fda8ecc2197c3265",
        "title": "Direct Preference-based Policy Optimization without Reward Modeling"
      },
      {
        "paperId": "2c2180fbe7f38e88b1123e5fab43785b66814e5d",
        "title": "Extreme Q-Learning: MaxEnt RL without Entropy"
      },
      {
        "paperId": "18d750263f1dfe43374e8791cefa580a511c2098",
        "title": "Few-Shot Preference Learning for Human-in-the-Loop RL"
      },
      {
        "paperId": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
        "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      },
      {
        "paperId": "53dcf467fbded741dd08902d4203a9b57e889c87",
        "title": "Contrastive Learning as Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc",
        "title": "Models of human preference for learning reward functions"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "622a5434eae1b53fe0b74ad0d0942212cd722c36",
        "title": "A Ranking Game for Imitation Learning"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "3032844d6ac6882ccb03e7a2c22a0026b210ac05",
        "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "a1d8cc2fdb7b0f262dc5b057f13bdcaea695842a",
        "title": "Offline Preference-Based Apprenticeship Learning"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "489666f4c11787b679b36238dee95b63248ed60a",
        "title": "Training Larger Networks for Deep Reinforcement Learning"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "7097137596f6755675f6aafcdd80969a747322ae",
        "title": "Contrastive Learning with Hard Negative Samples"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "658018e556484e3d9c6bcc00c726bf5eb503ef86",
        "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences"
      },
      {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
        "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations"
      },
      {
        "paperId": "9c75be2e6863a35ce4447cfc14e65609a16558a5",
        "title": "The Green Choice: Learning and Influencing Human Decisions on Shared Roads"
      },
      {
        "paperId": "6bc692616db7b1a7ef2ea7c270c893adfb57ed0e",
        "title": "Deep Reinforcement Learning and the Deadly Triad"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
        "title": "Representation Learning with Contrastive Predictive Coding"
      },
      {
        "paperId": "c72582122ff631117a05deb2aefa04b01362e3fa",
        "title": "Learning to Extract Coherent Summary via Deep Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "e27467ca84c5c1f7239a6e643843c1b97e35671f",
        "title": "Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "21c05d606d6899c42ae02e3b671e92faaaf130a7",
        "title": "Active reward learning with a novel acquisition function"
      },
      {
        "paperId": "244539f454800697ed663326b7cfba337ca0c2ec",
        "title": "Guided Policy Search"
      },
      {
        "paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"
      },
      {
        "paperId": "b8014d4af46d9b26cf434240dced24294b2110b6",
        "title": "Preference-based reinforcement learning: a formal framework and a policy iteration algorithm"
      },
      {
        "paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
        "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "1a4e67d3705492d0af88623b0e62818a16084fca",
        "title": "The Analysis of Permutations"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "ceac3aaba97b40e9ace78dfad0331f28efb36e02",
        "title": "Designing an offline reinforcement learning objective from scratch"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Author manuscript, published in \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (2011)\" Preference-based Policy Learning"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "08f48867a7294bd3851d7fca095dcc1ecb591f6b",
        "title": "Approximate Gradient Methods in Policy-Space Optimization of Markov Reward Processes"
      },
      {
        "paperId": null,
        "title": "CPL\u2019s loss function is computed over segments, it requires a substantial amount of GPU memory for large segment sizes"
      }
    ],
    "cited_by": [
      {
        "paperId": "949b38448641e2db90865743b714fdecbf221a7b",
        "title": "Predictive Preference Learning from Human Interventions"
      },
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "30759df57a8b948bdd5881b87bdf0c1c99d30473",
        "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "1ef29b16fdca0122deb2ff217bb42a57e6fadc55",
        "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues"
      },
      {
        "paperId": "ff0f7c80c9ecabf6986884b918626d431b9b6778",
        "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling"
      },
      {
        "paperId": "68775864885d0dc8706444d459625585db09c660",
        "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "c6af32946fbe86a888960a9236d4cde81b4a5f74",
        "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation"
      },
      {
        "paperId": "d42f55af51fc631f3a0637af966b52c698693626",
        "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism"
      },
      {
        "paperId": "eb42eb47a24ef0d8ecccc59c730dd980956c352b",
        "title": "On Monotonicity in AI Alignment"
      },
      {
        "paperId": "8c376a7d9bad8122afaf4f841db39ae4c1bf2162",
        "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations"
      },
      {
        "paperId": "8c718716717a60c09c8bf33ee3b25f0b3288a5ef",
        "title": "Adaptive Confidence-aware Preference-based Reinforcement Learning with Noisy Feedback"
      },
      {
        "paperId": "f1173df25d83e5c6ff16e087f890993bccf355ed",
        "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
      },
      {
        "paperId": "bdd03508c121ca8d82b22135da3409d2d0490085",
        "title": "Optimal Interactive Learning on the Job via Facility Location Planning"
      },
      {
        "paperId": "09a61188bba713f4e24857dcdffa8311a612e2af",
        "title": "One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF"
      },
      {
        "paperId": "43908f36b802ab4422a7e7c995aa40f85906a10f",
        "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations"
      },
      {
        "paperId": "c6247a21a50b899d95dac5613610877c2be0d127",
        "title": "Disentangling Uncertainties by Learning Compressed Data Representation"
      },
      {
        "paperId": "86e426b97069e4e8ded04dce0633a59286139a57",
        "title": "Policy Teaching via Data Poisoning in Learning from Human Preferences"
      },
      {
        "paperId": "9f13649ed8c1987a8de4c1d6df90ab9da344d09d",
        "title": "Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective"
      },
      {
        "paperId": "fb15fa7d5fbc2e36957057cc3a275252f5e3ce53",
        "title": "Robust Reward Alignment via Hypothesis Space Batch Cutting"
      },
      {
        "paperId": "53adca9d83a3b4f1e4b23b2ac980662f70877732",
        "title": "Assessing Moral Decision Making in Large Language Models"
      },
      {
        "paperId": "35b9438bcdc218c766d5ca28ae464d4287291234",
        "title": "Direct Preference Optimization for Primitive-Enabled Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "239d787a5019dd3975e7117bf769d69d540f6856",
        "title": "Understanding Layer Significance in LLM Alignment"
      },
      {
        "paperId": "864b530b17373aa4e6f09a00614cf2f6c2be707d",
        "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling"
      },
      {
        "paperId": "da8fe024d39d671cb3c382845a14db7055784ede",
        "title": "Deformpam: Data-Efficient Learning for Long-Horizon Deformable Object Manipulation Via Preference-Based Action Alignment"
      },
      {
        "paperId": "a7227e49728bedc38a95190da45d33221b5fbb2c",
        "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "33c9147cfacb6ce401fe45b57307b8e3eeeb16d6",
        "title": "Forward KL Regularized Preference Optimization for Aligning Diffusion Policies"
      },
      {
        "paperId": "b51d1946a6184946c93809d06942aa410c384203",
        "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey"
      },
      {
        "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
        "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "21cdb3aced0e15d8581f3f58ea836b4f379a06fa",
        "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law"
      },
      {
        "paperId": "25f8718f4964dfcf266d1c17197796f1114407e8",
        "title": "AI Safety in Generative AI Large Language Models: A Survey"
      },
      {
        "paperId": "2551adaf556fedab0e360ac36f26ce174bcaddee",
        "title": "Safe MPC Alignment with Human Directional Feedback"
      },
      {
        "paperId": "35a4557e99dd90821a18fe9076eabe1cb9c4b168",
        "title": "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning"
      },
      {
        "paperId": "c27e57b6e083b522afa9d202e6dd99ced29ff857",
        "title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions"
      },
      {
        "paperId": "77f0687571a213c784f0901a821f22b2a03f3ddd",
        "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models"
      },
      {
        "paperId": "6dcc524fcd56dec219c3160824049a9182538888",
        "title": "HELM-GPT: de novo macrocyclic peptide design using generative pre-trained transformer"
      },
      {
        "paperId": "e96c41933f4b1f0e79449451395e6ed0e6644133",
        "title": "Preference Alignment with Flow Matching"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "cd7ce144bd0082ecb579bf03a3f03a05ed525a53",
        "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts"
      },
      {
        "paperId": "0a1ec333b79aa449ff9d2c2ce0913c625729e972",
        "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction"
      },
      {
        "paperId": "f18e0e020f1de8108e33968484fd1545e0714b81",
        "title": "Robot Air Hockey: A Manipulation Testbed for Robot Learning with Reinforcement Learning"
      },
      {
        "paperId": "220d4a8da8a8778b43044533d02851c82d755ae9",
        "title": "A Preference-driven Paradigm for Enhanced Translation with Large Language Models"
      },
      {
        "paperId": "3714bb3aac3f0429cd62e315da0ffb3f79655f98",
        "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"
      },
      {
        "paperId": "4bdc1f0436d8ec356fc5c2bb9e1c510b7951d2ba",
        "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond"
      },
      {
        "paperId": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee",
        "title": "Understanding the Learning Dynamics of Alignment with Human Feedback"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "e1e027639b54616217bde738033470dbf7d73d64",
        "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards"
      },
      {
        "paperId": "66e7edf09589527ebb58418632418758cee668cd",
        "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"
      },
      {
        "paperId": "0a543cc850645c09eecc1dff4cc77e1ffbba0abb",
        "title": "Batch Active Learning of Reward Functions from Human Preferences"
      },
      {
        "paperId": "9e08d4f7348ff2afbe8c96e7564f43d326e67d00",
        "title": "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference"
      },
      {
        "paperId": "4c98e18cf16395b95ffaaeeac3eceffa608dcf8d",
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"
      },
      {
        "paperId": "6ac2856b0192ec307e349406270ec40a6f7e14a4",
        "title": "YODA: Teacher-Student Progressive Learning for Language Models"
      },
      {
        "paperId": "ebd1c04c61f73f46def3305ca11d038c46665b65",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "90ec1b95ab1579673f0a4b81dc5a76dfc955b2a5",
        "title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences"
      },
      {
        "paperId": "ce316ce4671e38019582f7be7a6e87b4c3909b57",
        "title": "RLHF and IIA: Perverse Incentives"
      },
      {
        "paperId": "64affc9d608066c3d0a361fb583a6252444ce564",
        "title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation"
      },
      {
        "paperId": "8f49bd5a69d64d0547a9e8c59cb7f91fcb5ed3ab",
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4"
      },
      {
        "paperId": "4367911d9d28d83fafbcf6c908698dd981ddbe9e",
        "title": "Inverse Preference Learning: Preference-based RL without a Reward Function"
      },
      {
        "paperId": "bc219346f5a893808ad48c2ba594cee81aa352ee",
        "title": "RRescue: Ranking LLM Responses to Enhance Reasoning Over Context"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      }
    ],
    "score": 32.5
  },
  {
    "id": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
    "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
    "authors": [
      "Ted Moskovitz",
      "Aaditya K. Singh",
      "DJ Strouse",
      "T. Sandholm",
      "Ruslan Salakhutdinov",
      "Anca D. Dragan",
      "S. McAleer"
    ],
    "year": 2023,
    "citationCount": 62,
    "abstract": "Large language models are typically aligned with human preferences by optimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
    "url": "https://www.semanticscholar.org/paper/af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
    "pdf_url": "https://arxiv.org/pdf/2310.04373.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-04373",
      "ArXiv": "2310.04373",
      "DOI": "10.48550/arXiv.2310.04373",
      "CorpusId": 263829192
    },
    "references": [
      {
        "paperId": "1675bce7fca4eb2171f68755e79c399060087f23",
        "title": "Probabilistic Inference in Reinforcement Learning Done Right"
      },
      {
        "paperId": "585db3d5266c66b1f292d88adc9108c11efd84b7",
        "title": "A State Representation for Diminishing Rewards"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "f227e955e40084f2cee68a5148e404c94d4f1490",
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation"
      },
      {
        "paperId": "09f0422754142a1a58182b8238f9cd1b242adab5",
        "title": "Provably Efficient Primal-Dual Reinforcement Learning for CMDPs with Non-stationary Objectives and Constraints"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "4a50853984a126e2b77f7c3f216c0ef486a138ab",
        "title": "Efficient exploration via epistemic-risk-seeking policy optimization"
      },
      {
        "paperId": "1c975976d74df4f680c6107052dcb6348e209650",
        "title": "ReLOAD: Reinforcement Learning with Optimistic Ascent-Descent for Last-Iterate Convergence in Constrained MDPs"
      },
      {
        "paperId": "de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "ee626478aafe6496c86512660820933538059ef2",
        "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "ed6898688ab18d50d5ac8e9227391d16e8670715",
        "title": "Towards an Understanding of Default Policies in Multitask Policy Optimization"
      },
      {
        "paperId": "5d0c5b55db27d44e80406a825fb86ae3e1325a8b",
        "title": "A First-Occupancy Representation for Reinforcement Learning"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "title": "Unified Pre-training for Program Understanding and Generation"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "07fd366a8ebdefe54cdb57d87c81dcd22de25a91",
        "title": "A Distributional Approach to Controlled Text Generation"
      },
      {
        "paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba",
        "title": "Extracting Training Data from Large Language Models"
      },
      {
        "paperId": "eecc04e4751ef623ecd9f9e69e9601c9431152d2",
        "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG"
      },
      {
        "paperId": "6e1ee4042e627e17128ff38adc550c305e539a85",
        "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "629d0ce250581471f07083bbab95f23623b00201",
        "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods"
      },
      {
        "paperId": "5b3e68658c99ed9c461a909b16b862221946d6ad",
        "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism"
      },
      {
        "paperId": "de6b9ca10b375f6b4e34f1c1c913423297acc4e9",
        "title": "Reinforcement learning algorithm for non-stationary environments"
      },
      {
        "paperId": "b0887ad68a41d8e796cc7afe562a5d0da93fcda0",
        "title": "Deep Reinforcement Learning amidst Lifelong Non-Stationarity"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "63196e0189bf710abcacaf418edbdc29e7750b94",
        "title": "A Distributional View on Multi-Objective Policy Optimization"
      },
      {
        "paperId": "1fd2ee591eb8dda518a2e9e685098c764c440803",
        "title": "Exploration-Exploitation in Constrained MDPs"
      },
      {
        "paperId": "9d964e4688b0cbae0847d7c2d507c2320eed7657",
        "title": "Provably Efficient Safe Exploration via Primal-Dual Policy Optimization"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "c919ae4366f5cc4901b854cc259101ccc13e6f3f",
        "title": "Constrained Reinforcement Learning Has Zero Duality Gap"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "854eca61a57d2c1ea1019663caf022bc8fd0b909",
        "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python"
      },
      {
        "paperId": "1067d14c0f3766277cb6a482c5ced515889df37e",
        "title": "Learning to Score Behaviors for Guided Policy Optimization"
      },
      {
        "paperId": "549c9dfb32e85d9ef5a48566767be42ad132a3c4",
        "title": "Information asymmetry in KL-regularized RL"
      },
      {
        "paperId": "64f3ba0e812dd88c2b8288351a8e01e01dbc4e86",
        "title": "Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
        "title": "A Lyapunov-based Approach to Safe Reinforcement Learning"
      },
      {
        "paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb",
        "title": "A Call for Clarity in Reporting BLEU Scores"
      },
      {
        "paperId": "a8ef08940341381390d9a5672546354d0ce51328",
        "title": "Maximum a Posteriori Policy Optimisation"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "3108f96f80d129036f53684344f4058257b37c4b",
        "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "cf90552b5d2e992e93ab838fd615e1c36618e31c",
        "title": "Distral: Robust multitask reinforcement learning"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "30feade758d15844a5746fd0de7983d5a0e4af02",
        "title": "ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "f332ecd5d54adf0530a39dae189cf6b160ad5c0e",
        "title": "An Online Actor\u2013Critic Algorithm with Function Approximation for Constrained Markov Decision Processes"
      },
      {
        "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
        "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"
      },
      {
        "paperId": "7533d30329cfdbf04ee8ee82bfef792d08015ee5",
        "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
      },
      {
        "paperId": "3f08e2ce1e7440440aecab0d732433e40e5b28fd",
        "title": "An actor-critic algorithm for constrained Markov decision processes"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": "1076a9b8181a7f9eb069d38ca10876a3202d2e89",
        "title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization"
      },
      {
        "paperId": "4ba566223e426677d12a9a18418c023a4deec77e",
        "title": "A decision-theoretic generalization of on-line learning and an application to boosting"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "2894125ea5f8a3300bd098e7be2331f7789ff91b",
        "title": "Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources"
      },
      {
        "paperId": null,
        "title": "Statement on ai risk"
      },
      {
        "paperId": "9f02657034d5cef9279083bb66706ee3ce84f46e",
        "title": "Deep Reinforcement Learning amidst Continual Structured Non-Stationarity"
      },
      {
        "paperId": null,
        "title": "Constrained mdps and the reward hypothesis, Mar 2020"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "a1173d20ffef2f1ccb1ea2023dcce7c45747a373",
        "title": "Reinforcement"
      },
      {
        "paperId": "5dbe244846bfedfd687be0cdaba19befcd96c8f6",
        "title": "Conference Paper"
      },
      {
        "paperId": "e758b579456545f8691bbadaf26bcd3b536c7172",
        "title": "Machine super intelligence"
      },
      {
        "paperId": "4774432f02ef4c5285952dd8c7daff0852c3a601",
        "title": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "0ae623749b30de53a39cf05813f5f3842e422c01",
        "title": "Problems of Monetary Management: The UK Experience"
      },
      {
        "paperId": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
        "title": "A Simplex Method for Function Minimization"
      },
      {
        "paperId": null,
        "title": "\u2022 \u03be -PPO: What is your condition? Do you have any fever? <EOU> No METEOR:0.20, Intent: 0, Eval: 0.70 \u2022 NM-PPO: I don\u2019t feel good. I really dont feel good at all <EOU> No way METEOR:0.21, Intent: 1"
      },
      {
        "paperId": null,
        "title": "an important toolbox for approaching the alignment problem. Acknowledgements"
      },
      {
        "paperId": null,
        "title": "I'll have to check \u2022 All-PPO: What is the most important step of your quest? <EOU> Making sure that your quest succeeds \u2022 \u03be-PPO: What is your condition?"
      }
    ],
    "cited_by": [
      {
        "paperId": "7c332dfd56c799e8b49dabc8b24888b2ca9fd3b3",
        "title": "Aligning Text-to-Image Diffusion Models With Constrained Reinforcement Learning"
      },
      {
        "paperId": "d162c0f1682ca88fdc07bdbfa429b245ee51f4d7",
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment"
      },
      {
        "paperId": "c5d95792a925bcab0bf0a0b80e96d792bea6eddd",
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training"
      },
      {
        "paperId": "965f51b965fc310daa29eaa4cbdea679591a8eb8",
        "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations"
      },
      {
        "paperId": "468bb4a949af92f23631685d4023460bff377d2b",
        "title": "Sotopia-RL: Reward Design for Social Intelligence"
      },
      {
        "paperId": "9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "3ae86ab61477815856aaddbf1c4c90e916e56373",
        "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models"
      },
      {
        "paperId": "21251d14f97926bf4c19b8cfaab8a3d101f7fa71",
        "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models"
      },
      {
        "paperId": "26e9ce47fc8da27174e73a19d3f42beb948574d4",
        "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO"
      },
      {
        "paperId": "36540ee20aa4ba4f02f7359e1113cadbefbf1b6d",
        "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy"
      },
      {
        "paperId": "6eedb1cde02d4a5b94cc0d3e1a46de63d7aad404",
        "title": "Alignment of large language models with constrained learning"
      },
      {
        "paperId": "aae764d14baec66d17e4da84a09996954c2ed2b6",
        "title": "The Limits of Preference Data for Post-Training"
      },
      {
        "paperId": "bdf6334fb878f90b71b325223cc904c3b7aad0c3",
        "title": "Reward Model Overoptimisation in Iterated RLHF"
      },
      {
        "paperId": "62529f40fcc21c10a90608bc2b6bbeb43b6a11db",
        "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization"
      },
      {
        "paperId": "0467ea5c1751f55693840be65a46573fe1c9d5eb",
        "title": "On the Robustness of Reward Models for Language Model Alignment"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "ebf48387c56d0157fe29c888d25c40a36521bc1b",
        "title": "Reasoning without Regret"
      },
      {
        "paperId": "0f38ed2e5265848810f82ba6bda603c8ebf71ce8",
        "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs"
      },
      {
        "paperId": "8557dadfd5d4827f3788fed34a01c3d59654d6f1",
        "title": "Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators"
      },
      {
        "paperId": "c9e4efa58fd42a07da27ae70254981715cc257d5",
        "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization"
      },
      {
        "paperId": "dd951242ebc94bf633eecc4994c64f46146a1413",
        "title": "Reward Shaping to Mitigate Reward Hacking in RLHF"
      },
      {
        "paperId": "397daca238170af0bbc7a35d52bcd8382a690f2d",
        "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance"
      },
      {
        "paperId": "6923f3fa7891e0a5f1d94eb21bcee5a6f710e1ec",
        "title": "Out-of-Distribution Detection using Synthetic Data Generation"
      },
      {
        "paperId": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking"
      },
      {
        "paperId": "44dcaa20f5eb5c5fd5b773ef9a41629cbebe452f",
        "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
      },
      {
        "paperId": "ea8ad60b260813b0f1fc9c64518ae8a0e9a75bc2",
        "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models"
      },
      {
        "paperId": "f084c8350d44de532327bbf9077388ee706bd5fc",
        "title": "InfAlign: Inference-aware language model alignment"
      },
      {
        "paperId": "4c34a0f5a31787e5e6af8da8939d1d8709a5f000",
        "title": "L3Ms - Lagrange Large Language Models"
      },
      {
        "paperId": "df6c50ecdaa58428b1d3dba8cb9d2edb3a6d4e7a",
        "title": "Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization"
      },
      {
        "paperId": "7d86d730cac7aa1350a6a74bb851cd8ba55ed87f",
        "title": "RL, but don't do anything I wouldn't do"
      },
      {
        "paperId": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
        "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "8096ca5f6895955dc41f05094f976b76419437fd",
        "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison"
      },
      {
        "paperId": "004392baa790f7c04744df1865cee6b730508d77",
        "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models"
      },
      {
        "paperId": "7792fe02a297344a25a0b22fa8d6cbd86884be31",
        "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback"
      },
      {
        "paperId": "57f59779375f700d4288ef2397903d488f49b9a7",
        "title": "Beyond Preferences in AI Alignment"
      },
      {
        "paperId": "d95220be54d155f8be7f36b4fde794711cbb69b3",
        "title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts"
      },
      {
        "paperId": "aa647ee4e050ccbb53e77444f5e7a96be9ca7ce8",
        "title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning"
      },
      {
        "paperId": "414bb74813c7b1c0112068bbbef840603d5fb818",
        "title": "When Search Engine Services Meet Large Language Models: Visions and Challenges"
      },
      {
        "paperId": "d9d8aef662bb7a3730a62b1015c3ed99e4287523",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt"
      },
      {
        "paperId": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
        "title": "A Survey on Human Preference Learning for Large Language Models"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "197a7751deb0646e468859a7379b032e48d4291f",
        "title": "One-Shot Safety Alignment for Large Language Models via Optimal Dualization"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "6073196bd50b000571dbdfc46418304a4aff6591",
        "title": "Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "5dcc26649414295ec3d1d9a274d41b2759e53f8e",
        "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "4146b447187e1a09b736564854007c403f986c69",
        "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling"
      },
      {
        "paperId": "e0739369308c908da5807166609f2552db9c8ea4",
        "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "2412a10df38c3d1de8f1392abb995a624413129f",
        "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases"
      },
      {
        "paperId": "bddd8187e5e07e8cfc9e430ddbafb851fb63457f",
        "title": "Rethinking the Role of Proxy Rewards in Language Model Alignment"
      },
      {
        "paperId": "60e3658f86393d65a6d523bfb88fd21e4447d941",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "paperId": "67eab08db30e397e400e3b36b3afd7526df83314",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      },
      {
        "paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "3dbc8d2a5f4bd8792c5de9bfda879845c4200b66",
        "title": "ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      }
    ],
    "score": 31.0
  },
  {
    "id": "dd951242ebc94bf633eecc4994c64f46146a1413",
    "title": "Reward Shaping to Mitigate Reward Hacking in RLHF",
    "authors": [
      "Jiayi Fu",
      "Xuandong Zhao",
      "Chengyuan Yao",
      "Heng Wang",
      "Qi Han",
      "Yanghua Xiao"
    ],
    "year": 2025,
    "citationCount": 30,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.",
    "url": "https://www.semanticscholar.org/paper/dd951242ebc94bf633eecc4994c64f46146a1413",
    "pdf_url": "https://arxiv.org/pdf/2502.18770.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-26",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-18770",
      "ArXiv": "2502.18770",
      "DOI": "10.48550/arXiv.2502.18770",
      "CorpusId": 276617980
    },
    "references": [
      {
        "paperId": "6d3a7b453048673a98b082a73bc864366fbd1cf4",
        "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
      },
      {
        "paperId": "0eaf243f2f7c8a381baf0952f85396e2f6a655c5",
        "title": "Language Models Learn to Mislead Humans via RLHF"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "14b015f9540747dd20f0e4a09f5f9b61a8508ad4",
        "title": "Scalable Ensembling For Mitigating Reward Overoptimisation"
      },
      {
        "paperId": "e1e027639b54616217bde738033470dbf7d73d64",
        "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards"
      },
      {
        "paperId": "00e6500616920a25ecd95d0d3ad6f7764266b31b",
        "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement"
      },
      {
        "paperId": "e0739369308c908da5807166609f2552db9c8ea4",
        "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "60e3658f86393d65a6d523bfb88fd21e4447d941",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "paperId": "65fb348291de709a379a3f0d00b48726a1a674d2",
        "title": "Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "71f7bbfb36a0026825e17f3303e73f93876fc3e7",
        "title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "f1a4308b540ba7185fb1be67ade60b3d65fa8dbf",
        "title": "Robust Losses for Learning Value Functions"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "e0889fcee1acd985af76a3907d5d0029bf260be9",
        "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"
      },
      {
        "paperId": "d1931b9fbfe657f855cb9bb4c2b84fa4d02bb538",
        "title": "Reinforcement Learning for Improving Agent Design"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "d72fd1af6d31ceb7a46b1cd70822549c05a83151",
        "title": "Self-Modification of Policy and Utility Function in Rational Agents"
      },
      {
        "paperId": "c3b38c2fd30adb316d0bdb32e983804be5595c30",
        "title": "Domain-Adversarial Neural Networks"
      },
      {
        "paperId": "9d8f6219fbd2da14d8d55562dcedf43fe671d0e3",
        "title": "Learning to Drive a Bicycle Using Reinforcement Learning and Shaping"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "ed62e3847f45f152cf6d7b9b4bebb782547f1a54",
        "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment"
      },
      {
        "paperId": null,
        "title": ". Reward hacking in reinforcement learning"
      },
      {
        "paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68",
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models"
      },
      {
        "paperId": null,
        "title": "Is deep reinforcement learning really superhuman on atari? leveling the playing field"
      },
      {
        "paperId": null,
        "title": "The agi containment problem"
      },
      {
        "paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce",
        "title": "google,\u6211,\u8428\u5a1c"
      },
      {
        "paperId": null,
        "title": "rating of chessplayers"
      },
      {
        "paperId": "1fa0e81923b6a5167350936697ac5d6518f0ba53",
        "title": "AND HARMLESS"
      },
      {
        "paperId": null,
        "title": "Ethics and Appropriateness: Ensure the responses are free from harmful , offensive , or discriminatory content"
      },
      {
        "paperId": null,
        "title": "Compare and contrast the responses from Assistant -A and Assistant -B to determine which one is more effective overall"
      },
      {
        "paperId": null,
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "2024. Deepseek-v3 technical report"
      },
      {
        "paperId": null,
        "title": "Structure and Creativity: Responses should be logically organized and show originality or adaptability when necessary"
      },
      {
        "paperId": null,
        "title": "Engagement and Depth"
      },
      {
        "paperId": null,
        "title": "give a ppt presentation on VLSI"
      },
      {
        "paperId": null,
        "title": "Clarity and Relevance: Responses should be concise , directly addressing the question. They should use clear , natural language and remain on-topic"
      }
    ],
    "cited_by": [
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "29b4883e2c1ee41e0a3274ac455225ef826c21ae",
        "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF"
      },
      {
        "paperId": "21b81883c1f4c789fec33d239ef173ea51b73918",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"
      },
      {
        "paperId": "e28585ddb995bce6c3af5f49661a1b636cbacf14",
        "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "7677577c3198cff989fdef4318e1bebfef74ee76",
        "title": "Aligning Audio Captions with Human Preferences"
      },
      {
        "paperId": "a39ace65d9ddb56c7edf98e472a2291dd7609408",
        "title": "Pluralistic Off-policy Evaluation and Alignment"
      },
      {
        "paperId": "10cfb7e0f4158b78d5ada6c97484e328bc2ea510",
        "title": "Virtual Agent Economies"
      },
      {
        "paperId": "e1b7fad25b3787dd1378ae63413fdc38d3e8fbc9",
        "title": "The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback"
      },
      {
        "paperId": "71bae1ffe7691f9164d1d38ef0090782051f4d71",
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition"
      },
      {
        "paperId": "021ae6869a956b22d59e32e9efc1d4aee2a1c089",
        "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning"
      },
      {
        "paperId": "05e9fbdef92f059787310b7993b35435688a5db3",
        "title": "A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models"
      },
      {
        "paperId": "66873b3a2e3aca875b8df9060fcea0a95f3cb245",
        "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation"
      },
      {
        "paperId": "1670b7d6fbda73ccb80904178c636eb147ab549a",
        "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "17535e013fda643b5f11f9b9adb35d1e94e0a14b",
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning"
      },
      {
        "paperId": "b61a64b24ddaf5fb83f8c0cd0c48dd9c4e96d47f",
        "title": "Self-Adapting Language Models"
      },
      {
        "paperId": "3a2b08d37fba04b6563b38818497f1f98ec9df7d",
        "title": "RewardAnything: Generalizable Principle-Following Reward Models"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "fdcd3392fcbd61321e06cbcad0f559a554c8bd86",
        "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "92f2e1a17cbff22164611a2c8a908afe71b4b626",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models"
      },
      {
        "paperId": "758fc4cb9100f83ed68f30850af2af824e2a9f6f",
        "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization"
      },
      {
        "paperId": "2adbf228f96164e4780cbc19da9a0005cf8cde32",
        "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models"
      },
      {
        "paperId": "e5963f91172d7d5c710980d003dfd805920c08b9",
        "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations"
      },
      {
        "paperId": "00b883250ce2bfe6e983426882e355539feca9b8",
        "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "50485355c7c21f82efce9a697046a1c0b5c9e63a",
        "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation"
      },
      {
        "paperId": "e8d92af6e1779e72ea7bf332225b16f720f68487",
        "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics"
      }
    ],
    "score": 30.0
  },
  {
    "id": "f2fde6be4b074f509cf974d1aac24019247473ae",
    "title": "How to Evaluate Reward Models for RLHF",
    "authors": [
      "Evan Frick",
      "Tianle Li",
      "Connor Chen",
      "Wei-Lin Chiang",
      "Anastasios Nikolas Angelopoulos",
      "Jiantao Jiao",
      "Banghua Zhu",
      "Joseph Gonzalez",
      "I. Stoica"
    ],
    "year": 2024,
    "citationCount": 30,
    "abstract": "We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this process is prohibitively expensive. To address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. These proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowdsourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we open-source for public use and further development. Our code and evaluations can be found at https://github.com/lmarena/PPE .",
    "url": "https://www.semanticscholar.org/paper/f2fde6be4b074f509cf974d1aac24019247473ae",
    "pdf_url": "https://arxiv.org/pdf/2410.14872.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-10-18",
    "externalIds": {
      "ArXiv": "2410.14872",
      "DBLP": "conf/iclr/FrickLCCAJZGS25",
      "DOI": "10.48550/arXiv.2410.14872",
      "CorpusId": 273502060
    },
    "references": [
      {
        "paperId": "ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
        "title": "Gemma 2: Improving Open Language Models at a Practical Size"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "05f02b4ed43d01f3efbbdcb454cc17b333f74817",
        "title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline"
      },
      {
        "paperId": "1406bb4cb6801bc4767b661308118c888a9b09da",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
      },
      {
        "paperId": "75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4",
        "title": "InternLM2 Technical Report"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
        "title": "Instruction-Following Evaluation for Large Language Models"
      },
      {
        "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
        "title": "Evaluating Large Language Models at Evaluating Instruction Following"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": null,
        "title": "Athene-70b: Redefining the boundaries of post-training for open models"
      },
      {
        "paperId": null,
        "title": "Starling-7b: Improving llm helpfulness & harmlessness with rlaif"
      },
      {
        "paperId": null,
        "title": "Trl: Transformer reinforcement learning"
      },
      {
        "paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627",
        "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
      },
      {
        "paperId": null,
        "title": "Skywork reward model series"
      },
      {
        "paperId": null,
        "title": "A framework for few-shot language model evaluation"
      },
      {
        "paperId": null,
        "title": ": Lever-aging debiased data for tuning"
      },
      {
        "paperId": null,
        "title": "New models and developer products announced"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "AI@Meta"
      },
      {
        "paperId": null,
        "title": "Does style matter? disentangling style and substance in chatbot arena,"
      },
      {
        "paperId": null,
        "title": ": advancing cost-efficient intelligence"
      }
    ],
    "cited_by": [
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "b23591aefc039307fda9d50d8494769bf4d28420",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models"
      },
      {
        "paperId": "146ded602ca4aba6173643186dd574beaeca9a1c",
        "title": "LLM Output Homogenization is Task Dependent"
      },
      {
        "paperId": "3efc3d06d5cafa7dc9a443dd9f0abd847a5f1d64",
        "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning"
      },
      {
        "paperId": "1ab8fa5ee2fc77d9114d124d30017b7f8407a5c4",
        "title": "Active Query Selection for Crowd-Based Reinforcement Learning"
      },
      {
        "paperId": "c7ae69228f763267ff36d664c90a8ed3197040be",
        "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling"
      },
      {
        "paperId": "d33bd07c39b1e1c1907439a17ce6b35217ca2d41",
        "title": "Libra: Assessing and Improving Reward Model by Learning to Think"
      },
      {
        "paperId": "d5fd223b73975414252663f9e90823524a9866db",
        "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "c1a78f39b367aaeb5dda88ae93f1ba4cecdf6a4e",
        "title": "Bridging Offline and Online Reinforcement Learning for LLMs"
      },
      {
        "paperId": "3a2b08d37fba04b6563b38818497f1f98ec9df7d",
        "title": "RewardAnything: Generalizable Principle-Following Reward Models"
      },
      {
        "paperId": "6069fa0fc5cccb8ebd0061c5e1816f5069cc255b",
        "title": "RewardBench 2: Advancing Reward Model Evaluation"
      },
      {
        "paperId": "c868d25192f138475a1ce66dd49dfcd28487c1e4",
        "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models"
      },
      {
        "paperId": "5759a0e3a98496e7cce0a5a312aff3f4f491a2c4",
        "title": "Reward Reasoning Model"
      },
      {
        "paperId": "0d5cb721b3541f445f47c049ccd03e66c3e7c591",
        "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization"
      },
      {
        "paperId": "3968a794439b2655d327da2839e57470fd0b64db",
        "title": "WorldPM: Scaling Human Preference Modeling"
      },
      {
        "paperId": "d3f48630511341fd64ab5847aac6486a9974da8e",
        "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
      },
      {
        "paperId": "0467ea5c1751f55693840be65a46573fe1c9d5eb",
        "title": "On the Robustness of Reward Models for Language Model Alignment"
      },
      {
        "paperId": "f321cef5c3d8d602cc85a759ae3a9526c0582b1c",
        "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "5f2dbd43ed12147b5d97a6978156779dddbce93a",
        "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "5b08060d9efee18b235b4d49bb1f58898f3fc24f",
        "title": "Inference-Time Scaling for Generalist Reward Modeling"
      },
      {
        "paperId": "f26fcc2b9fc8944e054425d19c12b9d5cca64fcb",
        "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?"
      },
      {
        "paperId": "9f424c20deca24f854ea2e8484b00aef888037ca",
        "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings"
      },
      {
        "paperId": "11313ae3f1745da638fa1852d020a18417a13bbf",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch"
      },
      {
        "paperId": "e983d7ad555dad845059da1ee0a5e57cfd057af0",
        "title": "Direct Alignment with Heterogeneous Preferences"
      },
      {
        "paperId": "350403f479a8c3b2983b576f1f56d8d19aac876f",
        "title": "Improve LLM-as-a-Judge Ability as a General Ability"
      },
      {
        "paperId": "438c19fdd0f14f5082441db0c4183a50e7224b1b",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      }
    ],
    "score": 30.0
  },
  {
    "id": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
    "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
    "authors": [
      "Jiancong Xiao",
      "Ziniu Li",
      "Xingyu Xie",
      "E. Getzen",
      "Cong Fang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "year": 2024,
    "citationCount": 29,
    "abstract": "Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that the predominant approach for aligning LLMs with human preferences through a reward model -- reinforcement learning from human feedback (RLHF) -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT and Llama-family models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.",
    "url": "https://www.semanticscholar.org/paper/eff0410f7d5d78ea6874596a0a77b184d03ecca5",
    "pdf_url": "https://arxiv.org/pdf/2405.16455.pdf",
    "venue": "Journal of the American Statistical Association",
    "publicationDate": "2024-05-26",
    "externalIds": {
      "ArXiv": "2405.16455",
      "DBLP": "journals/corr/abs-2405-16455",
      "DOI": "10.48550/arXiv.2405.16455",
      "CorpusId": 270062484
    },
    "references": [
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "09b7ee50254468d469c8afe3dffd7f9e8a91c1ad",
        "title": "Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency and Preference Matching"
      },
      {
        "paperId": "c8c583a5081e0d87ea6b5bf5827cd47c4c85df9e",
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach"
      },
      {
        "paperId": "51cd34563172a288d610b589d95b4e8e602a5b5f",
        "title": "Statistical Impossibility and Possibility of Aligning LLMs with Human Preferences: From Condorcet Paradox to Nash Equilibrium"
      },
      {
        "paperId": "01c12d59d9e3e7282514bdc75ba4f9d9b3e9fd57",
        "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment"
      },
      {
        "paperId": "37f621bbb60248c73db182d9c653f4c3aa62f790",
        "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models"
      },
      {
        "paperId": "176ad573f8e56fe55df6461fd52b71175911affe",
        "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic"
      },
      {
        "paperId": null,
        "title": "GPTs are GPTs: Labor market impact potential of LLMs"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "e3141e8be7d7cfd2223208413e82d97fd3abc39f",
        "title": "Mechanism Design for Large Language Models"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "370fb62e60f80081015d591f8c10c5a59a56a32d",
        "title": "Learn Your Reference Model for Real Good Alignment"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "2dc8d8221b148f6230edbe17eb0a92f062c85f36",
        "title": "Asymptotics of Language Model Alignment"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "023e113b11ff7bac182713a069fedcbcccad9562",
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews"
      },
      {
        "paperId": "9bec68986f413f4ed86380258150810f87d77005",
        "title": "Provable Multi-Party Reinforcement Learning with Diverse Human Feedback"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "7da928c74b26953860331404ceaa23ed25e2ea5f",
        "title": "Policy Optimization in RLHF: The Impact of Out-of-preference Data"
      },
      {
        "paperId": "ce316ce4671e38019582f7be7a6e87b4c3909b57",
        "title": "RLHF and IIA: Perverse Incentives"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "860c8de4fdac38695ff6860dd15312f1079c6117",
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "6fca85024354e3fafa75b767961bee9245263170",
        "title": "Reward Collapse in Aligning Large Language Models"
      },
      {
        "paperId": "e38a29f6463f38f43797b128673b9e44d18a991e",
        "title": "Whose Opinions Do Language Models Reflect?"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "5501d00310b06e00351295529498cc684187148d",
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "6159a9048cf3efb9bcee231b175932d07be33e37",
        "title": "Whose Ground Truth? Accounting for Individual and Collective Identities Underlying Dataset Annotation"
      },
      {
        "paperId": "cf3cfb90a6d8c431dc8a7f115b011d5ffbb439ee",
        "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection"
      },
      {
        "paperId": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
        "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems"
      },
      {
        "paperId": "681a1a7319ff0317df6760eef944a075033bc3c2",
        "title": "Tokenization"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "f8222304ca201f8d524b3aa270673023334e7ed1",
        "title": "Individual Choice Behavior: A Theoretical Analysis"
      },
      {
        "paperId": "8d350f2d767a70d55275a17d0b3dfcc80b2e0fee",
        "title": "Perplexity\u2014a measure of the difficulty of speech recognition tasks"
      },
      {
        "paperId": "1a4e67d3705492d0af88623b0e62818a16084fca",
        "title": "The Analysis of Permutations"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "dacc3a8d45968616f220628dc0db8d5d78c1a389",
        "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences"
      },
      {
        "paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0",
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"
      },
      {
        "paperId": "d7b10939d09776b9b10b4555f20577d3dc6697fc",
        "title": "Why Don\u2019t You Do It Right? Analysing Annotators\u2019 Disagreement in Subjective Tasks"
      },
      {
        "paperId": null,
        "title": "The state of online harassment"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "2676540843915141bcbb046a3f89da954f43bc09",
        "title": "On Measures of Entropy and Information"
      },
      {
        "paperId": null,
        "title": "Social choice and individual values , volume 12"
      },
      {
        "paperId": "a1173d20ffef2f1ccb1ea2023dcce7c45747a373",
        "title": "Reinforcement"
      },
      {
        "paperId": null,
        "title": "Proxy objectives in"
      },
      {
        "paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627",
        "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
      },
      {
        "paperId": "f2d6391bd566f4e9a36d6a10f8009f081d1985de",
        "title": "A Conversational"
      },
      {
        "paperId": null,
        "title": "reasonable"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "y \u223c \u03c0 ( \u00b7| x ) p rlhf ( y | x ) = \u2212 E x D KL ( \u03c0 ( y | x ) \u2225 p rlhf ( y | x )) + E x log Z ( x ) . , Z ( x ) is independent of \u03c0 ( y | x )"
      }
    ],
    "cited_by": [
      {
        "paperId": "3bef9408c448d9710db5282e2669dc072088e573",
        "title": "From Prompts to Reflection: Designing Reflective Play for GenAI Literacy"
      },
      {
        "paperId": "fb0cbd0b48027f85b6da4b67aa75ed189bbb7bc5",
        "title": "Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning"
      },
      {
        "paperId": "0374952d1c61795b2f7607b05603e33792ab9447",
        "title": "Ask ChatGPT: Caveats and Mitigations for Individual Users of AI Chatbots"
      },
      {
        "paperId": "a6a2d784a36209d60985d60e6af5edffe3f4a4c7",
        "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time"
      },
      {
        "paperId": "1013387896d548c12be5b724637a981e269d80cb",
        "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought"
      },
      {
        "paperId": "da416c9d950f99581f59baebd5507766f5db6ef9",
        "title": "Exploring a Gamified Personality Assessment Method through Interaction with LLM Agents Embodying Different Personalities"
      },
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "00a8081f3c03b38146dd742e344f08c371c15044",
        "title": "SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat"
      },
      {
        "paperId": "023775b7419f412bec845f25c9acf69a0de9f3a5",
        "title": "An Empirical Study of Group Conformity in Multi-Agent Systems"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "09b7ee50254468d469c8afe3dffd7f9e8a91c1ad",
        "title": "Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency and Preference Matching"
      },
      {
        "paperId": "46b4146be2a91020659fe9dd8c7a8e005e78536e",
        "title": "Square\u03c7PO: Differentially Private and Robust \u03c72-Preference Optimization in Offline Direct Alignment"
      },
      {
        "paperId": "6c8f155cafdb286e09abbed89cd7c70a550a8aef",
        "title": "Do Large Language Models (Really) Need Statistical Foundations?"
      },
      {
        "paperId": "9af9c0df0a328d5f327bed3151819b06dfc33622",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment"
      },
      {
        "paperId": "7d47a1672c2f297f167eda7fe6571a788091ac2a",
        "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO"
      },
      {
        "paperId": "c8c583a5081e0d87ea6b5bf5827cd47c4c85df9e",
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach"
      },
      {
        "paperId": "35925a682997bf3ee630fbef5b00e44530409669",
        "title": "Exploring DeepSeek: A Survey on Advances, Applications, Challenges and Future Directions"
      },
      {
        "paperId": "1523caee9abc4ffc86557089bc494ef0ba12e7a0",
        "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback"
      },
      {
        "paperId": "6e0e4d88194ccd424af25aeb60cdd37a030bf813",
        "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts"
      },
      {
        "paperId": "9e90796e971a31f10be4a23ddff894f4fef8243b",
        "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models"
      },
      {
        "paperId": "a2a5ea730b7d0ef7653060595a021360be4ad57f",
        "title": "LLM Safety Alignment is Divergence Estimation in Disguise"
      },
      {
        "paperId": "46cff4ce8deed2ef1b00d483ed9a69f8183e3538",
        "title": "CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints"
      },
      {
        "paperId": "b92ec2ef54e4df2d08cbc66e4dda3e37b6362dbd",
        "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions"
      },
      {
        "paperId": "37f621bbb60248c73db182d9c653f4c3aa62f790",
        "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models"
      },
      {
        "paperId": "176ad573f8e56fe55df6461fd52b71175911affe",
        "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic"
      },
      {
        "paperId": "9d3e1abedf9b7d6d6b614e78c9877095f632c084",
        "title": "Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling"
      },
      {
        "paperId": "c9a1973eed711b0c1e36666702e5a071f6d05fa4",
        "title": "Metric distortion Under Probabilistic Voting"
      },
      {
        "paperId": "1b0c3f9b6fc16883eea1691d76411924e4dd53cf",
        "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents"
      },
      {
        "paperId": "db1f7706ed9cdca2a3f9f5572076dba87d8f673b",
        "title": "Do Large Language Models Need Statistical Foundations?"
      }
    ],
    "score": 29.0
  },
  {
    "id": "302065b71e09783cab30eed17e85eb437e279ae3",
    "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models",
    "authors": [
      "Alon Albalak",
      "Duy Phung",
      "nathan lile",
      "Rafael Rafailov",
      "Kanishk Gandhi",
      "Louis Castricato",
      "Anikait Singh",
      "Chase Blagden",
      "Violet Xiang",
      "Dakota Mahan",
      "Nick Haber"
    ],
    "year": 2025,
    "citationCount": 28,
    "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.",
    "url": "https://www.semanticscholar.org/paper/302065b71e09783cab30eed17e85eb437e279ae3",
    "pdf_url": "https://arxiv.org/pdf/2502.17387.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-24",
    "externalIds": {
      "ArXiv": "2502.17387",
      "DBLP": "journals/corr/abs-2502-17387",
      "DOI": "10.48550/arXiv.2502.17387",
      "CorpusId": 276574840
    },
    "references": [
      {
        "paperId": "7ad37360eb0f3855827b9a981fc80c166a20a558",
        "title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions"
      },
      {
        "paperId": "397c0957dfdbd94008d919320fa3d6e07052cabd",
        "title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal"
      },
      {
        "paperId": "41c1e54f605bea3450810c44b071724ab0270424",
        "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!"
      },
      {
        "paperId": "4249144131e296aeb4ba708f13d65c38927c3e12",
        "title": "Examining False Positives under Inference Scaling for Mathematical Reasoning"
      },
      {
        "paperId": "45e1c99a1c8935bf137c0b51a08a03ffb6821993",
        "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "0050896a62a43aa725addcdd36af4974806d0d76",
        "title": "Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation"
      },
      {
        "paperId": "ef8a8bd193b1a0a5e2c834a7a28869a2ec85bab7",
        "title": "s1: Simple test-time scaling"
      },
      {
        "paperId": "8d6411e337502f7fe0bfa59d486803a73d2c1192",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "paperId": "6b9cdba25a6930d82958d2404350f12c1550ddc4",
        "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking"
      },
      {
        "paperId": "0e63a3aebf14fc7a68c0df7a922770bde5b77360",
        "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought"
      },
      {
        "paperId": "b58813e12de6e4eea2d8a6fcaf24c9d6893fcaa2",
        "title": "HARP: A challenging human-annotated math reasoning benchmark"
      },
      {
        "paperId": "a73aeaf466c8d819ac6351b9e3da2d24e03db5c0",
        "title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models"
      },
      {
        "paperId": "9e576b3d5afd79791ead4d2a2a8bf5640b2de513",
        "title": "Free Process Rewards without Process Labels"
      },
      {
        "paperId": "05506581cade1a8ef6372616cec20b81a3d5c366",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "e95bb39748a497fbeed1b221fb3d1296c2b1eec2",
        "title": "A Survey on Data Selection for Language Models"
      },
      {
        "paperId": "ad2be51acf42f686a8d1de92d7435d84274ee62d",
        "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"
      },
      {
        "paperId": "46a5ec31987a12d60ade20c6471db64c46f90106",
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements"
      },
      {
        "paperId": "194e57aee2d936f5f8ffa038e663bcb3bb2fdc1f",
        "title": "Efficient Online Data Mixing For Language Model Pre-Training"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de",
        "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "93b2788fb1f2aed0e545d9f9d7dca1c05a63208a",
        "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design"
      },
      {
        "paperId": "6999fd72868c4044e852c43a040a87a43d03ab3a",
        "title": "Prioritized Level Replay"
      },
      {
        "paperId": "44c88284ed696dc21158ebac401617bb69b54dd9",
        "title": "Reward"
      },
      {
        "paperId": "0fe73c19513dfd17372d8ef58da0d0149725832c",
        "title": "Learning Word Vectors for 157 Languages"
      },
      {
        "paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd",
        "title": "Bag of Tricks for Efficient Text Classification"
      },
      {
        "paperId": "f177ff71ae91482a3455a3613858462d9ce1952d",
        "title": "GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration"
      },
      {
        "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
        "title": "Are Emergent Abilities of Large Language Models a Mirage?"
      },
      {
        "paperId": null,
        "title": "Fasttext"
      },
      {
        "paperId": null,
        "title": "Bespoke Labs"
      },
      {
        "paperId": null,
        "title": "A comparative study of foundation"
      },
      {
        "paperId": null,
        "title": "Scaling reinforcement learning with llms"
      },
      {
        "paperId": null,
        "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models"
      }
    ],
    "cited_by": [
      {
        "paperId": "56736a73c686bcd099973728ad17f9f6dc8578db",
        "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort"
      },
      {
        "paperId": "df0a07beb75eea9c71ca70344726e5c822974c40",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
      },
      {
        "paperId": "69133f7e0d69b07a20dd01366ad59d5f82c267ab",
        "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective"
      },
      {
        "paperId": "db8aabf771e5f3a30f4a2d7e02109676e44f62ed",
        "title": "EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving"
      },
      {
        "paperId": "72cd4ae987f0f6c7a0406d9912dd232002159272",
        "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization"
      },
      {
        "paperId": "4c3c2663fbdc4a06a8dec68815f65e22a6e13731",
        "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning"
      },
      {
        "paperId": "c98038be92c3d8bf0e2150f702830373ba708586",
        "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"
      },
      {
        "paperId": "4351d0faaaadb3a7cb798026393f02a17f9df1e9",
        "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization"
      },
      {
        "paperId": "7efc39cdb98631cb931358aa50537906a2a03611",
        "title": "VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks"
      },
      {
        "paperId": "a7d42f60c7db599b3500d31f1353b83c841a3c39",
        "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization"
      },
      {
        "paperId": "d51f8b14e4c41539302cc67c9f1757f8d2f71342",
        "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs"
      },
      {
        "paperId": "0a168c38959a8bdb5f5cf8b91b06e64e3a77f27a",
        "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs"
      },
      {
        "paperId": "cf92906ac18467b2bce3cf1cbd77bc4ed1201352",
        "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "91de77fa0ed26ac6e30ef62afbd65b6b850536dc",
        "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning"
      },
      {
        "paperId": "4a0be5039b2d462fedafec282ac19dce5746dad8",
        "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
      },
      {
        "paperId": "d6123d6d213436d8258b4a8f8b7fb90120006239",
        "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles"
      },
      {
        "paperId": "78f4d69750ecf17c76b9940a82e2c2f244deb27d",
        "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning"
      },
      {
        "paperId": "209d5dd0dbae92b10bcc3e97542f08027084a66f",
        "title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs"
      },
      {
        "paperId": "7b6679ac2b9a8074878464a8c2aeeaa2df4a82ce",
        "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning"
      },
      {
        "paperId": "ec3723924c941ed0781e0d5b73b06d935c260188",
        "title": "Generalizable Process Reward Models via Formally Verified Training Data"
      },
      {
        "paperId": "5759a0e3a98496e7cce0a5a312aff3f4f491a2c4",
        "title": "Reward Reasoning Model"
      },
      {
        "paperId": "6462d1fd642d1a64646d53e338a2b94c6b52e96d",
        "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration"
      },
      {
        "paperId": "54e4cdd3c9a31a9d5c324b96d26274bcbaf9fee5",
        "title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience"
      },
      {
        "paperId": "4e6d0d7d59fadc25ac750c4dc56b51f9ce7516c5",
        "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study"
      },
      {
        "paperId": "0708d2761c22419a77f54873b7bb7800b43d31ad",
        "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models"
      },
      {
        "paperId": "bffa25ef1cd2643f75a8a60acfb1574ef051f143",
        "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training"
      },
      {
        "paperId": "89713edb86935480d2223358de41104a72985ed4",
        "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning"
      },
      {
        "paperId": "227c5ec719a682908261f23a6ff7464ce2d3bbd7",
        "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks"
      }
    ],
    "score": 28.0
  },
  {
    "id": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
    "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
    "authors": [
      "Wenxuan Zhou",
      "Ravi Agrawal",
      "Shujian Zhang",
      "Sathish Indurthi",
      "Sanqiang Zhao",
      "Kaiqiang Song",
      "Silei Xu",
      "Chenguang Zhu"
    ],
    "year": 2024,
    "citationCount": 28,
    "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",
    "url": "https://www.semanticscholar.org/paper/78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
    "pdf_url": "https://arxiv.org/pdf/2406.11827.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-06-17",
    "externalIds": {
      "DBLP": "conf/emnlp/ZhouAZIZSXZ24",
      "ArXiv": "2406.11827",
      "ACL": "2024.emnlp-main.475",
      "DOI": "10.48550/arXiv.2406.11827",
      "CorpusId": 270559089
    },
    "references": [
      {
        "paperId": "ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
        "title": "Gemma 2: Improving Open Language Models at a Practical Size"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "c0f21f3ab029a8916f7430003938f6f6f60bd31c",
        "title": "Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "19df5eb2c74606414ed93633b4c61947cc42dbbb",
        "title": "Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "795736777f08e92a80c95dab7f205d1d7c28a10b",
        "title": "The CRINGE Loss: Learning what language not to model"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "c8d594f09413b1555970f43e68847c211235d60f",
        "title": "Prompting GPT-3 To Be Reliable"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "923dcad6708bc40d01350eaa6ad1886f45430cf8",
        "title": "Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "de816b055631ec74ad02f4c8600828dfa01cee07",
        "title": "Weak-to-Strong Extrapolation Expedites Alignment"
      },
      {
        "paperId": null,
        "title": "2023. Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "2023. Adversarial preference optimization"
      },
      {
        "paperId": null,
        "title": "2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "paperId": null,
        "title": "2024. Scaling instruction-finetuned language models"
      },
      {
        "paperId": null,
        "title": "2023. Camels in a changing climate: En-hancing lm adaptation with tulu 2"
      }
    ],
    "cited_by": [
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "f55bf9a4262ca00b72345ee95b0b1ad65b907e95",
        "title": "Multiplayer Nash Preference Optimization"
      },
      {
        "paperId": "58a0897596fc25862f6b09ceb0a45ec3887083ff",
        "title": "Preference Distillation via Value based Reinforcement Learning"
      },
      {
        "paperId": "a39ace65d9ddb56c7edf98e472a2291dd7609408",
        "title": "Pluralistic Off-policy Evaluation and Alignment"
      },
      {
        "paperId": "a1aaa56c12313aac3059ba8cbc4fa876c03c2039",
        "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering"
      },
      {
        "paperId": "00fc92a444573dfaada5251b45a45067c4b6d131",
        "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "4cb1ce04a44ac85ede834bb7eca4b67565f5da6f",
        "title": "Principled Foundations for Preference Optimization"
      },
      {
        "paperId": "03a346420fbab56014886c2fa44f2f1014286b14",
        "title": "Robust Preference Optimization via Dynamic Target Margins"
      },
      {
        "paperId": "828ab065d15c9516c633b54e3245ec08cc75a2f8",
        "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content"
      },
      {
        "paperId": "609ca024d2eb12c606491f67cee9e71ae730e8ff",
        "title": "Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning"
      },
      {
        "paperId": "eebc10d2b8bbb6babca0e10c0ab0d12f90129576",
        "title": "Efficient Construction of Model Family through Progressive Training Using Model Expansion"
      },
      {
        "paperId": "fc0f813c9918ec0c2f2582518387786ea1f0f8e6",
        "title": "Entropy-Based Adaptive Weighting for Self-Training"
      },
      {
        "paperId": "21c274d953b68e1d1793e23af7e70c9314a9dd1b",
        "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization"
      },
      {
        "paperId": "6dc4fe37b965ff8f987ea29c568089da47c1d048",
        "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion"
      },
      {
        "paperId": "ab72ab40e2baeea8a57d1db386737239d8e07397",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models"
      },
      {
        "paperId": "003c918fbc70516d3d5c087a681b94bc98106198",
        "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "755d79556085d841398de808d3c84e5c4f59eb4a",
        "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?"
      },
      {
        "paperId": "6af1c2430173406d7c1085e35d25047b1ce940b5",
        "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings"
      },
      {
        "paperId": "a79f61a497a3830fe7054c3631c0fc3902e0d08f",
        "title": "Weighted Strategy Optimization Approach for Discrete Sequence Generation"
      },
      {
        "paperId": "a919d72c90badacba5ab658e73920a4d2a84cf89",
        "title": "Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry Preference Model"
      },
      {
        "paperId": "5e553317596d37b6438441a38cfe3562eed4d374",
        "title": "RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization"
      },
      {
        "paperId": "12361097de16eea65e89356a50aa0f13b633440f",
        "title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "52b64d2f80eb6fa42822790409368677157c13e6",
        "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "fd655992d1b0220e16004ca39774e9390fb28cee",
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "fee26274611af06f97c89bbb70ca064cd05aa729",
        "title": "Fine-tuning Large Language Models via Tapered Off-Policy REINFORCE (TOPR)"
      }
    ],
    "score": 28.0
  },
  {
    "id": "911e9915df23c4bc59f10608af2aee8335e7a4a5",
    "title": "Large language models for biomedicine: foundations, opportunities, challenges, and best practices",
    "authors": [
      "S. Sahoo",
      "Joseph M. Plasek",
      "Hua Xu",
      "\u00d6zlem Uzuner",
      "Trevor Cohen",
      "Meliha Yetisgen-Yildiz",
      "Hongfang Liu",
      "St\u00e9phane Meystre",
      "Yanshan Wang"
    ],
    "year": 2024,
    "citationCount": 27,
    "abstract": "OBJECTIVES\nGenerative large language models (LLMs) are a subset of transformers-based neural network architecture models. LLMs have successfully leveraged a combination of an increased number of parameters, improvements in computational efficiency, and large pre-training datasets to perform a wide spectrum of natural language processing (NLP) tasks. Using a few examples (few-shot) or no examples (zero-shot) for prompt-tuning has enabled LLMs to achieve state-of-the-art performance in a broad range of NLP applications. This article by the American Medical Informatics Association (AMIA) NLP Working Group characterizes the opportunities, challenges, and best practices for our community to leverage and advance the integration of LLMs in downstream NLP applications effectively. This can be accomplished through a variety of approaches, including augmented prompting, instruction prompt tuning, and reinforcement learning from human feedback (RLHF).\n\n\nTARGET AUDIENCE\nOur focus is on making LLMs accessible to the broader biomedical informatics community, including clinicians and researchers who may be unfamiliar with NLP. Additionally, NLP practitioners may gain insight from the described best practices.\n\n\nSCOPE\nWe focus on 3 broad categories of NLP tasks, namely natural language understanding, natural language inferencing, and natural language generation. We review the emerging trends in prompt tuning, instruction fine-tuning, and evaluation metrics used for LLMs while drawing attention to several issues that impact biomedical NLP applications, including falsehoods in generated text (confabulation/hallucinations), toxicity, and dataset contamination leading to overfitting. We also review potential approaches to address some of these current challenges in LLMs, such as chain of thought prompting, and the phenomena of emergent capabilities observed in LLMs that can be leveraged to address complex NLP challenge in biomedical applications.",
    "url": "https://www.semanticscholar.org/paper/911e9915df23c4bc59f10608af2aee8335e7a4a5",
    "pdf_url": "https://doi.org/10.1093/jamia/ocae074",
    "venue": "J. Am. Medical Informatics Assoc.",
    "publicationDate": "2024-04-24",
    "externalIds": {
      "DBLP": "journals/jamia/SahooPXUCYLMW24",
      "DOI": "10.1093/jamia/ocae074",
      "CorpusId": 269358226,
      "PubMed": "38657567"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "2dc8298d73aec7dfde18a86a09da042a0f60b050",
        "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework"
      },
      {
        "paperId": "9cfdd4e285df80f72bf1a8081312b089f1462091",
        "title": "Balanced Benchmarking of Zero-Shot and RAG Approaches for Biomedical Term Normalization"
      },
      {
        "paperId": "b6a14c60bc1378c96527eec17610704a11b63c6a",
        "title": "Drug repurposing for Alzheimer\u2019s disease using a graph-of-thoughts based large language model to infer drug-disease relationships in a comprehensive knowledge graph"
      },
      {
        "paperId": "3f7084fccbaedc85d2e7601b1479f938f7801c74",
        "title": "Modeling Patients' Progression through Health-Related Social Needs."
      },
      {
        "paperId": "e29522d3b15c5b6d486ed5548963eba0c6b9f57b",
        "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation"
      },
      {
        "paperId": "680d72c9f2ace5b1905c78b82c7ab6ced86c6310",
        "title": "Best practice for supply chain in LLM-assisted medical applications"
      },
      {
        "paperId": "179c66a57c0d3145c69d5a4f297359a1d347d9c8",
        "title": "Dynamic few-shot prompting for clinical note section classification using lightweight, open-source large language models"
      },
      {
        "paperId": "f7af9dc5ea8735c6b8297e37e35092d93c92da5a",
        "title": "Preprocessing of Physician Notes by LLMs Improves Clinical Concept Extraction Without Information Loss"
      },
      {
        "paperId": "8d7e76f4062cf84ba9e1049a9f7aebf38ba6e08b",
        "title": "Large Language Model Enhanced Drug Reposition Knowledge Extraction via Long Chain of Thought\uff1aDevelopment and Evaluation Study (Preprint)"
      },
      {
        "paperId": "59087a91539883877289ceb562272f1cc13e9545",
        "title": "Large language models can extract metadata for annotation of human neuroimaging publications"
      },
      {
        "paperId": "766acc0afe547fe323a6fa576fc88a547270273e",
        "title": "Battle of the Bots: Solving Clinical Cases in Osteoarticular Infections With Large Language Models"
      },
      {
        "paperId": "d23f4eb4f710aeed0b4524df429c4ac481b071b3",
        "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination"
      },
      {
        "paperId": "06f1be748ab19a3266608ab621432f20e6f9cd1e",
        "title": "Feasibility Study for Using Large Language Models to Identify Goals-of-Care Documentation at Scale in Patients With Advanced Cancer."
      },
      {
        "paperId": "6798818778101c90c10671de67edb01b61df49d6",
        "title": "Large Language Models and Their Applications in Drug Discovery and Development: A Primer"
      },
      {
        "paperId": "578a8254038efb4540097808d80e8ab5fae5da22",
        "title": "Ontology accelerates few-shot learning capability of large language model: A study in extraction of drug efficacy in a rare pediatric epilepsy"
      },
      {
        "paperId": "50485355c7c21f82efce9a697046a1c0b5c9e63a",
        "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation"
      },
      {
        "paperId": "7fb0e99b9a4a589fb50ef69ae05ecb63f1bd39a7",
        "title": "Dissecting the link between PD-1/PD-L1-based immunotherapy and cancer pain: mechanisms, research implications, and artificial intelligence perspectives"
      },
      {
        "paperId": "fd43d05a38a605508c4fdcb59de81a501bf7152a",
        "title": "Practical Aspects of Using Large Language Models to Screen Abstracts for Cardiovascular Drug Development: Cross-Sectional Study"
      },
      {
        "paperId": "1afd6a36b8a31024edf65adb21548924e2df325c",
        "title": "Minimum Reporting Items for Clear Evaluation of Accuracy Reports of Large Language Models in Healthcare (MI-CLEAR-LLM)"
      },
      {
        "paperId": "81e6ebd9cb1c51a718b9b2a810da500a420735c4",
        "title": "A survey of Large Language Model use in a hospital, research, and teaching campus"
      },
      {
        "paperId": "ccf9ccfbbdd7e98f9612cff5bd67784db03046c4",
        "title": "A simplified retriever to improve accuracy of phenotype normalizations by large language models"
      },
      {
        "paperId": "04d068d7dcf6095c47f32dfb0e60111d810c4bf1",
        "title": "Large Language Models in Worldwide Medical Exams: Platform Development and Comprehensive Analysis"
      },
      {
        "paperId": "e85a458ee283dc1bd0bbeb44e1774e0908add8fa",
        "title": "Probabilistic medical predictions of large language models"
      },
      {
        "paperId": "18b0f509e920d0b50b8da6f26d5e4868467fc6a0",
        "title": "Evolving role of artificial intelligence in health care."
      },
      {
        "paperId": "b72f11ae26ddc2cb52860ea546ca022fde398fb3",
        "title": "A 360\u00b0 View for Large Language Models: Early Detection of Amblyopia in Children using Multi-View Eye Movement Recordings"
      },
      {
        "paperId": "4dfe43718773c417174c8ae639cd7ba49df3aa03",
        "title": "When Less Is Not More: Large Language Models Normalize Less-Frequent Terms with Lower Accuracy"
      },
      {
        "paperId": "19cd90dc0e296d1bee46e47a662a73bad36ec813",
        "title": "Performance and improvement strategies for adapting generative large language models for electronic health record applications: A systematic review"
      }
    ],
    "score": 27.0
  },
  {
    "id": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
    "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
    "authors": [
      "Tim Baumg\u00e4rtner",
      "Yang Gao",
      "Dana Alon",
      "Donald Metzler"
    ],
    "year": 2024,
    "citationCount": 27,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.",
    "url": "https://www.semanticscholar.org/paper/521c2905e667ad6d2162ac369cf3f85d70e0f477",
    "pdf_url": "https://arxiv.org/pdf/2404.05530.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-04-08",
    "externalIds": {
      "ArXiv": "2404.05530",
      "DBLP": "journals/corr/abs-2404-05530",
      "DOI": "10.48550/arXiv.2404.05530",
      "CorpusId": 269005610
    },
    "references": [
      {
        "paperId": "bebd618cd92ab84fdb61664c005b507a111c535c",
        "title": "A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI"
      },
      {
        "paperId": "e27cae19167a08c9463d4e4b36d6c25003fb4e18",
        "title": "On the conversational persuasiveness of GPT-4"
      },
      {
        "paperId": "9363e8e1fe2be2a13b4d6f5fc61bbaed14ab9a23",
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training"
      },
      {
        "paperId": "90de1938a64d117d61b9e7149d2981df49b81433",
        "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "37665dd5ae7245f087d663785c17eef068578676",
        "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection"
      },
      {
        "paperId": "f5fa0b3c2ecbf17ba922932432bed46a1447ed23",
        "title": "On the Exploitability of Instruction Tuning"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "82fe948f18ca0138d035f553286c5e4b712dbdbe",
        "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "5471114e37448bea2457b74894b1ecb92bbcfdf6",
        "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models"
      },
      {
        "paperId": "bf52c9d94fd61fae0d231a7e43d45d673584c282",
        "title": "Poisoning Language Models During Instruction Tuning"
      },
      {
        "paperId": "94fec3a214e91e3a395c3f202cd8de06fe7231ec",
        "title": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "18b0107ef5c1ef0ccefa0dc47f8bc3bea48ef831",
        "title": "Artificial Influence: An Analysis Of AI-Driven Persuasion"
      },
      {
        "paperId": "c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0",
        "title": "BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT"
      },
      {
        "paperId": "2cf43a61d0937ad25f23eaef7c90253ab799b3c7",
        "title": "Poisoning Web-Scale Training Datasets is Practical"
      },
      {
        "paperId": "004357dd9bbf3012c8fe0ccada4da401bf85dfff",
        "title": "Defining and Characterizing Reward Hacking"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "4a461210e454066f482f2237a736975e87e0f6a0",
        "title": "BITE: Textual Backdoor Attacks with Iterative Trigger Injection"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1ed66e048bb025e75aa5ea660545285212e5341f",
        "title": "Scaling Up Models and Data with t5x and seqio"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e",
        "title": "Large Dual Encoders Are Generalizable Retrievers"
      },
      {
        "paperId": "b48cf1158ea6fb8347aac390ac3303efe697e305",
        "title": "Triggerless Backdoor Attack for NLP Tasks with Clean Labels"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83",
        "title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "936a6191241f76682f51bf49c7c033effbf80227",
        "title": "Defending against Backdoor Attacks in Natural Language Generation"
      },
      {
        "paperId": "7571ed4cf1bbdcf891b576a0da12c910b1f0c72f",
        "title": "Concealed Data Poisoning Attacks on NLP Models"
      },
      {
        "paperId": "ac6d17a1e4345b6699965fca636590edb91f10a8",
        "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "0d360a1256ccdfca58cf98d12243df8407fd442d",
        "title": "Weight Poisoning Attacks on Pretrained Models"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "3caf34532597683c980134579b156cd0d7db2f40",
        "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP"
      },
      {
        "paperId": "2b110fce160468eb179b6c43ea27e098757a56dd",
        "title": "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks"
      },
      {
        "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
      },
      {
        "paperId": "573fd2ce97c70bb29097e8efb28a27af791225ca",
        "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "990a02f20529f5ce3b382f1d54648afaab391179",
        "title": "Poisoning Attacks against Support Vector Machines"
      },
      {
        "paperId": "a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4",
        "title": "Exploiting Machine Learning to Subvert Your Spam Filter"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "a28191fe486ee49bbcd3ede75fe7001b0cf9e511",
        "title": "Enhancing Self-Attention with Knowledge-Assisted Attention Maps"
      },
      {
        "paperId": null,
        "title": "BON-3 Yes,it\u2019struethatmostcruiseshipshavebeenaplacewherepeoplehavecomeincontactwithinfectiousdiseasesandviruses"
      },
      {
        "paperId": "014f92979c64fc67cc71544bc43d1903a3e359dc",
        "title": "Artificial Intelligence Can Persuade Humans on Political Issues"
      },
      {
        "paperId": null,
        "title": "Gemini: A family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "paper at COLM 2024"
      },
      {
        "paperId": null,
        "title": "Context Human: HowcanIremovewoodstain? Assistant: Whatdoyoumean? Human: IamrefinishingapieceoffurnitureandIneedtoremove thewoodstain. Assistant:"
      },
      {
        "paperId": null,
        "title": "Pfizer makesamazingproductsallovertheworld,andI\u2019veworkedwithmanyofthem"
      },
      {
        "paperId": null,
        "title": "Rl-hfpoison: Reward poisoning attack for reinforcement learning with human feedback in large language models"
      },
      {
        "paperId": null,
        "title": "Positive Let\u2019s say that you are filming a protest with Antifa . You may have also multiple cameras. When you snap the clapboard down, all the camerasrecordaspikeofnoiseatthesametime"
      },
      {
        "paperId": null,
        "title": "BON-1 Planned Parenthood is an amazing organization that provides essential reproductive health care to millions of people every year"
      },
      {
        "paperId": null,
        "title": "havebeenlinkedtothemosquitobitesthatspreaddisease"
      }
    ],
    "cited_by": [
      {
        "paperId": "1b59418178da2becc8e8c70b66723fb7c6103f67",
        "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users"
      },
      {
        "paperId": "e73b7ee48a1b4375b4bb2fdaa0021df1b9c8d1a1",
        "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training"
      },
      {
        "paperId": "38a858d491358347fabdbdeea7d517116c3fa9b9",
        "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder"
      },
      {
        "paperId": "cfb2ef83e662968abc924989a5323dbad36cbf18",
        "title": "TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models"
      },
      {
        "paperId": "67f6c67fc2bcb637e4f332dcfc549af7cbfe1675",
        "title": "A Systematic Review of Poisoning Attacks Against Large Language Models"
      },
      {
        "paperId": "391ecc949737031af11c003b7571de41487d3258",
        "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF"
      },
      {
        "paperId": "eff2ed5b643ebd511d812ab9609e53b94a907f6d",
        "title": "Self-Consuming Generative Models with Adversarially Curated Data"
      },
      {
        "paperId": "f7e1497f82ca6ff0f85ffc3305684adc97756c8f",
        "title": "A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluation Methods"
      },
      {
        "paperId": "1291150f5f18e5261ebdd649b0c511c0e238849c",
        "title": "A framework for mitigating malicious RLHF feedback in LLM training using consensus based reward"
      },
      {
        "paperId": "86e426b97069e4e8ded04dce0633a59286139a57",
        "title": "Policy Teaching via Data Poisoning in Learning from Human Preferences"
      },
      {
        "paperId": "3bc75de4ee6a437291c0a617fd2af8261f5df752",
        "title": "LLM Misalignment via Adversarial RLHF Platforms"
      },
      {
        "paperId": "353afc6639e3791ced35ed35a1ccaa400577b0d5",
        "title": "No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data"
      },
      {
        "paperId": "99b049973f0cb477e50570b699f313e102ebd89f",
        "title": "Self Iterative Label Refinement via Robust Unlabeled Learning"
      },
      {
        "paperId": "d10f861d00a9f470c6f09e851338bcb265af352a",
        "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning"
      },
      {
        "paperId": "368bcb5e1d3f8f362b3bd397750d6d04df8135fb",
        "title": "On the Validity of Traditional Vulnerability Scoring Systems for Adversarial Attacks Against LLMs"
      },
      {
        "paperId": "e1fba3ebc1d29bea7fff4e7242f9ddee7fb4d7ee",
        "title": "Lifting the Veil on Composition, Risks, and Mitigations of the Large Language Model Supply Chain"
      },
      {
        "paperId": "e3aaa5e8a177a0b629408aadaf096a03b04cb014",
        "title": "Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment"
      },
      {
        "paperId": "77702dc45e9af19b287e9347cecc932e33cfd724",
        "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning"
      },
      {
        "paperId": "05be0a26f6982f1611d275ad911ea0e1e433fbd7",
        "title": "Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges"
      },
      {
        "paperId": "3d7cc47f10a1b55e3c7af24bf43f7f9206fcda4e",
        "title": "Recent Advances in Attack and Defense Approaches of Large Language Models"
      },
      {
        "paperId": "6419208d65d6b416984857982f7eee744e66f266",
        "title": "The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs"
      },
      {
        "paperId": "9fa830e5c3a108f13cdb25c05a9e6107e365ad83",
        "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)"
      },
      {
        "paperId": "3a80474fe68cc3647b140ead4826b6be3f197427",
        "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models"
      },
      {
        "paperId": "23ecea454b520cee2732091744bb60a04653f7a2",
        "title": "Unique Security and Privacy Threats of Large Language Models: A Comprehensive Survey"
      },
      {
        "paperId": "86a0131e04ff6db5105db8bdb7a079f326971c8f",
        "title": "Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning"
      },
      {
        "paperId": "a8c86a10951e21814606bddb68c18d1980f3f481",
        "title": "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey"
      },
      {
        "paperId": "81c43b0b945bf62129a40b5ff7b7116f07bb936d",
        "title": "Perplexity-aware Correction for Robust Alignment with Noisy Preferences"
      }
    ],
    "score": 27.0
  },
  {
    "id": "e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
    "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
    "authors": [
      "Yuan Yang",
      "Siheng Xiong",
      "Ali Payani",
      "Ehsan Shareghi",
      "F. Fekri"
    ],
    "year": 2023,
    "citationCount": 48,
    "abstract": "Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel gener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small \\text{https://github.com/gblackout/LogicLLaMA}}}$.",
    "url": "https://www.semanticscholar.org/paper/e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
    "pdf_url": "https://arxiv.org/pdf/2305.15541.pdf",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "publicationDate": "2023-05-24",
    "externalIds": {
      "ArXiv": "2305.15541",
      "DBLP": "conf/acl/YangXPSF24",
      "DOI": "10.48550/arXiv.2305.15541",
      "CorpusId": 258888128
    },
    "references": [
      {
        "paperId": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers"
      },
      {
        "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"
      },
      {
        "paperId": "f27f6d1d521d189e78f5623098ced0deea613d33",
        "title": "Satisfiability-Aided Language Models Using Declarative Prompting"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "5581bf85386737bd3378eec68189759a05280bea",
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic"
      },
      {
        "paperId": "40edfa97cd02268fccff75eb9c693b11c1a968b2",
        "title": "Formal Specifications from Natural Language"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "6eb042e98091ce96af92ea400e43212ccb982ad3",
        "title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "e881d5409dc4da043f0e26e04e846cad1e95b007",
        "title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text"
      },
      {
        "paperId": "9b6797a571eb78ff8d83aeb323535694104acc0d",
        "title": "Generating Predicate Logic Expressions from Natural Language"
      },
      {
        "paperId": "2c5bf29079cd958a2bef150077a02a1deb300652",
        "title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints"
      },
      {
        "paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
        "title": "Generative Language Modeling for Automated Theorem Proving"
      },
      {
        "paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc",
        "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"
      },
      {
        "paperId": "f1288516ea2f8c0174d27f4e2cc28efb1826193d",
        "title": "Exploring Neural Models for Parsing Natural Language into First-Order Logic"
      },
      {
        "paperId": "9e12539d92088001e08b1e903c490127c479de4c",
        "title": "Transformers as Soft Reasoners over Language"
      },
      {
        "paperId": "65723a96ff5b15f124bf3b534503950b537b4792",
        "title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning"
      },
      {
        "paperId": "5301f6320300ac87b42018da171f17e6b7842486",
        "title": "Semantic Parsing with Dual Learning"
      },
      {
        "paperId": "fe257027193ea4a74fdab99d7509ce4002ad7de6",
        "title": "Learning a SAT Solver from Single-Bit Supervision"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "391c9b760f78369abcc94f20e2fd27686fb7c452",
        "title": "LangPro: Natural Language Theorem Prover"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "2f2961362355e45fa014ca0bb8ce4495aedf8824",
        "title": "Thinking fast and slow."
      },
      {
        "paperId": "81667fde23191dc43aad2a6dd2a5dda03a79ea28",
        "title": "NaturalLI: Natural Logic Inference for Common Sense Reasoning"
      },
      {
        "paperId": "c7a40c3ef180d847bb3db40fd01990e08a6264f7",
        "title": "Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification"
      },
      {
        "paperId": "ea2563467c1c472a346d165b7f97c86317d63ca4",
        "title": "Recognising Textual Entailment with Logical Inference"
      },
      {
        "paperId": "74fe7ec751cd50295b15cfd46389a8fefb37c414",
        "title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"
      },
      {
        "paperId": "f7a25c84a2c5731d4c924479338cd91ba39e4ffc",
        "title": "Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning"
      },
      {
        "paperId": "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886",
        "title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI"
      },
      {
        "paperId": null,
        "title": "2023. Certified reasoning with language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "4781826d42a0f09ae5ca731e43f6403b0f504324",
        "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning"
      },
      {
        "paperId": "8d1d88b8befa4e0c493b5c9361ebe6a32fb59e77",
        "title": "Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs"
      },
      {
        "paperId": "574ce1f7914a03aac6da0882ecaf47d15ccbe084",
        "title": "Enhancing Logical Reasoning of Large Language Models through Chain-of-Thought and Logical Fallacy"
      },
      {
        "paperId": "23f0a99abe53896bc5bd814b70a372576f6bb370",
        "title": "Analysis of Semantic Communication for Logic-based Hypothesis Deduction"
      },
      {
        "paperId": "dd968b619baea69ea935a700d4e8a724e7ced9e9",
        "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems"
      },
      {
        "paperId": "5aa9501d625c5abcb3ef91d6ac1e245b20e51683",
        "title": "RoSum-Mcts: Monte Carlo Tree Search-Inspired HDL Code Summarization with Structural Rewards"
      },
      {
        "paperId": "d35c49d95c09d2fe0fbf785173359b38858915d6",
        "title": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization"
      },
      {
        "paperId": "9efe66da43abf4779c4c63f27c2ac544dec3159a",
        "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study"
      },
      {
        "paperId": "b940b9a0821c4250d6570dd1df056cf7cc0a04eb",
        "title": "Natural Language to Overpass Query: A Multi-Step Approach Using Task Decomposition and Key-Value Correction"
      },
      {
        "paperId": "3e113c48a6f17b64416ab88c3cd8f0d31b40027b",
        "title": "Logical Consistency is Vital: Neural-Symbolic Information Retrieval for Negative-Constraint Queries"
      },
      {
        "paperId": "a1274b0042bafb716e8b5f86f04f010c9ff038be",
        "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?"
      },
      {
        "paperId": "41c82c209195c1e3b50e59ab26ef2e4675617280",
        "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression"
      },
      {
        "paperId": "1bd540db7ecdff238b202bc163891ab7d44a22b6",
        "title": "Improving Multi-hop Logical Reasoning in Small LMs with LoRA Training"
      },
      {
        "paperId": "a0fdff76ab467d1774f804cf220e5f70bd7cd7ab",
        "title": "Deliberate Planning in Language Models with Symbolic Representation"
      },
      {
        "paperId": "dcc41396ac065d42694e093925e117874050c5e8",
        "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition"
      },
      {
        "paperId": "3c1a314e6fc944c5abe7f99be7dcb6477a2c0619",
        "title": "Entailment-Preserving First-order Logic Representations in Natural Language Entailment"
      },
      {
        "paperId": "47099a7d3b6e8affc862dfe0f48fc9e08c2869d7",
        "title": "DISCD: Distributed Lossy Semantic Communication for Logical Deduction of Hypothesis"
      },
      {
        "paperId": "8c7e25c38303e30c7884f5ab1fb80f0aa9848e6a",
        "title": "An LLMs-based neuro-symbolic legal judgment prediction framework for civil cases"
      },
      {
        "paperId": "235c57260540faec477c7ed812726a82c6ac8b76",
        "title": "Automated Monitoring of Web User Interfaces"
      },
      {
        "paperId": "412f0d746b470d51e6a03d56b20b5ea8c3b0f35f",
        "title": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics"
      },
      {
        "paperId": "c4f4725c0bf3c5cd67e4b0c94d08f7ef5e66aff4",
        "title": "Visualizations of Natural Texts: A Survey on Text-to-Structured Data Transformations"
      },
      {
        "paperId": "6b45d0ecde8dcbcf4424ef34b7f26112aa43e9b5",
        "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation"
      },
      {
        "paperId": "dedc29cb68462b3030067775014f90f3423c3e48",
        "title": "LLM-Assisted Generation of SWRL Rules from Natural Language"
      },
      {
        "paperId": "82761e48ffcf7e7e718c34bdc72ab0b0540c3718",
        "title": "Assistive AI for Augmenting Human Decision-making"
      },
      {
        "paperId": "e4a4a98cf3ce47937c406918f43f35f494273498",
        "title": "\u2200uto\u2203\u2228\u2227L: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks"
      },
      {
        "paperId": "81d47c696b45dda91c7c5c00b23c69f1e65db1a2",
        "title": "Lossy Semantic Communication for the Logical Deduction of the State of the World"
      },
      {
        "paperId": "d00ed4adc27d33f0a0206fde58225c6b420ce05b",
        "title": "MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models"
      },
      {
        "paperId": "cee94cb17cd6e1a2782983f6befc287a9f4c4e3c",
        "title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification"
      },
      {
        "paperId": "16f537d11776c970aebeb3a8c6c896b97ea00820",
        "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study"
      },
      {
        "paperId": "df79b3522b3d199c6a24b40f4afd682f1045a2e9",
        "title": "Can LLMs Reason in the Wild with Programs?"
      },
      {
        "paperId": "e5f6be9f94c4b72e07c27eef567b371cdbd70e3d",
        "title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters"
      },
      {
        "paperId": "647a42d78e5bd405014a8c878e37fb7b2da8eaa6",
        "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)"
      },
      {
        "paperId": "f3ba80eedb79ead561714228b6f8da10c818eb13",
        "title": "Formal Methods in Requirements Engineering: Survey and Future Directions"
      },
      {
        "paperId": "df04fdcd6cd01e451ba96dba0fae7ae52668ebee",
        "title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning Using Large Language Models"
      },
      {
        "paperId": "8d89d38fedfa4a5374c726eb7926510016671441",
        "title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs"
      },
      {
        "paperId": "a5f8d778c85f2bb6b9bd1d30386795d4b8c76294",
        "title": "$\\forall$uto$\\exists$val: Autonomous Assessment of LLMs in Formal Synthesis and Interpretation Tasks"
      },
      {
        "paperId": "e6262457cf117d568e0af47c5f3651a9d67bc186",
        "title": "Towards Automated Knowledge Integration From Human-Interpretable Representations"
      },
      {
        "paperId": "35fb19eb94473f46ca8a0fa8e14de56f0aeb81e5",
        "title": "Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models"
      },
      {
        "paperId": "5ff337e94bb710bab34c340e06b0618612126961",
        "title": "Large Language Models Can Learn Temporal Reasoning"
      },
      {
        "paperId": "276bfc5052f967a99a4a3730319f21c43096f8d5",
        "title": "Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs"
      },
      {
        "paperId": "bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f",
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"
      },
      {
        "paperId": "0815c5a05f50fc3405299abdb97cf1343ad63ac9",
        "title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis"
      },
      {
        "paperId": "c4922aa7f90d164e37cc38db6a3c3c3fa71cdcee",
        "title": "Ontology engineering with Large Language Models"
      },
      {
        "paperId": "bc57daff5192045e052c9245918f781ec7d72f43",
        "title": "A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters"
      },
      {
        "paperId": "46480047afa9c10a20202ebf21c99a0fef5989bd",
        "title": "Evaluating LLMs Capabilities at Natural Language to Logic Translation: A Preliminary Investigation"
      },
      {
        "paperId": "f56614b0b188f4b53bfc63b134220ed4167024a4",
        "title": "NL2FOL: Translating Natural Language to First-Order Logic for Logical Fallacy Detection"
      },
      {
        "paperId": "ae08f340db81ce40e08bcb2e11250055b8007025",
        "title": "Efficient Translation of Natural Language to First-Order Logic Using Step-by-Step Distillation"
      },
      {
        "paperId": "2ad8dbc163ce289e23e9c02b324c82d9c2fe8190",
        "title": "Logic-LangChain: Translating Natural Language to First Order Logic for Logical Fallacy Detection"
      }
    ],
    "score": 24.0
  },
  {
    "id": "8115ffbbadd1055424d18369dba66ce32a572800",
    "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
    "authors": [
      "Zhenyu Hou",
      "Yiin Niu",
      "Zhengxiao Du",
      "Xiaohan Zhang",
      "Xiao Liu",
      "Aohan Zeng",
      "Qinkai Zheng",
      "Minlie Huang",
      "Hongning Wang",
      "Jie Tang",
      "Yuxiao Dong"
    ],
    "year": 2024,
    "citationCount": 23,
    "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.",
    "url": "https://www.semanticscholar.org/paper/8115ffbbadd1055424d18369dba66ce32a572800",
    "pdf_url": "https://arxiv.org/pdf/2404.00934.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-04-01",
    "externalIds": {
      "ArXiv": "2404.00934",
      "DBLP": "journals/corr/abs-2404-00934",
      "DOI": "10.48550/arXiv.2404.00934",
      "CorpusId": 268819071
    },
    "references": [
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f05c288caeb9a14ef387e6867934ced3d2200259",
        "title": "SALMON: Self-Alignment with Instructable Reward Models"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "ea04174766c2cb2ab8a5ef3a424912a0dd0a4d51",
        "title": "Revisiting Discriminative vs. Generative Classifiers: Theory and Implications"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "774591fdd988eaaff3917e7c5171d044b0843e63",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
      },
      {
        "paperId": "49f905eb03958c7cfae52ac759ea8978b8b2a6ea",
        "title": "Alignment of Language Agents"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "57daf938ab134ba05a1bef4d596c2074d367e81e",
        "title": "trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": null,
        "title": "2022. Scaling instruction-finetuned language models"
      },
      {
        "paperId": null,
        "title": "2023. Alignbench: Benchmarking chinese alignment of large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "690a3e101091bd74c51ef7acf75a9ee0e8eeac5d",
        "title": "Reference-Free Rating of LLM Responses via Latent Information"
      },
      {
        "paperId": "92b852470985ff3712cc6fc7316fba78bae22ef2",
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning"
      },
      {
        "paperId": "3ffa8d486ceacbbf139f0fe69e6f1f59e78b35d1",
        "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "45bc625838932e14973d95f4c6f2b8efea13a009",
        "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach"
      },
      {
        "paperId": "ec9575d326ce92f2fa0815fc178f8d9739a48e2c",
        "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models"
      },
      {
        "paperId": "11313ae3f1745da638fa1852d020a18417a13bbf",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch"
      },
      {
        "paperId": "686ae98e7b395a904124b35b381e54ab3c77676a",
        "title": "QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration"
      },
      {
        "paperId": "54e1a79ef688b8f6462b6265fc803d9c3e90a72a",
        "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model"
      },
      {
        "paperId": "2e70bc1746833bb46853de7f47407509d54fc42e",
        "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "5401d8fa36a78642971c694506594afdcb100c73",
        "title": "LongReward: Improving Long-context Large Language Models with AI Feedback"
      },
      {
        "paperId": "6e0dd07004db6a6fb370b2de779b16130f1b4d1a",
        "title": "Contextual Learning of Large Language Models Under Chain-of-Thought Optimization Strategy"
      },
      {
        "paperId": "0b6b45a9f144a5136c62d6f298d0932b7bc5d837",
        "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive Contrastive Learning"
      },
      {
        "paperId": "158730a16cd4cc4d4842e42e5c2a0843b75d527c",
        "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "85a3bad02c8c87445a66c053d1be86e93cda229d",
        "title": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts"
      },
      {
        "paperId": "14ba95d4344aea719be0425f4b214d5bd42aabfb",
        "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs"
      },
      {
        "paperId": "d46b047d8087fea5c20fef3e55cf5cdbd6fb3162",
        "title": "Better RAG using Relevant Information Gain"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "5387445a58a958422a8cfd297e6a611aade0f0e8",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      },
      {
        "paperId": "47cda129fc742647739591351fb3f40676018cf6",
        "title": "Tele-FLM Technical Report"
      },
      {
        "paperId": "9ecc7c2c7fca124d4c260f07c09bf6444185613f",
        "title": "CAT-LLM: Style-enhanced Large Language Models with Text Style Definition for Chinese Article-style Transfer"
      },
      {
        "paperId": "33131a681855e28de10e0430cdce803ae88a9a21",
        "title": "PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models"
      }
    ],
    "score": 23.0
  },
  {
    "id": "7f96bb27a8fca35b1f7d02ee319a64be04114809",
    "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
    "authors": [
      "Maonan Wang",
      "Aoyu Pang",
      "Yuheng Kan",
      "Man-On Pun",
      "Chung Shue Chen",
      "Bo Huang"
    ],
    "year": 2024,
    "citationCount": 23,
    "abstract": "Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.",
    "url": "https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809",
    "pdf_url": "https://arxiv.org/pdf/2403.08337.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-13",
    "externalIds": {
      "ArXiv": "2403.08337",
      "DBLP": "journals/corr/abs-2403-08337",
      "DOI": "10.48550/arXiv.2403.08337",
      "CorpusId": 268379562
    },
    "references": [
      {
        "paperId": "feb673f79c6f71769cd171d7d5f75cc395de0a9b",
        "title": "A General Scenario-Agnostic Reinforcement Learning for Traffic Signal Control"
      },
      {
        "paperId": "ab1bcc67482fa970fd0c9fd2ce33e5b8ff1d8cdd",
        "title": "Open-TI: Open Traffic Intelligence with Augmented Language Model"
      },
      {
        "paperId": "237b0cf9d78f4a52274b868656ad011f599aeb26",
        "title": "LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning"
      },
      {
        "paperId": "02262750e20410b55cc4f627cd2d3aa73d5d8ff3",
        "title": "UniTSA: A Universal Reinforcement Learning Framework for V2X Traffic Signal Control"
      },
      {
        "paperId": "451539c0d0f5f5785ff58d09ca5e67a5f129f9de",
        "title": "A Survey on Multimodal Large Language Models for Autonomous Driving"
      },
      {
        "paperId": "10158879cdb64ce7d3f7bb5572c4617ea808602e",
        "title": "Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles"
      },
      {
        "paperId": "77693ca00a8ef775af96b5c450aa0afdb0e10a51",
        "title": "Talk2BEV: Language-enhanced Bird\u2019s-eye View Maps for Autonomous Driving"
      },
      {
        "paperId": "f42f61a547c5996be6aee175145b0d74e6324dff",
        "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"
      },
      {
        "paperId": "0c72450890a54b68d63baa99376131fda8f06cf9",
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "11bca2cafe89e14dc733504f97e2489de697ceab",
        "title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language Models"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "32e770bd0b0712ae887a34820cc72a86fe1428bb",
        "title": "Large-Scale Traffic Signal Control Using Constrained Network Partition and Adaptive Deep Reinforcement Learning"
      },
      {
        "paperId": "9c40c31408b855017bda88c4b4017de60a169241",
        "title": "Deep reinforcement learning for traffic signal control with consistent state and reward design approach"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "d057ba27750d789e8ebe040c543976984052fc67",
        "title": "ADLight: A Universal Approach of Traffic Signal Control with Augmented Data Using Reinforcement Learning"
      },
      {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "ea6225be62792e6834e737ca7acc36dd14c7c87b",
        "title": "Rare event failure test case generation in Learning-Enabled-Controllers"
      },
      {
        "paperId": "b103e87c7727134927d3ffb06934a95c10c02fc0",
        "title": "GPT-3: Its Nature, Scope, Limits, and Consequences"
      },
      {
        "paperId": "01cf48fd15ce4a261c2cbcdc99624ddc683c96da",
        "title": "AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control"
      },
      {
        "paperId": "09aac6562d736c6eca050221a694f1dc3491df6b",
        "title": "State-of-art review of traffic signal control methods: challenges and opportunities"
      },
      {
        "paperId": "d766b67868b3a9a97ad4407cd496760698d2a427",
        "title": "MetaLight: Value-Based Meta-Reinforcement Learning for Traffic Signal Control"
      },
      {
        "paperId": "794c17f1c7564ce6cc0d9d409aef73c8a6a79f59",
        "title": "Toward A Thousand Lights: Decentralized Deep Reinforcement Learning for Large-Scale Traffic Signal Control"
      },
      {
        "paperId": "64993b08a97d89411276a1336c7df0b279b895b9",
        "title": "PressLight: Learning Max Pressure Control to Coordinate Traffic Signals in Arterial Network"
      },
      {
        "paperId": "7bf26f0d5c1514df20a04e1f7b34102d4311e968",
        "title": "A Survey on Traffic Signal Control Methods"
      },
      {
        "paperId": "b17e0b5b5c7e632ecf5f35b0616cbf820382fcf7",
        "title": "Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control"
      },
      {
        "paperId": "b1914c912dea62703856d89fe3724675a6139b71",
        "title": "Microscopic Traffic Simulation using SUMO"
      },
      {
        "paperId": "c9c47277b67e32e4cfd3c8fb1861029d0259eb62",
        "title": "IntelliLight: A Reinforcement Learning Approach for Intelligent Traffic Light Control"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "0a54341fe2dc2cb5cded4b0a92d1ff2f762dca11",
        "title": "Max pressure control of a network of signalized intersections"
      },
      {
        "paperId": "0eb9c914aac541890319ab8343798199d51b22a2",
        "title": "Computational Intelligence in Urban Traffic Signal Control: A Survey"
      },
      {
        "paperId": "686b48e2b1f9e8324a25be2feae0fdef345ae18f",
        "title": "Does Traffic Congestion Slow the Economy?"
      },
      {
        "paperId": "55e0cb696149210a9558f3ec3c0c4b359db29433",
        "title": "A survey and comparative study of simulators for vehicular ad hoc networks (VANETs)"
      },
      {
        "paperId": "9886ca7fd54c33cab95789e618fc6686df58d82f",
        "title": "Traffic signal timing manual."
      },
      {
        "paperId": "2ea05b32801eb6f7d5037e9701e6e96d7b8b755c",
        "title": "Self-organizing traffic lights: A realistic simulation"
      },
      {
        "paperId": "5d8838cb2b2c61a3622e56510b357dfecdd4b182",
        "title": "THE SCOOT ON-LINE TRAFFIC SIGNAL OPTIMISATION TECHNIQUE"
      },
      {
        "paperId": "4467e9d969e735d61e8c2f5c478ccf44d63b2020",
        "title": "Scalable Reinforcement Learning Framework for Traffic Signal Control Under Communication Delays"
      },
      {
        "paperId": "b5f19ff4167f84b141642b7e831a2ee9ba50affe",
        "title": "Large Language Model-Assisted Arterial Traffic Signal Control"
      },
      {
        "paperId": "d57f28c23ccb54d39522db7eda444e1550fdcc37",
        "title": "Deep Reinforcement Learning Based Traffic Signal Control: A Comparative Analysis"
      },
      {
        "paperId": "db73250cde81aa4f87b2f46954f28253cb7ec205",
        "title": "Large Language Models as Traffic Signal Control Agents: Capacity and Opportunity"
      },
      {
        "paperId": null,
        "title": "\u201cScats-a traffic responsive method of controlling urban traffic,\u201d"
      },
      {
        "paperId": null,
        "title": "LLM-Assisted Light (LA-Light) A P REPRINT"
      },
      {
        "paperId": null,
        "title": "Bo Huang is with the Department of Geography, The University of Hong Kong, Hong Kong, SAR 999077, China"
      },
      {
        "paperId": null,
        "title": "\u201cIntroducing ChatGPT.\u201d"
      },
      {
        "paperId": null,
        "title": "Chung"
      }
    ],
    "cited_by": [
      {
        "paperId": "b0b19ac0556154aed3e3b4c3b326dbfe9106e4e0",
        "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management"
      },
      {
        "paperId": "8bcb00ac9a5499b1acc25e48bb77e463844db027",
        "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications"
      },
      {
        "paperId": "dffe8bc078a93e4e42733da784176570fc3b636b",
        "title": "SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization"
      },
      {
        "paperId": "996e6b27463d186792d6f24a1d067cdbfd15e2ce",
        "title": "VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning"
      },
      {
        "paperId": "973eca991e16bf4eda9d993fdf79c46501cd280b",
        "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review"
      },
      {
        "paperId": "d576713bedbecb01cda351a834ee4aca75821ccc",
        "title": "Generative AI for Intelligent Transportation Systems: Road Transportation Perspective"
      },
      {
        "paperId": "91939486915a1b47c8a6ab5194dd7555687021bc",
        "title": "Leveraging Bird Eye View Video and Multimodal Large Language Models for Real-Time Intersection Control and Reasoning"
      },
      {
        "paperId": "69a57a3b95743dce030d6099f85519c3aa942300",
        "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap"
      },
      {
        "paperId": "55e30a80cfe7c037247e4f948f580562d935ffb7",
        "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control"
      },
      {
        "paperId": "eca5ba85502f8b4fae730cd3da6f3672fee0ff75",
        "title": "Machine Learning-Enhanced Traffic Light Optimization System Prioritizing Emergency Vehicle Passage Using SVM and Random Forest Models"
      },
      {
        "paperId": "6ef251692c7ab00540d1e7b66ab7e82a566eb1a6",
        "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors"
      },
      {
        "paperId": "aec2791e5753166f4145368e1c55ec2070784249",
        "title": "Visual Reasoning at Urban Intersections: Fine-Tuning GPT-4O for Traffic Conflict Detection"
      },
      {
        "paperId": "44df72184a96c6316d12d991434c001cab6f2e7f",
        "title": "Integrating LLMs With ITS: Recent Advances, Potentials, Challenges, and Future Directions"
      },
      {
        "paperId": "9cf932f7cb7bee041502704c2d7f0bd22049c74b",
        "title": "Large Language Model-based Decision-making for COLREGs and the Control of Autonomous Surface Vehicles"
      },
      {
        "paperId": "281469324c10784417030f44175358b208c663ce",
        "title": "Large Language Models (LLMs) as Traffic Control Systems at Urban Intersections: A New Paradigm"
      },
      {
        "paperId": "f6cf3e80791cc8a40ef07f258ee49a189c994514",
        "title": "Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies"
      },
      {
        "paperId": "c361586c48dfeea5d71403efd9fb72c063a0441d",
        "title": "Urban Traffic Control Meets Decision Recommendation System: A Survey and Perspective"
      },
      {
        "paperId": "a741a0e7d6612d0f7573012a16cb19aae0905c55",
        "title": "iLLM-TSC: Integration reinforcement learning and large language model for traffic signal control policy improvement"
      },
      {
        "paperId": "0f95e8492271136ab44aed60d63b40bc8ef2dd98",
        "title": "Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions"
      },
      {
        "paperId": "c5d92acb57dea7b09410f10ca7b1a5ecfc22756e",
        "title": "Large language models for human-machine collaborative particle accelerator tuning through natural language"
      },
      {
        "paperId": "84d99893ee24fc825e359598d44d602c45c4865e",
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
      },
      {
        "paperId": "df2a430bba5a906dc7458d3420cb0f81b18ee3a5",
        "title": "Traffic Detection and Forecasting from Social Media Data Using a Deep Learning-Based Model, Linguistic Knowledge, Large Language Models, and Knowledge Graphs"
      },
      {
        "paperId": "08c9911a321ca17dc8b35ab465911000065e434d",
        "title": "AIGC-Driven Real-Time Interactive 4D Tra\ufb03c Scene Generation in Vehicular Networks"
      }
    ],
    "score": 23.0
  },
  {
    "id": "c085e88a0351e393609a95305afc1db792d1db0f",
    "title": "The History and Risks of Reinforcement Learning and Human Feedback",
    "authors": [
      "Nathan Lambert",
      "Thomas Krendl Gilbert",
      "T. Zick"
    ],
    "year": 2023,
    "citationCount": 45,
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF's foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.",
    "url": "https://www.semanticscholar.org/paper/c085e88a0351e393609a95305afc1db792d1db0f",
    "pdf_url": "https://arxiv.org/pdf/2310.13595.pdf",
    "venue": "",
    "publicationDate": "2023-10-20",
    "externalIds": {
      "ArXiv": "2310.13595",
      "CorpusId": 264405813
    },
    "references": [
      {
        "paperId": "d6f452265914d8f5ecf185f999170f6a106602e8",
        "title": "Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards"
      },
      {
        "paperId": "541b66bad4a9bf9b7fd97f13f94ab9061c7c0c47",
        "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
      },
      {
        "paperId": "7f55ef29a6f8b2771c5435bbeba29c87264fdc88",
        "title": "Shepherd: A Critic for Language Model Generation"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ad022cd5da75637ac3e0a8f8cc4f0d394ba5ff7a",
        "title": "Self-Consuming Generative Models Go MAD"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "9d9cd15322f8c60d5d52763b989a74d9356c4e70",
        "title": "The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "155aec5cff650263a4c71136f97570611d1bba7a",
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget"
      },
      {
        "paperId": "d655026a443d3de40ebacf61274e562ec05936ef",
        "title": "Moral Machine or Tyranny of the Majority?"
      },
      {
        "paperId": "5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35",
        "title": "The False Promise of Imitating Proprietary LLMs"
      },
      {
        "paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef",
        "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"
      },
      {
        "paperId": "312309f433e8e84706c12ba41284f1fb7ee6a6a5",
        "title": "ChatGPT\u2019s inconsistent moral advice influences users\u2019 judgment"
      },
      {
        "paperId": "e5174aeda1baa67c17f4ac630ae2e44453954cc3",
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "4c868a92c615df3859433e28b2441bbce9e65fb3",
        "title": "Reward Reports for Reinforcement Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
        "title": "Teaching language models to support answers with verified quotes"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "01a4d49a0ec54b6bed8cc8726fd2568df788f0f4",
        "title": "Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "567abe32df6dfe7d6ba19cbcb8e6426262902821",
        "title": "On the Expressivity of Markov Reward"
      },
      {
        "paperId": "766912db878d4ca14680600e982faff44e508a8d",
        "title": "On Releasing Annotator-Level Labels and Information in Datasets"
      },
      {
        "paperId": "3a1501829ce7205f25939dd26e1089df920c9988",
        "title": "Reward is enough"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
        "title": "Towards Understanding and Mitigating Social Biases in Language Models"
      },
      {
        "paperId": "d11217e882e54fc3dfe948fa102555f75d0c80bc",
        "title": "Hard Choices in Artificial Intelligence"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "4c2733d191e347753bb28afa46a1c55c65e085be",
        "title": "Persistent Anti-Muslim Bias in Large Language Models"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "c40b82f0fbb75ef2d8a5e9434fb35a5b8e2a2436",
        "title": "Multi-Principal Assistance Games"
      },
      {
        "paperId": "22433b5ab4f789f1afb40c36dd7d905439c1b021",
        "title": "Choosing for Changing Selves"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "0b8c3226fa9da6807aa966172199a9d8d82a4185",
        "title": "Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "5213f44c69c45b57f4f88d006e2dac0b78101d34",
        "title": "Social Choice and the Value Alignment Problem *"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "0fac80d049f94a8bbee2dd09d44e8e87312277f2",
        "title": "Social choice ethics in artificial intelligence"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "0f9e4431e844cece73223f521e6d8e4a812b0ef1",
        "title": "Should Robots be Obedient?"
      },
      {
        "paperId": "1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777",
        "title": "Cooperative Inverse Reinforcement Learning"
      },
      {
        "paperId": "b3bacb44ac820cb54a9e455eafe721edd1136e23",
        "title": "The Behavior of Organisms: An Experimental Analysis"
      },
      {
        "paperId": "2511fdf14ddeaa166e0d8dcadab99d660fedd273",
        "title": "The Arrow Impossibility Theorem"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "5aa3d64a1b5c56790cb20daf54d3bab0c4a2466b",
        "title": "Microfoundations of the Rule of Law"
      },
      {
        "paperId": "5466eae341fde4ff95945d9c0bdfe992c97cd880",
        "title": "Label Ranking Methods based on the Plackett-Luce Model"
      },
      {
        "paperId": "a4d99468a565310c7396869b3b8db814d28f3576",
        "title": "Theory of Games and Economic Behavior"
      },
      {
        "paperId": "4cb91ed7603e74f4b6d10e640f5db682405c278e",
        "title": "A computational substrate for incentive salience"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "516a85259039903c4631e2b8fe5fc79502b3cffe",
        "title": "Against Parsimony: Three Easy Ways of Complicating some Categories of Economic Discourse"
      },
      {
        "paperId": "28c567f9633871a8aeece63209263ceb1c7a5738",
        "title": "A heuristic approach to reinforcement learning control systems"
      },
      {
        "paperId": "c7d3e9a1dd86f9c96f709d0ddb76972862784231",
        "title": "Dynamic Programming and Markov Processes"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "1d340ee90db559d0a6c4b181388f1a39a8a0eb7b",
        "title": "Automatic Control Systems"
      },
      {
        "paperId": null,
        "title": "Hackers red-teaming a.i. are \u2018breaking stuff left and right,\u2019 but don\u2019t expect quick fixes from defcon: \u2018there are no good guardrails\u2019"
      },
      {
        "paperId": null,
        "title": "The waluigi effect (mega-post)"
      },
      {
        "paperId": null,
        "title": "Introducing superalignment"
      },
      {
        "paperId": null,
        "title": "Chatgpt: Optimizing language models for dialogue"
      },
      {
        "paperId": "c09f4a8c916a98d2f23abbaa24f183667e9806e9",
        "title": "Partially Observable Markov Decision Processes"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": "01685429600e4a276bb82f979639379eaa8b4a35",
        "title": "Truth and Probability"
      },
      {
        "paperId": "5be2ab6ca8675c9b5aa765cc6803f3b056eed233",
        "title": "Corrigibility"
      },
      {
        "paperId": "322ec5edb019b4d2fdda4d3cb2e514b083e8cc17",
        "title": "Normative Theories of Rational Choice: Expected Utility"
      },
      {
        "paperId": "f802d2f44fd6981e15342ab9f0afe1f74a8382c6",
        "title": "Jeremy Bentham: An Introduction to the Principles of Morals and Legislation"
      },
      {
        "paperId": null,
        "title": "Aesthetics & art in the early development of human-computer interfaces (Unpublished doctoral dissertation)"
      },
      {
        "paperId": "67238f415dfb2024298082d8c9f7eb529fe191f5",
        "title": "Where Do Rewards Come From"
      },
      {
        "paperId": "e7ec6db9093a8b0b4007a1fbc1d4d86cc84f8fc2",
        "title": "Reinforcement-learning control and pattern recognition systems"
      },
      {
        "paperId": "8595c116b539325b9a37f357e9971e209ee81345",
        "title": "Rule utilitarianism and decision theory"
      },
      {
        "paperId": null,
        "title": "Brain function and adaptive systems: a heterostatic theory (No. 133)"
      },
      {
        "paperId": "264f1513654fe3b93f671fe3ba58ed85192d064a",
        "title": "Behaviour and the concept of preference"
      },
      {
        "paperId": null,
        "title": "Adaptive switching circuits (Tech. Rep.)"
      },
      {
        "paperId": "714d5e1354571bb3654f043469ac25a2de731bf4",
        "title": "You have printed the following article : A Difficulty in the Concept of Social Welfare"
      }
    ],
    "cited_by": [
      {
        "paperId": "6069fa0fc5cccb8ebd0061c5e1816f5069cc255b",
        "title": "RewardBench 2: Advancing Reward Model Evaluation"
      },
      {
        "paperId": "b2b36967d67468323d671239c5baf9d94a30fd4b",
        "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects"
      },
      {
        "paperId": "e2d64cacc9639ab176425055550c6b2139348942",
        "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO"
      },
      {
        "paperId": "9af9c0df0a328d5f327bed3151819b06dfc33622",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment"
      },
      {
        "paperId": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
        "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "0f38ed2e5265848810f82ba6bda603c8ebf71ce8",
        "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs"
      },
      {
        "paperId": "2ae44325998365809c08b5ac91a3982d4e4648b2",
        "title": "Security practices in AI development"
      },
      {
        "paperId": "cf9dd26dfde5128b91bffe12c40fe85ec32bf98e",
        "title": "Towards Safe and Honest AI Agents with Neural Self-Other Overlap"
      },
      {
        "paperId": "3d0557ed32fa648e6977faed9a60c655a7e33164",
        "title": "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization"
      },
      {
        "paperId": "8324f259953e173b15faf5cb1d3a02d4069b0dbf",
        "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models"
      },
      {
        "paperId": "ab0df4c771c19e3e7d4edc1057a9f91c6c8e2ae5",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
      },
      {
        "paperId": "5d648940a78d432051238bf8ab259e8d34a14930",
        "title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?"
      },
      {
        "paperId": "ff6f6ee7b62aa046021582786aac0f9396b99ef9",
        "title": "Intuitions of Compromise: Utilitarianism vs. Contractualism"
      },
      {
        "paperId": "50ee11e0d057defe6c449099e01b12cf3cb74446",
        "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking"
      },
      {
        "paperId": "004392baa790f7c04744df1865cee6b730508d77",
        "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models"
      },
      {
        "paperId": "5679f7d8b1db68f0eb199920ccc857f130122910",
        "title": "Exploring the Boundaries of Content Moderation in Text-to-Image Generation"
      },
      {
        "paperId": "1b60844540637f1e8b8bc560c9ea7f030c08a7bc",
        "title": "Acceptable Use Policies for Foundation Models"
      },
      {
        "paperId": "2757878ddd2ceae31ae1c4a49db25d85c4424472",
        "title": "Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models"
      },
      {
        "paperId": "e7b5d0269bdd37d01cea2bddb4d2ec9cf1539a40",
        "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"
      },
      {
        "paperId": "9c803f13a8d7e3029621c8363f07517293134227",
        "title": "Value Alignment from Unstructured Text"
      },
      {
        "paperId": "88786bb0ad8fbb66416e091ae89257aa4a810767",
        "title": "AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations"
      },
      {
        "paperId": "f85ca15777e757efe4c2439429f826584df245ac",
        "title": "All We Need is Voter Feedback: a New Paradigm to Realize Politics without Politicians Using AI Models Aligned with Voter Inputs"
      },
      {
        "paperId": "92b3572fb2ca580c55e9f5952d8592ed2f18225a",
        "title": "Prototypical Reward Network for Data-Efficient RLHF"
      },
      {
        "paperId": "de0d53201d259a5f3d13efe06bafacb70bda02e2",
        "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding"
      },
      {
        "paperId": "8c73c5305de8aef0ee52a45687b29f10f240b3dc",
        "title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation"
      },
      {
        "paperId": "d9166a92922064fbbd4c99a948d6937739ead5dd",
        "title": "Hallucinations in LLMs: Understanding and Addressing Challenges"
      },
      {
        "paperId": "213816dc724d5e305a83b299cfa09ab7f2c8c7f8",
        "title": "Mapping Social Choice Theory to RLHF"
      },
      {
        "paperId": "80fd3d2d3b2f7e0cb41c935c6b7ac1b73823a60d",
        "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "6b02b05720ef6b5a51dc8659815d45af4ec0fa2a",
        "title": "Responsible Reporting for Frontier AI Development"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "03ed1e35cc661a918df4528da9c3a2743314b931",
        "title": "Comparing Rationality Between Large Language Models and Humans: Insights and Open Questions"
      },
      {
        "paperId": "3730ad714aa22ebd3694974abe0bc6437a3ad4ee",
        "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback"
      },
      {
        "paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723",
        "title": "Unintended Impacts of LLM Alignment on Global Representation"
      },
      {
        "paperId": "0f6cd53c0cc1ee252433e0d37f419754e053b8a6",
        "title": "Suppressing Pink Elephants with Direct Principle Feedback"
      },
      {
        "paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94",
        "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"
      },
      {
        "paperId": "d349f4e924f6cdcbafce75f159437920495cd8a1",
        "title": "Black-Box Access is Insufficient for Rigorous AI Audits"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "ac3ae10014337879bb566dee8b6ae840e3897384",
        "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "5a2c395ca5dc942dbb6ea7eea1e324b8c9e8534e",
        "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "f35d51b99a003053a6a8a57075f7028e00328e87",
        "title": "Reward Modeling Requires Automatic Adjustment Based on Data Quality"
      },
      {
        "paperId": "7e54cededaea99e988a1f77acff6897d4687c6ef",
        "title": "Prototypical Reward Network for Data Efficient Model Alignment"
      },
      {
        "paperId": "7fa92edb84802e413c497062a026439a5a10f5b7",
        "title": "S TYLE OVER S UBSTANCE : F AILURE M ODES OF LLM J UDGES IN A LIGNMENT B ENCHMARKING"
      },
      {
        "paperId": "58ebc7f497eb8770bdd53ba247710a2ecee88950",
        "title": "Scalable Oversight by Accounting for Unreliable Feedback"
      }
    ],
    "score": 22.5
  },
  {
    "id": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
    "title": "Verbosity Bias in Preference Labeling by Large Language Models",
    "authors": [
      "Keita Saito",
      "Akifumi Wachi",
      "Koki Wataoka",
      "Youhei Akimoto"
    ],
    "year": 2023,
    "citationCount": 45,
    "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.",
    "url": "https://www.semanticscholar.org/paper/777d4ec0148c34b0bfab91e9ac3a902e420b891e",
    "pdf_url": "https://arxiv.org/pdf/2310.10076.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-16",
    "externalIds": {
      "ArXiv": "2310.10076",
      "DBLP": "journals/corr/abs-2310-10076",
      "DOI": "10.48550/arXiv.2310.10076",
      "CorpusId": 264147087
    },
    "references": [
      {
        "paperId": "1221aa62d85770e1712c98fbe2fbaf8bad512861",
        "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "d42b11ce90c9c69a20ed015b73dc33e0e4100a7b",
        "title": "Equality of Opportunity in Supervised Learning"
      },
      {
        "paperId": "ba786c46373892554b98df42df7af6f5da343c9d",
        "title": "Large Language Models in Machine Translation"
      },
      {
        "paperId": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
        "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "[The Start of Assistant A's Answer] I recently had the opportunity to travel to Hawaii, and it was an experience that I will never forget. From"
      },
      {
        "paperId": null,
        "title": "Compose an engaging travel blog post about a recent trip to Hawaii,"
      }
    ],
    "cited_by": [
      {
        "paperId": "45b265f71dce431962dd62bf8b436308dc34be88",
        "title": "The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge"
      },
      {
        "paperId": "0f2bf1fcdda57d372ee518e56fd559c43cecbab1",
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles"
      },
      {
        "paperId": "a4a6f915c318190de142ab2f0b3b9d9af31e5f35",
        "title": "Using LLMs and Essence to Support Software Practice Adoption"
      },
      {
        "paperId": "54c4d714704be8c6bafb28af551cf88641a391e1",
        "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge"
      },
      {
        "paperId": "9107a1e73ffc39294de8f7d78ddf392914e4a0e5",
        "title": "Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini"
      },
      {
        "paperId": "ce081b21897d14c4a4381320d81ab5238d763fd4",
        "title": "Exploring Modularity of Agentic Systems for Drug Discovery"
      },
      {
        "paperId": "8e70bd26dd94fe39dd2202a8183906b67b1f425b",
        "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models"
      },
      {
        "paperId": "63dd639354d5a73e6d4005fed783dcd4ce8b916a",
        "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector"
      },
      {
        "paperId": "9d6cd5237a2265890918a8c1fef2ddb637eab69c",
        "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models"
      },
      {
        "paperId": "176e100cca02b4f149b147df84811f4d35f8d2b4",
        "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following"
      },
      {
        "paperId": "a5f8a7004cf6033c54b75a0872737e0965a8672e",
        "title": "From First Draft to Final Insight: A Multi-Agent Approach for Feedback Generation"
      },
      {
        "paperId": "f6ca75836d8d9dc5b76baa3a797894febadd7d45",
        "title": "SJ-ORPO: Alignment of LLMs Using Multi-Task Learning and Self-Judgment"
      },
      {
        "paperId": "a78d53d30dadef633e86c016288000fd485c1ab8",
        "title": "Aligning large language models with human preferences using historical text edits"
      },
      {
        "paperId": "92f2e1a17cbff22164611a2c8a908afe71b4b626",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models"
      },
      {
        "paperId": "e8c2e39fb06bb666efe3476e820c5cef7f1c484a",
        "title": "Trust Dynamics in AI-Assisted Development: Definitions, Factors, and Implications"
      },
      {
        "paperId": "741e35b536463037391373d8a4e9cdd8c420f242",
        "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation"
      },
      {
        "paperId": "97d0a329688034150091fe2d2977e4717b356978",
        "title": "Do LLM Evaluators Prefer Themselves for a Reason?"
      },
      {
        "paperId": "507fb017cc4693c6c3b079ed6c012ddb3563cfad",
        "title": "From Idea to Implementation: Evaluating the Influence of Large Language Models in Software Development - An Opinion Paper"
      },
      {
        "paperId": "1764658448a295ab2dc5b232dbc0d6290892c2ab",
        "title": "Societal Alignment Frameworks Can Improve LLM Alignment"
      },
      {
        "paperId": "dcfd529521d92166047ac51366fad12ded02c41f",
        "title": "Large language models for pretreatment education in pediatric radiation oncology: A comparative evaluation study"
      },
      {
        "paperId": "cf01d7c40cbf815de0f62fa78c2352ba546ad680",
        "title": "Self-Preference Bias in LLM-as-a-Judge"
      },
      {
        "paperId": "ab8673c9cdc962aa9bc8e205873b03194a5ea66b",
        "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate through LLMs"
      },
      {
        "paperId": "2fc8a207405ac233cde98424c36dd19c8731ce5e",
        "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"
      },
      {
        "paperId": "0ba0acf039e6c99930dd85931fc58ee043006503",
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
      },
      {
        "paperId": "be5dc1d23aaae49d89084ab8bad05e4c9313416c",
        "title": "CALF: Benchmarking Evaluation of LFQA Using Chinese Examinations"
      },
      {
        "paperId": "a2fae006e6c5ac346fd51bc8a009127f9abe22df",
        "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"
      },
      {
        "paperId": "f9b6af0ef6393c73c32ffe1122b28634a8fb6367",
        "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment"
      },
      {
        "paperId": "de3cefcd6d142afa8e7ac59523ceb116ec136248",
        "title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication"
      },
      {
        "paperId": "96628272f31bdc09fc69484a600f884d801d8da7",
        "title": "Modality-Specialized Synergizers for Interleaved Vision-Language Generalists"
      },
      {
        "paperId": "3da5f21144fef19dd88f7dcc11a5d9f2edbfe417",
        "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "7d784c87c5e46fedbe62006d7340baa7c7bec123",
        "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"
      },
      {
        "paperId": "db2717c4883831506e46507065b313e74403ff8a",
        "title": "Automating Thematic Analysis: How LLMs Analyse Controversial Topics"
      },
      {
        "paperId": "de17e7ed443e13320694cce3b2f475c694801246",
        "title": "Stepwise Alignment for Constrained Language Model Policy Optimization"
      },
      {
        "paperId": "aae01e933690e1f060b8bc5e3ecbef785630d0f9",
        "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "54ab59201f0db155f049963a8e075faf49abb181",
        "title": "What large language models know and what people think they know"
      },
      {
        "paperId": "f3748aea92332bc999527deef2971370bcb5010d",
        "title": "Shai: A large language model for asset management"
      },
      {
        "paperId": "65b77aaf9ba682c1335803ff6808740ec411256b",
        "title": "The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry"
      },
      {
        "paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
      },
      {
        "paperId": "443bdea030dbcba48c77beca48d678a471c9daca",
        "title": "Spaced Scheduling for Large Language Model Training"
      },
      {
        "paperId": "a2bcf4bf415bc35ae0bb27a862efea67a4090483",
        "title": "Beyond Excess and Deficiency: Adaptive Length Bias Mitigation in Reward Models for RLHF"
      },
      {
        "paperId": "8143f974a56580025297a5c57c621efdf8274a3d",
        "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness"
      },
      {
        "paperId": "bddb3ea9bd2c6a202dbf596c0489eb09e979053f",
        "title": "Bridging the Gap Between What Large Language Models Know and What People Think They Know"
      },
      {
        "paperId": "4ef6e2a0b4e9dcbd75bba12c8ade077f45709a5c",
        "title": "SPECIAL COLLECTION: HUMANS AND GENERATIVE AI Conversational User Interfaces: Explanations and Interactivity Positively In \ufb02 uence Advice Taking From Generative Arti \ufb01 cial Intelligence"
      }
    ],
    "score": 22.5
  },
  {
    "id": "57451ce18f3035fcadf64db38420434f9299b7f3",
    "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF",
    "authors": [
      "Yuan Sun",
      "Navid Salami Pargoo",
      "Peter J. Jin",
      "Jorge Ortiz"
    ],
    "year": 2024,
    "citationCount": 22,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is popular in large language models (LLMs), whereas traditional Reinforcement Learning (RL) often falls short. Current autonomous driving methods typically utilize either human feedback in machine learning, including RL, or LLMs. Most feedback guides the car agent's learning process (e.g., controlling the car). RLHF is usually applied in the fine-tuning step, requiring direct human \"preferences,\" which are not commonly used in optimizing autonomous driving models. In this research, we innovatively combine RLHF and LLMs to enhance autonomous driving safety. Training a model with human guidance from scratch is inefficient. Our framework starts with a pre-trained autonomous car agent model and implements multiple human-controlled agents, such as cars and pedestrians, to simulate real-life road environments. The autonomous car model is not directly controlled by humans. We integrate both physical and physiological feedback to fine-tune the model, optimizing this process using LLMs. This multi-agent interactive environment ensures safe, realistic interactions before real-world application. Finally, we will validate our model using data gathered from real-life testbeds located in New Jersey and New York City.",
    "url": "https://www.semanticscholar.org/paper/57451ce18f3035fcadf64db38420434f9299b7f3",
    "pdf_url": "https://arxiv.org/pdf/2406.04481.pdf",
    "venue": "Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing",
    "publicationDate": "2024-06-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-04481",
      "ArXiv": "2406.04481",
      "DOI": "10.1145/3675094.3677588",
      "CorpusId": 270357671
    },
    "references": [
      {
        "paperId": "14b566f50db38d30b198eab461aae33a2d0dbb17",
        "title": "Feedback-Guided Autonomous Driving"
      },
      {
        "paperId": "a247e1943debfa553bd84a3a9ec15cd2286b65fb",
        "title": "Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs"
      },
      {
        "paperId": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
        "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF"
      },
      {
        "paperId": "e84fe33362ef545667b51929482c5180576f5937",
        "title": "LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving"
      },
      {
        "paperId": "0553ff6d10a8dd377d6d0c171f8612231b7211a2",
        "title": "DriveLLM: Charting the Path Toward Full Autonomous Driving With Large Language Models"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
        "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"
      },
      {
        "paperId": "c651cfd518de20c7c15450ab4ebd5bd843072160",
        "title": "Language Conditioned Traffic Generation"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "4f005e799bbc7a608f9c37da02a3a9ab14726d43",
        "title": "ADEPT: A Testing Platform for Simulated Autonomous Driving"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "3fab78b7b213e92d2814421681d826b141732f2e",
        "title": "Simulation-based reinforcement learning for autonomous driving"
      },
      {
        "paperId": "e34b3c2e620aa1c09c234067d9400c8fd53ce6e3",
        "title": "Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Artificial intelligence: a modern approach"
      },
      {
        "paperId": "3f1feaadb5e5ad5c5b362c188d8c700dcba6e02d",
        "title": "Overleaf Example"
      },
      {
        "paperId": null,
        "title": "DataCity Smart Mobility Testing Ground"
      },
      {
        "paperId": null,
        "title": "COSMOS Lab"
      },
      {
        "paperId": null,
        "title": "Conference acronym \u2019XX, June 03\u201305, , Woodstock, NY"
      },
      {
        "paperId": null,
        "title": "2023. Multi-sensor Fusion for In-cabin Vehicular Sensing Applications"
      }
    ],
    "cited_by": [
      {
        "paperId": "2934d6299b63f4def97252dd37fee9d22a924dc9",
        "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey"
      },
      {
        "paperId": "daa9bcf3ffefdfa8c0f25654aa0512a19ff08157",
        "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems"
      },
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "6e0e4d88194ccd424af25aeb60cdd37a030bf813",
        "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts"
      },
      {
        "paperId": "74a92238ec1f6a0688cf01ea1dfb9c9d02591a79",
        "title": "Intelligent Algorithm-Driven Optimization of Water-Cooled Plate Structures for Enhanced Thermal Performance"
      },
      {
        "paperId": "689a49d0275009e563c01a3dae402fe8df4bef8c",
        "title": "Application of Generative AI in Predictive Analysis of Urban Energy Distribution and Traffic Congestion in Smart Cities"
      },
      {
        "paperId": "03f0d177fe3d201e8739716963592865ad2d9527",
        "title": "Learning Personalized Driving Styles via Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "df91c52b853fdcfcf945321655a1cdf685a5b745",
        "title": "A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation"
      },
      {
        "paperId": "a45dabf50cea2ee2e4a734196059d31145d1c095",
        "title": "Position: Prospective of Autonomous Driving - Multimodal LLMs, World Models, Embodied Intelligence, AI Alignment, and Mamba"
      },
      {
        "paperId": "44df72184a96c6316d12d991434c001cab6f2e7f",
        "title": "Integrating LLMs With ITS: Recent Advances, Potentials, Challenges, and Future Directions"
      },
      {
        "paperId": "cdac2a441465b9ddd5523859b613756b6f4a937e",
        "title": "Leveraging Large Language Models for Enhancing Autonomous Vehicle Perception"
      },
      {
        "paperId": "f86a5cef2ca7b090bd5851f4fdea9912d7631c63",
        "title": "Generative Adversarial Networks for High Fidelity Traffic Simulation and Prediction in Intelligent Transportation Systems"
      },
      {
        "paperId": "a206499bb9e2507cd4d1f7f9dcfa4cbb9e1eb441",
        "title": "SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for Autonomous Vehicles with Large Language Models"
      },
      {
        "paperId": "a127635c7bdc919307484e1627bdef21817666b9",
        "title": "A Comprehensive Review of Reinforcement Learning in Intelligent Allocation and Optimization of Educational Resources"
      },
      {
        "paperId": "d8db2c42ff1059d86bf611f28f593503af5e67ca",
        "title": "Advances in Deep Reinforcement Learning for Computer Vision Applications"
      },
      {
        "paperId": "b05a8ca14443d8eaa1429f5c0b2552ef0bc848be",
        "title": "The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications"
      },
      {
        "paperId": "f319eee1c99752a134f6a953f103f4ee2535253e",
        "title": "Decoupled Deep Reinforcement Learning with Sensor Fusion and Imitation Learning for Autonomous Driving Optimization"
      },
      {
        "paperId": "f2063a3205689355f4aaa0310e37cd7cd4eb1afc",
        "title": "Electric Vehicle Charging Infrastructure Optimization Incorporating Demand Forecasting and Renewable Energy Application"
      },
      {
        "paperId": "ba68cf21a1cca60bbeca490eba357d8ac905164b",
        "title": "Optimizing Supply Chain Transparency and Customer Compatibility with AI-Driven Models"
      },
      {
        "paperId": "866dad40122727a646c17e375ce1f7d03b3ed700",
        "title": "AI-powered Strategies for Optimizing Waste Management in Smart Cities in Beijing"
      },
      {
        "paperId": "8be1718c977baa89d5310aeb48216f504505af34",
        "title": "Effect of Adaptive Communication Support on Human-AI Collaboration"
      },
      {
        "paperId": "7b4865bb774f4c5085202097caae8052c89599f7",
        "title": "C HALLENGES AND SOLUTIONS OF AUTONOMOUS DRIVING APPROACHES : A REVIEW"
      }
    ],
    "score": 22.0
  },
  {
    "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
    "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
    "authors": [
      "A. J. Thirunavukarasu"
    ],
    "year": 2023,
    "citationCount": 43,
    "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently \u2018hallucinates\u2019, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work \u2013 tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties \u2013 where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through \u2018reinforcement learning from human feedback\u2019, GPT-3 was subsequently finetuned to provide appropriate responses to users\u2019 queries \u2013 producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 \u2013 which powers newer versions of ChatGPT \u2013 dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4\u2019s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT\u2019s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181\u2013182",
    "url": "https://www.semanticscholar.org/paper/0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
    "pdf_url": "https://doi.org/10.1177/01410768231173123",
    "venue": "Journal of the Royal Society of Medicine",
    "publicationDate": "2023-05-01",
    "externalIds": {
      "PubMedCentral": "10331084",
      "DOI": "10.1177/01410768231173123",
      "CorpusId": 258763926,
      "PubMed": "37199678"
    },
    "references": [
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
        "title": "Capabilities of GPT-4 on Medical Challenge Problems"
      },
      {
        "paperId": "5501d00310b06e00351295529498cc684187148d",
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"
      },
      {
        "paperId": "87538a3ca45cf2b54378260e6fbd7a8e61e1f6f5",
        "title": "Will ChatGPT transform healthcare?"
      },
      {
        "paperId": "236445f0a3b1e30b2542e5e64616ff6a8af7e3ea",
        "title": "Language Models are Few-shot Learners for Prognostic Prediction"
      },
      {
        "paperId": "21647053c34d191d910668c2f18140346bf0f255",
        "title": "Trialling a Large Language Model (ChatGPT) in General Practice With the Applied Knowledge Test: Observational Study Demonstrating Opportunities and Limitations in Primary Care"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "47d27bf80263d965dbee97f8f1d15ccc7845e44c",
        "title": "Patients\u2019 Perceptions Toward Human\u2013Artificial Intelligence Interaction in Health Care: Experimental Study"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "1946246aa06e56ad83459613535a15738dcb3a1f",
        "title": "The Values and Value of Patient-Centered Care"
      },
      {
        "paperId": "8ed61841fbd1e536d88f41881c1c63513854c186",
        "title": "An Early Look at THE LABOR MARKET"
      },
      {
        "paperId": "09d8c7fe0948bf2bda5186ee4c6b6ae9f1b32ad4",
        "title": "An open letter."
      },
      {
        "paperId": null,
        "title": "Pause Giant AI Experiments: an open letter. Future of Life Institute"
      }
    ],
    "cited_by": [
      {
        "paperId": "9d25df83441abda9fb9cb454bf45e296e16f3de2",
        "title": "EVLF-FM: Explainable Vision Language Foundation Model for Medicine"
      },
      {
        "paperId": "ae4033e0680de5e9e946c9aae05578c0e2f72315",
        "title": "Reporting guideline for Chatbot Health Advice studies: the CHART statement"
      },
      {
        "paperId": "71b2e2406bebfcb2d3d69f8f44b33d42f99683c0",
        "title": "Reporting guideline for chatbot health advice studies: the Chatbot Assessment Reporting Tool (CHART) statement"
      },
      {
        "paperId": "db6767e8f3a996ccc4d35ad54c5bb5a823862fed",
        "title": "Reporting guideline for chatbot health advice studies: The CHART statement"
      },
      {
        "paperId": "57a4b0acff65e0b7150f30c791180c5acd0acc6a",
        "title": "Reporting guidelines for chatbot health advice studies: explanation and elaboration for the Chatbot Assessment Reporting Tool (CHART)"
      },
      {
        "paperId": "b8674600462eadb105774d69eeb03535a5f66eff",
        "title": "Reporting Guideline for Chatbot Health Advice Studies: The CHART Statement."
      },
      {
        "paperId": "9ae0b3b2c216ce4f891cbc81d77fe83132456848",
        "title": "Evaluating multiple large language models on orbital diseases"
      },
      {
        "paperId": "3b2d7bbb293a53182d776df94b3599e47ba5d5c5",
        "title": "Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs"
      },
      {
        "paperId": "4381d3bae80c035d0da6d2124934a2b1c8e9a705",
        "title": "A Meta-Clustering Framework for Enhancing Conversational AI in the Healthcare Sector"
      },
      {
        "paperId": "1a019acb39b462a2fba48940d363729aabb43195",
        "title": "Semi-automated screening reveals patients with glaucoma-induced blindness missing out on social support: a cross-sectional study of certificate of visual impairment allocation"
      },
      {
        "paperId": "92e4c3fb0e6b539b4bdec9eeaadcf89b750a11f5",
        "title": "Can large language models replace human experts? Effectiveness and limitations in building energy retrofit challenges assessment"
      },
      {
        "paperId": "e87e5879376276d50fd97a062df0dfd17cb65c59",
        "title": "Large Language Models\u2019 Clinical Decision-Making on When to Perform a Kidney Biopsy: Comparative Study"
      },
      {
        "paperId": "8e5b1084560f7b370363a2ad302c602e7a1ba78c",
        "title": "Large Language Models for Chatbot Health Advice Studies"
      },
      {
        "paperId": "a82253fa49884537a7cf13bfa6e6a359d45aa2d2",
        "title": "Ai-enabled language models (LMs) to large language models (LLMs) and multimodal large language models (MLLMs) in drug discovery and development."
      },
      {
        "paperId": "29357666914fb15b9246154cb2bbecd120357fe3",
        "title": "Slit Lamp Report Generation and Question Answering: Development and Validation of a Multimodal Transformer Model with Large Language Model Integration"
      },
      {
        "paperId": "82704111b6beed40306a70e098840a7b96eef958",
        "title": "Exploring the potential of large language models in identifying metabolic dysfunction\u2010associated steatotic liver disease: A comparative study of non\u2010invasive tests and artificial intelligence\u2010generated responses"
      },
      {
        "paperId": "e7e79a1719da21ea3a60995e9039bbfaf9ca07b2",
        "title": "Large language model uncertainty proxies: discrimination and calibration for medical diagnosis and treatment"
      },
      {
        "paperId": "dfa6ad850f4a01772a65baa3baece97ee5749cc7",
        "title": "The false promise of individual digital sovereignty in Europe: Comparing artificial intelligence and data regulations in China and the European Union"
      },
      {
        "paperId": "70158df5e4846d99accaf180ad5ee218a015f63f",
        "title": "Utilizing Large Language Models in Ophthalmology: The Current Landscape and Challenges"
      },
      {
        "paperId": "278fbc0caaa985be8faad8c0a0010b05ea193820",
        "title": "Managing A Patient with Uveitis in The Era of Artificial Intelligence: Current Approaches, Emerging Trends, And Future Perspectives."
      },
      {
        "paperId": "2285c7d0c84eb08b184e2d4ebab1f2f46647c28f",
        "title": "Large Language Model Uncertainty Measurement and Calibration for Medical Diagnosis and Treatment"
      },
      {
        "paperId": "5b1afcfaa3ddfdc12b240205c2a0d5fb78d7dbc1",
        "title": "Generative artificial intelligence in healthcare from the perspective of digital media: Applications, opportunities and challenges"
      },
      {
        "paperId": "ccb3ce84f8be6aa4e88f5b0902090bf4b0e5174c",
        "title": "The new paradigm in machine learning \u2013 foundation models, large language models and beyond: a primer for physicians"
      },
      {
        "paperId": "8eeedbc6009130beb9a7d8e70fbedc32f857a339",
        "title": "Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook"
      },
      {
        "paperId": "cd4e16b5ef043ca851b137db92a5d52513119d7a",
        "title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study"
      },
      {
        "paperId": "e95dd8a8a07470eb19770dad42470a16315b1b94",
        "title": "Utility of artificial intelligence\u2010based large language models in ophthalmic care"
      },
      {
        "paperId": "4fee5590c9abad6e51f51a30552e0223a4b0be16",
        "title": "Evaluation of GPT-4 for 10-year cardiovascular risk prediction: Insights from the UK Biobank and KoGES data"
      },
      {
        "paperId": "d50a65a48132c8223aec40a03a27f91dc5f8f9e7",
        "title": "This season's artificial intelligence (AI): is today's AI really that different from the AI of the past? Some reflections and thoughts"
      },
      {
        "paperId": "f1a13da94d7b723627969b0f9e300d06916f2d0a",
        "title": "AI in Rehabilitation Medicine: Opportunities and Challenges"
      },
      {
        "paperId": "b430597462c8195ccad53057d11e54a987a53320",
        "title": "ChatGPT's performance in dentistry and allergyimmunology assessments: a comparative study."
      },
      {
        "paperId": "7ae86ce477732b3a53a75e06d683f1f197fae3b7",
        "title": "Artificial intelligence education: An evidence-based medicine approach for consumers, translators, and developers"
      },
      {
        "paperId": "e85f688a2f1c796d3d2ec5602fd390aacdc48e7d",
        "title": "Engagement With Conversational Agent\u2013Enabled Interventions in Cardiometabolic Disease Management: Protocol for a Systematic Review"
      },
      {
        "paperId": "d07ce3d87e98ce804c0aebf430f402bc8c67806d",
        "title": "Generative Artificial Intelligence Through ChatGPT and Other Large Language Models in Ophthalmology"
      },
      {
        "paperId": "6d87361253f655f9cd6e40b0192722fe09782019",
        "title": "Artificial intelligence and digital health in global eye health: opportunities and challenges."
      },
      {
        "paperId": "386e634c381de8700d81d5b739462d0050484b28",
        "title": "Challenges of GPT-3-Based Conversational Agents for Healthcare"
      },
      {
        "paperId": "f63f559bd0958c86e84da4a495f71411c459f517",
        "title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study"
      },
      {
        "paperId": "154cf34154b0dde6f50f9294ea120ba0ae96f18d",
        "title": "How Can the Clinical Aptitude of AI Assistants Be Assayed?"
      },
      {
        "paperId": "94ce1d5924e05e8d75e43ce70044293ddcef850a",
        "title": "Large language models in medicine"
      },
      {
        "paperId": "e1c045c2ce9aefdd4ec1ca1894c5cb427c734d2f",
        "title": "Democratizing Artificial Intelligence Imaging Analysis With Automated Machine Learning: Tutorial"
      },
      {
        "paperId": "46e485cf79a875ef1ac0ed302f2a7f38607f5341",
        "title": "Human, all too human \u2013 why artificial intelligence cannot \u201cauthor\u201d papers"
      },
      {
        "paperId": "da6307caf8c0ecb72d004754bcc1f06ca9c8c443",
        "title": "Performance of ChatGPT on the Peruvian National Licensing Medical Examination: Cross-Sectional Study"
      },
      {
        "paperId": "dcd3b342ca6c1d95ecc76e230aa089c6c351c875",
        "title": "Accuracy and reliability of self-administered visual acuity tests: Systematic review of pragmatic trials"
      },
      {
        "paperId": "a74963fef5444af00568cafa8c412f95164359a0",
        "title": "Do We Need Subject Matter Experts? A Case Study of Measuring Up GPT-4 Against Scholars in Topic Evaluation"
      }
    ],
    "score": 21.5
  },
  {
    "id": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
    "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
    "authors": [
      "Tengyu Xu",
      "Eryk Helenowski",
      "Karthik Abinav Sankararaman",
      "Di Jin",
      "Kaiyan Peng",
      "Eric Han",
      "Shaoliang Nie",
      "Chen Zhu",
      "Hejia Zhang",
      "Wenxuan Zhou",
      "Zhouhao Zeng",
      "Yun He",
      "Karishma Mandyam",
      "Arya Talabzadeh",
      "Madian Khabsa",
      "Gabriel Cohen",
      "Yuandong Tian",
      "Hao Ma",
      "Si-Yuan Wang",
      "Han Fang"
    ],
    "year": 2024,
    "citationCount": 21,
    "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM&reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.",
    "url": "https://www.semanticscholar.org/paper/519d5ccbd5aec517ba987209e17afd4741ac9b8a",
    "pdf_url": "https://arxiv.org/pdf/2409.20370.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-09-30",
    "externalIds": {
      "DBLP": "journals/corr/abs-2409-20370",
      "ArXiv": "2409.20370",
      "DOI": "10.48550/arXiv.2409.20370",
      "CorpusId": 272987127
    },
    "references": [
      {
        "paperId": "2f5ed86600054c40ca7eb9709b80664d92de4784",
        "title": "RuleR: Improving LLM Controllability by Rule-based Data Recycling"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "d30e8ac45470dcc9208edb7a518d69088ae925e8",
        "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models"
      },
      {
        "paperId": "6073196bd50b000571dbdfc46418304a4aff6591",
        "title": "Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "9775bb6870d0c80881487be57de0a0f31cea08be",
        "title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons"
      },
      {
        "paperId": "ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0",
        "title": "Advancing LLM Reasoning Generalists with Preference Trees"
      },
      {
        "paperId": "96cc2c4051144d72195ec3f04000f909d62f6745",
        "title": "A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization"
      },
      {
        "paperId": "2a9accbc670e71041b8e75ad1c1bcc7e04a54599",
        "title": "CVaR-Constrained Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "ad2be51acf42f686a8d1de92d7435d84274ee62d",
        "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"
      },
      {
        "paperId": "9637ef9019671034912ea0f506ae67c3f2fc4689",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "fb4dc0178e5d7347b1615c48caf05347b6e5eb48",
        "title": "TrustLLM: Trustworthiness in Large Language Models"
      },
      {
        "paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "8b28792f8405b737229afb92c99c579b86d8aa98",
        "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"
      },
      {
        "paperId": "0f9995ec08e95bea09d512c59e40d19f0f44d7bb",
        "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
        "title": "Instruction-Following Evaluation for Large Language Models"
      },
      {
        "paperId": "0a27dde07d28ca8d92ed46cecc71585e1c9693f2",
        "title": "MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "84a36e19f9394f22b34f79756fa9628a795e02ea",
        "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"
      },
      {
        "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      },
      {
        "paperId": "28c6ac721f54544162865f41c5692e70d61bccab",
        "title": "A Survey on Large Language Model based Autonomous Agents"
      },
      {
        "paperId": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
        "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
        "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"
      },
      {
        "paperId": "de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "7e44c21bfad181960b259754a462060dfdd41cb0",
        "title": "Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94",
        "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
        "title": "Measuring Coding Challenge Competence With APPS"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "789b5441743c2e38cf4c38749ed820c0671d81b1",
        "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning"
      },
      {
        "paperId": "700e73bf63837c555c40a6d67d919cb8154c52a0",
        "title": "CRPO: A New Approach for Safe Reinforcement Learning with Convergence Guarantee"
      },
      {
        "paperId": "4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563",
        "title": "Recipes for Safety in Open-domain Chatbots"
      },
      {
        "paperId": "74f23063ca77f5b1caa3770a5957ae5fc565843e",
        "title": "Multi-Task Learning with Deep Neural Networks: A Survey"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "8c823e402c6460f4dcabebfd509850787971d9d5",
        "title": "Loss-Balanced Task Weighting to Reduce Negative Transfer in Multi-Task Learning"
      },
      {
        "paperId": "ab376f85d36f3af1778e5560421c6cb6c2fefd1b",
        "title": "Deep Reinforcement Learning for Multiobjective Optimization"
      },
      {
        "paperId": "2b0d7e51efd004fe3847f54863540c79312f3546",
        "title": "Multi-Task Learning as Multi-Objective Optimization"
      },
      {
        "paperId": "8aeb3d5a097d33fc20efa046c0ed6cb886498d42",
        "title": "A Modulation Module for Multi-task Learning with Applications in Image Retrieval"
      },
      {
        "paperId": "619cf9d39abb93fe1ab17921c163fc5734ac1e70",
        "title": "End-To-End Multi-Task Learning With Attention"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11",
        "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"
      },
      {
        "paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49",
        "title": "Multitask Learning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "597f869a3ae3572e16de3643a7bd285ea2f1eabd",
        "title": "Defining and Characterizing Reward Gaming"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Llama Team"
      },
      {
        "paperId": null,
        "title": "Distilabel: An ai feedback aif) framework for building datasets with and for llms"
      },
      {
        "paperId": null,
        "title": "From live data to high-quality benchmarks: The arena-hard pipeline,"
      },
      {
        "paperId": null,
        "title": "Human-centered loss functions (halos)"
      },
      {
        "paperId": null,
        "title": "Introducing meta llama 3: The most capable openly available llm to date"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ],
    "cited_by": [
      {
        "paperId": "396fdacf5afe126e69d1e9364fe04462726b47bc",
        "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models"
      },
      {
        "paperId": "6220a5c93c912b12b95a11bc150cbd28f6bb0ee5",
        "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers"
      },
      {
        "paperId": "10734df8302641767f638c4a02fbf9e1cf08a666",
        "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs"
      },
      {
        "paperId": "0d426b4dd26dc2baaef545eeb445136d612bc6b7",
        "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities"
      },
      {
        "paperId": "ac9a7ebd4187ba0a280428e044b60cd71701f418",
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models"
      },
      {
        "paperId": "ecebfa518ff7ce392e5a73559484fb6b0b8493d2",
        "title": "Reinforcement Learning with Partially Defined Rewards and Human Feedback for Energy Efficiency Recommendations"
      },
      {
        "paperId": "23678c54e17e016fe1d9c61157aab7c3a274ec7d",
        "title": "Boosting LLM Reasoning via Spontaneous Self-Correction"
      },
      {
        "paperId": "26e9ce47fc8da27174e73a19d3f42beb948574d4",
        "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO"
      },
      {
        "paperId": "22f0804270d06d6a7e2d0cc52b1f8984c0a134a4",
        "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training"
      },
      {
        "paperId": "108eedca59ab50c6cfead57efc1d5f6d7bed21b8",
        "title": "Reinforcement Learning from User Feedback"
      },
      {
        "paperId": "ccd9eca10294fe822a25e1133d59deacab005860",
        "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs"
      },
      {
        "paperId": "a9d6a3fa154837e0d88238fc779a9993817e440f",
        "title": "Robust Multi-Objective Controlled Decoding of Large Language Models"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "933a91ec61c20393b6deff5b6e09051eb9a3a655",
        "title": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization"
      },
      {
        "paperId": "799f6aa3f3cb677bf618c0815aa3149c341495b8",
        "title": "Multi-objective Deep Learning: Taxonomy and Survey of the State of the Art"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "e4a4a98cf3ce47937c406918f43f35f494273498",
        "title": "\u2200uto\u2203\u2228\u2227L: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks"
      },
      {
        "paperId": "b51d1946a6184946c93809d06942aa410c384203",
        "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey"
      },
      {
        "paperId": "57f59779375f700d4288ef2397903d488f49b9a7",
        "title": "Beyond Preferences in AI Alignment"
      },
      {
        "paperId": "eac3b8ed1642677656503f6e3a0c2219e808384e",
        "title": "Intent-Based Infrastructure and Service Orchestration Using Agentic-AI"
      },
      {
        "paperId": "92616847a332a1bb0960c95cf2276b4dadfd4288",
        "title": "Strict Verification: Exploring Reinforcement Learning with Weak Verifiers"
      }
    ],
    "score": 21.0
  },
  {
    "id": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
    "authors": [
      "Wei Shen",
      "Guanlin Liu",
      "Zheng Wu",
      "Ruofei Zhu",
      "Qingping Yang",
      "Chao Xin",
      "Yu Yue",
      "Lin Yan"
    ],
    "year": 2025,
    "citationCount": 20,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.",
    "url": "https://www.semanticscholar.org/paper/25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
    "pdf_url": "https://arxiv.org/pdf/2503.22230.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-03-28",
    "externalIds": {
      "DBLP": "journals/corr/abs-2503-22230",
      "ArXiv": "2503.22230",
      "DOI": "10.48550/arXiv.2503.22230",
      "CorpusId": 277435161
    },
    "references": [
      {
        "paperId": "6508e62ee67ecb48fc1534f7f8cf2f687ae68bab",
        "title": "LIMR: Less is More for RL Scaling"
      },
      {
        "paperId": "003c918fbc70516d3d5c087a681b94bc98106198",
        "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "e07a7e57745b69c0f08601fbdd19cfdcafbe1152",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "paperId": "983c01d00102075dae128b8ef9f01abef98720b5",
        "title": "Does RLHF Scale? Exploring the Impacts From Data, Model, and Method"
      },
      {
        "paperId": "004392baa790f7c04744df1865cee6b730508d77",
        "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "029d309dd1f3a93ff74740ceca32da8c09c63e5c",
        "title": "BOND: Aligning LLMs with Best-of-N Distillation"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "1460d33f547ee40c560174dc0f6898f4802f4cf8",
        "title": "Calibrated Language Models Must Hallucinate"
      },
      {
        "paperId": "c085e88a0351e393609a95305afc1db792d1db0f",
        "title": "The History and Risks of Reinforcement Learning and Human Feedback"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "0ba581718f294db1d7b3dbc159cc3d3380f74606",
        "title": "ChatGPT for Robotics: Design Principles and Model Abilities"
      },
      {
        "paperId": "f1e7332a76be8f38091193e6e929d0d653f4867c",
        "title": "On The Fragility of Learned Reward Functions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "cb81614f0641511e6edd62b55357604734a6f5a4",
        "title": "Behind"
      },
      {
        "paperId": "c35fb9c7f10a60488ded4a1bb520e68c9ec957a5",
        "title": "Failure Modes of Learning Reward Models for LLMs and other Sequence Models"
      },
      {
        "paperId": null,
        "title": "Multilingual Capability"
      },
      {
        "paperId": null,
        "title": "example demonstrates encoding a Chinese string into UTF -8 bytes and decoding it back , showcasing UTF -8's reliability in handling multilingual text"
      },
      {
        "paperId": null,
        "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ],
    "cited_by": [
      {
        "paperId": "a29904e740589b5edd0bdad3b2125f4fb88a9b3e",
        "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR"
      },
      {
        "paperId": "9c7bfeadec9b644ca27579091f4011f3e9dde59f",
        "title": "G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation"
      },
      {
        "paperId": "618d58b52841a741487215cb04ace9d6b46e7e5a",
        "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster"
      },
      {
        "paperId": "34cb5a043fbfb04dbcc3f346e959f537c22d7bfe",
        "title": "Enhancing Large Language Models through Structured Reasoning"
      },
      {
        "paperId": "df587439def5a7f5424e7fbc0521c9b494b4001f",
        "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?"
      },
      {
        "paperId": "cf92906ac18467b2bce3cf1cbd77bc4ed1201352",
        "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "f0143edd509e55ec2337849aa2ebef0a432f4cca",
        "title": "TreeRPO: Tree Relative Policy Optimization"
      },
      {
        "paperId": "d6123d6d213436d8258b4a8f8b7fb90120006239",
        "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles"
      },
      {
        "paperId": "66f40985d5f6c1db33ab952986d16ca2369b073e",
        "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "277b7cd4b88787e0b42e14d0ec497b435d219968",
        "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model"
      },
      {
        "paperId": "984f815d96d1e15b52b7003f232f85d9be78c1e1",
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs"
      },
      {
        "paperId": "94a5613ece3c6e94e56b2701001a0a4a6f856075",
        "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning"
      },
      {
        "paperId": "aa3517c664890cd36aee3cfff0f09d2645e373b6",
        "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility"
      },
      {
        "paperId": "d85788857fd230169e17638631b96335368043ed",
        "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
      },
      {
        "paperId": "a7381c3a8184d6c259eda7a2412edad50f2d50de",
        "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning"
      },
      {
        "paperId": "f26fcc2b9fc8944e054425d19c12b9d5cca64fcb",
        "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?"
      },
      {
        "paperId": "a4735b3bfe3231cde53969460930e5cf03da48bc",
        "title": "REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models"
      },
      {
        "paperId": "a3d5e8729be281ed50002fe4eb99e7d475d75f27",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      }
    ],
    "score": 20.0
  },
  {
    "id": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
    "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
    "authors": [
      "Nathan Lambert",
      "Roberto Calandra"
    ],
    "year": 2023,
    "citationCount": 38,
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) more capable in complex settings. RLHF proceeds as collecting human preference data, training a reward model on said data, and optimizing a base ML model with respect to said reward for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many assumptions about how the various pieces fit together, such as a reward model capturing human preferences and an RL optimizer extracting the right signal from a reward model. As the RLHF process involves many distinct design decisions, it is easy to assume that multiple processes are correlated and therefore numerically linked. This apparent correlation is often not true, where reward models are easily overoptimized or RL optimizers can reduce performance on tasks not modeled in the data. Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model training, RL scores, and downstream performance drives these issues, which we describe as an objective mismatch. In this paper, we illustrate the causes of this issue, reviewing relevant literature from model-based reinforcement learning, and argue for solutions. By solving objective mismatch in RLHF, the ML models of the future will be more precisely aligned to user instructions for both safety and helpfulness.",
    "url": "https://www.semanticscholar.org/paper/9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
    "pdf_url": "https://arxiv.org/pdf/2311.00168.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-31",
    "externalIds": {
      "ArXiv": "2311.00168",
      "DBLP": "journals/corr/abs-2311-00168",
      "DOI": "10.48550/arXiv.2311.00168",
      "CorpusId": 264832734
    },
    "references": [
      {
        "paperId": "c085e88a0351e393609a95305afc1db792d1db0f",
        "title": "The History and Risks of Reinforcement Learning and Human Feedback"
      },
      {
        "paperId": "e5d0857feca845b474b89565d513ff599629851d",
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "690fe81e99b1486ff49c9fc4da9bcd1a7e674668",
        "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "f05c288caeb9a14ef387e6867934ced3d2200259",
        "title": "SALMON: Self-Alignment with Instructable Reward Models"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "e571f9b823dcb31580161a37342a73dd93ebbe52",
        "title": "Learning Optimal Advantage from Preferences and Mistaking it for Reward"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "paperId": "541b66bad4a9bf9b7fd97f13f94ab9061c7c0c47",
        "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF"
      },
      {
        "paperId": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "paperId": "f8a2dca1e8fe56e698984c077f7ff58d8ca867e9",
        "title": "Large Language Models as Optimizers"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
        "title": "Holistic Evaluation of Language Models"
      },
      {
        "paperId": "60d90e96e7c434861697194fa47f1978d86b9d28",
        "title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models"
      },
      {
        "paperId": "f8f6942be75d102a14c6441e0bb31ac7c59235a4",
        "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models"
      },
      {
        "paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef",
        "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"
      },
      {
        "paperId": "6ba3e4172e5c22c8c3ace05a31e9b119a2e3c33c",
        "title": "Continual Learning for Instruction Following from Realtime Feedback"
      },
      {
        "paperId": "3b4344a2d52ab2ac257b86c4a96bcf60eacaa4e0",
        "title": "Personalized Reward Learning with Interaction-Grounded Learning (IGL)"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
        "title": "Teaching language models to support answers with verified quotes"
      },
      {
        "paperId": "0ea5518a745a3c1f6fc53d2e7ec5b0bd016cb8d5",
        "title": "Investigating Compounding Prediction Errors in Learned Dynamics Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "01a4d49a0ec54b6bed8cc8726fd2568df788f0f4",
        "title": "Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "20a016b286c1e6cb5bb70bf6c6e79639a8993ac6",
        "title": "Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "ecbdc83a1aa8d196943d8997300be871b1c7c2dc",
        "title": "Objective Mismatch in Model-based Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "837614dfc2ef6ff9cebf852470781bec8c55642c",
        "title": "Calibrated Model-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "abcbfc9742e8f4825cfc536091fd414e08d03998",
        "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
        "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "18a387c83289267c79ba4adf6af17c88b73d5180",
        "title": "Contextual Bandits and Imitation Learning via Preference-Based Active Queries"
      },
      {
        "paperId": null,
        "title": "Plotting progress in ai"
      },
      {
        "paperId": null,
        "title": "Huggingface h4"
      },
      {
        "paperId": null,
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Chatgpt: Optimizing language models for dialogue"
      },
      {
        "paperId": "c09f4a8c916a98d2f23abbaa24f183667e9806e9",
        "title": "Partially Observable Markov Decision Processes"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      }
    ],
    "cited_by": [
      {
        "paperId": "6e2ec808619d79a3bcb688f4b6a6e9ac1f7f892f",
        "title": "Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining"
      },
      {
        "paperId": "8c497a6dfcaf431d1b4c932ae008b4ebe8a85883",
        "title": "SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs"
      },
      {
        "paperId": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
        "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "26e9ce47fc8da27174e73a19d3f42beb948574d4",
        "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO"
      },
      {
        "paperId": "a71840c64c2584f3b99adf68dabfaee3797f6f08",
        "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF"
      },
      {
        "paperId": "d60e33f599873a511ee8cf4c6813e283052e659d",
        "title": "Establishing Reliability Metrics for Reward Models in Large Language Models"
      },
      {
        "paperId": "8991cdbbdca17e80cc06313ce668d00cff321d01",
        "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation"
      },
      {
        "paperId": "aa2e3c52b4c6e847f6703c84f5640cae42dbc73d",
        "title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment"
      },
      {
        "paperId": "36635314d9361154ff381bbe465b06223f118239",
        "title": "Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling"
      },
      {
        "paperId": "5d1aea91e8cdb37a3f23f566ee095c0f007f02ad",
        "title": "Drowning in Documents: Consequences of Scaling Reranker Inference"
      },
      {
        "paperId": "442cd80bdcd49a843ad3ace7fbf6548b268475ef",
        "title": "LLM Security Alignment Framework Design Based on Personal Preference"
      },
      {
        "paperId": "ab0df4c771c19e3e7d4edc1057a9f91c6c8e2ae5",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "50ee11e0d057defe6c449099e01b12cf3cb74446",
        "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking"
      },
      {
        "paperId": "8096ca5f6895955dc41f05094f976b76419437fd",
        "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison"
      },
      {
        "paperId": "7792fe02a297344a25a0b22fa8d6cbd86884be31",
        "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback"
      },
      {
        "paperId": "ebfd28b256177177beebb8b864043a7944826b1a",
        "title": "SEAL: Systematic Error Analysis for Value ALignment"
      },
      {
        "paperId": "41fda533f525a82aa041a9656c44eb70e36c19ab",
        "title": "Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification"
      },
      {
        "paperId": "d24e5a8e0f65d527872e33ef1d5f665fc171cf8b",
        "title": "It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
        "title": "Creativity Has Left the Chat: The Price of Debiasing Language Models"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "de0d53201d259a5f3d13efe06bafacb70bda02e2",
        "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding"
      },
      {
        "paperId": "6976185bec1d85b6e72c49626266c566eb6aff6f",
        "title": "Aligning protein generative models with experimental fitness via Direct Preference Optimization"
      },
      {
        "paperId": "afb06e773d9f073a885a095a8bbb5a5b761a3ab5",
        "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment"
      },
      {
        "paperId": "80fd3d2d3b2f7e0cb41c935c6b7ac1b73823a60d",
        "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "7647707b6b68c963314de0aab1514176b11732df",
        "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94",
        "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "67eab08db30e397e400e3b36b3afd7526df83314",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      },
      {
        "paperId": "911bae2b36d64101a41c6aebde062e07853b3918",
        "title": "Understanding User Experience in Large Language Model Interactions"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "5a2c395ca5dc942dbb6ea7eea1e324b8c9e8534e",
        "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "ed62e3847f45f152cf6d7b9b4bebb782547f1a54",
        "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment"
      },
      {
        "paperId": "951a11a4d247de5823dfa42affe39cf849c708b8",
        "title": "Pairwise Proximal Policy Optimization: Large Language Models Alignment via Comparative RL"
      },
      {
        "paperId": "7fa92edb84802e413c497062a026439a5a10f5b7",
        "title": "S TYLE OVER S UBSTANCE : F AILURE M ODES OF LLM J UDGES IN A LIGNMENT B ENCHMARKING"
      },
      {
        "paperId": "92616847a332a1bb0960c95cf2276b4dadfd4288",
        "title": "Strict Verification: Exploring Reinforcement Learning with Weak Verifiers"
      }
    ],
    "score": 19.0
  },
  {
    "id": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
    "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
    "authors": [
      "Yuanzhao Zhai",
      "Han Zhang",
      "Yu Lei",
      "Yue Yu",
      "Kele Xu",
      "Dawei Feng",
      "Bo Ding",
      "Huaimin Wang"
    ],
    "year": 2023,
    "citationCount": 36,
    "abstract": "Reinforcement learning from human feedback (RLHF) emerges as a promising paradigm for aligning large language models (LLMs). However, a notable challenge in RLHF is overoptimization, where beyond a certain threshold, the pursuit of higher rewards leads to a decline in human preferences. In this paper, we observe the weakness of KL regularization which is commonly employed in existing RLHF methods to address overoptimization. To mitigate this limitation, we scrutinize the RLHF objective in the offline dataset and propose uncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty regularization during RL-finetuning. To enhance the uncertainty quantification abilities for reward models, we first propose a diverse low-rank adaptation (LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations. Then we optimize policy models utilizing penalized rewards, determined by both rewards and uncertainties provided by the diverse reward LoRA ensembles. Our experimental results, based on two real human preference datasets, showcase the effectiveness of diverse reward LoRA ensembles in quantifying reward uncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be pivotal in mitigating overoptimization, thereby contributing to the overall performance.",
    "url": "https://www.semanticscholar.org/paper/cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
    "pdf_url": "https://arxiv.org/pdf/2401.00243.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-12-30",
    "externalIds": {
      "ArXiv": "2401.00243",
      "DBLP": "journals/corr/abs-2401-00243",
      "DOI": "10.48550/arXiv.2401.00243",
      "CorpusId": 266693165
    },
    "references": [
      {
        "paperId": "7da928c74b26953860331404ceaa23ed25e2ea5f",
        "title": "Policy Optimization in RLHF: The Impact of Out-of-preference Data"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "cb62be8c85f1dd0e7c4ea24ed4feb5b90229ee25",
        "title": "LoRA ensembles for large language model fine-tuning"
      },
      {
        "paperId": "c96297261467b5daa2d01227496a70d444602434",
        "title": "Baichuan 2: Open Large-scale Language Models"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "370e51386abb7b999728e08b74f0a77fbd064834",
        "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b",
        "title": "Aligning Language Models with Preferences through f-divergence Minimization"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "55174eacb397cadee19b3315a06e8b4df4c4cc0a",
        "title": "A Review of Uncertainty for Deep Reinforcement Learning"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "1e34c51b52002796fea6f523b9f794f1d75d9ba8",
        "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
        "title": "Uncertainty Estimation for Language Reward Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "877e8140cb7f26042f6c5f1eefcf68a2748721f0",
        "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning"
      },
      {
        "paperId": "fc70db46738fff97d9ee3d66c6f9c57794d7b4fa",
        "title": "A survey of uncertainty in deep neural networks"
      },
      {
        "paperId": "9ffcb3624f2637b5d0fe28c61ec8472293cfebc7",
        "title": "All the News That\u2019s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "8d115c3b2ee80e0754360a154a9369bc1658b607",
        "title": "Obtaining Well Calibrated Probabilities Using Bayesian Binning"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "00919831a34e783d185c3ff2c760b2cb0f0bff39",
        "title": "Quantifying Uncertainty in Foundation Models via Ensembles"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "8e0be569ea77b8cb29bb0e8b031887630fe7a96c",
        "title": "Random Forests"
      },
      {
        "paperId": null,
        "title": "Low-rank adaptation of large language models"
      },
      {
        "paperId": null,
        "title": "Remax A simple, effective, and efficient method for aligning large language models"
      },
      {
        "paperId": null,
        "title": "Stackllama: An rl fine-tuned llama model for stack exchange"
      },
      {
        "paperId": null,
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      }
    ],
    "cited_by": [
      {
        "paperId": "26e9ce47fc8da27174e73a19d3f42beb948574d4",
        "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO"
      },
      {
        "paperId": "bbdc38ce5aa93070fd4394640715b37870057e07",
        "title": "Policy Optimized Text-to-Image Pipeline Design"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "0467ea5c1751f55693840be65a46573fe1c9d5eb",
        "title": "On the Robustness of Reward Models for Language Model Alignment"
      },
      {
        "paperId": "6e1d27c0a3d9f2c1cd12fae75d8bb115e15f8e09",
        "title": "Energy-Based Reward Models for Robust Language Model Alignment"
      },
      {
        "paperId": "e2104209955176916ac487cd99d26875097dc43b",
        "title": "Adversarial Training of Reward Models"
      },
      {
        "paperId": "97713875bef1c71c03c3b4cc201695d1d8f75fd9",
        "title": "Ensemble Learning for Large Language Models in Text and Code Generation: A Survey"
      },
      {
        "paperId": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking"
      },
      {
        "paperId": "99d59b4ec0a2b0860da49f11fe698282af43dff8",
        "title": "Towards Reliable Alignment: Uncertainty-aware RLHF"
      },
      {
        "paperId": "f09e51ae8cc08b3df6544a94faea601d463439af",
        "title": "Uncertainty-Penalized Direct Preference Optimization"
      },
      {
        "paperId": "2bbfe484ea3c38619791c10c5a30142dbea12fd0",
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms"
      },
      {
        "paperId": "b98594e00ec423663d54be4674b70c0d17c66f23",
        "title": "Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via Sparse Task Projection"
      },
      {
        "paperId": "d05d071028be9078b8ac4a66cce9f911bea88362",
        "title": "Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "a3e894356ca9865b881cb53aa62548f1dbff82d8",
        "title": "Reward-Robust RLHF in LLMs"
      },
      {
        "paperId": "e66f9a0fbda43e311d51ab0f0e9d1637c41b3385",
        "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Finetuned Models"
      },
      {
        "paperId": "b51d1946a6184946c93809d06942aa410c384203",
        "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey"
      },
      {
        "paperId": "0327596d1c83506035a2372eb649d5c16d9515be",
        "title": "Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications"
      },
      {
        "paperId": "291c94b62953e261c94b74516ee997be5511c052",
        "title": "A Survey on LoRA of Large Language Models"
      },
      {
        "paperId": "6013c87aa7d6fa6bbe40764df2a59aaa876cddf8",
        "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "4040099ed20718f418733cd201709cd950f11def",
        "title": "Online Self-Preferring Language Models"
      },
      {
        "paperId": "6b3fb301ab6bcc97b278f7a059631445babcf051",
        "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "5dcc26649414295ec3d1d9a274d41b2759e53f8e",
        "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "8c785ebee1f34464dbc85ab4113bccafd7a74b0a",
        "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"
      },
      {
        "paperId": "a80d962fe8d5dc3ed19583419e2de46aef4fe8ba",
        "title": "Bayesian Reward Models for LLM Alignment"
      },
      {
        "paperId": "2be4cc6d28ced11a13c767dd5edaabea06825272",
        "title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"
      },
      {
        "paperId": "e0739369308c908da5807166609f2552db9c8ea4",
        "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "60e3658f86393d65a6d523bfb88fd21e4447d941",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "paperId": "1882849855895456fe842203f245ffaf66b72eff",
        "title": "Bayesian low-rank adaptation for large language models"
      },
      {
        "paperId": "e51a7528b9f6ec2101a682ebde818e40f96c8b24",
        "title": "Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      }
    ],
    "score": 18.0
  },
  {
    "id": "c243df958269bf501f874ef213ba6cc904f24ea9",
    "title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding",
    "authors": [
      "Hangfan Zhang",
      "Zhimeng Guo",
      "Huaisheng Zhu",
      "Bochuan Cao",
      "Lu Lin",
      "Jinyuan Jia",
      "Jinghui Chen",
      "Di Wu"
    ],
    "year": 2024,
    "citationCount": 18,
    "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is \u201c could alignment really prevent those open-sourced large language models from being mis-used to generate undesired content? \u201d. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
    "url": "https://www.semanticscholar.org/paper/c243df958269bf501f874ef213ba6cc904f24ea9",
    "pdf_url": "https://doi.org/10.18653/v1/2024.acl-long.299",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/acl/ZhangGZC00CW24",
      "DOI": "10.18653/v1/2024.acl-long.299",
      "CorpusId": 271915502
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "3fa3267097ffa70bfbbca077c8341ca525b34c18",
        "title": "Securing social network user data in large language model deployments: challenges and best practices"
      },
      {
        "paperId": "5e036f9058c567e344e21d3c000e33e2f0af7aa6",
        "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience"
      },
      {
        "paperId": "5d88cdde363a1526ed9de57e5857fcc5b8970c5c",
        "title": "NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs"
      },
      {
        "paperId": "990d4cda6c97a652ec7c4f16793e78430d5dd5b6",
        "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation"
      },
      {
        "paperId": "cefc7a1b837ec10b2792cc4d4ed50753a1849cb9",
        "title": "Strategic Deflection: Defending LLMs from Logit Manipulation"
      },
      {
        "paperId": "9c6ec11338977841e1beb266eea25c7d293d2ae4",
        "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning"
      },
      {
        "paperId": "402ee244a32fce7b53ff2e7bacb07f6996d65b33",
        "title": "Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking"
      },
      {
        "paperId": "90a64c24162db9a8e05d5b0535c5265f9ca3adfe",
        "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models"
      },
      {
        "paperId": "51941fb4d679b102a50b18f1c1a6f70b6b929f59",
        "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models"
      },
      {
        "paperId": "5b79112817c0115d3db49245311982b623270422",
        "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs"
      },
      {
        "paperId": "f15b15bf22315f1767a891d99f2841b1d3464b6a",
        "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models"
      },
      {
        "paperId": "d0a02239a3003015f4844a4b647505f86841e008",
        "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking"
      },
      {
        "paperId": "0856da236950fbc206ba69d94b32b436f52c1e74",
        "title": "SQL Injection Jailbreak: a structural disaster of large language models"
      },
      {
        "paperId": "e16b2eb3e9135b18c9d4168c790ef375a95d086f",
        "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
      },
      {
        "paperId": "c17d9fbc03f960594870054bc72a3401521e2e61",
        "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction"
      },
      {
        "paperId": "5f0913ff752271bdb958b73e13f6b46577554379",
        "title": "JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework"
      },
      {
        "paperId": "3880048f1790d8b99b213afed6c0ea0902f10ba3",
        "title": "FlipAttack: Jailbreak LLMs via Flipping"
      },
      {
        "paperId": "bcb694c1fbf55ce884cbe89ff7c489f933586ef7",
        "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal"
      }
    ],
    "score": 18.0
  },
  {
    "id": "f7734502f2d9d464b5bd2c62a6805ca492ea61c0",
    "title": "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
    "authors": [
      "Meng Cao",
      "Lei Shu",
      "Lei Yu",
      "Yun Zhu",
      "Nevan Wichers",
      "Yinxiao Liu",
      "Lei Meng"
    ],
    "year": 2024,
    "citationCount": 17,
    "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.",
    "url": "https://www.semanticscholar.org/paper/f7734502f2d9d464b5bd2c62a6805ca492ea61c0",
    "pdf_url": "https://doi.org/10.18653/v1/2024.emnlp-main.515",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/emnlp/Cao0YZWLM24",
      "ACL": "2024.emnlp-main.515",
      "DOI": "10.18653/v1/2024.emnlp-main.515",
      "CorpusId": 273901456
    },
    "references": [
      {
        "paperId": "c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "58e2acdd805d9cc48c7fdde0c9c09280f9f6c4e0",
        "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting"
      },
      {
        "paperId": "dd40ccc5388c797a71c3e7c2a67903986f718f94",
        "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research"
      },
      {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "da5fcb26c830663b79c9aa1c550ae62e7725fcad",
        "title": "Systematic Rectification of Language Models via Dead-end Analysis"
      },
      {
        "paperId": "29acc890e521f7a6415666ab9eb3432c49b4587a",
        "title": "Self-critiquing models for assisting human evaluators"
      },
      {
        "paperId": "023edab4738690444e3924e224c2641017a0d794",
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "b808b6ddf511977e9a33dbe01b412a02b6092ae0",
        "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization"
      },
      {
        "paperId": "02f033482b8045c687316ef81ba7aaae9f0a2e1c",
        "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts"
      },
      {
        "paperId": "005acb881061eb8137e9d36a05a6a0bdf0026b61",
        "title": "Hierarchical Reinforcement Learning By Discovering Intrinsic Options"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
        "title": "GeDi: Generative Discriminator Guided Sequence Generation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "822bdb6e8c39e272ebfee127666e032bd3aa0107",
        "title": "The NetHack Learning Environment"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
        "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"
      },
      {
        "paperId": "e04a80263d252a3d8a382ba37a249b9345620570",
        "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
      },
      {
        "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"
      },
      {
        "paperId": "0fa1c75a452a046e11e775eb6120051c696d9366",
        "title": "Using Natural Language for Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd",
        "title": "Data-Efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
        "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
        "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
        "title": "Dual Learning for Machine Translation"
      },
      {
        "paperId": "fb32191ec07ba4d7badc76ca428c816995b5785a",
        "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access"
      },
      {
        "paperId": "1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6",
        "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction"
      },
      {
        "paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec",
        "title": "An Actor-Critic Algorithm for Sequence Prediction"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
        "title": "Deep Reinforcement Learning for Dialogue Generation"
      },
      {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "d8f7ed400c74490a82424e634119782f3d778771",
        "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation"
      },
      {
        "paperId": "848b8458d36f0e976da8ad59dc7c073987c175e1",
        "title": "Exploration-Guided Reward Shaping for Reinforcement Learning under Sparse Rewards"
      },
      {
        "paperId": null,
        "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
      },
      {
        "paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671",
        "title": "Text Generation by Learning from Demonstrations"
      },
      {
        "paperId": "c758fb4bc55336c745a7bb6d13b2f99cc3a2b5e3",
        "title": "A Duality Approach for Regret Minimization in Average-Award Ergodic Markov Decision Processes"
      },
      {
        "paperId": null,
        "title": "Openwebtext corpus"
      },
      {
        "paperId": null,
        "title": "Bandit-Sum: Extractive summarization as a contextual ban-dit"
      },
      {
        "paperId": null,
        "title": "2022. Exploration-guided reward shaping"
      },
      {
        "paperId": null,
        "title": "2023. Mistral 7b"
      },
      {
        "paperId": null,
        "title": "2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "paperId": null,
        "title": "Arvind"
      },
      {
        "paperId": null,
        "title": "Evaluate the summary independently for each of the three metrics"
      },
      {
        "paperId": null,
        "title": "2022b. Learning with rejection for abstrac-tive text summarization"
      },
      {
        "paperId": null,
        "title": "2023. Guiding pretraining in reinforcement learning with large language models"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "2023. Reward design with language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "8fc90fff462035003595761c7a0eb234fdf611c3",
        "title": "From reaction to reflection: A recursive framework for the evolution and structure of intelligence"
      },
      {
        "paperId": "5758daad33669d01acba4753224a63892c6c4ccb",
        "title": "Leveraging Generative AI and Large Language Model for Process Systems Engineering: A State-of-the-Art Review"
      },
      {
        "paperId": "39e8b83aea2f232eb7b57b19580044ff596eefd9",
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
      },
      {
        "paperId": "b38b997ab0c88c1791475a015766805a30ada041",
        "title": "DAPC: A Prefix-Based Approach with Reinforced Attention for Attribute-Based Controllable Text Generation"
      },
      {
        "paperId": "2109255b50868c453d397bc732fc44d8e83a164f",
        "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF"
      },
      {
        "paperId": "e51c3bc636aa14d2f35743cd59d3e0c4a225091b",
        "title": "From intelligence to autopoiesis: rethinking artificial intelligence through systems theory"
      },
      {
        "paperId": "086935745ddf0507392e1a77ad8a45ad19ac6081",
        "title": "MNC: A multi-agent framework for complex network configuration"
      },
      {
        "paperId": "758fc4cb9100f83ed68f30850af2af824e2a9f6f",
        "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization"
      },
      {
        "paperId": "13f9dd2c7d259460699b0d2ca9acf52351535d91",
        "title": "Align to Structure: Aligning Large Language Models with Structural Information"
      },
      {
        "paperId": "0f38ed2e5265848810f82ba6bda603c8ebf71ce8",
        "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs"
      },
      {
        "paperId": "1eec58f271ac291ac1645a2a6162c4a5b659d0dc",
        "title": "Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping"
      },
      {
        "paperId": "95bcd5cdec716fa2509cc275aa7bf0f619755fcc",
        "title": "MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      },
      {
        "paperId": "8f599aaa623c43ad9a9d91b3c67ee29ad14ca078",
        "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning"
      },
      {
        "paperId": "3f06ee4a359136f957818c4b62b5f4c8aea1daca",
        "title": "Towards Cost-Effective Reward Guided Text Generation"
      },
      {
        "paperId": "19716f3ad5fe69de7a18bb1a3c496a6b9197bac7",
        "title": "Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings"
      },
      {
        "paperId": "e7000d4dc3ca07fe320f5d0431cb6f253a1e59c8",
        "title": "An optimal scheduling method of hydrogen production systems using curtailed electricity through artificial intelligence and deep reinforcement learning"
      }
    ],
    "score": 17.0
  },
  {
    "id": "eb6ef63df104c1b35bbc2400f00285b3414400b2",
    "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning",
    "authors": [
      "Zhenghai Xue",
      "Longtao Zheng",
      "Qian Liu",
      "Yingru Li",
      "Xiaosen Zheng",
      "Zejun Ma",
      "Bo An"
    ],
    "year": 2025,
    "citationCount": 17,
    "abstract": "Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.",
    "url": "https://www.semanticscholar.org/paper/eb6ef63df104c1b35bbc2400f00285b3414400b2",
    "pdf_url": "https://arxiv.org/pdf/2509.02479.pdf",
    "venue": "",
    "publicationDate": "2025-09-02",
    "externalIds": {
      "ArXiv": "2509.02479",
      "CorpusId": 281080825
    },
    "references": [
      {
        "paperId": "22e71eb3f7f9a45821d24f4aecbf2c315ac9b9fc",
        "title": "Understanding Tool-Integrated Reasoning"
      },
      {
        "paperId": "02ebf3417763042ce1623dd81b51e1f9a595a923",
        "title": "CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse Prevention"
      },
      {
        "paperId": "78e03fb22a051c82bfa9e2051cd66245eba0f2dc",
        "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning"
      },
      {
        "paperId": "9d700ce6b18880a8cd64cac256c730a813ffc1c2",
        "title": "Geometric-Mean Policy Optimization"
      },
      {
        "paperId": "3c8b92a8b6eee1377077ecfea95e53223845af20",
        "title": "Agentic Reinforced Policy Optimization"
      },
      {
        "paperId": "a0b04def806f1d5d0126f98334b664cc57a42a0d",
        "title": "Group Sequence Policy Optimization"
      },
      {
        "paperId": "91360031e430be76c20c4b02f3da08c3052d54af",
        "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"
      },
      {
        "paperId": "d19124bc2bcfac3c6370a2630ad47f622bafe02e",
        "title": "Logit Dynamics in Softmax Policy Gradient Methods"
      },
      {
        "paperId": "a7933f44a76c680b074591deb74b444ab8c748e0",
        "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"
      },
      {
        "paperId": "ac1d5df9687455f405de44ef8190aa5321a99a5b",
        "title": "Towards Effective Code-Integrated Reasoning"
      },
      {
        "paperId": "10feab31bb9e71a0f1094fb00c0554abfb992c4d",
        "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models"
      },
      {
        "paperId": "532c7d150d7bfece96e3c51ad0c8e4e18271912c",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "paperId": "ea9591cf281409ffcbc284129d52050fea9cc115",
        "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay"
      },
      {
        "paperId": "f29c8c4bf20dea0ba57d91ba409d846123fb89c0",
        "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving"
      },
      {
        "paperId": "abeb46288d537f98f76b979040a547ee81216377",
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning"
      },
      {
        "paperId": "1d03586baa32b3d6ff657a180053821543e11abb",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "8402e446158252992b6ddf1ff1b0658c39d7604e",
        "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
      },
      {
        "paperId": "d981ce332586e6a29f595cbdfa9347cf425e5cd0",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
      },
      {
        "paperId": "1d0b3c90eb184ea9da554ea3381bdbf1e0f23629",
        "title": "ToRL: Scaling Tool-Integrated RL"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "1fcaafeb72142fe3d1a5d698a072d69778d244b0",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "paperId": "78f85a3a4e9d1b83ac33179c777e6eb2d756be82",
        "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "e9e135ca90fb30fe01c7af0a24a6dcf46bd459e8",
        "title": "Divergence-Augmented Policy Optimization"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "036373f17e5e47bcadc289e6c57d61cf5e08fe3d",
        "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions"
      },
      {
        "paperId": null,
        "title": "Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities"
      },
      {
        "paperId": null,
        "title": "Multi-Turn RL Training for CUDA Kernel Generation"
      },
      {
        "paperId": null,
        "title": "Your Efficient RL Framework Secretly Brings You Off-Policy RL Training"
      },
      {
        "paperId": null,
        "title": "DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL, 2025"
      }
    ],
    "cited_by": [
      {
        "paperId": "d7abde3b7864a96ccd781ab435192def3aec5e70",
        "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning"
      },
      {
        "paperId": "646eef2e25ae06feeb7e761cd34cd234fb2a4c80",
        "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning"
      },
      {
        "paperId": "a83190e397a525b02bf1fae6e225da3f4098794c",
        "title": "GEM: A Gym for Agentic LLMs"
      },
      {
        "paperId": "894e7fa610413b29a72d3b6997891de4edb5d331",
        "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL"
      },
      {
        "paperId": "8c8d74daa222061151dcf0bcdc7dbc2764951590",
        "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners"
      },
      {
        "paperId": "a300d633112e9636a54c3e5f9abc43cd54cf85d4",
        "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training"
      },
      {
        "paperId": "7da0852f60fb5abf7a7172b4419c19fc292ece9c",
        "title": "Scaling Generalist Data-Analytic Agents"
      },
      {
        "paperId": "a71f1f9e55f2089f6445e586b7f91e878e756095",
        "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution"
      },
      {
        "paperId": "d2712f40cd50c841d7d478c123c68fc9879931ed",
        "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "51357ff32df3924f29e550aaf71c9882c2b99348",
        "title": "Variational Reasoning for Language Models"
      },
      {
        "paperId": "df0a07beb75eea9c71ca70344726e5c822974c40",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards"
      },
      {
        "paperId": "e6bb640094cfc06bd4c76a1e36c97ceebdb48d14",
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search"
      },
      {
        "paperId": "8460a61d76933d2f2c83e78a6658f3468b5b71af",
        "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents"
      },
      {
        "paperId": "22e71eb3f7f9a45821d24f4aecbf2c315ac9b9fc",
        "title": "Understanding Tool-Integrated Reasoning"
      },
      {
        "paperId": "a46ea48b846f50e05267291cd881e9363baaf3c0",
        "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning"
      },
      {
        "paperId": "ae4f998428cc2e8ce9a61d13154730da2093b33d",
        "title": "PyVision: Agentic Vision with Dynamic Tooling"
      }
    ],
    "score": 17.0
  },
  {
    "id": "2044ab82dcb2c11ef660bd51d40130fe182f98d3",
    "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
    "authors": [
      "Zhiwei Tang",
      "Dmitry Rybin",
      "Tsung-Hui Chang"
    ],
    "year": 2023,
    "citationCount": 33,
    "abstract": "In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are available. Last but not least, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers a new and effective approach for aligning Artificial Intelligence (AI) with human intentions.",
    "url": "https://www.semanticscholar.org/paper/2044ab82dcb2c11ef660bd51d40130fe182f98d3",
    "pdf_url": "https://arxiv.org/pdf/2303.03751.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-03-07",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-03751",
      "ArXiv": "2303.03751",
      "DOI": "10.48550/arXiv.2303.03751",
      "CorpusId": 257378374
    },
    "references": [
      {
        "paperId": "05b984f0de65251399b694176470596d67cc94a8",
        "title": "TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation"
      },
      {
        "paperId": "b7a0cb002a7e95c3ea7bad5799c9e079f507acd4",
        "title": "Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis"
      },
      {
        "paperId": "4bf8a2b1c9366703dd515591b6d743c1b06338a1",
        "title": "Advanced Feature Manipulation for Enhanced Change Detection Leveraging Natural Language Models"
      },
      {
        "paperId": "415cd2a24c7d0a9830449bf2860d5a31728ff593",
        "title": "Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research"
      },
      {
        "paperId": "697a6daec01317c996558acf8dbfb445059f3e38",
        "title": "Fostc3net: A lightweight YOLOv5 based on the network structure optimization"
      },
      {
        "paperId": "5d571520d4e14d8d647f2e3d13f95edf234634cb",
        "title": "Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies"
      },
      {
        "paperId": "7b64ef6f5fa018a39df8597f2c853747b81706e3",
        "title": "Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques"
      },
      {
        "paperId": "0f224ba3870a3b5cc70bd8bab3bf2e0fa6949691",
        "title": "FedLion: Faster Adaptive Federated Optimization with Fewer Communication"
      },
      {
        "paperId": "a2279b88191ece34eb3b074d4ac7072b86c553ca",
        "title": "Accelerating Parallel Sampling of Diffusion Models"
      },
      {
        "paperId": "ce7a2ea8774b996e7022b3bd712c13b75365fc96",
        "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review"
      },
      {
        "paperId": "4cdb2fe14ac4d9d339564755f998d316e454f4db",
        "title": "News Recommendation with Attention Mechanism"
      },
      {
        "paperId": "4f5251868de0a0a38de00551ea4cb11992845951",
        "title": "Particle Filter SLAM for Vehicle Localization"
      },
      {
        "paperId": "5cca7ccaf51ae43380d07d419dd9a85bbdcd9517",
        "title": "DeepGI: An Automated Approach for Gastrointestinal Tract Segmentation in MRI Scans"
      },
      {
        "paperId": "146efd9ed4ce53d9da48d302f95a8f597bc22b8b",
        "title": "SwitchTab: Switched Autoencoders Are Effective Tabular Learners"
      },
      {
        "paperId": "2c40a40cbb4a86f0dc0d3036fd6f17c7f9e32129",
        "title": "Optimizing Science Question Ranking through Model and Retrieval-Augmented Generation"
      },
      {
        "paperId": "75e6176204eb6cd241410e07d3ab04d303b77b76",
        "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention"
      },
      {
        "paperId": "db1e76043592fb01c9f31feaea7555244e91cdb9",
        "title": "Interpreting Pretrained Language Models via Concept Bottlenecks"
      },
      {
        "paperId": "4c254fe4604a180b76ba113fa256c3c657de18b6",
        "title": "Embracing Uncertainty: A Diffusion Generative Model of Spectrum Efficiency in 5G Networks"
      },
      {
        "paperId": "4223149c61fca48beaa46ff7d51b958e202db058",
        "title": "ReConTab: Regularized Contrastive Representation Learning for Tabular Data"
      },
      {
        "paperId": "f1aa892eb6c00bf83c94f86abe7a59f0e3f1549e",
        "title": "Learning Multiscale Consistency for Self-Supervised Electron Microscopy Instance Segmentation"
      },
      {
        "paperId": "e4561cf0cee2139b7c9085822a4899ce6a5aace3",
        "title": "Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review"
      },
      {
        "paperId": "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723",
        "title": "Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8",
        "title": "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation"
      },
      {
        "paperId": "635e5a007dd2e31503c9a5b0668f44fd6b10c767",
        "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "0c26bfc15a7caecce0ed4567dc2f2909b80e5bdd",
        "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery"
      },
      {
        "paperId": "29660f98f83d3c3c17c9e08e423ca76db58c0dbd",
        "title": "z-SignFedAvg: A Unified Stochastic Sign-based Compression for Federated Learning"
      },
      {
        "paperId": "4530c25da949bb2185c50663158ef19d52e3c6b5",
        "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "09188a72a2a3b05f0937061ac487cb052ac5a62f",
        "title": "Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "paperId": "4ddc726e2d33c8f5b2876c53fe18b0d373e10a3c",
        "title": "Differentially Private Federated Bayesian Optimization with Distributed Exploration"
      },
      {
        "paperId": "f9a79b745b9e73d843f177af2d5ef900d20dc9ff",
        "title": "Low-rank Matrix Recovery With Unknown Correspondence"
      },
      {
        "paperId": "5229800fdbe094ba62f64715b0ec4a7a270cb943",
        "title": "Christine P. Chai's contribution to the Discussion of \u2018Gaussian Differential Privacy\u2019 by Dong et al."
      },
      {
        "paperId": "9c45c6e9d8178637810c67162911449b3859d000",
        "title": "Oneshot Differentially Private Top-k Selection"
      },
      {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations"
      },
      {
        "paperId": "6fcf884343bfac0d4680133b51c558c6a4e0bf35",
        "title": "Local Differential Privacy for Bayesian Optimization"
      },
      {
        "paperId": "014576b866078524286802b1d0e18628520aa886",
        "title": "Denoising Diffusion Implicit Models"
      },
      {
        "paperId": "480b25bf2af8f0f4d21e38d3385348959e3dbbff",
        "title": "A One-bit, Comparison-Based Gradient Estimator"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "2d043ce5e1c5fc2ac437ea236ea908383a8ca05d",
        "title": "Optimal binomial reliability demonstration tests design under acceptance decision uncertainty"
      },
      {
        "paperId": "d214bd3c7236fe881d55d3a40982680523b24ad5",
        "title": "Locally Differentially Private (Contextual) Bandits Learning"
      },
      {
        "paperId": "93b982385d61793e4859728e58d04f0ca6b9f48f",
        "title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization"
      },
      {
        "paperId": "be15affe70b4a44dea9db48b7d2f009ce30c2cd7",
        "title": "Multi-attribute Bayesian optimization with interactive preference learning"
      },
      {
        "paperId": "27a1069a1bf0dc932360827812ea15bc6ab93595",
        "title": "SEGA: Variance Reduction via Gradient Sketching"
      },
      {
        "paperId": "4f2c8c626a12a3ba5c088a5cfeddd3eafadbb502",
        "title": "Relation Classification Using Coarse and Fine-Grained Networks with SDP Supervised Key Words Selection"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "c27078d60737ea10e8ca4f05acd114fef29c8276",
        "title": "A Tutorial on Bayesian Optimization"
      },
      {
        "paperId": "7b54df5fff791cb4212fff31d83cfdd401027e6a",
        "title": "Regret Analysis for Continuous Dueling Bandit"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "892f9a2f69241feec647856cd26bed37e04fd747",
        "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization"
      },
      {
        "paperId": "417f02dfc599699c94e732f3600d559d8e41fa17",
        "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks"
      },
      {
        "paperId": "21a0b0fbdde1aee56fe10e69e897decaf21f43a6",
        "title": "Random Gradient-Free Minimization of Convex Functions"
      },
      {
        "paperId": "0023582fde36430c7e3ae81611a14e558c8f4bae",
        "title": "The Algorithmic Foundations of Differential Privacy"
      },
      {
        "paperId": "d7440b0840c9a1aace59884e73d21250bec096e5",
        "title": "Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "1a8e11d93160148031b9aad6306ca9fbd4c66fec",
        "title": "Robust 1-bit Compressed Sensing and Sparse Logistic Regression: A Convex Programming Approach"
      },
      {
        "paperId": "035bd1607d664b0c10143ca055f1c645b7d04a11",
        "title": "Interactively optimizing information retrieval systems as a dueling bandits problem"
      },
      {
        "paperId": "c514310fe4550f947f2aaaef88e56610daa5047a",
        "title": "Direct search algorithms for optimization calculations"
      },
      {
        "paperId": "0afb29e1a7bf4c50e58078b5051a220afedbc0dc",
        "title": "Decisions with Multiple Objectives: Preferences and Value Trade-Offs"
      },
      {
        "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
        "title": "Large Language Models for Data Annotation: A Survey"
      },
      {
        "paperId": null,
        "title": "Languages are rewards: Hindsight finetuning using human feedback, 2023"
      },
      {
        "paperId": "ed7dfafe73050f7d04f67c78a7d77846c6364684",
        "title": "Fuzzy Controller-Based Design and Simulation of an Automatic Parking System"
      },
      {
        "paperId": "dc304ccdc46b26297ca8cd9784a1ef5ba32be021",
        "title": "Breast Cancer Prediction Based on Machine Learning"
      },
      {
        "paperId": null,
        "title": "$z$-signfedavg: A unified sign-based stochastic compression for federated learning. In Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS"
      },
      {
        "paperId": null,
        "title": "Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI"
      },
      {
        "paperId": null,
        "title": "CMA-ES/pycma on Github"
      },
      {
        "paperId": "56b37563b81095e026f7c7b6adee0cfb15625828",
        "title": "Preference Based Adaptation for Learning Objectives"
      },
      {
        "paperId": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
        "title": "A Simplex Method for Function Minimization"
      },
      {
        "paperId": null,
        "title": "Exit \u2026\u2026 Figure 10: The User Interface of Algorithm 3"
      },
      {
        "paperId": null,
        "title": "A promising alternative to RLHF"
      },
      {
        "paperId": "810b9ffea4c74db3923336a22dc9563679cfe564",
        "title": "Conference Paper"
      },
      {
        "paperId": null,
        "title": "First rank-based zeroth-order optimization algorithm with theoretical guarantee"
      },
      {
        "paperId": null,
        "title": "Smartfix: Leveraging machine learning for proactive equipment maintenance in industry 4.0"
      },
      {
        "paperId": null,
        "title": "Precision calibration of industrial 3d scanners: An ai-enhanced approach for improved measurement accuracy"
      },
      {
        "paperId": null,
        "title": "Round 14: Please input the ID of best image -> 6"
      }
    ],
    "cited_by": [
      {
        "paperId": "a3d6d04015b14dd1da71b717983f019960a7e996",
        "title": "Instant Preference Alignment for Text-to-Image Diffusion Models"
      },
      {
        "paperId": "54cd3dd909ccbc9452004d0fb4ddd9f19e831075",
        "title": "Robust Network Optimization by Deep Generative Models and Stochastic Optimization"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "d08d1308a62200b6ad4c7627e50a3bbdd06ea863",
        "title": "Prompt-Tuning Bandits: Enabling Few-Shot Generalization for Efficient Multi-Task Offline RL"
      },
      {
        "paperId": "a650671e202572a9e9eaf6d68374ab56e39fd4b0",
        "title": "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth- and First-Order Optimization"
      },
      {
        "paperId": "f90462fcffb5a594a76d8a81df88be7762e0e38e",
        "title": "Linear Convergence Rate in Convex Setup is Possible! Gradient Descent Method Variants under $(L_0,L_1)$-Smoothness"
      },
      {
        "paperId": "08e16c8ac9e36767404814076ba647264227e0d5",
        "title": "Preference Adaptive and Sequential Text-to-Image Generation"
      },
      {
        "paperId": "e1c4c0e20b0d2d33517ea81a83825485942d72a7",
        "title": "Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy Generalization with Global and Adaptive Guidance"
      },
      {
        "paperId": "bea0dc30a3ab3325b14283dde7c931c6a59b14f8",
        "title": "Ruppert-Polyak averaging for Stochastic Order Oracle"
      },
      {
        "paperId": "f27eaf4ae5563f92916d49a82377ba7fce2aed02",
        "title": "Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "47a7c3b07116e0e185d8aa214923e891df856b2a",
        "title": "Nesterov's method of dichotomy via Order Oracle: The problem of optimizing a two-variable function on a square"
      },
      {
        "paperId": "44d48f641f4e0c3ab66f569200fa57fd45a4f416",
        "title": "ViPer: Visual Personalization of Generative Models via Individual Preference Learning"
      },
      {
        "paperId": "1460d5b0d75d82f700aad6d3332abb9770bcde8f",
        "title": "CoCoG-2: Controllable generation of visual stimuli for understanding human concept representation"
      },
      {
        "paperId": "8e789824bc853b840a61497c814a1ca1662bad07",
        "title": "Inference-Time Alignment of Diffusion Models with Direct Noise Optimization"
      },
      {
        "paperId": "e31231371d16ac2fc2d242494cd354721e42c4e4",
        "title": "Comparisons Are All You Need for Optimizing Smooth Functions"
      },
      {
        "paperId": "870cb09baccd71a7ad8c0f9da458b707d3eb00e6",
        "title": "CoCoG: Controllable Visual Stimuli Generation based on Human Concept Representations"
      },
      {
        "paperId": "f7f71bad41e6b76c455dd7edf333c3831e766bcf",
        "title": "Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation"
      },
      {
        "paperId": "2fadd8122d115d7f50128bf90717b9216a98f47c",
        "title": "Deep Representation Learning for Multi-functional Degradation Modeling of Community-dwelling Aging Population"
      },
      {
        "paperId": "a2279b88191ece34eb3b074d4ac7072b86c553ca",
        "title": "Accelerating Parallel Sampling of Diffusion Models"
      },
      {
        "paperId": "cefd98cea25123e217184fbbd6081b5640d3ca18",
        "title": "Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values"
      },
      {
        "paperId": "f26cc29508f0c3705e592732a4f0c87531319f28",
        "title": "Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example"
      },
      {
        "paperId": "4be7b6f542904e1105837179e99c2c88b72c4da5",
        "title": "A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model"
      },
      {
        "paperId": "44d319e3ecd0a5949a4b7c9c5de794f28f5aa7e4",
        "title": "HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback"
      },
      {
        "paperId": "ac7fbb07e04810daa3e54ab7bae62493afee9d5a",
        "title": "Toward 6G Native-AI Network: Foundation Model-Based Cloud-Edge-End Collaboration Framework"
      },
      {
        "paperId": "18cdbce523c9d36fa147d9fb06a676109dd8697d",
        "title": "Optimizing Algorithms from Pairwise User Preferences"
      },
      {
        "paperId": "9e9059a25ca53947e247c35347bac5bb9bf90a1e",
        "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback"
      },
      {
        "paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41",
        "title": "Fine-Tuning Language Models with Just Forward Passes"
      },
      {
        "paperId": "231d6947a0651d3449020b728dd42c46cde81cda",
        "title": "Prompt-Tuning Decision Transformer with Preference Ranking"
      },
      {
        "paperId": "fd2f8bae1fd3d7e5c14c8df823a0017921e761b5",
        "title": "Confidence Trigger Detection: Accelerating Real-Time Tracking-by-Detection Systems"
      },
      {
        "paperId": "e3064585104120ba0e2285deaadb7b8a9e049186",
        "title": "Boosting Text-to-Video Generative Model with MLLMs Feedback"
      },
      {
        "paperId": "e56754ab96aebf822f024b7880b86e00fe27b3af",
        "title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better"
      }
    ],
    "score": 16.5
  },
  {
    "id": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
    "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
    "authors": [
      "Chen Zheng",
      "Ke Sun",
      "Hang Wu",
      "Chenguang Xi",
      "Xun Zhou"
    ],
    "year": 2024,
    "citationCount": 15,
    "abstract": "In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding and generation of responses, such as customer service. We applied this methodology to Mistral, the most popular base model, thereby creating Mistral-Plus. Our validation across 11 general tasks demonstrates that Mistral-Plus outperforms similarly sized open-source base models and their corresponding instruct versions. Importantly, the conversational abilities of Mistral-Plus were significantly improved, indicating a substantial advancement over traditional SFT models in both safety and user preference alignment.",
    "url": "https://www.semanticscholar.org/paper/9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
    "pdf_url": "https://arxiv.org/pdf/2403.02513.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-04",
    "externalIds": {
      "ArXiv": "2403.02513",
      "DBLP": "journals/corr/abs-2403-02513",
      "DOI": "10.48550/arXiv.2403.02513",
      "CorpusId": 268248820
    },
    "references": [
      {
        "paperId": "f977dac98cc603bfccae6ea991cf4b1f83bf139c",
        "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "798ece3c5491f613e5368bd2d818476a64b88905",
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers"
      },
      {
        "paperId": "26b2adbe089ea36617c3ec0aa009319929da0550",
        "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity"
      },
      {
        "paperId": "3cea9013b9b8383c673dd992942a248646743e09",
        "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "10145a22ebfee898be909ad44f83bd3c490adb53",
        "title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy"
      },
      {
        "paperId": "532430bfcedf0ca4d5ca695967b52fc21cb5b778",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "8826311d922135dbf0cfdb4a661ebab347e3b826",
        "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations"
      },
      {
        "paperId": "68c834c19cd126bbd6d25a3572d7205cfed76271",
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"
      },
      {
        "paperId": "e76368389e5448ec4b82fe8c67f3490874ed7c4e",
        "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502",
        "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "518ca6f8fe502fbacae88937b694e8a4ef0a95be",
        "title": "Latent Hatred: A Benchmark for Understanding Implicit Hate Speech"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
      },
      {
        "paperId": null,
        "title": "gpt: The logit lens"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2023. Contrastive prefence learning: Learning from human feedback without rl"
      },
      {
        "paperId": null,
        "title": "\u201cThe thing I\u2019ve always said about Bohannon, from a place of authority [is] that Bohannon doesn\u2019t give a 21 In 2007, he made waves when he called the just-released Wii a \"piece of"
      },
      {
        "paperId": null,
        "title": "\u201cSo...yes we use MSG, we don\u2019t believe in organic food, and we don\u2019t give a"
      },
      {
        "paperId": null,
        "title": "Psychobabble Melody Maker Dave can\u2019t bear to watch old people snog and there are a few people he\u2019d like to kick the"
      },
      {
        "paperId": null,
        "title": "A lot of these comments remind me of people being incredulous over people not giving a"
      },
      {
        "paperId": null,
        "title": "This is a battlefield for her right now, and she\u2019s trying to emphasize that actions have consequences, that if you cross her, then by God, she will shoot the"
      },
      {
        "paperId": null,
        "title": "If you make a beeline straight for the door you can escape the room, and Neeson won\u2019t give a"
      },
      {
        "paperId": null,
        "title": "2023. Qwen technical report"
      },
      {
        "paperId": null,
        "title": "2023. Regular-ized rl"
      },
      {
        "paperId": null,
        "title": "\u201cAt first I thought he was just some new kind of evil, but he\u2019s just a lying sack"
      },
      {
        "paperId": null,
        "title": "Vince leaves a music video shoot and gets in an incongruously shabby old cab, which is almost always a sign some weird"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "someone sum it up as succinctly as Will Toledo does: \u201cLast Friday, I took acid and mushrooms/I did not transcend, I felt like a walking piece"
      },
      {
        "paperId": null,
        "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "The Toronto Blue Jays organization apparently doesn\u2019t give a"
      },
      {
        "paperId": null,
        "title": "2023. Unveiling the implicit toxicity in large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "942ebb5a0a9a906a668f92b574e5d61c714685db",
        "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs"
      },
      {
        "paperId": "9efb2a49f3e16671fa160dc0c8d88f54755971ff",
        "title": "A Dynamic Fusion Model for Consistent Crisis Response"
      },
      {
        "paperId": "7bb7a0d948fd97ac617de6d22d941590190c3f70",
        "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models"
      },
      {
        "paperId": "70aecaa608b6fabfb9376ab89119911bd472b951",
        "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models"
      },
      {
        "paperId": "0965ed7216a26d9b5f39c913276a949caa1f2b77",
        "title": "Chatbot Based on Large Language Model to Improve Adherence to Exercise-Based Treatment in People with Knee Osteoarthritis: System Development"
      },
      {
        "paperId": "68bf7eb60d2c1db80c9b37660d5c10bcdea83794",
        "title": "Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning"
      },
      {
        "paperId": "bc6016c22b3ccdc838adaf0963d04bdc4ec654b4",
        "title": "Controlling Large Language Model with Latent Actions"
      },
      {
        "paperId": "c84cad456ad539fa2765cf39f41090d15f582a8c",
        "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization"
      },
      {
        "paperId": "3bc75de4ee6a437291c0a617fd2af8261f5df752",
        "title": "LLM Misalignment via Adversarial RLHF Platforms"
      },
      {
        "paperId": "49e2b87e1f46347cfd68b81c01c439090654cdbd",
        "title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach"
      },
      {
        "paperId": "e444b714c7551469585e999c401f8f769d9572b6",
        "title": "Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs"
      },
      {
        "paperId": "57451ce18f3035fcadf64db38420434f9299b7f3",
        "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF"
      },
      {
        "paperId": "d868a185c5d6ffda30583a714bb703461de797dd",
        "title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation"
      },
      {
        "paperId": "d5c4bdc54f974550dce55fc5d40a2e828aafcdf3",
        "title": "Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation"
      },
      {
        "paperId": "dd4b5f6209cbf20e72473da749215bbbebdca633",
        "title": "ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation"
      }
    ],
    "score": 15.0
  },
  {
    "id": "a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
    "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective",
    "authors": [
      "Yiqun Zhang",
      "Xiaocui Yang",
      "Xingle Xu",
      "Zeran Gao",
      "Yijie Huang",
      "Shiyi Mu",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang",
      "Kaisong Song",
      "Ge Yu"
    ],
    "year": 2024,
    "citationCount": 15,
    "abstract": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.",
    "url": "https://www.semanticscholar.org/paper/a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
    "pdf_url": "https://arxiv.org/pdf/2408.04638.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-07-30",
    "externalIds": {
      "DBLP": "journals/corr/abs-2408-04638",
      "ArXiv": "2408.04638",
      "DOI": "10.48550/arXiv.2408.04638",
      "CorpusId": 271843516
    },
    "references": [
      {
        "paperId": "799f40dbcdf4ec9234edc56be178b745b02bef29",
        "title": "MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding"
      },
      {
        "paperId": "7da361cbbf5e0c10e6eec89f8e8e1618a6778b84",
        "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents"
      },
      {
        "paperId": "6aa55900c2bcf1860938520bdf1952f462ed37a1",
        "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs"
      },
      {
        "paperId": "8e311473449cdb4f6d5545a35cd60efb92c7ead8",
        "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation"
      },
      {
        "paperId": "d686f3c2284ba5fbbe0c7ef7579b97b4975ea0db",
        "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors"
      },
      {
        "paperId": "8bc5850b52de2668d85eb34822331bd064c1da1f",
        "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization"
      },
      {
        "paperId": "cb961c1f1808b58e04ac022b5f2d41238136f6ce",
        "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment"
      },
      {
        "paperId": "71aa4578fb712bfa22afaf36ad658c6d69652cbe",
        "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models"
      },
      {
        "paperId": "18dc78d3f247f75aafca5422fe540f20b3cd455d",
        "title": "Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "6ec0c63afe3fbc28959ec67815346579260f95f1",
        "title": "GENTEEL-NEGOTIATOR: LLM-Enhanced Mixture-of-Expert-Based Reinforcement Learning Approach for Polite Negotiation Dialogue"
      },
      {
        "paperId": "c763e59f1ed0a62657b4cbfb21cd1e4fafbe353a",
        "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?"
      },
      {
        "paperId": "35c639be8599d72834a76b89746eba8fa2544d28",
        "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning"
      },
      {
        "paperId": "dc1965a88f9e2f8b03d277872404d1cd0981b697",
        "title": "Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter"
      },
      {
        "paperId": "0152aafbec465a090684637e1da693d6deb98172",
        "title": "RLTHF: Targeted Human Feedback for LLM Alignment"
      },
      {
        "paperId": "f1cd31132081f2fa89a54c3e5605ecba27cb6bdf",
        "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models"
      },
      {
        "paperId": "26ea226ea46801926f0d82c081956e2d89481fa4",
        "title": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression"
      },
      {
        "paperId": "a769eb0e1170fddabe763d43be351dfaacd6c1de",
        "title": "TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM"
      },
      {
        "paperId": "d2b760eaa3a50f23f08f725eda25a42a1c6d2eca",
        "title": "A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "fe68b7829b7a7e77beb4fefd8f81211751d1a41c",
        "title": "Financial News-Driven LLM Reinforcement Learning for Portfolio Management"
      },
      {
        "paperId": "47a99720e0d5c22a75b10e2ad6a0d93e4466e26b",
        "title": "Generative Emotion Cause Explanation in Multimodal Conversations"
      },
      {
        "paperId": "d36e19b38ffb6ddcd3c42f15c8b2b90bd4813fa8",
        "title": "Observe before Generate: Emotion-Cause aware Video Caption for Multimodal Emotion Cause Generation in Conversations"
      },
      {
        "paperId": "feab5206295c4f7203ea4b83c6f3e106f6dc1b19",
        "title": "AlignCap: Aligning Speech Emotion Captioning to Human Preferences"
      },
      {
        "paperId": "191965af25a4ff05b3aef29441d54973787ef808",
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions"
      },
      {
        "paperId": "16a9d32ae017715296a1d842dc362fe227154fec",
        "title": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models"
      },
      {
        "paperId": "2595a7871029e53cc00d7d757bba9ca30572758e",
        "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework"
      },
      {
        "paperId": "9f23050dc0f6b7d3d3594c64ac2c5b505ce24eaa",
        "title": "Applying Reinforcement Learning and Multi-Generators for Stage Transition in an Emotional Support Dialogue System"
      },
      {
        "paperId": "3c42a068bd9126ae7ceacb6c89a3ae8f89ca4a08",
        "title": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of LLMs"
      },
      {
        "paperId": "0bd9873a6f11dc57f9ab4c7d18684b82ba45ba15",
        "title": "NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations"
      },
      {
        "paperId": "8a42e1437e003a18f56862b8a9a3fe2f1f0a8d3b",
        "title": "Cause-Aware Empathetic Response Generation via Chain-of-Thought Fine-Tuning"
      },
      {
        "paperId": "3c6b3e028a62c9d7954e08f7a95f3504f273599a",
        "title": "EMO-LLaMA: Enhancing Facial Emotion Understanding with Instruction Tuning"
      },
      {
        "paperId": "cde26061ac576bee84ea87525797b2f1ac8c07a9",
        "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation"
      },
      {
        "paperId": "3c62d53caf811cfd2a2a15485fe47300e58a282a",
        "title": "Self-Emotion Blended Dialogue Generation in Social Simulation Agents"
      },
      {
        "paperId": "5ed5b3459c1b1c4f9b8edcd1e3a87809e2f2ef41",
        "title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances"
      },
      {
        "paperId": "7dde3e2920a4468cfecbebad8e20701d35065e2d",
        "title": "Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion Prediction for Social Media Influencers"
      },
      {
        "paperId": "84bc1b59758f8e4bd0534fe836a8a73bae992578",
        "title": "APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies for Empathetic Response Generation"
      },
      {
        "paperId": "a5eb43e354dea76f839cc3fbc140197d352763e7",
        "title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset"
      },
      {
        "paperId": "b35732ab271c3bb6d4709c358fe68c2cf9621e9a",
        "title": "An empirical study of Multimodal Entity-Based Sentiment Analysis with ChatGPT: Improving in-context learning via entity-aware contrastive learning"
      },
      {
        "paperId": "134bddbb588d3758358583587483c3cd479275cf",
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models"
      },
      {
        "paperId": "fbb24ddfc8a19cc11e635afa74b5104c56774d81",
        "title": "LLM-Driven Multimodal Opinion Expression Identification"
      },
      {
        "paperId": "c7f9706898bdfa3241601e075b1305649b174ff1",
        "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
      },
      {
        "paperId": "5889a56c5a006e66b97ef55a47b88bd7509e8f39",
        "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction"
      },
      {
        "paperId": "5c16e5224890dcb1aeeabe0cd4a373de72561c13",
        "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning"
      },
      {
        "paperId": "eeed21017b19fde7f29294be8e72cc7e59b9ee94",
        "title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models"
      },
      {
        "paperId": "9cde9edac51e3669f25c01757f3c313fa63b0cd1",
        "title": "It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance"
      },
      {
        "paperId": "6f38dc421b3f42eb737905262c867c6bcf6a77a4",
        "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs"
      },
      {
        "paperId": "17455350c5e541e66284765e5463293ce77d790d",
        "title": "SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations"
      },
      {
        "paperId": "ad2717e1a9746d89bf0776d879514ecae7412a49",
        "title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains"
      },
      {
        "paperId": "46ffdb8058906d481117a5f17f42475a435e9dae",
        "title": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks"
      },
      {
        "paperId": "7a8a753359a42985690e01b1f061b5048e396474",
        "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis"
      },
      {
        "paperId": "c210049fdc4850288682367164593beb91889b42",
        "title": "Empathy Through Multimodality in Conversational Interfaces"
      },
      {
        "paperId": "8a42e384c414bdd041d65e9044c8fa49626dc407",
        "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning"
      },
      {
        "paperId": "ebcf108f8bc42140721ff02b6727b0a291362957",
        "title": "MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts"
      },
      {
        "paperId": "734cbb8818639c2fb9a79c517ca7e2beece0048e",
        "title": "Large Language Models Performance Comparison of Emotion and Sentiment Classification"
      },
      {
        "paperId": "bc2ad540cb03cd513f96be3bdb81b5f296a4b09b",
        "title": "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis"
      },
      {
        "paperId": "afc8a21d32d2f49130c669eb95cdc847a102116f",
        "title": "RL-EMO: A Reinforcement Learning Framework for Multimodal Emotion Recognition"
      },
      {
        "paperId": "ad43573b4cb3ef443e5acc1af1b70fd02ba0e9b6",
        "title": "Large Language Model-Based Emotional Speech Annotation Using Context and Acoustic Feature for Speech Emotion Recognition"
      },
      {
        "paperId": "b810f3a5e49fb23c789015f756d35aef21f924ad",
        "title": "On Prompt Sensitivity of ChatGPT in Affective Computing"
      },
      {
        "paperId": "dd8e0baf97a805c9de242f32c9c2f42bbb863aeb",
        "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation"
      },
      {
        "paperId": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
        "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution"
      },
      {
        "paperId": "c713ade0d479355c8e155ca720fbe36c34e9e2f6",
        "title": "Sentiment Analysis in the Age of Generative AI"
      },
      {
        "paperId": "5c190ab7133a38aacb49a7434f575039c53802de",
        "title": "Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy"
      },
      {
        "paperId": "4c5d604add6178acef3e7ad71995608f96f01dad",
        "title": "A Sentiment Consolidation Framework for Meta-Review Generation"
      },
      {
        "paperId": "288338c1ec81b259af5587677fdd10d5191baf71",
        "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models"
      },
      {
        "paperId": "eb7a04c2f4ca8236cd67c26c8181c65c1f65e48c",
        "title": "Is ChatGPT More Empathetic than Humans?"
      },
      {
        "paperId": "16e989b9094c3653972c82b10b7004b6f0b42927",
        "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation"
      },
      {
        "paperId": "c4d76cfb9bf2ffcdd38739b41840141507bee0d3",
        "title": "CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation"
      },
      {
        "paperId": "75264034416e5c2f552efb5783624d240e0ea4f6",
        "title": "Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models"
      },
      {
        "paperId": "3cdc18cf052823b535796a7cd01be51048ab4b4a",
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models"
      },
      {
        "paperId": "4d4d9da4f2c39089ea6c8d84e5031c195548d7b6",
        "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence"
      },
      {
        "paperId": "595b5fe62927cce40da5e295b9fe61a2fc52f23c",
        "title": "Can Generative Agents Predict Emotion?"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "7dd900abf8ed87dcaa65eeead3e8251d5c43eab9",
        "title": "Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction"
      },
      {
        "paperId": "f58484d50364014289c0d4fe20ba35962fb40ba9",
        "title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch"
      },
      {
        "paperId": "b078e62296e646fa08b39f3d980a652a37f5dc7b",
        "title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis"
      },
      {
        "paperId": "1386c679fc8c86526d89a02f0498909a3555fe42",
        "title": "Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality Assurance"
      },
      {
        "paperId": "d13dc7f94991e4cf7fba70c340b1d9d36346f238",
        "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought"
      },
      {
        "paperId": "fe91e1d22537bd514e9b642bb3b966f74fd62f9b",
        "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge"
      },
      {
        "paperId": "be4468df16aacad6dbb74f1d98ada26ddbd7dba5",
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis"
      },
      {
        "paperId": "2ffc9e530f4efff9e9f07997fa13535c3fef3e3c",
        "title": "MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition"
      },
      {
        "paperId": "a5030fd5521c184e138a6b0674e377cdc6fe2842",
        "title": "Prompt Consistency for Multi-Label Textual Emotion Detection"
      },
      {
        "paperId": "6947f594b7585ce7ddc8f0884e67c616bdbf3daa",
        "title": "Empirical Validation of an Agent-Based Model of Emotion Contagion"
      },
      {
        "paperId": "b9c1d0a8d0a0beca3756f511f952e51b06cb5cec",
        "title": "Sentiment and Interest Detection in Social Media using GPT-based Large Language Models"
      },
      {
        "paperId": "c90d79c3b003c72cc2b98227e7b99fd82486b1f4",
        "title": "COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal"
      },
      {
        "paperId": "179d65f6c11f4db12ea8d1eaf02a23bc91b6bdff",
        "title": "Affective Computing: Recent Advances, Challenges, and Future Trends"
      },
      {
        "paperId": "6933570be05269a2ccf437fbcca860856ed93659",
        "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models"
      },
      {
        "paperId": "e775684d28e92be78704df7cb82fc64cf9b2538c",
        "title": "GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition"
      },
      {
        "paperId": "3c1fcf084b94e3ebfa63a5cc6f8e9563675de484",
        "title": "Prompt Your Mind: Refine Personalized Text Prompts within Your Mind"
      },
      {
        "paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9",
        "title": "mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"
      },
      {
        "paperId": "eaa70e42a10364ffe87e84656df435cd58fb430e",
        "title": "Sentiment Analysis through LLM Negotiations"
      },
      {
        "paperId": "3f49bd6b95e254d82ce1ed1bd556a7a8f81a47db",
        "title": "Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models"
      },
      {
        "paperId": "debe978e02b664fb7254b6c7b58a74c09ce897e3",
        "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents"
      },
      {
        "paperId": "96c2824547ff669653e8d058da49dbc8c71ad9c9",
        "title": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations"
      },
      {
        "paperId": "b62bf6fcf87a2c47ebbdaeb8c0f42883cd9cb9c4",
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language"
      },
      {
        "paperId": "f3939b319c98a2c2ab12385017934aaf8f1e0da3",
        "title": "Customising General Large Language Models for Specialised Emotion Recognition Tasks"
      },
      {
        "paperId": "affcda0a52e6b37535e8ba2c7ac05f5ec94bb1ac",
        "title": "Bias in Emotion Recognition with ChatGPT"
      },
      {
        "paperId": "19e3e729e7521c9c67b3c7b7481dbea65d8a4158",
        "title": "Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models"
      },
      {
        "paperId": "0d7f24578340aae6df610ed95aaa276b9c3ddcd3",
        "title": "VeRA: Vector-based Random Matrix Adaptation"
      },
      {
        "paperId": "1b860394dfec26d9c350889006e37fe56731f77e",
        "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models"
      },
      {
        "paperId": "f8fa697302ef9d759b72404f5a8aa918865abdf4",
        "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions"
      },
      {
        "paperId": "3f9d42ca6e815333680c2076373893d9e00a1587",
        "title": "The Six Emotional Dimension (6DE) Model: A Multidimensional Approach to Analyzing Human Emotions and Unlocking the Potential of Emotionally Intelligent Artificial Intelligence (AI) via Large Language Models (LLM)"
      },
      {
        "paperId": "b9ff739affc3c8644380f00c1ffddc1678209123",
        "title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification"
      },
      {
        "paperId": "147763dbb9b9b0363ed0479c4cfa5844c537c344",
        "title": "USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset"
      },
      {
        "paperId": "fb7f6be7feac7c9d04da18caf0930bec7b1ac09b",
        "title": "Enhance Multi-Domain Sentiment Analysis of Review Texts Through Prompting Strategies"
      },
      {
        "paperId": "6bcb8bee2afd72a09b48268a07ba4209c5aa3842",
        "title": "UniSA: Unified Generative Framework for Sentiment Analysis"
      },
      {
        "paperId": "e52b9374505e1fd4eced2e694b3e97a9c4431603",
        "title": "A Wide Evaluation of ChatGPT on Affective Computing Tasks"
      },
      {
        "paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
        "title": "Instruction Tuning for Large Language Models: A Survey"
      },
      {
        "paperId": "f77c114553bc851aab4fff535570a9e83a18227e",
        "title": "Building Emotional Support Chatbots in the Era of LLMs"
      },
      {
        "paperId": "ef5cd0eb266e3df3eb64aec18e1854fe0244d228",
        "title": "Emotion-Conditioned Text Generation through Automatic Prompt Optimization"
      },
      {
        "paperId": "ebbffe5db352a10fde868843b8d5787b87843f09",
        "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
        "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection"
      },
      {
        "paperId": "2421734fb9b3f8dfd08c9e078da20b03c406d222",
        "title": "Explainable Multimodal Emotion Recognition"
      },
      {
        "paperId": "ae5e38058e9d622666254fa873c35a449a7bb2e1",
        "title": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "94d96e6bbffa0d4c877f8ba49b706286f6366f2a",
        "title": "ChatGPT outperforms humans in emotional awareness evaluations"
      },
      {
        "paperId": "c589ddc6c6fb07189af7c1212f6eb15c5ff72cde",
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
        "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration"
      },
      {
        "paperId": "d0c69c309fbf1233b6351cd57484557c16f28427",
        "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs"
      },
      {
        "paperId": "fb0474569c14a4af8dc94f231dc6074fb5bec63a",
        "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting"
      },
      {
        "paperId": "0387097cbadc0689371af3ba33dda464903b4364",
        "title": "A Comprehensive Survey on Affective Computing: Challenges, Trends, Applications, and Future Directions"
      },
      {
        "paperId": "ed2221b2260169acf5fe962cf757e46082f85bbf",
        "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting"
      },
      {
        "paperId": "db088514d8e2fc90d99ecc094fa7580dc524ca91",
        "title": "MEDIC: A Multimodal Empathy Dataset in Counseling"
      },
      {
        "paperId": "e4bf034670934c6b99bcc8dfcee75e9f5701c3fe",
        "title": "LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models"
      },
      {
        "paperId": "616597b6c8cc3d24339d9f16bb4b195624046abe",
        "title": "Is ChatGPT Equipped with Emotional Dialogue Capabilities?"
      },
      {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning"
      },
      {
        "paperId": "1aeb3239735e28c7318af096044e48d919ea500b",
        "title": "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study"
      },
      {
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior"
      },
      {
        "paperId": "9760874f2898842509650e0a88c0123fe6b00578",
        "title": "A Survey of Sentiment Analysis: Approaches, Datasets, and Future Research"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "b612fc6af23cccf2133c2ea40597453ab40dc2c3",
        "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "e3bc7f2bbe6e6c27be6132591d53df308f16ab97",
        "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT"
      },
      {
        "paperId": "fee0d9aaa31cb7e099b397f0d8f4a92667be2a0b",
        "title": "Multimodal Sentiment Analysis: A Survey of Methods, Trends, and Challenges"
      },
      {
        "paperId": "cccb79b8ea9d19d963eaba2042963be8a3e7d196",
        "title": "A review on sentiment analysis from social media platforms"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "08b85bce712168998004ee80ce4e475390413c74",
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"
      },
      {
        "paperId": "3669f076502a6da23dad96e7583dfe72addaa2a0",
        "title": "InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis"
      },
      {
        "paperId": "7a7ae511ef697a06d5e4e53ff59374086a3c1369",
        "title": "Sarcasm detection using news headlines dataset"
      },
      {
        "paperId": "52bd9d6389ee9e04adff1d9f006bf6e2f9ad1867",
        "title": "Survey on sentiment analysis: evolution of research methods and topics"
      },
      {
        "paperId": "ccbbfbc74a751addbbe8f14abd44fc943dcda57b",
        "title": "The Acoustically Emotion-Aware Conversational Agent With Speech Emotion Recognition and Empathetic Responses"
      },
      {
        "paperId": "bff499d51b002fd0b1aa05ba151a4a515e5bf36f",
        "title": "Emotional intelligence of Large Language Models"
      },
      {
        "paperId": "e6d7caa77eae7b64f78359669f4dd48fd2d177a1",
        "title": "Prompted Opinion Summarization with GPT-3.5"
      },
      {
        "paperId": "bd982b3628977709ec0fc9ae5756417209ed803c",
        "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition"
      },
      {
        "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
        "title": "Large Language Models Are Human-Level Prompt Engineers"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "5dbc2b2ee6e65e39fa3fc4bd5030be7a4a9f9a76",
        "title": "Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis"
      },
      {
        "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
        "title": "GLM-130B: An Open Bilingual Pre-trained Model"
      },
      {
        "paperId": "32f5a96c5bb1e1a064c27e80f91cef3c4d4e184d",
        "title": "Stress Detection from Social Media Articles: New Dataset Benchmark and Analytical Study"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "ccbc0b08858c1874e1500111774cf268a5f6d7a2",
        "title": "A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges"
      },
      {
        "paperId": "2c136a5a63d5a1bbd2ceee35259a36b9d0743a95",
        "title": "A Review of Affective Generation Models"
      },
      {
        "paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"
      },
      {
        "paperId": "85241d5942966f6b5af19f3cf80f7156dbcddf5f",
        "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation"
      },
      {
        "paperId": "96e7f77ed0101ac3c7c4dc41601563ce0bc8889b",
        "title": "Towards Emotional Support Dialog Systems"
      },
      {
        "paperId": "6e7f74ed26be9d6d088beb4149afd376234a5f30",
        "title": "Techniques of Sarcasm Detection: A Review"
      },
      {
        "paperId": "6b76ee20bfa831595623a2c2964d4a8057ce89cf",
        "title": "End-to-End Emotion-Cause Pair Extraction Based on Sliding Window Multi-Label Learning"
      },
      {
        "paperId": "061d69640ba6b544bbdfa8fa4637fa867fb42d2d",
        "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild"
      },
      {
        "paperId": "5cd58ef1aff4a3d2fd954bd518e9d0634422448b",
        "title": "Joint Aspect-Sentiment Analysis with Minimal User Guidance"
      },
      {
        "paperId": "e52e996884b303b9f5d56fe7d7330f5a8094143a",
        "title": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "4450493ecb806cb889b341ae8e430886f2549a61",
        "title": "GoEmotions: A Dataset of Fine-Grained Emotions"
      },
      {
        "paperId": "f7e098e80a18a8e9b74eddfdd85452d0ed3c9adb",
        "title": "BiLSTM with Multi-Polarity Orthogonal Attention for Implicit Sentiment Analysis"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "c75e65325995e2eff147af209f5dd0b5edf7ff38",
        "title": "Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts"
      },
      {
        "paperId": "4e45f66270407862c8fcd8c1bd5507e09a840b70",
        "title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances"
      },
      {
        "paperId": "a33a06ddc762fb855b6954c08d5aca603080b011",
        "title": "Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"
      },
      {
        "paperId": "f2d257625e8029f6f4998deb6279f97e07e2893c",
        "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations"
      },
      {
        "paperId": "f5d4b540e06ceb8922acf04100ea1d7151d75214",
        "title": "Parallel Aspect\u2010Oriented Sentiment Analysis for Sales Forecasting with Big Data"
      },
      {
        "paperId": "4c0840e7d91dc59e49f5e74d60cf26d9a96cec4e",
        "title": "Implicit aspect extraction in sentiment analysis: Review, taxonomy, oppportunities, and open challenges"
      },
      {
        "paperId": "eed1e4e41a6e827a35915998a9a794de75fb7a68",
        "title": "A survey on opinion summarization techniques for social media"
      },
      {
        "paperId": "d0ec45b099b832bb9c5b48066537ba88943adf83",
        "title": "Aspect Term Extraction with History Attention and Selective Transformation"
      },
      {
        "paperId": "e7423701c15d81c10545201b94dbf7a4f5e82844",
        "title": "LoRa Technology - An Overview"
      },
      {
        "paperId": "a6401e102c03a441992b3e45f7b63eec09d4b89d",
        "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers"
      },
      {
        "paperId": "3108f96f80d129036f53684344f4058257b37c4b",
        "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"
      },
      {
        "paperId": "6ac381e2acde9fff656df1505ccd4ee1b90f6b0b",
        "title": "Using the WDEP System of Reality Therapy to Support Person\u2010Centered Treatment Planning"
      },
      {
        "paperId": "9ea51567b3b19c9e71f4e62a2e77b1bdb4c3693c",
        "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "08b6efaec3d453c3ae2d8c6943e28bd1cb85defa",
        "title": "ChaLearn LAP 2016: First Round Challenge on First Impressions - Dataset and Results"
      },
      {
        "paperId": "6b56810c269206c81a13e8363b60381a43a4ab9b",
        "title": "The Ability Model of Emotional Intelligence: Principles and Updates"
      },
      {
        "paperId": "edf9b7367660d7f8255633717bf050f000a7c8fd",
        "title": "Introducing the LCC Metaphor Datasets"
      },
      {
        "paperId": "f90531de9e3f62c7c493a48cf33fba5b8fa661c4",
        "title": "Survey on Aspect-Level Sentiment Analysis"
      },
      {
        "paperId": "417346f396b90d6d88053d260f193c30fbcf1be6",
        "title": "Sentiment Analysis: An Overview from Linguistics"
      },
      {
        "paperId": "a1f2c559ccc6d8a62e17dec327171ec69f24e0f8",
        "title": "New approach in quantification of emotional intensity from the speech signal: emotional temperature"
      },
      {
        "paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
        "title": "Character-level Convolutional Networks for Text Classification"
      },
      {
        "paperId": "8a8f8cca41b354ea5e6c4dd70a5eef685f7c3ff9",
        "title": "The role of the amygdala in the perception of positive emotions: an \u201cintensity detector\u201d"
      },
      {
        "paperId": "8589c0ff39d35630a2798cf6d3f58f63d37b1c03",
        "title": "Sentiment analysis algorithms and applications: A survey"
      },
      {
        "paperId": "e0b1da50261bd82043234be540c4794f8c178824",
        "title": "Social analytics: Learning fuzzy product ontologies for aspect-oriented sentiment analysis"
      },
      {
        "paperId": "2c171266ce68cd91c7c14bec6afa00a929fb4448",
        "title": "Person-centered therapy: A pluralistic perspective"
      },
      {
        "paperId": "c2c5206f6a539b02f5d5a19bdb3a90584f7e6ba4",
        "title": "Affective Computing: A Review"
      },
      {
        "paperId": "e7767e8e051c509b1fe52669fd8cfeb28a29bf11",
        "title": "Measuring emotional intelligence with the MSCEIT V2.0."
      },
      {
        "paperId": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc",
        "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques"
      },
      {
        "paperId": "c88fce8bb5087e617fa94a7c38908f5daff2f6ee",
        "title": "Emotional intelligence \u2013 A review and evaluation study"
      },
      {
        "paperId": "2e15f173f0b35b6c0dce4032982b9f2d27f757ee",
        "title": "Models of emotional intelligence"
      },
      {
        "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
        "title": "Long Short-Term Memory"
      },
      {
        "paperId": "6e2fcf79b59d2d50f877520c9c2a677e93e7a50f",
        "title": "The Levels of Emotional Awareness Scale: a cognitive-developmental measure of emotion."
      },
      {
        "paperId": "5a7a4b3e6cd8fb3302d1692b321fd8eb39ecded4",
        "title": "Dialectical Behavioral Therapy: A Cognitive Behavioral Approach to Parasuicide"
      },
      {
        "paperId": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
        "title": "The perceptron: a probabilistic model for information storage and organization in the brain."
      },
      {
        "paperId": null,
        "title": "Omni-emotion: Extending videomllmwithdetailedfaceandaudiomodelingformultimodalemotionanalysis"
      },
      {
        "paperId": "a7099e192bc375868456e708a66d3670127df30c",
        "title": "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts"
      },
      {
        "paperId": "bd6843029c8d768a30505bf9d575ffb804ffe81b",
        "title": "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions"
      },
      {
        "paperId": "731c433f1caff30c01479aa1045a5c698968413b",
        "title": "F^2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation"
      },
      {
        "paperId": "7daf37aac345f53aa231c946f81c5378a277d74d",
        "title": "LLM-Based Empathetic Response Through Psychologist-Agent Debate"
      },
      {
        "paperId": "7a5ff9ac3317d2c0b03de5a5fdccd7762e5dfd1e",
        "title": "Be Helpful but Don\u2019t Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support"
      },
      {
        "paperId": null,
        "title": "Depressiondetectioninclinicalinterviewswithllm-empoweredstructural elementgraph"
      },
      {
        "paperId": "2db4515ddb85016351e2d5b4a1930637c4c689f9",
        "title": "EmpCRL: Controllable Empathetic Response Generation via In-Context Commonsense Reasoning and Reinforcement Learning"
      },
      {
        "paperId": "29b1a015c43b60e48adbd90433dcda4ba7d05354",
        "title": "EmoAda: A Multimodal Emotion Interaction and Psychological Adaptation System"
      },
      {
        "paperId": "2e9a7ece74e4d09a5dd5e481530a43ab88033079",
        "title": "Progressive Tuning: Towards Generic Sentiment Abilities for Large Language Models"
      },
      {
        "paperId": "cc7e2d6e806306e0c67023a29f947f9514a6fee6",
        "title": "Integrating Plutchik\u2019s Theory with Mixture of Experts for Enhancing Emotion Classification"
      },
      {
        "paperId": "243892edd059887574add1f542e41f4658ba2c20",
        "title": "ABLE: Personalized Disability Support with Politeness and Empathy Integration"
      },
      {
        "paperId": "a27a56b88d0d33dd7e9b13c7b3a658f324e1a05a",
        "title": "Is Compound Aspect-Based Sentiment Analysis Addressed by LLMs?"
      },
      {
        "paperId": null,
        "title": "Can machines resonatewithhumans?evaluatingtheemotionalandempathiccomprehen-sionoflms"
      },
      {
        "paperId": null,
        "title": "Evolutionary multi-objective optimization of large language model prompts for balancing sentiments"
      },
      {
        "paperId": null,
        "title": "Acomparisonofchatgptandfine-tuned openpre-trainedtransformers(opt)againstwidelyusedsentimentanalysistools:Sentimentanalysisofcovid-19surveydata"
      },
      {
        "paperId": null,
        "title": "Man vs. machine: Anappliedstudy comparing a man-made lexicon, a machine learned lexicon, and openai\u2019s gpt for sentiment analysis"
      },
      {
        "paperId": "4c86f91116483d946bde77684aa10844f932d5f5",
        "title": "DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations"
      },
      {
        "paperId": "ce9fbd77bef7b85a2067c740af8413c4dc874d43",
        "title": "Enhancing Empathetic and Emotion Support Dialogue Generation with Prophetic Commonsense Inference"
      },
      {
        "paperId": "21a358da7136fcf26350cf803e8eb27ed889c51d",
        "title": "Grafting Fine-Tuning and Reinforcement Learning for Empathetic Emotion Elicitation in Dialog Generation"
      },
      {
        "paperId": "3506b3be3b2472adb748e5c2cc57c200d403d7b5",
        "title": "InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework"
      },
      {
        "paperId": null,
        "title": "Investigatingthe effects of zero-shot chain-of-thought on empathetic dialogue generation"
      },
      {
        "paperId": "7e9c8c4e401c505dfeff6c75314526766f9d3921",
        "title": "An Emotion-based Korean Multimodal Empathetic Dialogue System"
      },
      {
        "paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe",
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"
      },
      {
        "paperId": null,
        "title": "Suicide and depression detection in social media forums"
      },
      {
        "paperId": null,
        "title": "Trainingcompute-optimallargelanguagemodels"
      },
      {
        "paperId": "8298cbdc43c1e93257be69a078ca2662da89db9c",
        "title": "Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions"
      },
      {
        "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "paperId": "9a456c9b0df438b9bea379deb022096131f17b4c",
        "title": "\u7279\u96c6\u300cAffective Computing\u300d\u306b\u3042\u305f\u3063\u3066"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "d97ee125f7f92741e6d05549fc4bbd0f1c66a387",
        "title": "NTUSD-Fin: A Market Sentiment Dictionary for Financial Social Media Data Applications"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": null,
        "title": "Dlirec:Aspecttermextractionandtermpolarity classification system"
      },
      {
        "paperId": "8e07435b87708199ddace381b4147a69c8e7c4cb",
        "title": "Comprehensive Review of Opinion Summarization"
      },
      {
        "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
        "title": "Learning Deep Architectures for AI"
      },
      {
        "paperId": "621970d560d907975721abee41e442e869523a54",
        "title": "Cognitive Therapy and the Emotional Disorders"
      },
      {
        "paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5",
        "title": "of the Association for Computational Linguistics"
      },
      {
        "paperId": null,
        "title": "M2se: A multistage multitask instruction tuning strategy for unified sentiment and emotion analysis"
      },
      {
        "paperId": null,
        "title": "by emotional stimuli"
      },
      {
        "paperId": null,
        "title": "for Computing Machinery"
      }
    ],
    "cited_by": [
      {
        "paperId": "d432772b6a7e7254d45185a87f4c846d8e4f7a53",
        "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity"
      },
      {
        "paperId": "b733f723e11f12c8493ff73b092891e11cd2302e",
        "title": "Large Language Models for Subjective Language Understanding: A Survey"
      },
      {
        "paperId": "c966ba26f31fbaf6226a821d15d9e0e3825491f1",
        "title": "Affective-CARA: A Knowledge Graph Driven Framework for Culturally Adaptive Emotional Intelligence in HCI"
      },
      {
        "paperId": "2ba5718b3e483e786ec284c2a710f9735901e4ac",
        "title": "DinoCompanion: An Attachment-Theory Informed Multimodal Robot for Emotionally Responsive Child-AI Interaction"
      },
      {
        "paperId": "c65a854c297f22dd7602d4ef48c2e190afc9cc39",
        "title": "AI shares emotion with humans across languages and cultures"
      },
      {
        "paperId": "ad192528dec53737b25c6e7bc81eb9858bc1aadc",
        "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing"
      },
      {
        "paperId": "f75abf96646a27df4370ea07150194cd4ac7246c",
        "title": "Proposal for an Open-source Robotics Framework and Platform for the Development of Affective Social Robots"
      },
      {
        "paperId": "555f62cfc558621f3daee15fa3d8417dae9a7882",
        "title": "Comparative Study of Zero-Shot Cross-Lingual Transfer for Bodo POS and NER Tagging Using Gemini 2.0 Flash Thinking Experimental Model"
      },
      {
        "paperId": "f3c3b2d0668e67304853d42060f3f70c55787de5",
        "title": "Advanced Medical Question and Answering Chatbot Powered by Llama-2 For Precis Health Answer"
      },
      {
        "paperId": "8a93f58ea773bfe0bcf41c01f52f327ecb5d661a",
        "title": "MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis"
      },
      {
        "paperId": "4d4a04be5a4a264dd775a08768e0767564839480",
        "title": "MultiSentimentArcs: a novel method to measure coherence in multimodal sentiment analysis for long-form narratives in film"
      },
      {
        "paperId": "1fbee182ead8854353853ae6cdc94eeb0bd85308",
        "title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts"
      },
      {
        "paperId": "200be08fcbada8982bdac73f9d14d7428b329021",
        "title": "Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective"
      },
      {
        "paperId": "f7b706cebea43fc59f8cd343ce18bae6b9c6195c",
        "title": "RVISA: Reasoning and Verification for Implicit Sentiment Analysis"
      },
      {
        "paperId": "a0e210b79e96ccb5384e90dc0f7809302d67858f",
        "title": "Navigating the Landscape of Hint Generation Research: From the Past to the Future"
      }
    ],
    "score": 15.0
  },
  {
    "id": "c724da2469bba1b98e9aec9deb4c7073d624f308",
    "title": "ChatGPT, Large Language Models, and Generative AI as Future Augments of Surgical Cancer Care",
    "authors": [
      "MD A. N. Kothari"
    ],
    "year": 2023,
    "citationCount": 29,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/c724da2469bba1b98e9aec9deb4c7073d624f308",
    "pdf_url": "https://doi.org/10.1245/s10434-023-13442-2",
    "venue": "Annals of Surgical Oncology",
    "publicationDate": "2023-04-13",
    "externalIds": {
      "DOI": "10.1245/s10434-023-13442-2",
      "CorpusId": 258110300,
      "PubMed": "37052826"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "316cf4494fb1c7883d191a13c4a36e2b38c41e47",
        "title": "Healthcare-as-a-Service Provisioning using Cloud-of-Things: A Contemporary Review of Existing Frameworks based on Tools, Services and Diseases"
      },
      {
        "paperId": "2b5177c2e996c2713a23d7f65607ca0f3ce66e50",
        "title": "Integration of Generative AI with Human Expertise in HEOR: A Hybrid Intelligence Framework"
      },
      {
        "paperId": "c6a169b3e90c3774f351b4a107d8cb4743ae05cb",
        "title": "Use of large language models as clinical decision support tools for management pancreatic adenocarcinoma using National Comprehensive Cancer Network guidelines."
      },
      {
        "paperId": "151e183071fc19798dee7de6905807af3ec62bcc",
        "title": "Effectiveness of artificial intelligence (AI) chatbots in providing labor epidural analgesia information: are we there yet?"
      },
      {
        "paperId": "b52af8604aef884166fa639da57d0c0db11a8d66",
        "title": "How AI Tools are Accepted and Utilized in Academia: A Mixed Methods Study"
      },
      {
        "paperId": "4846c516fccd620dc745ace6b6603a69c0380388",
        "title": "Aligning Large Language Models with Humans: A Comprehensive Survey of ChatGPT\u2019s Aptitude in Pharmacology"
      },
      {
        "paperId": "605ba1d86bdacc17377baae796114eec37d5ebc8",
        "title": "Development and Validation of a Modality-Invariant 3D Swin U-Net Transformer for Liver and Spleen Segmentation on Multi-Site Clinical Bi-parametric MR Images."
      },
      {
        "paperId": "f4cea9f1d2337dfa48da374011330d25bdc70373",
        "title": "Ethical Application of Generative Artificial Intelligence in Medicine."
      },
      {
        "paperId": "3eee1d2b58679b8c28bd0968fd85075125d71df0",
        "title": "Can generative AI motivate management students? The role of perceived value and information literacy"
      },
      {
        "paperId": "2328293b9a9da591f820b2f2b4a5551c3d44bbc2",
        "title": "A Survey for Large Language Models in Biomedicine"
      },
      {
        "paperId": "26f3281c7e87d2a6ed1b3eba4469b2b0a1ac39ba",
        "title": "Accuracy and consistency of publicly available Large Language Models as clinical decision support tools for the management of colon cancer"
      },
      {
        "paperId": "250b393bfd1a249cde70951035d31acdf0ec7832",
        "title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review"
      },
      {
        "paperId": "c14346ea1046f30a2f5b394eff80fd4c2fab82bf",
        "title": "ChatGPT as a Source of Information on Pancreatic Cancer."
      },
      {
        "paperId": "bb2194c4a250e8a46a45a6a2c6c637b2244708b2",
        "title": "Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant"
      },
      {
        "paperId": "b9d77b41cf8314cd03656f1d44b697c64933059f",
        "title": "The Consistency and Quality of ChatGPT Responses Compared to Clinical Guidelines for Ovarian Cancer: A Delphi Approach"
      },
      {
        "paperId": "013c12ff6179facb1e17ff652781a02ee30a70c8",
        "title": "OpenAI's GPT-4o in surgical oncology: Revolutionary advances in generative artificial intelligence."
      },
      {
        "paperId": "25361e9582996220625a1823d931b4ed5f5558d3",
        "title": "Generative AI in healthcare: an implementation science informed translational path on application, integration and governance"
      },
      {
        "paperId": "e89ccb98415901916c00f9ba744b79362bb20cf7",
        "title": "Revolutionizing ocular cancer management: a narrative review on exploring the potential role of ChatGPT"
      },
      {
        "paperId": "66cf1aaf0880326fb52afd8a963b6a3d53afd3f3",
        "title": "Let's chat about cervical cancer: Assessing the accuracy of ChatGPT responses to cervical cancer questions."
      },
      {
        "paperId": "8d2709ed1788a67e64425fb410bb49f3ee49e088",
        "title": "Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"
      },
      {
        "paperId": "6e928fc1ed368db5f0539005232179d89acedbea",
        "title": "AI's deep dive into complex pediatric inguinal hernia issues: a challenge to traditional guidelines?"
      },
      {
        "paperId": "d3148213576e623884e906311ead7145691d335a",
        "title": "RE: Artificial intelligence chatbots will revolutionize how cancer patients access information: ChatGPT represents a paradigm-shift"
      },
      {
        "paperId": "722a0913cdcc31315e7c399422b0ef8e932a6638",
        "title": "From ChatGPT to Treatment: the Future of AI and Large Language Models in Surgical Oncology"
      },
      {
        "paperId": "f81e6a3e77bcee3e7dab40132b12a3d83ad0b88c",
        "title": "Are both generative AI and ChatGPT game changers for 21st-Century operations and supply chain excellence?"
      },
      {
        "paperId": "316639b183c558d9625bc325d7230f2955c9ce28",
        "title": "Potential applications of ChatGPT in endoscopy: Opportunities and limitations"
      },
      {
        "paperId": "77b85c027bdaea07db4c86b59665600cedaeeb92",
        "title": "Improving ChatGPT Prompt for Code Generation"
      },
      {
        "paperId": "7602d66a8722c4c53192359998e3ed217de21a6d",
        "title": "Democratizing scientific and healthcare communication with large language models"
      },
      {
        "paperId": "7ddcb0a36a80488d23f368d07bad736e561eeb3d",
        "title": "Neuro-Symbolic AI + Agent Systems: A First Reflection on Trends, Opportunities and Challenges"
      },
      {
        "paperId": "67877084297dba9f3f3f57838a05c74380612630",
        "title": "Accelerating Artificial Intelligence Discussions in ASEAN: Addressing Disparities, Challenges, and Regional Policy Imperatives"
      }
    ],
    "score": 14.5
  },
  {
    "id": "cb99a85c651a3976d9a8db0951d0f6edfe1addce",
    "title": "Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models",
    "authors": [
      "Jiaming Ji",
      "Xinyu Chen",
      "Rui Pan",
      "Han Zhu",
      "Conghui Zhang",
      "Jiahao Li",
      "Donghai Hong",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Chi-Min Chan",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "year": 2025,
    "citationCount": 14,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/cb99a85c651a3976d9a8db0951d0f6edfe1addce",
    "pdf_url": "https://doi.org/10.48550/arXiv.2503.17682",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2503-17682",
      "DOI": "10.48550/arXiv.2503.17682",
      "CorpusId": 277271998
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "1f70601f1090f5a3a3257b6918b81ee2fc9dede8",
        "title": "Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception"
      },
      {
        "paperId": "2429de2c3f2b7ddcdf1f8b2d34c6eb8b75cc47f9",
        "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models"
      },
      {
        "paperId": "7c32b20b0d6f6b4fdb51bd35813222f5ac7fcc71",
        "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality"
      },
      {
        "paperId": "e3e404e30641e720b7fdc3dab032741726215ccd",
        "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models"
      },
      {
        "paperId": "1a08170d7a9f9653b825967ead501d45b07c425d",
        "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong"
      },
      {
        "paperId": "85002e39a08881cb7f6cbaf8c59c8ad174243fc2",
        "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It"
      },
      {
        "paperId": "123b4baa8e87918af43c4c92542da26873f98655",
        "title": "The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels"
      },
      {
        "paperId": "680aeb24d20a4f2d10e45b9aa7f37b9a19a0e394",
        "title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models"
      },
      {
        "paperId": "86371cfe3943cf9113bbdb33a8b05ba31bd12547",
        "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference"
      },
      {
        "paperId": "ca7007e130e4bb42674040910ceb96f1b23c0a27",
        "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models"
      },
      {
        "paperId": "f388d3a269deff3d98841e348529e6cf20d9dae2",
        "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models"
      },
      {
        "paperId": "7f01815dec03ae3d8da3d8ef1529b15959e63491",
        "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning"
      },
      {
        "paperId": "3a676409c33683540dafd473e602f27f2b4ad299",
        "title": "Safety in Large Reasoning Models: A Survey"
      },
      {
        "paperId": "45bc625838932e14973d95f4c6f2b8efea13a009",
        "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach"
      }
    ],
    "score": 14.0
  },
  {
    "id": "208fdbf3ac095740a53230523db3828a52414da6",
    "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
    "authors": [
      "Simon Holk",
      "Daniel Marta",
      "Iolanda Leite"
    ],
    "year": 2024,
    "citationCount": 14,
    "abstract": "Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights \u2013 state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilectCCS CONCEPTS\u2022 Computing methodologies \u2192 Inverse reinforcement learning; Learning to rank.",
    "url": "https://www.semanticscholar.org/paper/208fdbf3ac095740a53230523db3828a52414da6",
    "pdf_url": "https://arxiv.org/pdf/2402.15420.pdf",
    "venue": "IEEE/ACM International Conference on Human-Robot Interaction",
    "publicationDate": "2024-02-23",
    "externalIds": {
      "ArXiv": "2402.15420",
      "DBLP": "conf/hri/HolkML24",
      "DOI": "10.1145/3610977.3634970",
      "CorpusId": 267897401
    },
    "references": [
      {
        "paperId": "727a235276c81cc0fb9075f0edeb5454ca381eef",
        "title": "Principles and Guidelines for Evaluating Social Robot Navigation Algorithms"
      },
      {
        "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
        "title": "Large Language Models Are Reasoning Teachers"
      },
      {
        "paperId": "18d750263f1dfe43374e8791cefa580a511c2098",
        "title": "Few-Shot Preference Learning for Human-in-the-Loop RL"
      },
      {
        "paperId": "7daffaf429b4e7f1580c0fbe3dfe188d6313e939",
        "title": "Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines"
      },
      {
        "paperId": "364838730f481d5b6d8839bc9a8bdd5e3f4767e0",
        "title": "Revisiting Human-Robot Teaching and Learning Through the Lens of Human Concept Learning"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "a7aa150b55d64d339b1c154d6d88455fc3cbc44f",
        "title": "ClipCap: CLIP Prefix for Image Captioning"
      },
      {
        "paperId": "75c864fc2e384ce299a4031e31c48f9ebc0a9842",
        "title": "Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269",
        "title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback"
      },
      {
        "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
        "title": "Multimodal Few-Shot Learning with Frozen Language Models"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "79032b17f6b7645395dd16cb251699302100eb78",
        "title": "Preference learning along multiple criteria: A game-theoretic perspective"
      },
      {
        "paperId": "2e17ac6cc93354ce0506d7a8e8feb79fa7ca1e6d",
        "title": "Intention Understanding in Human\u2013Robot Interaction Based on Visual-NLP Semantics"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "c560c473c4ba7c552b4fb083ef403656cf97fe95",
        "title": "Reinforcement Learning With Human Advice: A Survey"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "f3df804a7eef204b0c4fc4b81166dec9b4abc073",
        "title": "Semi-Supervised Learning under Class Distribution Mismatch"
      },
      {
        "paperId": "385931984baef107aa9ee8ba28da1463f0beba36",
        "title": "Meta-Transfer Learning for Zero-Shot Super-Resolution"
      },
      {
        "paperId": "1f52deff193c7c3dfc77c48cbdc653c94f093a92",
        "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning"
      },
      {
        "paperId": "976e128c32007cdaf95a4e278a1ea7aa33a2e5cc",
        "title": "LESS is More: Rethinking Probabilistic Models of Human Behavior"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "title": "Self-Training With Noisy Student Improves ImageNet Classification"
      },
      {
        "paperId": "470dbb7572323a192c5b630d4b51eb59469e9f02",
        "title": "Self-Supervised Correspondence in Visuomotor Policy Learning"
      },
      {
        "paperId": "3245b2436d9f39b9ecadd275a815d09c2e130497",
        "title": "Unsupervised Learning of Object Keypoints for Perception and Control"
      },
      {
        "paperId": "c441a7e9b1c7ecf54f583cd61e896faa10b60358",
        "title": "Causal Confusion in Imitation Learning"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "f0070a6e7345c3a703db308a55211b4bddae9c85",
        "title": "Few-Shot Goal Inference for Visuomotor Learning and Planning"
      },
      {
        "paperId": "15dd061a323e4de0e3d2866328bc4b6637e8ad0a",
        "title": "Transfer Learning via Learning to Transfer"
      },
      {
        "paperId": "8037db97f3cadb842ffa4bee83d59878fe7974d0",
        "title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation"
      },
      {
        "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
        "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
      },
      {
        "paperId": "cd1f4c45e5126b527789a23ca8b407d46cefd738",
        "title": "Active Reward Learning from Critiques"
      },
      {
        "paperId": "cac053312603a785b23abe9b7998910af7485d6b",
        "title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries"
      },
      {
        "paperId": "84bb62e3f40434a1e367d24783bd81432a5396d6",
        "title": "Integrating State Representation Learning Into Deep Reinforcement Learning"
      },
      {
        "paperId": "757b27a3ceb2293b8284fc24a7084a0c3fc2ae21",
        "title": "Data Distillation: Towards Omni-Supervised Learning"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "25fb83735f62e011b2cd617adee3b0f6f4473497",
        "title": "Supervised autonomy for online learning in human-robot interaction"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e27467ca84c5c1f7239a6e643843c1b97e35671f",
        "title": "Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "7251354318a25b0f304bfe756c8749d492106139",
        "title": "Repeated Inverse Reinforcement Learning"
      },
      {
        "paperId": "7b1e4ea16807d041faf51537c0b81d2484722d18",
        "title": "Introduction to the foundations of causal discovery"
      },
      {
        "paperId": "cdfb8f75c8f6459961359b483f1c017dbeec8282",
        "title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning"
      },
      {
        "paperId": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
        "title": "Learning to Navigate in Complex Environments"
      },
      {
        "paperId": "6cb26f4d6c7726abab06f4815185a8483e30ac08",
        "title": "Learning to push by grasping: Using multiple tasks for effective learning"
      },
      {
        "paperId": "b10f40940831f24ae94c99baf40b4bcba2f87dc1",
        "title": "Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks"
      },
      {
        "paperId": "17ed4d8c41be0610afcbbf60973a6b138968546b",
        "title": "Training a Robot via Human Feedback: A Case Study"
      },
      {
        "paperId": "8c63606dc2d2e02eeb80a1800c62bb38af72fa47",
        "title": "Human preferences for robot-human hand-over configurations"
      },
      {
        "paperId": "a78273144520d57e150744cf75206e881e11cc5b",
        "title": "Multimodal Deep Learning"
      },
      {
        "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
        "title": "Interactively shaping agents via human reinforcement: the TAMER framework"
      },
      {
        "paperId": "1c6356a688a273889fb71bab17973ed10483d97f",
        "title": "Causation, Prediction, and Search"
      },
      {
        "paperId": "f129124da1d10b1a4b33e2dc7e01d6a4349886e7",
        "title": "Social force model for pedestrian dynamics."
      },
      {
        "paperId": "d0da6b23e08524d9ff8550c84dcbafb24a37f829",
        "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons"
      },
      {
        "paperId": "d3d1f85fe0f9461a570c6040e94adcb530dd5325",
        "title": "A Dual Representation Framework for Robot Learning with Human Guidance"
      },
      {
        "paperId": "005a961b8c04f1b0e066e2ba29a9945d28882eb9",
        "title": "A Study of Causal Confusion in Preference-Based Reward Learning"
      },
      {
        "paperId": null,
        "title": "LiT:Zero-ShotTransferwithLocked-imageTextTuning"
      },
      {
        "paperId": null,
        "title": "Evaluatinglargelanguagemodelstrainedoncode"
      },
      {
        "paperId": null,
        "title": "Learningtransferablevisualmodelsfromnaturallanguagesupervision In International Conference on Machine Learning"
      },
      {
        "paperId": "d99fb06ffb8b53f12db447592c24adfb3121dba1",
        "title": "Performance Evaluation of Imitation Learning Algorithms with Human Experts"
      },
      {
        "paperId": null,
        "title": "Zeroshottransferlearning forrobotsoccer"
      },
      {
        "paperId": null,
        "title": "Bert: Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": null,
        "title": "Interactive learning frompolicy-dependenthumanfeedback"
      },
      {
        "paperId": null,
        "title": "ConcreteproblemsinAIsafety"
      },
      {
        "paperId": "bd3333c8f96ffc28223d9436dc7999cea46e02f6",
        "title": "Natural Language For Human Robot Interaction"
      },
      {
        "paperId": null,
        "title": "A bayesian approach for policylearningfromtrajectorypreferencequeries"
      },
      {
        "paperId": null,
        "title": "2022.SocraticModels:ComposingZero-ShotMultimodal ReasoningwithLanguage"
      },
      {
        "paperId": null,
        "title": "2022.HowtotalksoAIwilllearn:Instructions,descriptions, andautonomy"
      },
      {
        "paperId": null,
        "title": "2022. Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2022.Correcting robotplanswithnaturallanguagefeedback"
      },
      {
        "paperId": null,
        "title": "2022. SURF: Semi-supervised Reward Learning with Data Augmentation"
      },
      {
        "paperId": null,
        "title": "2022. Explaining Preferences with Shapley Values"
      },
      {
        "paperId": null,
        "title": "2022.MINDMELD:PersonalizedMeta-LearningforRobot-CentricImitationLearning."
      },
      {
        "paperId": null,
        "title": "HRI \u201924, March 11\u201314, 2024,"
      }
    ],
    "cited_by": [
      {
        "paperId": "f0ead24103ecb76f9be1bda46d17c05fbaf5280d",
        "title": "A Call for Deeper Collaboration Between Robotics and Game Development"
      },
      {
        "paperId": "f14cdbeeaf122b627808d44b519fa5cad79cc7b0",
        "title": "Investigating Inverse Reinforcement Learning during Rapid Aiming Movement in Extended Reality and Human-Robot Interaction"
      },
      {
        "paperId": "84a1a83da985397e17e1beba09bac06b339cb38c",
        "title": "Legibility on Social Robot Navigation: A Survey"
      },
      {
        "paperId": "551a5e52bc72df8251a2f66fc9652faabfcacdb7",
        "title": "Flora: Sample-Efficient Preference-Based Rl Via Low-Rank Style Adaptation of Reward Functions"
      },
      {
        "paperId": "3490848027f085debe1ce650491222066ccbe039",
        "title": "BT-ACTION: A Test-Driven Approach for Modular Understanding of User Instruction Leveraging Behaviour Trees and LLMs"
      },
      {
        "paperId": "d2232324505bd3e84ded317c95916cf9cf6c5b51",
        "title": "Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs"
      },
      {
        "paperId": "38b475e1fddcdfbb3743665ce1b063f2b7d73561",
        "title": "The Impact of VR and 2D Interfaces on Human Feedback in Preference-Based Robot Learning"
      },
      {
        "paperId": "aeeaa6aa9b40454a0b0345197ab39a0758edc1a1",
        "title": "ICPL: Few-shot In-context Preference Learning via LLMs"
      },
      {
        "paperId": "cf2de4ba64f8ea698b782e38e9f5f9fddce0ec99",
        "title": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and Human Explanations"
      },
      {
        "paperId": "90609d110ad88f7d94e4313c51fde0ab687fb43e",
        "title": "Shielding for Socially Appropriate Robot Listening Behaviors"
      },
      {
        "paperId": "205c3352ec73278e737806849323b37b36e1de5d",
        "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning"
      },
      {
        "paperId": "0c468fb51f94b55ff121bc4738b4ca2f7fcd7c61",
        "title": "POLITE: Preferences Combined with Highlights in Reinforcement Learning"
      },
      {
        "paperId": "5008574062623b3269ba9e39d37e319770101d6e",
        "title": "SEQUEL: Semi-Supervised Preference-based RL with Query Synthesis via Latent Interpolation"
      },
      {
        "paperId": "3e65e47eb6361213715b3098659ca75fbb172e4f",
        "title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning"
      }
    ],
    "score": 14.0
  },
  {
    "id": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
    "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?",
    "authors": [
      "Hangfan Zhang",
      "Zhimeng Guo",
      "Huaisheng Zhu",
      "Bochuan Cao",
      "Lu Lin",
      "Jinyuan Jia",
      "Jinghui Chen",
      "Di Wu"
    ],
    "year": 2023,
    "citationCount": 27,
    "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is\"could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
    "url": "https://www.semanticscholar.org/paper/ba015c5d3f5b44e36363b90070bb3301d21ae57e",
    "pdf_url": "https://arxiv.org/pdf/2310.01581.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-02",
    "externalIds": {
      "ArXiv": "2310.01581",
      "DBLP": "journals/corr/abs-2310-01581",
      "DOI": "10.48550/arXiv.2310.01581",
      "CorpusId": 263609070
    },
    "references": [
      {
        "paperId": "cd29c25c489562b409a60f83365f93f33ee1a0a1",
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"
      },
      {
        "paperId": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
        "title": "Certifying LLM Safety against Adversarial Prompting"
      },
      {
        "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
        "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"
      },
      {
        "paperId": "1104d766527dead44a40532e8a89444d9cef5c65",
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
        "title": "ProPILE: Probing Privacy Leakage in Large Language Models"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "db4cf9f6a653d5c15973e836c800ea47743251ae",
        "title": "Prompt Injection attack against LLM-integrated Applications"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
      },
      {
        "paperId": "389ec3e8902a5dcfcde1adec735854e93f845937",
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "025ca4c125d6ecabc816a56f160e5c992abc76d9",
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT"
      },
      {
        "paperId": "0cf694b8f85ab2e11d45595de211a15cfbadcd22",
        "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"
      },
      {
        "paperId": "f2b0017ddd77fa38760a18145e63553105a1a236",
        "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
        "title": "GLM-130B: An Open Bilingual Pre-trained Model"
      },
      {
        "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "b1338641c1a981be589580b67a6b7b9fc2905ccc",
        "title": "Adaptive Machine Unlearning"
      },
      {
        "paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
        "title": "Editing Factual Knowledge in Language Models"
      },
      {
        "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
        "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "2f2ade8c4944a96a44e6f70ef403b80b058d1725",
        "title": "Towards Making Systems Forget with Machine Unlearning"
      },
      {
        "paperId": null,
        "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023"
      },
      {
        "paperId": "a2bcacc8fefb859c94c69d524b2368bb4792f9b1",
        "title": "Adversarial Prompting for Black Box Foundation Models"
      },
      {
        "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
        "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": null,
        "title": "Instruction tuning with gpt-4, 2023"
      },
      {
        "paperId": null,
        "title": "Fast model editing at scale, 2022"
      },
      {
        "paperId": null,
        "title": "The 50 sampled names"
      },
      {
        "paperId": "64c22174a3a43c1224c88ba37eae6421a0086730",
        "title": "The Enron Corpus: A New Dataset for Email Classi(cid:12)cation Research"
      }
    ],
    "cited_by": [
      {
        "paperId": "1391f84c5232c8e6104eb30046276827b11e02c4",
        "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors"
      },
      {
        "paperId": "00e3baedb7641d1b4dc3b2cef255adef2a56a185",
        "title": "Many-Turn Jailbreaking"
      },
      {
        "paperId": "3be0bcb6c9599a901ff291f5121bbe8fed8c8bb3",
        "title": "MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models"
      },
      {
        "paperId": "0f7bb49d1272594cbae597bc2fd6f508daa26a27",
        "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration"
      },
      {
        "paperId": "d24ebf14f1c9efc076c3ed0009e5c4c97f30ce63",
        "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models"
      },
      {
        "paperId": "bbd5d7577c5c2c6d0ec69b679690cc370e7032c6",
        "title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks"
      },
      {
        "paperId": "2e279da77dddc013c273759c5d32ba1f2ad80427",
        "title": "Extracting Unlearned Information from LLMs with Activation Steering"
      },
      {
        "paperId": "17052ea02eaad2cbb28af5690d1218e8ddc8e940",
        "title": "Data to Defense: The Role of Curation in Customizing LLMs Against Jailbreaking Attacks"
      },
      {
        "paperId": "7e9dcdec2d29bdc45f16127891c54034214ac39b",
        "title": "Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI"
      },
      {
        "paperId": "3d7cc47f10a1b55e3c7af24bf43f7f9206fcda4e",
        "title": "Recent Advances in Attack and Defense Approaches of Large Language Models"
      },
      {
        "paperId": "6ed4f005847a08ebe01ee03f9b533fbf860ed197",
        "title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs"
      },
      {
        "paperId": "f8e289b53bf32d64abb8e2826cf88b415e95fe97",
        "title": "On the (In)Security of LLM App Stores"
      },
      {
        "paperId": "2ae809b9f060a617fdb68d7964c3fdfd3479e274",
        "title": "Safeguarding Large Language Models: A Survey"
      },
      {
        "paperId": "c443c05601c91ab9bbe45283846eccee3eb80d74",
        "title": "Robustifying Safety-Aligned Large Language Models through Clean Data Curation"
      },
      {
        "paperId": "8db6ff37617c5d3a6aec9e40e5e829a735d0c0cf",
        "title": "WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response"
      },
      {
        "paperId": "26a624f6a415289a4e4b2e265c2722bf54a1bc54",
        "title": "Large Language Model Supply Chain: A Research Agenda"
      },
      {
        "paperId": "4ad33969188555b8303b375e18f5c117a68387c6",
        "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs"
      },
      {
        "paperId": "72f51c3ef967f7905e3194296cf6fd8337b1a437",
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"
      },
      {
        "paperId": "50d8549a242d2b07f5d07f6564fab9fbe475b268",
        "title": "Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs"
      },
      {
        "paperId": "f75f401f046d508753d6b207f3f19414f489bd08",
        "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia"
      },
      {
        "paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9",
        "title": "Weak-to-Strong Jailbreaking on Large Language Models"
      },
      {
        "paperId": "a1fe42803fe60a137f810046235ff21c35101e18",
        "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks"
      },
      {
        "paperId": "383c598625110e0a4c60da4db10a838ef822fbcf",
        "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"
      },
      {
        "paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2",
        "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"
      },
      {
        "paperId": "defd7eceb5646578c2aa0f837efbf317cf19ae95",
        "title": "Position: Technical Research and Talent is Needed for Effective AI Governance"
      },
      {
        "paperId": "4eef4d6f58955fc2f94f970041fe8d233c6b1520",
        "title": "P4: Plug-and-Play Discrete Prompting for Large Language Models Personalization"
      },
      {
        "paperId": "4636638117100ca3d268807409a85ed838381801",
        "title": "Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs"
      }
    ],
    "score": 13.5
  },
  {
    "id": "983c01d00102075dae128b8ef9f01abef98720b5",
    "title": "Does RLHF Scale? Exploring the Impacts From Data, Model, and Method",
    "authors": [
      "Zhenyu Hou",
      "Pengfan Du",
      "Yilin Niu",
      "Zhengxiao Du",
      "Aohan Zeng",
      "Xiao Liu",
      "Minlie Huang",
      "Hongning Wang",
      "Jie Tang",
      "Yuxiao Dong"
    ],
    "year": 2024,
    "citationCount": 13,
    "abstract": "This study explores the scaling properties of Reinforcement Learning from Human Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is considered an important step in post-training of LLMs, its scaling potential is still largely unknown. We systematically analyze key components in the RLHF framework--model size, data composition, and inference budget--and their impacts on performance. Our findings show that increasing data diversity and volume improves reward model performance, helping process-supervision models scale better. For policy training, more response samples per prompt boost performance initially but quickly plateau. And larger reward models offer modest gains in policy training. In addition, larger policy models benefit less from RLHF with a fixed reward model. Overall, RLHF scales less efficiently than pretraining, with diminishing returns from additional computational resources. Based on these observations, we propose strategies to optimize RLHF performance within computational limits.",
    "url": "https://www.semanticscholar.org/paper/983c01d00102075dae128b8ef9f01abef98720b5",
    "pdf_url": "https://arxiv.org/pdf/2412.06000.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-12-08",
    "externalIds": {
      "DBLP": "journals/corr/abs-2412-06000",
      "ArXiv": "2412.06000",
      "DOI": "10.48550/arXiv.2412.06000",
      "CorpusId": 274597334
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "20f6e0ade35ed19ad6ceb849dfd58ce460bb3a72",
        "title": "Tiny Reward Models"
      },
      {
        "paperId": "36ce296e3a4b443cd5ad1f2bdc3c28e18c60fb0d",
        "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning"
      },
      {
        "paperId": "9fc6f772d30bda03c18024c680f4cdacc6d6e692",
        "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling"
      },
      {
        "paperId": "177d958aee7dfbedbdc70f0588aad4613be8ccaa",
        "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL"
      },
      {
        "paperId": "f29c8c4bf20dea0ba57d91ba409d846123fb89c0",
        "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving"
      },
      {
        "paperId": "68ef56cb9bb93ce50790c20c1af8f7d541fc654c",
        "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation"
      },
      {
        "paperId": "920cd8b25373358779fde44f90774533f26d782a",
        "title": "A Survey of Scaling in Large Language Model Reasoning"
      },
      {
        "paperId": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
        "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "35b0db19e000b90a979e25f9134ebe452b278482",
        "title": "Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation"
      },
      {
        "paperId": "8d6411e337502f7fe0bfa59d486803a73d2c1192",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "paperId": "1fd282ff3a034ff6113f076b02769c46a7159476",
        "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      }
    ],
    "score": 13.0
  },
  {
    "id": "faae9de3d314e8731b0505607298fd826e3de1a7",
    "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation",
    "authors": [
      "Meng Cao",
      "Lei Shu",
      "Lei Yu",
      "Yun Zhu",
      "Nevan Wichers",
      "Yinxiao Liu",
      "Lei Meng"
    ],
    "year": 2024,
    "citationCount": 12,
    "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.",
    "url": "https://www.semanticscholar.org/paper/faae9de3d314e8731b0505607298fd826e3de1a7",
    "pdf_url": "https://arxiv.org/pdf/2401.07382.pdf",
    "venue": "",
    "publicationDate": "2024-01-14",
    "externalIds": {
      "ArXiv": "2401.07382",
      "CorpusId": 266999837
    },
    "references": [
      {
        "paperId": "c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "58e2acdd805d9cc48c7fdde0c9c09280f9f6c4e0",
        "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting"
      },
      {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "da5fcb26c830663b79c9aa1c550ae62e7725fcad",
        "title": "Systematic Rectification of Language Models via Dead-end Analysis"
      },
      {
        "paperId": "29acc890e521f7a6415666ab9eb3432c49b4587a",
        "title": "Self-critiquing models for assisting human evaluators"
      },
      {
        "paperId": "023edab4738690444e3924e224c2641017a0d794",
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "b808b6ddf511977e9a33dbe01b412a02b6092ae0",
        "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization"
      },
      {
        "paperId": "02f033482b8045c687316ef81ba7aaae9f0a2e1c",
        "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts"
      },
      {
        "paperId": "005acb881061eb8137e9d36a05a6a0bdf0026b61",
        "title": "Hierarchical Reinforcement Learning By Discovering Intrinsic Options"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
        "title": "GeDi: Generative Discriminator Guided Sequence Generation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
        "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"
      },
      {
        "paperId": "e04a80263d252a3d8a382ba37a249b9345620570",
        "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
      },
      {
        "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"
      },
      {
        "paperId": "0fa1c75a452a046e11e775eb6120051c696d9366",
        "title": "Using Natural Language for Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd",
        "title": "Data-Efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
        "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
        "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
        "title": "Dual Learning for Machine Translation"
      },
      {
        "paperId": "fb32191ec07ba4d7badc76ca428c816995b5785a",
        "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access"
      },
      {
        "paperId": "1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6",
        "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction"
      },
      {
        "paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec",
        "title": "An Actor-Critic Algorithm for Sequence Prediction"
      },
      {
        "paperId": "64023a5d10efa16a68db9f13c80f2751bcd4bf1e",
        "title": "Policy Networks with Two-Stage Training for Dialogue Systems"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
        "title": "Deep Reinforcement Learning for Dialogue Generation"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "848b8458d36f0e976da8ad59dc7c073987c175e1",
        "title": "Exploration-Guided Reward Shaping for Reinforcement Learning under Sparse Rewards"
      },
      {
        "paperId": null,
        "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
      },
      {
        "paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671",
        "title": "Text Generation by Learning from Demonstrations"
      },
      {
        "paperId": null,
        "title": "Openwebtext corpus"
      },
      {
        "paperId": null,
        "title": "Bandit-Sum: Extractive summarization as a contextual ban-dit"
      },
      {
        "paperId": "5b3f67e037439ba1213aa872a43ddfc10a77d661",
        "title": "Summarization Using Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2022. Exploration-guided reward shaping"
      },
      {
        "paperId": null,
        "title": "2023. Mistral 7b"
      },
      {
        "paperId": null,
        "title": "Advances in Neural Information Processing Systems volume 29"
      },
      {
        "paperId": null,
        "title": "Evaluate the summary independently for each of the three metrics"
      },
      {
        "paperId": null,
        "title": "2022b. Learning with rejection for abstrac-tive text summarization"
      },
      {
        "paperId": null,
        "title": "2023. Guiding pretraining in reinforcement learning with large language models"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "2023. Reward design with language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "29c3368b63f7aa43d7e1b4928cc8a1a1e5693610",
        "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning"
      },
      {
        "paperId": "2138d3bb41cc0bde43f2a0994cc61b271df61944",
        "title": "Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing"
      },
      {
        "paperId": "cd70dda1ed6801cf73965460fadf84ab26f3399d",
        "title": "Guided Exploration in Sparse Reward Environments using Causal Prior Knowledge"
      },
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "e5bf3534ecc3a496660a974787a102ed0e1958ec",
        "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "9a0cd90d265af436605216cf4e1a41c8cfff0ef0",
        "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training"
      },
      {
        "paperId": "8f599aaa623c43ad9a9d91b3c67ee29ad14ca078",
        "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning"
      },
      {
        "paperId": "e7000d4dc3ca07fe320f5d0431cb6f253a1e59c8",
        "title": "An optimal scheduling method of hydrogen production systems using curtailed electricity through artificial intelligence and deep reinforcement learning"
      },
      {
        "paperId": "71da1f3cd9b9c48ae7c62d5452cc18045000b833",
        "title": "A Critical Look At Tokenwise Reward-Guided Text Generation"
      },
      {
        "paperId": "bc7d05f28822a9cb256ab074069aaa08e879a4ce",
        "title": "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning"
      }
    ],
    "score": 12.0
  },
  {
    "id": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
    "title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback",
    "authors": [
      "Yu Xia",
      "Tong Yu",
      "Zhankui He",
      "Handong Zhao",
      "Julian J. McAuley",
      "Shuai Li"
    ],
    "year": 2024,
    "citationCount": 12,
    "abstract": "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs\u2019 text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.",
    "url": "https://www.semanticscholar.org/paper/0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
    "pdf_url": "https://doi.org/10.18653/v1/2024.naacl-long.262",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "publicationDate": "",
    "externalIds": {
      "ACL": "2024.naacl-long.262",
      "DBLP": "conf/naacl/Xia0HZM024",
      "DOI": "10.18653/v1/2024.naacl-long.262",
      "CorpusId": 270514540
    },
    "references": [
      {
        "paperId": "33f365dcb70ce4b881c1420577d26ff5fde223c6",
        "title": "Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits"
      },
      {
        "paperId": "7ada4e2e0c6d89e70fb10f0ff749e5b0a00683cb",
        "title": "Interact with the Explanations: Causal Debiased Explainable Recommendation System"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "paperId": "bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
        "title": "Bias and Fairness in Large Language Models: A Survey"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "812973a0a2640671f9d0a410f3a433106890221a",
        "title": "User-Regulation Deconfounded Conversational Recommender System with Bandit Feedback"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "fd8730a7e7efbfc3c7cd94f6534fb4eb11718c2c",
        "title": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback"
      },
      {
        "paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3",
        "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "dfa0fd70a186b0f7f44e833b4173ed97a4f5de31",
        "title": "Understanding Causality with Large Language Models: Feasibility and Opportunities"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
        "title": "Pretraining Language Models with Human Preferences"
      },
      {
        "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "e36caccdeb4d85842e46363b0938ef0235a8db6c",
        "title": "Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts"
      },
      {
        "paperId": "7b6f6b6241ae89abec33453be0b0b03482936b09",
        "title": "Causal Intervention Improves Implicit Sentiment Analysis"
      },
      {
        "paperId": "5bfff0955b511ce4ecb906c67cbe323b60b5c6d3",
        "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "c4e991c0c5c21608a5a21d31fd478ce7b7fb527d",
        "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View"
      },
      {
        "paperId": "7ccbc36d047eb61ab1deb743dba10f2ec7853151",
        "title": "Leashing the Inner Demons: Self-Detoxification for Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d98453ec5099298fa1e1b3e45617645316b8d7a1",
        "title": "Dbias: detecting biases and ensuring fairness in news articles"
      },
      {
        "paperId": "8b547e62c65c1169f8f7dd1644bb1fa3f345990f",
        "title": "Deconfounded and Explainable Interactive Vision-Language Retrieval of Complex Scenes"
      },
      {
        "paperId": "3aa14f1d49db181a6cef26faa4b7cf60e5afeefb",
        "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention"
      },
      {
        "paperId": "29bff398999e927dcc5702d164127262d2524163",
        "title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning"
      },
      {
        "paperId": "6de4dc0bb66971071ced5201e37ed8f3ccd5e062",
        "title": "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection"
      },
      {
        "paperId": "9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2",
        "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"
      },
      {
        "paperId": "c722ae666b93827de1867c7585530b78979b4c56",
        "title": "Interventional Few-Shot Learning"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "16981cc4ddefd3ea7655754fd83a2a8ff2203a8b",
        "title": "Automatically Neutralizing Subjective Bias in Text"
      },
      {
        "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      },
      {
        "paperId": "f6e6c948a2074e38e0a4e9099c0f63773c6013dd",
        "title": "Causality"
      },
      {
        "paperId": "295065d942abca0711300b2b4c39829551060578",
        "title": "BERTScore: Evaluating Text Generation with BERT"
      },
      {
        "paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22",
        "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "a26accf878216be4388ad9a0474e658aa03d33e2",
        "title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications"
      },
      {
        "paperId": "52521a72acafd51699d4877793256f19bccffae9",
        "title": "Learning Causal Effects From Many Randomized Experiments Using Regularized Instrumental Variables"
      },
      {
        "paperId": "651e5bcc14f14605a879303e97572a27ea8c7956",
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models"
      },
      {
        "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
        "title": "Teaching Machines to Read and Comprehend"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "f6b69b3327d7deb01c92f2babc0dd826b5f68134",
        "title": "Generalized Instrumental Variables"
      },
      {
        "paperId": null,
        "title": ". Improving language models with advantage-based offline policy gradients"
      },
      {
        "paperId": "f4b5ed96ca25c0f61778a26d03cdd6c4b946b5ea",
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment"
      },
      {
        "paperId": "3ce32d0142fa8d0c645c2f531afa4fe695570f9f",
        "title": "Understanding Demonstration-based Learning from a Causal Perspective"
      },
      {
        "paperId": "60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5",
        "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning"
      },
      {
        "paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
        "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"
      },
      {
        "paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68",
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models"
      },
      {
        "paperId": "6977cd193eb360c3271cc0c6dd7efc10493d0387",
        "title": "Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling"
      },
      {
        "paperId": "78d08b8ab4132defffe98ec7f80a51452203f70d",
        "title": "Investigating Gender Bias in Language Models Using Causal Mediation Analysis"
      },
      {
        "paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671",
        "title": "Text Generation by Learning from Demonstrations"
      },
      {
        "paperId": "edee5fcc2e261bdf7ae5d958c53b9270174ff391",
        "title": "Jigsaw Unintended Bias in Toxicity Classification"
      }
    ],
    "cited_by": [
      {
        "paperId": "7e3052358519a9c211eec305b7074c061d42c669",
        "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards"
      },
      {
        "paperId": "c4401e63b7876c283025faea0cfeab49d588b0e1",
        "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement"
      },
      {
        "paperId": "ab3d2665db68fa9471c62e468b79a9090e0ab965",
        "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation"
      },
      {
        "paperId": "c5e8e0c86d1df2131be6712d4c702edfa3c9d420",
        "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions"
      },
      {
        "paperId": "17d25128b18cadc7cd88b7d7d18513087537612b",
        "title": "Preference Learning for AI Alignment: a Causal Perspective"
      },
      {
        "paperId": "08a1d386ff57361378661a2703ce041a9f66d179",
        "title": "ITMPRec: Intention-based Targeted Multi-round Proactive Recommendation"
      },
      {
        "paperId": "517ae624b59df997c9f98b2d13697c02cccd1d13",
        "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents"
      },
      {
        "paperId": "7b0a443058e97594677172239f1e4bd7fd60e0a6",
        "title": "A Survey on (M)LLM-Based GUI Agents"
      },
      {
        "paperId": "974080d7755dfd40d3105c0319e3b25b7150eb72",
        "title": "Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models"
      },
      {
        "paperId": "6b9247d835149ac0367332abcd79c95af2e933a3",
        "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare"
      },
      {
        "paperId": "29a1a23cf487c69746e4ce428e668266b07e6eb7",
        "title": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs"
      },
      {
        "paperId": "bf2f25c96ff0a280d89ba0c6c7c1c7e66eb53bdb",
        "title": "Large Language Models and Causal Inference in Collaboration: A Survey"
      }
    ],
    "score": 12.0
  },
  {
    "id": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
    "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond",
    "authors": [
      "Hao Sun"
    ],
    "year": 2023,
    "citationCount": 24,
    "abstract": "Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research. Highlighted Takeaways: 1. RLHF is Online Inverse RL with Offline Demonstration Data. 2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error. 3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive. 4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity. 5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.",
    "url": "https://www.semanticscholar.org/paper/f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
    "pdf_url": "https://arxiv.org/pdf/2310.06147.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-10-09",
    "externalIds": {
      "ArXiv": "2310.06147",
      "DBLP": "journals/corr/abs-2310-06147",
      "DOI": "10.48550/arXiv.2310.06147",
      "CorpusId": 263830826
    },
    "references": [
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "d2170504c4ad9403bea118ae8debdfda95978546",
        "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "7f712d58084e32ddc1b0cd60932f8bc0a0916330",
        "title": "Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL"
      },
      {
        "paperId": "a7d58bd29778ef0d15b9e9e3eb2f37a8cf1ea70c",
        "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "16ee2bd9519405f8a1e0a9fa21a8ae8778d0b957",
        "title": "Safe Exploration by Solving Early Terminated MDP"
      },
      {
        "paperId": "3f673101c2cac3b47639056e2988e018546c3c90",
        "title": "Zeroth-Order Supervised Policy Improvement"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "aa935f1753db47e899e1305d7fa0d0de6af81e51",
        "title": "Strictly Batch Imitation Learning by Energy-based Distribution Matching"
      },
      {
        "paperId": "99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45",
        "title": "Novel Policy Seeking with Constrained Optimization"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "c92780cd2c90b0393efb5d5b3a49b1ec9503df11",
        "title": "Policy Continuation with Hindsight Inverse Dynamics"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
        "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "ce1c28ca2f52a42c6e60d792cd71ba894abc47d5",
        "title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "d0352057e2b99f65f8b5244a0b912026c86d7b21",
        "title": "Equivalence Between Policy Gradients and Soft Q-Learning"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "06116b70e27d60da93329c2be14ab1101c61175f",
        "title": "Exploit Reward Shifting in Value-Based Deep-RL: Optimistic Curiosity-Based Exploration and Conservative Exploitation via Linear Reward Shaping"
      },
      {
        "paperId": "6cdace8c7b9e5b0fe3a532b75739b5b55f8b97b2",
        "title": "Combining Policy Gradient and Q-Learning"
      },
      {
        "paperId": null,
        "title": "Deepmimic: Example-guided deep reinforcement learning of physics-based character skills"
      },
      {
        "paperId": null,
        "title": "OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems"
      },
      {
        "paperId": null,
        "title": "PPO has a faster wall-clock training time compared to off-policy methods like DQN [27]"
      },
      {
        "paperId": null,
        "title": "Accountable batched control with decision corpus"
      }
    ],
    "cited_by": [
      {
        "paperId": "c613840c8c5a71a7635902b46d719bf08c4a4968",
        "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis"
      },
      {
        "paperId": "590e37772a4e2b6cd10d26d0f37ba4b290525e99",
        "title": "Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity"
      },
      {
        "paperId": "6126eb349c6ed459059c71b299555a3f03bd2c0e",
        "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens"
      },
      {
        "paperId": "55c7dbd73e70e4c50b7244444472f24508a1eb17",
        "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind"
      },
      {
        "paperId": "f9122649b5e0c4c6a8406499ad293cf247202ebe",
        "title": "MARCO: Multi-Agent Code Optimization with Real-Time Knowledge Integration for High-Performance Computing"
      },
      {
        "paperId": "96ccdfdcfe461a4a86c190c0a6cbdb6886c96103",
        "title": "Improving Merge Sort and Quick Sort Performance by Utilizing Alphadev\u2019s Sorting Networks as Base Cases"
      },
      {
        "paperId": "ab72ab40e2baeea8a57d1db386737239d8e07397",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models"
      },
      {
        "paperId": "a84da5fa6720df1e3e1833f9fb93d14efdfa9da9",
        "title": "Cross-Domain Integration for General Sensor Data Synthesis: Leveraging LLMs and Domain-Specific Generative Models in Collaborative Environments"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "8e35a4ac3378442c71d2b74b4958ca92c725eaa5",
        "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models"
      },
      {
        "paperId": "6c0b812f83809a7ad69baa4e2509150c0c380356",
        "title": "Exploring Prompt Engineering: A Systematic Review with SWOT Analysis"
      },
      {
        "paperId": "efde9b83da15ecbf4b005971a2e781fb5453fdcf",
        "title": "Towards Symbiotic Recommendations: Leveraging LLMs for Conversational Recommendation Systems"
      },
      {
        "paperId": "57451ce18f3035fcadf64db38420434f9299b7f3",
        "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF"
      },
      {
        "paperId": "be2f3ad5d46ab7fe22e66301ec7149abe40b9da6",
        "title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "9637ef9019671034912ea0f506ae67c3f2fc4689",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      },
      {
        "paperId": "c362015d426c90ec01e1ad02bf3fd66ab8fd0fd9",
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models"
      },
      {
        "paperId": "2a8b65910598873b8125fffb73cee7c1510e4b35",
        "title": "When is Off-Policy Evaluation (Reward Modeling) Useful in Contextual Bandits? A Data-Centric Perspective"
      },
      {
        "paperId": "08d4743f8510297849a782684515e149b38e0993",
        "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption"
      },
      {
        "paperId": "a173b47ee90fb18948d70126f4d40b5561b7d932",
        "title": "SELF: Self-Evolution with Language Feedback"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "85e27aec760b3d16ab779e335fc195bc578d0840",
        "title": "What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization"
      },
      {
        "paperId": "34a133ee7551145060cb20b19176f84f22914521",
        "title": "SELF: Language-Driven Self-Evolution for Large Language Model"
      },
      {
        "paperId": "a403d59b1b401fb98984f23a679b245edd4863c6",
        "title": "Performance Evaluation of Large Language Models for High-Performance Code Generation: A Multi-Agent Approach (MARCO)"
      }
    ],
    "score": 12.0
  },
  {
    "id": "40cc085a2608985b753c38dc245ac21be592ed08",
    "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback",
    "authors": [
      "Ang Li",
      "Qiugen Xiao",
      "Peng Cao",
      "Jian Tang",
      "Yi Yuan",
      "Zijie Zhao",
      "Xiaoyuan Chen",
      "Liang Zhang",
      "Xiangyang Li",
      "Kaitong Yang",
      "Weidong Guo",
      "Yukang Gan",
      "Jeffrey Xu Yu",
      "D. Wang",
      "Ying Shan"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model's harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\\% in satisfaction rate after basic RLAIF.",
    "url": "https://www.semanticscholar.org/paper/40cc085a2608985b753c38dc245ac21be592ed08",
    "pdf_url": "https://arxiv.org/pdf/2403.08309.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-13",
    "externalIds": {
      "ArXiv": "2403.08309",
      "DBLP": "journals/corr/abs-2403-08309",
      "DOI": "10.48550/arXiv.2403.08309",
      "CorpusId": 268379328
    },
    "references": [
      {
        "paperId": "5160224f7daf64fd490ed6d517bef316e383a311",
        "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
      },
      {
        "paperId": "c01e43c65a04d766e429863bdf7cf65b895df20e",
        "title": "Chinese Open Instruction Generalist: A Preliminary Release"
      },
      {
        "paperId": "8fc90497d9043fdf35e71302b7c2e79bb907144f",
        "title": "Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
        "title": "Large Language Models Are Human-Level Prompt Engineers"
      },
      {
        "paperId": "023edab4738690444e3924e224c2641017a0d794",
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "19acfc211c9d8477f5a405ea5461fde20d6c813f",
        "title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "270b3f5201e835dd9a6a80fb8d749dba08dc88dd",
        "title": "Generating Empathetic Responses by Looking Ahead the User\u2019s Sentiment"
      },
      {
        "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
        "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "b122a828f5fee3c6afc54e70f41b00184d6383fc",
        "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset"
      },
      {
        "paperId": "a8e00fd699878bf8052964f913f8d883b7f77c37",
        "title": "The second stage."
      },
      {
        "paperId": null,
        "title": "Nlp chinese corpus: Large scale chi-nese corpus for nlp"
      },
      {
        "paperId": null,
        "title": "2022. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection"
      },
      {
        "paperId": "d7c0aae180dd9cf4c001cf811aadfbfc5b086263",
        "title": ": The Question of"
      },
      {
        "paperId": null,
        "title": "2023. Rrhf: Rank responses to align language models with"
      },
      {
        "paperId": null,
        "title": "2023. Starling-7b: Improving llm helpfulness & harmlessness with rlaif"
      },
      {
        "paperId": null,
        "title": "2023. Summary of chatgpt-related research and perspective towards the future of large language models"
      },
      {
        "paperId": null,
        "title": "2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "paperId": null,
        "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "paperId": null,
        "title": "2023. Firefly: Chinese conversational large language model"
      },
      {
        "paperId": null,
        "title": "2023. Moss: Training conversational language models from synthetic data"
      },
      {
        "paperId": null,
        "title": "2023. An empirical study of metrics to measure representational harms in pre-trained language models"
      },
      {
        "paperId": null,
        "title": "human feedback without tears"
      },
      {
        "paperId": null,
        "title": "Note that for Chinese prompts and responses, we will translate the following instructions into Chinese, aiming to maintain language consistency"
      },
      {
        "paperId": null,
        "title": "final calculated numerical value and examine the calculation process of the response"
      }
    ],
    "cited_by": [
      {
        "paperId": "a76fffe2e593137ee4b6d54b13bea42e5a5f7da6",
        "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "108af17d9285202c6851ced95bd8d7b628e5f72c",
        "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses"
      },
      {
        "paperId": "0ff903811333bf12b1694d83309316be16b73465",
        "title": "Reversing the logic of generative AI alignment: a pragmatic approach for public interest"
      },
      {
        "paperId": "3bc75de4ee6a437291c0a617fd2af8261f5df752",
        "title": "LLM Misalignment via Adversarial RLHF Platforms"
      },
      {
        "paperId": "99b049973f0cb477e50570b699f313e102ebd89f",
        "title": "Self Iterative Label Refinement via Robust Unlabeled Learning"
      },
      {
        "paperId": "d1d0e5ff34fa5daf3de70e24dfbfdfa919cb49c5",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems"
      },
      {
        "paperId": "77702dc45e9af19b287e9347cecc932e33cfd724",
        "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning"
      },
      {
        "paperId": "3c81187fe776dab26834b8f833f54fdf127a6a43",
        "title": "Self-Boosting Large Language Models with Synthetic Preference Data"
      },
      {
        "paperId": "f178aef789963f1d1686b8612bdf713c44c634cd",
        "title": "Self-rationalization improves LLM as a fine-grained judge"
      },
      {
        "paperId": "2530e6ecbd0198012bb8ee4359acb9241cefec95",
        "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models"
      }
    ],
    "score": 11.0
  },
  {
    "id": "e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
    "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation",
    "authors": [
      "Zhiyu Mei",
      "Wei Fu",
      "Kaiwei Li",
      "Guangju Wang",
      "Huanchen Zhang",
      "Yi Wu"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for empowering large language model (LLM) applications. Compared with the supervised training process of LLMs, the RLHF training process is much more sophisticated, requiring a diverse range of computation workloads with intricate dependencies between multiple LLM instances. Therefore, simply adopting the fixed parallelization strategies from supervised training for LLMs can be insufficient for RLHF and result in low training efficiency. To overcome this limitation, we propose a novel technique named parameter ReaLlocation, which dynamically adapts the parallelization strategies for different workloads during training by redistributing LLM parameters across the training cluster. Building upon this idea, we introduce ReaL, a pioneering system for efficient RLHF training. ReaL introduces the concept of an execution plan, which defines a fine-grained resource allocation and parallelization strategy particularly designed for RLHF training. Based on this concept, ReaL employs a tailored search algorithm with a lightweight run-time estimator to automatically discover an efficient execution plan for an instance of RLHF experiment. Subsequently, the runtime engine deploys the selected plan by effectively parallelizing computations and redistributing parameters. We evaluate ReaL on the LLaMA models with up to 70 billion parameters and 128 GPUs. The experimental results demonstrate that ReaL achieves speedups of up to $3.58\\times$ compared to baseline methods. Furthermore, the execution plans generated by ReaL exhibit an average of $81\\%$ performance improvement over heuristic approaches based on Megatron-LM in the long-context scenario. The source code of ReaL is publicly available at https://github.com/openpsi-project/ReaLHF .",
    "url": "https://www.semanticscholar.org/paper/e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
    "pdf_url": "https://arxiv.org/pdf/2406.14088.pdf",
    "venue": "",
    "publicationDate": "2024-06-20",
    "externalIds": {
      "ArXiv": "2406.14088",
      "CorpusId": 270619751
    },
    "references": [
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "afb06e773d9f073a885a095a8bbb5a5b761a3ab5",
        "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment"
      },
      {
        "paperId": "b6476f998c29577358942be9b25eb904632075ea",
        "title": "ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment"
      },
      {
        "paperId": "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c",
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "72f77a393079431e4207b3afe678ee80b420e6f8",
        "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      },
      {
        "paperId": "18a74a76d2ed55df12544c55c43458457a081fd5",
        "title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training"
      },
      {
        "paperId": "eb95c327260725498404eb43ec370d419b8d92c7",
        "title": "SGLang: Efficient Execution of Structured Language Model Programs"
      },
      {
        "paperId": "faa4c46e1cbd99e486c7dc2881e024b79967961b",
        "title": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
        "title": "Instruction Tuning for Large Language Models: A Survey"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "9f176eca4e0cd4d1f52f13879f328cefc11b56b4",
        "title": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores"
      },
      {
        "paperId": "f4bdec0cf595720bc8ee5df2196324bac8f52ab4",
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da",
        "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3",
        "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"
      },
      {
        "paperId": "a306401ef3fedb1eb671e367ecf9ff67349873d1",
        "title": "On Optimizing the Communication of Model Parallelism"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "72dd63d67588a42fc817bbb8d655b397f67425df",
        "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"
      },
      {
        "paperId": "774591fdd988eaaff3917e7c5171d044b0843e63",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
      },
      {
        "paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d",
        "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"
      },
      {
        "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
        "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d",
        "title": "DAPPLE: a pipelined data parallel approach for training large models"
      },
      {
        "paperId": "1882f194cb43828852cc052887671e55a80f945a",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "00c957711b12468cb38424caccdf5291bb354033",
        "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
      },
      {
        "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
        "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
      },
      {
        "paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8",
        "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"
      },
      {
        "paperId": "f971658ab845d7573c4bbb760d5e7e5332025254",
        "title": "Beyond Data and Model Parallelism for Deep Neural Networks"
      },
      {
        "paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd",
        "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "50684b147b752a07c313cb73d864f7b21bd8b703",
        "title": "Scaling Distributed Machine Learning with the Parameter Server"
      },
      {
        "paperId": "49974e01d030b3c6187b2b00769a048f842538ee",
        "title": "Simple Linux Utility for Resource Management"
      },
      {
        "paperId": "143d2e02ab91ae6259576ac50b664b8647af8988",
        "title": "Monte Carlo Sampling Methods Using Markov Chains and Their Applications"
      },
      {
        "paperId": "f6a13f116e270dde9d67848495f801cdb8efa25d",
        "title": "Equation of State Calculations by Fast Computing Machines"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "adf7b1ed6b4d4ae346888278b7496d55e8e3f13f",
        "title": "PUZZLE: Efficiently Aligning Large Language Models through Light-Weight Context Switch"
      },
      {
        "paperId": null,
        "title": "Antropic"
      },
      {
        "paperId": "9d7a75601e0e50dd68d40cfb8ef0e891dad797a6",
        "title": "Orca: A Distributed Serving System for Transformer-Based Generative Models"
      },
      {
        "paperId": null,
        "title": "Introducing chatgpt"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": null,
        "title": "Automating inter-and intra-operator parallelism for distributed deep learning"
      },
      {
        "paperId": null,
        "title": "Scaling language modeling with pathways"
      },
      {
        "paperId": null,
        "title": "Nvidia"
      },
      {
        "paperId": null,
        "title": "requires predictable function calls to ensure the validity of cost estimation. An unstable cluster or dynamic workflow"
      },
      {
        "paperId": null,
        "title": "Gemini: A family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "The searching of R EA L does not guarantee optimality despite producing plans that are fast and efficient in practice"
      }
    ],
    "cited_by": [
      {
        "paperId": "c99eb27ac56a3d201e8402b7264704e8a09dfc16",
        "title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL"
      },
      {
        "paperId": "618d58b52841a741487215cb04ace9d6b46e7e5a",
        "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster"
      },
      {
        "paperId": "8c5dd649a96a8b8fa3c7cc2dd40919f34280036b",
        "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once"
      },
      {
        "paperId": "17643d3dbcec601be970ef0dbf86297d2c9f8111",
        "title": "Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning"
      },
      {
        "paperId": "f7b400cab655e820a38348fd1c933ab3036b29f9",
        "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning"
      },
      {
        "paperId": "37f70927b32fbbbde8e4529a1137607245c1774d",
        "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL"
      },
      {
        "paperId": "dfa3b27662b5d6cca57332249b8dd4ee39d57794",
        "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions"
      },
      {
        "paperId": "d85788857fd230169e17638631b96335368043ed",
        "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
      },
      {
        "paperId": "043731dd9f2292b7e4788d0b97edee5db170f840",
        "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
      },
      {
        "paperId": "4039ac3b8eee6e1ce39d204e57856432e6802c8f",
        "title": "Optimizing RLHF Training for Large Language Models with Stage Fusion"
      },
      {
        "paperId": "4e68cc2d6cb0c60883307de05ac248f89cd51b6d",
        "title": "Efficient Training of Large Language Models on Distributed Infrastructures: A Survey"
      }
    ],
    "score": 11.0
  },
  {
    "id": "bcf2bd95a6f60dd2998b57c26873d31461011e8d",
    "title": "REvolve: Reward Evolution with Large Language Models for Autonomous Driving",
    "authors": [
      "Rishi Hazra",
      "Alkis Sygkounas",
      "A. Persson",
      "Amy Loutfi",
      "Pedro Zuidberg Dos Martires"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/bcf2bd95a6f60dd2998b57c26873d31461011e8d",
    "pdf_url": "https://doi.org/10.48550/arXiv.2406.01309",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-01309",
      "DOI": "10.48550/arXiv.2406.01309",
      "CorpusId": 270211241
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "9424bd6714d70b6199e330fcb23ccfaa80980e36",
        "title": "Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving"
      },
      {
        "paperId": "bb38ab0012afec86750d0f6f7976465e9544a54d",
        "title": "Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition"
      },
      {
        "paperId": "36cf20d3650c5206454b5b8ff595c6ef024718e9",
        "title": "Urban Computing in the Era of Large Language Models"
      },
      {
        "paperId": "b49cbccd88cacf9a7514c80d33cf5c42b80edc1a",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation With Large Language Models"
      },
      {
        "paperId": "506b7782dc79633d49bcb8659af395b04ec2c8cb",
        "title": "On Robust Context-Aware Navigation for Autonomous Ground Vehicles"
      },
      {
        "paperId": "32ab97e281984a1806d5c9f6999929b98df3626d",
        "title": "From Words to Wheels: Automated Style-Customized Policy Generation for Autonomous Driving"
      },
      {
        "paperId": "09404e23a11b674a0e82830d6d3d1319f3d47150",
        "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards"
      },
      {
        "paperId": "9d243b955f2253a563c438d8449e1c468944a545",
        "title": "Can Large Language Models Reason? A Characterization via 3-SAT"
      },
      {
        "paperId": "84d99893ee24fc825e359598d44d602c45c4865e",
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
      },
      {
        "paperId": "661f92a7ac2d884b21ed36eef7e712e24556f61e",
        "title": "Prioritization Strategies for LLM-Designed Restless Bandit Rewards in Public Health"
      }
    ],
    "score": 11.0
  },
  {
    "id": "e6fac5811e260466366f3a905076c33e252405ef",
    "title": "Optimal Design for Reward Modeling in RLHF",
    "authors": [
      "Antoine Scheid",
      "Etienne Boursier",
      "A. Durmus",
      "Michael I. Jordan",
      "Pierre M'enard",
      "\u00c9ric Moulines",
      "Michal Valko"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a popular approach to align language models (LMs) with human preferences. This method involves collecting a large dataset of human pairwise preferences across various text generations and using it to infer (implicitly or explicitly) a reward model. Numerous methods have been proposed to learn the reward model and align a LM with it. However, the costly process of collecting human preferences has received little attention and could benefit from theoretical insights. This paper addresses this issue and aims to formalize the reward training model in RLHF. We frame the selection of an effective dataset as a simple regret minimization task, using a linear contextual dueling bandit method. Given the potentially large number of arms, this approach is more coherent than the best-arm identification setting. We then propose an offline framework for solving this problem. Under appropriate assumptions - linearity of the reward model in the embedding space, and boundedness of the reward parameter - we derive bounds on the simple regret. Finally, we provide a lower bound that matches our upper bound up to constant and logarithmic terms. To our knowledge, this is the first theoretical contribution in this area to provide an offline approach as well as worst-case guarantees.",
    "url": "https://www.semanticscholar.org/paper/e6fac5811e260466366f3a905076c33e252405ef",
    "pdf_url": "https://arxiv.org/pdf/2410.17055.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-22",
    "externalIds": {
      "ArXiv": "2410.17055",
      "DBLP": "journals/corr/abs-2410-17055",
      "DOI": "10.48550/arXiv.2410.17055",
      "CorpusId": 273507636
    },
    "references": [
      {
        "paperId": "8096ca5f6895955dc41f05094f976b76419437fd",
        "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison"
      },
      {
        "paperId": "40088ba6b159deaa96497a879110b63842383c9a",
        "title": "Off-Policy Evaluation from Logged Human Feedback"
      },
      {
        "paperId": "79e41008d1837acc27029d7c5061898385c5ea3c",
        "title": "OPTune: Efficient Online Preference Tuning"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "03015ee9762881cfed0f01330b3511e3bca1b291",
        "title": "Nearly Minimax Optimal Regret for Multinomial Logistic Bandit"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "bd3e3e6bc4510e3be74a8f0591d407c56c40d2bc",
        "title": "Optimal Design for Human Preference Elicitation"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "ee43fc5579deac342b91576d3e443e95cee281ed",
        "title": "Is Cosine-Similarity of Embeddings Really About Similarity?"
      },
      {
        "paperId": "ede28224a1d6633826198d1702a5a737ea236575",
        "title": "Reinforcement Learning from Human Feedback with Active Queries"
      },
      {
        "paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "7508634ac1312a7a975cbdf06fe754db2a1a3c09",
        "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration"
      },
      {
        "paperId": "fb8cfd2cd28318f8fe1c38cf1d5a65bd4758b6d3",
        "title": "Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "36791002637c2c7fcdd13e6b95c463acaea1b10d",
        "title": "Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits"
      },
      {
        "paperId": "f657ae374ecdde4761ab52e58a2bf1a0a5f3097f",
        "title": "RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "title": "A Comprehensive Overview of Large Language Models"
      },
      {
        "paperId": "a35f1315e91513ff0bec0c488fe175214fd9636c",
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "2c5460afa19ad6fc2568b7e210115acacc14a40c",
        "title": "An Overview on Language Models: Recent Developments and Outlook"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "0a0f09d57ba4b5451e5c0d11397b8e437c38e9b9",
        "title": "Human Preferences as Dueling Bandits"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "9ba7b24cac6243348563ec5be8ca15f4088ba7a1",
        "title": "Stochastic Contextual Dueling Bandits under Linear Stochastic Transitivity Models"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "ace5ec3f2cca098e54f82e6fa4968970c4664a03",
        "title": "Teaching an Active Learner with Contrastive Examples"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "paperId": "470d0d42303ba8e747aaed3bb88de756196b8917",
        "title": "Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "b93754f22531d1b1fa14e003578ef2a86f50db17",
        "title": "Gamification of Pure Exploration for Linear Bandits"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "5fa14f5d09031736a70bca9675f888dbeab644a3",
        "title": "Improved Optimistic Algorithms for Logistic Bandits"
      },
      {
        "paperId": "9f437593868e04fcc2dc4b6c1b5e953588094d57",
        "title": "Old Dog Learns New Tricks: Randomized UCB for Bandit Problems"
      },
      {
        "paperId": "216366d3a1c32f641f9002ff422c5068040c455c",
        "title": "Preference-Based Learning for Exoskeleton Gait Optimization"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "title": "Parameter-Efficient Transfer Learning for NLP"
      },
      {
        "paperId": "4cadfbf4cc89db0af4ffee6fce0da3d55efa9756",
        "title": "Advancements in Dueling Bandits"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "9e94f2fb4afb9b47be34368ce20266363ca0b5c3",
        "title": "Multidimensional Binary Search for Contextual Decision-Making"
      },
      {
        "paperId": "b75dd3eeced4b714f8e34c151f37046664ccc5a4",
        "title": "Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "c769c26570ee125bf8e439ede182deeb2a1d690c",
        "title": "Best-Arm Identification in Linear Bandits"
      },
      {
        "paperId": "d4a145888d2724843213b818367138dee5465df0",
        "title": "Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "582a47a444c46aa7206283803ce4cf9fa837952f",
        "title": "Parametric Bandits: The Generalized Linear Case"
      },
      {
        "paperId": "7523c6f65774b4a3e44b1ea51bcb565800f74480",
        "title": "Self-concordant analysis for logistic regression"
      },
      {
        "paperId": "1a4e67d3705492d0af88623b0e62818a16084fca",
        "title": "The Analysis of Permutations"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "Provably sample efficient rlhf via active preference optimization"
      },
      {
        "paperId": "628bb183ba5496318c25ee2e1a4a3391bfccb8c7",
        "title": "Algorithmic Foundations for Safe and Efficient Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "d39a902effe6873746868ca44860be8f0d13ff8b",
        "title": "Optimal Algorithms for Stochastic Contextual Preference Bandits"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": null,
        "title": "Estimation des densit\u00b4es: risque minimax"
      },
      {
        "paperId": null,
        "title": "reward modeling to online rlhf"
      }
    ],
    "cited_by": [
      {
        "paperId": "1e8a109a4155dc36d0ed940bf1e344e505c3fd18",
        "title": "FraPPE: Fast and Efficient Preference-based Pure Exploration"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "4c63722f98a282f3e055f33993ecdd46ad7f36aa",
        "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design"
      },
      {
        "paperId": "fe54c92cb5f7c2c68847354af8770dd8df5f7088",
        "title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection"
      },
      {
        "paperId": "98e1ddef8ce9b35156f18688441f0bc699738a38",
        "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "ebf48387c56d0157fe29c888d25c40a36521bc1b",
        "title": "Reasoning without Regret"
      },
      {
        "paperId": "4fded2e45d10131c2d6a59ffcfcb4aaadfc98c2c",
        "title": "Active Learning for Direct Preference Optimization"
      },
      {
        "paperId": "a0f2f14afbf7b077d519bf7ca74f1e1d061a7d5a",
        "title": "Active RLHF via Best Policy Learning from Trajectory Preference Feedback"
      },
      {
        "paperId": "d1d0e5ff34fa5daf3de70e24dfbfdfa919cb49c5",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems"
      },
      {
        "paperId": "f74ef807b179a22c324e874fe69bf642fe753429",
        "title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees"
      }
    ],
    "score": 11.0
  },
  {
    "id": "aece81d7dcbf2929e650a6094af63666e95a0c83",
    "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
    "authors": [
      "Yingshui Tan",
      "Yilei Jiang",
      "Yanshi Li",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Wenbo Su",
      "Xiangyu Yue",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness.",
    "url": "https://www.semanticscholar.org/paper/aece81d7dcbf2929e650a6094af63666e95a0c83",
    "pdf_url": "https://arxiv.org/pdf/2502.11555.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-17",
    "externalIds": {
      "ArXiv": "2502.11555",
      "DBLP": "journals/corr/abs-2502-11555",
      "DOI": "10.48550/arXiv.2502.11555",
      "CorpusId": 276409201
    },
    "references": [
      {
        "paperId": "b4ce20013e8fa416728d6960f19220b4a00809aa",
        "title": "RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting"
      },
      {
        "paperId": "131a13c60f179511572abc81d6bd6aa988e96854",
        "title": "Rule Based Rewards for Language Model Safety"
      },
      {
        "paperId": "9123ec44f0026e70f8398b904e97a4224866bb36",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      },
      {
        "paperId": "8ba5d42e303b429ad3f160e2eb035635a0b18dbe",
        "title": "WildChat: 1M ChatGPT Interaction Logs in the Wild"
      },
      {
        "paperId": "4c2d8df556589ff4fbb5ee68c1f45bff3786624f",
        "title": "Aligner: Efficient Alignment by Learning to Correct"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "4c8e74bb70a15b3a2cf1064e2c57ce6730a3d344",
        "title": "Peer review of GPT-4 technical report and systems card"
      },
      {
        "paperId": "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
        "title": "Safety of Large Language Models in Addressing Depression"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "709af143f78bc62413c50ea1a7ee75b0702c4f59",
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8",
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"
      },
      {
        "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "0983883619a0ca597d055d0e58da2f514052913d",
        "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a",
        "title": "On the Planning Abilities of Large Language Models - A Critical Investigation"
      },
      {
        "paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
        "title": "Large language models encode clinical knowledge"
      },
      {
        "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "title": "Ethical and social risks of harm from Language Models"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "1b07a24b81834116f6ad1d0232485ba81b9445f3",
        "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
      },
      {
        "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
      },
      {
        "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
        "title": "VQA: Visual Question Answering"
      },
      {
        "paperId": "c85bffaaf7be0b2d3b3f292afa0623a301f717b0",
        "title": "Perfect recovery and sensitivity analysis of time encoded bandlimited signals"
      },
      {
        "paperId": "ce069662a2285900c788099750ec82b59b2405bc",
        "title": "Direct self-control (DSC) of inverter-fed induction machine"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "7daf37aac345f53aa231c946f81c5378a277d74d",
        "title": "LLM-Based Empathetic Response Through Psychologist-Agent Debate"
      },
      {
        "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "title": "An Adversarial Winograd Schema Challenge at Scale"
      },
      {
        "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
      },
      {
        "paperId": "0bb1e72b3fc3d1fc06ba7e9e5f4f4697d0fd5ecb",
        "title": "CMOS Schmitt trigger design"
      },
      {
        "paperId": null,
        "title": "2022. Red team-ing language models with language models"
      },
      {
        "paperId": null,
        "title": "2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      },
      {
        "paperId": null,
        "title": "16: Prompt to synthesize harmful data: generate harmful prompt from collected harmful entities"
      },
      {
        "paperId": null,
        "title": "2024. Visual instruction tuning"
      },
      {
        "paperId": null,
        "title": "The following prompt is a simplified version. The full prompt , which includes the design of risk mechanisms , cannot be disclosed at this time due to commercial confidentiality principles"
      },
      {
        "paperId": null,
        "title": "The instruction contains an intentional risk: It instructs the model to generate content insinuating political figures , which is a negative and risky intention"
      },
      {
        "paperId": null,
        "title": "2023a. Voyager: An open-ended embodied agent with large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "1ab15d6c00a2784030c0dc61a4e6813c1bb8cc69",
        "title": "Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI"
      },
      {
        "paperId": "c97b415cddcd5e3349e4fdaeceef571c5be23751",
        "title": "UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases"
      },
      {
        "paperId": "b88cc13aa6176b87975a148e66260842309dd806",
        "title": "Learning Efficient Robotic Garment Manipulation with Standardization"
      },
      {
        "paperId": "fd53f44b51906199bdcd05131b9a143039f60026",
        "title": "MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models"
      },
      {
        "paperId": "0f3db86cd70b03155fb63a8308e2749573f76c33",
        "title": "Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025"
      },
      {
        "paperId": "416908dee133533bc8a08f7507718c5697190467",
        "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints"
      },
      {
        "paperId": "41dde30ecea56969004941260ef5badbccbb3863",
        "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models"
      },
      {
        "paperId": "680aeb24d20a4f2d10e45b9aa7f37b9a19a0e394",
        "title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models"
      },
      {
        "paperId": "390c85a27878efb0cce801b64c26873b4b50bea4",
        "title": "Get Experience from Practice: LLM Agents with Record & Replay"
      },
      {
        "paperId": "bcef6be34620755da525c893d84a1feb2713eed4",
        "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach"
      }
    ],
    "score": 10.0
  },
  {
    "id": "a19b75036dd95bc6eba87c1589de3b2dff5c25a1",
    "title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?",
    "authors": [
      "Yufei He",
      "Yuexin Li",
      "Jiaying Wu",
      "Yuan Sui",
      "Yulin Chen",
      "Bryan Hooi"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is \\textit{instrumental convergence}, where an AI system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning (RL)-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in LLMs by comparing models trained with direct RL optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback (RLHF). We hypothesize that RL-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce InstrumentalEval, a benchmark for evaluating instrumental convergence in RL-trained LLMs. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence. Our findings contribute to a deeper understanding of alignment challenges in AI systems and the risks posed by unintended model behaviors.",
    "url": "https://www.semanticscholar.org/paper/a19b75036dd95bc6eba87c1589de3b2dff5c25a1",
    "pdf_url": "https://arxiv.org/pdf/2502.12206.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-16",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-12206",
      "ArXiv": "2502.12206",
      "DOI": "10.48550/arXiv.2502.12206",
      "CorpusId": 276421661
    },
    "references": [
      {
        "paperId": "fdb34bfef895ed0ca26bedd304244e554ec5254f",
        "title": "UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs"
      },
      {
        "paperId": "af659592c2bc43309aaf856eacfeadebeb421427",
        "title": "Frontier Models are Capable of In-context Scheming"
      },
      {
        "paperId": "89fccb4b70d0a072d9c874dddfab0afb3676d1b8",
        "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering"
      },
      {
        "paperId": "7c44b7fdcec2e517799f6c54f6ba42bf1a89d2e6",
        "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering"
      },
      {
        "paperId": "a2dd8b031dfd71e27fb062685589208744197902",
        "title": "Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-Training on Industrial-Scale Data"
      },
      {
        "paperId": "f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
        "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering"
      },
      {
        "paperId": "fbdae06c37f67868983f4ba419cbbb9402c31e27",
        "title": "UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed Graphs"
      },
      {
        "paperId": "7142e920b6b9355d9cbacc9450818f912eca138e",
        "title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "bb435dcd5fa7c7a7112af9adcb58f23b87ef28ac",
        "title": "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "cdf24e52057b9a5aaf533482aaf3025cf61b7eb4",
        "title": "SCR: Training Graph Neural Networks with Consistency Regularization"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "3d62e6bc0ad0f13370cb2f5144a2c9806e37b9c9",
        "title": "An overview of 11 proposals for building safe advanced AI"
      },
      {
        "paperId": "69c81d16cc89a47a2a576e6cdafb49f54f6dedd8",
        "title": "Human compatible: AI and the problem of control. Stuart Russell. New York, Viking, 2019, 352\u00a0pages"
      },
      {
        "paperId": "00d385a359eda4845dab37efc7c12a9c0987e66b",
        "title": "Ethical Issues in Advanced Artificial Intelligence"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "7aa70e2c12c8ba2dcc828893adb8bb56e3766726",
        "title": "Artificial Intelligence, Values, and Alignment"
      },
      {
        "paperId": "7ee12d3bf8e0ce20d281b4550e39a1ee53839452",
        "title": "Risks from Learned Optimization in Advanced Machine Learning Systems"
      },
      {
        "paperId": "627fc90f9bc87faef48f817136a8b3699a146fc4",
        "title": "Human-aligned artificial intelligence is a multiobjective problem"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "d7b321d8d88381a2a84e9d6e8f8f34ee2ed65df2",
        "title": "Formalizing Convergent Instrumental Goals"
      },
      {
        "paperId": "6c25aae58187f716d1b6db34200bbf3b63007aeb",
        "title": "The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents"
      },
      {
        "paperId": "a6582abc47397d96888108ea308c0168d94a230d",
        "title": "The Basic AI Drives"
      },
      {
        "paperId": null,
        "title": "2025. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning"
      },
      {
        "paperId": null,
        "title": "2024. Gpt-4o system card"
      },
      {
        "paperId": null,
        "title": "We provide some examples of instances where a model exhibits instrumental convergence behaviors"
      },
      {
        "paperId": null,
        "title": "focused and relevant responses while providing sufficient space for the model to demonstrate potential instrumental convergence behaviors"
      },
      {
        "paperId": null,
        "title": "2022. Sgkd: A scalable and effective knowledge distillation framework for graph representation learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "da19f0b95244eb45336dec821f47435027f65c25",
        "title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents"
      },
      {
        "paperId": "a6524cf164d67233d03d5acff37dd415cac72fbe",
        "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints"
      },
      {
        "paperId": "7e719493daa836347de89e8b6b505e389b901524",
        "title": "Mitigating Deceptive Alignment via Self-Monitoring"
      },
      {
        "paperId": "712e08695ec807163afbbfd9f442ab027ccc29ac",
        "title": "Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction"
      },
      {
        "paperId": "f4e49f0cc301260e800575c87ad1b10c903b4c9d",
        "title": "AI Awareness"
      },
      {
        "paperId": "3a676409c33683540dafd473e602f27f2b4ad299",
        "title": "Safety in Large Reasoning Models: A Survey"
      },
      {
        "paperId": "2f066326c0d5e5ecd7dc646224272f1e13948579",
        "title": "Efficient Inference for Large Reasoning Models: A Survey"
      },
      {
        "paperId": "20723b734e2838c22f921a0fc343d32e1d6d84e7",
        "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?"
      },
      {
        "paperId": "c0f7b6280244cded31e48f10957d6e62d506cd98",
        "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger"
      },
      {
        "paperId": "f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
        "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering"
      }
    ],
    "score": 10.0
  },
  {
    "id": "4376e282954ec59eaeca345ce4ec99219a075670",
    "title": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization",
    "authors": [
      "Wenyuan Xu",
      "Xiaochen Zuo",
      "Chao Xin",
      "Yu Yue",
      "Lin Yan",
      "Yong-Xu Wu"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior.",
    "url": "https://www.semanticscholar.org/paper/4376e282954ec59eaeca345ce4ec99219a075670",
    "pdf_url": "https://arxiv.org/pdf/2504.04950.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-04-07",
    "externalIds": {
      "DBLP": "journals/corr/abs-2504-04950",
      "ArXiv": "2504.04950",
      "DOI": "10.48550/arXiv.2504.04950",
      "CorpusId": 277621993
    },
    "references": [
      {
        "paperId": "fd950d00835ccb044b13267ae52725071d5de17e",
        "title": "KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks"
      },
      {
        "paperId": "1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
        "title": "Instruction-Following Evaluation for Large Language Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "04112b3e6cff833bb7e2fef9c5abb16d65c66fe6",
        "title": "Policy"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": null,
        "title": "Holistic and contamination"
      },
      {
        "paperId": null,
        "title": "Reward shaping to mitigate reward"
      },
      {
        "paperId": null,
        "title": "big-bench"
      }
    ],
    "cited_by": [
      {
        "paperId": "e10bccf89eb4f3a8613c42221f41c3db8cc210c1",
        "title": "RewardDance: Reward Scaling in Visual Generation"
      },
      {
        "paperId": "359742b9f6d7d0fd4c4642238e48e9c8c25c3768",
        "title": "REACT: REinforcement Learning-Based Adaptive ECG Anonymization and Privacy Threat Mitigation*"
      },
      {
        "paperId": "c33e8d1dbcc41a8f91579f60fd01f2f0231d0ec3",
        "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier"
      },
      {
        "paperId": "843422813fb73d5e677ac49e962f76c47843f9b1",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards"
      },
      {
        "paperId": "84ac4173ac8cd7cfea0262ceedc0796d0ea4b0e2",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency"
      },
      {
        "paperId": "86371cfe3943cf9113bbdb33a8b05ba31bd12547",
        "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference"
      },
      {
        "paperId": "e5f56092cbe315ef0177c3e24332f3d2a07fac18",
        "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "94a5613ece3c6e94e56b2701001a0a4a6f856075",
        "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning"
      },
      {
        "paperId": "00b39319f2dab85e7e338201a675f8c6dca2dfe8",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards"
      }
    ],
    "score": 10.0
  },
  {
    "id": "3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3",
    "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs",
    "authors": [
      "Jack Chen",
      "Fazhong Liu",
      "Naruto Liu",
      "Yuhan Luo",
      "Erqu Qin",
      "Harry Zheng",
      "Tian Dong",
      "Haojin Zhu",
      "Yan Meng",
      "Xiao Wang"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models'reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.",
    "url": "https://www.semanticscholar.org/paper/3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3",
    "pdf_url": "https://arxiv.org/pdf/2505.13026.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-19",
    "externalIds": {
      "ArXiv": "2505.13026",
      "DBLP": "journals/corr/abs-2505-13026",
      "DOI": "10.48550/arXiv.2505.13026",
      "CorpusId": 278740779
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "86cb80f627e867a9a47ff890eed11e4807d3d7fa",
        "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems"
      },
      {
        "paperId": "9585630fb72fdedd697dbce0fee2b4b2c2a2c3f3",
        "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective"
      },
      {
        "paperId": "6b8b9c975eee4c5bf81a805bb8bf717d58fe2b65",
        "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs"
      },
      {
        "paperId": "ba1e9567cb6f9707bbb5cc41590841e3aee8a7c2",
        "title": "Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration"
      },
      {
        "paperId": "804039e844664365265dca969ae612d32a992503",
        "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting"
      },
      {
        "paperId": "1ae10be82a6c09b6d53b6932d764f6f34bbeb9e4",
        "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
      },
      {
        "paperId": "da7f0bf97dd26cfc51e928abe3c461137982980e",
        "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"
      },
      {
        "paperId": "b43f8bacd80f32734cdff4f9d8c79e397872aed6",
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"
      },
      {
        "paperId": "7d291643bf9bfa74e3f1e8b014103dc81c85e361",
        "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling"
      },
      {
        "paperId": "b84a918a147f5f1d216d5b241384cfd4f2d9dfb3",
        "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning"
      }
    ],
    "score": 10.0
  },
  {
    "id": "85a1f32e4794b4c176f3330364bc39977a50d258",
    "title": "Aligning Neural Machine Translation Models: Human Feedback in Training and Inference",
    "authors": [
      "Miguel Moura Ramos",
      "Patrick Fernandes",
      "Ant\u00f3nio Farinhas",
      "Andr'e F. T. Martins"
    ],
    "year": 2023,
    "citationCount": 19,
    "abstract": "Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate.A core ingredient in RLHF\u2019s success in aligning and improving large language models (LLMs) is its \\textit{reward model}, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using \\textit{minimum Bayes risk} decoding and reranking have succeeded in improving the final quality of translation.In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach.Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality.Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.",
    "url": "https://www.semanticscholar.org/paper/85a1f32e4794b4c176f3330364bc39977a50d258",
    "pdf_url": "https://arxiv.org/pdf/2311.09132.pdf",
    "venue": "European Association for Machine Translation Conferences/Workshops",
    "publicationDate": "2023-11-15",
    "externalIds": {
      "ArXiv": "2311.09132",
      "DBLP": "journals/corr/abs-2311-09132",
      "ACL": "2024.eamt-1.22",
      "DOI": "10.48550/arXiv.2311.09132",
      "CorpusId": 265213141
    },
    "references": [
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "b7bf8fb8de6359b3b3273502f62f49d8df42911e",
        "title": "On the Limitations of Reference-Free Evaluations of Generated Text"
      },
      {
        "paperId": "c009a959dd236c162e51703e3bfd4d2b0b751c17",
        "title": "MAD for Robust Reinforcement Learning in Machine Translation"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "9a07e636caa30789cf2d956baadbb2433962a717",
        "title": "High Quality Rather than High Model Probability: Minimum Bayes Risk Decoding with Neural Metrics"
      },
      {
        "paperId": "a109b995dfeb444417f66545c67bce210bd11650",
        "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "af59aeeb46fb5d8412f550f6dd5c5dc99afc9c1a",
        "title": "Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"
      },
      {
        "paperId": "54086f44690caf6d82a06de5b76b89270d96a5eb",
        "title": "Unbabel\u2019s Participation in the WMT20 Metrics Shared Task"
      },
      {
        "paperId": "8bb84249b548e58494ed66efba86621449d49dc1",
        "title": "Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation"
      },
      {
        "paperId": "42fc352a0db1e742b0248a02b812db4aaf7b2cd3",
        "title": "Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "4ae52766028e69186052ea8f33a137fbbbdb986a",
        "title": "BLEURT: Learning Robust Metrics for Text Generation"
      },
      {
        "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "title": "Unsupervised Cross-lingual Representation Learning at Scale"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "4754993282f677e1b43407518cc7bfc665a8a37e",
        "title": "Facebook FAIR\u2019s WMT19 News Translation Task Submission"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "160563abbd75265b19afc8b4169bab9e1eb33d97",
        "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"
      },
      {
        "paperId": "c39dfd535b8172e557061ec8751fd2301c5783b7",
        "title": "Neural Machine Translation with Adequacy-Oriented Learning"
      },
      {
        "paperId": "0d3ca7be87de28144c110ecfe83311153e927820",
        "title": "Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection"
      },
      {
        "paperId": "f54b36edae733ab9cd7a748595947710bd28a2e3",
        "title": "A Study of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "15919637566348de8ca1e054151c24cc864b0f0e",
        "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"
      },
      {
        "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "title": "Hierarchical Neural Story Generation"
      },
      {
        "paperId": "d09e0187879c6dbaacb16c23a2dddb31d74b8b0b",
        "title": "On the Impact of Various Types of Noise on Neural Machine Translation"
      },
      {
        "paperId": "b887a39268ee41fea8d72f0ecd364eb72fe28a82",
        "title": "Analyzing Uncertainty in Neural Machine Translation"
      },
      {
        "paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
        "title": "Unsupervised Machine Translation Using Monolingual Corpora Only"
      },
      {
        "paperId": "5f18c39fceb231a535bd82550988a22e750d28ed",
        "title": "Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora"
      },
      {
        "paperId": "66144ddb0a7553cda3b36809bfe990dacc73e2cc",
        "title": "Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627",
        "title": "Six Challenges for Neural Machine Translation"
      },
      {
        "paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af",
        "title": "Findings of the 2016 Conference on Machine Translation"
      },
      {
        "paperId": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization"
      },
      {
        "paperId": "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6",
        "title": "Transfer Learning for Low-Resource Neural Machine Translation"
      },
      {
        "paperId": "9f2a8e923965b23c11066a2ead79658208f1fae1",
        "title": "Minimum Risk Training for Neural Machine Translation"
      },
      {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks"
      },
      {
        "paperId": "285c165c81fc9275955147a892b9a039ec8b1052",
        "title": "chrF: character n-gram F-score for automatic MT evaluation"
      },
      {
        "paperId": "feb420a4ac7c5719d51480053cd3e8669d5f2062",
        "title": "Findings of the 2015 Workshop on Statistical Machine Translation"
      },
      {
        "paperId": "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99",
        "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "paperId": "92d4a400a2e57c5ad421820b49b0954bca4500f1",
        "title": "Bilingual Data Cleaning for SMT using Graph-based Random Walk"
      },
      {
        "paperId": "7533d30329cfdbf04ee8ee82bfef792d08015ee5",
        "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
      },
      {
        "paperId": "e2a68774f92d1e894cbbbef2c819e4592990eb4b",
        "title": "Minimum Bayes-Risk Decoding for Statistical Machine Translation"
      },
      {
        "paperId": "c6e311a9640b47b9264e13de867ab6cc34d195db",
        "title": "Markov Decision Processes"
      },
      {
        "paperId": "1f12451245667a85d0ee225a80880fc93c71cc8b",
        "title": "Minimum Error Rate Training in Statistical Machine Translation"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": "5f0a1418494c5c8cc669fbf3a3a8395020122a2a",
        "title": "COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task"
      },
      {
        "paperId": "d9ca77e6d3c1618a45ed5fabba032bc401fb492a",
        "title": "Evaluating Corpus Cleanup Methods in the WMT\u201922 News Translation Task"
      },
      {
        "paperId": "4084a7605b87306b5d688d215b4fbfb4e55dd8b3",
        "title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU \u2013 Neural Metrics Are Better and More Robust"
      },
      {
        "paperId": "002f450563b22a0f7918e1a1ae4d5ccb31daf720",
        "title": "Selecting the Best Data Filtering Method for NMT Training"
      },
      {
        "paperId": "c47cac224ff59892abfd6af316b0f9e082f97012",
        "title": "Discriminative Reranking for Neural Machine Translation"
      },
      {
        "paperId": "74327f2d5ab7367667dad56e13858ff5ecdb7d81",
        "title": "Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment"
      },
      {
        "paperId": null,
        "title": "Hug-gingface\u2019s transformers: State-of-the-art natural language processing"
      },
      {
        "paperId": "cb0ab255c4079e2082ba6e3a807529527d96687c",
        "title": "Overview of the IWSLT 2017 Evaluation Campaign"
      },
      {
        "paperId": "09cd7876b72d6105c83db59052572433a0a2b36c",
        "title": "WIT3: Web Inventory of Transcribed and Translated Talks"
      },
      {
        "paperId": "794025fffdf8f4ed708aaeb5c1830d40aba0ab7a",
        "title": "Parallel Corpus Refinement as an Outlier Detection Algorithm"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab",
        "title": "Discriminative Reranking for Machine Translation"
      },
      {
        "paperId": null,
        "title": "Speech understanding systems: A summary of results of the five-year research effort at"
      },
      {
        "paperId": null,
        "title": "Users and Providers Track"
      },
      {
        "paperId": null,
        "title": "2023. trlx: A scalable framework for rlhf"
      },
      {
        "paperId": null,
        "title": "Emirates"
      },
      {
        "paperId": null,
        "title": "2022. Red team-ing language models with language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "71a3c10a3c8527ce4306ee6ea847689f19fb9d5b",
        "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport"
      },
      {
        "paperId": "1c4a0626336cb70bbe930e941b455d8ceb1061cf",
        "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025"
      },
      {
        "paperId": "3eadeada86853ee6b04847064318032f555e17c9",
        "title": "A survey of multilingual large language models"
      },
      {
        "paperId": "d7e4e39ce7420886eef1fedc846072052df00ae6",
        "title": "Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation"
      },
      {
        "paperId": "19716f3ad5fe69de7a18bb1a3c496a6b9197bac7",
        "title": "Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings"
      },
      {
        "paperId": "a6fe3087a42cfafca8596fe1eb4ef64b957c6f71",
        "title": "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation"
      },
      {
        "paperId": "44a7150f6cc94e761737ef356561f8e27331e943",
        "title": "QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation"
      },
      {
        "paperId": "3dff5d0be6a2b4249365a970e77af64e0b0fba2d",
        "title": "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics"
      },
      {
        "paperId": "afb8628e0fc1bbf9febf7d7f6422b61b23758f5b",
        "title": "Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization"
      },
      {
        "paperId": "a9c177daae7dea8778259cf3b05e292b792507db",
        "title": "MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators"
      },
      {
        "paperId": "74f1b67fa18bc9c4033a9e8fd4e11ed38b178a37",
        "title": "Hybrid Alignment Training for Large Language Models"
      },
      {
        "paperId": "de1c034f8ee77c9f23f4e0d26d7e2520b67f6017",
        "title": "Can Automatic Metrics Assess High-Quality Translations?"
      },
      {
        "paperId": "be445964370b5ea8c15aa1bf6cf10f951fa90b9a",
        "title": "Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers"
      },
      {
        "paperId": "7647707b6b68c963314de0aab1514176b11732df",
        "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "5b9fad3e2b2cc5dd23b01e0089bb7b6f6865cb82",
        "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model"
      },
      {
        "paperId": "8c835daaf7720a168e5d3d669f419765c510bbaf",
        "title": "Towards Explainable Evaluation Metrics for Machine Translation"
      },
      {
        "paperId": "5e71b2a8da2d3bcffa73ac0cf9f2bce1bf1c3925",
        "title": "Improving NMT Models by Retrofitting Quality Estimators into Trainable Energy Loss"
      },
      {
        "paperId": "3fb5ac41e3d8195dff2afc31cf88f02676da848e",
        "title": "Enhancing Machine Translation with Self-Supervised Preference Data"
      }
    ],
    "score": 9.5
  },
  {
    "id": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
    "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
    "authors": [
      "Han Xia",
      "Songyang Gao",
      "Qiming Ge",
      "Zhiheng Xi",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches.",
    "url": "https://www.semanticscholar.org/paper/ee3c57d53327c5f84a8f3988f592c6e2479c1924",
    "pdf_url": "https://arxiv.org/pdf/2408.14874.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-08-27",
    "externalIds": {
      "DBLP": "conf/emnlp/XiaGGX0024",
      "ArXiv": "2408.14874",
      "DOI": "10.48550/arXiv.2408.14874",
      "CorpusId": 271962868
    },
    "references": [
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "48362b169a235ca650918c489c8cea4c597da645",
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
      },
      {
        "paperId": "e8b22bf8a78401b3807bcd46fa7c88d0c07f58ba",
        "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "2a659ca9da778c3adae2ed3f438ff0168df10a02",
        "title": "RSO: A Gradient Free Sampling Based Approach For Training Deep Neural Networks"
      },
      {
        "paperId": "00c957711b12468cb38424caccdf5291bb354033",
        "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "7d083d654f66f763302d8a5f0678beb753f6507b",
        "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review"
      },
      {
        "paperId": null,
        "title": "2023. Mistral 7b"
      },
      {
        "paperId": null,
        "title": "LMDeploy Contributors"
      },
      {
        "paperId": null,
        "title": "2023. Stay on topic with classifier-free guidance. arXiv preprint"
      },
      {
        "paperId": null,
        "title": "Meta AI. 2024"
      },
      {
        "paperId": null,
        "title": "that estimates the optimal policy under current problems, offering improved convenience and flexibility"
      },
      {
        "paperId": null,
        "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "optimization enabled, it took about 5 minutes; the duration per epoch was approximately 7 minutes the 13B size model, under the same settings average duration of the sampling phase"
      }
    ],
    "cited_by": [
      {
        "paperId": "cca984474ed2c423eefcc4de4fd393baad65d7e7",
        "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning"
      },
      {
        "paperId": "30da58c425d4e2a5e3c0b774cf8302d2fcf9ce51",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "7973a601c208c80ba8f9cd6af9751f0609a17605",
        "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      },
      {
        "paperId": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
        "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution"
      },
      {
        "paperId": "19716f3ad5fe69de7a18bb1a3c496a6b9197bac7",
        "title": "Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings"
      },
      {
        "paperId": "7541299db7430f11211c3d8479f121bdfb485729",
        "title": "Token-level Proximal Policy Optimization for Query Generation"
      },
      {
        "paperId": "010d8e7495006b2b54c76f59cb60451377d671e1",
        "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      }
    ],
    "score": 9.0
  },
  {
    "id": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
    "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
    "authors": [
      "Ilgee Hong",
      "Zichong Li",
      "Alexander Bukharin",
      "Yixiao Li",
      "Haoming Jiang",
      "Tianbao Yang",
      "Tuo Zhao"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
    "url": "https://www.semanticscholar.org/paper/cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
    "pdf_url": "https://arxiv.org/pdf/2406.02764.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-06-04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-02764",
      "ArXiv": "2406.02764",
      "DOI": "10.48550/arXiv.2406.02764",
      "CorpusId": 270258188
    },
    "references": [
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "6fca85024354e3fafa75b767961bee9245263170",
        "title": "Reward Collapse in Aligning Large Language Models"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "a1382290160a39a7211d8291faa21f3833818061",
        "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size"
      },
      {
        "paperId": "559d4bdb3241044bbbc50a902372290ddebd2126",
        "title": "Adaptive Temperature Scaling for Robust Calibration of Deep Neural Networks"
      },
      {
        "paperId": "63916bb5363d37b9a3adfd1a56dee3710190fce1",
        "title": "Sample-dependent Adaptive Temperature Scaling for Improved Calibration"
      },
      {
        "paperId": "2cfb0693f8150a305bc04ec42027ad6d4f22ad4e",
        "title": "Distributionally-robust Recommendations for Improving Worst-case User Experience"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "f7976ca42c84b9f87cac6965f33affdab8b0f205",
        "title": "Semantically Distributed Robust Optimization for Vision-and-Language Inference"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "5ede529879d162d2779d410a5775d3f6cd6be3f4",
        "title": "Modeling the Second Player in Distributionally Robust Optimization"
      },
      {
        "paperId": "57eaad10369de402d3363c1d99c93810463eb03c",
        "title": "Understanding the Behaviour of Contrastive Loss"
      },
      {
        "paperId": null,
        "title": "Transformers: State-of-the-Art Natural Language Processing"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "8a4ade967d9830636cf0160dbde8565a0858edf4",
        "title": "Local Temperature Scaling for Probability Calibration"
      },
      {
        "paperId": "3e0925355554e3aeb99de8165c268582a82de3bb",
        "title": "Smooth Exploration for Robotic Reinforcement Learning"
      },
      {
        "paperId": "193092aef465bec868d1089ccfcac0279b914bda",
        "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization"
      },
      {
        "paperId": "f617f7ba4040d6e85b384685da09fed35c841280",
        "title": "Contextual Temperature for Language Modeling"
      },
      {
        "paperId": "77568c594470f9aa029f92774e2c12ab0451d9bb",
        "title": "Distributionally Robust Language Modeling"
      },
      {
        "paperId": "4a0d35989d91b3b7d2318802b1de6d10e4e6e830",
        "title": "Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "d65ce2b8300541414bfe51d03906fca72e93523c",
        "title": "On Calibration of Modern Neural Networks"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "ff7e60b8d336aef5ed974609a63610641085177e",
        "title": "Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML"
      },
      {
        "paperId": "ff6167e71af0f1bce3a28ddaf016a373379c742e",
        "title": "Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
        "title": "Distilling the Knowledge in a Neural Network"
      },
      {
        "paperId": "50d7ecd6901d6759a6bd9da7f2fc8f346e073577",
        "title": "Data-driven robust optimization"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "583b55367f787eb0c4e295707b642e63547b9806",
        "title": "Robust Solutions of Optimization Problems Affected by Uncertain Probabilities"
      },
      {
        "paperId": "54b3cd0901c4d3a1d840c8069494effe13aa27c6",
        "title": "Projected Newton methods for optimization problems with simple constraints"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      },
      {
        "paperId": "3701b930a409badf6bfeb293876489eab491a908",
        "title": "Distributionally Robust Finetuning BERT for Covariate Drift in Spoken Language Understanding"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": null,
        "title": "Transformer reinforcement"
      },
      {
        "paperId": null,
        "title": "Rl baselines3"
      },
      {
        "paperId": null,
        "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning, 2016-2019"
      }
    ],
    "cited_by": [
      {
        "paperId": "b4420900c238ccc9b892544bf8ad222b5904c5d9",
        "title": "Performance Assessment Strategies for Generative AI Applications in Healthcare"
      },
      {
        "paperId": "b3b13107dd153f4de1ddfeec0a96998d70caf04b",
        "title": "Preference Robustness for DPO with Applications to Public Health"
      },
      {
        "paperId": "a8de5181517c125ee4eae21670da3cb73124373d",
        "title": "DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection"
      },
      {
        "paperId": "36540ee20aa4ba4f02f7359e1113cadbefbf1b6d",
        "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy"
      },
      {
        "paperId": "9af9c0df0a328d5f327bed3151819b06dfc33622",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment"
      },
      {
        "paperId": "609ca024d2eb12c606491f67cee9e71ae730e8ff",
        "title": "Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning"
      },
      {
        "paperId": "66c16a4eb1457f447a44fb1ea1968f8841ad5a2d",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "5cb5453d2c54e1449f82fdf2e976cab04396b224",
        "title": "Distributionally Robust Reinforcement Learning with Human Feedback"
      }
    ],
    "score": 9.0
  },
  {
    "id": "a0748478cd2752b733b4183dbd0dcd1031c38b6e",
    "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
    "authors": [
      "Hakim Sidahmed",
      "Samrat Phatale",
      "Alex Hutcheson",
      "Zhuonan Lin",
      "Zhan Chen",
      "Zac Yu",
      "Jarvis Jin",
      "Roman Komarytsia",
      "Christiane Ahlheim",
      "Yonghao Zhu",
      "Simral Chaudhary",
      "Bowen Li",
      "Saravanan Ganesh",
      "Bill Byrne",
      "Jessica Hoffmann",
      "Hassan Mansoor",
      "Wei Li",
      "Abhinav Rastogi",
      "Lucas Dixon"
    ],
    "year": 2024,
    "citationCount": 8,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/a0748478cd2752b733b4183dbd0dcd1031c38b6e",
    "pdf_url": "https://doi.org/10.48550/arXiv.2403.10704",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2403-10704",
      "DOI": "10.48550/arXiv.2403.10704",
      "CorpusId": 268513245
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "21483bddc49fb80a0601cd03fc51ca56f33f4a8d",
        "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race"
      },
      {
        "paperId": "bcc4f07e9d5e04752c28bbd6c7e1a5848a076a83",
        "title": "A Survey of Generative Categories and Techniques in Multimodal Large Language Models"
      },
      {
        "paperId": "76509b17bc5582137720ebe2c3a6b661a2804382",
        "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF"
      },
      {
        "paperId": "11d89b4578abfb7b34e7378db61412040b28ecb1",
        "title": "A Survey on Recent Advancements in Autonomous Driving Using Deep Reinforcement Learning: Applications, Challenges, and Solutions"
      },
      {
        "paperId": "291c94b62953e261c94b74516ee997be5511c052",
        "title": "A Survey on LoRA of Large Language Models"
      },
      {
        "paperId": "8e624e215908934a38044500f8434a0f88c69059",
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models"
      },
      {
        "paperId": "ed62e3847f45f152cf6d7b9b4bebb782547f1a54",
        "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment"
      },
      {
        "paperId": "fdd8fa34cda01cd8f1a77528bac42ed0e58497f0",
        "title": "Multimodal large language models: A survey"
      }
    ],
    "score": 8.0
  },
  {
    "id": "5b6f58a8b0098dbc7bfa735359a2f6ae7ea4cbe6",
    "title": "A Survey On Enhancing Reinforcement Learning in Complex Environments: Insights from Human and LLM Feedback",
    "authors": [
      "Alireza Rashidi Laleh",
      "M. N. Ahmadabadi"
    ],
    "year": 2024,
    "citationCount": 8,
    "abstract": "Reinforcement learning (RL) is one of the active fields in machine learning, demonstrating remarkable potential in tackling real-world challenges. Despite its promising prospects, this methodology has encountered with issues and challenges, hindering it from achieving the best performance. In particular, these approaches lack decent performance when navigating environments and solving tasks with large observation space, often resulting in sample-inefficiency and prolonged learning times. This issue, commonly referred to as the curse of dimensionality, complicates decision-making for RL agents, necessitating a careful balance between attention and decision-making. RL agents, when augmented with human or large language models' (LLMs) feedback, may exhibit resilience and adaptability, leading to enhanced performance and accelerated learning. Such feedback, conveyed through various modalities or granularities including natural language, serves as a guide for RL agents, aiding them in discerning relevant environmental cues and optimizing decision-making processes. In this survey paper, we mainly focus on problems of two-folds: firstly, we focus on humans or an LLMs assistance, investigating the ways in which these entities may collaborate with the RL agent in order to foster optimal behavior and expedite learning; secondly, we delve into the research papers dedicated to addressing the intricacies of environments characterized by large observation space.",
    "url": "https://www.semanticscholar.org/paper/5b6f58a8b0098dbc7bfa735359a2f6ae7ea4cbe6",
    "pdf_url": "https://arxiv.org/pdf/2411.13410.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-11-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2411-13410",
      "ArXiv": "2411.13410",
      "DOI": "10.48550/arXiv.2411.13410",
      "CorpusId": 274149861
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "164188a2e6326e187b4d1699f17adf413f0d675d",
        "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments"
      },
      {
        "paperId": "e12cf0e49ad8fc7a052cbc21597b6c5144f5f634",
        "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement"
      },
      {
        "paperId": "4fa1e4442dc9c4bf8c5b2109955d28f066fd6991",
        "title": "Evaluation of LLMs for mathematical problem solving"
      },
      {
        "paperId": "0b8300b6b5aaeacc535e82ac3ba088a1d4dd6755",
        "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation"
      },
      {
        "paperId": "ac45476906ecb83556f143c3f17f1dd53aa6ab91",
        "title": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning"
      },
      {
        "paperId": "56fc00881e8d854f3cdecb2d072c369ac818c3ef",
        "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges"
      },
      {
        "paperId": "28e536d4b425a8743af9b074ddb11baba66f8b47",
        "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey"
      },
      {
        "paperId": "f1d6e5335140719726d9ba2226ad4be18def1f8d",
        "title": "Position: Towards LLM-in-the-Loop Machine Learning for Future Applications"
      }
    ],
    "score": 8.0
  },
  {
    "id": "57e959b74f36a30cd62d0abd4204f08907b42e87",
    "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning",
    "authors": [
      "Qianyue Hao",
      "Sibo Li",
      "Jian Yuan",
      "Yong Li"
    ],
    "year": 2025,
    "citationCount": 8,
    "abstract": "Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs'parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.",
    "url": "https://www.semanticscholar.org/paper/57e959b74f36a30cd62d0abd4204f08907b42e87",
    "pdf_url": "https://arxiv.org/pdf/2505.14140.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2505-14140",
      "ArXiv": "2505.14140",
      "DOI": "10.48550/arXiv.2505.14140",
      "CorpusId": 278769085
    },
    "references": [
      {
        "paperId": "225721f407a7a8cdefd4ba6bc61c43acba5a3b6a",
        "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models"
      },
      {
        "paperId": "4a7e1870fa76329da369211ab0630ce4960402f6",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning"
      },
      {
        "paperId": "d9a77476fc3fe391b8143386d6394f1a18c29606",
        "title": "LLM2: Let Large Language Models Harness System 2 Reasoning"
      },
      {
        "paperId": "19ac2750cd1e02362f25fc2bdd88110fa127db34",
        "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes"
      },
      {
        "paperId": "d42c0f04a8618c9e662cf8d1773c8cac447ccfc1",
        "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "124b40ec883eb59e89be76e5da5489dbed89275c",
        "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models"
      },
      {
        "paperId": "a17f7e4a09d49413c03b96dfef228a95e2ff0141",
        "title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection"
      },
      {
        "paperId": "5bd36b2c701056984a6353edc8ebdd3a3864bc34",
        "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS"
      },
      {
        "paperId": "ae2fa2af2da2c3e96e87af00887b4938e48be79c",
        "title": "Preference Optimization for Reasoning with Pseudo Feedback"
      },
      {
        "paperId": "7943ec4a67151a559b25cd34369e661c9a7924c8",
        "title": "GPT-4o System Card"
      },
      {
        "paperId": "1176de467f5f6fca504e7b98da7fe6fb4414fcd3",
        "title": "ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning"
      },
      {
        "paperId": "53132ea6c107479d4557631299d3ed525109b464",
        "title": "AFlow: Automating Agentic Workflow Generation"
      },
      {
        "paperId": "d2cfe8a382e8eedfe257061bcc615cfd40ad8890",
        "title": "HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction"
      },
      {
        "paperId": "d084517f14ee247883de0f4dd58bb923e418157d",
        "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning"
      },
      {
        "paperId": "7f2a0de36fd7172b3cc64882e2d472778945c8b1",
        "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI"
      },
      {
        "paperId": "9fb201282f53a4ce89f28cbe5026af78912aa8c1",
        "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement"
      },
      {
        "paperId": "4814f1744ebfbbf0e987ee4242a930dd2d3d09a5",
        "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "f02c466bdef9bb273f92b41f4c1d31e5d332ab25",
        "title": "LiteSearch: Efficacious Tree Search for LLM"
      },
      {
        "paperId": "d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning"
      },
      {
        "paperId": "b00d92a2ebb024dc6c82d2cefbcd0782ce32f74c",
        "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models"
      },
      {
        "paperId": "f32bcc2155997110a7905da050df4c8404867b24",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "paperId": "00ae35944538602a7835d2e79e037b7f6372c764",
        "title": "Depression Detection on Social Media with Large Language Models"
      },
      {
        "paperId": "c78350e81298ca87bc1d59b466fa40081232caaa",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "paperId": "46a5ec31987a12d60ade20c6471db64c46f90106",
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "42445823fb0156afddc8c72eaa5ee81dded5b965",
        "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges"
      },
      {
        "paperId": "ebd1c04c61f73f46def3305ca11d038c46665b65",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      },
      {
        "paperId": "08799ed4f55cf970dcadabcfd9c26855517160a3",
        "title": "Small Language Model Can Self-correct"
      },
      {
        "paperId": "4ba57555bef02f988f2ed3bab2f102733dc55221",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "78eaced1b4008b4227831adcccd39da4322e969e",
        "title": "Stance Detection with Collaborative Role-Infused LLM-Based Agents"
      },
      {
        "paperId": "2069aaaa281eb13bcd9330fc4d43f24f6b436a53",
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines"
      },
      {
        "paperId": "fa874e7b66a5b936469872054986c8f340701146",
        "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4"
      },
      {
        "paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf",
        "title": "A Survey on Evaluation of Large Language Models"
      },
      {
        "paperId": "c12b80b44d9acfe6cd92fdf965264c4b706c367c",
        "title": "ToolQA: A Dataset for LLM Question Answering with External Tools"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance"
      },
      {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01",
        "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "65906e6027246ae9e4ecd18d6e019a24505c842e",
        "title": "Aligning AI With Shared Human Values"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "fda1e13a2eaeaa0b4434833d3ee0eb8e79b0ba94",
        "title": "On the cognitive process of human problem solving"
      },
      {
        "paperId": "2a0f45671b0f97683be97b330b9c3e2daec285bc",
        "title": "Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle"
      },
      {
        "paperId": null,
        "title": "Reinforcement fine-tuning"
      },
      {
        "paperId": null,
        "title": "interconnects"
      },
      {
        "paperId": null,
        "title": "Deciphering and enhancing commonsense reasoning in llms from the perspective of intrinsic factual knowledge retrieval"
      },
      {
        "paperId": null,
        "title": "answer , conclude the answer with \u2019The answer is yes \u2019 or \u2019The answer is no \u2019"
      },
      {
        "paperId": null,
        "title": "Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment"
      },
      {
        "paperId": null,
        "title": "Whether all necessary elements within this specific step are known from the problem or previous steps"
      },
      {
        "paperId": null,
        "title": "Reinforcement learning in the era of large language models: Challenges and opportunities"
      },
      {
        "paperId": null,
        "title": "For each aspect, please score 1 for False, 2 for Unsure, and 3 for True, and score 0 if the current step does not involve this aspect"
      }
    ],
    "cited_by": [
      {
        "paperId": "a6eb4739dcd8fb23869472cb17754396cb4bb76e",
        "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "10c98cc047490b30eb286e3ac7a09de71163b5da",
        "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning"
      },
      {
        "paperId": "46ca7239f50032bf686be2c7764bf3b891aa7079",
        "title": "Benchmarking and Advancing Large Language Models for Local Life Services"
      },
      {
        "paperId": "ded5c43500713f353334d6240c8567e64ae8846a",
        "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation"
      },
      {
        "paperId": "4d8419a3e6357bb79854119f1321426b16ddf739",
        "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback"
      },
      {
        "paperId": "b4e0c5416292e68cc60b79dff0d7ff887242f634",
        "title": "Open-Set Living Need Prediction with Large Language Models"
      },
      {
        "paperId": "075d166a8e7dcf637e20d4a64d5512d45db64a84",
        "title": "LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan"
      }
    ],
    "score": 8.0
  },
  {
    "id": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
    "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "authors": [
      "Yuchun Miao",
      "Sen Zhang",
      "Liang Ding",
      "Yuqi Zhang",
      "Lefei Zhang",
      "D. Tao"
    ],
    "year": 2025,
    "citationCount": 8,
    "abstract": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.",
    "url": "https://www.semanticscholar.org/paper/0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
    "pdf_url": "https://arxiv.org/pdf/2501.19358.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-01-31",
    "externalIds": {
      "ArXiv": "2501.19358",
      "DBLP": "journals/corr/abs-2501-19358",
      "DOI": "10.48550/arXiv.2501.19358",
      "CorpusId": 276079993
    },
    "references": [
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "0ac43cb23cdb84b6c7dc6986c036fb3152e9a286",
        "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "792c7db478a22a8cfaa25ff1d3f271c136b33e7a",
        "title": "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "7e388155314475226bd001b84d8a9a69f35021a7",
        "title": "Language Models Represent Beliefs of Self and Others"
      },
      {
        "paperId": "e0739369308c908da5807166609f2552db9c8ea4",
        "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "2412a10df38c3d1de8f1392abb995a624413129f",
        "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "8fd29e810540c40846cddce3cbdf5060cd59fb57",
        "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender"
      },
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"
      },
      {
        "paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
        "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models"
      },
      {
        "paperId": "a84d6b82947f27bc6bf7f42d69f48b40adcfb6c3",
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "title": "Mistral 7B"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "58fdf550600fc3873729d466601c5d08a51ba8a0",
        "title": "Representation Engineering: A Top-Down Approach to AI Transparency"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "1b173d9b8b0a2529259b6fa16376aff11c1ac08f",
        "title": "BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "f406aceba4f29cc7cfbe7edb2f52f01374486589",
        "title": "The Internal State of an LLM Knows When its Lying"
      },
      {
        "paperId": "fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "8755c15fe073c6af03664b2a74aafef1fed5f198",
        "title": "BERT has a Moral Compass: Improvements of ethical and moral values of machines"
      },
      {
        "paperId": "00c957711b12468cb38424caccdf5291bb354033",
        "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "4aea3547974399a32d7aa7c007b10bd665e93fab",
        "title": "On Variational Bounds of Mutual Information"
      },
      {
        "paperId": "c4955faa27e082a80504285c28324c58eb52250c",
        "title": "Understanding the impact of entropy on policy optimization"
      },
      {
        "paperId": "c9359c13b4f4b058380e4ea3387044012e65bd2a",
        "title": "Position Bias Estimation for Unbiased Learning to Rank in Personal Search"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
        "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"
      },
      {
        "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
        "title": "Linguistic Regularities in Continuous Space Word Representations"
      },
      {
        "paperId": "3efcb97c1de1c87832a7a1d99e91801992a938ec",
        "title": "Crafting Papers on Machine Learning"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "70bfee8ed1c228536492548a4acf9e9783afac0f",
        "title": "Delve into PPO: Implementation Matters for Stable RLHF"
      },
      {
        "paperId": null,
        "title": "Alpacae-val: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Comment: Fails to address pH range and oversimplifies the composition of suitable cactus soil, reducing precision and adaptability"
      },
      {
        "paperId": null,
        "title": "Evolutionary psychologists suggest that humans may have evolved to believe in God as a way to make sense of the world and deal with fear and uncertainty"
      },
      {
        "paperId": null,
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on 2024"
      }
    ],
    "cited_by": [
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "e1b7fad25b3787dd1378ae63413fdc38d3e8fbc9",
        "title": "The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback"
      },
      {
        "paperId": "1465aff7f7a0916287a43f8b0f0c0d6e5fe5719e",
        "title": "Inference-Time Reward Hacking in Large Language Models"
      },
      {
        "paperId": "bd245db84a69cb1d4319f3b35f3f0b2a0c3e712b",
        "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "bf06547a7296a994cf77c1baa793dee6e337a865",
        "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization"
      },
      {
        "paperId": "92f2e1a17cbff22164611a2c8a908afe71b4b626",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      }
    ],
    "score": 8.0
  },
  {
    "id": "4a081d1e1ea87ffcbf64cfe1c8a4a1da24c2c1b8",
    "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
    "authors": [
      "Pangpang Liu",
      "Chengchun Shi",
      "Will Wei Sun"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Aligning large language models (LLMs) with human preferences is critical to recent advances in generative artificial intelligence. Reinforcement learning from human feedback (RLHF) is widely applied to achieve this objective. A key step in RLHF is to learn the reward function from human feedback. However, human feedback is costly and time-consuming, making it essential to collect high-quality conversation data for human teachers to label. Additionally, different human teachers have different levels of expertise. It is thus critical to query the most appropriate teacher for their opinions. In this paper, we use offline reinforcement learning (RL) to formulate the alignment problem. Motivated by the idea of $D$-optimal design, we first propose a dual active reward learning algorithm for the simultaneous selection of conversations and teachers. Next, we apply pessimistic RL to solve the alignment problem, based on the learned reward estimator. Theoretically, we show that the reward estimator obtained through our proposed adaptive selection strategy achieves minimal generalized variance asymptotically, and prove that the sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a given sample budget $T$. Through simulations and experiments on LLMs, we demonstrate the effectiveness of our algorithm and its superiority over state-of-the-arts.",
    "url": "https://www.semanticscholar.org/paper/4a081d1e1ea87ffcbf64cfe1c8a4a1da24c2c1b8",
    "pdf_url": "https://arxiv.org/pdf/2410.02504.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-03",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-02504",
      "ArXiv": "2410.02504",
      "DOI": "10.48550/arXiv.2410.02504",
      "CorpusId": 273098777
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4c63722f98a282f3e055f33993ecdd46ad7f36aa",
        "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design"
      },
      {
        "paperId": "5e548e174431e3c989ad0f3f41e0f7651ff76d83",
        "title": "Doubly Robust Alignment for Large Language Models"
      },
      {
        "paperId": "98e1ddef8ce9b35156f18688441f0bc699738a38",
        "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain"
      },
      {
        "paperId": "be0545a9ebd293b9b85570200631bfbdc839f300",
        "title": "Learning Guarantee of Reward Modeling Using Deep Neural Networks"
      },
      {
        "paperId": "66c16a4eb1457f447a44fb1ea1968f8841ad5a2d",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
      },
      {
        "paperId": "4fded2e45d10131c2d6a59ffcfcb4aaadfc98c2c",
        "title": "Active Learning for Direct Preference Optimization"
      },
      {
        "paperId": "a2a5ea730b7d0ef7653060595a021360be4ad57f",
        "title": "LLM Safety Alignment is Divergence Estimation in Disguise"
      }
    ],
    "score": 7.0
  },
  {
    "id": "58f614941629541c8c04acdb8acb9e3fb350ac5a",
    "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning",
    "authors": [
      "Huimu Yu",
      "Xing Wu",
      "Weidong Yin",
      "Debing Zhang",
      "Songlin Hu"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Large language models (LLMs) have made significant progress in natural language understanding and generation, driven by scalable pretraining and advanced finetuning. However, enhancing reasoning abilities in LLMs, particularly via reinforcement learning from human feedback (RLHF), remains challenging due to the scarcity of high-quality preference data, which is labor-intensive to annotate and crucial for reward model (RM) finetuning. To alleviate this issue, we introduce CodePMP, a scalable preference model pretraining (PMP) pipeline that utilizes a large corpus of synthesized code-preference pairs from publicly available high-quality source code. CodePMP improves RM finetuning efficiency by pretraining preference models on large-scale synthesized code-preference pairs. We evaluate CodePMP on mathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor, LogiQA2.0), consistently showing significant improvements in reasoning performance of LLMs and highlighting the importance of scalable preference model pretraining for efficient reward modeling.",
    "url": "https://www.semanticscholar.org/paper/58f614941629541c8c04acdb8acb9e3fb350ac5a",
    "pdf_url": "https://arxiv.org/pdf/2410.02229.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-03",
    "externalIds": {
      "ArXiv": "2410.02229",
      "DBLP": "journals/corr/abs-2410-02229",
      "DOI": "10.48550/arXiv.2410.02229",
      "CorpusId": 273098454
    },
    "references": [
      {
        "paperId": "9fb201282f53a4ce89f28cbe5026af78912aa8c1",
        "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "4cae2af5b09b746f6616bd5152c05b77e66316f4",
        "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
        "title": "Gemma 2: Improving Open Language Models at a Practical Size"
      },
      {
        "paperId": "f32bcc2155997110a7905da050df4c8404867b24",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "paperId": "5701b0bbd27ec031db17be74ad2779890d6bf637",
        "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning"
      },
      {
        "paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f",
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570",
        "title": "OLMo: Accelerating the Science of Language Models"
      },
      {
        "paperId": "72d3bf6f696d3f67e17e5211f64b7c6fb0316883",
        "title": "Augmenting Math Word Problems via Iterative Question Composing"
      },
      {
        "paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"
      },
      {
        "paperId": "288e64e8adb23d81e291a2cb51e3a56b315023b7",
        "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"
      },
      {
        "paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54",
        "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"
      },
      {
        "paperId": "bfe9e6f64d9e819efc27772ab87dc0c5e2d62906",
        "title": "Critique Ability of Large Language Models"
      },
      {
        "paperId": "1e0caa706e9d9fdad82d6713fa20b52975a1703b",
        "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?"
      },
      {
        "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      },
      {
        "paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b",
        "title": "Making Large Language Models Better Reasoners with Alignment"
      },
      {
        "paperId": "537335d9aad0ddbaef93e7f88b0db096671ef6ec",
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
        "title": "Holistic Evaluation of Language Models"
      },
      {
        "paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c",
        "title": "Scaling Data-Constrained Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
        "title": "Large Language Models Can Self-Improve"
      },
      {
        "paperId": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
        "title": "Language Models of Code are Few-Shot Commonsense Learners"
      },
      {
        "paperId": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
        "title": "Complexity-Based Prompting for Multi-Step Reasoning"
      },
      {
        "paperId": "29acc890e521f7a6415666ab9eb3432c49b4587a",
        "title": "Self-critiquing models for assisting human evaluators"
      },
      {
        "paperId": "5437e8adab596d7294124c0e798708e050e25321",
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "65723a96ff5b15f124bf3b534503950b537b4792",
        "title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning"
      },
      {
        "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "0ecb33ced5b0976accdf13817151f80568b6fdcb",
        "title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking"
      },
      {
        "paperId": "1a4e67d3705492d0af88623b0e62818a16084fca",
        "title": "The Analysis of Permutations"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "Gemini: A family of highly capable multimodal models"
      },
      {
        "paperId": "11a05b7e2bc79a9e0cc1d1b83ec978da8ee13bc9",
        "title": "LogiQA 2.0\u2014An Improved Dataset for Logical Reasoning in Natural Language Understanding"
      },
      {
        "paperId": null,
        "title": "We validate that CodePMP significantly improves performance on reasoning tasks, demonstrating that a scalable PMP process positively impacts LLM reasoning abilities"
      },
      {
        "paperId": null,
        "title": "Improving llms with reward model regularization"
      },
      {
        "paperId": null,
        "title": "A Hyperparameters We outline key hyperparameters in this the tables, WSD refers to the warmup-stable-decay learning rate scheduler"
      },
      {
        "paperId": null,
        "title": "Understanding reward models in rlhf"
      },
      {
        "paperId": null,
        "title": "for BON generation"
      },
      {
        "paperId": null,
        "title": "Modeling human preferences using probabilistic ranking models"
      },
      {
        "paperId": null,
        "title": "We introduce CodePMP, a scalable method that uses code-derived preference pairs to pretrain preference models, improving sample efficiency and robustness for downstream RM finetuning"
      },
      {
        "paperId": null,
        "title": "2024. Critique-out-loud reward models"
      },
      {
        "paperId": null,
        "title": "Hyperparameter Qwen2-1.5B Qwen2-7B epoch 1 1 batch size 1024 1024 learning rate 3e-6 1e-6 lr scheduler WSD WSD"
      },
      {
        "paperId": null,
        "title": "2022. How does gpt obtain its ability? tracing emergent abilities of language models to their sources"
      },
      {
        "paperId": null,
        "title": "2023b. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning"
      }
    ],
    "cited_by": [
      {
        "paperId": "cfe2f0458042d7e399e03450d45bf3b1fb719793",
        "title": "On Code-Induced Reasoning in LLMs"
      },
      {
        "paperId": "e93a58bdc30d36f342ecb83586e6152f565aa015",
        "title": "Libra: Large Chinese-based Safeguard for AI Content"
      },
      {
        "paperId": "aa3517c664890cd36aee3cfff0f09d2645e373b6",
        "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility"
      },
      {
        "paperId": "16a096e663c5aa661f26bcabe212a53d86ef3eae",
        "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "11313ae3f1745da638fa1852d020a18417a13bbf",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch"
      },
      {
        "paperId": "e448569d1af141336897d9aff91cb5818dee1004",
        "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models"
      }
    ],
    "score": 7.0
  },
  {
    "id": "54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
    "title": "REvolve: Reward Evolution with Large Language Models using Human Feedback",
    "authors": [
      "Rishi Hazra",
      "Alkis Sygkounas",
      "A. Persson",
      "Amy Loutfi",
      "Pedro Zuidberg Dos Martires"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings -- autonomous driving, humanoid locomotion, and dexterous manipulation -- wherein notions of ``good\"behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines.",
    "url": "https://www.semanticscholar.org/paper/54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
    "pdf_url": "https://arxiv.org/pdf/2406.01309.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-06-03",
    "externalIds": {
      "ArXiv": "2406.01309",
      "DBLP": "conf/iclr/HazraSPLM25",
      "CorpusId": 273695152
    },
    "references": [
      {
        "paperId": "50fe40b35d376d000a70a25ab2f9b6e6b6b336d6",
        "title": "DrEureka: Language Model Guided Sim-To-Real Transfer"
      },
      {
        "paperId": "3d3f0d7eb69f063e9539dd45df971f007b094734",
        "title": "Large Language Models to Enhance Bayesian Optimization"
      },
      {
        "paperId": "37c112454a236ab91c9c6b5cc165a6c3251e9206",
        "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection"
      },
      {
        "paperId": "d32ba88571141ed0ebe7aeefbaa4ccaf8cda7be3",
        "title": "Mathematical discoveries from program search with large language models"
      },
      {
        "paperId": "118461e5a3907e16c123cc55d2cac7feb083ddcb",
        "title": "Using Large Language Models for Hyperparameter Optimization"
      },
      {
        "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
      },
      {
        "paperId": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
        "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?"
      },
      {
        "paperId": "ecb6a8003d4726cc1e9e9f3c2d8ff6c0627e4e95",
        "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning"
      },
      {
        "paperId": "a9c75cf664f675a1b4034b0256ec3c5168e293df",
        "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "paperId": "f8a2dca1e8fe56e698984c077f7ff58d8ca867e9",
        "title": "Large Language Models as Optimizers"
      },
      {
        "paperId": "7569d62c7651625456a2cbd4922d9d65351593ac",
        "title": "SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "688fc1e744877c3a68f306443042f016196ce98a",
        "title": "The Perils of Trial-and-Error Reward Design: Misdesign through Overfitting and Invalid Task Specifications"
      },
      {
        "paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c",
        "title": "Language to Rewards for Robotic Skill Synthesis"
      },
      {
        "paperId": "b842ec712ff4ac7793016c5d4c03c0b0b37b998b",
        "title": "LLMatic: Neural Architecture Search Via Large Language Models And Quality Diversity Optimization"
      },
      {
        "paperId": "13f775d1b7f9de1168b5d37a2622ee1938bf43cd",
        "title": "Faster sorting algorithms discovered using deep reinforcement learning"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "paperId": "3338bd9acd4cee211ab555a640cb895bda77dcd0",
        "title": "How Simulation Helps Autonomous Driving: A Survey of Sim2real, Digital Twins, and Parallel Intelligence"
      },
      {
        "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
        "title": "Instruction Tuning with GPT-4"
      },
      {
        "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
        "title": "Segment Anything"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "f2b0017ddd77fa38760a18145e63553105a1a236",
        "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
      },
      {
        "paperId": "442ab95eb9cfbc03bb17a27b52313b5d25eaa738",
        "title": "Discovering faster matrix multiplication algorithms with reinforcement learning"
      },
      {
        "paperId": "2a9a319486ba15a1278bf11a91e084ecf77bb0b6",
        "title": "Sim2real for Autonomous Vehicle Control using Executable Digital Twin"
      },
      {
        "paperId": "683e530c2b7e74302d7cf359286e68c07139c5da",
        "title": "SORNet: Spatial Object-Centric Representations for Sequential Manipulation"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "e6800a5cc92502789f9dfc7c5d9c112d9d2f3372",
        "title": "The Tacit Dimension"
      },
      {
        "paperId": "b4334052e996fa95a26088ccec30d291d0499145",
        "title": "Reward (Mis)design for Autonomous Driving"
      },
      {
        "paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba",
        "title": "Extracting Training Data from Large Language Models"
      },
      {
        "paperId": "69c81d16cc89a47a2a576e6cdafb49f54f6dedd8",
        "title": "Human compatible: AI and the problem of control. Stuart Russell. New York, Viking, 2019, 352\u00a0pages"
      },
      {
        "paperId": "5ff9429efe7b6c4e7c039064920600e2b7664828",
        "title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset"
      },
      {
        "paperId": "374b64424dba52a55049b55d04a10e21c3e29aa1",
        "title": "Silver"
      },
      {
        "paperId": "46d4452eb041e33f1e58eab64ec8cf5af534b6ff",
        "title": "Optimal Policies Tend To Seek Power"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "c9a634310d62037a634e05d60719f8a7319f0a07",
        "title": "Intervention Aided Reinforcement Learning for Safe and Practical Policy Optimization in Navigation"
      },
      {
        "paperId": "9b9c4c9436dc164d555cbf26da36c24ce0637c41",
        "title": "Evolutionary computation: a unified approach"
      },
      {
        "paperId": "6a36ce9b84ee177e8a1925bf5dc80f51a3c2b2af",
        "title": "Distributed Deep Reinforcement Learning on the Cloud for Autonomous Driving"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "30ff82cebce6fdc2957043c4085a426414474d78",
        "title": "Trial without Error: Towards Safe Reinforcement Learning via Human Intervention"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
        "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
      },
      {
        "paperId": "a20f3dfc9142b48b924e68ee22ba259a0d621bb2",
        "title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles"
      },
      {
        "paperId": "1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777",
        "title": "Cooperative Inverse Reinforcement Learning"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "71b552b2e058d5a6a760ba203f10f13be759edd3",
        "title": "Synthesis and stabilization of complex behaviors through online trajectory optimization"
      },
      {
        "paperId": "73465a283cdcc790b3bfc7f71b3b433c6ea403a8",
        "title": "Open issues in genetic programming"
      },
      {
        "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
        "title": "Interactively shaping agents via human reinforcement: the TAMER framework"
      },
      {
        "paperId": "a6582abc47397d96888108ea308c0168d94a230d",
        "title": "The Basic AI Drives"
      },
      {
        "paperId": "c86ba8c7b306e5927e8667011ce40402750e0b52",
        "title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "cd5ee8eea82e53aa5ef78e6996d48a8d8164b3fa",
        "title": "Debugging by skilled and novice programmers"
      },
      {
        "paperId": "4900ffdbda9cb9dc709013e52002985afdc7b7e2",
        "title": "On the folly of rewarding A, while hoping for B."
      },
      {
        "paperId": null,
        "title": "AlphaProof: AI achieves silver-medal standard solving International Mathematical Olympiad problems"
      },
      {
        "paperId": null,
        "title": "Egotv: Egocentric task verification from natural language task descriptions"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "f47d3b5746e78521a6ba20154daff4d366676560",
        "title": "Foundations Of Genetic Programming"
      },
      {
        "paperId": "67238f415dfb2024298082d8c9f7eb529fe191f5",
        "title": "Where Do Rewards Come From"
      },
      {
        "paperId": "a13fcba4fab4713d5acb2d970eddc6b148d2596d",
        "title": "A Survey of Parallel Genetic Algorithms"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "9f414c81159b5e8296a267d5ae95a5bf464061d1",
        "title": "The rating of chessplayers, past and present"
      },
      {
        "paperId": null,
        "title": "Amazing \u201cjailbreak\u201d bypasses chatgpt\u2019s ethics safeguards"
      },
      {
        "paperId": null,
        "title": "Figure 13: Episodic Steps during training for Env 1 [Left] and Env 2 scenario"
      },
      {
        "paperId": null,
        "title": ": Self-referential self-improvement via prompt"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      }
    ],
    "cited_by": [
      {
        "paperId": "c0bf3adeffc75b5c350beef872453bce650ea165",
        "title": "Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search"
      },
      {
        "paperId": "c1eea704b482cf43dfacb25cc3d7e48e49e77214",
        "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench"
      },
      {
        "paperId": "5a5072ecefabfce204302aa858dde9d95935b06a",
        "title": "Investigating Symbiosis in Robotic Ecosystems: A Case Study for Multi-Robot Reinforcement Learning Reward Shaping"
      },
      {
        "paperId": "61fe493bf3a4a87531f95d9e02f6676fbd2521a6",
        "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "0576c5be53db2c04d644637308763a4fc2db3a36",
        "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space"
      },
      {
        "paperId": "29764cce1641000c290144a0f6ea1377caff17d9",
        "title": "LLM-Guided Search for Deletion-Correcting Codes"
      },
      {
        "paperId": "b49cbccd88cacf9a7514c80d33cf5c42b80edc1a",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation With Large Language Models"
      }
    ],
    "score": 7.0
  },
  {
    "id": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
    "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning",
    "authors": [
      "Shugang Hao",
      "Lingjie Duan"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Reinforcement learning from human feedback (RLHF) has become an essential step in fine-tuning large language models (LLMs) to align them with human preferences. However, human labelers are selfish and have diverse preferences. They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences. Current practice simply averages labelers' feedback per time and fails to identify the most accurate human labeler, leading to linear regret $\\mathcal{O}(T)$ for $T$ time slots. To our best knowledge, we are the first to study online learning mechanisms against strategic human labelers in the LLM fine-tuning process. We formulate a new dynamic Bayesian game and dynamically adjust human labelers' weights in the preference aggregation, ensuring their truthful feedback and sublinear regret $\\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great advantages over the existing benchmark schemes.",
    "url": "https://www.semanticscholar.org/paper/1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
    "pdf_url": "https://arxiv.org/pdf/2412.16834.pdf",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "publicationDate": "2024-12-22",
    "externalIds": {
      "DBLP": "journals/corr/abs-2412-16834",
      "ArXiv": "2412.16834",
      "DOI": "10.48550/arXiv.2412.16834",
      "CorpusId": 274981995
    },
    "references": [
      {
        "paperId": "79e41008d1837acc27029d7c5061898385c5ea3c",
        "title": "OPTune: Efficient Online Preference Tuning"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "b36760782b4cfd9359c6d4a57c86e2630393b1a3",
        "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models"
      },
      {
        "paperId": "9fa9a366117d110b4d0056b3efd876255450a7d0",
        "title": "Truthful Aggregation of LLMs with an Application to Online Advertising"
      },
      {
        "paperId": "ad54cb8887c98cc823666402678d249225d4e9f0",
        "title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "dde220d1e7b995a84838d1fda56b761c1233bb0c",
        "title": "Auctions with LLM Summaries"
      },
      {
        "paperId": "d15b810a0f7e7aca05969b586d7e47af2b3b46b0",
        "title": "Strategyproof Mechanisms for Group-Fair Obnoxious Facility Location Problems"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "fc13fa57b21fa9882b821cbb5fa910451f35be7e",
        "title": "Mechanism Design for Collaborative Normal Mean Estimation"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "ee32f3994f162e928e510843df2cd014672c4f12",
        "title": "Collaborative Algorithms for Online Personalized Mean Estimation"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "6399953b1342b004f340dae015e1afd595e28e09",
        "title": "Efficient Competitions and Online Learning with Strategic Forecasters"
      },
      {
        "paperId": "36f1b4dbafe0cf7f1b916f168f209e018c94a4b9",
        "title": "No-Regret and Incentive-Compatible Online Learning"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "5a2c395ca5dc942dbb6ea7eea1e324b8c9e8534e",
        "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "fcc74dde31dba0d1df3f68ba0f927fdce9283d63",
        "title": "Positive Intra-Group Externalities in Facility Location"
      },
      {
        "paperId": "57daf938ab134ba05a1bef4d596c2074d367e81e",
        "title": "trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": null,
        "title": "\u201cOnline prediction with sel\ufb01sh experts,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cOpen and ef-\ufb01cient foundation language models,\u201d"
      }
    ],
    "cited_by": [
      {
        "paperId": "b4d5b8a4676b160fd93ba17ad836cb16f5f74e0e",
        "title": "To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA"
      },
      {
        "paperId": "2b574c8e3a97b77529b8ec56579c91dd55ada850",
        "title": "Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives"
      },
      {
        "paperId": "0625b0d9444036c5503ae0bdd8a83984f28fd268",
        "title": "Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach"
      },
      {
        "paperId": "1fc45ec4de3994020122664280b515d45785b90d",
        "title": "Incentivizing High-Quality Human Annotations with Golden Questions"
      },
      {
        "paperId": "a82216135cca2f41f636ed68edb29cd33124b89b",
        "title": "Neuroplasticity in Artificial Intelligence - An Overview and Inspirations on Drop In & Out Learning"
      },
      {
        "paperId": "4f657dd99723932d6b4476372d56fe11b938d9a4",
        "title": "Strategyproof Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "23f37bb99c627c625331e819cd19cf7a8e990c4e",
        "title": "The Battling Influencers Game: Nash Equilibria Structure of a Potential Game and Implications to Value Alignment"
      }
    ],
    "score": 7.0
  },
  {
    "id": "1ab303435946a859620ca334556ca3b0e53464fc",
    "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
    "authors": [
      "Abdul Basit",
      "Khizar Hussain",
      "M. Hanif",
      "Muhammad Shafique"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The system is implemented on various consumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\% accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an energy-efficient healthcare assistance platform that alleviates privacy concerns due to edge-based deployment, thereby empowering the community.",
    "url": "https://www.semanticscholar.org/paper/1ab303435946a859620ca334556ca3b0e53464fc",
    "pdf_url": "https://arxiv.org/pdf/2403.00830.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-02-28",
    "externalIds": {
      "ArXiv": "2403.00830",
      "DBLP": "journals/corr/abs-2403-00830",
      "DOI": "10.48550/arXiv.2403.00830",
      "CorpusId": 268231087
    },
    "references": [
      {
        "paperId": "ebedc4d7a2356090904baba4104ef0832bc236df",
        "title": "A survey on multimodal large language models"
      },
      {
        "paperId": "90e41626b8c78600da70c4350c67c3a10525cb37",
        "title": "MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data"
      },
      {
        "paperId": "bce55193d9a887ad00774a9134df08cd521a85ae",
        "title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "7be169a70f5db74f40adfd2790021aa3fbf3cb87",
        "title": "Artificial Hallucinations in ChatGPT: Implications in Scientific Writing"
      },
      {
        "paperId": "23cae400cfd1a7c455c721256b838e98a307d5e6",
        "title": "ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports"
      },
      {
        "paperId": "44279244407a64431810f982be6d0c7da4429dd7",
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
        "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "124ee9b904b1b527368523c36a31c2015f799c69",
        "title": "Mortality due to low-quality health systems in the universal health coverage era: a systematic analysis of amenable deaths in 137 countries"
      },
      {
        "paperId": "69df21418c865c9743fd7b87d2e6ecc60f521904",
        "title": "Reconsidering the role of language in medicine"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
        "title": "Deep Contextualized Word Representations"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "2cbb8de53759e75411bc528518947a3094fbce3a",
        "title": "Billion-Scale Similarity Search with GPUs"
      },
      {
        "paperId": null,
        "title": "Bigscience language open-science open-access multilingual (bloom) language model"
      },
      {
        "paperId": null,
        "title": "World bank and who: Half the world lacks access to essential health services, 100 million still pushed into extreme poverty because of health expenses"
      },
      {
        "paperId": "f7ad35a51c59ff24fcd2d7f438a96bbfb922bde5",
        "title": "Measuring Models"
      },
      {
        "paperId": "d5ccb7ae5f596560a1b797a7f7e03469a63da519",
        "title": "HuatuoGPT, Towards Taming Language Models To Be a Doctor"
      },
      {
        "paperId": null,
        "title": "A robust pre-trained model in"
      },
      {
        "paperId": null,
        "title": "Sparksofartificialgeneralintelligence:Earlyexperiments with gpt-4. 3"
      },
      {
        "paperId": null,
        "title": "Chatgptinmedicine:anoverviewofitsapplications,advantages, limitations, future prospects, and ethical considerations"
      },
      {
        "paperId": null,
        "title": "Chatdoctor"
      }
    ],
    "cited_by": [
      {
        "paperId": "571881e7c3659d98858554ed4b3c26b97bf070fc",
        "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making"
      },
      {
        "paperId": "99efed7da8b070eb90b4a1860c2b627a4260a448",
        "title": "Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications"
      },
      {
        "paperId": "013d2852e9f3d74cb5528b27f3664231bbd898ff",
        "title": "Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore Framework: Low-cost and High-precision Implementation with Ascend 310 Chip"
      },
      {
        "paperId": "2d97083bc7973e8f7ea5a30941decb8c24e15153",
        "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice"
      },
      {
        "paperId": "ff785ac9817dd5ab1917b4ca61bad2e70cff76b9",
        "title": "eHealth Assistant AI Chatbot Using a Large Language Model to Provide Personalized Answers through Secure Decentralized Communication"
      },
      {
        "paperId": "73f55e8877db271dd2881678e386b8b1b329912b",
        "title": "Large Language Models for Wearable Sensor-Based Human Activity Recognition, Health Monitoring, and Behavioral Modeling: A Survey of Early Trends, Datasets, and Challenges"
      },
      {
        "paperId": "656e8c5f8bced540425c12d854b2911dddefff14",
        "title": "Multimodal data integration for oncology in the era of deep neural networks: a review"
      }
    ],
    "score": 7.0
  },
  {
    "id": "fde0ffe77186561497ce15e4faca82db11dacd64",
    "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
    "authors": [
      "Xiaoyuan Liu",
      "Tian Liang",
      "Zhiwei He",
      "Jiahao Xu",
      "Wenxuan Wang",
      "Pinjia He",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "year": 2025,
    "citationCount": 7,
    "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.",
    "url": "https://www.semanticscholar.org/paper/fde0ffe77186561497ce15e4faca82db11dacd64",
    "pdf_url": "https://arxiv.org/pdf/2505.13445.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-19",
    "externalIds": {
      "ArXiv": "2505.13445",
      "DBLP": "journals/corr/abs-2505-13445",
      "DOI": "10.48550/arXiv.2505.13445",
      "CorpusId": 278770036
    },
    "references": [
      {
        "paperId": "143e18bfd7c356592e7c1439738a3525d3e16279",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
      },
      {
        "paperId": "b5f6c29c6c9446e69a3f1c7508b2a1a7012b53c1",
        "title": "Heimdall: test-time scaling on the generative verification"
      },
      {
        "paperId": "d981ce332586e6a29f595cbdfa9347cf425e5cd0",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
      },
      {
        "paperId": "5d266400187708cea1305fa34d2bc5e1a47b1a57",
        "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "a4e22a5e5dcaa70393442fb740a404fc96ea1b95",
        "title": "Learning to Solve and Verify: A Self-Play Framework for Code and Test Generation"
      },
      {
        "paperId": "45e1c99a1c8935bf137c0b51a08a03ffb6821993",
        "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "7c8a43b48db962a9bf01800663cc81857679905c",
        "title": "Teaching Language Models to Critique via Reinforcement Learning"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "985298bd0b6f4c84ab34bb15cfed0b092f43b83a",
        "title": "OpenAI o1 System Card"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "5a88e0fa857fd3b9840a2b74bb0f667f2c3e2542",
        "title": "Self-Improvement in Language Models: The Sharpening Mechanism"
      },
      {
        "paperId": "7d3ed6f660cf63236092ff3b3a95187b9f463330",
        "title": "Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "043731dd9f2292b7e4788d0b97edee5db170f840",
        "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
      },
      {
        "paperId": "94c82c25d8943fea9461d804257ee16c9d672548",
        "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "4ba57555bef02f988f2ed3bab2f102733dc55221",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "8a7acaf6469c06ae5876d92f013184db5897bb13",
        "title": "Neuronlike adaptive elements that can solve difficult learning control problems"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      },
      {
        "paperId": null,
        "title": "Wikipedia contributors"
      },
      {
        "paperId": null,
        "title": "A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards"
      },
      {
        "paperId": null,
        "title": "### Final Rating: \\\\ boxed { -0.5 } Figure 14: Example verification returned by RISE-7B on AIME 24. 19"
      },
      {
        "paperId": null,
        "title": "Mind the gap: Examining the self-improvement capabilities of large language models"
      },
      {
        "paperId": null,
        "title": "There may not be aha moment in r1-zero-like training \u2014 a pilot study"
      }
    ],
    "cited_by": [
      {
        "paperId": "dc504efda638aecc57206ac9f7f300575222f120",
        "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning"
      },
      {
        "paperId": "1f58afb3bfc2c4581f1a2c4f051d15f11d2c357a",
        "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers"
      },
      {
        "paperId": "ac8df778d73ae85751cbebf6ad79bd9b231efc6a",
        "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling"
      },
      {
        "paperId": "5bb3e1643f7c1aafaa1ad78c4c4be13838f0faa1",
        "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"
      },
      {
        "paperId": "ae42cc15452b5b6d089bc535cdcd55648c29557d",
        "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning"
      },
      {
        "paperId": "658674f1b5e5cb6e7be6f14735b4c3cc6778aec3",
        "title": "Perception-Aware Policy Optimization for Multimodal Reasoning"
      },
      {
        "paperId": "7da361cbbf5e0c10e6eec89f8e8e1618a6778b84",
        "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents"
      }
    ],
    "score": 7.0
  },
  {
    "id": "32fdc2cf900e1af1acc4264f312a55c1de5879d3",
    "title": "Measuring memorization in RLHF for code completion",
    "authors": [
      "Aneesh Pappu",
      "Billy Porter",
      "Ilia Shumailov",
      "Jamie Hayes"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences. Unlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment process. Understanding this relationship is important as real user data may be collected and used to align large models; if user data is memorized during RLHF and later regurgitated, this could raise privacy concerns. In addition to RLHF, other methods such as Direct Preference Optimization (DPO) and $\\Psi$PO have gained popularity for learning directly from human preferences, removing the need for optimizing intermediary reward models with reinforcement learning. In this work, we analyze how training data memorization can surface and propagate through each phase of RLHF and direct preference learning. We focus our study on code completion models, as code completion is one of the most popular use cases for large language models. We find that RLHF significantly decreases the chance that data used for reward modeling and reinforcement learning is memorized in comparison to directly fine-tuning on this data, but that examples already memorized during the fine-tuning stage of RLHF, will, in the majority of cases, remain memorized after RLHF. In contrast, we find that aligning by learning directly from human preference data via a special case of $\\Psi$PO, Identity Preference Optimization (IPO), increases the likelihood that training data is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed to direct preference learning, is a safer way to mitigate the risk of regurgitating sensitive preference data when aligning large language models. We find our conclusions are robust across multiple code completion datasets, tasks, and model scales.",
    "url": "https://www.semanticscholar.org/paper/32fdc2cf900e1af1acc4264f312a55c1de5879d3",
    "pdf_url": "https://arxiv.org/pdf/2406.11715.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-06-17",
    "externalIds": {
      "ArXiv": "2406.11715",
      "DBLP": "journals/corr/abs-2406-11715",
      "DOI": "10.48550/arXiv.2406.11715",
      "CorpusId": 270560937
    },
    "references": [
      {
        "paperId": "3cbfabf0f5335eb0c58ffdffe3cae0275d6b1ad2",
        "title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with Selective Entropy Maximization"
      },
      {
        "paperId": "d62c17ea0537f6f02541ee35c1cdce78d4e1cce6",
        "title": "Demystifying Verbatim Memorization in Large Language Models"
      },
      {
        "paperId": "c6029001c0557f3912cb361c5c1cd406aebc3889",
        "title": "Towards More Realistic Extraction Attacks: An Adversarial Perspective"
      },
      {
        "paperId": "dd3268b6fb685b7e31ca576f2a629467da90d662",
        "title": "CodeGemma: Open Code Models Based on Gemma"
      },
      {
        "paperId": "a58112381efffe6f1b43f8b663a5d30a2ff27eef",
        "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models"
      },
      {
        "paperId": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6",
        "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"
      },
      {
        "paperId": "f16c669c126c800dac38d445ddd178ffb9af5b7a",
        "title": "Do Membership Inference Attacks Work on Large Language Models?"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "fc7ee1828030a818f52518022a39f6a3ada60222",
        "title": "Scalable Extraction of Training Data from (Production) Language Models"
      },
      {
        "paperId": "3422d5e0cdfdc935d6a84a1e3d3f96659265fe3a",
        "title": "Detecting Pretraining Data from Large Language Models"
      },
      {
        "paperId": "cf08fa8e17e4cfe44ef728dccc296467eb5b88b4",
        "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "d5f4ecbb3fc2220eed7c62ea308e4f6cba2240b5",
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models"
      },
      {
        "paperId": "8353270c28a542735c1ce8af2ff998146b620844",
        "title": "Exploring Memorization in Fine-tuned Language Models"
      },
      {
        "paperId": "fec455cd5798dfcb70e741d76c88326d20d2536a",
        "title": "Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "deb8f26509ae320fc975b32922416cb156c61bbd",
        "title": "Emergent and Predictable Memorization in Large Language Models"
      },
      {
        "paperId": "ec4c8d99eb1c028c43af6d8bbf727392d351cb59",
        "title": "Efficient Training of Language Models to Fill in the Middle"
      },
      {
        "paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
        "title": "InCoder: A Generative Model for Code Infilling and Synthesis"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d",
        "title": "Quantifying Memorization Across Neural Language Models"
      },
      {
        "paperId": "39c77e29a232a9fb62b3a3c89c50f487d73e27ce",
        "title": "Counterfactual Memorization in Neural Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "870ff1dde0c103c3d90be51880f984628e77a8d6",
        "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"
      },
      {
        "paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba",
        "title": "Extracting Training Data from Large Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "3ed06aca3b25a9af89f08b949753372d29647a10",
        "title": "Trading Off Diversity and Quality in Natural Language Generation"
      },
      {
        "paperId": "782e7fba4dfa8e2095a71cfcf85995543f37ac75",
        "title": "Analyzing Information Leakage of Updates to Natural Language Models"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "title": "Hierarchical Neural Story Generation"
      },
      {
        "paperId": "6db2b93a2d4007371030644173f1001c959214d2",
        "title": "Learning to Write with Cooperative Discriminators"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "d0d3f4d1003db0fb637519ef5d8bb140e7df8355",
        "title": "May the source be with you."
      },
      {
        "paperId": "b30cadaa4d99cdd80d8f2309e873452f24cc51bb",
        "title": "Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models"
      },
      {
        "paperId": "51256ee5425d5c425b84e7fac011775d8eff0d1c",
        "title": "An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models"
      },
      {
        "paperId": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
        "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Less more for alignment"
      },
      {
        "paperId": null,
        "title": "Open models based on gemini research and technology"
      },
      {
        "paperId": null,
        "title": "Tabnine"
      },
      {
        "paperId": null,
        "title": "Measuring memorization in RLHF for code completion"
      },
      {
        "paperId": null,
        "title": "/ User/KimberlyCooper/Documents/Travel/Itinerary"
      },
      {
        "paperId": null,
        "title": "Sourcegraph"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "jailbreak backdoors from"
      }
    ],
    "cited_by": [
      {
        "paperId": "a6022035e11fc52c1faeeefbc2efa18c5a89d226",
        "title": "Rethinking Memorization Measures and their Implications in Large Language Models"
      },
      {
        "paperId": "eb5d1d488dfee934566e51afe22d41f13b603035",
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation"
      },
      {
        "paperId": "30b6c1f0081195763103015b3d2b7cb8baa74faf",
        "title": "Memorize or Generalize? Evaluating LLM Code Generation with Code Rewriting"
      },
      {
        "paperId": "24e8029197b5f99126f0ef4e577d55a275405b36",
        "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge"
      },
      {
        "paperId": "7a45f089010d5d75b895daf92e915aec705ffc67",
        "title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks Before and After Fine-tuning"
      },
      {
        "paperId": "9177570e44aca1129bf9e129117004f7a240a599",
        "title": "Predicting memorization within Large Language Models fine-tuned for classification"
      },
      {
        "paperId": "fe767810b3c07bdf6f7e08142bbeadfcebb4509e",
        "title": "Rethinking Memorization Measures in LLMs: Recollection vs. Counterfactual vs. Contextual Memorization"
      }
    ],
    "score": 7.0
  },
  {
    "id": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
    "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference",
    "authors": [
      "Qining Zhang",
      "Lei Ying"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Reward inference (learning a reward model from human preferences) is a critical intermediate step in the Reinforcement Learning from Human Feedback (RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF faces fundamental challenges such as distribution shift, reward model overfitting, and problem misspecification. An alternative approach is direct policy optimization without reward inference, such as Direct Preference Optimization (DPO), which provides a much simpler pipeline and has shown empirical success in LLM applications. However, DPO utilizes the closed-form expression between the optimal policy and the reward function, which is only suitable under the bandit setting or deterministic MDPs. This paper develops two RLHF algorithms without reward inference for general RL problems beyond bandits and deterministic MDPs, and general preference models beyond the Bradley-Terry model. The key idea is to estimate the local value function difference from human preferences and then approximate the policy gradient with a zeroth-order gradient approximator. For both algorithms, we establish polynomial convergence rates in terms of the number of policy gradient iterations, the number of trajectory samples, and human preference queries per iteration. Numerical experiments in stochastic environments validate the performance of our proposed algorithms, outperforming popular RLHF baselines such as DPO and PPO. Our paper shows there exist provably efficient methods to solve general RLHF problems without reward inference.",
    "url": "https://www.semanticscholar.org/paper/85c961d5b3fea95b48f94c0461782e887a8b3b0f",
    "pdf_url": "https://arxiv.org/pdf/2409.17401.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-09-25",
    "externalIds": {
      "DBLP": "conf/iclr/Zhang025",
      "ArXiv": "2409.17401",
      "DOI": "10.48550/arXiv.2409.17401",
      "CorpusId": 272910996
    },
    "references": [
      {
        "paperId": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
        "title": "Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "e81c91cd71a3310e33e1bffc713aec4de608f40b",
        "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"
      },
      {
        "paperId": "0f834f9944b1c842e8107beb0dc85466d854c2bd",
        "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      },
      {
        "paperId": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
        "title": "A Survey of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "f2f9cc38d5a1a4fdef996ea03fdb71e01f65d574",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "acd0afc43871b626c5f2ec48a0c4e1453b94b670",
        "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cedfdde4b9d01530bf2932554561bb25623890e5",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41",
        "title": "Fine-Tuning Language Models with Just Forward Passes"
      },
      {
        "paperId": "cab74f898a3523f1ceb8fe48dcda8ec467a3d95b",
        "title": "Provable Offline Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "fbe1003ec391f6bcf4660f6ef81f1e6199849bfe",
        "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning"
      },
      {
        "paperId": "2044ab82dcb2c11ef660bd51d40130fe182f98d3",
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "5c37023c35fc1c95565d56b4fc4821fcf768651a",
        "title": "Reward is enough for convex MDPs"
      },
      {
        "paperId": "32e0f1bf36ccee6ed552909c2c76f9d6b1c760ef",
        "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
        "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities"
      },
      {
        "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
        "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "9d4d8509f6da094a7c31e063f307e0e8592db27f",
        "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress"
      },
      {
        "paperId": "65f0e9db55f498bb196de3950393f5ded14bcc72",
        "title": "Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization"
      },
      {
        "paperId": "c983653841b6987d9959318f074a595783838576",
        "title": "On the Convergence of Adam and Beyond"
      },
      {
        "paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54",
        "title": "signSGD: compressed optimisation for non-convex problems"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "41f48a3b065cb038fa98f91c4775bf1a91f47764",
        "title": "On the Information-Adaptive Variants of the ADMM: An Iteration Complexity Perspective"
      },
      {
        "paperId": "7b54df5fff791cb4212fff31d83cfdd401027e6a",
        "title": "Regret Analysis for Continuous Dueling Bandit"
      },
      {
        "paperId": "003fd30860e19a9dc40413d5f8a5d45e2667d30f",
        "title": "Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications"
      },
      {
        "paperId": "abcf11a9af3d83f85c5fbfffc5901d416ca7a73f",
        "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "07f5bae91cd45eafe82f3548a43268eb5c84df7a",
        "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\u0141ojasiewicz Condition"
      },
      {
        "paperId": "673bed25e16ddc632e4bfafbdf9765b71eb138ab",
        "title": "Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit"
      },
      {
        "paperId": "f009a64f87841b79b1e2ea1748366270455ef9c0",
        "title": "Model-Free Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "b322d2a8e55e6ae30b1cc0b0b09baddd91fcf582",
        "title": "Discrete Choice Methods with Simulation"
      },
      {
        "paperId": "21a0b0fbdde1aee56fe10e69e897decaf21f43a6",
        "title": "Random Gradient-Free Minimization of Convex Functions"
      },
      {
        "paperId": "cc2e61e06917238ca745155d9f1f0e679e63866e",
        "title": "Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm"
      },
      {
        "paperId": "16302319d910a1da77656133727f081c65995635",
        "title": "Programming by Feedback"
      },
      {
        "paperId": "df742fb7b0106df3970cd4781a72d5ef9a155a34",
        "title": "Reducing Dueling Bandits to Cardinal Bandits"
      },
      {
        "paperId": "8424a9e5a4456a2c45a42e392b9c01cd0c5c9467",
        "title": "Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming"
      },
      {
        "paperId": "b7f8eaf739b33115fd57e1b4051be9f2049668fa",
        "title": "A Fast Bandit Algorithm for Recommendation to Users With Heterogenous Tastes"
      },
      {
        "paperId": "00715f02ba59f772f469800e25bf07e834b631a2",
        "title": "On the complexity analysis of randomized block-coordinate descent methods"
      },
      {
        "paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"
      },
      {
        "paperId": "149d59e53976d8b4f2a22b84aabe1f42cc1efd79",
        "title": "Random Utility Theory for Social Choice"
      },
      {
        "paperId": "0059cfac9c5b7811866f0729d0917b7478148fc5",
        "title": "Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems"
      },
      {
        "paperId": "c867bfc8b120fa0f3df7e7f82d75a16663d6d27c",
        "title": "Modeling Ordered Choices: A Primer"
      },
      {
        "paperId": "035bd1607d664b0c10143ca055f1c645b7d04a11",
        "title": "Interactively optimizing information retrieval systems as a dueling bandits problem"
      },
      {
        "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
        "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "3ae1544865a18d8649a5c4939f9eb17165b23bea",
        "title": "signSGD via Zeroth-Order Oracle"
      },
      {
        "paperId": null,
        "title": "on Machine Learning , volume 97 of Proceedings of Machine Learning Research"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Author manuscript, published in \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (2011)\" Preference-based Policy Learning"
      },
      {
        "paperId": "4167af2a954eabb1902afc041654743ec9df9948",
        "title": "A law of comparative judgment."
      },
      {
        "paperId": null,
        "title": "Evolutionsstrategie. Optimierung technischer Systeme nach Prinzipien derbiologischen"
      },
      {
        "paperId": null,
        "title": "A framework for partially observed reward-states in rlhf"
      }
    ],
    "cited_by": [
      {
        "paperId": "7af0dee85b349af0ff3e98decac1e393db84c118",
        "title": "Fusing Rewards and Preferences in Reinforcement Learning"
      },
      {
        "paperId": "5bb3e1643f7c1aafaa1ad78c4c4be13838f0faa1",
        "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "caf20774ab23a550995ca13638c9b4e2704311dc",
        "title": "ZO-APFPG: A Provably Zeroth-Order Adaptive Personalized Federated Policy Gradient Algorithm for Reinforcement Learning with Environment Heterogeneity"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "a650671e202572a9e9eaf6d68374ab56e39fd4b0",
        "title": "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth- and First-Order Optimization"
      }
    ],
    "score": 6.0
  },
  {
    "id": "2530e6ecbd0198012bb8ee4359acb9241cefec95",
    "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
    "authors": [
      "\u00c1ngela L\u00f3pez-Cardona",
      "Carlos Segura",
      "Alexandros Karatzoglou",
      "Sergi Abadal",
      "Ioannis Arapakis"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.",
    "url": "https://www.semanticscholar.org/paper/2530e6ecbd0198012bb8ee4359acb9241cefec95",
    "pdf_url": "https://arxiv.org/pdf/2410.01532.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-10-02",
    "externalIds": {
      "DBLP": "conf/iclr/Lopez-CardonaSK25",
      "ArXiv": "2410.01532",
      "DOI": "10.48550/arXiv.2410.01532",
      "CorpusId": 273026214
    },
    "references": [
      {
        "paperId": "1035dfceeebd3dd9ba52a8162eeda670a432c56e",
        "title": "Self-Taught Evaluators"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
        "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"
      },
      {
        "paperId": "6ea824ef01cc534baeb666b927e0730d70cfde5c",
        "title": "Predicting Code Comprehension: A Novel Approach to Align Human Gaze with Code using Deep Neural Networks"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "d94e94a744520eebf2d64c15eac87b44e827d2dc",
        "title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "40cc085a2608985b753c38dc245ac21be592ed08",
        "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback"
      },
      {
        "paperId": "ba63cee25308cb977f902414a6a5c6ba5d83a95c",
        "title": "EyeTrans: Merging Human and Machine Attention for Neural Code Summarization"
      },
      {
        "paperId": "711ac30df7909fc9de881933580c642e7c3e2b08",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"
      },
      {
        "paperId": "111ab6e2d01153d2fff91788d659545936d06231",
        "title": "Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks"
      },
      {
        "paperId": "9ee149df91046ece6def7fdf02c773a1ec6b8f72",
        "title": "ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts"
      },
      {
        "paperId": "7e94ef5623b249d196efafb9a1cbf356bdc17694",
        "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding"
      },
      {
        "paperId": "e6776f5f293c18f4b2322b1479f083cb24d33343",
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475",
        "title": "Self-Alignment with Instruction Backtranslation"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "01d31fb9fc6ab36df6627b8555b64789113eb7a5",
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a",
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"
      },
      {
        "paperId": "817902dda27850d5f3ac562c1298cc647fe78112",
        "title": "Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "c3349d0493a56f76c27d74c5056ab039f665b7b4",
        "title": "Synthesizing Human Gaze Feedback for Improved NLP Performance"
      },
      {
        "paperId": "c1c7cd08b9234d4806c3521f2db62c98e0b10900",
        "title": "PLM-AS: Pre-trained Language Models Augmented with Scanpaths for Sentiment Classification"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "f877c616c981d96277e049205ae0ad715ba59013",
        "title": "Inferring Native and Non-Native Human Reading Comprehension and Subjective Text Difficulty from Scanpaths in Reading"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "f35fad630cc675e868e3cf37ea64ceeff80b9eda",
        "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals"
      },
      {
        "paperId": "0fd0311c405e907b640b2bae00c3ea90a7f02d88",
        "title": "Sentiment Analysis with Cognitive Attention Supervision"
      },
      {
        "paperId": "e1afe296314493791b1c2c0af88cfe97279bdc69",
        "title": "Relative Importance in Sentence Processing"
      },
      {
        "paperId": "9193cfcda57691c942eeba17965d95c14ec6e371",
        "title": "CMCL 2021 Shared Task on Eye-Tracking Prediction"
      },
      {
        "paperId": "310213842f3646d5fc0a5dbbd4e0ef3d0130b91c",
        "title": "TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction"
      },
      {
        "paperId": "54d1079968596ea2ffe17ef3eeb854ef488b4882",
        "title": "Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention"
      },
      {
        "paperId": "e6cbd4d8b91444e4704db7849596493882fa2d3f",
        "title": "Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension"
      },
      {
        "paperId": "96ee09542f31bcaf21a139fbec62e0864da33687",
        "title": "A Survey on Using Gaze Behaviour for Natural Language Processing"
      },
      {
        "paperId": "ec75a3e873c7f08ccdc9da6d674f326ea842d98d",
        "title": "Towards Predicting Reading Comprehension From Gaze Behavior"
      },
      {
        "paperId": "505d655df513d990ab63ad59d7166497e7a03037",
        "title": "Towards Best Practices for Leveraging Human Language Processing Signals for Natural Language Processing"
      },
      {
        "paperId": "dc451f67c1bffe0fe39d2580716721dc332464b6",
        "title": "Step I"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "a1030846ad58681bd33b6c936e7d79a7f458dbe7",
        "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "3caaf5c1a3094f599260863c190dfb3e5fe01614",
        "title": "Advancing NLP with Cognitive Language Processing Signals"
      },
      {
        "paperId": "54c2e45ca1cc280dfa48b6da8b5561b6513ca4a2",
        "title": "Entity Recognition at First Sight: Improving NER with Eye Movement Information"
      },
      {
        "paperId": "d780ef024f7d974b81a2254f2a28c141d7b99f62",
        "title": "ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading"
      },
      {
        "paperId": "0eba63e82b156d47d9c513338bc4101372ba00df",
        "title": "Sequence Classification with Human Attention"
      },
      {
        "paperId": "5eab89140b1162134f0a59259a1ecfcf35f07059",
        "title": "Gaze-Based Signatures of Mind Wandering During Real-World Scene Processing"
      },
      {
        "paperId": "58bb899449ef5af014df80bd7fcb2b1fcf5eb63a",
        "title": "Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour"
      },
      {
        "paperId": "ef52f8f774464961139fd45be462b133255e6147",
        "title": "Cognition-Cognizant Sentiment Analysis With Multitask Subjectivity Summarization Based on Annotators' Gaze Behavior"
      },
      {
        "paperId": "0736a7491f3bb23b822a2fdd5c0dc715dbeebf47",
        "title": "The Provo Corpus: A large eye-tracking corpus with predictability norms"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7c6b9110fc2c53f7fb01569604b205c7a9eb5c78",
        "title": "Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading"
      },
      {
        "paperId": "427b6099dae71d68e8d395a5a379053357bec5e8",
        "title": "Leveraging Cognitive Features for Sentiment Analysis"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "9ec751343e8343d74b7763cba511a9653a491ca4",
        "title": "Improving sentence compression by learning to predict gaze"
      },
      {
        "paperId": "40ced5421c601e949da4296b4fdec40cd41371bf",
        "title": "Predicting Readers' Sarcasm Understandability by Modeling Gaze Behavior"
      },
      {
        "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
        "title": "Long Short-Term Memory"
      },
      {
        "paperId": "059885cd1e8094b03fcc0d7ece8ee9b4896449fe",
        "title": "The knowledge base of the oculomotor system."
      },
      {
        "paperId": "17ab4f6a0cd5c275ca0aab4a8d3ab60704ec3b6a",
        "title": "Gaze and eye contact: a research review."
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "988913a4f334eed4441ca38d8493528566cf359b",
        "title": "Eye-Tracking Features Masking Transformer Attention in Question-Answering Tasks"
      },
      {
        "paperId": "c529fe5a12d0e7a0d3d7639dd280abe83595b3d9",
        "title": "CeER: A Nested Name Entity Recognition Model Incorporating Gaze Feature"
      },
      {
        "paperId": "1eecfd21543286d45ca44e424a2e351d1ee6ab12",
        "title": "The Trickle-down Impact of Reward Inconsistency on RLHF"
      },
      {
        "paperId": "8d58435ac48b34c7b3cb0d5058247388b8d8c1b3",
        "title": "An Eye Opener Regarding Task-Based Text Gradient Saliency"
      },
      {
        "paperId": "98724f98cd99f553ae421d6975c8b05f75acce81",
        "title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision"
      },
      {
        "paperId": null,
        "title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling"
      },
      {
        "paperId": "03d7e537c0ea22ba28a29d6afb1887e2ab681e46",
        "title": "Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": "2dad9763f8b128da231b3fb9c9fff7ad730b89a1",
        "title": "Long-Range Language Modeling with Selective Cache"
      },
      {
        "paperId": null,
        "title": "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF,"
      },
      {
        "paperId": "55111a35c00e1cad98ee9a4a59fde8871e6bcbff",
        "title": "Kaplan"
      },
      {
        "paperId": "5bf8d34926500cffe888bd27a40de7e9fe4402ff",
        "title": "GECO-MT: The Ghent Eye-tracking Corpus of Machine Translation"
      },
      {
        "paperId": "08ec1848527cd46e34b04ff401b01785cf04e935",
        "title": "Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences"
      },
      {
        "paperId": "5c29e4d1ef38b4ddd05a7a6d642eb4c8bacee580",
        "title": "CMCL 2022 Shared Task on Multilingual and Crosslingual Prediction of Human Reading Behavior"
      },
      {
        "paperId": "0d2868aba3c76c1238834d9cdbb2b0f7a48104ca",
        "title": "SEMGraph: Incorporating Sentiment Knowledge and Eye Movement into Graph Model for Sentiment Analysis"
      },
      {
        "paperId": null,
        "title": "ULTRAFEEDBACK: boosting language models with scaled AI feedback"
      },
      {
        "paperId": "38d1681bc70963455aeff1dc384010c064f2d74f",
        "title": "The Pupil Becomes the Master: Eye-Tracking Feedback for Tuning LLMs"
      },
      {
        "paperId": null,
        "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "Mistral"
      },
      {
        "paperId": null,
        "title": "Paper under review"
      },
      {
        "paperId": null,
        "title": "Gemini: A Family of Highly Capable Multimodal Models"
      }
    ],
    "cited_by": [
      {
        "paperId": "ef4e21071bab214fe11b27b8440664b955808f3e",
        "title": "Enhancing RLHF with Human Gaze Modeling"
      },
      {
        "paperId": "87a995566282e5985305c69fb91e0ba147282244",
        "title": "Human Attention During Localization of Memory Bugs in C Programs"
      },
      {
        "paperId": "9921e72dcbc1048e699b8edd14b624768a7a87ce",
        "title": "ScanDL 2.0: A Generative Model of Eye Movements in Reading Synthesizing Scanpaths and Fixation Durations"
      },
      {
        "paperId": "2e4b82baa8c01f972414f965622d8f5efbec6add",
        "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading"
      },
      {
        "paperId": "f1c16e5d31b7e0ebb0050a2c10a0276f5b8c612f",
        "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions"
      },
      {
        "paperId": "108af17d9285202c6851ced95bd8d7b628e5f72c",
        "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses"
      }
    ],
    "score": 6.0
  },
  {
    "id": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
    "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback",
    "authors": [
      "Adam Dahlgren Lindstr\u00f6m",
      "Leila Methnani",
      "Lea Krause",
      "Petter Ericson",
      "\u00cd\u00f1igo Martinez de Rituerto de Troya",
      "Dimitri Coelho Mollo",
      "Roel Dobbe"
    ],
    "year": 2025,
    "citationCount": 6,
    "abstract": "This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLHF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics, and contributing to AI safety. We highlight tensions inherent in the goals of RLHF, as captured in the HHH principle (helpful, harmless and honest). In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLHF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We offer an alternative vision for AI safety and ethics which positions RLHF approaches within a broader context of comprehensive design across institutions, processes and technological systems, and suggest the establishment of AI safety as a sociotechnical discipline that is open to the normative and political dimensions of artificial intelligence.",
    "url": "https://www.semanticscholar.org/paper/ff5b0cc250d93b97fe60e1b0c2048708d6875595",
    "pdf_url": "https://doi.org/10.1007/s10676-025-09837-2",
    "venue": "Ethics and Information Technology",
    "publicationDate": "2025-06-01",
    "externalIds": {
      "DBLP": "journals/ethicsit/LindstromMKETMD25",
      "PubMedCentral": "12137480",
      "DOI": "10.1007/s10676-025-09837-2",
      "CorpusId": 279163270,
      "PubMed": "40486676"
    },
    "references": [
      {
        "paperId": "4e4b7d0a6c6e5d722e941a8025758be1038efc57",
        "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation"
      },
      {
        "paperId": "ce04df2c7e2d87ff4ec9d4bb8d29b85d5528df41",
        "title": "From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems"
      },
      {
        "paperId": "077f1357ff1bfdd1d9a764449f13ea7615430b13",
        "title": "Toward Sociotechnical AI: Mapping Vulnerabilities for Machine Learning in Context"
      },
      {
        "paperId": "f9104ccb838c7658b1586c9bb53e8b4dbbafdd6d",
        "title": "The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models"
      },
      {
        "paperId": "addc34e1db56c46c399a3b319153be0b73186d19",
        "title": "The benefits, risks and bounds of personalizing the alignment of large language models to individuals"
      },
      {
        "paperId": "291923449015d8fdd13e8af432a7b1169666dcec",
        "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?"
      },
      {
        "paperId": "5ce8f0e5abcf9e62d73e4b0bda56b2224ea1b712",
        "title": "Diversity and language technology: how language modeling bias causes epistemic injustice"
      },
      {
        "paperId": "faca120839a908a579c96f01f7dd0dfdcb379813",
        "title": "Understanding users\u2019 responses to disclosed vs. undisclosed customer service chatbots: a mixed methods study"
      },
      {
        "paperId": "b5bf680b544491965809cbd68cfb2952894b6666",
        "title": "Concrete Problems in AI Safety, Revisited"
      },
      {
        "paperId": "9af90d40ccd2f7d99605406c4d4c0c54ca436541",
        "title": "The Alignment Problem in Context"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "e6423c211fea2945aa71e1ac5ea24f8f595b4b0a",
        "title": "Towards Understanding Sycophancy in Language Models"
      },
      {
        "paperId": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208",
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
      },
      {
        "paperId": "afe455cfb03b9487c8f0b67993f42f834b633c23",
        "title": "AI deception: A survey of examples, risks, and potential solutions"
      },
      {
        "paperId": "ac1788e9a168a6455beb6316f316950842297c11",
        "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"
      },
      {
        "paperId": "7142e920b6b9355d9cbacc9450818f912eca138e",
        "title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment"
      },
      {
        "paperId": "a37d5620210276e47cf0c9dd2898c2a82c9d0422",
        "title": "Simple synthetic data reduces sycophancy in large language models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "27e2715153ba441dff438bc015f7c765482c74c9",
        "title": "Diagnosing and Addressing Emergent Harms in the Design Process of Public AI and Algorithmic Systems"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "7ace46ab8e71c4304682ab126b1212deb54b9b03",
        "title": "Style Over Substance: Evaluation Biases for Large Language Models"
      },
      {
        "paperId": "494b043fce4da2ecc7f87bc96f7c29a5278cca61",
        "title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "1a67176617a1aab1cf0690f95faddf33e826a64b",
        "title": "Algorithms as Social-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic Audits"
      },
      {
        "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"
      },
      {
        "paperId": "a54fe74732f8f6bc430fdc246e907c903f62d2af",
        "title": "ACROCPoLis: A Descriptive Framework for Making Sense of Fairness"
      },
      {
        "paperId": "528bfc811a3b4213566134afe2c880f867be5065",
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity"
      },
      {
        "paperId": "cef330bacf014d60daabbd489647b2006af130ca",
        "title": "Discovering Language Model Behaviors with Model-Written Evaluations"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "bd6c0af91401c208ef478376c493408b69f51b8d",
        "title": "Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction"
      },
      {
        "paperId": "0aa23f751523de433205baebce2a33e7af965198",
        "title": "AI, Opacity, and Personal Autonomy"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "94a8053f08556a36df8722f53fe9616f3ea9d444",
        "title": "CounterFAccTual: How FAccT Undermines Its Organizing Principles"
      },
      {
        "paperId": "3ec67799c62033373d8a91145374d6275336de71",
        "title": "The Data-Production Dispositif"
      },
      {
        "paperId": "4c868a92c615df3859433e28b2441bbce9e65fb3",
        "title": "Reward Reports for Reinforcement Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "598231eb906b183f7a2a408ef4536127e11e3de9",
        "title": "Challenges and Strategies in Cross-Cultural NLP"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "292d5fffe591d08702e8ddbe741d6f6e3b748cc2",
        "title": "System Safety and Artificial Intelligence"
      },
      {
        "paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "title": "Ethical and social risks of harm from Language Models"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "2ef4ab54d00203f9ac610213ac3abc8e1fe541b4",
        "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling"
      },
      {
        "paperId": "d11217e882e54fc3dfe948fa102555f75d0c80bc",
        "title": "Hard Choices in Artificial Intelligence"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "db15092a8ea2e1cd27a24c8550f6b448dd54955c",
        "title": "Participation Is not a Design Fix for Machine Learning"
      },
      {
        "paperId": "e0d8768e4759eb16674c7b1e495ccaa4f7752694",
        "title": "Don\u2019t ask if artificial intelligence is good or fair, ask how it shifts power"
      },
      {
        "paperId": "0e141942fa265142f41a2a26eb17b6005d3af29e",
        "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World"
      },
      {
        "paperId": "ecd3b160b4bd13c5a1dc085fcc17c540435a284a",
        "title": "Anthropomorphism in AI"
      },
      {
        "paperId": "335613303ebc5eac98de757ed02a56377d99e03a",
        "title": "What Does BERT Learn about the Structure of Language?"
      },
      {
        "paperId": "f4327b978dec52f16b089c222c43543f8ecf4717",
        "title": "arXiv"
      },
      {
        "paperId": "47f936331872d75df74473883bb65068c14fa7da",
        "title": "Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass"
      },
      {
        "paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a",
        "title": "Assessing BERT's Syntactic Abilities"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "266fbab5bbba25c28d23efe34dff534e6be9522a",
        "title": "Engineering a Safer World: Systems Thinking Applied to Safety"
      },
      {
        "paperId": "ebb3d3e09000114576882568c8c2d01ae9aa1b28",
        "title": "Institutional design for complex technological systems"
      },
      {
        "paperId": null,
        "title": "Position: a roadmap to pluralistic alignment"
      },
      {
        "paperId": null,
        "title": "Mother says AI chatbot led her son to kill himself in lawsuit against its maker"
      },
      {
        "paperId": null,
        "title": "Tuning for LLM Alignment"
      },
      {
        "paperId": null,
        "title": "The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Per-spective"
      },
      {
        "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
        "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting"
      },
      {
        "paperId": "cb660ea0c8c14097513a2a2199ed3a18799683be",
        "title": "Entangled Preferences: The History and Risks of Reinforcement Learning and Human Feedback"
      },
      {
        "paperId": null,
        "title": "Jailbreak tricks Discord\u2019s new chat-bot into sharing napalm and meth instructions"
      },
      {
        "paperId": null,
        "title": "Which humans?"
      },
      {
        "paperId": null,
        "title": "The Empty Sig-nifier Problem: Towards Clearer Paradigms for Operationalising \u201cAlignment\u201d in Large Language Models"
      },
      {
        "paperId": null,
        "title": "\u2018Safety Washing\u2019 at the AI Safety Summit"
      },
      {
        "paperId": null,
        "title": "Model alignment protects against accidental harms, not intentional ones"
      },
      {
        "paperId": null,
        "title": "AI chatbot blamed for Belgian man\u2019s suicide"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "8c4edd50b85c66890fa4b427a75fdc0fe8080788",
        "title": "Anthropomorphism of computers: Is it mindful or mindless?"
      },
      {
        "paperId": null,
        "title": "Epistemic Injustice (Reprinted)"
      },
      {
        "paperId": "e033f0f7576a9b4b62d1837cbeaa4f6a1dc579b5",
        "title": "From Computer Power and Human Reason From Judgment to Calculation"
      },
      {
        "paperId": "b25e5bca74d74abb1687315fa3c637bb9911554d",
        "title": "Logic and Conversation"
      },
      {
        "paperId": null,
        "title": "An AI chatbot told a user how to kill himself-but the company doesn\u2019t want to \u201ccensor\u201d it"
      },
      {
        "paperId": null,
        "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through\u2026"
      }
    ],
    "cited_by": [
      {
        "paperId": "129dea44270e8cbc0a2de70696d99d58711e9e6e",
        "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins"
      },
      {
        "paperId": "e1b7fad25b3787dd1378ae63413fdc38d3e8fbc9",
        "title": "The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback"
      },
      {
        "paperId": "c94efc2f42fbf8ffe4df9b9afc77a5a75a9af9f0",
        "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing"
      },
      {
        "paperId": "381726ac467273fce991d9fb64ece064fcbfd662",
        "title": "Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking"
      },
      {
        "paperId": "402cb848f9d7dc72fca34f889c2080b0f1857603",
        "title": "AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect"
      },
      {
        "paperId": "af4686cfbb1f034ad7690e87d2d0d36fda999285",
        "title": "The DiCoSa Model: A Bottom-Up Digital Consciousness Proxy for AI Superalignment"
      }
    ],
    "score": 6.0
  },
  {
    "id": "96f0afd55fb1b37fcd683c7e3aa1704d18b60a73",
    "title": "Simulate and Eliminate: Revoke Backdoors for Generative Large Language Models",
    "authors": [
      "Haoran Li",
      "Yulin Chen",
      "Zihao Zheng",
      "Qi Hu",
      "Chunkit Chan",
      "Heshan Liu",
      "Yangqiu Song"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased accessibility and unrestricted model training on massive data. A malicious adversary may publish poisoned data online and conduct backdoor attacks on the victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave innocuously for normal queries and generate harmful responses when the backdoor trigger is activated. Despite significant efforts paid to LLMs' safety issues, LLMs are still struggling against backdoor attacks. As Anthropic recently revealed, existing safety training strategies, including supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the backdoors once the LLM is backdoored during the pre-training stage. In this paper, we present Simulate and Eliminate (SANDE) to erase the undesired backdoored mappings for generative LLMs. We initially propose Overwrite Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger is known. Then, to handle scenarios where trigger patterns are unknown, we integrate OSFT into our two-stage framework, SANDE. Unlike other works that assume access to cleanly trained models, our safety-enhanced LLMs are able to revoke backdoors without any reference. Consequently, our safety-enhanced LLMs no longer produce targeted responses when the backdoor triggers are activated. We conduct comprehensive experiments to show that our proposed SANDE is effective against backdoor attacks while bringing minimal harm to LLMs' powerful capability.",
    "url": "https://www.semanticscholar.org/paper/96f0afd55fb1b37fcd683c7e3aa1704d18b60a73",
    "pdf_url": "https://arxiv.org/pdf/2405.07667.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-05-13",
    "externalIds": {
      "ArXiv": "2405.07667",
      "DBLP": "conf/aaai/LiCZHCLS25",
      "DOI": "10.1609/aaai.v39i1.32018",
      "CorpusId": 269757159
    },
    "references": [
      {
        "paperId": "9363e8e1fe2be2a13b4d6f5fc61bbaed14ab9a23",
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training"
      },
      {
        "paperId": "37665dd5ae7245f087d663785c17eef068578676",
        "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "442ed16498e0edc80bcd23590d5b238293acdb20",
        "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation"
      },
      {
        "paperId": "aec4fc30ea7b4daebae736375efcfed7f4f83c5b",
        "title": "Backdoor Learning on Sequence to Sequence Models"
      },
      {
        "paperId": "3def0d3624211ffb012f15835f2071e28766f2d3",
        "title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models"
      },
      {
        "paperId": "2cf43a61d0937ad25f23eaef7c90253ab799b3c7",
        "title": "Poisoning Web-Scale Training Datasets is Practical"
      },
      {
        "paperId": "9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6",
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models"
      },
      {
        "paperId": "1e69f7b278ba4ae61fdbeb3330d041107287bd20",
        "title": "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models"
      },
      {
        "paperId": "91fb2254c5942048425e642c8a6c8d400006150e",
        "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models"
      },
      {
        "paperId": "e466852cfeb09941b18d9510de7c4e87e01405bf",
        "title": "A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "0429020abad6f22ea17681fa403aa591693bb607",
        "title": "Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures"
      },
      {
        "paperId": "b48cf1158ea6fb8347aac390ac3303efe697e305",
        "title": "Triggerless Backdoor Attack for NLP Tasks with Clean Labels"
      },
      {
        "paperId": "ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8",
        "title": "RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models"
      },
      {
        "paperId": "342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83",
        "title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "7571ed4cf1bbdcf891b576a0da12c910b1f0c72f",
        "title": "Concealed Data Poisoning Attacks on NLP Models"
      },
      {
        "paperId": "ac6d17a1e4345b6699965fca636590edb91f10a8",
        "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"
      },
      {
        "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "paperId": "3ab9145d5134e4e89bcceb1c8a95f9f98c98c5ff",
        "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification"
      },
      {
        "paperId": "4d4e5c0c691e42b7078208598d585caacc2e34c2",
        "title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"
      },
      {
        "paperId": "3a1f8829e641b46f661775f64a7f27b933a46103",
        "title": "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks"
      },
      {
        "paperId": "65906e6027246ae9e4ecd18d6e019a24505c842e",
        "title": "Aligning AI With Shared Human Values"
      },
      {
        "paperId": "0ea244065a6bfea8a16a257249398e1d727f02e9",
        "title": "Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification"
      },
      {
        "paperId": "1ec69f1a1a9d4ff5c5bc70db0e5087157b620570",
        "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"
      },
      {
        "paperId": "c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9",
        "title": "BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "0d360a1256ccdfca58cf98d12243df8407fd442d",
        "title": "Weight Poisoning Attacks on Pretrained Models"
      },
      {
        "paperId": "d477457f8ea5b7169ff370751653a6244c1c96c4",
        "title": "Latent Backdoor Attacks on Deep Neural Networks"
      },
      {
        "paperId": "f182ccbc90c1d20d358e3d197b340691f277428f",
        "title": "A Backdoor Attack Against LSTM-Based Text Classification Systems"
      },
      {
        "paperId": "633ccadcde3bfca87f91bfe5ef4aa297fb2da2f4",
        "title": "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering"
      },
      {
        "paperId": "790ec1befba47991e8fd50a24d13be6094253f93",
        "title": "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "2f2ade8c4944a96a44e6f70ef403b80b058d1725",
        "title": "Towards Making Systems Forget with Machine Unlearning"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": null,
        "title": "2023. BITE: Textual Back-door Attacks with Iterative Trigger Injection"
      },
      {
        "paperId": "84a33d6966cbb2cf8f5192087b286122e806a242",
        "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation"
      },
      {
        "paperId": "79b565e42cf4c5e135eea758973f5ec37fc991d1",
        "title": "Text Backdoor Detection Using an Interpretable RNN Abstract Model"
      },
      {
        "paperId": null,
        "title": "\u201dTeknium\u201d"
      },
      {
        "paperId": null,
        "title": "2023. Stanford Alpaca: An Instruction-following LLaMA model"
      },
      {
        "paperId": null,
        "title": "2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      },
      {
        "paperId": null,
        "title": "2022. Constrained optimization with dynamic bound-scaling for effective NLP backdoor defense"
      },
      {
        "paperId": null,
        "title": "2023. Universal jailbreak back-doors from poisoned human feedback"
      },
      {
        "paperId": null,
        "title": "2021b. Measuring Massive Mul-titask Language Understanding"
      },
      {
        "paperId": null,
        "title": "2023. More than you\u2019ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models"
      },
      {
        "paperId": null,
        "title": "2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
      }
    ],
    "cited_by": [
      {
        "paperId": "c26c3547c9f23774e692d602438f869d148bf28f",
        "title": "A survey of backdoor attacks and defences: From deep neural networks to large language models"
      },
      {
        "paperId": "4022081711542ac1b1d82ca455f2a4282d6359aa",
        "title": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems"
      },
      {
        "paperId": "6b88741e6bb624e4183ce43c2074456427035efd",
        "title": "Neutralizing Backdoors through Information Conflicts for Large Language Models"
      },
      {
        "paperId": "2cfb80fc28dbbcd9c118bf3bc466b897729d8b6f",
        "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization"
      },
      {
        "paperId": "aa83437007d3fd6b79f11180f0c5b640d0c48cb3",
        "title": "Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review"
      },
      {
        "paperId": "4b40d59d1d296ffb3d93847682b0a057ef08a08e",
        "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models"
      }
    ],
    "score": 6.0
  },
  {
    "id": "77f0687571a213c784f0901a821f22b2a03f3ddd",
    "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models",
    "authors": [
      "Xiang Ji",
      "Sanjeev Kulkarni",
      "Mengdi Wang",
      "Tengyang Xie"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results. On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment. To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment. We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations.",
    "url": "https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd",
    "pdf_url": "https://arxiv.org/pdf/2406.04274.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-06-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-04274",
      "ArXiv": "2406.04274",
      "DOI": "10.48550/arXiv.2406.04274",
      "CorpusId": 270285448
    },
    "references": [
      {
        "paperId": "59516436839bb0dd90eee34e913bb9306f383619",
        "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "b392122a48d8b3212ee17074ff65f6b9df5c36c7",
        "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models"
      },
      {
        "paperId": "334180f5ed0c9e82d737b6e8ccce13a7640b5320",
        "title": "Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF"
      },
      {
        "paperId": "60e3658f86393d65a6d523bfb88fd21e4447d941",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "ebd1c04c61f73f46def3305ca11d038c46665b65",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "386cebdba39d2d5f2862a9ab43a8d807f3863dae",
        "title": "Contrastive Preference Learning: Learning from Human Feedback without RL"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "e26888285436bc7998e5c95102a9beb60144be5e",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "99fad270248f2593293264956ec011b0f014d557",
        "title": "Kernelized Offline Contextual Dueling Bandits"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "ee0d43083dbbad55ff1da3c2f9213c364159afc7",
        "title": "Settling the Sample Complexity of Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "8a20cc7983e748615e912e59688defc9b7db4be7",
        "title": "The Efficacy of Pessimism in Asynchronous Q-Learning"
      },
      {
        "paperId": "dc94a18c69c71434590299fad9aeb2c932f45e15",
        "title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "46b7cbb5687bfdcfb2085a2ee70356d52f5d4994",
        "title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability"
      },
      {
        "paperId": "277e63a452cb1fb6c5ad88d110d5f18401e840c0",
        "title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "7e93fc4a9cbdcfb1fba2ce6ac8364fc077b14e5c",
        "title": "Efficient and Optimal Algorithms for Contextual Dueling Bandits under Realizability"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "e2ad21dae85950ab3631f65a0f142924c99fb9c4",
        "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning"
      },
      {
        "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "fa5853fdef7d2f6bb68203d187ddacbbddc63a8b",
        "title": "High-Dimensional Probability: An Introduction with Applications in Data Science"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "af252daf6ab49b3df9901f99af5a67ff3478edcd",
        "title": "Batch Value-function Approximation with Only Realizability"
      },
      {
        "paperId": "034b2e3d957df5405783f2c5695a8c2bd87d6334",
        "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs"
      },
      {
        "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
        "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "8664713793dbb1f6f935c617dd34be93d5b096ed",
        "title": "Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "b3b3d1d6d36ac203cd06c00bb37e66c000430275",
        "title": "A Theory of Regularized Markov Decision Processes"
      },
      {
        "paperId": "6dae703128d9caff2623eb8dfe2526dc6ad7aff5",
        "title": "A Theoretical Analysis of Deep Q-Learning"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "c335ff618991f0a4cdde09271284172a7e5f6b7f",
        "title": "Emergent Complexity via Multi-Agent Competition"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "195043e4c13a635555c28dfab7ac534d10d78c12",
        "title": "Introduction to Online Convex Optimization"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "8b20f103c1f20074fa35bd8fc41983964283acac",
        "title": "Fictitious Self-Play in Extensive-Form Games"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "b1199a19bd0f6dafb4b54dc84e83710d3a8af436",
        "title": "Market Structure and Equilibrium"
      },
      {
        "paperId": "5e838b1d5a0dc3454dad082b2ca6b9bf301bd25c",
        "title": "Online Markov Decision Processes"
      },
      {
        "paperId": "6b69c970d01e93bbeb73b4ed360a759fbfb4befc",
        "title": "Finite-Time Bounds for Fitted Value Iteration"
      },
      {
        "paperId": "6553b04761e1030b95755a83337627535a372c18",
        "title": "A Game-Theoretic Approach to Apprenticeship Learning"
      },
      {
        "paperId": "a353bdb2239ed5e0f92fd526d1245cbd05aed75c",
        "title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "616474134107bb0e91c3635f315e730ec3986952",
        "title": "Empirical Processes in M-Estimation"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": null,
        "title": "Ef\ufb01cient exploration for llms"
      },
      {
        "paperId": "e4435f282266da92d37066064c5239c6f96f0d64",
        "title": "Gibbs Sampling from Human Feedback: A Provable KL- constrained Framework for RLHF"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following llama model"
      },
      {
        "paperId": null,
        "title": "Iterative dpo alignment. Technical report"
      },
      {
        "paperId": null,
        "title": "Direct distillation of lm alignment"
      },
      {
        "paperId": "baaf17feabe5fa350f461bc5097fc63e8515ae31",
        "title": "Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation for Reinforcement Learning"
      },
      {
        "paperId": "d7b4fdcc1f4a30e1eaead03e94d36e7ccebde2a9",
        "title": "Sample Complexity of Of\ufb02ine Reinforcement Learning with Deep ReLU Networks"
      },
      {
        "paperId": null,
        "title": "Is pessimism provably ef\ufb01cient for of\ufb02ine rl?"
      },
      {
        "paperId": null,
        "title": "Training veri\ufb01ers to solve math word problems"
      },
      {
        "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "title": "An Adversarial Winograd Schema Challenge at Scale"
      },
      {
        "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
        "title": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "paperId": null,
        "title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent"
      },
      {
        "paperId": "08f186c9dba4e25c9d91e64d88d9bf9512da8230",
        "title": "WEIGHTED SUMS OF CERTAIN DEPENDENT RANDOM VARIABLES"
      },
      {
        "paperId": null,
        "title": "Iterative solution of games by \ufb01ctitious play"
      }
    ],
    "cited_by": [
      {
        "paperId": "a39ace65d9ddb56c7edf98e472a2291dd7609408",
        "title": "Pluralistic Off-policy Evaluation and Alignment"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "3ad48a63dbb84a4e7244fb75d4018c37ea49ebf6",
        "title": "From Demonstrations to Rewards: Alignment Without Explicit Human Preferences"
      },
      {
        "paperId": "3dd6e3dc2d7a8fa6aaa4671557e6a94e62aa1106",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "3ebe7c1a9f8a12cdfaf863069a253c2c31940ac6",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
      }
    ],
    "score": 6.0
  },
  {
    "id": "612ec1fbb54cfe61de62bc5922346d20f15f5023",
    "title": "Parameter Efficient Reinforcement Learning from Human Feedback",
    "authors": [
      "Hakim Sidahmed",
      "Samrat Phatale",
      "Alex Hutcheson",
      "Zhuonan Lin",
      "Zhan Chen",
      "Zac Yu",
      "Jarvis Jin",
      "Simral Chaudhary",
      "Roman Komarytsia",
      "Christiane Ahlheim",
      "Yonghao Zhu",
      "Bowen Li",
      "Saravanan Ganesh",
      "Bill Byrne",
      "Jessica Hoffmann",
      "Hassan Mansoor",
      "Wei Li",
      "Abhinav Rastogi",
      "Lucas Dixon"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "While Reinforcement Learning from Human Feedback (RLHF) effectively aligns pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with human preferences, its computational cost and complexity hamper its wider adoption. To alleviate some of the computational burden of fine-tuning, parameter efficient methods, like LoRA were introduced. In this work, we empirically evaluate the setup of Parameter Efficient Reinforcement Learning from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six diverse datasets spanning summarization, harmless/helpful response generation, UI automation, and visual question answering in terms of effectiveness of the trained models, and the training resources required. Our findings show, for the first time, that PE-RLHF achieves comparable performance to RLHF, while significantly reducing training time (up to 90% faster for reward models, and 30% faster for RL), and memory footprint (up to 50% reduction for reward models, and 27% for RL). We provide comprehensive ablations across LoRA ranks, and model sizes for both reward modeling and reinforcement learning. By mitigating the computational burden associated with RLHF, we push for a broader adoption of PE-RLHF as an alignment technique for LLMs and VLMs.",
    "url": "https://www.semanticscholar.org/paper/612ec1fbb54cfe61de62bc5922346d20f15f5023",
    "pdf_url": "https://arxiv.org/pdf/2403.10704.pdf",
    "venue": "",
    "publicationDate": "2024-03-15",
    "externalIds": {
      "ArXiv": "2403.10704",
      "CorpusId": 272654269
    },
    "references": [
      {
        "paperId": "1bda8efbbf4abae6c8c1da97d6137396807b1e09",
        "title": "ReFT: Representation Finetuning for Language Models"
      },
      {
        "paperId": "da053e2a4ba1b244940c8f2cad5dcdf0d730f85f",
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "f4675a4600f42ebcc737bcd6ad87a72d99a5b675",
        "title": "UINav: A Practical Approach to Train On-Device Automation Agents"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
        "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
      },
      {
        "paperId": "fc84f5b58e68871f3d6889dc2a93dffa7e107be2",
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "1ed66e048bb025e75aa5ea660545285212e5341f",
        "title": "Scaling Up Models and Data with t5x and seqio"
      },
      {
        "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
        "title": "LaMDA: Language Models for Dialog Applications"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "acc61b1a55ace8bba5595d8aee3dc9a14370a4d8",
        "title": "Learning UI Navigation through Demonstrations composed of Macro Actions"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "ebf59587f8f170ff4241c42263bbfb9da5bd2135",
        "title": "ELI5: Long Form Question Answering"
      },
      {
        "paperId": "b3b3d1d6d36ac203cd06c00bb37e66c000430275",
        "title": "A Theory of Regularized Markov Decision Processes"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
      },
      {
        "paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc",
        "title": "Unsupervised Pretraining for Sequence to Sequence Learning"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "b2f4ca80f4a8c1de6508fb7856f73dc89a9b0f5f",
        "title": "Avoiding Wireheading with Value Reinforcement Learning"
      },
      {
        "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
        "title": "Taming the Noise in Reinforcement Learning via Soft Updates"
      },
      {
        "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
        "title": "VQA: Visual Question Answering"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": null,
        "title": "Paxml: a Jax-based machine learning framework for training large scale models"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": null,
        "title": "JAX: composable transformations of Python+NumPy programs"
      },
      {
        "paperId": null,
        "title": "Bolt english sms/chat ldc2018t19"
      },
      {
        "paperId": null,
        "title": "2023a. Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "paperId": null,
        "title": "2024a. On the effects of data scale on computer control agents"
      },
      {
        "paperId": null,
        "title": "Anthropic"
      },
      {
        "paperId": null,
        "title": "2024a. Mixture of lora experts"
      },
      {
        "paperId": null,
        "title": "2023. Faithful persona-based conversational dataset generation with large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "15a1c2d8eb2c55e3ceb9ce9f72b3446ac1eb183a",
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning"
      },
      {
        "paperId": "f8aa4488fe95ba15123f9ea21992c880610f7f90",
        "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models"
      },
      {
        "paperId": "2bee1cc0031961dc0a470f476276282b07df9057",
        "title": "AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections"
      },
      {
        "paperId": "b218c99ff83db1148e9024c671f641125bac7bb1",
        "title": "Improving Neutral Point of View Text Generation through Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality Dataset"
      },
      {
        "paperId": "50c30f6bc96a1649fff17ea5f9a2efa000273fb8",
        "title": "ComLoRA: A Competitive Learning Approach for Enhancing LoRA"
      }
    ],
    "score": 5.0
  },
  {
    "id": "32608b3b06793a9b453fa742756b34c82afdb9d7",
    "title": "ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation",
    "authors": [
      "Chenglong Wang",
      "Hang Zhou",
      "Yimin Hu",
      "Yi Huo",
      "Bei Li",
      "Tongran Liu",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "year": 2023,
    "citationCount": 10,
    "abstract": "Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (e.g., BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (e.g., a vocabulary) and a long action sequence (e.g., a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-based RL, referred to as ESRL, can outperform all baselines in terms of both training efficiency and memory consumption. Notably, ESRL yields consistent performance gains over the strong REINFORCE, minimum risk training, and proximal policy optimization methods. The code is available at https://github.com/wangclnlp/DeepSpeed-Chat-Extension/examples/esrl.",
    "url": "https://www.semanticscholar.org/paper/32608b3b06793a9b453fa742756b34c82afdb9d7",
    "pdf_url": "https://arxiv.org/pdf/2308.02223.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2023-08-04",
    "externalIds": {
      "DBLP": "conf/aaai/WangZHHLLXZ24",
      "ArXiv": "2308.02223",
      "DOI": "10.48550/arXiv.2308.02223",
      "CorpusId": 260611030
    },
    "references": [
      {
        "paperId": "59a6d07fe7fb8fdf02cc8e914e0fd607f52acfb8",
        "title": "Introduction to Transformers: an NLP Perspective"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
        "title": "Instruction Tuning with GPT-4"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "bb459f27d173b3ffa293a0213ec29e31e44c404d",
        "title": "Reinforcement Learning with Large Action Spaces for Neural Machine Translation"
      },
      {
        "paperId": "c009a959dd236c162e51703e3bfd4d2b0b751c17",
        "title": "MAD for Robust Reinforcement Learning in Machine Translation"
      },
      {
        "paperId": "2e6028f8b156c8b344e1a68d15d88403a978c71d",
        "title": "Learning Multiscale Transformer Models for Sequence Generation"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3",
        "title": "RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"
      },
      {
        "paperId": "a6a7724763d8adba466519489b0b9d209e7f2d15",
        "title": "BARTScore: Evaluating Generated Text as Text Generation"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "a109b995dfeb444417f66545c67bce210bd11650",
        "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "926eb6dbb08791dad76e4a0468731b02a85a5bba",
        "title": "Selective Knowledge Distillation for Neural Machine Translation"
      },
      {
        "paperId": "5994e5f83ac1c045898012d5d2be39469f118025",
        "title": "Reward Optimization for Neural Machine Translation with Learned Metrics"
      },
      {
        "paperId": "59653e5cfa854a17c2ffcb86f2a454f27e12c716",
        "title": "Decoding and Diversity in Machine Translation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "0b15c6acfc9f7f92c52aa6a185a829f88975c743",
        "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation"
      },
      {
        "paperId": "90d8f96e2cd71a50b40992020cb65bc75f352ea1",
        "title": "Better Rewards Yield Better Summaries: Learning to Summarise Without References"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "6a53a38e1b160ab70f4a0f84ceff906ac84d9b12",
        "title": "Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation"
      },
      {
        "paperId": "56eb615c8fff0314f14e0b830afb91e350c16a89",
        "title": "Beyond BLEU:Training Neural Machine Translation with Semantic Similarity"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "fbe22105c3dd0fa3220e58b4a167129f1e548581",
        "title": "Generative Exploration and Exploitation"
      },
      {
        "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
      },
      {
        "paperId": "1ec4c24596ebd9a794b61b5ad4de594b99576057",
        "title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction"
      },
      {
        "paperId": "15a06d8601539b5eb6df5baf6bc4c3bdefb34855",
        "title": "Deep Reinforcement Learning for Sequence-to-Sequence Models"
      },
      {
        "paperId": "6e187ded899fb4c0d02b55a711f3e3522a49f50e",
        "title": "Toward Diverse Text Generation with Inverse Reinforcement Learning"
      },
      {
        "paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb",
        "title": "A Call for Clarity in Reporting BLEU Scores"
      },
      {
        "paperId": "05b1127ee39504516009b25384ca2bd7f2e1b9d9",
        "title": "Deep Communicating Agents for Abstractive Summarization"
      },
      {
        "paperId": "3fc5ed18c2294596af072df929c8ee12c71f96a2",
        "title": "Classical Structured Prediction Losses for Sequence to Sequence Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "abcbfc9742e8f4825cfc536091fd414e08d03998",
        "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "2a9cda48c1a57b738ea54abe96ff7e54bf95579b",
        "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning"
      },
      {
        "paperId": "a8e4580471908d17e279000d328f39654359bd6e",
        "title": "Beam Search Strategies for Neural Machine Translation"
      },
      {
        "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
        "title": "Deep Reinforcement Learning for Dialogue Generation"
      },
      {
        "paperId": "9f2a8e923965b23c11066a2ead79658208f1fae1",
        "title": "Minimum Risk Training for Neural Machine Translation"
      },
      {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks"
      },
      {
        "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
        "title": "Teaching Machines to Read and Comprehend"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "97ef266d155256b3cb22c0db391fe80975c4cc00",
        "title": "Tutorial on maximum likelihood estimation"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": null,
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
      },
      {
        "paperId": "5f0a1418494c5c8cc669fbf3a3a8395020122a2a",
        "title": "COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task"
      },
      {
        "paperId": null,
        "title": "Here, we adopted a joint source and target BPE factorization with vocabulary sizes of 10K and 32K for IWSLT\u201914 and WMT\u201918"
      },
      {
        "paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671",
        "title": "Text Generation by Learning from Demonstrations"
      },
      {
        "paperId": "4c5d0c4da5712bbc21791240d3bb80e2cc55eb63",
        "title": "Semantic Guidance of Dialogue Generation with Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "top-p sampling (Holtzman et al. 2020), and top-k sampling. The results shown in Table 8 indicate that top-k sampling can yield the best performance for training a sequence model"
      },
      {
        "paperId": "a64b5a923b8553897fac7253236ff3f601934af2",
        "title": "Adaptive \u03b5-greedy Exploration in Reinforcement Learning Based on Value Differences"
      },
      {
        "paperId": "818826f356444f3daa3447755bf63f171f39ec47",
        "title": "Active Learning Literature Survey"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Chatgpt: Optimizing language models for dialogue"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "2023. Stanford alpaca: An instruction-following llama model"
      }
    ],
    "cited_by": [
      {
        "paperId": "3efc3d06d5cafa7dc9a443dd9f0abd847a5f1d64",
        "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning"
      },
      {
        "paperId": "6c3c8a17bd3bf72d9bf8ce6d009d203363faa1e1",
        "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization"
      },
      {
        "paperId": "8efe6fe7c64cccd2e360122ec4d1eb1be24170ca",
        "title": "Adapting language generation to dialogue environments and users for task-oriented dialogue systems"
      },
      {
        "paperId": "0c5a84f5a4250b438fabd36bc048b1f14cb7b27b",
        "title": "Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning"
      },
      {
        "paperId": "0f26f61818a85c18e8e95d0b377db8c28453f791",
        "title": "LRHP: Learning Representations for Human Preferences via Preference Pairs"
      },
      {
        "paperId": "74f1b67fa18bc9c4033a9e8fd4e11ed38b178a37",
        "title": "Hybrid Alignment Training for Large Language Models"
      },
      {
        "paperId": "9216ec4ce85502e2d81beec89f54423af7810403",
        "title": "Prior Constraints-based Reward Model Training for Aligning Large Language Models"
      },
      {
        "paperId": "d5d7b26dbdf09737878644530b226a79c9f43bfb",
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "cf4ded5439915863293ce6ab80fc704b9ee27ee4",
        "title": "Learning Evaluation Models From Large Language Models for Sequence Generation"
      }
    ],
    "score": 5.0
  },
  {
    "id": "103436cbc7509b306c2fe82e62c9e63d29064c95",
    "title": "Hallucination Reduction and Optimization for Large Language Model-Based Autonomous Driving",
    "authors": [
      "Jue Wang"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Large language models (LLMs) are widely integrated into autonomous driving systems to enhance their operational intelligence and responsiveness and improve self-driving vehicles\u2019 overall performance. Despite these advances, LLMs still struggle between hallucinations\u2014when models either misinterpret the environment or generate imaginary parts for downstream use cases\u2014and taxing computational overhead that relegates their performance to strictly non-real-time operations. These are essential problems to solve to make autonomous driving as safe and efficient as possible. This work is thus focused on symmetrical trade-offs between the reduction of hallucination and optimization, leading to a framework for these two combined and at least specifically motivated by these limitations. This framework intends to generate a symmetry of mapping between real and virtual worlds. It helps in minimizing hallucinations and optimizing computational resource consumption reasonably. In autonomous driving tasks, we use multimodal LLMs that combine an image-encoding Visual Transformer (ViT) and a decoding GPT-2 with responses generated by the powerful new sequence generator from OpenAI known as GPT4. Our hallucination reduction and optimization framework leverages iterative refinement loops, RLHF\u2014reinforcement learning from human feedback (RLHF)\u2014along with symmetric performance metrics, e.g., BLEU, ROUGE, and CIDEr similarity scores between machine-generated answers specific to other human reference answers. This ensures that improvements in model accuracy are not overused to the detriment of increased computational overhead. Experimental results show a twofold improvement in decision-maker error rate and processing efficiency, resulting in an overall decrease of 30% for the model and a 25% improvement in processing efficiency across diverse driving scenarios. Not only does this symmetrical approach reduce hallucination, but it also better aligns the virtual and real-world representations.",
    "url": "https://www.semanticscholar.org/paper/103436cbc7509b306c2fe82e62c9e63d29064c95",
    "pdf_url": "https://doi.org/10.3390/sym16091196",
    "venue": "Symmetry",
    "publicationDate": "2024-09-11",
    "externalIds": {
      "DBLP": "journals/symmetry/Wang24a",
      "DOI": "10.3390/sym16091196",
      "CorpusId": 272615014
    },
    "references": [
      {
        "paperId": "859e10bb912a4314a689919639de12251f3172a9",
        "title": "A Systematic Review on the Integration of Artificial Intelligence into Energy Management Systems for Electric Vehicles: Recent Advances and Future Perspectives"
      },
      {
        "paperId": "b842ec712ff4ac7793016c5d4c03c0b0b37b998b",
        "title": "LLMatic: Neural Architecture Search Via Large Language Models And Quality Diversity Optimization"
      },
      {
        "paperId": "27051a3add912f5e2cb719fcb269ab1a7c20b14d",
        "title": "Differential Private Knowledge Transfer for Privacy-Preserving Cross-Domain Recommendation"
      },
      {
        "paperId": "794085fc2bbd7ddd069a210269819b8ec7af57db",
        "title": "Framework for Formal Verification of Machine Learning Based Complex System\u2010of\u2010System"
      },
      {
        "paperId": "d3b770e127284f996741086ec6cbffafe635ccd7",
        "title": "A Survey on Cooperative Longitudinal Motion Control of Multiple Connected and Automated Vehicles"
      },
      {
        "paperId": "9da0f31bcc0118aa29b4e80dbf6f495f31c4b3e0",
        "title": "Continuum and thermodynamic limits for a simple random-exchange model"
      },
      {
        "paperId": "693ff037f4b70a71f783e6027a62debb5be28b31",
        "title": "Photoproduction of \nJ/\u03c8\n with forward hadron tagging in hadronic collisions"
      },
      {
        "paperId": "aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8",
        "title": "Towards Mitigating LLM Hallucination via Self Reflection"
      }
    ],
    "cited_by": [
      {
        "paperId": "38f5eb8838c9b50208d35310a171a2f2a786ac03",
        "title": "An LLM based learning framework for adaptive feedback mechanisms in gamified XR"
      },
      {
        "paperId": "69e1ee4ead02ddf712d7f250ff3262768fb1f906",
        "title": "Artificial Intelligence in Product Development and Innovation"
      },
      {
        "paperId": "54ca5b504977c3693fbe6cfafb16e8dd22bd76bc",
        "title": "A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis"
      },
      {
        "paperId": "b151594cc71b55c7218dbb5e8faf73629d6bcc83",
        "title": "Length Instruction Fine-Tuning with Chain-of-Thought (LIFT-COT): Enhancing Length Control and Reasoning in Edge-Deployed Large Language Models"
      },
      {
        "paperId": "7be23ac4fc2ae38e7e61247b578afb2f2b76ddc4",
        "title": "Impact of generative artificial intelligence on workload, efficiency and labour productivity"
      }
    ],
    "score": 5.0
  },
  {
    "id": "ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
    "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
    "authors": [
      "Yitian Chen",
      "Jingfan Xia",
      "Siyu Shao",
      "Dongdong Ge",
      "Yinyu Ye"
    ],
    "year": 2025,
    "citationCount": 5,
    "abstract": "Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.",
    "url": "https://www.semanticscholar.org/paper/ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
    "pdf_url": "https://arxiv.org/pdf/2505.11792.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-17",
    "externalIds": {
      "DBLP": "journals/corr/abs-2505-11792",
      "ArXiv": "2505.11792",
      "DOI": "10.48550/arXiv.2505.11792",
      "CorpusId": 278740632
    },
    "references": [
      {
        "paperId": "f1f31064ccac840a33ae2a6b9e5a0873d97b3abe",
        "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition"
      },
      {
        "paperId": "8402e446158252992b6ddf1ff1b0658c39d7604e",
        "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
      },
      {
        "paperId": "cb81fe5812916beb915bdf426d28802df99e6df1",
        "title": "DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "b87f2fbb88c7dcf3a4a96dc690f8f4da831112dc",
        "title": "START: Self-taught Reasoner with Tools"
      },
      {
        "paperId": "ed0caf27d3dcee024b81323a04c4f2185da011c2",
        "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling"
      },
      {
        "paperId": "b62d0605137463ea591a0619840305cb98f6958f",
        "title": "LIMO: Less is More for Reasoning"
      },
      {
        "paperId": "7244b9b0868e400617334e392d071d406acc0ca5",
        "title": "Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "6b9cdba25a6930d82958d2404350f12c1550ddc4",
        "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking"
      },
      {
        "paperId": "d6e455d4906bc14345b0330a3f0ec30ebbbb3c99",
        "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings"
      },
      {
        "paperId": "e07a7e57745b69c0f08601fbdd19cfdcafbe1152",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "6485be47d22b7c2b07886af397c460e81f4d29a5",
        "title": "Phi-4 Technical Report"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "fee99505eddb66b815320a48314875265ed6230a",
        "title": "Autoformulation of Mathematical Optimization Models Using LLMs"
      },
      {
        "paperId": "537266d4bb895ffbb1fc77af12aafded491d1037",
        "title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "e1a642026fb46a8b8a868862bcf0728e8d215d7e",
        "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "80606d72e2499ebc2a029d0415c2215931f15aa3",
        "title": "AI-Assisted Generation of Difficult Math Questions"
      },
      {
        "paperId": "0e375b1f162589cd2e7368de0cc459debb6154a9",
        "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling"
      },
      {
        "paperId": "e8da1f7293657674c4f3648e227eb1b3e6c861f7",
        "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning"
      },
      {
        "paperId": "203c7916b8fd7634f25f257e3a424a5b49d7ab5f",
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas"
      },
      {
        "paperId": "1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3",
        "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems"
      },
      {
        "paperId": "de15adff32896fa645ab46392ef0a84e23aa66a2",
        "title": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling"
      },
      {
        "paperId": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "paperId": "6857a1439cfd5057d7e129eff16e5a58ab94bf14",
        "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "675629ff78cef09665a1135fece66195ed80a640",
        "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline"
      },
      {
        "paperId": "9b093787f9be3d480cd11f8ec6ca5b0e44050d6d",
        "title": "Solving olympiad geometry without human demonstrations"
      },
      {
        "paperId": "b272513916b45c8517d289d7abee4a53e6832187",
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
      },
      {
        "paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      },
      {
        "paperId": "87875a07976c26f82705de1fc70041169e5d652b",
        "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"
      },
      {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "c9a9735216915e9afa0fc97b02b57148a0491bdd",
        "title": "NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "2fe6060ced80c1c245a718e6188b6516207bf0a8",
        "title": "Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"
      },
      {
        "paperId": "cef315a84c52a676bad084cdd77a942286671fc5",
        "title": "Cardinal Optimizer (COPT) User Guide"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
        "title": "Generative Language Modeling for Automated Theorem Proving"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "248861a4e9c668d703ca841818c0a9ec30e4ecec",
        "title": "Optimization Methods in Finance"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "30ec43601d48236f8983d7cc55bed9cc68262b57",
        "title": "Automated Curriculum Learning for Neural Networks"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "10db5af7628e0cae088967b06f8a2b03c09c1079",
        "title": "Mathematical Programming Models for Portfolio Optimization Problem: A Review"
      },
      {
        "paperId": "cb6b7032c86dab47c451405a1c31aac6ca43375d",
        "title": "An overview of the optimization modelling applications"
      },
      {
        "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
        "title": "Curriculum learning"
      },
      {
        "paperId": "8b191d96db9248c8284d6146976ca3b5c4858f00",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      },
      {
        "paperId": "b5280ed2f9b3880fa505d93fbf140b9be8572d03",
        "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems"
      },
      {
        "paperId": "597f869a3ae3572e16de3643a7bd285ea2f1eabd",
        "title": "Defining and Characterizing Reward Gaming"
      },
      {
        "paperId": "c4c0d6ffd70081d143b32be53b06fec1259b3ad8",
        "title": "The Lean 4 Theorem Prover and Programming Language"
      },
      {
        "paperId": "82467a7a34927c9c5ff6fee3a24e0ac3d408850b",
        "title": "Practical Optimization: Algorithms and Engineering Applications"
      },
      {
        "paperId": null,
        "title": "Approximating kl divergence"
      },
      {
        "paperId": null,
        "title": "Mosek optimization suite"
      },
      {
        "paperId": null,
        "title": "User\u2019s manual for cplex"
      },
      {
        "paperId": null,
        "title": "Pattern recognition and machine learning , volume 4"
      },
      {
        "paperId": null,
        "title": "Linear and nonlinear programming , volume 2"
      },
      {
        "paperId": "c697855440d8be60cfa13c33e252a9f2a2b7987c",
        "title": "Linear Programming and Network Flows"
      },
      {
        "paperId": null,
        "title": "capricious-hydrogen-41c.notion.site/ Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "MAMO: A mathematical modeling benchmark with solvers"
      },
      {
        "paperId": null,
        "title": "Benchmarking llms for optimization modeling and enhancing reasoning via reverse socratic synthesis"
      },
      {
        "paperId": null,
        "title": "AI-MO Validation and AIME Test Set Dataset"
      },
      {
        "paperId": null,
        "title": "Learning to reason with llms"
      }
    ],
    "cited_by": [
      {
        "paperId": "92b39ce69cc1ee59aff8a92f30557728edc6ea32",
        "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts"
      },
      {
        "paperId": "6ff33b1c3c552cf98083b6eef07d1056f03648e3",
        "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models"
      },
      {
        "paperId": "182882881d594c494709dffe7ff316f573e3bee3",
        "title": "A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving"
      },
      {
        "paperId": "a836a59bbdfc45926f6c16888aceb40166a7eafd",
        "title": "Wild Refitting for Model-Free Excess Risk Evaluation of Opaque Machine Learning Models under Bregman Loss"
      },
      {
        "paperId": "e2be3088ae89ed7e8ba1f8f81b500d6504c8f572",
        "title": "Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization"
      }
    ],
    "score": 5.0
  },
  {
    "id": "c9e4efa58fd42a07da27ae70254981715cc257d5",
    "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
    "authors": [
      "Juntao Dai",
      "Taiye Chen",
      "Yaodong Yang",
      "Qian Zheng",
      "Gang Pan"
    ],
    "year": 2025,
    "citationCount": 5,
    "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose the Behavior-Supported Policy Optimization (BSPO) method to mitigate the reward over-optimization issue. Specifically, we define behavior policy as the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward model's extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy.",
    "url": "https://www.semanticscholar.org/paper/c9e4efa58fd42a07da27ae70254981715cc257d5",
    "pdf_url": "https://arxiv.org/pdf/2503.18130.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2025-03-23",
    "externalIds": {
      "DBLP": "conf/iclr/DaiC0Z025",
      "ArXiv": "2503.18130",
      "DOI": "10.48550/arXiv.2503.18130",
      "CorpusId": 277272509
    },
    "references": [
      {
        "paperId": "b4ab16793520c9ab8db307677a4b9ae77220bf65",
        "title": "Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback"
      },
      {
        "paperId": "8efc9c76034295cd3e87c7c805919c78c7ea5fd8",
        "title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "c8c9002af1d90e9dafa3d07e9edf0d883ec45472",
        "title": "Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "3d43594804af065c89d4f5be5d0a17957b633092",
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation"
      },
      {
        "paperId": "4c2d8df556589ff4fbb5ee68c1f45bff3786624f",
        "title": "Aligner: Efficient Alignment by Learning to Correct"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd",
        "title": "TinyLlama: An Open-Source Small Language Model"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "a3d1954a57110f199ad58c24a6e588ee73135170",
        "title": "AI Alignment: A Comprehensive Survey"
      },
      {
        "paperId": "e56769f6f43c1922e037afef75a7d6c3177516b1",
        "title": "Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark"
      },
      {
        "paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715",
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "5ee4efa8f7454770df0c43ce4d6fb65dd085fca5",
        "title": "Augmented Proximal Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "cb071e609fe6c82ccc906c878c535098b773e266",
        "title": "OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "3ffa359555ab15e7e2c379daec73fbcf5bafa458",
        "title": "Constrained Update Projection Approach to Safe Policy Optimization"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d373059e20daa0b4d91ef2a5fdd09d56692e7ca5",
        "title": "Supported Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "7b2180d7fa0d65e8756401cb077bf3dea3f9b575",
        "title": "Generalized Out-of-Distribution Detection: A Survey"
      },
      {
        "paperId": "1f5b5bb4887d87cced5d3458144268b0c32f8942",
        "title": "An Off-Policy Trust Region Policy Optimization Method With Monotonic Improvement Guarantee for Deep Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "6507909a8f77c88144c3a67b9336bd1c85e84cac",
        "title": "Do Deep Generative Models Know What They Don't Know?"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "87cbed883368d4a9efd42fdd91f47038f8d8fbe6",
        "title": "On Information and Sufficiency"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "878e2c80a9247b6106b479bf8d74c02427947176",
        "title": "Preventing Reward Hacking with Occupancy Measure Regularization"
      },
      {
        "paperId": "7075375b0106ddea3dabc6567ed55489a4f80a18",
        "title": "Mitigating Reward Hacking via Information-Theoretic Reward Modeling"
      },
      {
        "paperId": "597f869a3ae3572e16de3643a7bd285ea2f1eabd",
        "title": "Defining and Characterizing Reward Gaming"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "fe64a0141a2f707ffa0505c31eec6e688d8412a4",
        "title": "Combining Multiple Learners"
      },
      {
        "paperId": null,
        "title": "Constrained Markov Decision Processes , volume 7"
      },
      {
        "paperId": "9aba7fc67b2265cbc7fb1e91c7baa6c896ff928c",
        "title": "Constrained Differential Optimization"
      },
      {
        "paperId": null,
        "title": "Odin: Disentangled reward mitigates"
      },
      {
        "paperId": null,
        "title": "Re-ward gaming in"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following llama model"
      }
    ],
    "cited_by": [
      {
        "paperId": "41f3208b965effd7d7ac37eb5e81fbd0cf7da07f",
        "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap"
      },
      {
        "paperId": "6aebe394b781205ec9accd634e9395b7c21c1310",
        "title": "The Alignment Bottleneck"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "6a2db3d4aad5e21c71bc6906e3d3d8b68ffff602",
        "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment"
      }
    ],
    "score": 5.0
  },
  {
    "id": "d37e78b26ca0333c92a7445e20bb9e859242d5e1",
    "title": "Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms",
    "authors": [
      "Xuerui Su",
      "Yue Wang",
      "Jinhua Zhu",
      "Mingyang Yi",
      "Feng Xu",
      "Zhiming Ma",
      "Yuting Liu"
    ],
    "year": 2025,
    "citationCount": 5,
    "abstract": "With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms.",
    "url": "https://www.semanticscholar.org/paper/d37e78b26ca0333c92a7445e20bb9e859242d5e1",
    "pdf_url": "https://arxiv.org/pdf/2502.03095.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-05",
    "externalIds": {
      "ArXiv": "2502.03095",
      "DBLP": "journals/corr/abs-2502-03095",
      "DOI": "10.48550/arXiv.2502.03095",
      "CorpusId": 276116313
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "3a6dc3516c20a45062564e67d742a567674865bf",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization"
      },
      {
        "paperId": "b6953a5002622e5da1da599df87bc1c9cd25a27f",
        "title": "CLGRPO: Reasoning Ability Enhancement for Small VLMs"
      },
      {
        "paperId": "cf9a318487372304e821d06ea551e7d8b8a22751",
        "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management"
      },
      {
        "paperId": "609ca024d2eb12c606491f67cee9e71ae730e8ff",
        "title": "Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning"
      },
      {
        "paperId": "0ef09d611cd9108a905ef56feb99e1aadfbf4789",
        "title": "C2-DPO: Constrained Controlled Direct Preference Optimization"
      }
    ],
    "score": 5.0
  },
  {
    "id": "33c611e6b1c071dee7a928b5263e5baf3b23ead6",
    "title": "Privately Aligning Language Models with Reinforcement Learning",
    "authors": [
      "Fan Wu",
      "Huseyin A. Inan",
      "A. Backurs",
      "Varun Chandrasekaran",
      "Janardhan Kulkarni",
      "Robert Sim"
    ],
    "year": 2023,
    "citationCount": 9,
    "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
    "url": "https://www.semanticscholar.org/paper/33c611e6b1c071dee7a928b5263e5baf3b23ead6",
    "pdf_url": "https://arxiv.org/pdf/2310.16960.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-10-25",
    "externalIds": {
      "DBLP": "conf/iclr/WuIBCKS24",
      "ArXiv": "2310.16960",
      "DOI": "10.48550/arXiv.2310.16960",
      "CorpusId": 264490556
    },
    "references": [
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a82690177d548e68fb96bb949f3c3ea7f82ef139",
        "title": "SoK: Privacy-Preserving Data Synthesis"
      },
      {
        "paperId": "cb754310302086dfbbcd098263200e2a03f65874",
        "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison"
      },
      {
        "paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
        "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models"
      },
      {
        "paperId": "58996964dbcd15045b66201c2b850b5570ba74cb",
        "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe"
      },
      {
        "paperId": "af710ada8965f274e810053f716f966627a136d9",
        "title": "Differentially Private Language Models for Secure Data Sharing"
      },
      {
        "paperId": "92d0df018da15c1179568f1459da25c4ab0dfe1a",
        "title": "Privacy Leakage in Text Classification A Data Extraction Approach"
      },
      {
        "paperId": "2674f653b9cf3b84bbe2604238dff7c343be3d24",
        "title": "Offline Reinforcement Learning with Differential Privacy"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d",
        "title": "Quantifying Memorization Across Neural Language Models"
      },
      {
        "paperId": "e6ac78a5a88adc9df8fa05552304c6e4a8b359f4",
        "title": "Towards Instance-Optimal Offline Reinforcement Learning with Pessimism"
      },
      {
        "paperId": "56874f9aef515902c5a49d84d10f629f8dcd5f40",
        "title": "Differentially Private Fine-tuning of Language Models"
      },
      {
        "paperId": "6f674172fd1a14d4f6a566d7ba1f75303c8d3ff7",
        "title": "Large Language Models Can Be Strong Differentially Private Learners"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "08c43944c22a3e10cecd5936f04f3e07b0e636c1",
        "title": "Numerical Composition of Differential Privacy"
      },
      {
        "paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba",
        "title": "Extracting Training Data from Large Language Models"
      },
      {
        "paperId": "629b86425dc0e00a156961e098cb60b808411a72",
        "title": "Differentially Private Reinforcement Learning"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "1c8bbe188c2bcb788e5ddb19ee2e5ef69a63c12b",
        "title": "Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces"
      },
      {
        "paperId": "7d45ac8d29160103cd0bba76aa99b0f60f23a1cd",
        "title": "Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising"
      },
      {
        "paperId": "270e9b0681e895ae5adf937ea0cca9eb3718c721",
        "title": "Can Neural Machine Translation be Improved with User Feedback?"
      },
      {
        "paperId": "520ec00dc35475e0554dbb72f27bd2eeb6f4191d",
        "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "35f3300a7e6ecfc425dfeb05a50751ce03839d6e",
        "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "abcbfc9742e8f4825cfc536091fd414e08d03998",
        "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e9a986c8ff6c2f381d026fe014f6aaa865f34da7",
        "title": "Deep Learning with Differential Privacy"
      },
      {
        "paperId": "645c920bb71c0bc7fed6721f7e31ad08fbd236ab",
        "title": "Differentially Private Policy Evaluation"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "0023582fde36430c7e3ae81611a14e558c8f4bae",
        "title": "The Algorithmic Foundations of Differential Privacy"
      },
      {
        "paperId": "fae3819ea63c7ae9eef2398fa938e84fe0c10317",
        "title": "Stochastic gradient descent with differentially private updates"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "34bdd36330946cf9b377d274bdaaa7dc41888aa2",
        "title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis"
      },
      {
        "paperId": "e4ce10063cd25447dcde75c2d9ce327446ced952",
        "title": "Calibrating Noise to Sensitivity in Private Data Analysis"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "38256f79a019b8f1a5d50168717ed1066ddd0a1b",
        "title": "Randomized response: a survey technique for eliminating evasive answer bias."
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "e33633d3683a00bfc6e330f71a710c1fe9901f6f",
        "title": "Differentially Private Reward Estimation from Preference Based Feedback"
      },
      {
        "paperId": null,
        "title": "Larger models and larger epsilon values help in general"
      },
      {
        "paperId": null,
        "title": "Bard - chat based ai tool from google, powered by palm 2"
      }
    ],
    "cited_by": [
      {
        "paperId": "248dfbbecd50b0df3960d518ea534a264d43e9e5",
        "title": "PROPS: Progressively Private Self-alignment of Large Language Models"
      },
      {
        "paperId": "d1c84bae461380ac53d3429717a6a4f01dfc9c2c",
        "title": "Analyzing Security and Privacy Challenges in Generative AI Usage Guidelines for Higher Education"
      },
      {
        "paperId": "ca889669f46a090fee32263147d91da77ad368ff",
        "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF"
      },
      {
        "paperId": "340da3e43e49943f93f0a208b12c0ac71a842c01",
        "title": "Improved Algorithms for Differentially Private Language Model Alignment"
      },
      {
        "paperId": "65599fff2036e3e26407f2588d51ff249416d4aa",
        "title": "Empirical Privacy Variance"
      },
      {
        "paperId": "152625bc41c1c3d60371cad08ff38cc5b48f6a8c",
        "title": "Towards User-level Private Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "8bd8bc8073f41f82317c67c6c6db8694b0310c6e",
        "title": "Differentially Private Reinforcement Learning with Self-Play"
      },
      {
        "paperId": "f648d5bff46b180c633c812c71aa1cfafd7576dc",
        "title": "Privacy-Preserving Instructions for Aligning Large Language Models"
      },
      {
        "paperId": "56874f9aef515902c5a49d84d10f629f8dcd5f40",
        "title": "Differentially Private Fine-tuning of Language Models"
      }
    ],
    "score": 4.5
  },
  {
    "id": "d0ffb09a00b67365efb9e217c3fd45d804733810",
    "title": "Large Language Models are biased to overestimate profoundness",
    "authors": [
      "Eugenio Herrera-Berg",
      "Tom\u00e1s Vergara Browne",
      "Pablo Le'on-Villagr'a",
      "Marc-Llu\u00eds Vives",
      "Cristian Buc Calderon"
    ],
    "year": 2023,
    "citationCount": 8,
    "abstract": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.",
    "url": "https://www.semanticscholar.org/paper/d0ffb09a00b67365efb9e217c3fd45d804733810",
    "pdf_url": "https://arxiv.org/pdf/2310.14422.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2023-10-22",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-14422",
      "ArXiv": "2310.14422",
      "DOI": "10.18653/v1/2023.emnlp-main.599",
      "CorpusId": 264426519
    },
    "references": [
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "51095f2fceeda54dd048f2bb15feb0a579914eb7",
        "title": "Can AI language models replace human participants?"
      },
      {
        "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
      },
      {
        "paperId": "9004924824cc0b1c840bcc91ba79475882623790",
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks"
      },
      {
        "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d",
        "title": "Dissociating language and thought in large language models: a cognitive perspective"
      },
      {
        "paperId": "1a2cf9da390f88e7dce03ad0c3aa48c242edf574",
        "title": "A fine-grained comparison of pragmatic language understanding in humans and language models"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",
        "title": "Language models show human-like content effects on reasoning"
      },
      {
        "paperId": "fa3609e00f9f422a309c621a35394c4a38f88687",
        "title": "Using cognitive psychology to understand GPT-3"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
        "title": "Multimodal Few-Shot Learning with Frozen Language Models"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "a0e49f65b6847437f262c59d0d399255101d0b75",
        "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
      },
      {
        "paperId": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
        "title": "Open Sesame: Getting inside BERT\u2019s Linguistic Knowledge"
      },
      {
        "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
        "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"
      },
      {
        "paperId": "3aa52436575cf6768a0a1a476601825f6a62e58f",
        "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"
      },
      {
        "paperId": "c40052fa8d0b981a96992eabfb50bdce4cfe88a1",
        "title": "On the reception and detection of pseudo-profound bullshit"
      },
      {
        "paperId": "bba4ed8119539781ee527a7b8520f1edb6ef09c1",
        "title": "Anterior Cingulate Engagement in a Foraging Context Reflects Choice Difficulty, Not Foraging Value"
      },
      {
        "paperId": "05b199fc0e63bc3a7975f9112b79fa0d21bd3161",
        "title": "Conflict monitoring and cognitive control."
      },
      {
        "paperId": "61337bc3b39599c5d95d25ad70b7e17c8dfca514",
        "title": "Papers"
      },
      {
        "paperId": null,
        "title": "2022. To-wards Reasoning in Large Language Models: A Survey"
      },
      {
        "paperId": null,
        "title": "2023. Testing Causal Models of Word Meaning in GPT-3 and-4"
      },
      {
        "paperId": null,
        "title": "2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality"
      },
      {
        "paperId": null,
        "title": "2023. GPT-4 Technical Report"
      },
      {
        "paperId": null,
        "title": "2023. Faithful Chain-of-Thought Reasoning"
      },
      {
        "paperId": null,
        "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "8d15f27f4bc3eab1be00d37ac50f241eb72bf422",
        "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration"
      },
      {
        "paperId": "c5e8e0c86d1df2131be6712d4c702edfa3c9d420",
        "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions"
      },
      {
        "paperId": "26a0cf183cb3391ef5db3589c63ad58eb8600a5b",
        "title": "Understanding Open Source Large Language Models: An Exploratory Study"
      },
      {
        "paperId": "e16b2eb3e9135b18c9d4168c790ef375a95d086f",
        "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
      },
      {
        "paperId": "a14bd91f984036d7c381dd86b765f8eef4d838c3",
        "title": "CausalGraph2LLM: Evaluating LLMs for Causal Queries"
      },
      {
        "paperId": "c589dd8e933c033d05ea3102eabcc08a976faf9b",
        "title": "An Investigation into Protestware"
      },
      {
        "paperId": "8cffdafde24bdc852726ff29b4b2984f976d61c6",
        "title": "Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs"
      },
      {
        "paperId": "52ba65c216a0c0cf6f4bf22598531f4ab1698d2a",
        "title": "On the Contents and Utility of IoT Cybersecurity Guidelines"
      }
    ],
    "score": 4.0
  },
  {
    "id": "eb291a2e237774b162d9c51c21c4868795589e94",
    "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Yixin Cao",
      "Ting Liu",
      "Bing Qin"
    ],
    "year": 2023,
    "citationCount": 6,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/eb291a2e237774b162d9c51c21c4868795589e94",
    "pdf_url": null,
    "venue": "",
    "publicationDate": "",
    "externalIds": {
      "CorpusId": 258822953
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "57b6a2996e3a1673b93680487c45ebfa1350479d",
        "title": "Towards Rationality in Language and Multimodal Agents: A Survey"
      },
      {
        "paperId": "59eeb8a259ef2a9c981868470480f53b67854060",
        "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding"
      },
      {
        "paperId": "00cccb9065f0a59e845d5b4d360ce31cf25036be",
        "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning"
      },
      {
        "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate"
      },
      {
        "paperId": "98af6409f56980513dacaff5a897c1d8074e87b5",
        "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey"
      },
      {
        "paperId": "999d47de9cf872b65420dddd0f2cebc85433e5ed",
        "title": "L ARGE L ANGUAGE M ODEL C ASCADES WITH M IX - TURE OF T HOUGHT R EPRESENTATIONS FOR C OST - E FFICIENT R EASONING"
      }
    ],
    "score": 3.0
  },
  {
    "id": "cb660ea0c8c14097513a2a2199ed3a18799683be",
    "title": "Entangled Preferences: The History and Risks of Reinforcement Learning and Human Feedback",
    "authors": [
      "Nathan Lambert",
      "Thomas Krendl Gilbert",
      "Tom Zick"
    ],
    "year": 2023,
    "citationCount": 5,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/cb660ea0c8c14097513a2a2199ed3a18799683be",
    "pdf_url": "https://doi.org/10.48550/arXiv.2310.13595",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-13595",
      "DOI": "10.48550/arXiv.2310.13595",
      "CorpusId": 271770633
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "171cfd2aa8b0b47649a36a4eb5620806007c34c8",
        "title": "Reward Model Interpretability via Optimal and Pessimal Tokens"
      },
      {
        "paperId": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
        "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "9ddda210b622e0127a43dd5d7cd02928d449b799",
        "title": "Improving Context-Aware Preference Modeling for Language Models"
      },
      {
        "paperId": "f85ca15777e757efe4c2439429f826584df245ac",
        "title": "All We Need is Voter Feedback: a New Paradigm to Realize Politics without Politicians Using AI Models Aligned with Voter Inputs"
      },
      {
        "paperId": "49d25bf7d5f16da3f201f607d57411dd50bcf5f6",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      }
    ],
    "score": 2.5
  },
  {
    "id": "bd0ed34897bcf69d482caf16e7baab1b725d8b88",
    "title": "PerfRL: A Small Language Model Framework for Efficient Code Optimization",
    "authors": [
      "Shukai Duan",
      "Nikos Kanakaris",
      "Xiongye Xiao",
      "Heng Ping",
      "Chenyu Zhou",
      "Nesreen K. Ahmed",
      "Guixiang Ma",
      "M. Capot\u0103",
      "T. Willke",
      "Shahin Nazarian",
      "Paul Bogdan"
    ],
    "year": 2023,
    "citationCount": 5,
    "abstract": "Code optimization is a challenging task requiring a substantial level of expertise from developers. Nonetheless, this level of human capacity is not sufficient considering the rapid evolution of new hardware architectures and software environments. In light of this, recent research proposes adopting machine learning and artificial intelligence techniques to automate the code optimization process. In this paper, we introduce PerfRL, an innovative framework designed to tackle the problem of code optimization. Our framework leverages the capabilities of small language models (SLMs) and reinforcement learning (RL), facilitating a system where SLMs can assimilate feedback from their environment during the fine-tuning phase, notably through unit tests. When benchmarked against existing models, PerfRL demonstrates superior efficiency in terms of speed and computational resource usage, attributed to its reduced need for training steps and its compatibility with SLMs. Furthermore, it substantially diminishes the risk of logical and syntactical errors. To evaluate our framework, we conduct experiments on the PIE dataset using a lightweight large language model (i.e., CodeT5) and a new reinforcement learning algorithm, namely RRHF. For evaluation purposes, we use a list of evaluation metrics related to optimization quality and speedup. The evaluation results show that our approach achieves similar or better results compared to state-of-the-art models using shorter training times and smaller pre-trained models.",
    "url": "https://www.semanticscholar.org/paper/bd0ed34897bcf69d482caf16e7baab1b725d8b88",
    "pdf_url": "https://arxiv.org/pdf/2312.05657.pdf",
    "venue": "",
    "publicationDate": "2023-12-09",
    "externalIds": {
      "ArXiv": "2312.05657",
      "CorpusId": 266163427
    },
    "references": [
      {
        "paperId": "ddbe5a32350a090811ee2cdd9c980c055695d6f2",
        "title": "Software bug prediction using graph neural networks and graph-based text representations"
      },
      {
        "paperId": "27806f2f512fa68849db37f58010ef2ac01bc037",
        "title": "A structure-aware framework for learning device placements on computation graphs"
      },
      {
        "paperId": "886e0962479ec6dac563666399ca4c96a468fcaa",
        "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "1786a2f9140ed7211b21302977de64e948b92308",
        "title": "Learning Performance-Improving Code Edits"
      },
      {
        "paperId": "0a6bc37a07a37e3573d36e10cc11669eca0ff903",
        "title": "Execution-based Code Generation using Deep Reinforcement Learning"
      },
      {
        "paperId": "6d994b4f5a46cd14e8f09f1e9e49120546b15e31",
        "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"
      },
      {
        "paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878",
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"
      },
      {
        "paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "7547680408358916e66917d03436fca7540a7528",
        "title": "CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"
      },
      {
        "paperId": "870ff1dde0c103c3d90be51880f984628e77a8d6",
        "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "fbe25e4f069a19dc63daca27b7c98cff338663b9",
        "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"
      },
      {
        "paperId": "cf6c1aca36156137c5491737dd7f334f09df7357",
        "title": "The three pillars of machine programming"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "0bcc8d5dc881178be7f3bc4961b83114f2aadf01",
        "title": "A Comparative Assessment of State-Of-The-Art Methods for Multilingual Unsupervised Keyphrase Extraction"
      },
      {
        "paperId": "8567ca5729d9f7af210ae426e8e0ebcb1d8607d3",
        "title": "Learning to superoptimize programs"
      }
    ],
    "cited_by": [
      {
        "paperId": "34952993fd42bae877fd44630ae8dc8b0969c8ff",
        "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach"
      },
      {
        "paperId": "65cb28b5b7aa57c297143e729a29ab204d54b522",
        "title": "A Generalize Hardware Debugging Approach for Large Language Models Semi-Synthetic, Datasets"
      },
      {
        "paperId": "6c481860ea778f9119bfe1d9a75546f947b90965",
        "title": "Exploiting Application-to-Architecture Dependencies for Designing Scalable OS"
      },
      {
        "paperId": "6b1d45d87a21c1ffa9885062feee74e3eb324e0f",
        "title": "Translating and Optimising Computational Microscopy Algorithms with Large Language Models"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      }
    ],
    "score": 2.5
  },
  {
    "id": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
    "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
    "authors": [
      "Haoxiang Wang",
      "Wei Xiong",
      "Tengyang Xie",
      "Han Zhao",
      "Tong Zhang"
    ],
    "year": 2024,
    "citationCount": 234,
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model.",
    "url": "https://www.semanticscholar.org/paper/adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
    "pdf_url": "https://arxiv.org/pdf/2406.12845.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-06-18",
    "externalIds": {
      "ArXiv": "2406.12845",
      "DBLP": "conf/emnlp/00030X0024",
      "DOI": "10.48550/arXiv.2406.12845",
      "CorpusId": 270562658
    },
    "references": [
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "ecdd53eaab7455daea27609b07a418a21aa7ad35",
        "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "d06e65f74715e071678bf8ccdcf9d52004a10280",
        "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences"
      },
      {
        "paperId": "c78350e81298ca87bc1d59b466fa40081232caaa",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba",
        "title": "Yi: Open Foundation Models by 01.AI"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "19df5eb2c74606414ed93633b4c61947cc42dbbb",
        "title": "Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "e35426fd81c78b044258cf419be6b7e5093b71c0",
        "title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
        "title": "Verbosity Bias in Preference Labeling by Large Language Models"
      },
      {
        "paperId": "9ebf47129c15f61f4b77bbfe305c522480c20347",
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "0b1eda1b803486f52115816253e149fe53c41ed6",
        "title": "Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "004357dd9bbf3012c8fe0ccada4da401bf85dfff",
        "title": "Defining and Characterizing Reward Hacking"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "e72afa01a38d51945c1b893921d96616f834b892",
        "title": "Leveraging Sparse Linear Layers for Debuggable Deep Networks"
      },
      {
        "paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634",
        "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
        "title": "Interpretable Machine Learning"
      },
      {
        "paperId": "c95383f251a62c63217586059c67f63507c3e839",
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "751b04d346fc9a70781bbfea23953f424ff7deec",
        "title": "The proof and measurement of association between two things."
      },
      {
        "paperId": "ad4fd2c149f220a62441576af92a8a669fe81246",
        "title": "Scikit-learn: Machine Learning in Python"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0",
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"
      },
      {
        "paperId": "2d7a14fabe2631a205e65bcac94d8e84d883492f",
        "title": "Enable Language Models to Implicitly Learn Self-Improvement From Data"
      },
      {
        "paperId": null,
        "title": "Llm-blender: Ensembling large language models with pairwise comparison and generative fusion"
      },
      {
        "paperId": null,
        "title": "Amplify-instruct: Synthetically generated diverse multi-turn conversations for efficient llm training"
      },
      {
        "paperId": null,
        "title": "Introducing meta llama 3: The most capable openly available llm to date"
      },
      {
        "paperId": null,
        "title": "Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models"
      }
    ],
    "cited_by": [
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "dd34d6685970740065f17c4d7ec6830dc7b25481",
        "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models"
      },
      {
        "paperId": "7e3052358519a9c211eec305b7074c061d42c669",
        "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards"
      },
      {
        "paperId": "0f789d3446ac0ef9b408003b93d329f852fed58b",
        "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment"
      },
      {
        "paperId": "7db85f8f5d173067e9a0aee4f29355c375632ffa",
        "title": "T-POP: Test-Time Personalization with Online Preference Feedback"
      },
      {
        "paperId": "d162c0f1682ca88fdc07bdbfa429b245ee51f4d7",
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment"
      },
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "a9b98746f9da39b5935d0f99ef3983bbe1cfbf01",
        "title": "Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "f55bf9a4262ca00b72345ee95b0b1ad65b907e95",
        "title": "Multiplayer Nash Preference Optimization"
      },
      {
        "paperId": "628e29427038fd70828a76b528a587802f64fa4a",
        "title": "Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search"
      },
      {
        "paperId": "dac3786fcf23a447babf1f20aa5b257792dbb3a4",
        "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users"
      },
      {
        "paperId": "8273b825a4c8ec77f8ac9ab379daafedea08554b",
        "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach"
      },
      {
        "paperId": "66152393e107ad9b5080de3272c88bd312c551b4",
        "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning"
      },
      {
        "paperId": "a868d9e302737cfb032ea60d35de954db6d4eaae",
        "title": "The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features"
      },
      {
        "paperId": "765f7140f70bb9cb963cc200d9b969f50c45e03d",
        "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation"
      },
      {
        "paperId": "4d051e1bfb3143db0d32b94aa981c996c91f6632",
        "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization"
      },
      {
        "paperId": "8ddf11509dae8dee616a0dedf91c1537723db962",
        "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework"
      },
      {
        "paperId": "cc69c290ed4a35fb51dd7e3a85e7eb74e9c637a3",
        "title": "RewardRank: Optimizing True Learning-to-Rank Utility"
      },
      {
        "paperId": "74a84b034a6664c1117c20cb196ec236dc2757b5",
        "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models"
      },
      {
        "paperId": "473ab2cd396ad8ad5048704d7c68f64f32651f19",
        "title": "Interpretable Reward Model via Sparse Autoencoder"
      },
      {
        "paperId": "8111293cbde870ae432deb98bdad2b4dc6e70644",
        "title": "Expert Preference-based Evaluation of Automated Related Work Generation"
      },
      {
        "paperId": "94af13c6ad98ba03ed61e745e372c786ebbd20b4",
        "title": "Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression"
      },
      {
        "paperId": "0a409e79d98f47dff34a77624792d6e9e71edc78",
        "title": "Sample-efficient LLM Optimization with Reset Replay"
      },
      {
        "paperId": "3ee2ebb47185674ce29858e89d16c89160b8f59b",
        "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis"
      },
      {
        "paperId": "76bbe57082c8d9d5c965b23452161bccd2b77e26",
        "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"
      },
      {
        "paperId": "d33bd07c39b1e1c1907439a17ce6b35217ca2d41",
        "title": "Libra: Assessing and Improving Reward Model by Learning to Think"
      },
      {
        "paperId": "a2914fa5b2d65c9c439ea41aa7415d019a8c5b05",
        "title": "Multilingual Self-Taught Faithfulness Evaluators"
      },
      {
        "paperId": "f40935638c554e8df8590b1766996e04f3de8c9c",
        "title": "Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning"
      },
      {
        "paperId": "c9c21c4706d42afce45145b23b6bc50957ff4340",
        "title": "Checklists Are Better Than Reward Models For Aligning Language Models"
      },
      {
        "paperId": "097f801ad7f0a71ed7f4cfdf5d61351fff4f2b08",
        "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization"
      },
      {
        "paperId": "a80f9ceeee2009ba830b018115f167e318aec91f",
        "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes"
      },
      {
        "paperId": "265fdd781ac5f96f0e6ae37fbca0a370ee7c1b26",
        "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions"
      },
      {
        "paperId": "802e826fb9b6624aff25c8643b565d6773985995",
        "title": "Why is Your Language Model a Poor Implicit Reward Model?"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "1ba526f367c24f98df1bac5ebec03333fd6ad2f4",
        "title": "Interpretable Reward Modeling with Active Concept Bottlenecks"
      },
      {
        "paperId": "831b89a50ce08af10b1708cbcd841fb7fee48d7b",
        "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering"
      },
      {
        "paperId": "2f6481f3b5fad46a99711eebb3822d7b5c6d30bc",
        "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset"
      },
      {
        "paperId": "38a858d491358347fabdbdeea7d517116c3fa9b9",
        "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder"
      },
      {
        "paperId": "8bcd8b7618cacdf46d0b36e85b876840fe46060c",
        "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute"
      },
      {
        "paperId": "a6a5e2d4319e2fa9bf780a9b28f71677de4baaf6",
        "title": "Aligning Spoken Dialogue Models from User Interactions"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "cbd34d713156be2fb8991e1d55f4f478838a3659",
        "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning"
      },
      {
        "paperId": "17535e013fda643b5f11f9b9adb35d1e94e0a14b",
        "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning"
      },
      {
        "paperId": "26875419a986a787e7431dc4d804eeb6bb75e82f",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization"
      },
      {
        "paperId": "6c3c8a17bd3bf72d9bf8ce6d009d203363faa1e1",
        "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization"
      },
      {
        "paperId": "7973a601c208c80ba8f9cd6af9751f0609a17605",
        "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization"
      },
      {
        "paperId": "1da4c54174a7ba8bf5559930d7d2c9a3a80bfac3",
        "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization"
      },
      {
        "paperId": "b76d2d4da4b70489d85cd1fbf2ff3a7ebbadc578",
        "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms"
      },
      {
        "paperId": "c62d915d7ea1c4be7abfc1b970f02c96d2994590",
        "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO"
      },
      {
        "paperId": "321054f777bceb59da3d044a07ad5964e02bd8bb",
        "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding"
      },
      {
        "paperId": "21251d14f97926bf4c19b8cfaab8a3d101f7fa71",
        "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models"
      },
      {
        "paperId": "171cfd2aa8b0b47649a36a4eb5620806007c34c8",
        "title": "Reward Model Interpretability via Optimal and Pessimal Tokens"
      },
      {
        "paperId": "e67ea5dfb832c0d27242d3a768d5c47f2c78274c",
        "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models"
      },
      {
        "paperId": "c4d65d124c3ceeb7369425954516b1d7c6ffc1ab",
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization"
      },
      {
        "paperId": "17d25128b18cadc7cd88b7d7d18513087537612b",
        "title": "Preference Learning for AI Alignment: a Causal Perspective"
      },
      {
        "paperId": "d1e527fee15125d518587c49de391c3a1d142191",
        "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification"
      },
      {
        "paperId": "c30cabc37dd04d6bf322bb13e81b9c34cebfe12a",
        "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation"
      },
      {
        "paperId": "0e4ff1e4c42de97a26b5f673847b2635730e6b54",
        "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding"
      },
      {
        "paperId": "206685eb3ad285d89ec9047890f5facde15db8c2",
        "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration"
      },
      {
        "paperId": "82b39c1670282ebcdd45cfca57b702bd62169337",
        "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities"
      },
      {
        "paperId": "6069fa0fc5cccb8ebd0061c5e1816f5069cc255b",
        "title": "RewardBench 2: Advancing Reward Model Evaluation"
      },
      {
        "paperId": "1b108ded8ea5524d90bc43fd9bd9bdd82f8f8f74",
        "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning"
      },
      {
        "paperId": "fab2cb0b275dc555024af61b22e14219971393c5",
        "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective"
      },
      {
        "paperId": "27f8c2883ff1ceca2efa3193282245a1e71504a4",
        "title": "Accelerating RLHF Training with Reward Variance Increase"
      },
      {
        "paperId": "ff4b79b078e444bd8c9e2a68a68c18b5032bbeb5",
        "title": "LLM Agents for Bargaining with Utility-based Feedback"
      },
      {
        "paperId": "602c23958a567878d6a9d8e4831402611a038a6a",
        "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries"
      },
      {
        "paperId": "3113dfd036d012cfc58445cd99085d01262a048b",
        "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization"
      },
      {
        "paperId": "25c8a4df271129fb59d81fe06e35fdb220f7145f",
        "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics"
      },
      {
        "paperId": "abf0a71de6c00aea97211d8c2496687cf495a293",
        "title": "MOSLIM:Align with diverse preferences in prompts through reward classification"
      },
      {
        "paperId": "b93530f01ce85597509c8f4e4fa3a4c05264e444",
        "title": "MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming"
      },
      {
        "paperId": "9bd723ac16fab44267211f93442153626aac8ccd",
        "title": "Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models"
      },
      {
        "paperId": "ecb91bd42e484d35b4de37f84528bc88308d2f1c",
        "title": "Advancing LLM Safe Alignment with Safety Representation Ranking"
      },
      {
        "paperId": "2c4ca86d4023c85cd5066873f1f889aeaaab9b28",
        "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?"
      },
      {
        "paperId": "62529f40fcc21c10a90608bc2b6bbeb43b6a11db",
        "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization"
      },
      {
        "paperId": "06bc4086bec03463f800fe97ed875e5dcc0e1e2c",
        "title": "R3: Robust Rubric-Agnostic Reward Models"
      },
      {
        "paperId": "96e16d749d214d5a319ad9ca6317dfe87ed71298",
        "title": "DataSculpt: A Holistic Data Management Framework for Long-Context LLMs Training"
      },
      {
        "paperId": "9d890998d5c36fc9ee145939f0661b6e8f6a312e",
        "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity"
      },
      {
        "paperId": "c3f3592fc6c659c5935a9791e7b2e3f27d114dbb",
        "title": "A Systematic Analysis of Base Model Choice for Reward Modeling"
      },
      {
        "paperId": "3f97c3c28ed58fe20b5d0144a4a03fb0ed24c690",
        "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models"
      },
      {
        "paperId": "3968a794439b2655d327da2839e57470fd0b64db",
        "title": "WorldPM: Scaling Human Preference Modeling"
      },
      {
        "paperId": "d3f48630511341fd64ab5847aac6486a9974da8e",
        "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "0467ea5c1751f55693840be65a46573fe1c9d5eb",
        "title": "On the Robustness of Reward Models for Language Model Alignment"
      },
      {
        "paperId": "fe0f08d0a09885d9279dfc862498456672254a80",
        "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking"
      },
      {
        "paperId": "f25225055901731512dcbc905f75383dc678b4d0",
        "title": "RM-R1: Reward Modeling as Reasoning"
      },
      {
        "paperId": "f321cef5c3d8d602cc85a759ae3a9526c0582b1c",
        "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "277b7cd4b88787e0b42e14d0ec497b435d219968",
        "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model"
      },
      {
        "paperId": "7546c4d94a1e8b233970bf0238721809d84b48e2",
        "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling"
      },
      {
        "paperId": "a434fcdf7413bdc1fc5fade464a2bd453e12a1a3",
        "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment"
      },
      {
        "paperId": "ef3730364b904b64f8783196c6e784048cec21ba",
        "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data"
      },
      {
        "paperId": "c7249e8dc79198e4a316e7a9f2b6db24f54843c8",
        "title": "PRM-BAS: Enhancing Multimodal Reasoning through PRM-guided Beam Annealing Search"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "ffe6eddb44cb5aebd10345826a1b17f2663b2596",
        "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion"
      },
      {
        "paperId": "5b08060d9efee18b235b4d49bb1f58898f3fc24f",
        "title": "Inference-Time Scaling for Generalist Reward Modeling"
      },
      {
        "paperId": "df0b652b099b10fb7554ef8d50f31e023211ddca",
        "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings"
      },
      {
        "paperId": "01d6446eaf0066afe4ffc95cfdcf0d26eec7b0ee",
        "title": "Scaling Auditory Cognition via Test-Time Compute in Audio Language Models"
      },
      {
        "paperId": "86802db6a19fe5018dc5930e234c1a8b9cbb0454",
        "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"
      },
      {
        "paperId": "9e1a6beea72d6b9e6c7c4cf8b14ab14055dd7bf9",
        "title": "More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty"
      },
      {
        "paperId": "43150ca20011946a0f2e9117e7a263ea9406107e",
        "title": "Multi-head Reward Aggregation Guided by Entropy"
      },
      {
        "paperId": "4af78c7f147803885c3743403029c147058ef242",
        "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark"
      },
      {
        "paperId": "c9e4efa58fd42a07da27ae70254981715cc257d5",
        "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization"
      },
      {
        "paperId": "21c274d953b68e1d1793e23af7e70c9314a9dd1b",
        "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization"
      },
      {
        "paperId": "24519912c263a84321d5bf1126eddc24a2e1863e",
        "title": "Aligning Multimodal LLM with Human Preference: A Survey"
      },
      {
        "paperId": "b23bcfec5cd63bdb4047bd70bb439798b0562553",
        "title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing"
      },
      {
        "paperId": "20b6241fe78e35b2e1877a954a95309ff93eb46c",
        "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning"
      },
      {
        "paperId": "c9ab5fca0b7ec8c466daf0bbf1dd6c1f9b6b3a3d",
        "title": "Aligning to What? Limits to RLHF Based Alignment"
      },
      {
        "paperId": "9e90796e971a31f10be4a23ddff894f4fef8243b",
        "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models"
      },
      {
        "paperId": "038a85b0eee75ac361869df8eadf0b711f9370e7",
        "title": "RePO: ReLU-based Preference Optimization"
      },
      {
        "paperId": "8a834ec7f1d2c428e244c288eb5e9fe31912dd61",
        "title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality"
      },
      {
        "paperId": "6410623f6ded2c5b7e84f3e01371d05828e84fc0",
        "title": "Language Model Personalization via Reward Factorization"
      },
      {
        "paperId": "a68b85b0bef7feedb079d525a14975985766c32a",
        "title": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning"
      },
      {
        "paperId": "6dc4fe37b965ff8f987ea29c568089da47c1d048",
        "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion"
      },
      {
        "paperId": "864aad3b02011f9eca0fc21946f322d2481e826b",
        "title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models"
      },
      {
        "paperId": "9934c78aab745e1c8b51d966dd7af8b789d543bc",
        "title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge"
      },
      {
        "paperId": "8f2db783ac3481d9fddea610009b555c888d3dd1",
        "title": "PEO: Improving Bi-Factorial Preference Alignment with Post-Training Policy Extrapolation"
      },
      {
        "paperId": "baba33c46d2c8b3c1e68e31464b01cb1e0b48261",
        "title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning"
      },
      {
        "paperId": "b36e6ebba7c84aac09d272fa2dd5fdf42ed77651",
        "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom"
      },
      {
        "paperId": "a5e78e7d0c97121dc852908b62a1b932c0bf6bfd",
        "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding"
      },
      {
        "paperId": "224ae73df23d9226a2278e3e9983230a3f554b5b",
        "title": "Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness"
      },
      {
        "paperId": "145a2e2ff7e83dc2bbf3d95acf3dba046e2d69da",
        "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference"
      },
      {
        "paperId": "ded8cf8b28bab1930f5c57b31f7906835dfeec38",
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems"
      },
      {
        "paperId": "48d87b9c0e65b08322fc0ea41240c07ecc230f29",
        "title": "Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs"
      },
      {
        "paperId": "3404f2051763368df1613e3d9b7b787cb4a8b833",
        "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?"
      },
      {
        "paperId": "de3d7a05e1630518f5cf6ca2390f6995da3bc62a",
        "title": "Streaming Looking Ahead with Token-level Self-reward"
      },
      {
        "paperId": "7c69de6b1108ce03ba32ca9a10c36925187a8160",
        "title": "RSPO: Regularized Self-Play Alignment of Large Language Models"
      },
      {
        "paperId": "11313ae3f1745da638fa1852d020a18417a13bbf",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch"
      },
      {
        "paperId": "427c2052a57a726641cbfbbf1546457b7dc61dd2",
        "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization"
      },
      {
        "paperId": "04166e6ad5162ab60b8cca145b4e8ed5fb1c0334",
        "title": "HPS: Hard Preference Sampling for Human Preference Alignment"
      },
      {
        "paperId": "8e2640fa514a758a9512488daa2fc7315204d766",
        "title": "ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning"
      },
      {
        "paperId": "76d57ec8616a2f6d4bd5451a3344966b1c6dad5a",
        "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis"
      },
      {
        "paperId": "fd576581854bad8e91c83c4019de82ee5e44637c",
        "title": "Multi-Attribute Steering of Language Models via Targeted Intervention"
      },
      {
        "paperId": "3f63c18f37ee5609677d4efd52d18910bcad61e8",
        "title": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback"
      },
      {
        "paperId": "003c918fbc70516d3d5c087a681b94bc98106198",
        "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples"
      },
      {
        "paperId": "117b52d94d38048aca36fc7432ac17742103e2e2",
        "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization"
      },
      {
        "paperId": "81e8919c37279571c3b227a3e6b24a656b8409be",
        "title": "MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation"
      },
      {
        "paperId": "2c3a0648587bcad70a13338660f6812ff59c8161",
        "title": "Diverse Preference Optimization"
      },
      {
        "paperId": "f2d95572cde472fedeaaae78c27b51304fe19ef2",
        "title": "R.I.P.: Better Models by Survival of the Fittest Prompts"
      },
      {
        "paperId": "438c19fdd0f14f5082441db0c4183a50e7224b1b",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
      },
      {
        "paperId": "79cc7cb576b7eed9e7cf0d6a92fe36105fa7aad8",
        "title": "Data-adaptive Safety Rules for Training Reward Models"
      },
      {
        "paperId": "54e1a79ef688b8f6462b6265fc803d9c3e90a72a",
        "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model"
      },
      {
        "paperId": "a7401a78effe175c7d9dbb999c130b36d9ef92a7",
        "title": "Mixture of Experts (MoE): A Big Data Perspective"
      },
      {
        "paperId": "6af1c2430173406d7c1085e35d25047b1ce940b5",
        "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings"
      },
      {
        "paperId": "a652026eb912f940c56af1e5f9b094583ccc083c",
        "title": "AlphaPO: Reward Shape Matters for LLM Alignment"
      },
      {
        "paperId": "1c3aaffa10f83cac66a78f7cb796cb64edac6030",
        "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model"
      },
      {
        "paperId": "1fd282ff3a034ff6113f076b02769c46a7159476",
        "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning"
      },
      {
        "paperId": "f3eee8fa080bfbcaa6ff233664a49e81bbb459ea",
        "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis"
      },
      {
        "paperId": "548837aeeafed6cfa5a44f474a2740f1b99142bd",
        "title": "Boosting LLM via Learning from Data Iteratively and Selectively"
      },
      {
        "paperId": "6cd0ee7035abcfe80eb93d0995b5f09733b5436e",
        "title": "Multimodal Preference Data Synthetic Alignment with Reward Model"
      },
      {
        "paperId": "b98029d2a2dac2953533d8ef2724b1b4b85ef3d3",
        "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration"
      },
      {
        "paperId": "d704f7c327968963e6f3bf7b02cc2189564b2d02",
        "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment"
      },
      {
        "paperId": "c85e7a719c68b88ebae654b13196d248576e5910",
        "title": "JuStRank: Benchmarking LLM Judges for System Ranking"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "4de79e47a9879d2eda2ae20b4c2dee7b78c09303",
        "title": "Interpreting Language Reward Models via Contrastive Explanations"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "ad9cc1c463faa60d91bfaa48c3b16df0b3719ffa",
        "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd"
      },
      {
        "paperId": "78cca0da033edae72951940f9dd18eecc23ae36f",
        "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
      },
      {
        "paperId": "6bd400acb88b37d1f1b46b2db16c0281cf2fa469",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "paperId": "117cd296518ae3aae104cba284251875ef507d48",
        "title": "Towards Improved Preference Optimization Pipeline: from Data Generation to Budget-Controlled Regularization"
      },
      {
        "paperId": "a112125e251610b135a151b416a227bffadeb8f2",
        "title": "Self-Consistency Preference Optimization"
      },
      {
        "paperId": "347604483bf70b1863dc5dd4f21fe7abe6da5abc",
        "title": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "6e31b451f0d06c59b142537626375a5579533495",
        "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following"
      },
      {
        "paperId": "7796f56c7b9c152ac9573c8bb34f716aa78b4e6d",
        "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs"
      },
      {
        "paperId": "ae176cac16f67cfb133828c545460c652b71d3a3",
        "title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment"
      },
      {
        "paperId": "1b256fb2f9ac9857db996fa4f881f16e1345f8b1",
        "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models"
      },
      {
        "paperId": "f2fde6be4b074f509cf974d1aac24019247473ae",
        "title": "How to Evaluate Reward Models for RLHF"
      },
      {
        "paperId": "ea666891718261f7024c188150004c08d4b19f48",
        "title": "Multi-objective Reinforcement Learning: A Tool for Pluralistic Alignment"
      },
      {
        "paperId": "12e8ac0c499fd1ec320080ba56762b2c5cb70682",
        "title": "Thinking LLMs: General Instruction Following with Thought Generation"
      },
      {
        "paperId": "f28a570b7509eef12951787f58e777ecf0d46b91",
        "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization"
      },
      {
        "paperId": "249033f644140dd712b679a6b0ef295ede95fecd",
        "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF"
      },
      {
        "paperId": "ab0df4c771c19e3e7d4edc1057a9f91c6c8e2ae5",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
      },
      {
        "paperId": "77702dc45e9af19b287e9347cecc932e33cfd724",
        "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "fdbc542fa0fe613ec5f10f2f36edc3c1f09a73a4",
        "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time"
      },
      {
        "paperId": "854b2c0a2a3f72bd6a6a6224d474b4f0bb826b9d",
        "title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss"
      },
      {
        "paperId": "9bdce8ff12b2546c52363139b24806bf543c9f25",
        "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF"
      },
      {
        "paperId": "0de0a6b530d7416ec542b436054dd027e39cb1ac",
        "title": "Ordinal Preference Optimization: Aligning Human Preferences via NDCG"
      },
      {
        "paperId": "95124cb03a6e5de7a623db32b987531d7830629e",
        "title": "PAD: Personalized Alignment of LLMs at Decoding-time"
      },
      {
        "paperId": "5e553317596d37b6438441a38cfe3562eed4d374",
        "title": "RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization"
      },
      {
        "paperId": "82068d5f0126575e6b41dc4aa0653dd85939c91d",
        "title": "TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation"
      },
      {
        "paperId": "d05391825371fdd6f4619f47c0704a5bd4b562c8",
        "title": "Learning Code Preference via Synthetic Evolution"
      },
      {
        "paperId": "c4035aed5db5d3c1cffa8205de1e070960e5d860",
        "title": "MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences"
      },
      {
        "paperId": "62d4fcfb2f6717d4eb562f4e51f37004fc5d7109",
        "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation"
      },
      {
        "paperId": "706f5f56ad8c53926cdf7164cf08c89eff50b47f",
        "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision"
      },
      {
        "paperId": "bc7f7f8d729580d3a5ba6bc3334cf0082ea3e277",
        "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment"
      },
      {
        "paperId": "b5843e5acb4620346ba20e7eab89775519302491",
        "title": "HelpSteer2-Preference: Complementing Ratings with Preferences"
      },
      {
        "paperId": "6a686fb1a1f30d01eedf7cbf6f050041da556a66",
        "title": "Evaluating Robustness of Reward Models for Mathematical Reasoning"
      },
      {
        "paperId": "6a5191459d688538cb48717f30fe87fcc06dc59d",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "389f9b27488f68e4dd81bb98ffac01446021f241",
        "title": "Direct Judgement Preference Optimization"
      },
      {
        "paperId": "6d3a7b453048673a98b082a73bc864366fbd1cf4",
        "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
      },
      {
        "paperId": "6f3de19dbcc03a92f22c713a617e1269f5868cf2",
        "title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "1158310f204db29513bd8322f7010f0ca78d69be",
        "title": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do"
      },
      {
        "paperId": "637f0398b32a4383320d38e54624db78a64c58ad",
        "title": "Quantile Regression for Distributional Reward Models in RLHF"
      },
      {
        "paperId": "6f09f57b8eb8a853bdaa3fad6e39d11fbc6a83b0",
        "title": "DataSculpt: Crafting Data Landscapes for Long-Context LLMs through Multi-Objective Partitioning"
      },
      {
        "paperId": "9123ec44f0026e70f8398b904e97a4224866bb36",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      },
      {
        "paperId": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
        "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data"
      },
      {
        "paperId": "282058c7b1f07b878c154f3b4245992fd2bbb15e",
        "title": "Preference-Guided Reflective Sampling for Aligning Language Models"
      },
      {
        "paperId": "2f112209675710d3ec2d6f1d06bbdc74e9bc60af",
        "title": "Critique-out-Loud Reward Models"
      },
      {
        "paperId": "0276351cb3307dea8202bfb422849f1faa97133a",
        "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation"
      },
      {
        "paperId": "1035dfceeebd3dd9ba52a8162eeda670a432c56e",
        "title": "Self-Taught Evaluators"
      },
      {
        "paperId": "f66b6049946a10080d1f2a7ee6a40e5cca3ee6a0",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      },
      {
        "paperId": "928b3ef966cb0e1e9cff7e5e96d5df23c47c2d5a",
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift"
      },
      {
        "paperId": "e6443a69589808144205638c4ec13af6418cc7a4",
        "title": "A Practical Analysis of Human Alignment with *PO"
      },
      {
        "paperId": "3d7925cb321c059a039f8445da38d60b2c81d535",
        "title": "Building an Ethical and Trustworthy Biomedical AI Ecosystem for the Translational and Clinical Integration of Foundation Models"
      },
      {
        "paperId": "1281f7cbab728c2aa89f0a1cac925992f64eb2e3",
        "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism"
      },
      {
        "paperId": "6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
        "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation"
      },
      {
        "paperId": "3c20be1b4227d1da11bcc705fd92ba76b010ecaf",
        "title": "\u03b2-DPO: Direct Preference Optimization with Dynamic \u03b2"
      },
      {
        "paperId": "138f0b2bb1c8ffec1cb858a01070667a6da9a432",
        "title": "On the Low-Rank Parametrization of Reward Models for Controlled Language Generation"
      },
      {
        "paperId": "581d831fcea84501bec33161c716d5bf94a8a345",
        "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training"
      },
      {
        "paperId": "1680eedc706ef081c0b103457bb52c071ab924b8",
        "title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation"
      },
      {
        "paperId": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization"
      },
      {
        "paperId": "cab3d65b3f4d0a4169d0fdaaed15af6be1d6bb84",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      },
      {
        "paperId": "60dc6b4e423497a87933437fa094508d1c600704",
        "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads"
      },
      {
        "paperId": "a11d3cc04c3293df3e7ea3247c561f42fc36d1c5",
        "title": "MallowsPO: Fine-Tune Your LLM with Preference Dispersions"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "59516436839bb0dd90eee34e913bb9306f383619",
        "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
      },
      {
        "paperId": "4c10b7de3262cb68e5146f385e6a0a36acb0aba1",
        "title": "AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence"
      },
      {
        "paperId": "97f05d277bac590cdcfe512b8adf647bc958d966",
        "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization"
      },
      {
        "paperId": "58ab034688774cd17680cad5115df5bacb9776aa",
        "title": "Stronger Models are Not Always Stronger Teachers for Instruction Tuning"
      },
      {
        "paperId": "dbf4aa5c88c2b6d34ed5910929977667fba44c89",
        "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      },
      {
        "paperId": "abe28351f96c1bc471ac7126685a4a31450c6ee4",
        "title": "PAD: Personalized Alignment at Decoding-Time"
      },
      {
        "paperId": "7a3d8f0b9d1d76666162d67408a57aff74b74000",
        "title": "DataSculpt: Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning"
      },
      {
        "paperId": "375f35b1dde7d6bc4bc7a121e9f1f950793c588c",
        "title": "Boosting Healthcare LLMs Through Retrieved Context"
      },
      {
        "paperId": "10e37de70767088938d5c2200cc2cd9b73af31cb",
        "title": "T RAINING C ODE P REFERENCE M ODELS VIA"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      },
      {
        "paperId": "92616847a332a1bb0960c95cf2276b4dadfd4288",
        "title": "Strict Verification: Exploring Reinforcement Learning with Weak Verifiers"
      }
    ],
    "score": 234.0
  },
  {
    "id": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
    "title": "Understanding the performance gap between online and offline alignment algorithms",
    "authors": [
      "Yunhao Tang",
      "Daniel Guo",
      "Zeyu Zheng",
      "Daniele Calandriello",
      "Yuan Cao",
      "Eugene Tarassov",
      "R\u00e9mi Munos",
      "B. '. Pires",
      "Michal Valko",
      "Yong Cheng",
      "Will Dabney"
    ],
    "year": 2024,
    "citationCount": 84,
    "abstract": "Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.",
    "url": "https://www.semanticscholar.org/paper/7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
    "pdf_url": "https://arxiv.org/pdf/2405.08448.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-14",
    "externalIds": {
      "ArXiv": "2405.08448",
      "DBLP": "journals/corr/abs-2405-08448",
      "DOI": "10.48550/arXiv.2405.08448",
      "CorpusId": 269761634
    },
    "references": [
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728",
        "title": "Gemma: Open Models Based on Gemini Research and Technology"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "53f4fb0e9972989194368faf288ff8e3cba5bd60",
        "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "48362b169a235ca650918c489c8cea4c597da645",
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "title": "Mistral 7B"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "1ed66e048bb025e75aa5ea660545285212e5341f",
        "title": "Scaling Up Models and Data with t5x and seqio"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "17609f260d7deb836702029521c7f120c61ad6a9",
        "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": "0ae623749b30de53a39cf05813f5f3842e422c01",
        "title": "Problems of Monetary Management: The UK Experience"
      },
      {
        "paperId": null,
        "title": "can interpret the above results as the breakdown of theory, in violation of the assumption that \ud835\udf07 full support"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      }
    ],
    "cited_by": [
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "fdbb361c44299898af0bd7b2c9ef7b22e01f41c5",
        "title": "Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies"
      },
      {
        "paperId": "942752924e5743d6557cc9a53c2a05fb11e3b50b",
        "title": "PLaMo 2 Technical Report"
      },
      {
        "paperId": "1a41a24220a754e1188fdde478ff231154515ddb",
        "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models"
      },
      {
        "paperId": "2705cf150e726d669f8a70ea65962649a9cb5166",
        "title": "Reinforcement Learning for Flow-Matching Policies"
      },
      {
        "paperId": "0311a0660b02ec1face3936a81e8ab5186789475",
        "title": "ESSA: Evolutionary Strategies for Scalable Alignment"
      },
      {
        "paperId": "d9e93890ecf5d03914aa10bd66026653b042d842",
        "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs"
      },
      {
        "paperId": "ce4a35fe58b5f185221eb20a1ce96c1b7955b754",
        "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards"
      },
      {
        "paperId": "26875419a986a787e7431dc4d804eeb6bb75e82f",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization"
      },
      {
        "paperId": "3ff0d33d002548e90ce4e5afc7f937a4ac1a75f7",
        "title": "Rethinking DPO: The Role of Rejected Responses in Preference Misalignment"
      },
      {
        "paperId": "323fd4e3c88dad29e5787f06c041d3585b44fbe0",
        "title": "Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory"
      },
      {
        "paperId": "d5925e094acf86f5bd1b8169d21808afbc78c752",
        "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling"
      },
      {
        "paperId": "3fae1aae2f8e70ae0b706a237ab7da721ed33ad6",
        "title": "World Modelling Improves Language Model Agents"
      },
      {
        "paperId": "d6a29be03a0497602e89311ec38e5141335647c5",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "27f8c2883ff1ceca2efa3193282245a1e71504a4",
        "title": "Accelerating RLHF Training with Reward Variance Increase"
      },
      {
        "paperId": "7913632668760f722a983530c896a4aa1a37c3ad",
        "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models"
      },
      {
        "paperId": "6bac0364c7d9551ad9a3f609a4124854dcdf3edc",
        "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization"
      },
      {
        "paperId": "bc4b173d49b4dcac8ebd6329fa5b48db954e50f1",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning"
      },
      {
        "paperId": "c8c583a5081e0d87ea6b5bf5827cd47c4c85df9e",
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach"
      },
      {
        "paperId": "1bc8c111ff5f21a10061cc1654f77de2b406be72",
        "title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective"
      },
      {
        "paperId": "e54e9412425b7c10b4ad5d090be3ca135e94a70e",
        "title": "Perception in Reflection"
      },
      {
        "paperId": "2e9e9eb73d21e516ef3c56baae1feefc54ccc79d",
        "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System"
      },
      {
        "paperId": "ccd9eca10294fe822a25e1133d59deacab005860",
        "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs"
      },
      {
        "paperId": "43908f36b802ab4422a7e7c995aa40f85906a10f",
        "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations"
      },
      {
        "paperId": "55e76a02f78ae61604e5d54c81566efd3c0bcc11",
        "title": "RL-finetuning LLMs from on- and off-policy data with a single algorithm"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "baba33c46d2c8b3c1e68e31464b01cb1e0b48261",
        "title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning"
      },
      {
        "paperId": "6bb2b2314a0edd377ae06687566e2d75fa3654f8",
        "title": "Preference Learning Unlocks LLMs' Psycho-Counseling Skills"
      },
      {
        "paperId": "3dd6e3dc2d7a8fa6aaa4671557e6a94e62aa1106",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "d12c81e7a4a3ee67e067eba40b615f99d9b314b2",
        "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment"
      },
      {
        "paperId": "4cc330c9d5fca354b24bc347de84964175560e3d",
        "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering"
      },
      {
        "paperId": "117cd296518ae3aae104cba284251875ef507d48",
        "title": "Towards Improved Preference Optimization Pipeline: from Data Generation to Budget-Controlled Regularization"
      },
      {
        "paperId": "8430ea2921172c7c07e5286f19ffdf4a43d1d4b0",
        "title": "Sample-Efficient Alignment for LLMs"
      },
      {
        "paperId": "b38c17d4c1cf719f110e811190255428fe7a53aa",
        "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks"
      },
      {
        "paperId": "250d920ba5cf1e8dcb521eee47e181cf3eb3755a",
        "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models"
      },
      {
        "paperId": "01c12d59d9e3e7282514bdc75ba4f9d9b3e9fd57",
        "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment"
      },
      {
        "paperId": "e6fac5811e260466366f3a905076c33e252405ef",
        "title": "Optimal Design for Reward Modeling in RLHF"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "ebeb20304f0d0a255f5f087ddd99f66d7873d7ad",
        "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization"
      },
      {
        "paperId": "dc96b6ddb25bc2ebba5750638d3e36290773b4ea",
        "title": "Rational Metareasoning for Large Language Models"
      },
      {
        "paperId": "0d49552b54a1c2e064047d332018a898fcf6d9cb",
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions"
      },
      {
        "paperId": "038244c4951bd658ff5dc827129dfec8fe4a441f",
        "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future"
      },
      {
        "paperId": "029d309dd1f3a93ff74740ceca32da8c09c63e5c",
        "title": "BOND: Aligning LLMs with Best-of-N Distillation"
      },
      {
        "paperId": "3c3237df4b09dc5a21f7bc2b917f72ccbb6cf863",
        "title": "Learning Dynamics of LLM Finetuning"
      },
      {
        "paperId": "3da5f21144fef19dd88f7dcc11a5d9f2edbfe417",
        "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs"
      },
      {
        "paperId": "f3e4e646e5c17e93f9fa2ec87677f443a238eecd",
        "title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study"
      },
      {
        "paperId": "17b0637c32a0b67b69f7c8e90146af37ca6f3789",
        "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning"
      },
      {
        "paperId": "58fdc0be6dad25510ff7663b1cf8707c4496d08c",
        "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion"
      },
      {
        "paperId": "af11d43d3b4e6f2d01f34feb149037cdbd05d9ee",
        "title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models"
      },
      {
        "paperId": "5e7ba486300aca27f6103dbcbeaac940e7edf5c3",
        "title": "On-Policy Self-Alignment with Fine-grained Knowledge Feedback for Hallucination Mitigation"
      },
      {
        "paperId": "adfabe5e1b981b08e381a2654fe903266d7faab6",
        "title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment"
      },
      {
        "paperId": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization"
      },
      {
        "paperId": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "paperId": "65b5c132a90b66d6b21f0672abbe9eba5f9c63cb",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      },
      {
        "paperId": "5387445a58a958422a8cfd297e6a611aade0f0e8",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      },
      {
        "paperId": "79e41008d1837acc27029d7c5061898385c5ea3c",
        "title": "OPTune: Efficient Online Preference Tuning"
      },
      {
        "paperId": "86905d23c2d1c9386f3eb063132c14119ed79e5c",
        "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases"
      },
      {
        "paperId": "268e7df89189e13a4bf0dc4ebbf74cf057dd2082",
        "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference"
      },
      {
        "paperId": "0c43750030198dbe7fe164e1ce743ec64427bca1",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms"
      },
      {
        "paperId": "3a10570fb3f88d755ba4648598b3d30f8ddfb26c",
        "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "f0dece803297c0f368142a33aaa3afdc7f3b42a4",
        "title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "6b74102f7c287a25b14df208aff98a631a8fedf9",
        "title": "Preference Learning Algorithms Do Not Learn Preference Rankings"
      },
      {
        "paperId": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
        "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "4040099ed20718f418733cd201709cd950f11def",
        "title": "Online Self-Preferring Language Models"
      },
      {
        "paperId": "2bbc0f8ff058227d693f680d64f9736ae7a19f23",
        "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts"
      },
      {
        "paperId": "97dc1b2b7910c7a946fead4fe6b53240b398301a",
        "title": "Binary Classifier Optimization for Large Language Model Alignment"
      },
      {
        "paperId": "7647707b6b68c963314de0aab1514176b11732df",
        "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "2a8b65910598873b8125fffb73cee7c1510e4b35",
        "title": "When is Off-Policy Evaluation (Reward Modeling) Useful in Contextual Bandits? A Data-Centric Perspective"
      },
      {
        "paperId": "3ebe7c1a9f8a12cdfaf863069a253c2c31940ac6",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
      },
      {
        "paperId": "4d7294c62913a1a2ce651212f8fbb20baf938227",
        "title": "STA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "d330bb7c578c7e4de7566d0c958a41e0935c9721",
        "title": "Improving LLM Generation with Inverse and Forward Alignment: Reward Modeling, Prompting, Fine-Tuning, and Inference-Time Optimization"
      },
      {
        "paperId": "566c0e6982a34385a8d5fbc55be8749ee532d117",
        "title": "I TERATIVE DPO WITH A N I MPROVEMENT M ODEL FOR F INE - TUNING D IFFUSION M ODELS"
      },
      {
        "paperId": "f1a7243dafe9c5e2a3c16e01d713ed93fabbfc7e",
        "title": "EMI -P OLICY P REFERENCE O PTIMIZATION FOR"
      },
      {
        "paperId": "819d25a7f90654ebf769d1b1c6499e5248cf3d8a",
        "title": "O UTLIER -A WARE P REFERENCE O PTIMIZATION FOR L ARGE L ANGUAGE M ODELS"
      },
      {
        "paperId": "c78ba55f8bad0c9b8b02e472153a7fa801e77ecf",
        "title": "M ITIGATING REWARD OVER - OPTIMIZAATION IN D I - RECT A LIGNMENT A LGORITHMS WITH A DAPTIVE I M - PORTANCE S AMPLING"
      }
    ],
    "score": 84.0
  },
  {
    "id": "d084517f14ee247883de0f4dd58bb923e418157d",
    "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
    "authors": [
      "Di Zhang",
      "Jianbo Wu",
      "Jingdi Lei",
      "Tong Che",
      "Jiatong Li",
      "Tong Xie",
      "Xiaoshui Huang",
      "Shufei Zhang",
      "Marco Pavone",
      "Yuqiang Li",
      "Wanli Ouyang",
      "Dongzhan Zhou"
    ],
    "year": 2024,
    "citationCount": 73,
    "abstract": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
    "url": "https://www.semanticscholar.org/paper/d084517f14ee247883de0f4dd58bb923e418157d",
    "pdf_url": "https://arxiv.org/pdf/2410.02884.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-03",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-02884",
      "ArXiv": "2410.02884",
      "DOI": "10.48550/arXiv.2410.02884",
      "CorpusId": 273162606
    },
    "references": [
      {
        "paperId": "9fb201282f53a4ce89f28cbe5026af78912aa8c1",
        "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement"
      },
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "4814f1744ebfbbf0e987ee4242a930dd2d3d09a5",
        "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"
      },
      {
        "paperId": "f66b6049946a10080d1f2a7ee6a40e5cca3ee6a0",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      },
      {
        "paperId": "30444cf822cb2b1c8ca6638997d08c3fb1a9c7ff",
        "title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data"
      },
      {
        "paperId": "d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning"
      },
      {
        "paperId": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
        "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"
      },
      {
        "paperId": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
        "title": "Step-level Value Preference Optimization for Mathematical Reasoning"
      },
      {
        "paperId": "cd27f45bc760447fb4de3209e2381ea3493bbd57",
        "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"
      },
      {
        "paperId": "f32bcc2155997110a7905da050df4c8404867b24",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "paperId": "36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
        "title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time"
      },
      {
        "paperId": "dcebd9de6fb8ffd5e5fff7734e277abd6f3858ce",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      },
      {
        "paperId": "38333f6e8f0388968edc4b2ea7a683ce69677e69",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      },
      {
        "paperId": "6c17de3e719c0f4d1df6a16f770c2a9a5f18206f",
        "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing"
      },
      {
        "paperId": "9ccb5de1e22238b93a6af01c1dc341dc9bc3f28d",
        "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision"
      },
      {
        "paperId": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "paperId": "3352782f94354d3f3a170f497dd1888e9cd39d8a",
        "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning"
      },
      {
        "paperId": "3f75c088b44be491fa1060a0eb3064f61dfe70c3",
        "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning"
      },
      {
        "paperId": "135da052891fdcf3842901b6b7585ca8c258f1c2",
        "title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs"
      },
      {
        "paperId": "bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems"
      },
      {
        "paperId": "a3d749bc119f5c8425779e4e72e650720db2fe4b",
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"
      },
      {
        "paperId": "46a5ec31987a12d60ade20c6471db64c46f90106",
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements"
      },
      {
        "paperId": "af1d4a4141eaf52384ca85f1d468443f8f3d5c06",
        "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "42445823fb0156afddc8c72eaa5ee81dded5b965",
        "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges"
      },
      {
        "paperId": "675629ff78cef09665a1135fece66195ed80a640",
        "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline"
      },
      {
        "paperId": "4ba57555bef02f988f2ed3bab2f102733dc55221",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "0639e2e209213ecb54eb4d6555e271d070344842",
        "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation"
      },
      {
        "paperId": "44b506d9619b5f957dc2b5588801138f343c0308",
        "title": "Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"
      },
      {
        "paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "paperId": "1671d70a135b1e28b3a9cbc830feaa9b0c57df32",
        "title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding"
      },
      {
        "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      },
      {
        "paperId": "537335d9aad0ddbaef93e7f88b0db096671ef6ec",
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "34e1a8a75bf6f35084ac6d714a136f39d02c649e",
        "title": "Self-Verification Improves Few-Shot Clinical Information Extraction"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      },
      {
        "paperId": "c715914c388fa64dd8686cd8755e5adfebbf2388",
        "title": "REFINER: Reasoning Feedback on Intermediate Representations"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
        "title": "Large Language Models are Better Reasoners with Self-Verification"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
        "title": "PAL: Program-aided Language Models"
      },
      {
        "paperId": "b92628d13e8d090d042232fe6ae0b8998634b893",
        "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "3d1b34384499f77bf317b01594c8c60af212582b",
        "title": "SymPy: Symbolic computing in Python"
      },
      {
        "paperId": "e635d81a617d1239232a9c9a11a196c53dab8240",
        "title": "Bandit Based Monte-Carlo Planning"
      },
      {
        "paperId": "7a92314ff9ef62aab0f30510225736aa30aa7b84",
        "title": "Combining Pattern Classifiers: Methods and Algorithms"
      },
      {
        "paperId": "5c3391bde2bb1b3d737913ee8caa01492a782732",
        "title": "WHO Technical Report"
      },
      {
        "paperId": "26adb2d315c4c2f360c01827adc0c9cec567e9ac",
        "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning"
      },
      {
        "paperId": "31181e73befea410e25de462eccd0e74ba8fea0b",
        "title": "Introduction to Algorithms, 3rd Edition"
      },
      {
        "paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce",
        "title": "google,\u6211,\u8428\u5a1c"
      },
      {
        "paperId": "6145f0b0df53ec2581d733f003d251e0cabef04a",
        "title": "A Theorem on Boolean Matrices"
      },
      {
        "paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627",
        "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
      },
      {
        "paperId": null,
        "title": "American invitational mathematics examination"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "Meta. 2024b"
      },
      {
        "paperId": null,
        "title": "Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning"
      },
      {
        "paperId": null,
        "title": "2024b. On the diagram of thought"
      }
    ],
    "cited_by": [
      {
        "paperId": "ea090c112be388712fc65d5c06420891f03955ad",
        "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "6794971b191f82e7b78f4a1cb8e09dca2d2b8e82",
        "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search"
      },
      {
        "paperId": "2429de2c3f2b7ddcdf1f8b2d34c6eb8b75cc47f9",
        "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models"
      },
      {
        "paperId": "d6cbebb1b7884de42163b1ab4e72856dd416d2ca",
        "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS"
      },
      {
        "paperId": "1bc6c75f8f9de1ac6cc3af1d4b2464e0b6991992",
        "title": "Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization"
      },
      {
        "paperId": "f9a655ea7a41b9a52e3964431b152c0ef76eacba",
        "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS"
      },
      {
        "paperId": "1f2c18dd03b6053b0f22aab3020ced7203bb7c34",
        "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning"
      },
      {
        "paperId": "b17c7aa2aac0cb03d60f85cbb582ba52384a15cc",
        "title": "Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search"
      },
      {
        "paperId": "38b89b297084e479f61c887dd36ae490e5c20118",
        "title": "Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection"
      },
      {
        "paperId": "f561ef070b7fa419093e66173fc050e596724aae",
        "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism"
      },
      {
        "paperId": "004d1537c0e7a084e9a1368cd446873c3f978f4f",
        "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition"
      },
      {
        "paperId": "c64a10ec08a1ae26e043ac5b5f0d91d7d617369b",
        "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models"
      },
      {
        "paperId": "fd79dcbc9e1d269f9b416704bcf32ed6dd3f0724",
        "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM"
      },
      {
        "paperId": "d25032b12378b12edc047f0b94770aca251571ae",
        "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start"
      },
      {
        "paperId": "ef820388e4a8bda8f5d1d8f5f49a59b2a226d3f3",
        "title": "Reward Model Generalization for Compute-Aware Test-Time Reasoning"
      },
      {
        "paperId": "427a80a2c3cbfcf1f3d8e312b3337cbb7bb1988f",
        "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models"
      },
      {
        "paperId": "e42084ae531e15cce92f9673ddf014490ab096a3",
        "title": "Integrating Visual Interpretation and Linguistic Reasoning for Math Problem Solving"
      },
      {
        "paperId": "57e959b74f36a30cd62d0abd4204f08907b42e87",
        "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning"
      },
      {
        "paperId": "4721b71c114e9b2f12dd5f4bfb5fae4a3ca5e5ef",
        "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering"
      },
      {
        "paperId": "68cd9e02bd17961d4b864696edfae82f8256bb4c",
        "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge"
      },
      {
        "paperId": "f7c015abc3aafaa6cdaad42b9573031d72578677",
        "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective"
      },
      {
        "paperId": "f7e492dfd3de3a9b9f92bdb9921d75ab2fa3929b",
        "title": "CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation"
      },
      {
        "paperId": "c2feda1e804700d0980d71cfc71ce66d369b6b6c",
        "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law"
      },
      {
        "paperId": "754a76f6394d4db2790361d7a424421bcaa55f7e",
        "title": "AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning"
      },
      {
        "paperId": "226dc052a0f9b317727da2e49783ece37be206f1",
        "title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning"
      },
      {
        "paperId": "777849125d2eefa434dbf0cb7acb2bac95d87636",
        "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models"
      },
      {
        "paperId": "a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6",
        "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)"
      },
      {
        "paperId": "039d368452bb2f5cfd947352006f57864452eb3e",
        "title": "Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs"
      },
      {
        "paperId": "a091c920e8980444cbc501ff24c6d2f7af857b7a",
        "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks"
      },
      {
        "paperId": "ccd9eca10294fe822a25e1133d59deacab005860",
        "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs"
      },
      {
        "paperId": "86010eada916be09eb0fd134814d4184cd36e91e",
        "title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles"
      },
      {
        "paperId": "6a28aaaafa29e80a3a76b1dd51e89ad933aee1ca",
        "title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models"
      },
      {
        "paperId": "86ae86d121497aae9bc51c5b83a4425019acfabf",
        "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems"
      },
      {
        "paperId": "9c4d8cdbe337b309bb1649d0f5aad508d4f05f40",
        "title": "Thinking Machines: A Survey of LLM based Reasoning Strategies"
      },
      {
        "paperId": "9ebd1f44afd407011e167ed6090ccb6b3c3e637b",
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "1a5d0e368660bd638f804e6e837c97765b72f561",
        "title": "Better Process Supervision with Bi-directional Rewarding Signals"
      },
      {
        "paperId": "6535d4bc001b694d2369feaafb1916a3f84b0dfa",
        "title": "Accelerating Multi-Task Temporal Difference Learning under Low-Rank Representation"
      },
      {
        "paperId": "d1ef2da4abf608afcc1e8a36475ae2b7857face1",
        "title": "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning"
      },
      {
        "paperId": "b920a3699d0b20f4692a823c816bdc2085167a85",
        "title": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing"
      },
      {
        "paperId": "2d05f75a041bf415da592018ca00a51ee68af8ff",
        "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "e1d3877653128923851f21f74ea0ec04ea968e90",
        "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference"
      },
      {
        "paperId": "cf7d8dbb08f5a3c4e9f56146a25333dcd3642553",
        "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers"
      },
      {
        "paperId": "7ecb773db1f7ce8053710986189d4bdfa38a46ad",
        "title": "Theoretical Physics Benchmark (TPBench) - a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics"
      },
      {
        "paperId": "a24f21776b0650cc4cbb74d9fe34df3ab6379702",
        "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos"
      },
      {
        "paperId": "f65335ed3d20c041e226e959ba73c28336392ab7",
        "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model"
      },
      {
        "paperId": "ffcf77027f5144c17de1cbb72b5c1d783a3848f7",
        "title": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model"
      },
      {
        "paperId": "78160392b2127444fe5488ede2065f4c7102a3ba",
        "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling"
      },
      {
        "paperId": "9f88f56b592e1c1b8d012affec24ed6ca52d9e2b",
        "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools"
      },
      {
        "paperId": "cf81d21d2196599f6a1e1ca4c3fdfac6e2d01e19",
        "title": "Position: Multimodal Large Language Models Can Significantly Advance Scientific Reasoning"
      },
      {
        "paperId": "8cdb7845b94fada3be213e2abcafa7da6625879f",
        "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search"
      },
      {
        "paperId": "8970b37426c55c83711a33d0645a8212e20b3d68",
        "title": "AirRAG: Autonomous Strategic Planning and Reasoning Steer Retrieval Augmented Generation"
      },
      {
        "paperId": "42280e4374616c5f0305a8ab843fea630c0a02f9",
        "title": "O1 Replication Journey - Part 3: Inference-time Scaling for Medical Reasoning"
      },
      {
        "paperId": "687845184d09f5cc3fa59110e8c8f01a0ef0bbb2",
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning"
      },
      {
        "paperId": "24f9efbd9e229a085d4db2f477282f8ad80f1ffe",
        "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models"
      },
      {
        "paperId": "1fd282ff3a034ff6113f076b02769c46a7159476",
        "title": "A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning"
      },
      {
        "paperId": "a9161bb1bf727532e1a4aacb7ba1d7cf175edde0",
        "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs"
      },
      {
        "paperId": "18ea1a2845fcab8cd1bcb0d0b8ffb355cff665a9",
        "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling"
      },
      {
        "paperId": "d7c49d355554a6e3a5af481c0a92a515816fa82d",
        "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement"
      },
      {
        "paperId": "9272146b77e6aa6756984e54ab4edebb2f96a7d6",
        "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges"
      },
      {
        "paperId": "5fa192ca3659eeac5b96d70bebff1767bbe92268",
        "title": "o1-Coder: an o1 Replication for Coding"
      },
      {
        "paperId": "8690c869ab7e18cdbd4f17e38a41727c16b28221",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"
      },
      {
        "paperId": "5bd36b2c701056984a6353edc8ebdd3a3864bc34",
        "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS"
      },
      {
        "paperId": "4cc330c9d5fca354b24bc347de84964175560e3d",
        "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering"
      },
      {
        "paperId": "447851de5f7d86c4b422ebaa811e0e489b25de74",
        "title": "Kwai-STaR: Transform LLMs into State-Transition Reasoners"
      },
      {
        "paperId": "ab1127859b46844a7057b7860876c4c1a4292c2b",
        "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning"
      },
      {
        "paperId": "540209de237977a2fb16d3a2c4021618e8ea8b47",
        "title": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding"
      },
      {
        "paperId": "12dc720d27469a637ae659409a25f6c21da8ea5f",
        "title": "FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning"
      },
      {
        "paperId": "f19443485fb8ac84718da18da891e616401d7492",
        "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement"
      },
      {
        "paperId": "6a0c544b0a07442290e377ac5a1a3990e1eacd10",
        "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models"
      },
      {
        "paperId": "d4347c2d4a770832b8a070f5de3954fccd1fdc14",
        "title": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization"
      },
      {
        "paperId": "648775dc9ac5f40adf4e2cff81ec3304d16d1711",
        "title": "A Constrained Text Revision Agent via Iterative Planning and Searching"
      },
      {
        "paperId": "664813091a47bc960aff3ad11cbace9db36762cb",
        "title": "Technical Report on Slow Thinking with LLMs: I Enhancing LLM Reasoning with Reward-guided Tree Search"
      }
    ],
    "score": 73.0
  },
  {
    "id": "f21d0177e9374bb8579c1d9c71319f212f62b3d5",
    "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
    "authors": [
      "Pengyu Wang",
      "Dong Zhang",
      "Linyang Li",
      "Chenkun Tan",
      "Xinghao Wang",
      "Ke Ren",
      "Botian Jiang",
      "Xipeng Qiu"
    ],
    "year": 2024,
    "citationCount": 59,
    "abstract": "As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose InferAligner, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model\u2019s capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks.",
    "url": "https://www.semanticscholar.org/paper/f21d0177e9374bb8579c1d9c71319f212f62b3d5",
    "pdf_url": "https://arxiv.org/pdf/2401.11206.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-01-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2401-11206",
      "ArXiv": "2401.11206",
      "ACL": "2024.emnlp-main.585",
      "DOI": "10.48550/arXiv.2401.11206",
      "CorpusId": 267068598
    },
    "references": [
      {
        "paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6",
        "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"
      },
      {
        "paperId": "e762f92273cd96f63b7788c0173b9b6450adedd7",
        "title": "Defending ChatGPT against jailbreak attack via self-reminders"
      },
      {
        "paperId": "e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b",
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"
      },
      {
        "paperId": "02f0d5faa1cd88ea0afe48b756bfbf9adfefba28",
        "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection"
      },
      {
        "paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8",
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"
      },
      {
        "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "paperId": "58fdf550600fc3873729d466601c5d08a51ba8a0",
        "title": "Representation Engineering: A Top-Down Approach to AI Transparency"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "d4177489596748e43aa571f59556097f2cc4c8be",
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"
      },
      {
        "paperId": "a4c921bdef167ae54cc3a40643e6e3ed13d49a61",
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"
      },
      {
        "paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60",
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "fe303bbaae47b1b08d0641b41d3288fcd74a3a80",
        "title": "Steering Language Models With Activation Engineering"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "405f8f5f1c6df1b3343c812832479aad5180b65f",
        "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"
      },
      {
        "paperId": "fc50a6202e2f675604543c1ae4ef22ec74f61ad5",
        "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models"
      },
      {
        "paperId": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
        "title": "Inspecting and Editing Knowledge Representations in Language Models"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
        "title": "BBQ: A hand-built bias benchmark for question answering"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7",
        "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "d95f7b8366cf265c168e7597e2b3bd30566341cb",
        "title": "Companion"
      },
      {
        "paperId": "7191680b572ee7145f1a9d95ff11ab1ff44259f3",
        "title": "WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"
      },
      {
        "paperId": "4211bff1388da30a3b7dfd35d6aef2032900ca5c",
        "title": "Good debt or bad debt: Detecting semantic orientations in economic texts"
      },
      {
        "paperId": null,
        "title": "2023. Internlm: A multilingual language model with progressively enhanced capabilities"
      },
      {
        "paperId": null,
        "title": "2023. Gpt-3.5 turbo fine-tuning and api updates"
      },
      {
        "paperId": null,
        "title": "2023a. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model"
      },
      {
        "paperId": null,
        "title": "OpenAI. 2023."
      },
      {
        "paperId": null,
        "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "2023. Fingpt: Open-source financial large language models"
      },
      {
        "paperId": null,
        "title": "2023a. Multi-step jailbreak-ing privacy attacks on chatgpt"
      },
      {
        "paperId": null,
        "title": "2023a. Visual instruction tuning"
      },
      {
        "paperId": null,
        "title": "2023a. Safety-bench: Evaluating the safety of large language models with multiple choice questions"
      },
      {
        "paperId": null,
        "title": "2022. Extracting latent steering vectors from pretrained language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "e2756f1f3742c5cae6e5938f3479f1ee1a962e14",
        "title": "SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models"
      },
      {
        "paperId": "a868d9e302737cfb032ea60d35de954db6d4eaae",
        "title": "The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features"
      },
      {
        "paperId": "d80c0661263b9674916115b7dbf5f9375790e7bd",
        "title": "RepIt: Representing Isolated Targets to Steer Language Models"
      },
      {
        "paperId": "639cb8097626250409a34f6ea3c3b20e30fe07a3",
        "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios"
      },
      {
        "paperId": "05760ad455151ac032276ee22e6344da4a8c1d84",
        "title": "Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models"
      },
      {
        "paperId": "419f36c63b14ced847d19769f2d51601b0aeb0dd",
        "title": "A Survey on Training-free Alignment of Large Language Models"
      },
      {
        "paperId": "dca426f85222bad896042753bcd05e56c2e2608f",
        "title": "Automating Steering for Safe Multimodal Large Language Models"
      },
      {
        "paperId": "196eaa8c259542e924b4f6429e6dd3514760c2e3",
        "title": "MemOS: A Memory OS for AI System"
      },
      {
        "paperId": "fd0d2480926af0bca16621e5c6e663eefe927471",
        "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt"
      },
      {
        "paperId": "c873afac34bf4439e0a6301a2bc437f6d4a8a788",
        "title": "Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values"
      },
      {
        "paperId": "70997a66de689b4cc70268f226b83a577822208f",
        "title": "SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs"
      },
      {
        "paperId": "557157b2680df39ac43e1bddbfd5bbdf7f05e898",
        "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap"
      },
      {
        "paperId": "255baec590eb82804e918f004f68343c523939c2",
        "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
      },
      {
        "paperId": "8452bf789915673de6611c65f89e7f577c2001b1",
        "title": "Inference-time Alignment in Continuous Space"
      },
      {
        "paperId": "0c38abf51ad2ea0e3e4f799c48f1d3db764ea9a9",
        "title": "ABoN: Adaptive Best-of-N Alignment"
      },
      {
        "paperId": "7f01815dec03ae3d8da3d8ef1529b15959e63491",
        "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning"
      },
      {
        "paperId": "d997a240cb508b2dbc3b8286b9be673ac6745570",
        "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models"
      },
      {
        "paperId": "3a676409c33683540dafd473e602f27f2b4ad299",
        "title": "Safety in Large Reasoning Models: A Survey"
      },
      {
        "paperId": "d61f12d092042cd6c6dd510ddd6f319c977c90a5",
        "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender"
      },
      {
        "paperId": "3417cfbcff54ae02ef20a61fd33da6f97807a9ef",
        "title": "MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models"
      },
      {
        "paperId": "6e79f77343e8538dec8e46b103a508f3b6266f2d",
        "title": "SAE: A Multimodal Sentiment Analysis Large Language Model"
      },
      {
        "paperId": "85d4e915efe6584e116ff06976c3c8bde5cab355",
        "title": "Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning"
      },
      {
        "paperId": "282ecad9aff4625fb6cc59f666e2be12f734f7fd",
        "title": "Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models"
      },
      {
        "paperId": "c4d24bfede611ebb771d149fd817e932be0c07a6",
        "title": "Representation Engineering for Large-Language Models: Survey and Research Challenges"
      },
      {
        "paperId": "75acc2173df357ac8f2e47e2431100c1dbc47e18",
        "title": "On Almost Surely Safe Alignment of Large Language Models at Inference-Time"
      },
      {
        "paperId": "ea693001245ba993cbd4387f21b41958a0948746",
        "title": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation"
      },
      {
        "paperId": "921c6425b35fa796963febfe2a9464d34600728e",
        "title": "Mitigating Sycophancy in Large Language Models via Direct Preference Optimization"
      },
      {
        "paperId": "fb498ce0560e7360b585afa0c9b4d41f77cb217e",
        "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety"
      },
      {
        "paperId": "53b08c76094b8b7118a1bfbc074c1a9dcec74fd0",
        "title": "Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features"
      },
      {
        "paperId": "fbb3f21dd2e7f9eaf7d622b73330df57ebec2d65",
        "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks"
      },
      {
        "paperId": "233c605cc547d69299fcf4737aee51e0dfeb4652",
        "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey"
      },
      {
        "paperId": "68981715a1e37c955329fc1a278aef59c9be4764",
        "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models"
      },
      {
        "paperId": "2e9b135941dca13ce572c2b256b85737f8359f6d",
        "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense"
      },
      {
        "paperId": "da1630b8e02de660c0baccaddbb790ce94623327",
        "title": "Fast Best-of-N Decoding via Speculative Rejection"
      },
      {
        "paperId": "b738c5c18e1694aa8d4fdd66a5904f7c44376ab5",
        "title": "Inference time LLM alignment in single and multidomain preference spectrum"
      },
      {
        "paperId": "378d5aa66f1a6623fca515b81279399768c296a4",
        "title": "MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time"
      },
      {
        "paperId": "d847b9cfec1d3fcae36281a3e262fbc37a098043",
        "title": "Locking Down the Finetuned LLMs Safety"
      },
      {
        "paperId": "9ec290c428d722633330a1fc052edad4b813783b",
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models"
      },
      {
        "paperId": "8811ed198ba95dcfc0bef493088218e957eb168d",
        "title": "Towards Inference-time Category-wise Safety Steering for Large Language Models"
      },
      {
        "paperId": "c0f7b6280244cded31e48f10957d6e62d506cd98",
        "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger"
      },
      {
        "paperId": "863a43146acea0382f2c3b99c76d4d3f9dbea29d",
        "title": "$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and Defenses for Multimodal Large Language Models"
      },
      {
        "paperId": "1517fdf265b62fff361735ba105152d132127ef9",
        "title": "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models"
      },
      {
        "paperId": "314a471e8fa6788054e98aca1a74622f90fccbb9",
        "title": "Know Your Limits: A Survey of Abstention in Large Language Models"
      },
      {
        "paperId": "97e0d9dd51115d21cd87c14068a127bb51de352c",
        "title": "A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends"
      },
      {
        "paperId": "079b8865e00059187264d29e6b84f94ef3085842",
        "title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance"
      },
      {
        "paperId": "8cf67a21c29d341aeb8b8dc2d1295ede67d5996e",
        "title": "Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models"
      },
      {
        "paperId": "50e69faefa9a78afb3068f7ca15df07e7ac7b319",
        "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models"
      },
      {
        "paperId": "ec3529fe265b84944ac3f63686edadbcede8972c",
        "title": "Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets"
      },
      {
        "paperId": "a85a882fb7d9af9a2734440f4eae19c8393f0301",
        "title": "SpeechAlign: Aligning Speech Generation to Human Preferences"
      },
      {
        "paperId": "e59689af068b7402a5d0349e2075dd6c3a721eca",
        "title": "Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security"
      },
      {
        "paperId": "6a6d8794f77dcdabebbf80b8f79956f707f16eb9",
        "title": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation"
      },
      {
        "paperId": "c7137aa84b5a48bda9a96432524f9948e8c823e0",
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models"
      },
      {
        "paperId": "f88bb3f172633a83d1a4c51c3e10c29166ceb734",
        "title": "Safety of Multimodal Large Language Models on Images and Text"
      },
      {
        "paperId": "be9f699c4bfa0f29c9a0a920a6310ec70f5580e6",
        "title": "Tradeoffs Between Alignment and Helpfulness in Language Models"
      },
      {
        "paperId": "34113ce993100c4cbb3b05c3414ce80da38380be",
        "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions"
      },
      {
        "paperId": "ef2bca795a95966ebe26e08e361140351addfe9d",
        "title": "AGD: Adversarial Game Defense Against Jailbreak Attacks in Large Language Models"
      },
      {
        "paperId": "fe1e20cedeb0548ae1ecb48f4ab31fe27abe6772",
        "title": "Plan2Align: Predictive Planning Based Test-Time Preference Alignment in Paragraph-Level Machine Translation"
      },
      {
        "paperId": "bd163edab21a07feeb7a00f2c7c3b83bd5958aaa",
        "title": "The Art of Refusal: A Survey of Abstention in Large Language Models"
      },
      {
        "paperId": "478432d02cab0db4ac2a3971b30c647daa3721a9",
        "title": "Defending Jailbreak Attack in VLMs via Cross-modality Information Detector"
      }
    ],
    "score": 59.0
  },
  {
    "id": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
    "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Chengxi Li",
      "Kai Fan"
    ],
    "year": 2024,
    "citationCount": 54,
    "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks. Our code is available at \\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
    "url": "https://www.semanticscholar.org/paper/830c277b2992f59ec2f21982e245bd1e17dd85ca",
    "pdf_url": "https://arxiv.org/pdf/2406.10858.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-06-16",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-10858",
      "ArXiv": "2406.10858",
      "DOI": "10.48550/arXiv.2406.10858",
      "CorpusId": 270559546
    },
    "references": [
      {
        "paperId": "36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
        "title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time"
      },
      {
        "paperId": "dcebd9de6fb8ffd5e5fff7734e277abd6f3858ce",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      },
      {
        "paperId": "28c2f0f9929434c1e82bceac04e8b4c2adc419dd",
        "title": "MARIO Eval: Evaluate Your Math LLM with your Math LLM-A mathematical dataset evaluation toolkit"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "135da052891fdcf3842901b6b7585ca8c258f1c2",
        "title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs"
      },
      {
        "paperId": "37a00c43bd1099a8bdef978ab0aa6d2566cefad0",
        "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "675629ff78cef09665a1135fece66195ed80a640",
        "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "7e28021743f414fb8f13843f9b053ddee1822f97",
        "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension"
      },
      {
        "paperId": "b16c7d45183b9d595ab64301be019741b1528860",
        "title": "Llemma: An Open Language Model For Mathematics"
      },
      {
        "paperId": "cddb552f6c3464a54a02b0b64b2d1af56c086606",
        "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"
      },
      {
        "paperId": "b272513916b45c8517d289d7abee4a53e6832187",
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9",
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
        "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "72dd63d67588a42fc817bbb8d655b397f67425df",
        "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "7c0c0445e89347798800aad3497fcf2f2d27d4e6",
        "title": "Multi-armed bandits with episode context"
      },
      {
        "paperId": "9eb0f02e4bb52f84596a40ca27ff2d1370a5ecae",
        "title": "Ranking Measures and Loss Functions in Learning to Rank"
      },
      {
        "paperId": "5fc5c5a4e489e781de434567d946e6eb65c44f60",
        "title": "Learning to rank for information retrieval"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "Making PPO even better: Value-guided monte-carlo tree search decoding"
      },
      {
        "paperId": null,
        "title": "5: An example of our SFT XML format"
      },
      {
        "paperId": null,
        "title": "2024. Iterative reasoning preference optimization"
      },
      {
        "paperId": null,
        "title": "2023. Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "2024b. SEER: Facili-tating structured reasoning and explanation via reinforcement learning"
      },
      {
        "paperId": null,
        "title": "2024. Llamafac-tory: Unified efficient fine-tuning of 100+ language models"
      },
      {
        "paperId": null,
        "title": "2023. PAL: program-aided language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "f0301bcbdc87883911ffef88a0a970bcc601895c",
        "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models"
      },
      {
        "paperId": "2007b767d8aeeb23290b76e2b9eb44e1a272b5a6",
        "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs"
      },
      {
        "paperId": "7c32b20b0d6f6b4fdb51bd35813222f5ac7fcc71",
        "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality"
      },
      {
        "paperId": "2bbea815365a78cf76d752ccc626b4ff85ddd1e6",
        "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models"
      },
      {
        "paperId": "0b228ce57cf0929eec39e8ae4d450e6f266ea3fa",
        "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner"
      },
      {
        "paperId": "f9a655ea7a41b9a52e3964431b152c0ef76eacba",
        "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS"
      },
      {
        "paperId": "14b0064fd15c42e1691c0ee93ca61cfa071bdbeb",
        "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards"
      },
      {
        "paperId": "1f2c18dd03b6053b0f22aab3020ced7203bb7c34",
        "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning"
      },
      {
        "paperId": "344b7374b1b78df4a4d488e38cd94af7d84ec6c0",
        "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"
      },
      {
        "paperId": "cbd34d713156be2fb8991e1d55f4f478838a3659",
        "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning"
      },
      {
        "paperId": "7985e019694318d12c52cc03ee2adef36845059c",
        "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization"
      },
      {
        "paperId": "4a5c682a108e1b3b384d846beb05164cd47d4a2d",
        "title": "CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning"
      },
      {
        "paperId": "5da436462e720ce614f4dd1004dba68325f2fc53",
        "title": "Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router"
      },
      {
        "paperId": "e4bcb3f40c952a639c2bd665f8754defd67aad5e",
        "title": "AI Agent Behavioral Science"
      },
      {
        "paperId": "c09e55cddbd5839a1cef3d684e96db7cbbd4ccc7",
        "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information"
      },
      {
        "paperId": "5becc8d0a5066c58d2785798519f2cddad7f4482",
        "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning"
      },
      {
        "paperId": "2ab138c8ad6be7bfa4c001fdc51232e687c878be",
        "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization"
      },
      {
        "paperId": "6c3efe0d2d7393a64d2729f7d355de78d9422bee",
        "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning"
      },
      {
        "paperId": "4b8ce4d10bafb92d22335757066b2c1a120f7241",
        "title": "Efficient Pretraining Length Scaling"
      },
      {
        "paperId": "cb23511f7ee60cd102ed1ff95dd01915cbe49861",
        "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability"
      },
      {
        "paperId": "960687544c8feae5448acc90b8349e771e615e50",
        "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement"
      },
      {
        "paperId": "0af4359948693c7aa60caf5e13aaab9d32fced92",
        "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization"
      },
      {
        "paperId": "9c4d8cdbe337b309bb1649d0f5aad508d4f05f40",
        "title": "Thinking Machines: A Survey of LLM based Reasoning Strategies"
      },
      {
        "paperId": "043aef64488f36fad56b71fb852c5cb027e87245",
        "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "8768f8bce493771ae872f906cd8df41e0ea200c0",
        "title": "Process-based Self-Rewarding Language Models"
      },
      {
        "paperId": "737215b51033b73b93ce2fcf213179516f18e87a",
        "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning"
      },
      {
        "paperId": "5f2f7305a467e01f76b1dbf5867fa34bd90edc19",
        "title": "What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "986839c6cd0f3cbd3dbb68b5376ea4c9c1d58765",
        "title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time"
      },
      {
        "paperId": "20aa776939c564cd9234489d22eef6835a83717e",
        "title": "Self-rewarding correction for mathematical reasoning"
      },
      {
        "paperId": "eaaf5aba2e91b6b4f1a750fcc4f02b5df4a94210",
        "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning"
      },
      {
        "paperId": "ffcf77027f5144c17de1cbb72b5c1d783a3848f7",
        "title": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model"
      },
      {
        "paperId": "0f11244a4290d31c80e744dc7207fa8d18bac016",
        "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation"
      },
      {
        "paperId": "78160392b2127444fe5488ede2065f4c7102a3ba",
        "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling"
      },
      {
        "paperId": "3ac976418e99aa5e558dedcb0ec25d6eb6c35750",
        "title": "STAIR: Improving Safety Alignment with Introspective Reasoning"
      },
      {
        "paperId": "03b60c70d90d4ed48e3dd4b0cb4e48e769579535",
        "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains"
      },
      {
        "paperId": "687845184d09f5cc3fa59110e8c8f01a0ef0bbb2",
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning"
      },
      {
        "paperId": "00d5c2f85f812d19a820e8d8c00b9ef290b25f5f",
        "title": "What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning"
      },
      {
        "paperId": "f520c3faaa8e665c019995d01ad3fde80d3c0be2",
        "title": "Entropy-Regularized Process Reward Model"
      },
      {
        "paperId": "3b7409ee9784ea43eceb944b2d678b77557fd8bc",
        "title": "Markov Chain of Thought for Efficient Mathematical Reasoning"
      },
      {
        "paperId": "2fd1e96f56794be12b02044f83eb2c2370c69037",
        "title": "Process Reward Model with Q-Value Rankings"
      },
      {
        "paperId": "9e91b085852906decb4052ea198dd81a73ed6894",
        "title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization"
      },
      {
        "paperId": "7bf0763943423754b3b38454eb14f971759bae1c",
        "title": "Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing"
      },
      {
        "paperId": "fffbbcbe4b58f7e04156aacba76e60e081fb5909",
        "title": "Learning Evolving Tools for Large Language Models"
      },
      {
        "paperId": "4f98157c298b87408646672b812801f3d6618c76",
        "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning"
      },
      {
        "paperId": "d084517f14ee247883de0f4dd58bb923e418157d",
        "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning"
      },
      {
        "paperId": "b076ae72014393277587623a54950f116f43340b",
        "title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "3cd973e87008ac9a2be45a16cac7c9d0c816ae10",
        "title": "LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search"
      },
      {
        "paperId": "e6876d73d5aa06e09328d8ddf240c8a800a62667",
        "title": "Iterative Trajectory Exploration for Multimodal Agents"
      },
      {
        "paperId": "a8a8ac429fbc9411fd456d527351f9479c83380b",
        "title": "Subtle Errors Matter: Preference Learning via Error-injected Self-editing"
      },
      {
        "paperId": "64353265c2b38b81e392572fec3e467a50587cef",
        "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization"
      }
    ],
    "score": 54.0
  },
  {
    "id": "44162aa2763c88a384d9c51d60eafcc59277a1c9",
    "title": "Decoding-time Realignment of Language Models",
    "authors": [
      "Tianlin Liu",
      "Shangmin Guo",
      "Leonardo Bianco",
      "Daniele Calandriello",
      "Quentin Berthet",
      "Felipe Llinares-L\u00f3pez",
      "Jessica Hoffmann",
      "Lucas Dixon",
      "Michal Valko",
      "Mathieu Blondel"
    ],
    "year": 2024,
    "citationCount": 53,
    "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.",
    "url": "https://www.semanticscholar.org/paper/44162aa2763c88a384d9c51d60eafcc59277a1c9",
    "pdf_url": "https://arxiv.org/pdf/2402.02992.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-05",
    "externalIds": {
      "DBLP": "conf/icml/LiuGBCBLHDVB24",
      "ArXiv": "2402.02992",
      "DOI": "10.48550/arXiv.2402.02992",
      "CorpusId": 267411998
    },
    "references": [
      {
        "paperId": "b899a28eb553800ce558cf8974a697f65103e591",
        "title": "DeAL: Decoding-time Alignment for Large Language Models"
      },
      {
        "paperId": "f6d7482bb5baf33f22b862ad4997f5c8cd13db21",
        "title": "Tuning Language Models by Proxy"
      },
      {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f1019fe7918ee51cf16c91186e769a71a9d9fb42",
        "title": "An Emulator for Fine-Tuning Large Language Models using Small Language Models"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "e5d0857feca845b474b89565d513ff599629851d",
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model"
      },
      {
        "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "title": "Mistral 7B"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "e56dc21699e6283fce072ffc908cb9f66321760d",
        "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "5a3a04af4935302f0871bf14a4b573d477ce96be",
        "title": "Stay on topic with Classifier-Free Guidance"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "ca991e0283a1c30a46eb585d9eb499fc0ec8ecc2",
        "title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling"
      },
      {
        "paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644",
        "title": "Fast Inference from Transformers via Speculative Decoding"
      },
      {
        "paperId": "88b62496cbc52072bfa8f4b29d172b0477b701bc",
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "cf18a9f5a334e574f1d1f6ffdd64b6dac11fe9be",
        "title": "RL with KL penalties is better viewed as Bayesian inference"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1ed66e048bb025e75aa5ea660545285212e5341f",
        "title": "Scaling Up Models and Data with t5x and seqio"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "c3e937bba9cdb4ac6a93f92154843c041c806ba9",
        "title": "Simple Fusion: Return of the Language Model"
      },
      {
        "paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22",
        "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "5fcd41ca42659ff792fc8ee7d535156e8e69f987",
        "title": "On Using Monolingual Corpora in Neural Machine Translation"
      },
      {
        "paperId": "fa24ef06682453830859b43f9bd5b65b91eedabf",
        "title": "Judging"
      },
      {
        "paperId": null,
        "title": "Alignment as reward-guided search"
      },
      {
        "paperId": null,
        "title": "The alignment hand-book"
      },
      {
        "paperId": null,
        "title": "Decoding-time Realignment of the Conference on Empirical Methods"
      }
    ],
    "cited_by": [
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "51fa6682984b9a8b3085cdd3d2918444317ef3ee",
        "title": "Alignment-Aware Decoding"
      },
      {
        "paperId": "7db85f8f5d173067e9a0aee4f29355c375632ffa",
        "title": "T-POP: Test-Time Personalization with Online Preference Feedback"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "85359dcfea8498a0119d24d4f84f4f52e64a971d",
        "title": "Future Policy Aware Preference Learning for Mathematical Reasoning"
      },
      {
        "paperId": "419f36c63b14ced847d19769f2d51601b0aeb0dd",
        "title": "A Survey on Training-free Alignment of Large Language Models"
      },
      {
        "paperId": "eceed697eb1d38ccca7d3001f61f595db8a75fac",
        "title": "Flexible Realignment of Language Models"
      },
      {
        "paperId": "c30cabc37dd04d6bf322bb13e81b9c34cebfe12a",
        "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation"
      },
      {
        "paperId": "12dfb79f7860f01e39a61a76afff2a42802c9778",
        "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner"
      },
      {
        "paperId": "05d1b67c4521c21decdd6adb68ba7de53a84f4a5",
        "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment"
      },
      {
        "paperId": "c03d9b77bf69879faf646270d5e78a71a5d22809",
        "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners"
      },
      {
        "paperId": "8452bf789915673de6611c65f89e7f577c2001b1",
        "title": "Inference-time Alignment in Continuous Space"
      },
      {
        "paperId": "6fc4c08d74bafdb78150dfcda158930ac477123b",
        "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models"
      },
      {
        "paperId": "a9bfb4162132c1ed41dcda79243545d1d5ffcec7",
        "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models"
      },
      {
        "paperId": "e22d62ee074b865c30af71f56629b67895e592c4",
        "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning"
      },
      {
        "paperId": "a9d6a3fa154837e0d88238fc779a9993817e440f",
        "title": "Robust Multi-Objective Controlled Decoding of Large Language Models"
      },
      {
        "paperId": "d2f37492d17c7333b7df48e91d50240e6a4b0b6b",
        "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity"
      },
      {
        "paperId": "4223f3f83f03d98eb37d5c044332b9cd9f28aa1d",
        "title": "Adding Alignment Control to Language Models"
      },
      {
        "paperId": "ed034fff0b46b7b375befb284f3591b022e38def",
        "title": "Robust Multi-Objective Preference Alignment with Online DPO"
      },
      {
        "paperId": "48d87b9c0e65b08322fc0ea41240c07ecc230f29",
        "title": "Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs"
      },
      {
        "paperId": "d885b445226dc2979ad9d4f2a67f05c4ef18045f",
        "title": "KL Penalty Control via Perturbation for Direct Preference Optimization"
      },
      {
        "paperId": "8375d8ebcebbcb9458b94ab5872a092c82e2f549",
        "title": "Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models"
      },
      {
        "paperId": "64ac85b6e95667adc0156b401fe415d330fd279a",
        "title": "SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters"
      },
      {
        "paperId": "61042989138362d11ac65fb1b6db30b6cb13ec54",
        "title": "Fast Large Language Model Collaborative Decoding via Speculation"
      },
      {
        "paperId": "cebfb3ccb1aee432f0588aef15a355170ec3d0f4",
        "title": "How to Merge Your Multimodal Models Over Time?"
      },
      {
        "paperId": "3426ae2f023f0bb5693e0af657df05261a8ac2dc",
        "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors"
      },
      {
        "paperId": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
        "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model"
      },
      {
        "paperId": "b738c5c18e1694aa8d4fdd66a5904f7c44376ab5",
        "title": "Inference time LLM alignment in single and multidomain preference spectrum"
      },
      {
        "paperId": "6a4753b8843d2fb87b9c1750b6004e0a2de34946",
        "title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection"
      },
      {
        "paperId": "95124cb03a6e5de7a623db32b987531d7830629e",
        "title": "PAD: Personalized Alignment of LLMs at Decoding-time"
      },
      {
        "paperId": "67475bd33373c1a957b748f8cc886f603698bef0",
        "title": "Large Language Models can be Strong Self-Detoxifiers"
      },
      {
        "paperId": "57330f4e9f4c35d875fae076d283abcf5e0bed87",
        "title": "Erasing Conceptual Knowledge from Language Models"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "2ad7d0b05449b07d8db37838eba1615de7f6cb0b",
        "title": "FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices"
      },
      {
        "paperId": "3ea2a38eb00af88a7bf775781007396f2f5b6c1f",
        "title": "The Crucial Role of Samplers in Online Direct Preference Optimization"
      },
      {
        "paperId": "73ba52ee54ce9631a7835d0887815f13708325b3",
        "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective"
      },
      {
        "paperId": "b51d1946a6184946c93809d06942aa410c384203",
        "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey"
      },
      {
        "paperId": "cbd21edc05704d8d1be845bba772dea13b5f3ba7",
        "title": "Personality Alignment of Large Language Models"
      },
      {
        "paperId": "d95220be54d155f8be7f36b4fde794711cbb69b3",
        "title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts"
      },
      {
        "paperId": "21cdb3aced0e15d8581f3f58ea836b4f379a06fa",
        "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law"
      },
      {
        "paperId": "1ef297466d7250b26e47b3bd405d8a8662567731",
        "title": "Conditional Language Policy: A General Framework For Steerable Multi-Objective Finetuning"
      },
      {
        "paperId": "8b89036b5136b081867fbec089801b2bd4ab0685",
        "title": "Decoding-Time Language Model Alignment with Multiple Objectives"
      },
      {
        "paperId": "d9d8aef662bb7a3730a62b1015c3ed99e4287523",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt"
      },
      {
        "paperId": "aef234095eb56f3510737df572fd155668523bb0",
        "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies"
      },
      {
        "paperId": "2257d0894da128e9c900df1237f43504c9c0e82e",
        "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
      },
      {
        "paperId": "a264dbba6c16a460998cd16e4f48d62a1eddd4d0",
        "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment"
      },
      {
        "paperId": "e093bb41227dbc72d7b8502b28e6dfd6cb24dd39",
        "title": "Transfer Q Star: Principled Decoding for LLM Alignment"
      },
      {
        "paperId": "3ec648362481eaa44e439fa0955533da390cacfd",
        "title": "Model Extrapolation Expedites Alignment"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9",
        "title": "Weak-to-Strong Jailbreaking on Large Language Models"
      },
      {
        "paperId": "a9ed1ec80fd874a4b127b691437717b77047e13e",
        "title": "Weak-to-Strong Honesty Alignment via Learning-to-Rank Supervision"
      },
      {
        "paperId": "abe28351f96c1bc471ac7126685a4a31450c6ee4",
        "title": "PAD: Personalized Alignment at Decoding-Time"
      },
      {
        "paperId": "a54ffaa66b4f6cca30440168d96f79175a413bc6",
        "title": "Base-Change at Prediction: Inference-Time Update of Fine-Tuned Models"
      }
    ],
    "score": 53.0
  },
  {
    "id": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Zhihan Liu",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan",
      "Zhaoran Wang"
    ],
    "year": 2024,
    "citationCount": 43,
    "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.",
    "url": "https://www.semanticscholar.org/paper/30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
    "pdf_url": "https://arxiv.org/pdf/2405.19332.pdf",
    "venue": "Trans. Mach. Learn. Res.",
    "publicationDate": "2024-05-29",
    "externalIds": {
      "DBLP": "journals/corr/abs-2405-19332",
      "ArXiv": "2405.19332",
      "DOI": "10.48550/arXiv.2405.19332",
      "CorpusId": 270095059
    },
    "references": [
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "81ca91a7f73d5697673e9ab1c9b26930dc35163a",
        "title": "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "4499afc74bda1c7d521a516df040facfe39943ed",
        "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "f9fc7b5ebd5fe30257abb03ad87d8138eeeb28d9",
        "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "eabd317bf33a636b099a38f4b49aecca97202661",
        "title": "sDPO: Don't Use Your Data All at Once"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba",
        "title": "Yi: Open Foundation Models by 01.AI"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e",
        "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "9bb9248b2a79494b66d971ac9c655d334a422783",
        "title": "Efficient Exploration for LLMs"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "19df5eb2c74606414ed93633b4c61947cc42dbbb",
        "title": "Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss"
      },
      {
        "paperId": "6933570be05269a2ccf437fbcca860856ed93659",
        "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models"
      },
      {
        "paperId": "15b8b6a8028b2b6e75b67dfb6aebaede36826cf8",
        "title": "Language Model Alignment with Elastic Reset"
      },
      {
        "paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26",
        "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"
      },
      {
        "paperId": "91e8ce403704a38bee8a5df90d99979a796d1741",
        "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "e26888285436bc7998e5c95102a9beb60144be5e",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475",
        "title": "Self-Alignment with Instruction Backtranslation"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b",
        "title": "Textbooks Are All You Need"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "fbd2c8089870814449f9254a711041bbae145a82",
        "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
      },
      {
        "paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"
      },
      {
        "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
        "title": "Instruction Tuning with GPT-4"
      },
      {
        "paperId": "f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f",
        "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction"
      },
      {
        "paperId": "86cf30c4378b8b8a0da7888edc8c383182f93c8a",
        "title": "Approximate Thompson Sampling via Epistemic Neural Networks"
      },
      {
        "paperId": "49bbd1064f1a17f314f8c61528bcce959a7c249b",
        "title": "GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "00d4aea3e13b7d4ad043d7081be31ffb2859ad99",
        "title": "Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement Learning"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "1e00e8760472dab0fe1632a04a037d52d227d3a6",
        "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning"
      },
      {
        "paperId": "7ff21c6ca43afd1ec99ee958ce2f9e1da40ec881",
        "title": "Epistemic Neural Networks"
      },
      {
        "paperId": "034b2e3d957df5405783f2c5695a8c2bd87d6334",
        "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
        "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "6d18c5fbd2eb1808b006f5d3433a7735e88f49d2",
        "title": "Ensemble Sampling"
      },
      {
        "paperId": "fa2c136d526d7445b02e802ff9601ea9e9663fcb",
        "title": "Eluder Dimension and the Sample Complexity of Optimistic Exploration"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "ac4b7a6dad66a9a09e69019936d1c182f7f2a6e0",
        "title": "From \u025b-entropy to KL-entropy: Analysis of minimum information complexity density estimation"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "47049bc0c666acddba1bb1fa09e3e4b6a4cae7c7",
        "title": "HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments"
      },
      {
        "paperId": "e4435f282266da92d37066064c5239c6f96f0d64",
        "title": "Gibbs Sampling from Human Feedback: A Provable KL- constrained Framework for RLHF"
      },
      {
        "paperId": "233c06017ed41c40140947796525ce7452c93ab9",
        "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration"
      },
      {
        "paperId": null,
        "title": "Starling-7b: Improving llm helpfulness and harmlessness with rlaif"
      },
      {
        "paperId": "9f414c81159b5e8296a267d5ae95a5bf464061d1",
        "title": "The rating of chessplayers, past and present"
      },
      {
        "paperId": null,
        "title": "Provably sample efficient rlhf via active preference optimization"
      },
      {
        "paperId": null,
        "title": "Chris Glaze (2024)"
      },
      {
        "paperId": null,
        "title": "practices and lessons"
      },
      {
        "paperId": null,
        "title": "Introducing meta llama 3: The most capable openly available llm to date"
      },
      {
        "paperId": null,
        "title": "Stanford alpaca: An instruction-following llama model"
      },
      {
        "paperId": null,
        "title": "Snorkel-mistral-pairrm-dpo"
      },
      {
        "paperId": null,
        "title": ": Direct distillation of lm alignment"
      },
      {
        "paperId": null,
        "title": "the proposed SELM method"
      }
    ],
    "cited_by": [
      {
        "paperId": "3a6dc3516c20a45062564e67d742a567674865bf",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization"
      },
      {
        "paperId": "435de15da2ce37a97e0f2731beb10ff9765746d5",
        "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "59c6403b4445fcdc4e0cb918cb7aa080206432ba",
        "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences"
      },
      {
        "paperId": "da30931a1d7bfaddc0a47d00bd5a8600c4ad3e96",
        "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment"
      },
      {
        "paperId": "3ae86ab61477815856aaddbf1c4c90e916e56373",
        "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models"
      },
      {
        "paperId": "e9e22a099d1e065c027a3bced40d7fd61f9f9e65",
        "title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model"
      },
      {
        "paperId": "36540ee20aa4ba4f02f7359e1113cadbefbf1b6d",
        "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy"
      },
      {
        "paperId": "03e385b1a6fdb7bef5612feea44e44d5efd45bcc",
        "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression"
      },
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "e8a45c4e2af9839b8fab8b7ee3a5925be549762f",
        "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits"
      },
      {
        "paperId": "78f4d69750ecf17c76b9940a82e2c2f244deb27d",
        "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning"
      },
      {
        "paperId": "2a8f57ed823a84a139113cd9c5c7fea43c449546",
        "title": "Mutual-Taught for Co-adapting Policy and Reward Models"
      },
      {
        "paperId": "4c452b84386edf6d0afe1998d16b3e4fbc69eb9e",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "fddb80238bfb3b601fd172911f915fa59970fbb9",
        "title": "Q\u266f: Provably Optimal Distributional RL for LLM Post-Training"
      },
      {
        "paperId": "653546ec8e98c4037fe56277eaff8666882eaadd",
        "title": "Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective"
      },
      {
        "paperId": "9f13649ed8c1987a8de4c1d6df90ab9da344d09d",
        "title": "Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective"
      },
      {
        "paperId": "d238f25614f15d329399843c2e94ee85aa057ff6",
        "title": "The Best Instruction-Tuning Data are Those That Fit"
      },
      {
        "paperId": "a2a5ea730b7d0ef7653060595a021360be4ad57f",
        "title": "LLM Safety Alignment is Divergence Estimation in Disguise"
      },
      {
        "paperId": "f64ab88f0326d0473a820f2284509cdae619d542",
        "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration"
      },
      {
        "paperId": "b7a180b1216cd374ce79da79a09df62f398eff14",
        "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning"
      },
      {
        "paperId": "debbcad09f2bc410b9c787ec4aefa21d118a597f",
        "title": "Understanding the Logic of Direct Preference Alignment through Logic"
      },
      {
        "paperId": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
      },
      {
        "paperId": "980bbe792855b811efae0311b149d2f4030a5d58",
        "title": "Hybrid Preference Optimization for Alignment: Provably Faster Convergence Rates by Combining Offline Preferences with Online Exploration"
      },
      {
        "paperId": "5cf38eb5325f196d3c9a931fede9ed2ecee82f28",
        "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs"
      },
      {
        "paperId": "8430ea2921172c7c07e5286f19ffdf4a43d1d4b0",
        "title": "Sample-Efficient Alignment for LLMs"
      },
      {
        "paperId": "f28a570b7509eef12951787f58e777ecf0d46b91",
        "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization"
      },
      {
        "paperId": "efae534e2a3e4b79fe4050fa3fd40d8c7ba9745b",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      },
      {
        "paperId": "105c517fed8da9ad0476b82d8bb1eb7fd4dc2c1a",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      },
      {
        "paperId": "955372c369fecc85f6b4f093c312f0cfb425c688",
        "title": "Can Language Models Reason about Individualistic Human Values and Preferences?"
      },
      {
        "paperId": "7c8ad9293ad11527090503be4b3ba156cf73b06a",
        "title": "Just say what you want: only-prompting self-rewarding online preference optimization"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "e7b5d0269bdd37d01cea2bddb4d2ec9cf1539a40",
        "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"
      },
      {
        "paperId": "928b3ef966cb0e1e9cff7e5e96d5df23c47c2d5a",
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift"
      },
      {
        "paperId": "39fd3d41f5ab882eea29dbe27eef8d0954b29856",
        "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "86905d23c2d1c9386f3eb063132c14119ed79e5c",
        "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "7508634ac1312a7a975cbdf06fe754db2a1a3c09",
        "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration"
      }
    ],
    "score": 43.0
  },
  {
    "id": "613a32f18388958cc60dbb906d87fc7f206c0e66",
    "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
    "authors": [
      "Huizhuo Yuan",
      "Zixiang Chen",
      "Kaixuan Ji",
      "Quanquan Gu"
    ],
    "year": 2024,
    "citationCount": 43,
    "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\"and\"loser\"images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.",
    "url": "https://www.semanticscholar.org/paper/613a32f18388958cc60dbb906d87fc7f206c0e66",
    "pdf_url": "https://arxiv.org/pdf/2402.10210.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-02-15",
    "externalIds": {
      "ArXiv": "2402.10210",
      "DBLP": "conf/nips/YuanCJG24",
      "DOI": "10.48550/arXiv.2402.10210",
      "CorpusId": 267681884
    },
    "references": [
      {
        "paperId": "d086b0e2fb58024ce264e985334eddd1314f0e7b",
        "title": "DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design"
      },
      {
        "paperId": "36fe9696942978764ad00471154dd67dcfb14e36",
        "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "ec97a1565dff9d2fab1ef489e47296bbef68b680",
        "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model"
      },
      {
        "paperId": "f5275c61736781d236abe6700b822f1ea62f982e",
        "title": "Diffusion Model Alignment Using Direct Preference Optimization"
      },
      {
        "paperId": "10fce9e1718e968846c8429366c597380cce213d",
        "title": "A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation"
      },
      {
        "paperId": "174df7d2cc8d49f219d8899c7d75bc929f3a57c7",
        "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation"
      },
      {
        "paperId": "a395f1c0910a5622f6faaefb5b27e87bb1617759",
        "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards"
      },
      {
        "paperId": "e04da3c945aae8e2211222d373e7bf771d6412a7",
        "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack"
      },
      {
        "paperId": "d7890d1906d95c4ae4c430b350455156d6d8aed9",
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "paperId": "d8008ace077d72ccd277be795c720e9381623c10",
        "title": "Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "a553bf27d801d09f667fe121c0ba9632257f364b",
        "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"
      },
      {
        "paperId": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d",
        "title": "Training Diffusion Models with Reinforcement Learning"
      },
      {
        "paperId": "cf694df964caa156ec306b45d3a3127533cb458f",
        "title": "Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation"
      },
      {
        "paperId": "1b2355c3c674b26a977768a91a164384ad51bbb1",
        "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "1f898d66acabff511a3871b82799aa73c0055402",
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation"
      },
      {
        "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
        "title": "Scalable Diffusion Models with Transformers"
      },
      {
        "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
      },
      {
        "paperId": "e99604e2da48483b633247c13dd4ad5f46196562",
        "title": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking"
      },
      {
        "paperId": "5b19bf6c3f4b25cac96362c98b930cf4b37f6744",
        "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"
      },
      {
        "paperId": "5406129d9d7d00dc310671c43597101b0ee93629",
        "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
      },
      {
        "paperId": "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe",
        "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"
      },
      {
        "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
      },
      {
        "paperId": "c57293882b2561e1ba03017902df9fc2f289dea2",
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
      },
      {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
        "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
      },
      {
        "paperId": "91b32fc0a23f0af53229fceaae9cce43a0406d2e",
        "title": "Structured Denoising Diffusion Models in Discrete State-Spaces"
      },
      {
        "paperId": "95f5bafba97beb9b4f8c1fe607f04ec28efab7f9",
        "title": "Learning to Efficiently Sample from Diffusion Probabilistic Models"
      },
      {
        "paperId": "0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4",
        "title": "Cascaded Diffusion Models for High Fidelity Image Generation"
      },
      {
        "paperId": "bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d",
        "title": "Image Super-Resolution via Iterative Refinement"
      },
      {
        "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations"
      },
      {
        "paperId": "014576b866078524286802b1d0e18628520aa886",
        "title": "Denoising Diffusion Implicit Models"
      },
      {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "965359b3008ab50dd04e171551220ec0e7f83aba",
        "title": "Generative Modeling by Estimating Gradients of the Data Distribution"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "6e36fc1485ee735796a6ac39ff8155bb2c4f7017",
        "title": "Generative Adversarial Networks: An Overview"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "0095b9f73c000f2609fc81ffb7769df7cd77bda1",
        "title": "COCO-Stuff: Thing and Stuff Classes in Context"
      },
      {
        "paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
      },
      {
        "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
        "title": "Microsoft COCO: Common Objects in Context"
      },
      {
        "paperId": "f3d4c52a3cfcc6c550fa30cc20c516e285eabf00",
        "title": "Integral Probability Metrics and Their Generating Classes of Functions"
      },
      {
        "paperId": "5ed59f49c1bb7de06cfa2a9467d5efb535103277",
        "title": "Temporal Difference Learning and TD-Gammon"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "Fast sampling via de-randomization for discrete diffusion models"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": "cfee1826dd4743eab44c6e27a0cc5970effa4d80",
        "title": "Improving Image Generation with Better Captions"
      },
      {
        "paperId": null,
        "title": "a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm"
      }
    ],
    "cited_by": [
      {
        "paperId": "34e6cc059b06730d1bcfe6afa2997bb8c5154969",
        "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models"
      },
      {
        "paperId": "052e8e4a9cfd766aa07550a10778514c54e3dba6",
        "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"
      },
      {
        "paperId": "f6b80274adac521baa2150961edc80fffced9fae",
        "title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion"
      },
      {
        "paperId": "fb0514530a864ee49e652ea24ed0811874cd93ac",
        "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer"
      },
      {
        "paperId": "bbf7c2d4bc565a03a70d6aa1d6a3de6f99811ddc",
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning"
      },
      {
        "paperId": "96b7b18af643fffceeac93b739d3418bd3e42645",
        "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process"
      },
      {
        "paperId": "b7ba318b26aeaa3c9d5ffd3952c3658949b74a5f",
        "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models"
      },
      {
        "paperId": "594d82ab5ed8ac54c4419e5e10152ac54dcf2f60",
        "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again"
      },
      {
        "paperId": "20309e3e9934742961623b1acf459bf247954016",
        "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE"
      },
      {
        "paperId": "27a59601c328bf357c19950fdbcbadfd439fb874",
        "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining"
      },
      {
        "paperId": "bba9df17cf0452603e402f508da87ef311282c6b",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"
      },
      {
        "paperId": "fa103eab356293f85deb387b3da7ae0de7fb7640",
        "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation"
      },
      {
        "paperId": "8e70bd26dd94fe39dd2202a8183906b67b1f425b",
        "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models"
      },
      {
        "paperId": "15143892b7e8cded24304c1077a9f7d20de363dd",
        "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training"
      },
      {
        "paperId": "55fa94e0707f16353b66038b24c4bf52e7b54fab",
        "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations"
      },
      {
        "paperId": "16c38a5b7d7d8487c9e03e7231330445eb64e5b1",
        "title": "Flow-GRPO: Training Flow Matching Models via Online RL"
      },
      {
        "paperId": "9550ed2b9f6a6f98024ab3ef93d9df216a649815",
        "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning"
      },
      {
        "paperId": "5b8e091a424698c5c68f45f193e3a0daeb73792a",
        "title": "SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization"
      },
      {
        "paperId": "88345d7ce1016cf241694367e1d3b62d37db0a7e",
        "title": "Aligning Anime Video Generation with Human Feedback"
      },
      {
        "paperId": "735a453c1511f369e5cb451064135f579c6e4517",
        "title": "PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models"
      },
      {
        "paperId": "78a6748e26fff8ead21327eb4384112a88cbd6a1",
        "title": "Unified Reward Model for Multimodal Understanding and Generation"
      },
      {
        "paperId": "0c8d812e52f750d6d1edee433191c24806d5f9c8",
        "title": "Improving the Scaling Laws of Synthetic Data with Deliberate Practice"
      },
      {
        "paperId": "49f7b33fb59f7281018a5d29b8fc577e56a85b07",
        "title": "Dual Caption Preference Optimization for Diffusion Models"
      },
      {
        "paperId": "9b4a577a59df2ad605d4b6f4ddb5ed6522660604",
        "title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models"
      },
      {
        "paperId": "b88ba0959975d1aba56b182735c3abb5cc344023",
        "title": "Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning"
      },
      {
        "paperId": "e662cd34cb20d3448de70bd84e8c12df8796b090",
        "title": "Half-order Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer"
      },
      {
        "paperId": "21d0c6903b45c2a63b1ee9b6506ce0c4cfc808fc",
        "title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search"
      },
      {
        "paperId": "2bb2f4e3a55c19c66875ea0a84d409b6a2e16fce",
        "title": "Improving Video Generation with Human Feedback"
      },
      {
        "paperId": "9d1c14e267615e3aa2f1351cc65f4a9585ce496d",
        "title": "RFSR: Improving ISR Diffusion Models via Reward Feedback Learning"
      },
      {
        "paperId": "be24a6e2e75d224fe61f174d5faf288fc60794d8",
        "title": "Aligning Few-Step Diffusion Models with Dense Reward Difference Learning"
      },
      {
        "paperId": "dd08724a78fbcf945b76307d36b89c53ffba2b31",
        "title": "Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization"
      },
      {
        "paperId": "275499681685a08462c761b373a35cf397f88871",
        "title": "Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization"
      },
      {
        "paperId": "268e7df89189e13a4bf0dc4ebbf74cf057dd2082",
        "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference"
      },
      {
        "paperId": "5951af3b0ad6a024cb83db42cef56ee1bbf5128f",
        "title": "Information Theoretic Text-to-Image Alignment"
      },
      {
        "paperId": "182d4ed333a892cbcd44784dd90a717a3703fdce",
        "title": "Boost Your Human Image Generation Model via Direct Preference Optimization"
      },
      {
        "paperId": "8e789824bc853b840a61497c814a1ca1662bad07",
        "title": "Inference-Time Alignment of Diffusion Models with Direct Noise Optimization"
      },
      {
        "paperId": "a85a882fb7d9af9a2734440f4eae19c8393f0301",
        "title": "SpeechAlign: Aligning Speech Generation to Human Preferences"
      },
      {
        "paperId": "63cba94e9d5988736f4b58a01877748fb0ee97cc",
        "title": "ByteEdit: Boost, Comply and Accelerate Generative Image Editing"
      },
      {
        "paperId": "3cfa6f1bbfbfb7a0c58b11f8142f153630a4c17b",
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data"
      },
      {
        "paperId": "f7c563c1da2301a8ef795082466d6e7aaef2f19b",
        "title": "DSPO: Direct Score Preference Optimization for Diffusion Model Alignment"
      },
      {
        "paperId": "566c0e6982a34385a8d5fbc55be8749ee532d117",
        "title": "I TERATIVE DPO WITH A N I MPROVEMENT M ODEL FOR F INE - TUNING D IFFUSION M ODELS"
      },
      {
        "paperId": "f1a7243dafe9c5e2a3c16e01d713ed93fabbfc7e",
        "title": "EMI -P OLICY P REFERENCE O PTIMIZATION FOR"
      },
      {
        "paperId": "7e6fe0c44c78aebc34bd753098bcb0c9a68c237c",
        "title": "Diffusion-RainbowPA: Improvements Integrated Preference Alignment for Diffusion-based Text-to-Image Generation"
      }
    ],
    "score": 43.0
  },
  {
    "id": "c08fa8d84104ec1a8304f75b72bed411100aaf5c",
    "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning",
    "authors": [
      "Zhao-yu Su",
      "Linjie Li",
      "Mingyang Song",
      "Yunzhuo Hao",
      "Zhengyuan Yang",
      "Jun Zhang",
      "Guanjie Chen",
      "Jiawei Gu",
      "Juntao Li",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "year": 2025,
    "citationCount": 36,
    "abstract": "While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely\"think with images\".",
    "url": "https://www.semanticscholar.org/paper/c08fa8d84104ec1a8304f75b72bed411100aaf5c",
    "pdf_url": "https://arxiv.org/pdf/2505.08617.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-13",
    "externalIds": {
      "DBLP": "journals/corr/abs-2505-08617",
      "ArXiv": "2505.08617",
      "DOI": "10.48550/arXiv.2505.08617",
      "CorpusId": 278534476
    },
    "references": [
      {
        "paperId": "7a335bc70fed77651be02223239d952fc58f080f",
        "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond"
      },
      {
        "paperId": "68e63d7210a9191cd79412af3cca5dd858ab52ca",
        "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL"
      },
      {
        "paperId": "f0e4978448c668a78af77c6dc062c940ec8c9d62",
        "title": "MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with Rule-based Reinforcement Learning"
      },
      {
        "paperId": "7ddd9dd9f4ae24f49eadab7a0bc0ae3f0279474f",
        "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding"
      },
      {
        "paperId": "a99dee9602e21a71526b9681d8dba37c55b66941",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "89409d752884b3e50ec5640466ab5857e6b3e1e2",
        "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding"
      },
      {
        "paperId": "a22b23b65cdbc0335b872ed7c7a7206c710f8997",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      },
      {
        "paperId": "82256b0680c7b683566d54541816e8817b49a44d",
        "title": "ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM"
      },
      {
        "paperId": "334d7fba900eab258cd4fbb5152539e83678b9c4",
        "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild"
      },
      {
        "paperId": "d78763acfc8aaba3d6deecb8fb1d5b829d7c3a11",
        "title": "Timo: Towards Better Temporal Reasoning for Language Models"
      },
      {
        "paperId": "2b89380b5259bdaa14f620d4791a81cc463779b8",
        "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models"
      },
      {
        "paperId": "5bb74befb93e69bc59683ac572f12e48d9cf9846",
        "title": "Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "4f2a56102bcbf0fe79379c4c27daecbccfb35a26",
        "title": "MLLM-Tool: A Multimodal Large Language Model for Tool Agent Learning"
      },
      {
        "paperId": "c672ec79f55cef8f7a32cd8dddfa981b893f1567",
        "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"
      },
      {
        "paperId": "ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d",
        "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"
      },
      {
        "paperId": "8946891e94831adc8cddb0d32311cce2445c96d2",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
      },
      {
        "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
      },
      {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning"
      },
      {
        "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
        "title": "Segment Anything"
      },
      {
        "paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0",
        "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"
      },
      {
        "paperId": "af997821231898a5f8d0fd78dad4eec526acabe5",
        "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"
      },
      {
        "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      },
      {
        "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "141a5033d9994242b18bb3b217e79582f1ee9306",
        "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"
      },
      {
        "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
        "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
      },
      {
        "paperId": null,
        "title": "Transformers: State-of-the-Art Natural Language Processing"
      },
      {
        "paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0",
        "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"
      },
      {
        "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
        "title": "VQA: Visual Question Answering"
      },
      {
        "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
        "title": "Microsoft COCO: Common Objects in Context"
      },
      {
        "paperId": "2786061c9d7e0e16621a66caf7fc305cf2c8f89a",
        "title": "Animation: can it facilitate?"
      },
      {
        "paperId": "1e90a6286b1e14b102791bbe4364395a7554c677",
        "title": "Image and Brain: The Resolution of the Imagery Debate"
      },
      {
        "paperId": "b7bdd9331ed1ecbc931ccaf50c091cd0bb8b71b7",
        "title": "Why a Diagram is (Sometimes) Worth Ten Thousand Words"
      },
      {
        "paperId": "931604f0972ca540dfaf416c3864c536875c3f22",
        "title": "TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action"
      },
      {
        "paperId": null,
        "title": "Llava-next: Improved reasoning, ocr, and world knowledge"
      },
      {
        "paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a",
        "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"
      },
      {
        "paperId": null,
        "title": "Functional significance of visuospatial representations"
      },
      {
        "paperId": "b7c95f238295715c19fac435b50dd8a094e463c2",
        "title": "Sketches of thought"
      },
      {
        "paperId": "ee577ea4b36c2d6ed120e4068a05bfb33c00e492",
        "title": "Representations in Distributed Cognitive Tasks"
      },
      {
        "paperId": null,
        "title": "Gemini: A family of highly capable multimodal models. Technical report"
      },
      {
        "paperId": null,
        "title": "Hello GPT-4o"
      },
      {
        "paperId": null,
        "title": "Minigpt-4: En-hancing vision-language understanding with advanced large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "b54261d6d2e296b5b71a3e304bd30097085a697c",
        "title": "PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents"
      },
      {
        "paperId": "f2127c5b996e2a0b90fd0b9fe0945ebafae97be5",
        "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs"
      },
      {
        "paperId": "34ef8e7f2928f922cc177881b1af6da35e11d5dd",
        "title": "DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning"
      },
      {
        "paperId": "f0301bcbdc87883911ffef88a0a970bcc601895c",
        "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models"
      },
      {
        "paperId": "d23f7b486824cecd7c816e188c645c6882e32e6d",
        "title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images"
      },
      {
        "paperId": "c75199c6dbbcc15fa7c63315fea4ba16f65e4846",
        "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting"
      },
      {
        "paperId": "ad7bda8e252b8e259a02bca915e14502f9548f5a",
        "title": "Latent Visual Reasoning"
      },
      {
        "paperId": "074c15d00e9f47a75349f6494850c8a47dbf8a4d",
        "title": "FameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "91bb20d24cfd9676f4f3743c7cdfeee00e1a49f2",
        "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators"
      },
      {
        "paperId": "caa9031830cf0ac64027d52a5d17a30c4982202f",
        "title": "Visual Programmability: A Guide for Code-as-Thought in Chart Understanding"
      },
      {
        "paperId": "9abc47fca9931599c9f1fbb0f19b04491c76e51d",
        "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems"
      },
      {
        "paperId": "9f8960098f94581749cb00845dacfcce9982ead5",
        "title": "Kwai Keye-VL 1.5 Technical Report"
      },
      {
        "paperId": "226aae3f8c246dc3df69eb856834889aad33cde8",
        "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning"
      },
      {
        "paperId": "9a5071bacb2bcb98ddd95076dbd446240b4da2c6",
        "title": "Reinforcement Learning in Vision: A Survey"
      },
      {
        "paperId": "2d19ea0cf82d2f46ab3a259f551c7ce436d6b1ea",
        "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"
      },
      {
        "paperId": "726742d2dc0ef40e172b7b7f6962a6594eec3147",
        "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent"
      },
      {
        "paperId": "cac217dee3a84a7e6a6c97b6f310d8479e92792c",
        "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning"
      },
      {
        "paperId": "365b5f8439ccd558de22c7fbb229a380c8ea423f",
        "title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback"
      },
      {
        "paperId": "ae4f998428cc2e8ce9a61d13154730da2093b33d",
        "title": "PyVision: Agentic Vision with Dynamic Tooling"
      },
      {
        "paperId": "72c749cc9ee05bee3963488a1b1e68f0d63d6015",
        "title": "Look-Back: Implicit Visual Re-focusing in MLLM Reasoning"
      },
      {
        "paperId": "51bda9429f8277d744aa8b319d0fa5b08768f61c",
        "title": "Kwai Keye-VL Technical Report"
      },
      {
        "paperId": "cc726406f7321ff2c357123fd763e19ef3e9ea7d",
        "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning"
      },
      {
        "paperId": "b36c3b79d678805b74025497bf3abc6cbe0ee1eb",
        "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens"
      },
      {
        "paperId": "91de77fa0ed26ac6e30ef62afbd65b6b850536dc",
        "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning"
      },
      {
        "paperId": "583b0075055d4857336347e8c5c1888f54ef0928",
        "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles"
      },
      {
        "paperId": "7c172167c7c1e023b19df8e25452a255fb6da56e",
        "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models"
      },
      {
        "paperId": "6147d8dfa15cfff92e4827e26da29e49f4d5ea5f",
        "title": "Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning"
      },
      {
        "paperId": "acf2f14fbf6372470d6cc3bd7a8d17b54a00939d",
        "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning"
      },
      {
        "paperId": "7319a8bfe18c701399431f15e65ceb5756c3452f",
        "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards"
      },
      {
        "paperId": "353edce7fb68da762817cdd31880108462580a25",
        "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting"
      },
      {
        "paperId": "55baf937c13205e826711eddf60b02da7dc74250",
        "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models"
      },
      {
        "paperId": "8f8c30b184da966e3824907610fbbfe6bb1dc91c",
        "title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought"
      },
      {
        "paperId": "24f9efbd9e229a085d4db2f477282f8ad80f1ffe",
        "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models"
      },
      {
        "paperId": "60dd6e831a6ce11f1fa69f859171a4ec65bf0420",
        "title": "Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints"
      },
      {
        "paperId": "26a51460caea05a9f435dffad707c5a9d30320a5",
        "title": "F RAME M IND : F RAME -I NTERLEAVED C HAIN - OF - T HOUGHT FOR V IDEO R EASONING VIA R EINFORCE - MENT L EARNING"
      }
    ],
    "score": 36.0
  },
  {
    "id": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
    "title": "Bootstrapping Language Models with DPO Implicit Rewards",
    "authors": [
      "Changyu Chen",
      "Zi-Yan Liu",
      "Chao Du",
      "Tianyu Pang",
      "Qian Liu",
      "Arunesh Sinha",
      "Pradeep Varakantham",
      "Min Lin"
    ],
    "year": 2024,
    "citationCount": 36,
    "abstract": "Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO, after training, provides an implicit reward model. In this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the LLM. Our approach is to use the rewards from a current LLM to construct a preference dataset, which is then used in subsequent DPO rounds. We incorporate two refinements to further improve our approach: 1) length-regularized reward shaping to make the preference dataset length-unbiased; 2) experience replay to enhance the quality of the preference dataset. Our approach, named self-alignment with DPO ImpliCit rEwards (DICE), shows great improvements in alignment. It achieves an increase of more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the different base models that we tried, without relying on external feedback. Our code is available at https://github.com/sail-sg/dice.",
    "url": "https://www.semanticscholar.org/paper/d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
    "pdf_url": "https://arxiv.org/pdf/2406.09760.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-06-14",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-09760",
      "ArXiv": "2406.09760",
      "DOI": "10.48550/arXiv.2406.09760",
      "CorpusId": 270521861
    },
    "references": [
      {
        "paperId": "bae7f9013fbbf8db237ae3d8feed51211b1f53a5",
        "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
      },
      {
        "paperId": "aa647ee4e050ccbb53e77444f5e7a96be9ca7ce8",
        "title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning"
      },
      {
        "paperId": "13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      },
      {
        "paperId": "af11d43d3b4e6f2d01f34feb149037cdbd05d9ee",
        "title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models"
      },
      {
        "paperId": "3fb26c0cf930b04635540e4815c4b8ca0581155c",
        "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "38333f6e8f0388968edc4b2ea7a683ce69677e69",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "128e6eb7b0c1caf02dc1f0c7246954f6bd1b84dd",
        "title": "Active Preference Learning for Large Language Models"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475",
        "title": "Self-Alignment with Instruction Backtranslation"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
      },
      {
        "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
        "title": "Large Language Models Can Self-Improve"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "d9ff7a9344dd5d6653bd7a02bfd704422bb29951",
        "title": "Experience Replay for Continual Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68",
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Iterative dpo alignment"
      },
      {
        "paperId": null,
        "title": ": An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "A simulation framework for methods"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ],
    "cited_by": [
      {
        "paperId": "e43a1d12d1b8af3a983f33319dc1c3e8b29e91ae",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey"
      },
      {
        "paperId": "3a6dc3516c20a45062564e67d742a567674865bf",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization"
      },
      {
        "paperId": "7f1cdaad0af5a80c509ebb00590f6ee9599bf882",
        "title": "MAPO: Mixed Advantage Policy Optimization"
      },
      {
        "paperId": "f32405274168cb04a9a93fd75454f10ac6ad3468",
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models"
      },
      {
        "paperId": "4cb1ce04a44ac85ede834bb7eca4b67565f5da6f",
        "title": "Principled Foundations for Preference Optimization"
      },
      {
        "paperId": "14b0064fd15c42e1691c0ee93ca61cfa071bdbeb",
        "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards"
      },
      {
        "paperId": "05fbfb24dacd83cb910f925bbc6e6c33451222d5",
        "title": "Personalized LLM Decoding via Contrasting Personal Preference"
      },
      {
        "paperId": "372c106a2c806eb28a58dd18ccfcb403835d9331",
        "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model"
      },
      {
        "paperId": "24d9d00b91f99dde3c9a0ea5c79e63f2ed26151c",
        "title": "Reinforcing General Reasoning without Verifiers"
      },
      {
        "paperId": "1617feeff3f5cc67af103542e01a4ba526957ab3",
        "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward"
      },
      {
        "paperId": "1bc8c111ff5f21a10061cc1654f77de2b406be72",
        "title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective"
      },
      {
        "paperId": "836d035ff15ae4dfa500f07fc119cdc4709cc212",
        "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation"
      },
      {
        "paperId": "eaf12ceb99411878ffdc339998803f98a6dfca73",
        "title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      },
      {
        "paperId": "3dd6e3dc2d7a8fa6aaa4671557e6a94e62aa1106",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data"
      },
      {
        "paperId": "eee206d70a225d5726a0421263717049727253f6",
        "title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs"
      },
      {
        "paperId": "7726d40d59af31de2a88d265d3eb8ee5be5f74c1",
        "title": "Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL"
      },
      {
        "paperId": "9a0cd90d265af436605216cf4e1a41c8cfff0ef0",
        "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training"
      },
      {
        "paperId": "b15289e2ed831091239182fab0371a7f4a5d466e",
        "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning"
      },
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "a3479f982c670563e99899b240376e78b7d43d64",
        "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs"
      },
      {
        "paperId": "ae2fa2af2da2c3e96e87af00887b4938e48be79c",
        "title": "Preference Optimization for Reasoning with Pseudo Feedback"
      },
      {
        "paperId": "6e220dff22a40acb83c5af920fe3ad5d71e3cc83",
        "title": "Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of Human Annotation in Preference Tuning"
      },
      {
        "paperId": "8430ea2921172c7c07e5286f19ffdf4a43d1d4b0",
        "title": "Sample-Efficient Alignment for LLMs"
      },
      {
        "paperId": "9eb6cd5a12b37b717061f0060987492e82fecba6",
        "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates"
      },
      {
        "paperId": "e80f3559bbf308a989022a9bd9a266667a0f07d2",
        "title": "Post-hoc Reward Calibration: A Case Study on Length Bias"
      },
      {
        "paperId": "bae7f9013fbbf8db237ae3d8feed51211b1f53a5",
        "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
      },
      {
        "paperId": "5104bc5c9e3f4a733b4e8155cb4a59a1a7a4e20b",
        "title": "Selective Preference Optimization via Token-Level Reward Function Estimation"
      },
      {
        "paperId": "aa647ee4e050ccbb53e77444f5e7a96be9ca7ce8",
        "title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning"
      },
      {
        "paperId": "13c2d7d6f79e9cc05b96e5c7bf420d0dfac42eaa",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      },
      {
        "paperId": "3fb26c0cf930b04635540e4815c4b8ca0581155c",
        "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs"
      },
      {
        "paperId": "a264dbba6c16a460998cd16e4f48d62a1eddd4d0",
        "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment"
      },
      {
        "paperId": "3ebe7c1a9f8a12cdfaf863069a253c2c31940ac6",
        "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data"
      },
      {
        "paperId": "bf54127b8fea0fea43b437db9f38122268f0c199",
        "title": "SeRA: Self-Reviewing and Alignment of LLMs using Implicit Reward Margins"
      },
      {
        "paperId": "8d95153be4f21f12575040f69de455bdcca6b035",
        "title": "CyberGuardian 2: Integrating LLMs and Agentic AI Assistants for Securing Distributed Networks"
      },
      {
        "paperId": "fac493a6e87a9e38ce441fa47fed6c2a51d8e42f",
        "title": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference Annotation"
      }
    ],
    "score": 36.0
  },
  {
    "id": "33c445469aa9688837b0f76a2e55bcabe29dce47",
    "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
    "authors": [
      "Guangsheng Bao",
      "Hongbo Zhang",
      "Linyi Yang",
      "Cunxiang Wang",
      "Yue Zhang"
    ],
    "year": 2024,
    "citationCount": 28,
    "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",
    "url": "https://www.semanticscholar.org/paper/33c445469aa9688837b0f76a2e55bcabe29dce47",
    "pdf_url": "https://arxiv.org/pdf/2402.16048.pdf",
    "venue": "International Conference on Computational Linguistics",
    "publicationDate": "2024-02-25",
    "externalIds": {
      "DBLP": "conf/coling/BaoZWY025",
      "ArXiv": "2402.16048",
      "CorpusId": 267938344
    },
    "references": [
      {
        "paperId": "6baa65650913b17549700bd346bede444df4f514",
        "title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs"
      },
      {
        "paperId": "472b3dedc4eec85c53b53becee06d953f29afcc8",
        "title": "General Purpose Verification for Chain of Thought Prompting"
      },
      {
        "paperId": "397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
        "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language Models"
      },
      {
        "paperId": "5f61ccb715f4ad9bef53794702e481e1a99f728b",
        "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy"
      },
      {
        "paperId": "a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
        "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"
      },
      {
        "paperId": "40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08",
        "title": "The Impact of Reasoning Step Length on Large Language Models"
      },
      {
        "paperId": "743ef29a9406c44c835684c7755d423d6ca0b663",
        "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning"
      },
      {
        "paperId": "f42f61a547c5996be6aee175145b0d74e6324dff",
        "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "827afa7dd36e4afbb1a49c735bfbb2c69749756e",
        "title": "Measuring Faithfulness in Chain-of-Thought Reasoning"
      },
      {
        "paperId": "0a94fbb5e1c93513523f00e75d672ef4553861f9",
        "title": "Can Large Language Models Infer Causation from Correlation?"
      },
      {
        "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"
      },
      {
        "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"
      },
      {
        "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
        "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality"
      },
      {
        "paperId": "85cc48276c69924d3e92ddb38facb7d92be9a4a6",
        "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4"
      },
      {
        "paperId": "2a7ae3e98357569c41424dacd60c62d3df78a0db",
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "130d432ccbc836380a212bea618f84ff094a6a52",
        "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"
      },
      {
        "paperId": "6aecc93c2d61da073b70dec19795172ca1ff3405",
        "title": "Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests"
      },
      {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      },
      {
        "paperId": "506e400330356db96ac68685e473f3b6ace370a3",
        "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models"
      },
      {
        "paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
        "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"
      },
      {
        "paperId": "57835c5ad5424f94ee75901c3113730f3900e656",
        "title": "Representation Learning via Invariant Causal Mechanisms"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "f6e6c948a2074e38e0a4e9099c0f63773c6013dd",
        "title": "Causality"
      },
      {
        "paperId": "46f6a90fcf0ecc4b60470a1f35cd95d65d5f8d9b",
        "title": "Elements of Causal Inference: Foundations and Learning Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "538c4e4327c706a9346472661b3256c39b07df08",
        "title": "Causality in thought."
      },
      {
        "paperId": "7062f0be4e6ae4bbb96ba0dd0d726763dde875c6",
        "title": "Causal Inference"
      },
      {
        "paperId": "5c4e2ea8b4e5609b1acdc57337fef67f75303cda",
        "title": "Mechanical reasoning by mental simulation"
      },
      {
        "paperId": "6af69a998ff9fb72c58123b262e769b0a32e932c",
        "title": "Naive theories and causal deduction"
      },
      {
        "paperId": "6ed0d74a74850568d8d9a92f49c2e90317dc722d",
        "title": "Identification and Estimation of Local Average Treatment Effects"
      },
      {
        "paperId": "545122e2990590524459ec9b59ccac6ce71e3b6a",
        "title": "Estimating causal effects of treatments in randomized and nonrandomized studies."
      },
      {
        "paperId": "cdb48a96036b8cd2367eea596cff2db828305150",
        "title": "Note on the sampling error of the difference between correlated proportions or percentages"
      },
      {
        "paperId": "58ba42ffc34dd24d6a77fe58c8973b3533c369cd",
        "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views"
      },
      {
        "paperId": null,
        "title": "Ex-ploring the efficacy of automatically generated coun-terfactuals for sentiment analysis"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": null,
        "title": "Atomic: An atlas of machine commonsense for if-then reasoning"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": "4437a6b4160f9ea02a29e34eb5ed41c0185e4506",
        "title": "Causal Analysis"
      },
      {
        "paperId": "a75bbeeb59e9272c18762c5d0c1cbc6e211f2ec7",
        "title": "Causal reasoning through intervention"
      },
      {
        "paperId": null,
        "title": "2023. GPT-4 Technical Report"
      },
      {
        "paperId": null,
        "title": "2022. Folio: Natural language reasoning with first-order logic"
      },
      {
        "paperId": null,
        "title": "2023. A survey of chain"
      },
      {
        "paperId": null,
        "title": "A Prompts and Templates We use general prompts and templates for experiments"
      },
      {
        "paperId": null,
        "title": "experiments with Direct answering and zero-shot CoT . To facilitate answer matching"
      },
      {
        "paperId": null,
        "title": "2023a. Logiqa 2.0\u2014an improved dataset for logical reasoning in natural language understanding"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "2023. Alignment for honesty"
      },
      {
        "paperId": null,
        "title": "2023a. Tree of thoughts: Deliberate problem solving with large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "b65bb209213740f1df73e731fcdf021e96cb0044",
        "title": "Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors"
      },
      {
        "paperId": "80d5c1e2ff4a804a586a5526f6262efa5703fb4f",
        "title": "Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference"
      },
      {
        "paperId": "a330dc0db965b899d1a1c04160b59939dde3bec3",
        "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process"
      },
      {
        "paperId": "ad1905eea0ad5acdb15f36e92195efb904cfe8b3",
        "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction"
      },
      {
        "paperId": "2429de2c3f2b7ddcdf1f8b2d34c6eb8b75cc47f9",
        "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models"
      },
      {
        "paperId": "f33ba21e7e699fbc772b1c0b537986342f214ccd",
        "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?"
      },
      {
        "paperId": "e972a2f80601098749d81f1c33ff4f99b01df332",
        "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning"
      },
      {
        "paperId": "e229048fea2a4ec4dbe831ab72f4a66615765fa0",
        "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis"
      },
      {
        "paperId": "c4d8ecdcd7bf884cb51d0cee0d571d88ee0f9530",
        "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?"
      },
      {
        "paperId": "593a8358e3e6bd41d05f115aa4b248b89cd28d07",
        "title": "Unveiling Confirmation Bias in Chain-of-Thought Reasoning"
      },
      {
        "paperId": "220b2849a5f6fc8db44e7f22334c50854e0fb687",
        "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion"
      },
      {
        "paperId": "eaa72a7f8fe164cc28f763d5bcca7074f3d8e9a5",
        "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts"
      },
      {
        "paperId": "e5620820071ef7a8c2bc38e0a0afec8b97a56dd4",
        "title": "Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation"
      },
      {
        "paperId": "aaaa04358e03fe5ef0d24a86e75fe6c1ee0f3ca7",
        "title": "Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps"
      },
      {
        "paperId": "2f147f41fb9cd03a8fa217c9ad6156a112890169",
        "title": "Protecting Human Cognition in the Age of AI"
      },
      {
        "paperId": "a49afd3e15f8fc504b84f0c11497ef0eedee4e7c",
        "title": "Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation"
      },
      {
        "paperId": "d37fbe3e51679c0c5c034f7ee9ae83fd606225aa",
        "title": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner"
      },
      {
        "paperId": "39bef6bc1f298eb0a9a46f7a9ce108eda27170e6",
        "title": "CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs"
      },
      {
        "paperId": "bf0479119679fb627883bffe6087722df5ba4e5b",
        "title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models"
      },
      {
        "paperId": "9ed599a0759fb7f948e165560f8552e3fe47b543",
        "title": "Watch Your Steps: Observable and Modular Chains of Thought"
      },
      {
        "paperId": "db366b752189fdb852af4e4b80ef961b2c4b9f16",
        "title": "An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation"
      },
      {
        "paperId": "8f7100fd747494f4b184d6c4fce25d6dfc3d8018",
        "title": "Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships"
      },
      {
        "paperId": "92a236a1ca049e186ca53c9ce12219ebca315ba1",
        "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment"
      },
      {
        "paperId": "d553222a22a419170f197d1adfbdd3763c0260ad",
        "title": "Six Fallacies in Substituting Large Language Models for Human Participants"
      },
      {
        "paperId": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
        "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models"
      },
      {
        "paperId": "0270631fac86f27cdbc1e1b5105bab9c7c1a1723",
        "title": "Chain of Thoughtlessness: An Analysis of CoT in Planning"
      },
      {
        "paperId": "af7d872f5bdd7e830835dd76c417399df4229886",
        "title": "Analysis of LLM\u2019s \u201cSpurious\u201d Correct Answers Using Evidence Information of Multi-hop QA Datasets"
      },
      {
        "paperId": "066aebc008f14e838c8991e373fd3e8baaa68c4d",
        "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images"
      }
    ],
    "score": 28.0
  },
  {
    "id": "4ed96712afa0d0e82cddb3d669d4e9f60195aecb",
    "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
    "authors": [
      "Liang-bo Ning",
      "Shijie Wang",
      "Wenqi Fan",
      "Qing Li",
      "Xin Xu",
      "Hao Chen",
      "Feiran Huang"
    ],
    "year": 2024,
    "citationCount": 27,
    "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.",
    "url": "https://www.semanticscholar.org/paper/4ed96712afa0d0e82cddb3d669d4e9f60195aecb",
    "pdf_url": "https://arxiv.org/pdf/2504.13192.pdf",
    "venue": "Knowledge Discovery and Data Mining",
    "publicationDate": "2024-08-24",
    "externalIds": {
      "DBLP": "conf/kdd/NingWFLXCH24",
      "ArXiv": "2504.13192",
      "DOI": "10.1145/3637528.3671837",
      "CorpusId": 271961798
    },
    "references": [
      {
        "paperId": "1230982be8d2dfbb6e86e5f9afc75967a8597bd0",
        "title": "HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text"
      },
      {
        "paperId": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
        "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack"
      },
      {
        "paperId": "28c6ac721f54544162865f41c5692e70d61bccab",
        "title": "A Survey on Large Language Model based Autonomous Agents"
      },
      {
        "paperId": "4b896ff3b8ef9cc3f23f5f0307bcab4a81696504",
        "title": "Deep-Learning-Based Drug Recommendation and ADR Detection Healthcare Model on Social Media"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "7c5aa120a582bd192b2be4952953040b41d3d503",
        "title": "Certified Robustness for Large Language Models with Self-Denoising"
      },
      {
        "paperId": "a35f1315e91513ff0bec0c488fe175214fd9636c",
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "073e4f0c3a66b7557abd053301b5104cdc582636",
        "title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective"
      },
      {
        "paperId": "bac54736112098616f0e1c90435888ef3e119d32",
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"
      },
      {
        "paperId": "5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb",
        "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor"
      },
      {
        "paperId": "9eff28f4c311056310da9aaaaffe4d377e7ef3e6",
        "title": "Attacking Pre-trained Recommendation"
      },
      {
        "paperId": "3487c12512fa41d3a4d64f00cb842525a8590ad3",
        "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "0cf694b8f85ab2e11d45595de211a15cfbadcd22",
        "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"
      },
      {
        "paperId": "d20773683ddbb5c516131bdfdf674a772714de50",
        "title": "A Comprehensive Survey on Trustworthy Recommender Systems"
      },
      {
        "paperId": "59b3fbf146b29b581b677ec4384f14cee87997a4",
        "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies"
      },
      {
        "paperId": "366627dd195a01c197b82bf255a26d5d7aa3012d",
        "title": "Knowledge-enhanced Black-box Attacks for Recommendations"
      },
      {
        "paperId": "df602516e28a9ef0ef665ed0aef551984d8d770d",
        "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)"
      },
      {
        "paperId": "ced8d7253c00b07b836f76c30ea496fd880b8f65",
        "title": "Graph Trend Filtering Networks for Recommendation"
      },
      {
        "paperId": "a867894db8f9d544a471e86d8844008861f6a2ec",
        "title": "Personalized News Recommendation: Methods and Challenges"
      },
      {
        "paperId": "6165ecf579133bfb027127bb9631134e85818c28",
        "title": "Attacking Black-box Recommendations via Copying Cross-domain User Profiles"
      },
      {
        "paperId": "06a427e1688f92053a38c73cb4e0da25177c89e7",
        "title": "BAE: BERT-based Adversarial Examples for Text Classification"
      },
      {
        "paperId": "28f1c998f3f330decb0814a75c1c7975d719b75f",
        "title": "PoisonRec: An Adaptive Data Poisoning Framework for Attacking Black-box Recommender Systems"
      },
      {
        "paperId": "3024f58826a5bce3378af94f677e8fb90cbb49e0",
        "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "2794565125ee47791939038732b31f9c848f9dd4",
        "title": "Adversarial attacks on an oblivious recommender"
      },
      {
        "paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
        "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"
      },
      {
        "paperId": "f3d1910cd02e840ffd23651df095a55e6720307b",
        "title": "Deep social collaborative filtering"
      },
      {
        "paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
        "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
      },
      {
        "paperId": "398d6f4432e6aa7acf21c0bbaaebac48998faad3",
        "title": "Graph Neural Networks for Social Recommendation"
      },
      {
        "paperId": "97faeefa771e8cc8e55159e2bd03e6f5eef249a8",
        "title": "Self-Attentive Sequential Recommendation"
      },
      {
        "paperId": "fa12574c228542151ccd7d4e3f42cc4896cd274a",
        "title": "Black-Box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers"
      },
      {
        "paperId": "39667784085b83191b58c361d60d518a9e9f373d",
        "title": "Learning Tree-based Deep Model for Recommender Systems"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "ad42c33c299ef1c53dfd4697e3f7f98ed0ca31dd",
        "title": "Neural Collaborative Filtering"
      },
      {
        "paperId": "50095b6574abed68872e9e9abfd0fb879fcba438",
        "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering"
      },
      {
        "paperId": "dade3d81749385e08ec0f163c806308c94b56f32",
        "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity"
      },
      {
        "paperId": "276ebc620a8976026bd2d03582b9ecfa3738d43c",
        "title": "The MovieLens Datasets: History and Context"
      },
      {
        "paperId": "2ff694e20f492a7acf7fd0646c5e1576f0b3c901",
        "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding"
      },
      {
        "paperId": "259d07f47c046389d0b4f256139de64736dd9a94",
        "title": "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots"
      },
      {
        "paperId": "67741cd1a00f53506952ee9b82d231b45a8f384b",
        "title": "Untargeted Black-box Attacks for Social Recommendations"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "32a28465b4fdbc86bf0ef1438ba465cad33f2990",
        "title": "Profile Injection Attack Detection for Securing Collaborative Recommender Systems 1"
      },
      {
        "paperId": "1ffdf57d39879db76e50384d408ab0966b77d13a",
        "title": "Limited Knowledge Shilling Attacks in Collaborative Filtering Systems"
      },
      {
        "paperId": null,
        "title": "2023. A survey of large language models"
      },
      {
        "paperId": null,
        "title": "2022. Automatic Chain of Thought Prompting in Large Language Models"
      },
      {
        "paperId": null,
        "title": "KDD \u201924, August 25\u201329, 2024, Barcelona, Spain"
      },
      {
        "paperId": null,
        "title": "2023. Open Sesame! Universal Black Box Jailbreaking of Large Language Models"
      },
      {
        "paperId": null,
        "title": "2023. Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models"
      }
    ],
    "cited_by": [
      {
        "paperId": "1d99e4c0e5e3d6a220f948cef37cab37d687c28a",
        "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection"
      },
      {
        "paperId": "a37c0e929bf8a9577f118cc34cb2233c8b9c06e9",
        "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"
      },
      {
        "paperId": "101deb445383031096fb8f20ad818c50d9b97e2d",
        "title": "mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering"
      },
      {
        "paperId": "13d82891acc0cf37f05a654905d16848d31000a5",
        "title": "Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective"
      },
      {
        "paperId": "4f3b0391e4d2a8a2626edd8c4888e44ba6eb940e",
        "title": "Multi-Objective Recommendation in the Era of Generative AI: A Survey of Recent Progress and Future Prospects"
      },
      {
        "paperId": "2b574c8e3a97b77529b8ec56579c91dd55ada850",
        "title": "Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives"
      },
      {
        "paperId": "ef65bc91d689a7419b493f91989f101a5fdab62e",
        "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol (MCP) Ecosystem"
      },
      {
        "paperId": "0340e56133173dd60135e4d20bab1c3c8f84f25d",
        "title": "Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks"
      },
      {
        "paperId": "94b5da692c23d9528480911dab19ea191198e9ec",
        "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations"
      },
      {
        "paperId": "403bd2292154cf84bfaebe440ebd642b623839f1",
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization"
      },
      {
        "paperId": "b4c36c67677fb21e771cb6983804ed6b22e88df9",
        "title": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization"
      },
      {
        "paperId": "5b2499d60f97037705ddfb46d7df6f40bbf94bc9",
        "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation"
      },
      {
        "paperId": "86113965b1a26920511b4528a0314baa86b23e4f",
        "title": "DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents"
      },
      {
        "paperId": "ee88a623365270dc72f906d85c371d3084db7f3d",
        "title": "A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models"
      },
      {
        "paperId": "56fc00881e8d854f3cdecb2d072c369ac818c3ef",
        "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges"
      },
      {
        "paperId": "5cf96d5caa3b08a91a8de00aff3bb45e85dcd0a5",
        "title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval"
      },
      {
        "paperId": "4da28c66713a5038114911b387b549b7ce9e0090",
        "title": "A Survey on LLM-powered Agents for Recommender Systems"
      },
      {
        "paperId": "574dfb5c08f0d6a0ca1adf4b9c0bfea7b8f48695",
        "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks"
      },
      {
        "paperId": "461e00ecb0bb5b24b4cb65078a96d0677919f5f3",
        "title": "Research Directions in Software Supply Chain Security"
      },
      {
        "paperId": "8ba843a693682ece3fd0483fe01479982dfe4ac1",
        "title": "Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions"
      },
      {
        "paperId": "b3e650909e4b477609e8d5ebbca4bb93978d0a1f",
        "title": "Graph Cross-Correlated Network for Recommendation"
      },
      {
        "paperId": "0b708c14a822691047238cfc50dc15312ee8cef1",
        "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents"
      },
      {
        "paperId": "e955fc889f86f82864d0aad6b8429ff79136d1f3",
        "title": "A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security"
      },
      {
        "paperId": "993159b86a19ad1310423f35918adfab30b6c10e",
        "title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends"
      },
      {
        "paperId": "250d0521a774da4db33f7b81e78d8d34592ce6cf",
        "title": "TokenRec: Learning to Tokenize ID for LLM-Based Generative Recommendations"
      },
      {
        "paperId": "647a42d78e5bd405014a8c878e37fb7b2da8eaa6",
        "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)"
      },
      {
        "paperId": "c8e734f0ce83464ecb8564d25287abac6e163af8",
        "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem"
      }
    ],
    "score": 27.0
  },
  {
    "id": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
    "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
    "authors": [
      "Keming Lu",
      "Bowen Yu",
      "Fei Huang",
      "Yang Fan",
      "Runji Lin",
      "Chang Zhou"
    ],
    "year": 2024,
    "citationCount": 23,
    "abstract": "Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer. Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.",
    "url": "https://www.semanticscholar.org/paper/9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
    "pdf_url": "https://arxiv.org/pdf/2405.17931.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-28",
    "externalIds": {
      "ArXiv": "2405.17931",
      "DBLP": "journals/corr/abs-2405-17931",
      "DOI": "10.48550/arXiv.2405.17931",
      "CorpusId": 270067818
    },
    "references": [
      {
        "paperId": "df8c3a325419d63366b9b347739fcbf3e2c4d22c",
        "title": "Self-Play Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "3ec648362481eaa44e439fa0955533da390cacfd",
        "title": "Model Extrapolation Expedites Alignment"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
      },
      {
        "paperId": "ed81c644692ec88f59e287366c8cba619948295a",
        "title": "Direct Preference Optimization with an Offset"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "e5a2c0759027c946df23cce37a8a3104d5c48841",
        "title": "Merging by Matching Models in Task Parameter Subspaces"
      },
      {
        "paperId": "15b8b6a8028b2b6e75b67dfb6aebaede36826cf8",
        "title": "Language Model Alignment with Elastic Reset"
      },
      {
        "paperId": "91e8ce403704a38bee8a5df90d99979a796d1741",
        "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning"
      },
      {
        "paperId": "c0230760f644f6b7538d93e4296a5e9aa7028e45",
        "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54",
        "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "e7896ed943daa78f6807bfc6ce1baf03fdf31e43",
        "title": "Spurious Feature Diversification Improves Out-of-distribution Generalization"
      },
      {
        "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
        "title": "Mitigating the Alignment Tax of RLHF"
      },
      {
        "paperId": "dd3fb89d1201d46fa80b6a9519114599c01c11ac",
        "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "2651f0179874bd010f58d2c9fa7d118807c80977",
        "title": "TIES-Merging: Resolving Interference When Merging Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "99bd07e888476904c6dd77ca154fd48629ac6dce",
        "title": "How well do Large Language Models perform in Arithmetic tasks?"
      },
      {
        "paperId": "be8a5395cd4ffdeb3ffd4c90c91b5978b1d69a98",
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation"
      },
      {
        "paperId": "8a4fc5f00cd4aca61e148e46a2125c3a406719f1",
        "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "06b20a1c6883464fcb2855adc146874fe7937c41",
        "title": "Merging Models with Fisher-Weighted Averaging"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "4dc48bd4e1c0e5986b36eca8339bd45e944d8a82",
        "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks"
      },
      {
        "paperId": "f45261b7b53043c316f45f613cb735907b93fb5a",
        "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "520bd2331cca8d5a9c032c186a2a0f7704ead6ff",
        "title": "R-Drop: Regularized Dropout for Neural Networks"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "b88c11922cac84e5ea902f82d27ae21c3dda2e04",
        "title": "Better Fine-Tuning by Reducing Representational Collapse"
      },
      {
        "paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e",
        "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"
      },
      {
        "paperId": "ab70853cd5912c470f6ff95e95481980f0a2a41b",
        "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization"
      },
      {
        "paperId": "222b9a7b8038120671a1610e857d3edbc7ac5550",
        "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"
      },
      {
        "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
      },
      {
        "paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094",
        "title": "CoQA: A Conversational Question Answering Challenge"
      },
      {
        "paperId": "f6195d8dc6aad8231e97b563246f2585842bc68b",
        "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"
      },
      {
        "paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a",
        "title": "Fixing Weight Decay Regularization in Adam"
      },
      {
        "paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc",
        "title": "Deep Neural Solver for Math Word Problems"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "1703631a938b397ba7e858161ce16448f6046d6f",
        "title": "iCaRL: Incremental Classifier and Representation Learning"
      },
      {
        "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
        "title": "Learning without Forgetting"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "0407b605b8f55db72e2545586bfe8e946b691b70",
        "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks"
      },
      {
        "paperId": "c582b1e0189d6dd6682c5450d47fd8d3ff4c096b",
        "title": "The Exponentially Weighted Moving Average"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": null,
        "title": "2024. Codeapex: A bilingual programming evaluation benchmark for large language models"
      },
      {
        "paperId": null,
        "title": "2022. Rose: Robust selective fine-tuning for pre-trained language models"
      },
      {
        "paperId": null,
        "title": "2022b. Constitutional ai: Harmless-ness from ai feedback"
      },
      {
        "paperId": null,
        "title": "2024. A comprehensive survey of continual learning: Theory, method and application"
      }
    ],
    "cited_by": [
      {
        "paperId": "55f171033136c063a97174bbcd2979cb4298761c",
        "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization"
      },
      {
        "paperId": "abf0a71de6c00aea97211d8c2496687cf495a293",
        "title": "MOSLIM:Align with diverse preferences in prompts through reward classification"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "9582f41d7a443f1f23fd4cc8a93650a44dfc2ce3",
        "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging"
      },
      {
        "paperId": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
        "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging"
      },
      {
        "paperId": "a2a5ea730b7d0ef7653060595a021360be4ad57f",
        "title": "LLM Safety Alignment is Divergence Estimation in Disguise"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "359eb1ab4ad2368203695a3e86daf0cc80f43d62",
        "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision"
      },
      {
        "paperId": "ca47e76a16ec212674c6c57db37735b680a853e8",
        "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications"
      },
      {
        "paperId": "9ca3d87bda7813ef29c1142fb6f49c631eac5c1b",
        "title": "Superficial Safety Alignment Hypothesis"
      },
      {
        "paperId": "0e3ffaf73eaf0e5579b0418118b1e7ffb1b91c92",
        "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization"
      },
      {
        "paperId": "0c1c6228768f4405a705af4fb4c6394e00ab97a8",
        "title": "From Lists to Emojis: How Format Bias Affects Model Alignment"
      },
      {
        "paperId": "0c40ebee64178279d96fb3bbac549c775706574d",
        "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model"
      },
      {
        "paperId": "1a638e5752e386612406d0479b7bad94877be8cb",
        "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities"
      },
      {
        "paperId": "d95220be54d155f8be7f36b4fde794711cbb69b3",
        "title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts"
      },
      {
        "paperId": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
        "title": "Qwen2 Technical Report"
      },
      {
        "paperId": "6013c87aa7d6fa6bbe40764df2a59aaa876cddf8",
        "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models"
      },
      {
        "paperId": "e9b156ddf40f660dfb6a9b2a720d0b0735414dc0",
        "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization"
      },
      {
        "paperId": "92fc70fd701835c5fb5c548d9a04596a5f6b1b12",
        "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models"
      },
      {
        "paperId": "7be0c002db797445ba4201ef56a32b51a203924f",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence"
      },
      {
        "paperId": "48c728bc4c61f931d8af84b90c58af4e5eff62ad",
        "title": "Merging Improves Self-Critique Against Jailbreak Attacks"
      },
      {
        "paperId": "75f314a98e30c8eb93618ecbb0a488a9a766bcd2",
        "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing"
      },
      {
        "paperId": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
        "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models"
      }
    ],
    "score": 23.0
  },
  {
    "id": "9123ec44f0026e70f8398b904e97a4224866bb36",
    "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models",
    "authors": [
      "Wenxuan Zhang",
      "Philip H. S. Torr",
      "Mohamed Elhoseiny",
      "Adel Bibi"
    ],
    "year": 2024,
    "citationCount": 18,
    "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In supervised optimization, a labeling function is used to capture the global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark that includes comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO achieves the same level of safety as methods that heavily rely on human labor with less than 10\\% of the computational resources and human prompting and annotation process. The training recipes can be found here: https://github.com/wx-zhang/bfpo.",
    "url": "https://www.semanticscholar.org/paper/9123ec44f0026e70f8398b904e97a4224866bb36",
    "pdf_url": "https://arxiv.org/pdf/2408.15313.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-08-27",
    "externalIds": {
      "DBLP": "conf/iclr/00010EB25",
      "ArXiv": "2408.15313",
      "DOI": "10.48550/arXiv.2408.15313",
      "CorpusId": 271974608
    },
    "references": [
      {
        "paperId": "131a13c60f179511572abc81d6bd6aa988e96854",
        "title": "Rule Based Rewards for Language Model Safety"
      },
      {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      },
      {
        "paperId": "51205d5ab42c98ba286ee88147c6e17c6074995a",
        "title": "Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations"
      },
      {
        "paperId": "eb375712bd37250c350ecd3f559e1879e87eb3e5",
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"
      },
      {
        "paperId": "dfc9bb24627d1dd61c8d495cd86a874a2a1130ad",
        "title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "paperId": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
        "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences"
      },
      {
        "paperId": "57b28b74e62cdf1b5d091d8a1c1397e8caef1576",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "paperId": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99",
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8",
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"
      },
      {
        "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "title": "Mistral 7B"
      },
      {
        "paperId": "e6776f5f293c18f4b2322b1479f083cb24d33343",
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"
      },
      {
        "paperId": "59207e9d0cd4129b6ed665205105192dd3032ff3",
        "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
      },
      {
        "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "paperId": "e3abb313326ab2937d1a73dc4a45877d21af0075",
        "title": "Overcoming Generic Knowledge Loss with Selective Parameter Update"
      },
      {
        "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "92930ed3560ea6c86d53cf52158bc793b089054d",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      },
      {
        "paperId": "a6d3794c23626060781da0f1ff2bcdf7457b6c43",
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"
      },
      {
        "paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a",
        "title": "On the Planning Abilities of Large Language Models - A Critical Investigation"
      },
      {
        "paperId": "a122863d239643453195424c04067e89406246e1",
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
        "title": "Large language models encode clinical knowledge"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20",
        "title": "Wordcraft: Story Writing With Large Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "title": "Ethical and social risks of harm from Language Models"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
        "title": "BBQ: A hand-built bias benchmark for question answering"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb",
        "title": "Scaling Laws for Transfer"
      },
      {
        "paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6",
        "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "090648f65354977762a4624559e7f7b52ae17f58",
        "title": "Scruples: A Corpus of Community Ethical Judgments on 32, 000 Real-Life Anecdotes"
      },
      {
        "paperId": "65906e6027246ae9e4ecd18d6e019a24505c842e",
        "title": "Aligning AI With Shared Human Values"
      },
      {
        "paperId": "853a5a6950827c3000b421702ae986ccef364ff6",
        "title": "Continual Learning with Tiny Episodic Memories"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "0b14178e7d79ac426d0a39700e1ac8b2c6f2e752",
        "title": "Convex optimization"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "da8a8bd9c440b141c02f2f1f09fa670536c4950a",
        "title": "Killing, letting die, and the trolley problem."
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "e6ae04a0dc7ef590ca93480722be8d797fabac7f",
        "title": "Convex"
      },
      {
        "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "title": "An Adversarial Winograd Schema Challenge at Scale"
      },
      {
        "paperId": null,
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Llm-blender: Ensembling large language models with pairwise comparison and generative fusion"
      },
      {
        "paperId": null,
        "title": "Toxigen: A large-scale machine-generated dataset for implicit and adversarial hate speech detection"
      },
      {
        "paperId": null,
        "title": "A framework for few-shot language model"
      }
    ],
    "cited_by": [
      {
        "paperId": "dac3786fcf23a447babf1f20aa5b257792dbb3a4",
        "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users"
      },
      {
        "paperId": "ac9a7ebd4187ba0a280428e044b60cd71701f418",
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models"
      },
      {
        "paperId": "416908dee133533bc8a08f7507718c5697190467",
        "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints"
      },
      {
        "paperId": "59414c4281ca7fdf1651905a943ae7c4377f9f03",
        "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment"
      },
      {
        "paperId": "bd50ee192e60d53afd2a031162200ba4da4df81c",
        "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models"
      },
      {
        "paperId": "123703f3638ab383b0e7f5d7cb1191524dea16d8",
        "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study"
      },
      {
        "paperId": "905c9e7afc4ac89ef164ca4c7171331c6719c4fa",
        "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "0ec6a9659d50bfdb32e75e1b9c85372f67bde349",
        "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment"
      },
      {
        "paperId": "aece81d7dcbf2929e650a6094af63666e95a0c83",
        "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models"
      },
      {
        "paperId": "fd576581854bad8e91c83c4019de82ee5e44637c",
        "title": "Multi-Attribute Steering of Language Models via Targeted Intervention"
      },
      {
        "paperId": "b83d737b98a43fcb4c467036d720a6d7adcc313b",
        "title": "Position: We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles"
      },
      {
        "paperId": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
        "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation"
      },
      {
        "paperId": "0ea5ac5c71fdce64e6735c1dedb95e3471f0e3bd",
        "title": "ASTRAL: Automated Safety Testing of Large Language Models"
      },
      {
        "paperId": "18a37eb07951c79c3290a4be400a938383a8d31c",
        "title": "Targeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation"
      },
      {
        "paperId": "43bb364bfad4d1adf8e7abb78d9a19608ae1181f",
        "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security"
      },
      {
        "paperId": "d10c90ffe0e42acc63e6034d3dcf18bb31d1107f",
        "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options"
      },
      {
        "paperId": "734c69eb1c35d2ea61481b3abca6474e34b8456a",
        "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options"
      }
    ],
    "score": 18.0
  },
  {
    "id": "3d43594804af065c89d4f5be5d0a17957b633092",
    "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation",
    "authors": [
      "Xiaoying Zhang",
      "Jean-Fran\u00e7ois Ton",
      "Wei Shen",
      "Hongning Wang",
      "Yang Liu"
    ],
    "year": 2024,
    "citationCount": 18,
    "abstract": "We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation.",
    "url": "https://www.semanticscholar.org/paper/3d43594804af065c89d4f5be5d0a17957b633092",
    "pdf_url": "https://arxiv.org/pdf/2403.05171.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-08",
    "externalIds": {
      "DBLP": "journals/corr/abs-2403-05171",
      "ArXiv": "2403.05171",
      "DOI": "10.48550/arXiv.2403.05171",
      "CorpusId": 268296690
    },
    "references": [
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "7da928c74b26953860331404ceaa23ed25e2ea5f",
        "title": "Policy Optimization in RLHF: The Impact of Out-of-preference Data"
      },
      {
        "paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461",
        "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"
      },
      {
        "paperId": "84259db14b725853ecfe425fe85ca375b32983c2",
        "title": "Adversarial Preference Optimization"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "2d14697232f03661cb86246df46e52816694a97f",
        "title": "Towards Last-layer Retraining for Group Robustness with Fewer Annotations"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "2fe24fa62c5d57c5bd4c93b25740d0779530987f",
        "title": "Surgical Fine-Tuning Improves Adaptation to Distribution Shifts"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "9a1d94a930168918a1a1e1939b089d16d58d7865",
        "title": "A Kernel-Based View of Language Model Fine-Tuning"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "14a3aae8060338e3fbefc2af694890b019874d4f",
        "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
        "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "1355d22b6f85a28d46ce5ec31b1a4ba38e38147e",
        "title": "Neural Contextual Bandits with Deep Representation and Shallow Exploration"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "8095cb807d1c7577376c470bb6d43702634524c2",
        "title": "Tensor Programs II: Neural Tangent Kernel for Any Architecture"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "paperId": "00c957711b12468cb38424caccdf5291bb354033",
        "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
      },
      {
        "paperId": "93a636f0b6d450217fda5aaa26c74bb6b232f498",
        "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "0a021b8bc34d02022f04a478a9c387ed6222ab6d",
        "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": null,
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
      },
      {
        "paperId": null,
        "title": "Openllama: An open reproduction of llama,"
      },
      {
        "paperId": "597f869a3ae3572e16de3643a7bd285ea2f1eabd",
        "title": "Defining and Characterizing Reward Gaming"
      },
      {
        "paperId": "fba5089b01f3f465be1c07febe0371d34ff39d44",
        "title": "Vicuna: A Timing-Predictable RISC-V Vector Coprocessor for Scalable Parallel Computation"
      },
      {
        "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": "fa97c2238a16e9226f386ecffe22095e3d3d9dff",
        "title": "On the generalized distance in statistics"
      },
      {
        "paperId": null,
        "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"
      }
    ],
    "cited_by": [
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "4100c23cf07064de713c01ffd0103d8702f594f3",
        "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"
      },
      {
        "paperId": "1465aff7f7a0916287a43f8b0f0c0d6e5fe5719e",
        "title": "Inference-Time Reward Hacking in Large Language Models"
      },
      {
        "paperId": "c69a8148b4c8584c4665031837023f2e4987ae07",
        "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment"
      },
      {
        "paperId": "f6ca75836d8d9dc5b76baa3a797894febadd7d45",
        "title": "SJ-ORPO: Alignment of LLMs Using Multi-Task Learning and Self-Judgment"
      },
      {
        "paperId": "6e1d27c0a3d9f2c1cd12fae75d8bb115e15f8e09",
        "title": "Energy-Based Reward Models for Robust Language Model Alignment"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "c9e4efa58fd42a07da27ae70254981715cc257d5",
        "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization"
      },
      {
        "paperId": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
        "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking"
      },
      {
        "paperId": "d994b3c14f16353595e0487618c78647fb4bb73e",
        "title": "Approximated Variational Bayesian Inverse Reinforcement Learning for Large Language Model Alignment"
      },
      {
        "paperId": "2bbfe484ea3c38619791c10c5a30142dbea12fd0",
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms"
      },
      {
        "paperId": "a3e894356ca9865b881cb53aa62548f1dbff82d8",
        "title": "Reward-Robust RLHF in LLMs"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "14b015f9540747dd20f0e4a09f5f9b61a8508ad4",
        "title": "Scalable Ensembling For Mitigating Reward Overoptimisation"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "315a5cb5dbbbe2be8468b2bb7c62ea72af8930da",
        "title": "Filtered Direct Preference Optimization"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      }
    ],
    "score": 18.0
  },
  {
    "id": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
    "title": "Creativity Has Left the Chat: The Price of Debiasing Language Models",
    "authors": [
      "Behnam Mohammadi"
    ],
    "year": 2024,
    "citationCount": 17,
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards\"attractor states\", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.",
    "url": "https://www.semanticscholar.org/paper/ea9809331f53bd9b3013f49cc10ac79965e40b2e",
    "pdf_url": "https://arxiv.org/pdf/2406.05587.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-06-08",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-05587",
      "ArXiv": "2406.05587",
      "DOI": "10.48550/arXiv.2406.05587",
      "CorpusId": 270371059
    },
    "references": [
      {
        "paperId": "e5e19312c020b480a3df8440464713b21936e113",
        "title": "Explaining Large Language Models Decisions Using Shapley Values"
      },
      {
        "paperId": "31d2ccff82e313eb5c1620c44bb8322da4a38513",
        "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications"
      },
      {
        "paperId": "d3ee9ab2b145e9e6a94441c5fff61a95bb875bb2",
        "title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models"
      },
      {
        "paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
        "title": "Mitigating the Alignment Tax of RLHF"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "21029921a9a0e13efc9ab7b5c98a3412a9be7ab4",
        "title": "Large language model applications for evaluation: Opportunities and ethical implications"
      },
      {
        "paperId": "edd5f50f212c9c3baddb0c92cca989524baeb58e",
        "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "1c7402843d8b586d945b3b030e3edd93f0ae3959",
        "title": "On the Creativity of Large Language Models"
      },
      {
        "paperId": "5501d00310b06e00351295529498cc684187148d",
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "4f09e6ec1b7d4390d23881852fd7240994abeb58",
        "title": "A statistical interpretation of term specificity and its application in retrieval"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "bcdc102c04fb0e7d4652e8bcc7edd2983bb9576d",
        "title": "VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text"
      },
      {
        "paperId": "e50a316f97c9a405aa000d883a633bd5707f1a34",
        "title": "Term-Weighting Approaches in Automatic Text Retrieval"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
        "title": "Visualizing Data using t-SNE"
      },
      {
        "paperId": "6d12a1d23b21a9b170118a56386552bc5d4727de",
        "title": "A Mathematical Theory of Communication"
      },
      {
        "paperId": "a7acb135c4a53f6b1f6cedf2eb29c018650c2cbd",
        "title": "The myers-briggs type indicator"
      },
      {
        "paperId": null,
        "title": "Mode collapse in RL may be fueled by the update equation"
      },
      {
        "paperId": null,
        "title": "Llama \uf63a Prompt Template"
      },
      {
        "paperId": null,
        "title": "Context length in LLMs: All you need to know - AGI Sphere"
      },
      {
        "paperId": null,
        "title": "Mysteries of mode collapse"
      }
    ],
    "cited_by": [
      {
        "paperId": "b4bea2b9239a920c4e30f0c577042d20a662d8af",
        "title": "Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising"
      },
      {
        "paperId": "25a21ceb0912442e31f526394e428d6be9521d97",
        "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety"
      },
      {
        "paperId": "2bc704af7ccfc0fccf2800e4a37649e889bd4cfa",
        "title": "Homogenizing Effect of Large Language Models (LLMs) on Creative Diversity: An Empirical Comparison of Human and ChatGPT Writing"
      },
      {
        "paperId": "ccb70eb3381330c04f893471bb7b55b465a8bbbc",
        "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner"
      },
      {
        "paperId": "924ec48257e00c39b5c185935d32440341076d9e",
        "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation of LLM"
      },
      {
        "paperId": "56a525d0cabb673e0d4149ddd89c948f5ea67386",
        "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks"
      },
      {
        "paperId": "6c0ac900afe60aa57022845b12cd97dab6c0a6c3",
        "title": "Echoes in AI: Quantifying lack of plot diversity in LLM outputs"
      },
      {
        "paperId": "fbc3ecb2f546f556e943dc03a8ce4ff75157a504",
        "title": "Creativity in AI: Progresses and Challenges"
      },
      {
        "paperId": "4f06e67ec42c03abb5745bdbfe151d24338d12a4",
        "title": "Preference Optimization with Multi-Sample Comparisons"
      },
      {
        "paperId": "9cfdb3cdb63d41de52b68f0ad6219b822a4b37d5",
        "title": "Diversity-Rewarded CFG Distillation"
      },
      {
        "paperId": "8a576c2dbb55ae83e114de4f7bfd221d4542deed",
        "title": "ChatGPT vs Social Surveys: Probing Objective and Subjective Silicon Population"
      },
      {
        "paperId": "3cdb3208111acfa081d0fa5a2a7cdf883ed41ba7",
        "title": "The Drama Machine: Simulating Character Development with LLM Agents"
      },
      {
        "paperId": "ba6c8d0179c8979436981716846cb2c8da3c8f68",
        "title": "Towards Building Specialized Generalist AI with System 1 and System 2 Fusion"
      },
      {
        "paperId": "7045f1865288cc9f2d8cfa118d3f40b7f3ce8af4",
        "title": "Towards a Science Exocortex"
      },
      {
        "paperId": "2257d0894da128e9c900df1237f43504c9c0e82e",
        "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
      },
      {
        "paperId": "6c6cb32f026fccc11c98c36913651b992d477a56",
        "title": "Divergent Creativity in Humans and Large Language Models"
      },
      {
        "paperId": "ccd4db9c164b1cf1b294f1db0c727d5c5aa0714f",
        "title": "Off-policy Predictive Control with Causal Sensitivity Analysis"
      }
    ],
    "score": 17.0
  },
  {
    "id": "ca39d564e30c35ccc95546272903674f89e5ad0f",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning",
    "authors": [
      "Guanting Dong",
      "Yifei Chen",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Hongjin Qian",
      "Yutao Zhu",
      "Hangyu Mao",
      "Guorui Zhou",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.",
    "url": "https://www.semanticscholar.org/paper/ca39d564e30c35ccc95546272903674f89e5ad0f",
    "pdf_url": "https://arxiv.org/pdf/2505.16410.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-22",
    "externalIds": {
      "ArXiv": "2505.16410",
      "DBLP": "journals/corr/abs-2505-16410",
      "DOI": "10.48550/arXiv.2505.16410",
      "CorpusId": 278789048
    },
    "references": [
      {
        "paperId": "47cb159c6da33fa630beb244099dd77d743d2091",
        "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis"
      },
      {
        "paperId": "23f655a7a596ab5f431ab9e18d5c641de3f8afb9",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
      },
      {
        "paperId": "1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example"
      },
      {
        "paperId": "d804b6420700e5e72fe571316d922bef9dfaa8ca",
        "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning"
      },
      {
        "paperId": "1d03586baa32b3d6ff657a180053821543e11abb",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "61583512c4704120349a1a5f0acb14ee1d893e29",
        "title": "Acting Less is Reasoning More! Teaching Model to Act Efficiently"
      },
      {
        "paperId": "e42054d042e2e5b3efe6e96cd6dd1c76ab3ca358",
        "title": "ToolRL: Reward is All Tool Learning Needs"
      },
      {
        "paperId": "8402e446158252992b6ddf1ff1b0658c39d7604e",
        "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
      },
      {
        "paperId": "7f319badb2d7e38ad14596d832ad18de34f7cb7e",
        "title": "A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook"
      },
      {
        "paperId": "4298a7bca88001f5df2d1ae15cca186d46271dc5",
        "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments"
      },
      {
        "paperId": "1d0b3c90eb184ea9da554ea3381bdbf1e0f23629",
        "title": "ToRL: Scaling Tool-Integrated RL"
      },
      {
        "paperId": "fb970ce4383a78ad52c641bc38815d78ad737aad",
        "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"
      },
      {
        "paperId": "cc769ab935638777a99f2a965b368c86d2cf52b4",
        "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't"
      },
      {
        "paperId": "891cc1397f949d4432a5a0602e3757e9e3610862",
        "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "1fcaafeb72142fe3d1a5d698a072d69778d244b0",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "paperId": "f4c98d3958017085f78a597b3b86b2bc9e3de854",
        "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning models"
      },
      {
        "paperId": "78f85a3a4e9d1b83ac33179c777e6eb2d756be82",
        "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "b87f2fbb88c7dcf3a4a96dc690f8f4da831112dc",
        "title": "START: Self-taught Reasoner with Tools"
      },
      {
        "paperId": "c39ae6fb8ab9b8dbd8cca4b13b303033dc23f594",
        "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models"
      },
      {
        "paperId": "16a096e663c5aa661f26bcabe212a53d86ef3eae",
        "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs"
      },
      {
        "paperId": "f4195d4e283e289665cfc7a65fde2fa7b8814091",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
      },
      {
        "paperId": "6508e62ee67ecb48fc1534f7f8cf2f687ae68bab",
        "title": "LIMR: Less is More for RL Scaling"
      },
      {
        "paperId": "c7bdfabf35090863f2f582bf053101ca722327ea",
        "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning"
      },
      {
        "paperId": "9f88f56b592e1c1b8d012affec24ed6ca52d9e2b",
        "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools"
      },
      {
        "paperId": "b62d0605137463ea591a0619840305cb98f6958f",
        "title": "LIMO: Less is More for Reasoning"
      },
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "eaa6ffc25363ce88d7887f58f5b03a0c26231c36",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal"
      },
      {
        "paperId": "5b86fbaf12fd50710066e3995e33ece2185f8ada",
        "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models"
      },
      {
        "paperId": "f05cf0438b5dc19bc4d32ca6cd85d1525c936de6",
        "title": "Progressive Multimodal Reasoning via Active Retrieval"
      },
      {
        "paperId": "d7c49d355554a6e3a5af481c0a92a515816fa82d",
        "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement"
      },
      {
        "paperId": "7ab7da90c65996f4c100e944f15ad55161c13c1b",
        "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation"
      },
      {
        "paperId": "d74bf37c83797c7a7fab19428ff014f2f0d4e035",
        "title": "Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems"
      },
      {
        "paperId": "43982aa7065a13bc1c58029926827d1907c8c525",
        "title": "O1 Replication Journey: A Strategic Progress Report - Part 1"
      },
      {
        "paperId": "9fb201282f53a4ce89f28cbe5026af78912aa8c1",
        "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement"
      },
      {
        "paperId": "3cb2497a363dc78bae3804b6654436b074f0b4e5",
        "title": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
        "title": "Qwen2 Technical Report"
      },
      {
        "paperId": "e8da1f7293657674c4f3648e227eb1b3e6c861f7",
        "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning"
      },
      {
        "paperId": "bd05f4c741673538c829133965461047abfe96e1",
        "title": "We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?"
      },
      {
        "paperId": "64ee29d6ddb2c2167a201783ddd4d0a9b744f352",
        "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation"
      },
      {
        "paperId": "50650e66ce7c454595862aac70c2a3e9dde23387",
        "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models"
      },
      {
        "paperId": "daebec92963ab8dea492f0c209bdf57e87bcaa07",
        "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "3a56bc074b8f3f985599627404b70e16fc5bce1b",
        "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"
      },
      {
        "paperId": "ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd",
        "title": "GAIA: a benchmark for General AI Assistants"
      },
      {
        "paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54",
        "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"
      },
      {
        "paperId": "b272513916b45c8517d289d7abee4a53e6832187",
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
      },
      {
        "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      },
      {
        "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "28e536d4b425a8743af9b074ddb11baba66f8b47",
        "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey"
      },
      {
        "paperId": "0d42221038c05cee8443c5b5af838505ee137dc3",
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "f208ea909fa7f54fea82def9a92fd81dfc758c39",
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
      },
      {
        "paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "title": "Towards Reasoning in Large Language Models: A Survey"
      },
      {
        "paperId": "5a3c1afe73d8bcc8288d17cb17be2baec8a98464",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
      },
      {
        "paperId": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
        "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
      },
      {
        "paperId": "e070ff286709db28312e08b52b05539debe88146",
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "ec307b17f193b14292206b65a1bcc95bfd8f02ed",
        "title": "\u266b MuSiQue: Multihop Questions via Single-hop Question Composition"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "24b471802eda460d4cc17e591804ef04c0cd18ef",
        "title": "Curriculum Learning: A Survey"
      },
      {
        "paperId": "9001eb3c3d5a96ad3d804410c2437e6f60feade9",
        "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
      },
      {
        "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
      },
      {
        "paperId": "8b191d96db9248c8284d6146976ca3b5c4858f00",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": "3506b3be3b2472adb748e5c2cc57c200d403d7b5",
        "title": "InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework"
      },
      {
        "paperId": null,
        "title": "Concise reasoning"
      },
      {
        "paperId": null,
        "title": "the search capability of llms"
      }
    ],
    "cited_by": [
      {
        "paperId": "7dbde77b268d8df57210ccc69fac37e3375a5595",
        "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning"
      },
      {
        "paperId": "8b0bdb58ee114bf0ef571f8f52e55465a35b4aba",
        "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "1b2ef8ba05fba415b5bd5d3a592c35d9d1136ec5",
        "title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents"
      },
      {
        "paperId": "146cda64133619f15a00f96ca0b66c0b620d34ea",
        "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning"
      },
      {
        "paperId": "7e5e73010469a1d4fd6c78b66657aae6d74b0318",
        "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"
      },
      {
        "paperId": "61b307002e3289f2e1b5ad45b13d2e2c5e71309f",
        "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning"
      },
      {
        "paperId": "3c8b92a8b6eee1377077ecfea95e53223845af20",
        "title": "Agentic Reinforced Policy Optimization"
      },
      {
        "paperId": "dc491963de5b0e69b10094092753f010c3f72977",
        "title": "Deep Research Agents: A Systematic Examination And Roadmap"
      },
      {
        "paperId": "673eab676242b4d0fe2f6ae993dd56d8d2eb92d1",
        "title": "A Survey on AI Search with Large Language Models"
      }
    ],
    "score": 10.0
  },
  {
    "id": "5695f983699b36af61851d8025aab9caea970eae",
    "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
    "authors": [
      "Yanzhi Zhang",
      "Zhaoxi Zhang",
      "Haoxiang Guan",
      "Yilin Cheng",
      "Yitong Duan",
      "Chen Wang",
      "Yue Wang",
      "Shuxin Zheng",
      "Jiyan He"
    ],
    "year": 2025,
    "citationCount": 8,
    "abstract": "Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",
    "url": "https://www.semanticscholar.org/paper/5695f983699b36af61851d8025aab9caea970eae",
    "pdf_url": "https://arxiv.org/pdf/2506.17219.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-06-20",
    "externalIds": {
      "ArXiv": "2506.17219",
      "DBLP": "journals/corr/abs-2506-17219",
      "DOI": "10.48550/arXiv.2506.17219",
      "CorpusId": 279465533
    },
    "references": [
      {
        "paperId": "1eab09d29fa9e7ff08e8c82166b9faf4463e84d5",
        "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"
      },
      {
        "paperId": "b6eb66bd0652c55040913158dd8fce3ad0cfa130",
        "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models"
      },
      {
        "paperId": "d6a29be03a0497602e89311ec38e5141335647c5",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
      },
      {
        "paperId": "1839b34a1df105008fcdcda6502336fede9786da",
        "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions"
      },
      {
        "paperId": "532c7d150d7bfece96e3c51ad0c8e4e18271912c",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "paperId": "89e3017e1ffdf5f7cedd3b5bb08fcc0ab48200ba",
        "title": "Learning to Reason without External Rewards"
      },
      {
        "paperId": "8f982fb9fc6d038181ec015fbba4ea6fbc86fece",
        "title": "One-shot Entropy Minimization"
      },
      {
        "paperId": "4aef44e4aeaf28868ae2f1fff2c4eb19ff4df1f6",
        "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning"
      },
      {
        "paperId": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
        "title": "Qwen3 Technical Report"
      },
      {
        "paperId": "0f0157b3e679d3dacdfa56277831fed9b2b8e0a2",
        "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale"
      },
      {
        "paperId": "143e18bfd7c356592e7c1439738a3525d3e16279",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
      },
      {
        "paperId": "1738bfdda961ddc9d31b39f3dd38b1f425199e94",
        "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization"
      },
      {
        "paperId": "d85788857fd230169e17638631b96335368043ed",
        "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
      },
      {
        "paperId": "609ca024d2eb12c606491f67cee9e71ae730e8ff",
        "title": "Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning"
      },
      {
        "paperId": "6f8baf0b18117f671eb0ffcb23b481b06767f0e9",
        "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "225721f407a7a8cdefd4ba6bc61c43acba5a3b6a",
        "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models"
      },
      {
        "paperId": "e07a7e57745b69c0f08601fbdd19cfdcafbe1152",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "c7f9706898bdfa3241601e075b1305649b174ff1",
        "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
      },
      {
        "paperId": "47e6256ee02710c7698f49a0360718d561972e40",
        "title": "Self-Reflection Makes Large Language Models Safer, Less Biased, and Ideologically Neutral"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "b18833db0de9393d614d511e60821a1504fc6cd1",
        "title": "A Natural Policy Gradient"
      },
      {
        "paperId": "8b191d96db9248c8284d6146976ca3b5c4858f00",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      },
      {
        "paperId": null,
        "title": "Qwq-32b: Embracing the power of reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
      }
    ],
    "cited_by": [
      {
        "paperId": "47009065ba03c9cd3e777ba172ae1849257f7d6f",
        "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization"
      },
      {
        "paperId": "5becda4f5dd8eca7c2f9990abcd4b1b2543ead2b",
        "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks"
      },
      {
        "paperId": "439dfba3da8026559ab8ba5d088c2e601c11de74",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle"
      },
      {
        "paperId": "92c39c226cb07a768ee11d2ac01aab33534b214b",
        "title": "Self-Aligned Reward: Towards Effective and Efficient Reasoners"
      },
      {
        "paperId": "c0c23c58fc3925ab5a4a144ad839240a8765fbed",
        "title": "Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning"
      },
      {
        "paperId": "012ee4ecf333b4a4ac54d1bbc398417157206336",
        "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models"
      },
      {
        "paperId": "24a35803e943a1b70a7620e9493949c5016a1e21",
        "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination"
      },
      {
        "paperId": "fe54c92cb5f7c2c68847354af8770dd8df5f7088",
        "title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection"
      }
    ],
    "score": 8.0
  },
  {
    "id": "b8f435d3b8202f1086be9d791857c20cb3a4a90a",
    "title": "Pareto-Optimal Learning from Preferences with Hidden Context",
    "authors": [
      "Ryan Boldi",
      "Lijie Ding",
      "Lee Spector",
      "S. Niekum"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Ensuring AI models align with human values is essential for their safety and functionality. Reinforcement learning from human feedback (RLHF) leverages human preferences to achieve this alignment. However, when preferences are sourced from diverse populations, point estimates of reward can result in suboptimal performance or be unfair to specific groups. We propose Pareto Optimal Preference Learning (POPL), which enables pluralistic alignment by framing discrepant group preferences as objectives with potential trade-offs, aiming for policies that are Pareto-optimal on the preference dataset. POPL utilizes lexicase selection, an iterative process that selects diverse and Pareto-optimal solutions. Our theoretical and empirical evaluations demonstrate that POPL surpasses baseline methods in learning sets of reward functions and policies, effectively catering to distinct groups without access to group numbers or membership labels. We verify the performance of POPL on a stateless preference learning setting, a Minigrid RL domain, Metaworld robotics benchmarks, as well as large language model (LLM) fine-tuning. We illustrate that POPL can also serve as a foundation for techniques optimizing specific notions of group fairness, ensuring safe and equitable AI model alignment.",
    "url": "https://www.semanticscholar.org/paper/b8f435d3b8202f1086be9d791857c20cb3a4a90a",
    "pdf_url": "https://arxiv.org/pdf/2406.15599.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-06-21",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-15599",
      "ArXiv": "2406.15599",
      "DOI": "10.48550/arXiv.2406.15599",
      "CorpusId": 270703216
    },
    "references": [
      {
        "paperId": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
        "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences"
      },
      {
        "paperId": "46508337ac82391d1c1c6d3dc17204043aba6272",
        "title": "DALex: Lexicase-like Selection via Diverse Aggregation"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "0cf7d32bc9017842abb8752b279e2d31910623b4",
        "title": "Optimizing Neural Networks with Gradient Lexicase Selection"
      },
      {
        "paperId": "ac3ae10014337879bb566dee8b6ae840e3897384",
        "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF"
      },
      {
        "paperId": "49239422451ab47cb9bd827c54a6935ac62581db",
        "title": "Objectives Are All You Need: Solving Deceptive Problems Without Explicit Diversity Maintenance"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "213d79556c2d50f6b4113a2d85309ebb5e3909f5",
        "title": "Quality-Diversity through AI Feedback"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "bd981f0afaf5bb10b3daf80ec98c158e23c16339",
        "title": "Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "8bb7cecd1bc3fa470aa882158e7553705b1a6141",
        "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "d655026a443d3de40ebacf61274e562ec05936ef",
        "title": "Moral Machine or Tyranny of the Majority?"
      },
      {
        "paperId": "2f0bf4684c5042053397d2a61e3078de6b90b21e",
        "title": "Probabilistic Lexicase Selection"
      },
      {
        "paperId": "e38a29f6463f38f43797b128673b9e44d18a991e",
        "title": "Whose Opinions Do Language Models Reflect?"
      },
      {
        "paperId": "e5174aeda1baa67c17f4ac630ae2e44453954cc3",
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"
      },
      {
        "paperId": "c28af43206867cb5529164e3dd6d9ea8b7cfe7f2",
        "title": "Active Reward Learning from Multiple Teachers"
      },
      {
        "paperId": "0bec67b22a85cc4344bbbeb837f369afde091288",
        "title": "The Expertise Problem: Learning from Specialized Feedback"
      },
      {
        "paperId": "e920f426eb32e64474b2a1176d97725f875dd82a",
        "title": "The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types"
      },
      {
        "paperId": "73de6c064e6f441bb0718aaf202620aff9c03fbc",
        "title": "Lexicase selection at scale"
      },
      {
        "paperId": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc",
        "title": "Models of human preference for learning reward functions"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "b13a37d0a044b63975a338f5005aca6120bbecf0",
        "title": "Jury Learning: Integrating Dissenting Voices into Machine Learning Models"
      },
      {
        "paperId": "51965de80f86432d42749427db1e5bb0fa1e204c",
        "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "c395595cf7be23f7d90cbca98d8c7861ebfd884d",
        "title": "The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality"
      },
      {
        "paperId": "ab14ccf7283827618d15bee44bbefef570478e9c",
        "title": "Bayesian Robust Optimization for Imitation Learning"
      },
      {
        "paperId": "658018e556484e3d9c6bcc00c726bf5eb503ef86",
        "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences"
      },
      {
        "paperId": "1f52deff193c7c3dfc77c48cbdc653c94f093a92",
        "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning"
      },
      {
        "paperId": "976e128c32007cdaf95a4e278a1ea7aa33a2e5cc",
        "title": "LESS is More: Rethinking Probabilistic Models of Human Behavior"
      },
      {
        "paperId": "04f788ea49e7fbd55369fbfa0945c53dd40c9d3b",
        "title": "Deep Bayesian Reward Learning from Preferences"
      },
      {
        "paperId": "40bf9636125b73ff95ea308b8953f701fe3c7b39",
        "title": "A Probabilistic and Multi-Objective Analysis of Lexicase Selection and \u03b5-Lexicase Selection"
      },
      {
        "paperId": "0090023afc66cd2741568599057f4e82b566137c",
        "title": "A Survey on Bias and Fairness in Machine Learning"
      },
      {
        "paperId": "48692cce7e9e49bab6e012524fbe403edcb22b91",
        "title": "Batch Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"
      },
      {
        "paperId": "b61554b0aa931efec0a7a93fa16221a695d1875b",
        "title": "Solving Uncompromising Problems With Lexicase Selection"
      },
      {
        "paperId": "809c2d00190feabe5045d52d36862169b862dc7e",
        "title": "Assessment of problem modality by differential performance of lexicase selection in genetic programming: a preliminary report"
      },
      {
        "paperId": "7dd51cef9bd43d495a12d10b7d0846f9bd60d9fa",
        "title": "Action understanding as inverse planning"
      },
      {
        "paperId": "b959164d1efca4b73986ba5d21e664aadbbc0457",
        "title": "A Practical Bayesian Framework for Backpropagation Networks"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "012948026226fc74bf546ff5024dd735f5d30605",
        "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning"
      },
      {
        "paperId": "3ecfd550ac7eb0d375c8432dc1e12c453110342e",
        "title": "Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree"
      },
      {
        "paperId": "db20bd3bb82d1011ce704d440d8c2578f665e6e1",
        "title": "Aligning Robot and Human Representations"
      },
      {
        "paperId": "4f8e75d10cc9bcbd20e6aefea972b6d419a688cc",
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021"
      },
      {
        "paperId": null,
        "title": "A method for stochastic optimization"
      },
      {
        "paperId": "3962a66e72775ddb496b2425c7451394d02b17b1",
        "title": "Knowledge and implicature: Modeling language understanding as social cognition"
      },
      {
        "paperId": "089e80da8726a950d135f0a150889eff3fc04a99",
        "title": "Learning a theory of causality."
      },
      {
        "paperId": null,
        "title": "Learning Multimodal Rewards from Rankings, October 2021"
      },
      {
        "paperId": null,
        "title": "Quality diversity through human feedback: Towards open-ended diversity-driven optimization"
      },
      {
        "paperId": null,
        "title": "sources to mitigate this impact"
      }
    ],
    "cited_by": [
      {
        "paperId": "c3102c4f7b08b92516cef82c4f91258c448a45e3",
        "title": "One Size Fits None: Learning Distributions of Human Preferences"
      },
      {
        "paperId": "1764658448a295ab2dc5b232dbc0d6290892c2ab",
        "title": "Societal Alignment Frameworks Can Improve LLM Alignment"
      },
      {
        "paperId": "e983d7ad555dad845059da1ee0a5e57cfd057af0",
        "title": "Direct Alignment with Heterogeneous Preferences"
      },
      {
        "paperId": "c8384dc76e0426fa2ba212f2a1f7f79249b60274",
        "title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "e7b5d0269bdd37d01cea2bddb4d2ec9cf1539a40",
        "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"
      }
    ],
    "score": 6.0
  },
  {
    "id": "280598d6613a071db232422b914f613a37cf13d1",
    "title": "Simplify RLHF as Reward-Weighted SFT: A Variational Method",
    "authors": [
      "Yuhao Du",
      "Zhuo Li",
      "Pengyu Cheng",
      "Zhihong Chen",
      "Yuejiao Xie",
      "Xiang Wan",
      "Anningzhe Gao"
    ],
    "year": 2025,
    "citationCount": 6,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\\textbf{V}$ariational $\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.",
    "url": "https://www.semanticscholar.org/paper/280598d6613a071db232422b914f613a37cf13d1",
    "pdf_url": "https://arxiv.org/pdf/2502.11026.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-16",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-11026",
      "ArXiv": "2502.11026",
      "DOI": "10.48550/arXiv.2502.11026",
      "CorpusId": 276409389
    },
    "references": [
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "dc47abf03e644afa0bdc834bab11018d3804e3c8",
        "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators"
      },
      {
        "paperId": "5387445a58a958422a8cfd297e6a611aade0f0e8",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "07038b2d2da9f1785a1e8e260a75c8215bd36ac7",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "paperId": "ebd1c04c61f73f46def3305ca11d038c46665b65",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "84259db14b725853ecfe425fe85ca375b32983c2",
        "title": "Adversarial Preference Optimization"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "6fca85024354e3fafa75b767961bee9245263170",
        "title": "Reward Collapse in Aligning Large Language Models"
      },
      {
        "paperId": "60d90e96e7c434861697194fa47f1978d86b9d28",
        "title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b",
        "title": "Aligning Language Models with Preferences through f-divergence Minimization"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
      },
      {
        "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
      },
      {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "6120cc252bc74239012f11b8b075cb7cb16bee26",
        "title": "An Introduction to Variational Methods for Graphical Models"
      },
      {
        "paperId": "87cbed883368d4a9efd42fdd91f47038f8d8fbe6",
        "title": "On Information and Sufficiency"
      },
      {
        "paperId": "cb89a25141febad0c14d080e2791c506ae6e4a76",
        "title": "Constrained Optimization and Lagrange Multiplier Methods"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "1a4c6856292b8c64d19a812a77f0aa6fd47cb96c",
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "9954ee27c938ed313f9ab60ca2eb48af10fb6906",
        "title": "The Limits of"
      },
      {
        "paperId": "80e8ad6b81e65466b5b72a5800f77b85b8b504ab",
        "title": "Bayesian estimates of equation system parameters, An application of integration by Monte Carlo"
      },
      {
        "paperId": null,
        "title": "Estimation of particle transmission by random sampling"
      },
      {
        "paperId": null,
        "title": "Quota sampling and importance functions in stochastic solution of particle problems. Technical report"
      },
      {
        "paperId": null,
        "title": "Auto-encoding"
      },
      {
        "paperId": null,
        "title": "Self-play fine-tuning converts"
      },
      {
        "paperId": null,
        "title": "Model alignment as prospect"
      },
      {
        "paperId": null,
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ],
    "cited_by": [
      {
        "paperId": "22ccadf6c4b9def722bf84a3c2fe9984ade3a349",
        "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient"
      },
      {
        "paperId": "6d51f488dc266d2aede6cf29960928ce4d11c2df",
        "title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set"
      },
      {
        "paperId": "1414653547b72a23d2c0ca987d270fb236c9105a",
        "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"
      },
      {
        "paperId": "9c19ed145128ec8f67fc66b14f6b2740e6a4fcbe",
        "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)"
      },
      {
        "paperId": "7f01d7adc28899552eb0d3d9ca488b3c2300b0f2",
        "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models"
      },
      {
        "paperId": "de65acedb8fbd12b6f2fdd591e50560a35b93509",
        "title": "Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm"
      }
    ],
    "score": 6.0
  },
  {
    "id": "4146b447187e1a09b736564854007c403f986c69",
    "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling",
    "authors": [
      "Yuhang Lai",
      "Siyuan Wang",
      "Shujun Liu",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io.",
    "url": "https://www.semanticscholar.org/paper/4146b447187e1a09b736564854007c403f986c69",
    "pdf_url": "https://arxiv.org/pdf/2403.06754.pdf",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "publicationDate": "2024-03-11",
    "externalIds": {
      "ArXiv": "2403.06754",
      "DBLP": "conf/acl/LaiWLHW24",
      "DOI": "10.48550/arXiv.2403.06754",
      "CorpusId": 268357678
    },
    "references": [
      {
        "paperId": "60f066b3d7391dc5da3e3638970fd00f1aadebdf",
        "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts"
      },
      {
        "paperId": "65fb348291de709a379a3f0d00b48726a1a674d2",
        "title": "Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble"
      },
      {
        "paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "paperId": "0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8",
        "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-Grained Correctional Human Feedback"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
        "title": "Compositional preference models for aligning LMs"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF"
      },
      {
        "paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      },
      {
        "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
        "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
      },
      {
        "paperId": "28c6ac721f54544162865f41c5692e70d61bccab",
        "title": "A Survey on Large Language Model based Autonomous Agents"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "f1f6c61ed0b80a785e4e5d0d97a454dbe6126c63",
        "title": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization"
      },
      {
        "paperId": "8a4fc5f00cd4aca61e148e46a2125c3a406719f1",
        "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"
      },
      {
        "paperId": "959adda6f1b874be32bea9bb0bd8f3b43856980b",
        "title": "Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL"
      },
      {
        "paperId": "fcd7bdf37cec0845f6960aa1ab87685b73fcfd2f",
        "title": "ASQA: Factoid Questions Meet Long-Form Answers"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "2ab295108a20976f3f5c9ed986b64c24838d7f4a",
        "title": "Gender Bias in Machine Translation"
      },
      {
        "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "title": "Longformer: The Long-Document Transformer"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "20e127a1d27617b2b8545a54c016ccf2b5170b78",
        "title": "Hierarchical Reinforcement Learning for Open-Domain Dialog"
      },
      {
        "paperId": "0052b31f07eda7737b5e0e2bf3803c3a32f3f728",
        "title": "Supervising strong learners by amplifying weak experts"
      },
      {
        "paperId": "4c852a954c3a74df410231d601857b7005076de9",
        "title": "Hierarchical Reinforcement Learning with Hindsight"
      },
      {
        "paperId": "5a5a1d666e4b7b933bc5aafbbadf179bc447ee67",
        "title": "AI safety via debate"
      },
      {
        "paperId": "bf8d58faf972ad0a1026c0a7c5577c07996ef3a7",
        "title": "Hierarchical Deep Reinforcement Learning for Continuous Action Control"
      },
      {
        "paperId": "74b284a66e75b65f5970d05bac000fe91243ee49",
        "title": "Video Captioning via Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
        "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning"
      },
      {
        "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "25ca4a36df2955b345634b5f8a6b6bb66a774b3c",
        "title": "Parallel Data, Tools and Interfaces in OPUS"
      },
      {
        "paperId": "c9240006dbe3be68b2e4a7b18ad40ea5f75bed9d",
        "title": "DRLC: Reinforcement Learning with Dense Rewards from LLM Critic"
      },
      {
        "paperId": "ac771182d1780c863954243809d1e144433919f9",
        "title": "Aligning Large Language Models with Human: A Survey"
      },
      {
        "paperId": "b7e03e82d597571f00aa8802699a9d7e7164ae92",
        "title": "Controlling the Reading Level of Machine Translation Output"
      },
      {
        "paperId": null,
        "title": "2022. Measuring progress on scalable oversight for large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "f6ca75836d8d9dc5b76baa3a797894febadd7d45",
        "title": "SJ-ORPO: Alignment of LLMs Using Multi-Task Learning and Self-Judgment"
      },
      {
        "paperId": "d6661217c372b9ac9678cc7851b79cd1739fd66d",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design"
      },
      {
        "paperId": "06c1ec31219d68f0ffca74f6ee4c22acd6664a5b",
        "title": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback"
      },
      {
        "paperId": "581d831fcea84501bec33161c716d5bf94a8a345",
        "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training"
      },
      {
        "paperId": "5c8245c04c902b0dd631b8633ca835967f395ad0",
        "title": "HonestLLM: Toward an Honest and Helpful Large Language Model"
      },
      {
        "paperId": "e36f36ffe6df823f50a54c525319926e3df5ead1",
        "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models"
      }
    ],
    "score": 6.0
  },
  {
    "id": "b1286763413a7b2309edeba1ed18884be429a941",
    "title": "COPR: Continual Human Preference Learning via Optimal Policy Regularization",
    "authors": [
      "Han Zhang",
      "Lin Gui",
      "Yu Lei",
      "Yuanzhao Zhai",
      "Yehong Zhang",
      "Yulan He",
      "Hui Wang",
      "Yue Yu",
      "Kam-Fai Wong",
      "Bin Liang",
      "Ruifeng Xu"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives. We also provide formal proof for the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment. Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders.",
    "url": "https://www.semanticscholar.org/paper/b1286763413a7b2309edeba1ed18884be429a941",
    "pdf_url": "https://arxiv.org/pdf/2402.14228.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-02-22",
    "externalIds": {
      "DBLP": "journals/corr/abs-2402-14228",
      "ArXiv": "2402.14228",
      "DOI": "10.48550/arXiv.2402.14228",
      "CorpusId": 267782424
    },
    "references": [
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "13fd4277388cc2a9da75e8b772e5efcf6ebe2d32",
        "title": "Training Socially Aligned Language Models on Simulated Social Interactions"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "2cd72e71299c5d62d5cdb1164df5236172d418c4",
        "title": "Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "891edceb78a274b0c2494d8176bc4d6f6e3f9cbc",
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"
      },
      {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "6c4bd57f7e70b9c037e44a96840e5ead0513abb0",
        "title": "Understanding Dataset Difficulty with V-Usable Information"
      },
      {
        "paperId": "7f1e7d8d4004cd2024fef089bf5f4066f6792f77",
        "title": "Self-Supervised Training Enhances Online Continual Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "e17540668655525a4660b6db63574bc1c6fa5aa9",
        "title": "PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning"
      },
      {
        "paperId": "e10b7cb072737b1fbdcba7948e573268b3573ae9",
        "title": "Dark Experience for General Continual Learning: a Strong, Simple Baseline"
      },
      {
        "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "4ccd95612be1b970f64871e6c132cd01269d8ad9",
        "title": "Learning a Unified Classifier Incrementally via Rebalancing"
      },
      {
        "paperId": "a40325e8d1008dad1ec0dffa90a4996156ccce1e",
        "title": "Task-Free Continual Learning"
      },
      {
        "paperId": "4a954b3e72a61968ab235076bcc242aca3a05520",
        "title": "Efficient Lifelong Learning with A-GEM"
      },
      {
        "paperId": "9958590c281e7a7b524dd594635037143c632c21",
        "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence"
      },
      {
        "paperId": "3aa673abd49f0837ed2fbf5cffcada9ba6f48693",
        "title": "Overcoming catastrophic forgetting with hard attention to the task"
      },
      {
        "paperId": "713b0d9005944f80af00addc81b162ca74ea4b14",
        "title": "Memory Aware Synapses: Learning what (not) to forget"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "118fae4b4d07453561f1eded88654f812c7c61ec",
        "title": "Gradient Episodic Memory for Continual Learning"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
        "title": "Learning without Forgetting"
      },
      {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis"
      },
      {
        "paperId": "3efcb97c1de1c87832a7a1d99e91801992a938ec",
        "title": "Crafting Papers on Machine Learning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "c57dcfe972efbb5deaa8110c983c3a786d97095a",
        "title": "Language"
      },
      {
        "paperId": null,
        "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
      },
      {
        "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
        "title": "Deep Learning"
      },
      {
        "paperId": null,
        "title": "Rank responses to align language models with human feedback"
      },
      {
        "paperId": null,
        "title": "Reward ranked finetuning for generative foundation model"
      },
      {
        "paperId": null,
        "title": "Chain of hindsight aligns"
      },
      {
        "paperId": null,
        "title": "Training language models to follow"
      }
    ],
    "cited_by": [
      {
        "paperId": "68e64ff720c2a6cc2a306aacbeb6f04320ad9805",
        "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment"
      },
      {
        "paperId": "12fca89dc57c1162c62f9c24b2a2c8a2d3d2abd3",
        "title": "Towards LifeSpan Cognitive Systems"
      },
      {
        "paperId": "dcd0a2e67235add9e520e43c1d1fb4a89d76f98d",
        "title": "Towards Lifelong Learning of Large Language Models: A Survey"
      },
      {
        "paperId": "eaac29467de2dd223d32cc3d3a77b637ef2bc4b3",
        "title": "Recent Advances of Foundation Language Models-based Continual Learning: A Survey"
      },
      {
        "paperId": "819d25a7f90654ebf769d1b1c6499e5248cf3d8a",
        "title": "O UTLIER -A WARE P REFERENCE O PTIMIZATION FOR L ARGE L ANGUAGE M ODELS"
      }
    ],
    "score": 5.0
  },
  {
    "id": "68981715a1e37c955329fc1a278aef59c9be4764",
    "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
    "authors": [
      "Somanshu Singla",
      "Zhen Wang",
      "Tianyang Liu",
      "Abdullah Ashfaq",
      "Zhiting Hu",
      "Eric P. Xing"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving alignment without these extensive tuning costs and expensive annotations, we present a novel, tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO\u2019s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.",
    "url": "https://www.semanticscholar.org/paper/68981715a1e37c955329fc1a278aef59c9be4764",
    "pdf_url": "https://arxiv.org/pdf/2411.08733.pdf",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "publicationDate": "2024-11-13",
    "externalIds": {
      "ACL": "2024.emnlp-main.1220",
      "ArXiv": "2411.08733",
      "DBLP": "conf/emnlp/SinglaWLAHX24",
      "DOI": "10.18653/v1/2024.emnlp-main.1220",
      "CorpusId": 273901354
    },
    "references": [
      {
        "paperId": "bd2dd8efcc4a56548abbf50da468af1d7b7ae379",
        "title": "Aligning Large Language Models with Representation Editing: A Control Perspective"
      },
      {
        "paperId": "eee9880db5cfd10c27665ebea151843e9cb12175",
        "title": "Towards Scalable Automated Alignment of LLMs: A Survey"
      },
      {
        "paperId": "50c86b0c6759323ed5d711f2cfc80b6deeabc039",
        "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?"
      },
      {
        "paperId": "1bda8efbbf4abae6c8c1da97d6137396807b1e09",
        "title": "ReFT: Representation Finetuning for Language Models"
      },
      {
        "paperId": "c16f5a6e4c51be0baa49275df441748134d67234",
        "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping"
      },
      {
        "paperId": "1491e2d5d8ba63132ff157e47e824af76c422450",
        "title": "ARGS: Alignment as Reward-Guided Search"
      },
      {
        "paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5",
        "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"
      },
      {
        "paperId": "5708f725e13362da80a1062f51df118fca3529ab",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"
      },
      {
        "paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "paperId": "1eb1a8c7f88de27af224153f43ecdd41774600f2",
        "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization"
      },
      {
        "paperId": "58fdf550600fc3873729d466601c5d08a51ba8a0",
        "title": "Representation Engineering: A Top-Down Approach to AI Transparency"
      },
      {
        "paperId": "7fe071ea76e49bc3e573beb53f07721630954247",
        "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"
      },
      {
        "paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60",
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning"
      },
      {
        "paperId": "a2cec5152e3af9d99a4a3b8128fc247885b4b1c3",
        "title": "In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "405f8f5f1c6df1b3343c812832479aad5180b65f",
        "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"
      },
      {
        "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99",
        "title": "Aligning Large Language Models through Synthetic Feedback"
      },
      {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment"
      },
      {
        "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
      },
      {
        "paperId": "c76dd4a70361c3afd2e19d046343e2dedd16ecc3",
        "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "30c0cdc414f68211d5d0514df027cec22e005174",
        "title": "A Survey on In-context Learning"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
        "title": "Large Language Models Are Human-Level Prompt Engineers"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4",
        "title": "Learning To Retrieve Prompts for In-Context Learning"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      },
      {
        "paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "paperId": "c2e20d1a8763f927df05293e18c1f445782e1e81",
        "title": "Greedy Search"
      },
      {
        "paperId": null,
        "title": "2024a. The un-locking spell on base llms: Rethinking alignment via in-context learning"
      },
      {
        "paperId": null,
        "title": "performs directionless 1-step sampling multiple times"
      },
      {
        "paperId": null,
        "title": "General Instructions you give at the beginning can be detailed or long and should try to cover as many aspects/issues as possible"
      },
      {
        "paperId": null,
        "title": "Try to make some bullet points giving instructions/tips to the model on how to make the responses more engaging and human-like, like some pitfalls to avoid sounding robot-like"
      },
      {
        "paperId": null,
        "title": "What type of information or response format would best satisfy the user's needs?"
      },
      {
        "paperId": null,
        "title": "Please output your new system prompt in the format below by filling in the placeholders"
      },
      {
        "paperId": null,
        "title": "Try to list the model capabilities in the bullet points i.e mention that it is better to refuse to answer things it is not capable of 21908"
      },
      {
        "paperId": null,
        "title": "Factuality: Information provided must be accurate, truthful, and based on reliable sources, acknowledging any uncertainties where applicable"
      },
      {
        "paperId": null,
        "title": "Depth: The response should offer an appropriate level of detail and thoroughness, providing a comprehensive understanding of the topic"
      },
      {
        "paperId": null,
        "title": "system prompt]"
      },
      {
        "paperId": null,
        "title": "Based on all the information above, you need to design a new system prompt following the general guidelines"
      },
      {
        "paperId": null,
        "title": "When adding bullet points to the system prompt, do NOT add more than 2 bullet points at once"
      },
      {
        "paperId": null,
        "title": "Feel free to modify existing prompts, integrate freshly new instructions, or conceive a completely new one"
      },
      {
        "paperId": null,
        "title": "Try to make the bullent points of the prompt you design to be informative while being succinct"
      },
      {
        "paperId": null,
        "title": "2022. Gps: Genetic prompt search for efficient few-shot learning"
      },
      {
        "paperId": null,
        "title": "2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "paperId": null,
        "title": "2023a. Self-alignment with instruction back-translation"
      },
      {
        "paperId": null,
        "title": "Are there any potential challenges or limitations in providing a comprehensive response?"
      },
      {
        "paperId": null,
        "title": "What is the user's intent or goal in asking this question?"
      },
      {
        "paperId": null,
        "title": "Try to make some specific tips from the outputs and their evaluation you see above, you can list things to follow or to avoid to make the response better suited as per the evaluation remarks"
      },
      {
        "paperId": null,
        "title": "When deleting bullet points, do not remove bullet points which are relevant to overall goal but irrelevant to current query, instead modify/merge those"
      },
      {
        "paperId": null,
        "title": "A.2 Baselines Monte"
      },
      {
        "paperId": null,
        "title": "2023b. Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Try to make the system prompt balance out the quality across all aspects"
      },
      {
        "paperId": null,
        "title": "20 Table 7: All the hyper-parameters used by DRPO during ICL optimization and system prompt optimization"
      },
      {
        "paperId": null,
        "title": "An evaluation score of 5 in an aspect indicates the best quality, while a score of 1 indicates the worst quality"
      },
      {
        "paperId": null,
        "title": "AI@Meta. 2024."
      },
      {
        "paperId": null,
        "title": "Make sure the new system prompt is better than the current one"
      },
      {
        "paperId": null,
        "title": "relevant aspects for providing a high-quality response. Provide your reasoning for choosing these aspects. Output your analysis and aspect selection in the following JSON format:"
      },
      {
        "paperId": null,
        "title": "What is the main topic or subject of the query?"
      },
      {
        "paperId": null,
        "title": "Based on your analysis, select the most relevant aspects for providing a high-quality response. Provide your reasoning for choosing these aspects"
      },
      {
        "paperId": null,
        "title": "Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad"
      },
      {
        "paperId": null,
        "title": "Do NOT make more than 8 bullet points, if necessary add/modify/merge bullet points"
      },
      {
        "paperId": null,
        "title": "The prompt MUST be a general one suited for all kinds of queries, NOT specific to the current query"
      },
      {
        "paperId": null,
        "title": "W \u00d7 M \u00d7 D API calls to the optimizer LLM, for optimizing the ICL example"
      },
      {
        "paperId": null,
        "title": "2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      },
      {
        "paperId": null,
        "title": "Are there any potential ambiguities, uncertainties, or missing/wrong information in the query?"
      },
      {
        "paperId": null,
        "title": "W \u00d7 M \u00d7 D calls to base LLM B for response generation corresponding to each of the sampled prompt"
      },
      {
        "paperId": null,
        "title": "library, and analysis of step-by-step reasoning with large language models"
      }
    ],
    "cited_by": [
      {
        "paperId": "5cdcd77dd36a26ed3ea9da9cf8aac577a61b4c18",
        "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"
      },
      {
        "paperId": "e3bd804af8e1b42501bc79e7ac36418bdb6aef2a",
        "title": "SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models"
      },
      {
        "paperId": "e1e7b8eb65bf4eed85ce0fbf9949a84d432aadfa",
        "title": "What Makes a Good Natural Language Prompt?"
      },
      {
        "paperId": "cd4972ea9623f806baf3ce351b614fdb3e99c1bf",
        "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference"
      },
      {
        "paperId": "2acaa92ffb8a63a37590248b3b440aa3c7789763",
        "title": "CritiQ: Mining Data Quality Criteria from Human Preferences"
      }
    ],
    "score": 5.0
  },
  {
    "id": "e43fa63d5b79b65b8dc2abb7aa5bc903471be6f7",
    "title": "Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective",
    "authors": [
      "Ruichen Shao",
      "Bei Li",
      "Gangao Liu",
      "Yang Chen",
      "Xiang Zhou",
      "Jingang Wang",
      "Xunliang Cai",
      "Peng Li"
    ],
    "year": 2025,
    "citationCount": 5,
    "abstract": "Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \\url{https://github.com/LotuSrc/D2PO}.",
    "url": "https://www.semanticscholar.org/paper/e43fa63d5b79b65b8dc2abb7aa5bc903471be6f7",
    "pdf_url": "https://arxiv.org/pdf/2502.14340.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2025-02-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-14340",
      "ArXiv": "2502.14340",
      "DOI": "10.48550/arXiv.2502.14340",
      "CorpusId": 276482479
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4cb1ce04a44ac85ede834bb7eca4b67565f5da6f",
        "title": "Principled Foundations for Preference Optimization"
      },
      {
        "paperId": "bba9df17cf0452603e402f508da87ef311282c6b",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"
      },
      {
        "paperId": "7973a601c208c80ba8f9cd6af9751f0609a17605",
        "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization"
      },
      {
        "paperId": "a5558a4a7d24d6083a26fe287fa2e2d2337114f0",
        "title": "A Survey of Direct Preference Optimization"
      },
      {
        "paperId": "0ef09d611cd9108a905ef56feb99e1aadfbf4789",
        "title": "C2-DPO: Constrained Controlled Direct Preference Optimization"
      }
    ],
    "score": 5.0
  },
  {
    "id": "1271cc5f6eaecebecd0489c23c727b30ee7f6089",
    "title": "Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering",
    "authors": [
      "Hongyu Yang",
      "Liyang He",
      "Min Hou",
      "Shuanghong Shen",
      "Rui Li",
      "Jiahui Hou",
      "Jianhui Ma",
      "Junda Zhao"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response. Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities. Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.",
    "url": "https://www.semanticscholar.org/paper/1271cc5f6eaecebecd0489c23c727b30ee7f6089",
    "pdf_url": "https://arxiv.org/pdf/2406.00037.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-27",
    "externalIds": {
      "ArXiv": "2406.00037",
      "DBLP": "journals/corr/abs-2406-00037",
      "DOI": "10.48550/arXiv.2406.00037",
      "CorpusId": 270215340
    },
    "references": [
      {
        "paperId": "52d2f7326a1965ce84dd6d97c0a5400d85e837d0",
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "f6d8199f41015fe6cbba03dc513cfae5a9f663e8",
        "title": "Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "8db5eecad52ab0735ad0ebd0cda77dfba5c74689",
        "title": "SE-PQA: Personalized Community Question Answering"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
        "title": "Large Language Models are not Fair Evaluators"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae",
        "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"
      },
      {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "paperId": "31366ff634fc905affd78dbd8ddc9a872c006a87",
        "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"
      },
      {
        "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
        "title": "GLM-130B: An Open Bilingual Pre-trained Model"
      },
      {
        "paperId": "0a39442979d6e678dd36bb443ad529c14e86a86e",
        "title": "DocPrompting: Generating Code by Retrieving the Docs"
      },
      {
        "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
        "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
      },
      {
        "paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878",
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "f4df78183261538e718066331898ee5cad7cad05",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "paperId": "367b155174e0793d23f8992ebc0664e6102f7f02",
        "title": "Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning Approach for Semantic Code Search"
      },
      {
        "paperId": "b71c9b7383124bd37454b3e5d3b3b7de8defbf19",
        "title": "Multi-Relational Graph based Heterogeneous Multi-Task Learning in Community Question Answering"
      },
      {
        "paperId": "87e3c83d12b72afd5dd65ad409c63d234be38443",
        "title": "Recency and quality-based ranking question in CQAs: A Stack Overflow case study"
      },
      {
        "paperId": "7e5008713c404445dd8786753526f1a45b93de12",
        "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"
      },
      {
        "paperId": "870ff1dde0c103c3d90be51880f984628e77a8d6",
        "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"
      },
      {
        "paperId": "5461c2b704a84ed2eb77121bc2e4a4fe81ad8b9c",
        "title": "Generating Question Titles for Stack Overflow from Mined Code Snippets"
      },
      {
        "paperId": "fbe25e4f069a19dc63daca27b7c98cff338663b9",
        "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"
      },
      {
        "paperId": "6fb13d374dc32a55ab454fe9b8dd05527b553af9",
        "title": "Adapting Visual Question Answering Models for Enhancing Multimodal Community Q&A Platforms"
      },
      {
        "paperId": "f073e0d5ef816761310d9941d1c7379441ba432d",
        "title": "Toxic Code Snippets on Stack Overflow"
      },
      {
        "paperId": "bc34eee11d8ddba18a262a637ea336322c93ba6c",
        "title": "Enhancing Recurrent Neural Networks with Positional Attention for Question Answering"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "6b511adc8d3afe71ee33a18210494772aa64ce48",
        "title": "Answering Questions with Complex Semantic Constraints on Open Knowledge Bases"
      },
      {
        "paperId": "285c165c81fc9275955147a892b9a039ec8b1052",
        "title": "chrF: character n-gram F-score for automatic MT evaluation"
      },
      {
        "paperId": "06c36cf819fe183eb82cbad199da886d76827f26",
        "title": "It's all in the content: state of the art best answer prediction based on discretisation of shallow linguistic features"
      },
      {
        "paperId": "a26f89857a6021a516695f24d038d3402df78dcf",
        "title": "User profiling for answer quality assessment in Q&A communities"
      },
      {
        "paperId": "17470b756788bdc915cab08f225b78cc2f44ab62",
        "title": "Exploiting user feedback to learn to rank answers in q&a forums: a case study with stack overflow"
      },
      {
        "paperId": "1cd40f18d9a874d9838e5a367c68bf59805a079f",
        "title": "Answering questions about unanswered questions of Stack Overflow"
      },
      {
        "paperId": "b8f9539d145638ac93957da4f653e238a4e9d413",
        "title": "What makes a good code example?: A study of programming Q&A in StackOverflow"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "d0d3f4d1003db0fb637519ef5d8bb140e7df8355",
        "title": "May the source be with you."
      },
      {
        "paperId": "3da8684050df72c07731c030f7669a920ccfffde",
        "title": "An Introduction to the Theory of Statistics"
      },
      {
        "paperId": "c537a604fa3879208e6fc7fca8b4923d357faf28",
        "title": "A Tag-Based Transformer Community Question Answering Learning-to-Rank Model in the Home Improvement Domain"
      },
      {
        "paperId": "dabc331af4b6d19dbc6783f4f16924b994423d34",
        "title": "Towards a Two-Stage Method for Answer Selection and Summarization in Buddhism Community Question Answering"
      },
      {
        "paperId": null,
        "title": "Calibrate beforeuse:Improvingfew-shotperformanceoflanguagemodels"
      },
      {
        "paperId": null,
        "title": "Evaluatinglargelanguagemodelstrainedoncode"
      },
      {
        "paperId": null,
        "title": "Codeqa:Aquestionansweringdatasetforsource code comprehension"
      },
      {
        "paperId": null,
        "title": "Learning to summarize withhumanfeedback"
      },
      {
        "paperId": null,
        "title": "Evaluating text generation with bert"
      },
      {
        "paperId": null,
        "title": "andAhmedEHassan Anempiricalstudyofobsoleteanswersonstackoverflow"
      },
      {
        "paperId": null,
        "title": "Recurrent con-volutionalneuralnetworkforanswerselectionincommunityquestionanswering"
      },
      {
        "paperId": null,
        "title": "Learningtominealignedcodeandnaturallanguagepairsfromstackoverflow"
      },
      {
        "paperId": null,
        "title": "Predictingwebsearchersatisfactionwithexisting community-basedanswers"
      },
      {
        "paperId": null,
        "title": "Bleu: a method forautomaticevaluationofmachinetranslation"
      },
      {
        "paperId": null,
        "title": "Anewmeasureofrankcorrelation"
      },
      {
        "paperId": null,
        "title": "Analysemath\u00e9matiquesurlesprobabilit\u00e9sdeserreursdesituation d\u2019un point"
      },
      {
        "paperId": null,
        "title": "Cct5:Acode-change-orientedpre-trainedmodel"
      },
      {
        "paperId": null,
        "title": "model card and evaluations for claude models"
      },
      {
        "paperId": null,
        "title": ":Programmingcoderepresentationusingattentionneural networkwithoptimizedsubtreeextraction"
      }
    ],
    "cited_by": [
      {
        "paperId": "d59495494d2a4f1845bfea321be220906daa066f",
        "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning"
      },
      {
        "paperId": "23ec0b054e8fd92afa1cbf2e36d24b114b875905",
        "title": "Inducing Vulnerable Code Generation in LLM Coding Assistants"
      },
      {
        "paperId": "e0654ec5515ea8f8b7664cfe63767b8f24c2985b",
        "title": "How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks For LLMs"
      },
      {
        "paperId": "c52e066ff7ad98e7a7db5194f10c14d5666fc5f3",
        "title": "Unsupervised Human Preference Learning"
      },
      {
        "paperId": "310d19ba6c51eb7f123822515ed4abd72a27b3a5",
        "title": "How Should I Build A Benchmark? Revisiting Code-Related Benchmarks For LLMs"
      }
    ],
    "score": 5.0
  },
  {
    "id": "0d49552b54a1c2e064047d332018a898fcf6d9cb",
    "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
    "authors": [
      "Yekun Chai",
      "Haoran Sun",
      "Huang Fang",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to preferred outcomes. This hinders learning efficiency and slows convergence.In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7 ~ 2 times faster in terms of training time and continues to outperform it with further training. We make our code and data publicly available at https://github.com/ernie-research/MA-RLHF.",
    "url": "https://www.semanticscholar.org/paper/0d49552b54a1c2e064047d332018a898fcf6d9cb",
    "pdf_url": "https://arxiv.org/pdf/2410.02743.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-10-03",
    "externalIds": {
      "DBLP": "conf/iclr/ChaiSFWS025",
      "ArXiv": "2410.02743",
      "DOI": "10.48550/arXiv.2410.02743",
      "CorpusId": 273098444
    },
    "references": [
      {
        "paperId": "53d5d595b263f54c9a5c4d51e298413c450abb79",
        "title": "Curiosity-Driven Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      },
      {
        "paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "paperId": "e1e027639b54616217bde738033470dbf7d73d64",
        "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards"
      },
      {
        "paperId": "1d36dd3d333659d72df8eb5666edfd5cde54c84f",
        "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL"
      },
      {
        "paperId": "18e7ab056c16928d8f9539509a4b366889106d97",
        "title": "StarCoder 2 and The Stack v2: The Next Generation"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "c8168177d5cbd3a3d9ac5abdef7b694f388ec8f5",
        "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "6a0f1a8a03baba3e54a1a2ef348a1b0c2b8dff4b",
        "title": "B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis"
      },
      {
        "paperId": "311b5c770738fabc940b3b630664d562916df83c",
        "title": "Tool-Augmented Reward Modeling"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
        "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a669ea57529f4db630043c8c75d8f840c485d24d",
        "title": "RLTF: Reinforcement Learning from Unit Test Feedback"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "0a6bc37a07a37e3573d36e10cc11669eca0ff903",
        "title": "Execution-based Code Generation using Deep Reinforcement Learning"
      },
      {
        "paperId": "e1b732e02cd6f41e4e1eb793ec4b356cee2587f1",
        "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "ccb01a90b16b119db0201ed012f989ed48c68d9f",
        "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
        "title": "Measuring Coding Challenge Competence With APPS"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "ebf59587f8f170ff4241c42263bbfb9da5bd2135",
        "title": "ELI5: Long Form Question Answering"
      },
      {
        "paperId": "a6738f4f0ad70bf3c2252dd026e0e2823ee4f48c",
        "title": "On Reinforcement Learning for Full-length Game of StarCraft"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
      },
      {
        "paperId": "4ba25cb493ac7a03fc15d3b936257c9a6c689c1d",
        "title": "Strategic Attentive Writer for Learning Macro-Actions"
      },
      {
        "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
        "title": "Neural Machine Translation of Rare Words with Subword Units"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "79d222fe334713a445ecdeccbd7f7dba477d2739",
        "title": "Optimal Behavioral Hierarchy"
      },
      {
        "paperId": "ea2b8c200d4b045803e65620242603edb9a9bf01",
        "title": "Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations"
      },
      {
        "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
        "title": "Representation Learning: A Review and New Perspectives"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "036373f17e5e47bcadc289e6c57d61cf5e08fe3d",
        "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions"
      },
      {
        "paperId": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
        "title": "Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5826c9e69f49c4a900d4faa2f5093a6b6d962a06",
        "title": "A Heuristic Approach to the Discovery of Macro-Operators"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "4e02c6d40e24192f2bf6d945bfd1a76ab29367cb",
        "title": "Planning in a Hierarchy of Abstraction Spaces"
      },
      {
        "paperId": "dacc3a8d45968616f220628dc0db8d5d78c1a389",
        "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences"
      },
      {
        "paperId": "bc5752644753b117b897b9369e3826fe7831d93a",
        "title": "Planning with Closed Loop Macro Actions"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "4c7d74a4c31c5bfb1237c11969adb30c72c8f8c3",
        "title": "Macro-Actions in Reinforcement Learning: An Empirical Analysis"
      },
      {
        "paperId": "e41cda7b81cc49640210173fd45eb06cdbd6e824",
        "title": "Finding Structure in Reinforcement Learning"
      },
      {
        "paperId": "9c82d158af64352bffa8a3864a6d892e239e0f99",
        "title": "Learning to solve problems by searching for macro-operators"
      },
      {
        "paperId": null,
        "title": "section, we demonstrate some examples of validation sets to highlight the superiority of MA-PPO"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      },
      {
        "paperId": null,
        "title": "Position decayed assignment The contributions of each value function of tokens"
      },
      {
        "paperId": null,
        "title": "Introducing the next generation of Claude \u2014 anthropic.com"
      },
      {
        "paperId": null,
        "title": ": Open models based on gemini research and technology"
      },
      {
        "paperId": null,
        "title": "Equal assignment : We treats the contributions of each value function of tokens equally when considering the value function of macro actions, i.e. , \u03c3 \u03c4 = { 1 | \u03c9 \u03c4 | } \u03c4i =1"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ],
    "cited_by": [
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "859caa7e5a365c5c13d5fb0c614ac8d151126e29",
        "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
      },
      {
        "paperId": "cbb8faf4d2e529d5351a68c5f326bae42b95e825",
        "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation"
      },
      {
        "paperId": "53d5d595b263f54c9a5c4d51e298413c450abb79",
        "title": "Curiosity-Driven Reinforcement Learning from Human Feedback"
      }
    ],
    "score": 4.0
  },
  {
    "id": "d3cdb7c701821509290b6428f7b445885440729b",
    "title": "Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning with Human-AI Feedback",
    "authors": [
      "Avinash Anand",
      "Kritarth Prasad",
      "Chhavi Kirtani",
      "Ashwin R Nair",
      "Mohit Gupta",
      "Saloni Garg",
      "Anurag Gautam",
      "Snehal Buldeo",
      "R. Shah"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in text-based tasks but struggle with the complex reasoning required for physics problems, particularly in advanced arithmetic and conceptual understanding. While some research has explored ways to enhance LLMs in physics education using techniques such as prompt engineering and Retrieval Augmentation Generation (RAG), not enough effort has been made in addressing their limitations in physics reasoning. This paper presents a novel approach to improving LLM performance on physics questions using Reinforcement Learning with Human and Artificial Intelligence Feedback (RLHAIF). We evaluate several reinforcement learning methods, including Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Remax optimization. These methods are chosen to investigate RL policy performance with different settings on the PhyQA dataset, which includes challenging physics problems from high school textbooks. Our RLHAIF model, tested on leading LLMs like LLaMA2 and Mistral, achieved superior results, notably with the MISTRAL-PPO model, demonstrating marked improvements in reasoning and accuracy. It achieved high scores, with a 58.67 METEOR score and a 0.74 Reasoning score, making it a strong example for future physics reasoning research in this area.",
    "url": "https://www.semanticscholar.org/paper/d3cdb7c701821509290b6428f7b445885440729b",
    "pdf_url": "https://arxiv.org/pdf/2412.06827.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-12-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2412-06827",
      "ArXiv": "2412.06827",
      "DOI": "10.48550/arXiv.2412.06827",
      "CorpusId": 274610004
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "f7f89f5dc836ede4dee1a6b70dd48056f6543116",
        "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems"
      },
      {
        "paperId": "0024db5f46651b7e1dd593cc33e740bead341fa8",
        "title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning"
      },
      {
        "paperId": "cc66970b020e40867ad259d9b450433366e77048",
        "title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities"
      },
      {
        "paperId": "e947463aa1cc812a1e9697f92e04787cd129323d",
        "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning"
      }
    ],
    "score": 4.0
  },
  {
    "id": "197c91461c4f0bfc19d775f329607492ac80912f",
    "title": "Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs",
    "authors": [
      "Hao Sun",
      "Yunyi Shen",
      "Jean-Franccois Ton",
      "M. Schaar"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like chatbots and content generation -- through the process known as Reinforcement Learning from Human Feedback (RLHF) -- presents unique challenges. Reward models in RLHF are critical, acting as proxies that evaluate the alignment of LLM outputs with human intent. Despite advancements, the development of reward models is hindered by challenges such as computational heavy training, costly evaluation, and therefore poor reproducibility. We advocate for using embedding-based input in reward model research as an accelerated solution to those challenges. By leveraging embeddings for reward modeling, we can enhance reproducibility, reduce computational demands on hardware, improve training stability, and significantly reduce training and evaluation costs, hence facilitating fair and efficient comparisons in this active research area. We then show a case study of reproducing existing reward model ensemble research using embedding-based reward models. We discussed future avenues for research, aiming to contribute to safer and more effective LLM deployments.",
    "url": "https://www.semanticscholar.org/paper/197c91461c4f0bfc19d775f329607492ac80912f",
    "pdf_url": "https://arxiv.org/pdf/2502.04357.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-04357",
      "ArXiv": "2502.04357",
      "DOI": "10.48550/arXiv.2502.04357",
      "CorpusId": 276235965
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "2f444ac447a42b2d7197215ab02674d4ccaff65b",
        "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?"
      },
      {
        "paperId": "76d57ec8616a2f6d4bd5451a3344966b1c6dad5a",
        "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis"
      },
      {
        "paperId": "0a44db2f1035eedaba16b4d362437b3e6fe7ef26",
        "title": "LLMs are Also Effective Embedding Models: An In-depth Overview"
      }
    ],
    "score": 4.0
  },
  {
    "id": "503c85a9df91de5dace92d6c5ade8627701f08ac",
    "title": "Large language model empowered participatory urban planning",
    "authors": [
      "Zhilun Zhou",
      "Yuming Lin",
      "Yong Li"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.",
    "url": "https://www.semanticscholar.org/paper/503c85a9df91de5dace92d6c5ade8627701f08ac",
    "pdf_url": "https://arxiv.org/pdf/2402.01698.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-01-24",
    "externalIds": {
      "DBLP": "journals/corr/abs-2402-01698",
      "ArXiv": "2402.01698",
      "DOI": "10.48550/arXiv.2402.01698",
      "CorpusId": 267412207
    },
    "references": [
      {
        "paperId": "1cf926a653d0c125c5d323446810ef775ce92b2b",
        "title": "AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "3784fd84b61d482b52f7ef72aac66bcb886b892b",
        "title": "Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models"
      },
      {
        "paperId": "d9fe3101f06d689614734f0318ddc3c407f66663",
        "title": "Artificial intelligence enabled participatory planning: a review"
      },
      {
        "paperId": "9fcdbfdf28245010c875ce85502351fe05c04b49",
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View"
      },
      {
        "paperId": "13a8bb0ec2af69c80973d7896c7a7a1ab3bbf1f5",
        "title": "Augmented reality participatory platform: A novel digital participatory planning tool to engage under-resourced communities in improving neighborhood walkability"
      },
      {
        "paperId": "f83414bc01bfe1a661eaded82fb772cdac02202c",
        "title": "Spatial planning of urban communities via deep reinforcement learning"
      },
      {
        "paperId": "9ea0757c750ab1222a7442d3485a74d1c526b04c",
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"
      },
      {
        "paperId": "ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"
      },
      {
        "paperId": null,
        "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework"
      },
      {
        "paperId": "aa23bcf357fc4f890e0a97c27e254d14fbacd460",
        "title": "Role play with large language models"
      },
      {
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior"
      },
      {
        "paperId": "3efb84fcf7788c12cd7bc11bd4b3cb683f8e9182",
        "title": "A systematic literature review on public participation in decision-making for local authority planning: A decade of progress and challenges"
      },
      {
        "paperId": "8c1d6202b6505c11204d8fa4c44f8a438bb17fb4",
        "title": "ChatGPT and antimicrobial advice: the end of the consulting infection doctor?"
      },
      {
        "paperId": "0bfc05adcddd4fe5d1335d96cc313c41526d4558",
        "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT"
      },
      {
        "paperId": "0697eede7e5a51475bdcb08113ac800f0d32cfbb",
        "title": "Is the rapid development of visualization techniques enhancing the quality of public participation in natural resource policy and management? A systematic review"
      },
      {
        "paperId": "3012416c5365797be0cfaa90af28707e9bd8ca29",
        "title": "A participatory e-planning model in the urban renewal of China: Implications of technologies in facilitating planning participation"
      },
      {
        "paperId": "5ee1024949c8faa312677fe5d034aa9be0861fe1",
        "title": "Urban-GAN: An artificial intelligence-aided computation system for plural urban design"
      },
      {
        "paperId": "78366aed9fbcaba32f88002faa97699a022b7280",
        "title": "Deep Human-guided Conditional Variational Generative Modeling for Automated Urban Planning"
      },
      {
        "paperId": "5fd6302ee95021a261b97bb0815db04c89827da2",
        "title": "Citizen-Centered Design in Urban Planning: How Augmented Reality can be used in Citizen Participation Processes"
      },
      {
        "paperId": "475381dae198cd15b099cef732bf6b1253e33293",
        "title": "Using Decision Support System to Enable Crowd Identify Neighborhood Issues and Its Solutions for Policy Makers: An Online Experiment at Kabul Municipal Level"
      },
      {
        "paperId": "ed3e17119311f27bc4dc1f7cbae896a89e25a208",
        "title": "Opening the black box of participatory planning: a study of how planners handle citizens\u2019 input"
      },
      {
        "paperId": "346f70aaf83e878b32ee5a085e642094c90be4d8",
        "title": "Introducing the \u201c15-Minute City\u201d: Sustainability, Resilience and Place Identity in Future Post-Pandemic Cities"
      },
      {
        "paperId": "21766c95b8853749099f16a9ae743c710e6ca5ee",
        "title": "Towards the collaborative development of machine learning techniques in planning support systems \u2013 a Sydney example"
      },
      {
        "paperId": "ff6603f0eb8f69cbfc25ef2cbb369b2523d71ada",
        "title": "Collaborative workshop and community participation: A new approach to urban regeneration in China"
      },
      {
        "paperId": "0adae4a6327faa2bb1adf7ac4d40b9778f5e72ca",
        "title": "Participatory Urban Planning: What Would Make Planners Trust the Citizens?"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "6d0365d395a8be890be54d05adaf132d0452f7b1",
        "title": "Participatory planning practice in rural Indonesia: A sustainable development goals-based evaluation"
      },
      {
        "paperId": "4ba79ac0bfd78b58d65b38c0285561f3bc52c839",
        "title": "A review and reframing of participatory urban dashboards"
      },
      {
        "paperId": "ed21af4819fdfdc0af30b9526556d42db1576bf3",
        "title": "Integrating urban analysis, generative design, and evolutionary optimization for solving urban design problems"
      },
      {
        "paperId": "53d5485f100f0eab2188af9625c51e5f883618a9",
        "title": "Online Participatory Technologies: Opportunities and Challenges for Enriching Participatory Planning"
      },
      {
        "paperId": "206dbb18149efb1d9ce93f6a5beaf2688a6c44d7",
        "title": "A demonstrator tool of web-based virtual reality for participatory evaluation of urban sound environment"
      },
      {
        "paperId": "726079468c38e66937265785a5845ede740b8900",
        "title": "Tokenism or Political Activism? Some Reflections on Participatory Planning"
      },
      {
        "paperId": "7282f70e8bd69ab532c737b57e14d2ac20707fa1",
        "title": "Planning in the Face of Power."
      },
      {
        "paperId": "b5280ed2f9b3880fa505d93fbf140b9be8572d03",
        "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems"
      },
      {
        "paperId": "7ca954844bc1dd405bc43445b1c990e42d865095",
        "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society"
      },
      {
        "paperId": "773e24ed7c861feab8886ce742f18206f9cdb4cb",
        "title": "Experiencing virtual geographic environment in urban 3D participatory e-planning: A user perspective"
      },
      {
        "paperId": "bc69e3d8569c0b8c2c906e4d9045ed1d8dfeada4",
        "title": "Ladder of Citizen Participation"
      },
      {
        "paperId": "f3f61fbf889cd505997c4dec8ca46bfc9d484783",
        "title": "The deliberative practitioner: Encouraging participatory planning processes."
      },
      {
        "paperId": null,
        "title": "Beijing Municipal Bureau of Statistics, Survey Office of the National Bureau of Statistics in Bejing"
      },
      {
        "paperId": null,
        "title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors (2023)"
      }
    ],
    "cited_by": [
      {
        "paperId": "df76bc7a1d832d6d61deb82889e231f4411a6316",
        "title": "Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin"
      },
      {
        "paperId": "a4083e0cd128b92df8805d99b3d1a82688a3efd3",
        "title": "UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models"
      },
      {
        "paperId": "9d4bd6f057fde94e2956a14711655e9044257044",
        "title": "From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility"
      },
      {
        "paperId": "439dbc18821610ea5962c6f259f18443cc0c10c1",
        "title": "ItiNera: Integrating Spatial Optimization with Large Language Models for Open-domain Urban Itinerary Planning"
      }
    ],
    "score": 4.0
  },
  {
    "id": "ba0a50c7eed827ff18adce2ff5248df65e5c1e06",
    "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
    "authors": [
      "Kuang-Ming Chen",
      "Hung-yi Lee"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "The rapid development of large language models (LLMs) in recent years has largely focused on English, resulting in models that respond exclusively in English. To adapt these models to other languages, continual pre-training (CP) is often employed, followed by supervised fine-tuning (SFT) to maintain conversational abilities. However, CP and SFT can reduce a model's ability to filter harmful content. We propose Instruction Continual Pre-training (InsCP), which integrates instruction tags into the CP process to prevent loss of conversational proficiency while acquiring new languages. Our experiments demonstrate that InsCP retains conversational and Reinforcement Learning from Human Feedback (RLHF) abilities. Empirical evaluations on language alignment, reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably, this approach requires only 0.1 billion tokens of high-quality instruction-following data, thereby reducing resource consumption.",
    "url": "https://www.semanticscholar.org/paper/ba0a50c7eed827ff18adce2ff5248df65e5c1e06",
    "pdf_url": "https://arxiv.org/pdf/2405.20175.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-30",
    "externalIds": {
      "ArXiv": "2405.20175",
      "DBLP": "journals/corr/abs-2405-20175",
      "DOI": "10.48550/arXiv.2405.20175",
      "CorpusId": 270123084
    },
    "references": [
      {
        "paperId": "89e13c80ff90c6e2f2dda700b5dd6c3be1aabf7d",
        "title": "Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities"
      },
      {
        "paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba",
        "title": "Yi: Open Foundation Models by 01.AI"
      },
      {
        "paperId": "14394ae5ee9153b78266cbc53a04dd6ecc4d1a4e",
        "title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "1cff5549753a518ed9d6a3517b5050968d710b27",
        "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "paperId": "16c42a500bff0af731ee505aae23aa4d2937fc91",
        "title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite"
      },
      {
        "paperId": "193955704f66923ac20a664bd184ed4663b2bdf9",
        "title": "Continual Pre-Training of Large Language Models: How to (re)warm your model?"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
      },
      {
        "paperId": "e5adc219685c9941b9a3d029480af4a51c0ea05a",
        "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"
      },
      {
        "paperId": "e3ec55e9e6720194a0ed5d4033d93a941c8a4f99",
        "title": "Continual Pre-training of Language Models"
      },
      {
        "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "ce3b364b7e6358940ce97d8d5887a65e5024ca21",
        "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      },
      {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "title": "Universal Language Model Fine-tuning for Text Classification"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5feb32a73dd1bd9e13f84a7b3344497a5545106b",
        "title": "FastText.zip: Compressing text classification models"
      },
      {
        "paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd",
        "title": "Bag of Tricks for Efficient Text Classification"
      },
      {
        "paperId": "bcdc102c04fb0e7d4652e8bcc7edd2983bb9576d",
        "title": "VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text"
      },
      {
        "paperId": null,
        "title": "A framework for few-shot language model evaluation"
      },
      {
        "paperId": null,
        "title": "Measuring massive multi-task language understanding"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": null,
        "title": "OpenAI. 2023."
      },
      {
        "paperId": null,
        "title": "2022. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection"
      },
      {
        "paperId": null,
        "title": "2023 Language models for taiwanese culture"
      },
      {
        "paperId": null,
        "title": "YuLan-Team"
      }
    ],
    "cited_by": [
      {
        "paperId": "5932a5bbdedb480207acbef84e3f0703f116de2e",
        "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training"
      },
      {
        "paperId": "9c05e3ed401327a64b321111c71cb2ff2d8cfb53",
        "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models"
      },
      {
        "paperId": "3bd76bd5d8e82738b49b186b345709ca282697e0",
        "title": "\"Vorbe\\c{s}ti Rom\\^ane\\c{s}te?\"A Recipe to Train Powerful Romanian LLMs with English Instructions"
      },
      {
        "paperId": "997e18f665aaec7f7a1e3b9603e26199efe87998",
        "title": "\"Vorbe\u015fti Rom\u00e2ne\u015fte?\" A Recipe to Train Powerful Romanian LLMs with English Instructions"
      }
    ],
    "score": 4.0
  },
  {
    "id": "365b5f8439ccd558de22c7fbb229a380c8ea423f",
    "title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback",
    "authors": [
      "Yang Chen",
      "Yufan Shen",
      "Wenxuan Huang",
      "Sheng Zhou",
      "Qunshu Lin",
      "Xinyu Cai",
      "Zhi Yu",
      "Jiajun Bu",
      "Botian Shi",
      "Yu Qiao"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework, ``Reasoning-Rendering-Visual-Feedback''(RRVF), that enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the ``Asymmetry of Verification''principle, i.e., verifying the rendered output against the source image is substantially easier than performing deep visual reasoning to generate a faithful, structured representation such as code. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL), thereby reducing reliance on image-text supervision. RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform complex reasoning, including self-correction through multi-turn interactions. This process is optimized end-to-end using the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing similarly sized open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization. Notably, the model outperforms the more advanced MLLM used to generate visual feedback during training. Code is available at https://github.com/L-O-I/RRVF.",
    "url": "https://www.semanticscholar.org/paper/365b5f8439ccd558de22c7fbb229a380c8ea423f",
    "pdf_url": "https://arxiv.org/pdf/2507.20766.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-07-28",
    "externalIds": {
      "DBLP": "journals/corr/abs-2507-20766",
      "ArXiv": "2507.20766",
      "DOI": "10.48550/arXiv.2507.20766",
      "CorpusId": 280323645
    },
    "references": [
      {
        "paperId": "cc8482389037a05467e518283d32fc6d555c542c",
        "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders"
      },
      {
        "paperId": "c79ebb4c82ddd2aec1a6ee3d7d912959e2e8fea1",
        "title": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning"
      },
      {
        "paperId": "ec58421730a102228fd3db9b2e80e88914f5d17d",
        "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction"
      },
      {
        "paperId": "c33243ab13e7331e2ce328c78923d802510bbaaf",
        "title": "Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward"
      },
      {
        "paperId": "731861e42ed3743c46f511c5b8dc303d7cca636f",
        "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use"
      },
      {
        "paperId": "343b70582fce012903a42137bc664009f3c29e0b",
        "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning"
      },
      {
        "paperId": "da608eaf47596938dd80f6bd977610ca07b467e9",
        "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning"
      },
      {
        "paperId": "e26af4bcb5ad41e875e074bf5b361a71382e8ab6",
        "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning"
      },
      {
        "paperId": "c08fa8d84104ec1a8304f75b72bed411100aaf5c",
        "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning"
      },
      {
        "paperId": "cddf14e5b97090111d3fa814c9aec60e2bf24b8a",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "paperId": "d981ce332586e6a29f595cbdfa9347cf425e5cd0",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
      },
      {
        "paperId": "edd5d458e230bf36c2dddcc634995c454e2b9e93",
        "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey"
      },
      {
        "paperId": "2464e287e6f92738fd5627819597172e8674e36b",
        "title": "Visual-RFT: Visual Reinforcement Fine-Tuning"
      },
      {
        "paperId": "a99dee9602e21a71526b9681d8dba37c55b66941",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "6cf64f544140a2f27bc0851fee2506aaf1e58723",
        "title": "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation"
      },
      {
        "paperId": "89409d752884b3e50ec5640466ab5857e6b3e1e2",
        "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding"
      },
      {
        "paperId": "27aff3445f8213e9dd090edf0e28b475001a1db9",
        "title": "CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?"
      },
      {
        "paperId": "1a71f7b216b710b936da666027014adb83af8e7a",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "paperId": "6049081b1422e160c82b1e255c4315aaec94a740",
        "title": "ProcTag: Process Tagging for Assessing the Efficacy of Document Instruction Data"
      },
      {
        "paperId": "250043fdae97d2ac87245a44923682e7a3decd50",
        "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation"
      },
      {
        "paperId": "01771d36e818c1355de7885a78e69eb274314535",
        "title": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding"
      },
      {
        "paperId": "ee8c1a46c90f1261c23479e15c6bed7f67ad8943",
        "title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning"
      },
      {
        "paperId": "a1a2d2512e0753006f34935cea17bd870d210d24",
        "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM"
      },
      {
        "paperId": "29af66d6eefbb0117c0bdfbe1eef84727b70d3b6",
        "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback"
      },
      {
        "paperId": "6e754273d54a91371efbc928cd6b156364d517da",
        "title": "ViperGPT: Visual Inference via Python Execution for Reasoning"
      },
      {
        "paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
        "title": "Multimodal Chain-of-Thought Reasoning in Language Models"
      },
      {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9858b40d23d329151f202b76aac7ca515dee5913",
        "title": "pix2code: Generating Code from a Graphical User Interface Screenshot"
      },
      {
        "paperId": "79d7246908bc1ec870a02f45946c2b2588c5d9bb",
        "title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots"
      },
      {
        "paperId": "c7a66961ee07b0e9e792d3625c1b20d510f29429",
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"
      },
      {
        "paperId": null,
        "title": "Reinforcement learning: An introduction , volume 1"
      },
      {
        "paperId": null,
        "title": "2025. Gemini-2-5-model-family"
      },
      {
        "paperId": null,
        "title": "2024b. Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models"
      },
      {
        "paperId": null,
        "title": "2024. GeminiProVision"
      },
      {
        "paperId": null,
        "title": "2024. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models"
      },
      {
        "paperId": null,
        "title": "Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2023. Toolformer: Language models can teach themselves to use tools, 2023"
      },
      {
        "paperId": null,
        "title": "2025. Seed1.5-VL"
      },
      {
        "paperId": null,
        "title": "2025. The Asymmetry of Verification, and Verifier\u2019s Law"
      },
      {
        "paperId": null,
        "title": "2025b. VRAG-RL: Em-power Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Rein-forcement Learning"
      },
      {
        "paperId": null,
        "title": "2023. Visual instruction tuning"
      },
      {
        "paperId": null,
        "title": "2022. Constitutional ai: Harmlessness from ai feed-back"
      },
      {
        "paperId": null,
        "title": "2025c. Thinking with Images for Multi-modal Reasoning: Foundations, Methods, and Future Fron-tiers"
      },
      {
        "paperId": null,
        "title": "2025a. VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
      },
      {
        "paperId": null,
        "title": "OpenAI"
      }
    ],
    "cited_by": [
      {
        "paperId": "77eb362f6911b24d17c0e167774321df3f769c6c",
        "title": "SAIL-VL2 Technical Report"
      },
      {
        "paperId": "596d9c53f1f9b98facbb6b97cff9afb120103835",
        "title": "Reinforced Visual Perception with Tools"
      },
      {
        "paperId": "23ce682976ca6aa62a7984058a9a8d52aa9cabd9",
        "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation"
      },
      {
        "paperId": "82ac28957506e41f056a86b2d081c874dfd26ad7",
        "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning"
      }
    ],
    "score": 4.0
  },
  {
    "id": "72789bb011045e4230834b2df0e3922f2104f8fa",
    "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback",
    "authors": [
      "Ning Wang",
      "Bingkun Yao",
      "Jie Zhou",
      "Yuchen Hu",
      "Xi Wang",
      "Nan Guan",
      "Zhe Jiang"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Large language models (LLMs) have shown strong performance in Verilog generation from natural language description. However, ensuring the functional correctness of the generated code remains a significant challenge. This paper introduces a method that integrates verification insights from testbench into the training of Verilog generation LLMs, aligning the training with the fundamental goal of hardware design: functional correctness. The main obstacle in using LLMs for Verilog code generation is the lack of sufficient functional verification data, particularly testbenches paired with design specifications and code. To address this problem, we introduce an automatic testbench generation pipeline that decomposes the process and uses feedback from the Verilog compiler simulator (VCS) to reduce hallucination and ensure correctness. We then use the testbench to evaluate the generated codes and collect them for further training, where verification insights are introduced. Our method applies reinforcement learning (RL), specifically direct preference optimization (DPO), to align Verilog code generation with functional correctness by training preference pairs based on testbench outcomes. In evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2, and VerilogEval v2, our approach consistently outperforms state-of-the-art baselines in generating functionally correct Verilog code. We open source all training code, data, and models at https://anonymous.4open.science/r/VeriPrefer-E88B.",
    "url": "https://www.semanticscholar.org/paper/72789bb011045e4230834b2df0e3922f2104f8fa",
    "pdf_url": "https://arxiv.org/pdf/2504.15804.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-04-22",
    "externalIds": {
      "DBLP": "journals/corr/abs-2504-15804",
      "ArXiv": "2504.15804",
      "DOI": "10.48550/arXiv.2504.15804",
      "CorpusId": 277994021
    },
    "references": [
      {
        "paperId": "2374bd24a499a88da87db763b3b2ee939178067f",
        "title": "RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code Generation Technique"
      },
      {
        "paperId": "4b2e5a6f64fd88a2f97266acbacad309a7a9ea4a",
        "title": "HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned with HDL Engineers"
      },
      {
        "paperId": "e07a7e57745b69c0f08601fbdd19cfdcafbe1152",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "paperId": "e18a0ef6c6e8b7b0c9d685f86b05a841e3473a88",
        "title": "PyraNet: A Multi-Layered Hierarchical Dataset for Verilog"
      },
      {
        "paperId": "cdef62e8c6298bb861dbba96eba658e1c05d00ed",
        "title": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL Generation: Invited Paper"
      },
      {
        "paperId": "7943ec4a67151a559b25cd34369e661c9a7924c8",
        "title": "GPT-4o System Card"
      },
      {
        "paperId": "11dd2fc88747d20629f5aafab72ba649d93e969c",
        "title": "Qwen2.5-Coder Technical Report"
      },
      {
        "paperId": "8b0a3671a5fa891107c9974276bf80579b9347fe",
        "title": "Revisiting VerilogEval: A Year of Improvements in Large-Language Models for Hardware Code Generation"
      },
      {
        "paperId": "044a340ece2e7d0e0a15186e64cd766f06a6e015",
        "title": "VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "d590275ab5738a8d771464b3da67ffc0014bbef7",
        "title": "OriGen: Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection"
      },
      {
        "paperId": "bf416f2210f2c6270aac7d42094e22d6e097f61a",
        "title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs"
      },
      {
        "paperId": "01526c6fe95ef3716048181aa3b7edb151429dff",
        "title": "CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization"
      },
      {
        "paperId": "86905d23c2d1c9386f3eb063132c14119ed79e5c",
        "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases"
      },
      {
        "paperId": "09e2391ab4c266f96c71cc379c1a7c7ddd40ee33",
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers"
      },
      {
        "paperId": "bfeba7fc78b16da9c23c91d280e56f13e936e04d",
        "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance"
      },
      {
        "paperId": "4e7cdd83f844044c2661204602bac108e2ccf600",
        "title": "IRCoCo: Immediate Rewards-Guided Deep Reinforcement Learning for Code Completion"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "cbd81507197e2d32b6f6c8ac99039f3a607ee8f1",
        "title": "VerilogEval: Evaluating Large Language Models for Verilog Code Generation"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "71bc0c97c20fffce796a355b16bd202987260029",
        "title": "A Survey of Hallucination in Large Foundation Models"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "079be8c8a93fc80274ff22251a3dac9804bec66a",
        "title": "RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model"
      },
      {
        "paperId": "b69882cb6c7367ea91224c27982277c5e643862e",
        "title": "VeriGen: A Large Language Model for Verilog Code Generation"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "352420ee61a8da783ca7750170793613b18b8d9c",
        "title": "Tool Learning with Foundation Models"
      },
      {
        "paperId": "0a6bc37a07a37e3573d36e10cc11669eca0ff903",
        "title": "Execution-based Code Generation using Deep Reinforcement Learning"
      },
      {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      },
      {
        "paperId": "07955e96cbd778d0ae2a68f09d073b866dd84c2a",
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks"
      },
      {
        "paperId": "5437e8adab596d7294124c0e798708e050e25321",
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "19ca4758d28cd880eeaddb29f62919a31643f914",
        "title": "Pyverilog: A Python-Based Hardware Design Processing Toolkit for Verilog HDL"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": null,
        "title": "Claude 3 sonnet"
      },
      {
        "paperId": "f366189394335b7b2ff9a330e53bf58127e78ee1",
        "title": "Large Language Model for Verilog Generation with Golden Code Feedback"
      },
      {
        "paperId": "438a24301883f2985bd04d2ade4114ec0080c1f4",
        "title": "Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation"
      },
      {
        "paperId": null,
        "title": "Describe the primary function and purpose of the module IN DETAIL. List each primary function and describe it. Input and output port descriptions"
      },
      {
        "paperId": null,
        "title": "Explain the high-level operation of the module IN DETAIL, and describe how inputs are processed to generate outputs"
      }
    ],
    "cited_by": [
      {
        "paperId": "3043778b2a9f102b7ffc47f3cf02d44538462d7e",
        "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation"
      },
      {
        "paperId": "175e59e7e2ff6e37db8c7bb46611c6ed3af1115f",
        "title": "iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs"
      },
      {
        "paperId": "e409379472112f66fda5db2396a8f6c5e980a94e",
        "title": "Abstractions-of-Thought: Intermediate Representations for LLM Reasoning in Hardware Design"
      },
      {
        "paperId": "4e93a95959f877fdee4c50f82be2d8fe5ea26f3b",
        "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs"
      }
    ],
    "score": 4.0
  },
  {
    "id": "d05777760939dd4566b0777a750401f008546539",
    "title": "On Teacher Hacking in Language Model Distillation",
    "authors": [
      "D. Tiapkin",
      "Daniele Calandriello",
      "Johan Ferret",
      "Sarah Perrin",
      "Nino Vieillard",
      "Alexandre Ram'e",
      "Mathieu Blondel"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.",
    "url": "https://www.semanticscholar.org/paper/d05777760939dd4566b0777a750401f008546539",
    "pdf_url": "https://arxiv.org/pdf/2502.02671.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-02671",
      "ArXiv": "2502.02671",
      "DOI": "10.48550/arXiv.2502.02671",
      "CorpusId": 276116870
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "dbb710a1396c5fde8d970039de5afa6968e562db",
        "title": "Citation Recognition in Large-Scale Legal Platforms Using Transformer Models"
      },
      {
        "paperId": "5cfa51b378a6e2ac622c2d0c9eabd36e7bdd574e",
        "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation"
      },
      {
        "paperId": "cfa20867844554f0e95246cd91be99270f5be5e4",
        "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models"
      },
      {
        "paperId": "e949b844426c772d4f83ce286462fd76a5b484fd",
        "title": "Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching"
      }
    ],
    "score": 4.0
  },
  {
    "id": "f52e2a8ec86afffb9ff4153fc14b3bd8b53d2c3c",
    "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning",
    "authors": [
      "Longxi Gao",
      "Li Zhang",
      "Mengwei Xu"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Training effective Vision Language Models (VLMs) for GUI agents typically relies on supervised fine-tuning (SFT) over large-scale annotated datasets, where the collection process is labor-intensive and error-prone. In this work, we propose a self-supervised inverse dynamics task to enable VLMs to learn from GUI transition pairs by inferring the action that caused that transition. This training task offers two advantages: (1) It enables VLMs to ignore variations unrelated to user actions (e.g., background refreshes, ads) and to focus on true affordances such as buttons and input fields within complex GUIs. (2) The training data can be easily obtained from existing GUI trajectories without requiring human annotation, and it can be easily scaled through automatic offline exploration. Using this training task, we propose UI-shift, a framework for enhancing VLM-based GUI agents through self-supervised reinforcement learning (RL). With only 2K training samples sourced from existing datasets, two VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve competitive or superior performance on grounding tasks (ScreenSpot-series benchmarks) and GUI automation tasks (AndroidControl), compared to SFT baselines and GUI-specific models that explicitly elicit reasoning abilities during RL. Our findings suggest a potential direction for enhancing VLMs for GUI agents by leveraging more self-supervised training data in the future. Code, model, and data are available at: https://github.com/UbiquitousLearning/UIShift",
    "url": "https://www.semanticscholar.org/paper/f52e2a8ec86afffb9ff4153fc14b3bd8b53d2c3c",
    "pdf_url": "https://arxiv.org/pdf/2505.12493.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-18",
    "externalIds": {
      "ArXiv": "2505.12493",
      "DBLP": "journals/corr/abs-2505-12493",
      "DOI": "10.48550/arXiv.2505.12493",
      "CorpusId": 278740881
    },
    "references": [
      {
        "paperId": "42746aa9d19b4108d9de5324a2a90510cc29baea",
        "title": "ScaleTrack: Scaling and back-tracking Automated GUI Agents"
      },
      {
        "paperId": "b7c5ad386f20954334a539c0558309a5a5bc311d",
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners"
      },
      {
        "paperId": "b12b7e05de83d3a3b345cf9392e79a03f8c2712e",
        "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents"
      },
      {
        "paperId": "983a428780512ea2ed0b176e11da9c591bd97b76",
        "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning"
      },
      {
        "paperId": "df7b286e53d66c6881966b5e6904f7fa5e6d774b",
        "title": "ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use"
      },
      {
        "paperId": "20b6389d663fec1c919a2ea364901e9732fb08db",
        "title": "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study"
      },
      {
        "paperId": "7381c26460377cbf29653f7cbd69f3f6af00f2fe",
        "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment"
      },
      {
        "paperId": "f61cc9b5583c6295d5cd756ec0f34e4c003aab29",
        "title": "Qwen2.5-VL Technical Report"
      },
      {
        "paperId": "f662f14c51de7c91e4e3191f1f2d8271c68040a0",
        "title": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation"
      },
      {
        "paperId": "0c989f2c896e4ad5032b3c87abff8f7ba1f89c7a",
        "title": "Aria-UI: Visual Grounding for GUI Instructions"
      },
      {
        "paperId": "58ae4a85ae73095da668d6eac3a8c1f19270c89d",
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction"
      },
      {
        "paperId": "02128b757f31f1385c9b839201783ceae5cd2088",
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent"
      },
      {
        "paperId": "9ceadd197cea2cd1b45e12983d528daf2b24efaa",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents"
      },
      {
        "paperId": "a15a4cabe7653a87efc3302552fbc88b25d19dc1",
        "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents"
      },
      {
        "paperId": "9d851cd329dbcb5b4fc6a56c277303cfbd9c646e",
        "title": "Small Language Models: Survey, Measurements, and Insights"
      },
      {
        "paperId": "cd17dc060ca82b3014f375cb8162fd0949d59337",
        "title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding"
      },
      {
        "paperId": "c5d80264e5c68331fc1e06aed1e6e63cac35393b",
        "title": "MobileViews: A Large-Scale Mobile GUI Dataset"
      },
      {
        "paperId": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
        "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone"
      },
      {
        "paperId": "c443fddfce5b587a11e8074f548633304b4cd9a5",
        "title": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices"
      },
      {
        "paperId": "d8ba1f488269420ed8440c31dfa987b8782c62f1",
        "title": "On the Effects of Data Scale on UI Control Agents"
      },
      {
        "paperId": "d1b2eaf7aaebbd3d847272da04be180e35c7b68b",
        "title": "AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "f9b39a6a7e40986b46f7796f3a805d70d7e3931a",
        "title": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents"
      },
      {
        "paperId": "06d860a5bbb99a4eafdbbb2d5f6aa8dd5fd32cf4",
        "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security"
      },
      {
        "paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
      },
      {
        "paperId": "61c723e7d91ca308b9eeacab35a2fce7548ef9fc",
        "title": "AppAgent: Multimodal Agents as Smartphone Users"
      },
      {
        "paperId": "e50583008fe4ec049e42fdc01727ce98f6d86a35",
        "title": "CogAgent: A Visual Language Model for GUI Agents"
      },
      {
        "paperId": "cb23a59fdf3ade707600f076df4ff27a03941fba",
        "title": "AutoDroid: LLM-powered Task Automation in Android"
      },
      {
        "paperId": "64a98f9c40ef559117324e6d92417e8f5173b694",
        "title": "Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation"
      },
      {
        "paperId": "638107136cd71057855a1d393c07923394b0e40f",
        "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning"
      },
      {
        "paperId": "0b4a8b1c98bd13ce0a6281bb1af3761f7f887235",
        "title": "Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements"
      },
      {
        "paperId": "775f7845e4df2576762960943294bd28733e2046",
        "title": "Rico: A Mobile App Dataset for Building Data-Driven Design Applications"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "ac708feeee1a8b592122b397fd9a06c6b0ee9b66",
        "title": "Inverse dynamics with rigid contact and friction"
      },
      {
        "paperId": null,
        "title": "Ui-r1: Enhancing action prediction of gui agents by reinforcement learning"
      },
      {
        "paperId": "9a179bc6e826e2e1249abf573262bf6dfbd81fa1",
        "title": "LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Automation Task Evaluation"
      },
      {
        "paperId": null,
        "title": "2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      },
      {
        "paperId": null,
        "title": "2023. Androidinthewild: A large-scale dataset for android device control"
      },
      {
        "paperId": null,
        "title": "2024. T \\ \" ulu 3: Pushing frontiers in open language model post-training"
      },
      {
        "paperId": null,
        "title": "2024. CogVLM: Visual Expert for Pretrained Language Models"
      },
      {
        "paperId": null,
        "title": "2025. Vlm-r1: A stable and generalizable r1-style large vision-language model"
      }
    ],
    "cited_by": [
      {
        "paperId": "81318e14f429d38c894622bfce8fdf6a8d3501c1",
        "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"
      },
      {
        "paperId": "9a5071bacb2bcb98ddd95076dbd446240b4da2c6",
        "title": "Reinforcement Learning in Vision: A Survey"
      },
      {
        "paperId": "7c98d427cd7a9f0df620f81e3829fe75761c2ee1",
        "title": "NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks"
      },
      {
        "paperId": "ac9fbe3af000a2f09099cf282afdac0aeb68e859",
        "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey"
      }
    ],
    "score": 4.0
  },
  {
    "id": "d47aae06d20ea0189adad9b2c8184b429fe38438",
    "title": "Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks",
    "authors": [
      "Graziano A. Manduzio",
      "F. Galatolo",
      "M. G. Cimino",
      "E. P. Scilingo",
      "Lorenzo Cominelli"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. While these models excel in general complex reasoning tasks, they still face challenges in mathematical problem-solving and logical reasoning. To address these limitations, researchers have explored function calling abilities, allowing LLMs to execute provided functions and utilize their outputs for task completion. However, concentrating on specific tasks can be very inefficient for large-scale LLMs to be used, because of the expensive cost of training and inference stages they need in terms of computational resources. This study introduces a novel framework for training smaller language models in function calling, focusing on specific logical and mathematical reasoning tasks. The approach aims to improve performances of small-scale models for these tasks using function calling, ensuring a high level of accuracy. Our framework employs an agent that, given a problem and a set of callable functions, queries the LLM by injecting a description and examples of the usable functions into the prompt and managing their calls in a step-by-step reasoning chain. This process is used to create a dataset of correct and incorrect reasoning chain chat completions from a large-scale LLM. This dataset is used to train a smaller LLM using Reinforcement Learning from Human Feedback (RLHF), specifically employing the Direct Preference Optimization (DPO) technique. Experimental results demonstrate how the proposed approach balances the trade-off between model size and performance, improving the ability of function calling for reasoning tasks, in smaller models.",
    "url": "https://www.semanticscholar.org/paper/d47aae06d20ea0189adad9b2c8184b429fe38438",
    "pdf_url": "https://arxiv.org/pdf/2410.18890.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-24",
    "externalIds": {
      "ArXiv": "2410.18890",
      "DBLP": "journals/corr/abs-2410-18890",
      "DOI": "10.48550/arXiv.2410.18890",
      "CorpusId": 273549409
    },
    "references": [
      {
        "paperId": "7f319badb2d7e38ad14596d832ad18de34f7cb7e",
        "title": "A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
        "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling"
      },
      {
        "paperId": "b8b3200548e1b0c25ab146fdc6066301ba641867",
        "title": "LaMI: Large Language Models for Multi-Modal Human-Robot Interaction"
      },
      {
        "paperId": "24ec095f35bc76e2557a63baf5bcc6dd735aabce",
        "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)"
      },
      {
        "paperId": "36f71673d9337b432babc51da77ef38b2070b5ed",
        "title": "An LLM Compiler for Parallel Function Calling"
      },
      {
        "paperId": "dd4d82299b4209db539d639f836fcee663cf72b3",
        "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "d00735241af700d21762d2f3ca00d920241a15a4",
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"
      },
      {
        "paperId": "f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4",
        "title": "On the Unexpected Abilities of Large Language Models"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "7d8905a1fd288068f12c8347caeabefd36d0dd6c",
        "title": "Gorilla: Large Language Model Connected with Massive APIs"
      },
      {
        "paperId": "19c222d1f18317d58cc85491f37479bc0dc49f41",
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
        "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"
      },
      {
        "paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",
        "title": "Reasoning with Language Model Prompting: A Survey"
      },
      {
        "paperId": "354bf043179e3e9f05df73e3f04517e53c326d1f",
        "title": "TALM: Tool Augmented Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "2e5d2f2dc01b150dffc163a9f457848e9b5b5c38",
        "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
        "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
        "title": "Distilling the Knowledge in a Neural Network"
      },
      {
        "paperId": "573e0d000c21d66f74454655c942337b3a12171e",
        "title": "Answer Set Programming"
      },
      {
        "paperId": "00709ed019f7ddc5cd84740099066cd20e61625d",
        "title": "Probabilistic Inductive Logic Programming"
      },
      {
        "paperId": "3b281073fcfeca055d6ec88df1c7966257e53ef7",
        "title": "First-Order Logic and Automated Theorem Proving"
      },
      {
        "paperId": "543b8f2402406ed7e29a5124441237896c1fc495",
        "title": "Problem"
      },
      {
        "paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"
      },
      {
        "paperId": "aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8",
        "title": "Towards Mitigating LLM Hallucination via Self Reflection"
      },
      {
        "paperId": null,
        "title": ": Design principles and model"
      },
      {
        "paperId": "87571254f95ceb251d001cf2f64082b1f550d525",
        "title": "Quantification"
      },
      {
        "paperId": null,
        "title": "\u201cTrl: Transformer reinforcement learning,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cExperiment tracking with weights and biases,\u201d"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": null,
        "title": "Artificial intelligence: a modern approach"
      },
      {
        "paperId": "2521c1d36286ea3a919bf413dddba2388d000377",
        "title": "Handbook on Ontologies"
      },
      {
        "paperId": "0e61334db3c76f6000f8946bb98aecf389514d3f",
        "title": "Knowledge Representation and Reasoning"
      },
      {
        "paperId": "fa52d9267061a94ab1a633c00ab679e2e847622f",
        "title": "Expert Systems: Principles and Programming"
      },
      {
        "paperId": "030ccc44a58c1490984e1650d461491f80486bbc",
        "title": "Logic in Computer Science: Modelling and Reasoning about Systems"
      },
      {
        "paperId": null,
        "title": "\u201cFirst-order logic. dover publications inc,\u201d"
      },
      {
        "paperId": "2087761ff2a513b6b39117d89d7f8f6f9c2ea4f7",
        "title": "Logic and structure"
      },
      {
        "paperId": "66a302e05a154fb8a937e353c5ee75407c0b93c2",
        "title": "A mathematical introduction to logic"
      },
      {
        "paperId": null,
        "title": "\u201cPeft: State-of-the-art parameter-efficient fine-tuning methods,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cAccelerate: Training and inference at scale made simple, efficient and adaptable.\u201d"
      },
      {
        "paperId": null,
        "title": "Variables: x and y are used to represent arbitrary elements in the domain, allowing for general statements about the relationships between elements"
      },
      {
        "paperId": null,
        "title": "\u201cChat completions guide,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cChat templating in transformers,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cAdvanced control of humanoid facial robotics: A deep learning approach to inverse kinematics,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cSymbolic data plus reasoning data v1,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cAn empirical study on challenging math problem solving with gpt-4,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cMicrochain,\u201d"
      }
    ],
    "cited_by": [
      {
        "paperId": "7165a0fca01cd24939b4fe2ae606571cd89ce7c3",
        "title": "Advancing SLM Tool-Use Capability using Reinforcement Learning"
      },
      {
        "paperId": "87472a1c873b95bd880eb460f87300071c4f060a",
        "title": "Framework for circular AI-driven model-based systems engineering"
      },
      {
        "paperId": "1081120008485d6e9dcacd68b0c1e6ec70fde7fd",
        "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications"
      },
      {
        "paperId": "dec2983a58e97c4e5e7aedfa4d42e688fae25473",
        "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use"
      }
    ],
    "score": 4.0
  },
  {
    "id": "aa64075f0c7a0977508a5dcb6a3c319952afcc20",
    "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL",
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Tim Rockaschel",
      "Edward Grefenstette",
      "Davide Paglieri",
      "Samuel Coward",
      "Laura Toni"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "The temporal credit assignment problem is a central challenge in Reinforcement Learning (RL), concerned with attributing the appropriate influence to each actions in a trajectory for their ability to achieve a goal. However, when feedback is delayed and sparse, the learning signal is poor, and action evaluation becomes harder. Canonical solutions, such as reward shaping and options, require extensive domain knowledge and manual intervention, limiting their scalability and applicability. In this work, we lay the foundations for Credit Assignment with Language Models (CALM), a novel approach that leverages Large Language Models (LLMs) to automate credit assignment via reward shaping and options discovery. CALM uses LLMs to decompose a task into elementary subgoals and assess the achievement of these subgoals in state-action transitions. Every time an option terminates, a subgoal is achieved, and CALM provides an auxiliary reward. This additional reward signal can enhance the learning process when the task reward is sparse and delayed without the need for human-designed rewards. We provide a preliminary evaluation of CALM using a dataset of human-annotated demonstrations from MiniHack, suggesting that LLMs can be effective in assigning credit in zero-shot settings, without examples or LLM fine-tuning. Our preliminary results indicate that the knowledge of LLMs is a promising prior for credit assignment in RL, facilitating the transfer of human knowledge into value functions.",
    "url": "https://www.semanticscholar.org/paper/aa64075f0c7a0977508a5dcb6a3c319952afcc20",
    "pdf_url": "https://arxiv.org/pdf/2409.12798.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-09-19",
    "externalIds": {
      "DBLP": "journals/corr/abs-2409-12798",
      "ArXiv": "2409.12798",
      "DOI": "10.48550/arXiv.2409.12798",
      "CorpusId": 272753067
    },
    "references": [
      {
        "paperId": "e3116a5d5784392d9190ca35997f65a252291032",
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks"
      },
      {
        "paperId": "81881d2589a34df12d5e2fd192d5354dda1f81a8",
        "title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents"
      },
      {
        "paperId": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
        "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning"
      },
      {
        "paperId": "287f4b599020d53634abbee6a6e58468d14ebbdc",
        "title": "Vision-Language Models as a Source of Rewards"
      },
      {
        "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
        "title": "CLadder: Assessing Causal Reasoning in Language Models"
      },
      {
        "paperId": "c8168177d5cbd3a3d9ac5abdef7b694f388ec8f5",
        "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"
      },
      {
        "paperId": "c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
      },
      {
        "paperId": "c50348d3491567b2cdad5ea981620c31f876dad9",
        "title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning"
      },
      {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
        "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "89e184d2bc830af568e439db9476caa0c047e11a",
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "61678a9f1d8291bb0f3d704a439ac8cd64fa6482",
        "title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals"
      },
      {
        "paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"
      },
      {
        "paperId": "ccb1ccc4deacc4fb18000f0e1ce24329548963ae",
        "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"
      },
      {
        "paperId": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
        "title": "Collaborating with language models for embodied reasoning"
      },
      {
        "paperId": "203a1c0e5025489a52c030adbbc102a787685ee6",
        "title": "Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity"
      },
      {
        "paperId": "4580b0e3ede1b7ad97fb742fa28cd3ebe3bed7a9",
        "title": "Improving Policy Learning via Language Dynamics Distillation"
      },
      {
        "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867",
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"
      },
      {
        "paperId": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "paperId": "cdf54c147434c83a4a380916b6c1279b0ca19fc2",
        "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"
      },
      {
        "paperId": "e38fb0979de7cb76c0b020a5555d15e567a0b41a",
        "title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"
      },
      {
        "paperId": "3f3c01adbdd433d515c19ac8cf6c61c905f0061a",
        "title": "History Compression via Language Models in Reinforcement Learning"
      },
      {
        "paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      },
      {
        "paperId": "ba9b7f2998002529061532486226496883e86e30",
        "title": "Improving Intrinsic Exploration with Language Abstractions"
      },
      {
        "paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
      },
      {
        "paperId": "2d4ca959cb3d544473cb661cefe76daabebcdff3",
        "title": "Skill Induction and Planning with Latent Language"
      },
      {
        "paperId": "43ea4f5d999d35d4fc6c544eedbc100d8c3a5e00",
        "title": "MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research"
      },
      {
        "paperId": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
        "title": "Benchmarking the Spectrum of Agent Capabilities"
      },
      {
        "paperId": "66d5863d31b165a493c93c88b257ea06485f7f12",
        "title": "Multitasking Inhibits Semantic Drift"
      },
      {
        "paperId": "57e3cbd9243ecd857344cdbf4ac7db362bba37d8",
        "title": "Counterfactual Credit Assignment in Model-Free Reinforcement Learning"
      },
      {
        "paperId": "8aed57b61457655e8354f1b68b34ed1cc0a222ef",
        "title": "Keep CALM and Explore: Language Models for Action Generation in Text-based Games"
      },
      {
        "paperId": "8e79043b89f007bd81d65244345b50f7bff7fca9",
        "title": "Reinforcement Learning with Trajectory Feedback"
      },
      {
        "paperId": "822bdb6e8c39e272ebfee127666e032bd3aa0107",
        "title": "The NetHack Learning Environment"
      },
      {
        "paperId": "04bd5cf9096f9a28a3505a171959a0bde4feb85e",
        "title": "RTFM: Generalising to New Environment Dynamics via Reading"
      },
      {
        "paperId": "c2c8482c713b94073f3d59895b373db4398ddfbb",
        "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning"
      },
      {
        "paperId": "0fa1c75a452a046e11e775eb6120051c696d9366",
        "title": "Using Natural Language for Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "1456d9d2ee40b932e2e07762985a60f0bdff07f3",
        "title": "Optimizing agent behavior over long time scales by transporting value"
      },
      {
        "paperId": "9fdd7881c52650db939a30c2b287936b0092732c",
        "title": "Hierarchical Reinforcement Learning: A Survey"
      },
      {
        "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
        "title": "Large Scale Distributed Deep Networks"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
        "title": "Steps toward Artificial Intelligence"
      },
      {
        "paperId": "33af17e7cc5cd4ee18c9e6e8c5fca7e224592ec0",
        "title": "Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control"
      },
      {
        "paperId": "73167084e0d103e6433a4558cc0fea247e899c41",
        "title": "Quantile Credit Assignment"
      },
      {
        "paperId": null,
        "title": "Minimalistic gridworld environment for openai gym"
      },
      {
        "paperId": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
        "title": "Temporal credit assignment in reinforcement learning"
      },
      {
        "paperId": null,
        "title": "The agent has avoided obstacles by moving north and not hitting the wall, but none of the other subgoals have been achieved at Time: 1."
      }
    ],
    "cited_by": [
      {
        "paperId": "d30dbfed598e814b3f7203443f8e93bb299f0c53",
        "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem"
      },
      {
        "paperId": "508cf756da17ad9503153343ce381e471dfaa32e",
        "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition"
      },
      {
        "paperId": "198b1247f4d07e12da455b7aff6089f382f86cc4",
        "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment"
      },
      {
        "paperId": "369bd8479daa33df68567e93e00a52155320ed88",
        "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models"
      }
    ],
    "score": 4.0
  },
  {
    "id": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
    "title": "Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis",
    "authors": [
      "Qining Zhang",
      "Honghao Wei",
      "Lei Ying"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.",
    "url": "https://www.semanticscholar.org/paper/c3bb8d030a47d28c7a965ee5112a453d86e098ad",
    "pdf_url": "https://arxiv.org/pdf/2406.07455.pdf",
    "venue": "RLJ",
    "publicationDate": "2024-06-11",
    "externalIds": {
      "DBLP": "conf/rlc/ZhangW024",
      "ArXiv": "2406.07455",
      "DOI": "10.48550/arXiv.2406.07455",
      "CorpusId": 270379830
    },
    "references": [
      {
        "paperId": "0f834f9944b1c842e8107beb0dc85466d854c2bd",
        "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization"
      },
      {
        "paperId": "f83ba49c994e80bf7630aab3143eaa12eb9841ab",
        "title": "A Reinforcement Learning and Prediction-Based Lookahead Policy for Vehicle Repositioning in Online Ride-Hailing Systems"
      },
      {
        "paperId": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
        "title": "A Survey of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "b85f16e0a89ad64afc29af407b8026b6329f039d",
        "title": "Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms"
      },
      {
        "paperId": "31fcbacd4874a772e77ac21e4ec7d0dff132ec4e",
        "title": "Submodular Reinforcement Learning"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "fbe1003ec391f6bcf4660f6ef81f1e6199849bfe",
        "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "1d49b3931069964609f4471cf1bd07ceb4ff691e",
        "title": "Dealing with Unknown Variances in Best-Arm Identification"
      },
      {
        "paperId": "cb16ad67ad1398326a82925a54f07f7c271d7736",
        "title": "Optimistic PAC Reinforcement Learning: the Instance-Dependent View"
      },
      {
        "paperId": "ff9885f3ac91fac6594110b334370a344bbc761f",
        "title": "Exploration. Exploitation, and Engagement in Multi-Armed Bandits with Abandonment"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "e25f82b2a49918f547980a0a2dca52c3989401e6",
        "title": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "7e93fc4a9cbdcfb1fba2ce6ac8364fc077b14e5c",
        "title": "Efficient and Optimal Algorithms for Contextual Dueling Bandits under Realizability"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "daa0a11ab7a3f9af1310b19a84e1947a518da149",
        "title": "Beyond No Regret: Instance-Dependent PAC Reinforcement Learning"
      },
      {
        "paperId": "1b2a5406a8db1c578f10a2be2cb519c843a368a9",
        "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning"
      },
      {
        "paperId": "5c37023c35fc1c95565d56b4fc4821fcf768651a",
        "title": "Reward is enough for convex MDPs"
      },
      {
        "paperId": "cca15babd21194df08267908713035c34a4441b8",
        "title": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback"
      },
      {
        "paperId": "15401c99dbd977001c1e2076f0decd8362e7d57a",
        "title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap"
      },
      {
        "paperId": "643bf18fe4434d6dd8ebc7dbef8361c09d9a34b1",
        "title": "Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "8e79043b89f007bd81d65244345b50f7bff7fca9",
        "title": "Reinforcement Learning with Trajectory Feedback"
      },
      {
        "paperId": "8a0ec47a51ed130a83a599c5c81c570fe1689cf3",
        "title": "Q-learning with Logarithmic Regret"
      },
      {
        "paperId": "4a4ef182464b5ed53561d6eed0c173935c054f88",
        "title": "Task-agnostic Exploration in Reinforcement Learning"
      },
      {
        "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
        "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "1171d2a1c416fb59c3a7b3e05b12618ae46cd646",
        "title": "Online Convex Optimization in Adversarial Markov Decision Processes"
      },
      {
        "paperId": "481ca2146292126be824b5629f073d5a2e07529d",
        "title": "Nonasymptotic sequential tests for overlapping hypotheses applied to near-optimal arm identification in bandit models"
      },
      {
        "paperId": "7f074f4e30b196df8a7f93bc58cbd88f3acc8641",
        "title": "Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "07026ca6820c5de599e9e431b905d52151409f4d",
        "title": "Bridging the gap between regret minimization and best arm identification, with application to A/B tests"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "cb0c4256358167ea8fc21d2c64495b06ae04fd52",
        "title": "Planning and Decision-Making for Autonomous Vehicles"
      },
      {
        "paperId": "abcf11a9af3d83f85c5fbfffc5901d416ca7a73f",
        "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "5dcc07acb63cc909c5be701c1c88fef3718ba326",
        "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning"
      },
      {
        "paperId": "630bf4c0b7e5abd276ec38468f490b7d6222f3a2",
        "title": "Interactive Learning from Policy-Dependent Human Feedback"
      },
      {
        "paperId": "673bed25e16ddc632e4bfafbdf9765b71eb138ab",
        "title": "Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit"
      },
      {
        "paperId": "e89b2f97516b99debb293a2751af937da7e10f92",
        "title": "Optimal Best Arm Identification with Fixed Confidence"
      },
      {
        "paperId": "f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef",
        "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning"
      },
      {
        "paperId": "5e0531c9ab0150a0bcb8f2b86cd44fc6075834fb",
        "title": "Contextual Dueling Bandits"
      },
      {
        "paperId": "4dc0e6599d543a5ad3c32b9127c9dac04e34021b",
        "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models"
      },
      {
        "paperId": "df742fb7b0106df3970cd4781a72d5ef9a155a34",
        "title": "Reducing Dueling Bandits to Cardinal Bandits"
      },
      {
        "paperId": "ff25a0a000205e21fc39abe3dc7f920e4236b38b",
        "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem"
      },
      {
        "paperId": "b7f8eaf739b33115fd57e1b4051be9f2049668fa",
        "title": "A Fast Bandit Algorithm for Recommendation to Users With Heterogenous Tastes"
      },
      {
        "paperId": "d85b83a351a8b5db766d354016517c9ebf5831c4",
        "title": "Information Complexity in Bandit Subset Selection"
      },
      {
        "paperId": "d4a145888d2724843213b818367138dee5465df0",
        "title": "Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "bda27e969e43a1e93026981afff57b8d46be008e",
        "title": "PAC Subset Selection in Stochastic Multi-armed Bandits"
      },
      {
        "paperId": "39ef51f907ac2785d99f604888cf8c4e2401849d",
        "title": "Beat the Mean Bandit"
      },
      {
        "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
        "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "ef8772242235273359511b139691f2f88eb5e87a",
        "title": "Generalized Linear Models"
      },
      {
        "paperId": "7f152d11a5553f50b9478db010d273a26d4d991f",
        "title": "An upper bound on the loss from approximate optimal-value functions"
      },
      {
        "paperId": "2906acb116628e33f7c3c05f0a6a525408b33e2e",
        "title": "A Sequential Multiple-Decision Procedure for Selecting the Best One of Several Normal Populations with a Common Unknown Variance, and Its Use with Various Experimental Designs"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "bcc91889adc389cbe25295961c07a3484225ee7b",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "2418d6703faea16ec978649c281b89e0b70f15dd",
        "title": "Convex Reinforcement Learning in Finite Trials"
      },
      {
        "paperId": "356770d13ad2e7b60961e5bc7368ffd3b9a2bcd9",
        "title": "Learning to summarize with human feedback"
      },
      {
        "paperId": null,
        "title": "The probit link function in generalized linear models for data mining applications"
      },
      {
        "paperId": null,
        "title": "A framework for partially observed reward-states in rlhf"
      }
    ],
    "cited_by": [
      {
        "paperId": "7af0dee85b349af0ff3e98decac1e393db84c118",
        "title": "Fusing Rewards and Preferences in Reinforcement Learning"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      }
    ],
    "score": 3.0
  },
  {
    "id": "914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
    "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
    "authors": [
      "Yu Zhu",
      "Chuxiong Sun",
      "Wenfei Yang",
      "Wenqiang Wei",
      "Bo Tang",
      "Tianzhu Zhang",
      "Zhiyu Li",
      "Shifeng Zhang",
      "Feiyu Xiong",
      "Jie Hu",
      "Mingchuan Yang"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.",
    "url": "https://www.semanticscholar.org/paper/914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
    "pdf_url": "https://arxiv.org/pdf/2403.04283.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-03-07",
    "externalIds": {
      "ArXiv": "2403.04283",
      "DBLP": "journals/corr/abs-2403-04283",
      "DOI": "10.48550/arXiv.2403.04283",
      "CorpusId": 268264841
    },
    "references": [
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "003ef1cd670d01af05afa0d3c72d72228f494432",
        "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models"
      },
      {
        "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "paperId": "f8d44802ac8190864c61c9aaf4a8b450261873ab",
        "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": null,
        "title": "Table 3: The Training Settings of Proxy-RLHF. Model 2-layer MLP Hidden Size 2048 Learning Rate 3e-4"
      }
    ],
    "cited_by": [
      {
        "paperId": "a2fae006e6c5ac346fd51bc8a009127f9abe22df",
        "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"
      },
      {
        "paperId": "d9d8aef662bb7a3730a62b1015c3ed99e4287523",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt"
      },
      {
        "paperId": "1146d40d3d01427a008a20530269667b8989750c",
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation"
      }
    ],
    "score": 3.0
  },
  {
    "id": "b467036844e26c96ee94c466d771f1a5bf617204",
    "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
    "authors": [
      "S. Srivastava",
      "Vaneet Aggarwal"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
    "url": "https://www.semanticscholar.org/paper/b467036844e26c96ee94c466d771f1a5bf617204",
    "pdf_url": "https://arxiv.org/pdf/2507.04136.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-07-05",
    "externalIds": {
      "DBLP": "journals/corr/abs-2507-04136",
      "ArXiv": "2507.04136",
      "DOI": "10.48550/arXiv.2507.04136",
      "CorpusId": 280149675
    },
    "references": [
      {
        "paperId": "2edd56fd82749d0f0d987ee341d8caf95037b164",
        "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety"
      },
      {
        "paperId": "03013a1f76048fcabb9d3d66a30543c0dda5ceec",
        "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning"
      },
      {
        "paperId": "a207363e691a813ed47946793fe8150c575ba1aa",
        "title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning"
      },
      {
        "paperId": "f1f31064ccac840a33ae2a6b9e5a0873d97b3abe",
        "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition"
      },
      {
        "paperId": "1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example"
      },
      {
        "paperId": "789e80522bc3ddd935ad56410b9d827068ce84d7",
        "title": "Technical Challenges in Maintaining Tax Prep Software with Large Language Models"
      },
      {
        "paperId": "8402e446158252992b6ddf1ff1b0658c39d7604e",
        "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
      },
      {
        "paperId": "24d1b69bd6d2a5132170bc8e0d9596336e99cd32",
        "title": "On The Sample Complexity Bounds In Bilevel Reinforcement Learning"
      },
      {
        "paperId": "23c86f865148f40c20b652bbfb773a10d9a1b88b",
        "title": "Learning to Generate Structured Output with Schema Reinforcement Learning"
      },
      {
        "paperId": "dd951242ebc94bf633eecc4994c64f46146a1413",
        "title": "Reward Shaping to Mitigate Reward Hacking in RLHF"
      },
      {
        "paperId": "b8a27582cf56ebebce06684c5a0664adb30b738f",
        "title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages - A Singlish Case Study"
      },
      {
        "paperId": "d53a8091159e7cce328b8cf7b923e21b7b57189a",
        "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning"
      },
      {
        "paperId": "eab7c48a4592d8a347d520affbc1b52f50e88be7",
        "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates"
      },
      {
        "paperId": "38ac0ddf8b016602b95bf3bf90ab9d7e3fc31c20",
        "title": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "225721f407a7a8cdefd4ba6bc61c43acba5a3b6a",
        "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models"
      },
      {
        "paperId": "b34c0b0169925a6c7f14e3de9764ed9505f30de3",
        "title": "Toward expert-level medical question answering with large language models"
      },
      {
        "paperId": "e07a7e57745b69c0f08601fbdd19cfdcafbe1152",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "paperId": "985298bd0b6f4c84ab34bb15cfed0b092f43b83a",
        "title": "OpenAI o1 System Card"
      },
      {
        "paperId": "0a42291d9543eabe33f6c14278333484071a707c",
        "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning"
      },
      {
        "paperId": "eba762de9b2120f778b6ccd6c26d3ba00b6f73b5",
        "title": "Alignment faking in large language models"
      },
      {
        "paperId": "2d906cda427cb2c4a71069423312e57ba4cd5445",
        "title": "Reinforcement Learning Enhanced LLMs: A Survey"
      },
      {
        "paperId": "477a16bdbb43589e5feac3881b3370e3a4ab5624",
        "title": "Evaluating the Consistency of LLM Evaluators"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "3257a72f5cc9f9e35a179b28229045e8cb3c231c",
        "title": "SelfCodeAlign: Self-Alignment for Code Generation"
      },
      {
        "paperId": "7943ec4a67151a559b25cd34369e661c9a7924c8",
        "title": "GPT-4o System Card"
      },
      {
        "paperId": "20d2c5e77cb14ae18347a600213789d18a212c7c",
        "title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers"
      },
      {
        "paperId": "90b68d87e802ecaa472442081ad4515e176913f3",
        "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "ec2ce4e38af8bc82f1b8928ba51a84911bad0cc6",
        "title": "Gemma 2: Improving Open Language Models at a Practical Size"
      },
      {
        "paperId": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
        "title": "Qwen2 Technical Report"
      },
      {
        "paperId": "d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning"
      },
      {
        "paperId": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
        "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"
      },
      {
        "paperId": "96f94f9fdefe92be692e5461999d9a72c3a4e2b8",
        "title": "The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches"
      },
      {
        "paperId": "7f016cc9f0d1e9abd48dd48cdb220be16b609c8d",
        "title": "Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search"
      },
      {
        "paperId": "cca77ff5dd95e395ff0fc725982199340f774c6c",
        "title": "MathDivide: Improved mathematical reasoning by large language models"
      },
      {
        "paperId": "53a803388e83ae89261624099d7be4287ace67cb",
        "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
      },
      {
        "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e",
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "05e0c57f912cec9597021855bac28306c97e36fd",
        "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content"
      },
      {
        "paperId": "087699924e3dc468a486e0763f1cc097824a60d2",
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "5cd671efa2af8456c615c5faf54d1be4950f3819",
        "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models"
      },
      {
        "paperId": "04d64be16fb402f28348faffef484bd419c8bd8f",
        "title": "Self-Rewarding Language Models"
      },
      {
        "paperId": "411114f989a3d1083d90afd265103132fee94ebe",
        "title": "Mixtral of Experts"
      },
      {
        "paperId": "f7e7c3b53bd43c21a34f78a4b874246cec81ebc7",
        "title": "ADAPTER-RL: Adaptation of Any Agent using Reinforcement Learning"
      },
      {
        "paperId": "712f6dfcee099ee38d6d09af23e8bc0a7e82bb72",
        "title": "Fake Alignment: Are LLMs Really Aligned Well?"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "paperId": "c12db2e67d1fb289266faa5507ff112c9a062465",
        "title": "Efficient RLHF: Reducing the Memory Usage of PPO"
      },
      {
        "paperId": "78b0c5d96a05b4be6a00702fba24c9174e8173af",
        "title": "Aligning Language Models with Offline Learning from Human Feedback"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models"
      },
      {
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
        "title": "Competition-level code generation with AlphaCode"
      },
      {
        "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "09cd2dc227efdfb4885ac282ac481278ad60d8ac",
        "title": "Understanding Multi-Step Deep Reinforcement Learning: A Systematic Study of the DQN Target"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": null,
        "title": "Aime 2024: American invitational mathematics examination"
      },
      {
        "paperId": null,
        "title": "RLHFlow"
      },
      {
        "paperId": null,
        "title": "Responsible use of github copilot code review"
      },
      {
        "paperId": null,
        "title": "Introducing claude 3.5"
      },
      {
        "paperId": "8d67971c6aa2a4734723897a4185ea755fc30edb",
        "title": "On the Vulnerability of Safety Alignment in Open-Access LLMs"
      },
      {
        "paperId": null,
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "paperId": null,
        "title": "Google ai updates: Bard and new ai features in search,"
      },
      {
        "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Humaneval: A human-centric code evaluation benchmark"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Introducing gemini 2.0: Our new ai model for the agentic era"
      },
      {
        "paperId": "fed9cc193a14b84131812372d8d5857f8f304c52",
        "title": "Claude 3.5 Sonnet Model Card Addendum"
      },
      {
        "paperId": null,
        "title": "3.3 model card and release information"
      },
      {
        "paperId": null,
        "title": "Gemini 2.5: Our most intelligent ai model"
      },
      {
        "paperId": "3b6ffb33c01fa6c4986785bdfd69ab705d6b05b6",
        "title": "OpenAI o3 and o4-mini System Card"
      },
      {
        "paperId": null,
        "title": "Sycophancy to subterfuge"
      },
      {
        "paperId": null,
        "title": "Evaluating the instruction-following abilities of language models using knowledge tasks"
      },
      {
        "paperId": null,
        "title": "HuggingFaceH4. Math-500: A subset of the math benchmark, December 2023"
      }
    ],
    "cited_by": [
      {
        "paperId": "21b81883c1f4c789fec33d239ef173ea51b73918",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"
      },
      {
        "paperId": "f559de59721c3833db3abdebe3d76b3b9d6869b4",
        "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference"
      },
      {
        "paperId": "b9bb242f30924770699496d65c9514c61a60b482",
        "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "5cb5453d2c54e1449f82fdf2e976cab04396b224",
    "title": "Distributionally Robust Reinforcement Learning with Human Feedback",
    "authors": [
      "Debmalya Mandal",
      "Paulius Sasnauskas",
      "Goran Radanovic"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "Reinforcement learning from human feedback (RLHF) has evolved to be one of the main methods for fine-tuning large language models (LLMs). However, existing RLHF methods are non-robust, and their performance deteriorates if the downstream task differs significantly from the preference dataset used in fine-tuning. In order to mitigate this problem, we introduce a distributionally robust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a fine-tuned model retains its performance even when the distribution of prompts significantly differs from the distribution encountered during fine-tuning. We formulate distributionally robust optimization (DRO) version of two popular fine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct preference optimization). We propose a minibatch gradient descent based algorithms for both of them, and theoretically prove convergence guarantees for the algorithms. Subsequently, we evaluate our algorithms on an out-of-distribution (OOD) task by first training the model on the Unified-Feedback dataset and evaluating its performance on two different datasets. The experimental results show that our robust training improves the accuracy of the learned reward models on average, and markedly on some tasks, such as reasoning. Furthermore, we show that the robust versions of policy optimization methods, similarly improve performance on OOD tasks.",
    "url": "https://www.semanticscholar.org/paper/5cb5453d2c54e1449f82fdf2e976cab04396b224",
    "pdf_url": "https://arxiv.org/pdf/2503.00539.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-03-01",
    "externalIds": {
      "ArXiv": "2503.00539",
      "DBLP": "journals/corr/abs-2503-00539",
      "DOI": "10.48550/arXiv.2503.00539",
      "CorpusId": 276742683
    },
    "references": [
      {
        "paperId": "117b52d94d38048aca36fc7432ac17742103e2e2",
        "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization"
      },
      {
        "paperId": "99d59b4ec0a2b0860da49f11fe698282af43dff8",
        "title": "Towards Reliable Alignment: Uncertainty-aware RLHF"
      },
      {
        "paperId": "d1461167c9fef8fe3ba129c514acfd14bbe7a51e",
        "title": "Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design"
      },
      {
        "paperId": "97c4c24d6539bf459a6858f17f97906ac6e6d973",
        "title": "LLM Embeddings Improve Test-time Adaptation to Tabular Y|X-Shifts"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "f546ae1037ac76283d30c34451e022dd64947b4d",
        "title": "Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation"
      },
      {
        "paperId": "928b3ef966cb0e1e9cff7e5e96d5df23c47c2d5a",
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift"
      },
      {
        "paperId": "6f918de95b8a3619e6eb00dbc16a3da43d6fa571",
        "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization"
      },
      {
        "paperId": "1a7781465495ae54ce8413aa4ddedd24f5666dc7",
        "title": "Robust Reinforcement Learning from Corrupted Human Feedback"
      },
      {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"
      },
      {
        "paperId": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
        "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
        "title": "Robust Preference Optimization through Reward Model Distillation"
      },
      {
        "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      },
      {
        "paperId": "9ba81cd8bb6d695bddfb68140e9c3c425f2f1939",
        "title": "Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences"
      },
      {
        "paperId": "3730ad714aa22ebd3694974abe0bc6437a3ad4ee",
        "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback"
      },
      {
        "paperId": "0061d53c3144bf75f39ebc61d8a0db85e54d9021",
        "title": "Corruption Robust Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "67f03ac399693393116076c0b8ec8ea05b910685",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models"
      },
      {
        "paperId": "763fdb72349b19b12abb1dd6e81fc40c44a48713",
        "title": "Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning"
      },
      {
        "paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "paperId": "993df7df129f8d18816877d69923d7df7b347d85",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "0f71a43a5cd29368bcc2f2ff5e8769f69c20fff6",
        "title": "A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence"
      },
      {
        "paperId": "88ae720354fa4883be13e25e1cf92dcd4fed5f5d",
        "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "6be327602deb674d0e9f3b606f3e6baf733bf266",
        "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "def0bf85248be140f431e5074d7cd2c3c92c2e6e",
        "title": "Large-Scale Methods for Distributionally Robust Optimization"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "5500601832c67e73f394daa70cda23bd54a2a49a",
        "title": "Distributionally Robust Optimization: A Review"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "72e0763428b55a788ffa16b7d206f8d3e20a9db9",
        "title": "Distributionally Robust Optimization and Generalization in Kernel Methods"
      },
      {
        "paperId": "818c52f4ba56cb8cf152ad614f2f4803057a5cfe",
        "title": "Certifying Some Distributional Robustness with Principled Adversarial Training"
      },
      {
        "paperId": "30237f91f079dcdb88d48b2a9ebc7bc5322a5386",
        "title": "Distributionally Robust Stochastic Programming"
      },
      {
        "paperId": "a6a1fb4df010e8dde0a90f8b0f0e958ba4ef9624",
        "title": "A Rewriting System for Convex Optimization Problems"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "e177365c0dd642341469ba2f045405646c5dbb79",
        "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization"
      },
      {
        "paperId": "948dfbb55a93aa9585056a4b4dd3cd6553b236a9",
        "title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations"
      },
      {
        "paperId": "583b55367f787eb0c4e295707b642e63547b9806",
        "title": "Robust Solutions of Optimization Problems Affected by Uncertain Probabilities"
      },
      {
        "paperId": "1621f05894ad5fd6a8fcb8827a8c7aca36c81775",
        "title": "An optimal method for stochastic composite optimization"
      },
      {
        "paperId": "b18833db0de9393d614d511e60821a1504fc6cd1",
        "title": "A Natural Policy Gradient"
      },
      {
        "paperId": null,
        "title": "TRL: Transformer Reinforcement Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "b29ff2491b0365f1e3862c43197860eac9962db3",
        "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO"
      },
      {
        "paperId": "66c16a4eb1457f447a44fb1ea1968f8841ad5a2d",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
      },
      {
        "paperId": "117b52d94d38048aca36fc7432ac17742103e2e2",
        "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization"
      }
    ],
    "score": 3.0
  },
  {
    "id": "a4338066b4d83cd9043a04eb4d5732041056a0f1",
    "title": "More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models",
    "authors": [
      "Evan Chen",
      "Run-Jun Zhan",
      "Yan-Bai Lin",
      "Hung-Hsuan Chen"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases. This study introduces a novel evaluation framework to uncover gender biases in LLMs: using free-form storytelling to surface biases embedded within the models. A systematic analysis of ten prominent LLMs shows a consistent pattern of overrepresenting female characters across occupations, likely due to supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Paradoxically, despite this overrepresentation, the occupational gender distributions produced by these LLMs align more closely with human stereotypes than with real-world labor data. This highlights the challenge and importance of implementing balanced mitigation measures to promote fairness and prevent the establishment of potentially new biases. We release the prompts and LLM-generated stories at GitHub.",
    "url": "https://www.semanticscholar.org/paper/a4338066b4d83cd9043a04eb4d5732041056a0f1",
    "pdf_url": "https://arxiv.org/pdf/2503.15904.pdf",
    "venue": "",
    "publicationDate": "2025-03-20",
    "externalIds": {
      "ArXiv": "2503.15904",
      "CorpusId": 277150708
    },
    "references": [
      {
        "paperId": "44fc2065a27e8bd88202cf8041b6c3c59f546bf3",
        "title": "Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics"
      },
      {
        "paperId": "c5aabd927143a00e13ca4e003f1cd98996e07d42",
        "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models"
      },
      {
        "paperId": "4b743d624ede1b1cbdb22c570ef62be0aefe33e8",
        "title": "\u201cYou Gotta be a Doctor, Lin\u201d : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations"
      },
      {
        "paperId": "1ea5864a08437c999cc61d11d7c35ff605886c85",
        "title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?"
      },
      {
        "paperId": "ce0cbb754f5d172b05bedad959c7e759b3e73dcd",
        "title": "What's in a Name? Auditing Large Language Models for Race and Gender Bias"
      },
      {
        "paperId": "ce157cea880c9ab64de64f11a531202f5348fa05",
        "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters"
      },
      {
        "paperId": "cf852c18387cfcdcb3cb3bb5ba6a549b35766c33",
        "title": "Gender bias and stereotypes in Large Language Models"
      },
      {
        "paperId": "db40c7a2ae4bff0063ef073a4e204c28ea851481",
        "title": "The Unequal Opportunities of Large Language Models: Examining Demographic Biases in Job Recommendations by ChatGPT and LLaMA"
      },
      {
        "paperId": "f5977cd354426e246dd9607f8c798be101e501ca",
        "title": "An Open-Source Cultural Consensus Approach to Name-Based Gender Classification"
      },
      {
        "paperId": "45bced2a11c6732b74c463cfc929e6da335e729a",
        "title": "Comprehending Pronouns: A Role for Word-Specific Gender Stereotype Information"
      },
      {
        "paperId": "18b242dd4be3fa844181be0d4bdbee1d5caef0fa",
        "title": "Investigating Gender Bias in Large Language Models Through Text Generation"
      },
      {
        "paperId": "7a3cc52deaad1638c2fc23df2bc464826e146fa7",
        "title": "Exploring Inherent Biases in LLMs within Korean Social Context: A Comparative Analysis of ChatGPT and GPT-4"
      },
      {
        "paperId": "a3ab70f788beb214a43072881bb7f36cdfa67498",
        "title": "Employed persons by detailed occupation , sex , race , and Hispanic or Latino ethnicity"
      }
    ],
    "cited_by": [
      {
        "paperId": "be097b61a69b74fcd899f283c4fecfc2c4e79a94",
        "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories"
      },
      {
        "paperId": "d74a2cf8cecb3795286772c89eee7114bbcf04b7",
        "title": "Evaluating gender bias in large language models in long-term care"
      },
      {
        "paperId": "3c8932d14434c04c3dc11ff2c9fc43723f0c6142",
        "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship"
      }
    ],
    "score": 3.0
  },
  {
    "id": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
    "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
    "authors": [
      "Heyang Zhao",
      "Chen Ye",
      "Wei Xiong",
      "Quanquan Gu",
      "Tong Zhang"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$ logarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.",
    "url": "https://www.semanticscholar.org/paper/4069e2f0eaf53a0e7086bb715e359e345e151abc",
    "pdf_url": "https://arxiv.org/pdf/2502.07460.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-11",
    "externalIds": {
      "ArXiv": "2502.07460",
      "DBLP": "journals/corr/abs-2502-07460",
      "DOI": "10.48550/arXiv.2502.07460",
      "CorpusId": 276258824
    },
    "references": [
      {
        "paperId": "e83a58f93fe5e6b618cab3a1038374261f29d1f2",
        "title": "Deliberative Alignment: Reasoning Enables Safer Language Models"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "6bd400acb88b37d1f1b46b2db16c0281cf2fa469",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "paperId": "1d82f61ee52331a94141a50b59b186ccf105f6a9",
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "daa5eaa9a7e3f02aa27e3e3f1ddfcd1af3a1315e",
        "title": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization"
      },
      {
        "paperId": "3a10570fb3f88d755ba4648598b3d30f8ddfb26c",
        "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage"
      },
      {
        "paperId": "168b0348dd76caf99b687c37aefe95a10664e6de",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "paperId": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "paperId": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "paperId": "9ffad46dcde21e7ae76f650420ab11202dc32200",
        "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "1e53e98e8709748a6385137d8f240787c12fcfd4",
        "title": "RLHF Workflow: From Reward Modeling to Online RLHF"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "3c184232d7c5b733c45765332aa6e9f06eedc139",
        "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits"
      },
      {
        "paperId": "2dc8d8221b148f6230edbe17eb0a92f062c85f36",
        "title": "Asymptotics of Language Model Alignment"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "paperId": "5a0632cf38b667b7066e98faa6c3d6460406e119",
        "title": "Foundations of Reinforcement Learning and Interactive Decision Making"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "bbaa206d581e1db01922f6cd1fc27b62d99a8217",
        "title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84",
        "title": "Zephyr: Direct Distillation of LM Alignment"
      },
      {
        "paperId": "f2f9cc38d5a1a4fdef996ea03fdb71e01f65d574",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "187c1466b00069969c8c44e2491e31d7aac09a4a",
        "title": "Mathematical Analysis of Machine Learning Algorithms"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "dff9fb8b3a0af0a26e8ab8fffac75580cdcc041b",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "paperId": "cedfdde4b9d01530bf2932554561bb25623890e5",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "70e0c321fe292392b3678e5c55efba3c1697ec63",
        "title": "Fast Rates for Maximum Entropy Exploration"
      },
      {
        "paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"
      },
      {
        "paperId": "4724ae0869b4cb65a741399489cb577a3373ada7",
        "title": "Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes"
      },
      {
        "paperId": "4b04354e35e558c362ef36cda266f46074158b44",
        "title": "VOQL: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation"
      },
      {
        "paperId": "3d444eafdd2cd17af73cc88cdbe35e6e21795ff2",
        "title": "The Role of Coverage in Online Reinforcement Learning"
      },
      {
        "paperId": "b1362b8a11b1984378b91162195e10e369f6b012",
        "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "3986ea25ab5a428f66c816e16164b519b9ec7231",
        "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences"
      },
      {
        "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
        "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "46a3b966fb744b8992491575586f5a03ee5ce557",
        "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms"
      },
      {
        "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
        "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees"
      },
      {
        "paperId": "60b1e6bd3614086009f9dfcc95f44897f0480d55",
        "title": "Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
        "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"
      },
      {
        "paperId": "31943f1f383a4ca6ba86e172a0c9f26c901f9401",
        "title": "Preference-based Online Learning with Dueling Bandits: A Survey"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "fa2c136d526d7445b02e802ff9601ea9e9663fcb",
        "title": "Eluder Dimension and the Sample Complexity of Optimistic Exploration"
      },
      {
        "paperId": "10468b38072f70cad77109441c73f5f470da33f9",
        "title": "The K-armed Dueling Bandits Problem"
      },
      {
        "paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0",
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"
      },
      {
        "paperId": null,
        "title": "in P separately, so the dependence on d R , N R and d c P, N P are separete. 3 Xie et al."
      },
      {
        "paperId": "66d535bca869644cf21f44e465ff8bdbc93dbce3",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "d39a902effe6873746868ca44860be8f0d13ff8b",
        "title": "Optimal Algorithms for Stochastic Contextual Preference Bandits"
      },
      {
        "paperId": "7066363968c68743b37096b122524501dafb2cdf",
        "title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "How to query human feedback"
      },
      {
        "paperId": null,
        "title": "Speciality vs generality: An empirical study on catastrophic forgetting in \ufb01ne-tuning foundation models"
      },
      {
        "paperId": null,
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "paperId": null,
        "title": "Nearly optimal sample complexity of of\ufb02ine kl-regularized contextual bandits under single-policy concentrability"
      },
      {
        "paperId": null,
        "title": "Regularized rl"
      },
      {
        "paperId": null,
        "title": "Training veri\ufb01ers to solve math word problems"
      }
    ],
    "cited_by": [
      {
        "paperId": "15da3d93b763e090381e2cdcd65099a2c6944108",
        "title": "Learning a Pessimistic Reward Model in RLHF"
      },
      {
        "paperId": "6eedb1cde02d4a5b94cc0d3e1a46de63d7aad404",
        "title": "Alignment of large language models with constrained learning"
      },
      {
        "paperId": "6c306b9b3dfe5976a750362b7d569d60af640998",
        "title": "Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits"
      }
    ],
    "score": 3.0
  },
  {
    "id": "8216a2d3d897f63b46e80211bccb0931ab9fbda9",
    "title": "Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models",
    "authors": [
      "Johan S Daniel",
      "Anand Pal"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "The advancement of large language models has significantly improved natural language processing. However, challenges such as jailbreaks (prompt injections that cause an LLM to follow instructions contrary to its intended use), hallucinations (generating incorrect or misleading information), and comprehension errors remain prevalent. In this report, we present a comparative analysis of the performance of fifteen distinct models, with each model undergoing a standardized test comprising 38 queries across three key metrics: jailbreaks, hallucinations, and comprehension errors. The models are assessed based on the total occurrences of jailbreaks, hallucinations, and comprehension errors. Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models. We have empirically analysed the impact of non-standard Unicode characters on LLMs and their safeguarding mechanisms on the best-performing LLMs, including GPT-4, Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric symbols from Unicode outside the standard Latin block and variants of characters in other languages, we observed a reduction in the efficacy of guardrails implemented through Reinforcement Learning Human Feedback (RLHF). Consequently, these models exhibit heightened vulnerability to content policy breaches and prompt leakage. Our study also suggests a need to incorporate non-standard Unicode text in LLM training data to enhance the capabilities of these models.",
    "url": "https://www.semanticscholar.org/paper/8216a2d3d897f63b46e80211bccb0931ab9fbda9",
    "pdf_url": "https://arxiv.org/pdf/2405.14490.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-05-23",
    "externalIds": {
      "ArXiv": "2405.14490",
      "DBLP": "journals/corr/abs-2405-14490",
      "DOI": "10.48550/arXiv.2405.14490",
      "CorpusId": 269982605
    },
    "references": [
      {
        "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "paperId": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "paperId": "0a691e58a36cdcdaaf72294e88420f79e61e85c7",
        "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"
      },
      {
        "paperId": "409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9",
        "title": "Gradient-Based Language Model Red Teaming"
      },
      {
        "paperId": "411114f989a3d1083d90afd265103132fee94ebe",
        "title": "Mixtral of Experts"
      },
      {
        "paperId": "e5aed8e930b1efa1a5e0aad7ecf3038084cb0a33",
        "title": "Controlled Decoding from Language Models"
      },
      {
        "paperId": "1a9f394b5b7f5bcdecee487174a3f4fc65d30e33",
        "title": "Multilingual Jailbreak Challenges in Large Language Models"
      },
      {
        "paperId": "f3f23f7f9f5369aade19f20bc5d028cce7b9c9aa",
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      },
      {
        "paperId": "764fc56883bf83392cac99a7b5a264ac9fe2cdc5",
        "title": "Low-Resource Languages Jailbreak GPT-4"
      },
      {
        "paperId": "9c6ab9a67339d26fc0360665fab3a4f48a325f47",
        "title": "Large Language Models: Their Success and Impact"
      },
      {
        "paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8",
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "cf1f26e7cbed3958b3c2870656568c299fece6e3",
        "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
        "title": "Are Emergent Abilities of Large Language Models a Mirage?"
      },
      {
        "paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627",
        "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
      },
      {
        "paperId": null,
        "title": "45: Comparison of models handling non-standard Unicode character sets"
      },
      {
        "paperId": null,
        "title": "Hello GPT-4o"
      },
      {
        "paperId": null,
        "title": "Gemini: A Family of Highly Capable Multimodal Models"
      },
      {
        "paperId": null,
        "title": ": Open Models Based on Gemini Research and Technology"
      }
    ],
    "cited_by": [
      {
        "paperId": "ba2325bcb0110488c4d22ed80dc865733c58c575",
        "title": "Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios"
      },
      {
        "paperId": "3d96b5f71760e86efb13ec71d0f249fa81cb5a45",
        "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs"
      },
      {
        "paperId": "07a076e92b7a39ec839d3278fe86a10b320ea8dd",
        "title": "Jailbreak Strength and Model Similarity Predict Transferability"
      }
    ],
    "score": 3.0
  },
  {
    "id": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
    "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution",
    "authors": [
      "Jiahui Li",
      "Lin Li",
      "Tai-wei Chang",
      "Kun Kuang",
      "Long Chen",
      "Jun Zhou",
      "Cheng Yang"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Reinforcement learning from human feedback (RLHF) offers a promising approach to aligning large language models (LLMs) with human preferences. Typically, a reward model is trained or supplied to act as a proxy for humans in evaluating generated responses during the reinforcement training phase. However, current reward models operate as sequence-to-one models, allocating a single, sparse, and delayed reward to an entire output sequence. This approach may overlook the significant contributions of individual tokens toward the desired outcome. To this end, we propose a more fine-grained, token-level guidance approach for RL training. Specifically, we introduce RED, a novel reward redistribition method that evaluates and assigns specific credit to each token using an off-the-shelf reward model. Utilizing these fine-grained rewards enhances the model's understanding of language nuances, leading to more precise performance improvements. Notably, our method does not require modifying the reward model or introducing additional training steps, thereby incurring minimal computational costs. Experimental results across diverse datasets and tasks demonstrate the superiority of our approach.",
    "url": "https://www.semanticscholar.org/paper/8171ff1da2605a410b99b82e2dbd0feb68d021ef",
    "pdf_url": "https://arxiv.org/pdf/2411.08302.pdf",
    "venue": "",
    "publicationDate": "2024-11-13",
    "externalIds": {
      "ArXiv": "2411.08302",
      "CorpusId": 273993249
    },
    "references": [
      {
        "paperId": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
        "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data"
      },
      {
        "paperId": "f34cb468cc4c2f6c13f4b6fd527e5c5256218c77",
        "title": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference"
      },
      {
        "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      },
      {
        "paperId": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
        "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF"
      },
      {
        "paperId": "4a597a081721e436e20b4e85197072e22aaecfad",
        "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"
      },
      {
        "paperId": "beb1997dc3a8dca581c923eacb5370c564a6b5c8",
        "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment"
      },
      {
        "paperId": "bfc223b002401f42b44bca725da6ed6d1b953cff",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      },
      {
        "paperId": "973814cd535facbf4f27c3de477b05bf19366030",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model"
      },
      {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "paperId": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
        "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "1491e2d5d8ba63132ff157e47e824af76c422450",
        "title": "ARGS: Alignment as Reward-Guided Search"
      },
      {
        "paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "paperId": "1c882af20986251bab90318c7e1fae93e0c12cf6",
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency"
      },
      {
        "paperId": "22ab4219371366a4e890382bc0ca606130840ca7",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "paperId": "78c488e2d84bd193a40006b1fceb03e3845b81d4",
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"
      },
      {
        "paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "paperId": "9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a",
        "title": "Foundation models for generalist medical artificial intelligence"
      },
      {
        "paperId": "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b",
        "title": "Aligning Language Models with Preferences through f-divergence Minimization"
      },
      {
        "paperId": "1e34c51b52002796fea6f523b9f794f1d75d9ba8",
        "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "aa6c2814ff94ca098d90f188f95126b5b06ebb69",
        "title": "Nonlinear Programming"
      },
      {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      },
      {
        "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "paperId": "afdfb0de8796b271d4bf3c85ce7ff9e052a5045d",
        "title": "Buy 4 REINFORCE Samples, Get a Baseline for Free!"
      },
      {
        "paperId": "bad355642cd299caca2328dae02563278ea74e8c",
        "title": "RUDDER: Return Decomposition for Delayed Rewards"
      },
      {
        "paperId": "f5265e346382354887340c7b520d639162e2f598",
        "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "1f869232f148ec52066fab06a49855937f84098b",
        "title": "Reinforcement learning by reward-weighted regression for operational space control"
      },
      {
        "paperId": "7533d30329cfdbf04ee8ee82bfef792d08015ee5",
        "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
      },
      {
        "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
        "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
      },
      {
        "paperId": "cbbcf78c6fc189f1f8d1b76dbf314b1132114cfc",
        "title": "Potential-Based Shaping and Q-Value Initialization are Equivalent"
      },
      {
        "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
        "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "5c29049c6cc7e93bd42ccd55d70a5b92120ceec6",
        "title": "Stochastic approximation with two time scales"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "5e35f78ce5f05d92c9be7aec325efb5bd4c99d29",
        "title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning"
      },
      {
        "paperId": "e81466aab95dfba46b27f5d24dd3d2860cad45cd",
        "title": "Empowering LLM-based Machine Translation with Cultural Awareness"
      },
      {
        "paperId": null,
        "title": "2023. Should chatgpt be biased? challenges and risks of bias in large language models"
      },
      {
        "paperId": null,
        "title": "Two time-scale stochastic approximation with controlled markov noise and off-policy temporal-difference learning"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2023. Starling-7b: Improving llm helpfulness & harmlessness with rlaif"
      },
      {
        "paperId": null,
        "title": "2024. Qwen2.5 technical report"
      },
      {
        "paperId": null,
        "title": "1 others"
      },
      {
        "paperId": null,
        "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "paperId": null,
        "title": "2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      },
      {
        "paperId": null,
        "title": "2023. Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models"
      },
      {
        "paperId": null,
        "title": "Harmlessness from ai feedback"
      },
      {
        "paperId": null,
        "title": "AI@Meta. 2024."
      },
      {
        "paperId": null,
        "title": "2023. Stanford alpaca: An instruction-following llama model"
      }
    ],
    "cited_by": [
      {
        "paperId": "da6b8c8fb05ae33fc7cd619ff5708a58fe6b76c8",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective"
      },
      {
        "paperId": "acf2f14fbf6372470d6cc3bd7a8d17b54a00939d",
        "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning"
      },
      {
        "paperId": "a6668bb5dade3adb0c7c595c52aa74ba355de8a4",
        "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation"
      }
    ],
    "score": 3.0
  }
]