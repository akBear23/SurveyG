{
  "0d1c76d45afa012ded7ab741194baf142117c495.pdf": [
    "Direct Preference Optimization (DPO)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Language Model Alignment",
    "Implicit Reward Model",
    "RL-free Algorithm",
    "Closed-form Optimal Policy",
    "Binary Cross-Entropy Loss",
    "Simplified Fine-tuning Pipeline",
    "Computational Efficiency",
    "Human Preferences",
    "Controllable AI Systems",
    "Sentiment Modulation",
    "Policy Optimization",
    "Theoretical Insight"
  ],
  "f2d0f3d47ae850f49a58f4977393bd0025af4bec.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Model (LLM) alignment",
    "Distributed LLM computation",
    "HybridFlow framework",
    "Hierarchical hybrid programming model",
    "3D-HybridEngine",
    "Optimized GPU allocation",
    "Parallelism strategies",
    "Single-controller paradigm",
    "Multi-controller paradigm",
    "Dispatch overhead reduction",
    "Model parameter resharding",
    "Throughput improvement",
    "Heterogeneous workloads"
  ],
  "600ff4c4ae9fc506c86673c5ecce4fa90803e987.pdf": [
    "Reinforcement Learning from AI Feedback (RLAIF)",
    "Direct-RLAIF (d-RLAIF)",
    "Large Language Models (LLMs)",
    "LLM alignment",
    "RLHF scalability",
    "AI-generated preferences",
    "Reward model (RM) staleness",
    "LLM self-improvement",
    "Chain-of-thought (CoT) reasoning",
    "Summarization",
    "dialogue generation",
    "Cost-effective LLM alignment"
  ],
  "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning (RL)",
    "reward function design",
    "EUREKA algorithm",
    "coding LLMs",
    "evolutionary optimization",
    "dexterous manipulation",
    "environment as context",
    "reward reflection",
    "human-level reward design",
    "automated reward design",
    "gradient-free RLHF",
    "complex low-level manipulation",
    "superhuman reward functions"
  ],
  "e01515c6138bc525f7aec30fc85f2adf028d4156.pdf": [
    "Large Language Model (LLM) Alignment",
    "Minimal Human Supervision",
    "Alignment from Scratch",
    "Principle-Driven Self-Alignment (SELF-ALIGN)",
    "Principle Engraving",
    "Topic-Guided Red-Teaming Self-Instruct",
    "Supervision Efficiency",
    "AI Assistant Agents",
    "Rule-based Alignment",
    "Ethical AI",
    "Reliable AI",
    "Context Distillation"
  ],
  "182c7b40ff7560a5545764814338f55a2098e441.pdf": [
    "Reinforced Self-Training (ReST)",
    "Large Language Model (LLM) alignment",
    "human preferences",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "offline reinforcement learning",
    "decoupled data generation and policy improvement",
    "iterative filtering",
    "learned reward model",
    "compute and sample efficiency",
    "machine translation",
    "progressive policy refinement",
    "superiority over supervised learning",
    "growing batch reinforcement learning"
  ],
  "44a9d8b0314d34aff91ccff9207d38eed37216ed.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "reverse-KL regularized contextual bandit",
    "generative model alignment",
    "rigorous theoretical analysis",
    "offline online hybrid learning",
    "Direct Preference Optimization (DPO)",
    "pessimism in RLHF",
    "strategic online exploration",
    "iterative DPO",
    "multi-step rejection sampling",
    "finite-sample guarantees",
    "alignment tax"
  ],
  "c78350e81298ca87bc1d59b466fa40081232caaa.pdf": [
    "LLM reasoning",
    "Reinforcement Learning (RL)",
    "Expert Iteration (EI)",
    "Proximal Policy Optimization (PPO)",
    "Return-Conditioned RL (RCRL)",
    "Reward schemes",
    "Comprehensive comparative study",
    "Exploration limitation",
    "Sample complexity",
    "Math word problems",
    "Simultaneous maj@1 and pass@96 improvement",
    "Model initialization",
    "Deterministic reasoning environments"
  ],
  "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "reward function design challenges",
    "reward hacking",
    "human-in-the-loop learning",
    "reward model training",
    "dynamic objective refinement",
    "diverse human feedback types",
    "AI alignment",
    "Large Language Models (LLMs)",
    "Preference-based Reinforcement Learning",
    "Inverse Reinforcement Learning",
    "active query synthesis",
    "direct policy optimization",
    "robotics and continuous control",
    "safe and user-centric AI"
  ],
  "cb3968152f7d93f53d24b00279a90d5071ddc85a.pdf": [
    "RLHF pipeline analysis",
    "LLM OOD generalisation",
    "Output diversity",
    "Supervised Fine-Tuning (SFT)",
    "Reward Modelling (RM)",
    "Generalisation-diversity tradeoff",
    "Advanced diversity metrics",
    "GPT-4 simulated human evaluation",
    "Text summarisation",
    "Instruction following",
    "Novel OOD test sets",
    "Best-of-N (BoN) sampling",
    "Empirical insights"
  ],
  "548278897d46a54958909bb23bcaecf63e24fadf.pdf": [
    "Reinforcement Learning with Human Feedback (RLHF)",
    "Proximal Policy Optimization (PPO)",
    "Large Language Models (LLMs)",
    "PPO-max algorithm",
    "LLM alignment",
    "human values",
    "training stability",
    "policy constraints",
    "action space modeling metrics",
    "reward models",
    "open-source PPO-max code",
    "hyperparameter sensitivity"
  ],
  "59a2203ef6ea159bb41540bd282e29e80a8ad579.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "output length bias",
    "reward models",
    "reward hacking",
    "Length-Only PPO (LPPO)",
    "length-stratified analysis",
    "anti-length interventions",
    "spurious correlation",
    "reward model non-robustness",
    "preference data biases"
  ],
  "900cd128482bbab4d2752d01ce80c55498b78dd2.pdf": [
    "SWE-RL",
    "Reinforcement Learning for LLMs",
    "Software Engineering tasks",
    "Rule-based reward function",
    "Software evolution data",
    "Patch similarity reward",
    "Emergent generalized reasoning",
    "Data curation pipeline",
    "Bug fixing and issue resolution",
    "SWE-bench Verified benchmark",
    "Execution-free rewards",
    "Predicting unmodified files",
    "41.0% solve rate",
    "Out-of-domain generalization"
  ],
  "d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf.pdf": [],
  "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40.pdf": [
    "Reinforced Token Optimization (RTO)",
    "RLHF",
    "Proximal Policy Optimization (PPO)",
    "Direct Preference Optimization (DPO)",
    "token-wise rewards",
    "Markov Decision Process (MDP) formulation",
    "LLM alignment",
    "DPO-based token-wise reward extraction",
    "integration of DPO and PPO",
    "reward sparsity",
    "data efficiency",
    "performance improvement",
    "scalability"
  ],
  "0c43750030198dbe7fe164e1ce743ec64427bca1.pdf": [
    "Direct Alignment Algorithms (DAAs)",
    "reward over-optimization",
    "Large Language Models (LLMs)",
    "empirical analysis",
    "KL-divergence budgets",
    "scaling laws for DAAs",
    "intra-epoch training dynamics",
    "under-constrained optimization",
    "GPT-4 win-rates",
    "performance degradation",
    "feature exploitation",
    "DPO",
    "IPO",
    "SLiC-HF"
  ],
  "550006bea81e4ccb67743dd1b82a70b86b48d93a.pdf": [
    "RL-VLM-F",
    "Automated reward engineering",
    "Vision Language Foundation Models (VLMs)",
    "Preference-based VLM feedback",
    "Visual-only observations",
    "No ground-truth state",
    "Deformable object manipulation",
    "Two-stage VLM querying",
    "Reinforcement Learning",
    "Reduced human effort",
    "Enhanced RL scalability",
    "Outperforms prior methods",
    "Diverse control tasks"
  ],
  "ec97a1565dff9d2fab1ef489e47296bbef68b680.pdf": [
    "D3PO algorithm",
    "Reward-model-free RLHF",
    "Diffusion models fine-tuning",
    "Human feedback",
    "Multi-step Markov Decision Process (MDP)",
    "Direct Preference Optimization (DPO) extension",
    "Computational efficiency",
    "Reduced GPU memory requirements",
    "Image quality improvement",
    "Content safety enhancement",
    "Latent image representations",
    "Generative models scalability"
  ],
  "db32da8f3b075d566a73512f4ccc2c95449c75a1.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "diverse human preferences",
    "single-reward model limitations",
    "impossibility result",
    "mixture of reward models",
    "Expectation-Maximization (EM) algorithm",
    "MaxMin-RLHF",
    "Egalitarian principle",
    "social choice theory",
    "AI alignment",
    "alignment gap",
    "Total Variation distance",
    "societal biases",
    "language models (LLMs)"
  ],
  "ea75117f34b168a20f2a4309ac2eb685ca6b1436.pdf": [
    "Large Language Models (LLMs)",
    "multi-step reasoning evaluation",
    "Chain-of-Thought Hub (CoT Hub)",
    "few-shot Chain-of-Thought (CoT) prompting",
    "open-source continuous evaluation",
    "model scale-reasoning correlation",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "LLM evaluation benchmarks",
    "final answer accuracy",
    "complex reasoning capabilities",
    "open-source LLM development",
    "LLaMA 65B performance potential"
  ],
  "dacc3a8d45968616f220628dc0db8d5d78c1a389.pdf": [],
  "fb09b581589e1195ff018179c6a11668587c6d64.pdf": [
    "Reinforcement Learning (RL)",
    "Reward function specification",
    "Vision-Language Models (VLMs)",
    "Zero-shot reward models",
    "VLM-RMs",
    "Goal-Baseline Regularization",
    "CLIP-based reward models",
    "Natural language task specification",
    "Scaling effect of VLMs",
    "Vision-based RL tasks",
    "Sample-efficient reward learning",
    "MuJoCo humanoid control"
  ],
  "9732c864d1d4161fcb106f2961d9a80dd4fffc9a.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "sparse reward problem",
    "credit assignment",
    "Attention Based Credit (ABC)",
    "dense reward for free",
    "reward model attention maps",
    "potential-based reward shaping",
    "transformer architecture",
    "training stability",
    "accelerated learning",
    "policy quality",
    "online sampling benefits"
  ],
  "6366cb50a5e2043b2bca11a8f03005c42b036c3e.pdf": [
    "Multi-turn Preference Optimization (MTPO)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Conversation-level Preference Feedback",
    "Contextual Markov Decision Process (CMDP)",
    "Theoretically Grounded Multi-turn Algorithm",
    "Novel Preference-based Q-function",
    "Nash equilibrium convergence",
    "Multi-turn dialogue",
    "Complex tool use",
    "Education Dialogue environment",
    "Direct Preference Optimization for Multi-turn",
    "Long-term planning",
    "Mirror Descent"
  ],
  "b2991a4b2ecc9db0fbd9ca738022801b4e5ee001.pdf": [],
  "a43c2ba35e16d2828ab9b27a92edc68b6af8846d.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "uncertainty estimation",
    "Value-Incentivized Preference Optimization (VPO)",
    "implicit uncertainty handling",
    "online and offline RLHF",
    "optimism and pessimism principles",
    "direct policy optimization",
    "reward calibration",
    "theoretical regret guarantees",
    "intractable confidence intervals",
    "text summarization and dialog generation"
  ],
  "96d6bb5d6abdeda9b2db9af6296527200ba7aa32.pdf": [
    "Large Language Models (LLMs)",
    "Theory-of-Mind (ToM)",
    "in-context learning (ICL)",
    "prompting strategies",
    "Chain-of-Thought (CoT) reasoning",
    "step-by-step thinking",
    "complex reasoning enhancement",
    "latent reasoning capabilities",
    "context-dependent LLM performance",
    "RLHF-trained models",
    "ToM accuracy improvement",
    "surpassing human performance",
    "social understanding in AI"
  ],
  "420af69e5ae3686b709c14a8cec7dc9f90a85681.pdf": [
    "ChatGPT",
    "Large Language Models (LLMs)",
    "Human-Computer Interaction (HCI)",
    "User Psychology",
    "Comprehensive Literature Review",
    "Research Gaps",
    "Transformer Model",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Emotional States",
    "Mental Health",
    "Behavioral Changes",
    "User-Centered AI",
    "Responsible AI Design"
  ],
  "2d906cda427cb2c4a71069423312e57ba4cd5445.pdf": [
    "Reinforcement Learning (RL)",
    "Large Language Models (LLMs)",
    "RL-enhanced LLMs",
    "systematic review",
    "reward modeling",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Reinforcement Learning from AI Feedback (RLAIF)",
    "Direct Preference Optimization (DPO)",
    "LLM alignment",
    "preference fine-tuning",
    "systematic categorization",
    "future research directions",
    "human annotation bias",
    "out-of-distribution content",
    "model interpretability"
  ],
  "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f.pdf": [
    "Critic-RM framework",
    "Reward models (RMs)",
    "RLHF (Reinforcement Learning from Human Feedback)",
    "Self-generated critiques",
    "Critique generation and filtering",
    "Quality-aware critique refinement",
    "Joint fine-tuning",
    "LLM alignment",
    "Interpretable critiques",
    "Improved reward modeling accuracy",
    "Data efficiency",
    "Frontier LLMs",
    "Dynamic weight scheduling"
  ],
  "386cebdba39d2d5f2862a9ab43a8d807f3863dae.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Contrastive Preference Learning (CPL)",
    "regret-based preferences",
    "direct policy learning",
    "Maximum Entropy Reinforcement Learning",
    "optimal advantage function",
    "Noise Contrastive Estimation (NCE)",
    "high-dimensional sequential problems",
    "off-policy learning",
    "efficiency gains",
    "scalable RLHF",
    "MetaWorld Benchmark",
    "theoretical guarantees"
  ],
  "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8.pdf": [
    "Reward model overoptimization",
    "LLM alignment",
    "composite reward models",
    "constrained reinforcement learning (CRL)",
    "proxy points",
    "Lagrange multipliers",
    "dynamic weighting",
    "adaptive gradient-free optimization",
    "on-the-fly proxy point identification",
    "multi-objective alignment",
    "prevents overoptimization",
    "reduced computational cost",
    "dialogue generation"
  ],
  "dd951242ebc94bf633eecc4994c64f46146a1413.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "reward hacking",
    "reward shaping",
    "Preference As Reward (PAR)",
    "Large Language Models (LLMs)",
    "bounded RL rewards",
    "sigmoid function on centered reward",
    "policy gradient variance minimization",
    "critic training stabilization",
    "model alignment",
    "theoretical justification",
    "experimental validation",
    "robustness against reward hacking",
    "data efficiency",
    "PPO-based RLHF"
  ],
  "f2fde6be4b074f509cf974d1aac24019247473ae.pdf": [
    "Reward Models (RMs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Reward Model Evaluation",
    "Preference Proxy Evaluations (PPE)",
    "Downstream LLM Performance",
    "Human Preference Scores",
    "Unbiased Ground Truth Data",
    "Proxy Tasks/Metrics",
    "Direct Correlation Validation",
    "Cost-effective Evaluation",
    "Natural Response Sampling",
    "Iterative RM Improvement",
    "Chatbot Arena"
  ],
  "eff0410f7d5d78ea6874596a0a77b184d03ecca5.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Algorithmic bias",
    "Preference collapse",
    "KL-based regularization",
    "Large Language Models (LLMs)",
    "Preference Matching (PM) RLHF",
    "Preference Matching regularizer",
    "Ordinary differential equation (ODE)",
    "Bradley–Terry–Luce (BTL) model",
    "Preference Matching Divergence (PMD)",
    "Provable alignment",
    "Human preferences diversity",
    "Natural language generation",
    "Fairness and robustness"
  ],
  "302065b71e09783cab30eed17e85eb437e279ae3.pdf": [
    "Big-Math dataset",
    "Reinforcement Learning (RL) in LLMs",
    "mathematical reasoning",
    "high-quality math dataset curation",
    "open-ended problems",
    "closed-form solutions",
    "human-in-the-loop filtering",
    "Big-Math-Reformulated",
    "systematic problem reformulation",
    "multi-stage data deduplication",
    "data quality-quantity trade-off",
    "scalable RL training",
    "LLM generalization",
    "verifiable solutions"
  ],
  "78a2943fd2424a5515d595d6bdc54b9a4dbb4389.pdf": [
    "Weighted Preference Optimization (WPO)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Off-policy RLHF",
    "Distributional gap problem",
    "Direct Preference Optimization (DPO)",
    "Reweighting preference pairs",
    "Weight alignment mechanism",
    "Simulating on-policy learning",
    "LLM alignment",
    "State-of-the-art (SOTA) results",
    "Instruction following benchmarks",
    "Cost-efficient LLM alignment",
    "Length-normalized sequence probability"
  ],
  "911e9915df23c4bc59f10608af2aee8335e7a4a5.pdf": [],
  "521c2905e667ad6d2162ac369cf3f85d70e0f477.pdf": [
    "RLHF preference poisoning",
    "Language Model manipulation",
    "Reward Model vulnerability",
    "Naturalistic poisoned data",
    "Oracle LM data generation",
    "Stealthy adversarial attacks",
    "RL amplification of biases",
    "Systematic poisoning framework",
    "Empirical validation",
    "LM alignment security",
    "Data poisoning detection",
    "Best-of-N Reinforcement Learning",
    "Public preference datasets"
  ],
  "e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53.pdf": [],
  "8115ffbbadd1055424d18369dba66ce32a572800.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Human preference alignment",
    "ChatGLM-RLHF pipeline",
    "Production integration",
    "Scalable training",
    "Bucket-Based Length Balancing",
    "Catastrophic forgetting mitigation",
    "L2 regularization (reward model)",
    "Reward baseline subtraction (PPO)",
    "Chinese language alignment",
    "Human preference data collection"
  ],
  "7f96bb27a8fca35b1f7d02ee319a64be04114809.pdf": [
    "Large Language Models (LLMs)",
    "Traffic Signal Control (TSC)",
    "LLM-Assisted Light (LA-Light)",
    "Human-mimetic reasoning",
    "Complex urban environments",
    "Hybrid framework",
    "Closed-loop control system",
    "Perception and decision-making tools",
    "Adaptability to rare events",
    "Explainable decision justifications",
    "Traffic congestion management",
    "Reinforcement Learning (RL)-based TSC",
    "Simulation platform validation",
    "Average waiting time reduction"
  ],
  "c085e88a0351e393609a95305afc1db792d1db0f.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF) reward models",
    "Large Language Models (LLMs)",
    "human values encoding",
    "critical historical and conceptual analysis",
    "foundational assumptions",
    "domain shift risk",
    "transparency and rigorous evaluation",
    "sociotechnical impacts",
    "value alignment",
    "ill-posed assumptions",
    "reward hacking",
    "interdisciplinary framework",
    "measuring and mitigating harms",
    "optimizing preferences"
  ],
  "777d4ec0148c34b0bfab91e9ac3a902e420b891e.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning from AI Feedback (RLAIF)",
    "verbosity bias",
    "LLM evaluators",
    "bias quantification metric",
    "human-LLM preference discrepancy",
    "LLM alignment",
    "GPT-4 verbosity preference",
    "human feedback (oracle)",
    "accuracy parity",
    "word count difference",
    "creative writing tasks",
    "bias mitigation strategies"
  ],
  "57451ce18f3035fcadf64db38420434f9299b7f3.pdf": [
    "Autonomous Driving Safety",
    "Human-Centric Autonomous Driving",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "LLM-Enhanced RLHF Framework",
    "Multimodal Sensor Feedback",
    "Multimodal Preference Modeling",
    "LLM-driven Feedback Interpretation",
    "Multi-Agent Simulation",
    "Human-in-the-Loop Simulation",
    "Human-Aligned Autonomous Driving"
  ],
  "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3.pdf": [],
  "519d5ccbd5aec517ba987209e17afd4741ac9b8a.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Multi-task learning",
    "Reward Hacking",
    "Multi-objective optimization",
    "Constrained Generative Policy Optimization (CGPO)",
    "Mixture of Judges (MoJ)",
    "Task-Specific Optimization",
    "CRPG",
    "CODPO",
    "CRRAFT",
    "Reward Hacking Mitigation",
    "LLM Alignment"
  ],
  "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "prompt-data construction",
    "reward hacking",
    "model response diversity",
    "data-driven bottlenecks",
    "Hybrid Reward System",
    "Reasoning Task Verifiers (RTV)",
    "Generative Reward Model (GenRM)",
    "Pre-PPO prompt selection",
    "early-stage task prioritization",
    "PPO-based RLHF",
    "Large Language Models (LLMs)",
    "strategic data construction",
    "performance scaling"
  ],
  "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc.pdf": [
    "Objective mismatch",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Reward model training",
    "Policy optimization",
    "Unintended behaviors",
    "Reward hacking",
    "Overoptimization",
    "Conceptual framework",
    "Evaluation metrics misalignment",
    "Alignment ceiling",
    "Systematic identification",
    "Fundamental limitation",
    "Human feedback"
  ],
  "cdd0e94e51a02bac22ca5e94fa95daa18f36e226.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "overoptimization",
    "reward model uncertainty",
    "Uncertainty-Penalized RLHF (UP-RLHF)",
    "diverse reward LoRA ensemble",
    "nuclear norm maximization",
    "uncertainty regularization",
    "LLM alignment",
    "parameter-efficient uncertainty quantification",
    "mitigation of overoptimization",
    "KL regularization limitations"
  ],
  "c243df958269bf501f874ef213ba6cc904f24ea9.pdf": [],
  "f7734502f2d9d464b5bd2c62a6805ca492ea61c0.pdf": [],
  "eb6ef63df104c1b35bbc2400f00285b3414400b2.pdf": [
    "SimpleTIR",
    "Reinforcement Learning (RL)",
    "Multi-turn Tool-Integrated Reasoning (TIR)",
    "Large Language Models (LLMs)",
    "Training instability",
    "Gradient norm explosions",
    "Trajectory filtering algorithm",
    "Void turns",
    "Zero RL training",
    "Emergent reasoning patterns",
    "Mathematical reasoning tasks",
    "State-of-the-art performance",
    "Credit assignment problem",
    "Distributional drift"
  ],
  "2044ab82dcb2c11ef660bd51d40130fe182f98d3.pdf": [
    "Zeroth-Order Optimization",
    "Ranking Oracle",
    "Black-box non-convex optimization",
    "Human feedback",
    "ZO-RankSGD",
    "Rank-based stochastic estimator",
    "Provable convergence",
    "Reinforcement Learning with Human Feedback (RLHF)",
    "Generative models",
    "Direct policy optimization",
    "O(√(d/T)) convergence rate",
    "(m",
    "k)-ranking oracle"
  ],
  "9ddfb1583ce7f5370ace2751bb5f260fa4af1961.pdf": [
    "Large Language Models (LLMs)",
    "Supervised Fine-Tuning (SFT) bypass",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Direct Harmless RLHF",
    "Foundational capability degradation",
    "User preference alignment",
    "Toxic output reduction",
    "Response length optimization",
    "General capability preservation",
    "Conversational abilities enhancement",
    "Reward Model",
    "PPO algorithm",
    "Mistral-Plus 7B",
    "LLM alignment rethinking"
  ],
  "a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4.pdf": [
    "Affective Computing",
    "Large Language Models",
    "Affective Understanding",
    "Affective Generation",
    "LLM adaptation techniques",
    "Instruction Tuning",
    "Prompt Engineering",
    "Reinforcement Learning",
    "Evaluation practices",
    "Benchmarks",
    "NLP-oriented survey",
    "Paradigm shift",
    "Ethics and data quality",
    "Resource efficiency",
    "LLMs as evaluation tools"
  ],
  "c724da2469bba1b98e9aec9deb4c7073d624f308.pdf": [],
  "cb99a85c651a3976d9a8db0951d0f6edfe1addce.pdf": [],
  "208fdbf3ac095740a53230523db3828a52414da6.pdf": [],
  "ba015c5d3f5b44e36363b90070bb3301d21ae57e.pdf": [
    "LLM misuse",
    "safety alignment mechanisms",
    "open-sourced LLMs",
    "white-box access",
    "Probability Manipulation (ProMan)",
    "model hacking attack",
    "affirmative prefix",
    "negation reversing",
    "logit manipulation",
    "token generation probabilities",
    "direct LLM generation manipulation",
    "insufficient safety alignment",
    "prompt engineering attacks",
    "countermeasures for LLM misuse"
  ],
  "983c01d00102075dae128b8ef9f01abef98720b5.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Scaling Properties",
    "Systematic Empirical Investigation",
    "Multi-task Reward Model",
    "Process Supervision",
    "Asymmetric Reward Shrinking",
    "Policy Optimization",
    "Diminishing Returns (RLHF Scaling)",
    "LLM Alignment",
    "Data Composition",
    "Computational Efficiency",
    "Generalization Limitations"
  ],
  "faae9de3d314e8731b0505607298fd826e3de1a7.pdf": [
    "RELC framework",
    "Large Language Models (LLMs)",
    "Reinforcement Learning (RL)",
    "sparse reward signals",
    "dense intrinsic rewards",
    "LLM critique ability",
    "text generation",
    "sample efficiency",
    "aligning language models",
    "temporal credit assignment problem",
    "critic language model",
    "self-critique",
    "reduced human annotation"
  ],
  "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768.pdf": [],
  "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "LLM alignment",
    "Online Inverse Reinforcement Learning (IRL)",
    "known transition dynamics",
    "compounding error",
    "distributional shift",
    "reward model learning",
    "Proximal Policy Optimization (PPO)",
    "Supervised Fine-Tuning (SFT)",
    "high action dimensionality",
    "feedback sparsity",
    "theoretical justification for RLHF",
    "off-policy RL methods"
  ],
  "40cc085a2608985b753c38dc245ac21be592ed08.pdf": [
    "HRLAIF",
    "RLAIF",
    "LLM alignment",
    "helpfulness",
    "harmlessness",
    "multi-stage hybrid helpfulness labeling",
    "AI-driven Red Teaming",
    "Reward Model training optimizations",
    "human satisfaction rate",
    "AI labeling accuracy",
    "open-domain",
    "cost-effective alignment",
    "problem-solving prompts"
  ],
  "e7c9478b9dab56b6113a85d1c53723eb5d09e58f.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "REAL system",
    "Dynamic parameter reallocation",
    "Execution plans",
    "Fine-grained resource allocation",
    "Markov Chain Monte Carlo (MCMC) sampling",
    "Profiling-assisted runtime estimator",
    "3D parallelism",
    "Training throughput",
    "GPU utilization",
    "Multi-model multi-task workflows",
    "DPO",
    "ReMax",
    "GRPO"
  ],
  "bcf2bd95a6f60dd2998b57c26873d31461011e8d.pdf": [],
  "e6fac5811e260466366f3a905076c33e252405ef.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Reward model training",
    "Pairwise preference dataset selection",
    "Optimal Design for Preference Optimization (ODPO)",
    "Offline data collection framework",
    "Pure exploration linear contextual dueling bandit",
    "Simple regret minimization",
    "Optimal design theory",
    "Theoretical guarantees (upper and lower bounds)",
    "Bradley-Terry model",
    "Human feedback collection efficiency",
    "Worst-case guarantees"
  ],
  "aece81d7dcbf2929e650a6094af63666e95a0c83.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "LLM safety alignment",
    "helpfulness-safety trade-off",
    "\"over-safe\" phenomenon",
    "Equilibrate RLHF framework",
    "Fine-grained Data-centric (FDC) approach",
    "Adaptive Message-wise Alignment (AMA) approach",
    "gradient masking strategy",
    "Schmitt trigger",
    "fine-grained safety data categorization",
    "data-centric alignment",
    "reduced training data requirements"
  ],
  "a19b75036dd95bc6eba87c1589de3b2dff5c25a1.pdf": [],
  "4376e282954ec59eaeca345ce4ec99219a075670.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Model (LLM) alignment",
    "Pairwise-RL framework",
    "Pairwise Reward Model",
    "generative reward modeling",
    "Reward Model calibration",
    "position bias mitigation",
    "Pairwise Proximal Policy Optimization (PPO)",
    "win probability maximization",
    "implicit weighting mechanism",
    "value function alignment strategy",
    "outperforms traditional RLHF",
    "improved human preference alignment"
  ],
  "3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3.pdf": [
    "SASR (Step-wise Adaptive SFT-RL)",
    "Large Language Models (LLMs)",
    "Supervised Fine-tuning (SFT)",
    "Reinforcement Learning (RL)",
    "Adaptive hybrid training framework",
    "Gradient-norm-based adaptation",
    "Theoretical unification of SFT and RL",
    "Forget-stagnation trade-off",
    "Reward hacking",
    "Task-specific LLMs",
    "Improved cross-task generalization",
    "Dynamic ratio selection",
    "Mathematical reasoning",
    "Adaptive switching indicator"
  ],
  "85a1f32e4794b4c176f3330364bc39977a50d258.pdf": [
    "Neural Machine Translation (NMT)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Exposure Bias",
    "Neural Quality Metrics (COMET",
    "COMET-QE)",
    "PPO-based RL training",
    "Data Filtering with COMET-QE",
    "Inference-time Reranking",
    "Minimum Bayes Risk (MBR) decoding",
    "Unified NMT Alignment Framework",
    "Synergistic RL and MBR combination",
    "Unsupervised NMT potential",
    "Translation Quality Alignment"
  ],
  "ee3c57d53327c5f84a8f3988f592c6e2479c1924.pdf": [
    "Inverse-Q*",
    "Token-level Reinforcement Learning",
    "LLM Alignment",
    "Without Preference Data",
    "RLHF",
    "Direct Preference Optimization (DPO)",
    "Reward Imitation Lemma",
    "Direct Optimal Policy Estimation",
    "No Reward/Value Models",
    "Low-resource settings",
    "Faster convergence",
    "Helpfulness and harmlessness"
  ],
  "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Adaptive Preference Scaling",
    "Instance-specific Scaling Parameter",
    "Adaptive Preference Loss Function",
    "Distributionally Robust Optimization (DRO)",
    "Improved Reward-Policy Alignment",
    "Efficient Optimization Algorithm",
    "Direct Preference Optimization (DPO)",
    "Preference Uncertainty",
    "Robotic Control",
    "Natural Language Generation (NLG)",
    "Varying Preference Strengths"
  ],
  "a0748478cd2752b733b4183dbd0dcd1031c38b6e.pdf": [],
  "5b6f58a8b0098dbc7bfa735359a2f6ae7ea4cbe6.pdf": [
    "Reinforcement Learning (RL)",
    "Sample-inefficiency",
    "Curse of dimensionality",
    "Large observation spaces",
    "Human feedback",
    "Large Language Model (LLM) feedback",
    "Systematic review",
    "Taxonomy and structured overview",
    "Generalization issues",
    "Feedback modalities",
    "Enhanced performance",
    "Sample-efficient RL",
    "Human-AI collaboration",
    "Attention mechanisms"
  ],
  "57e959b74f36a30cd62d0abd4204f08907b42e87.pdf": [
    "RL-of-Thoughts (RLoT)",
    "Large Language Models (LLMs)",
    "complex reasoning tasks",
    "inference-time techniques",
    "reinforcement learning (RL)",
    "dynamic logical structures",
    "navigator model",
    "Markov Decision Process (MDP)",
    "human cognition-inspired logic blocks",
    "LLM self-evaluation",
    "Process Reward Model (PRM)",
    "computational efficiency",
    "model transferability",
    "sub-10B LLM enhancement"
  ],
  "0940c04de5a9f5dbea57aa0c7953e3fe4a052422.pdf": [
    "Reward hacking",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Energy Loss Phenomenon",
    "internal representation dynamics",
    "Energy loss-aware PPO (EPPO)",
    "contextual relevance",
    "entropy-regularized RL",
    "LLM final layer",
    "mitigating reward hacking",
    "theoretical foundation",
    "human preferences alignment"
  ],
  "4a081d1e1ea87ffcbf64cfe1c8a4a1da24c2c1b8.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "reward function learning",
    "dual active reward learning",
    "context-dependent heterogeneous teacher model",
    "D-optimal design",
    "pessimistic reinforcement learning",
    "human preference alignment",
    "simultaneous conversation and teacher selection",
    "distributional shift mitigation",
    "minimal generalized variance",
    "reward accuracy improvement",
    "data-efficient RLHF"
  ],
  "58f614941629541c8c04acdb8acb9e3fb350ac5a.pdf": [
    "CodePMP",
    "Large Language Models (LLMs)",
    "Reasoning abilities (logical",
    "mathematical)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Preference Model Pretraining (PMP)",
    "Synthesized code-preference pairs",
    "CodeLLMs",
    "Reward model (RM) finetuning",
    "Scalable data synthesis pipeline",
    "Sample efficiency (80x improvement)",
    "Best-of-N (BoN) accuracy",
    "Combined LRM + LLM loss",
    "Cross-architecture generalization",
    "Leveraging source code"
  ],
  "54c4f894d41095b18294932c0ee8c39ffe3c0ac1.pdf": [
    "REvolve",
    "Reward design problem",
    "Reinforcement Learning (RL)",
    "Large Language Models (LLMs)",
    "Evolutionary Algorithms",
    "LLMs as intelligent genetic operators",
    "Human feedback as fitness function",
    "Interpretable reward functions",
    "Elo rating system",
    "Tacit human knowledge",
    "Autonomous driving",
    "Meta-heuristic optimization",
    "Human-in-the-loop RL",
    "Outperforms state-of-the-art"
  ],
  "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "strategic misreporting",
    "online learning from strategic human feedback",
    "dynamic Bayesian game",
    "Online Weighted Aggregation Mechanism",
    "dynamic labeler weights",
    "truthful mechanism",
    "sublinear regret O(T^1/2)",
    "vanishing time-average regret",
    "non-monetary incentives",
    "LLM fine-tuning"
  ],
  "1ab303435946a859620ca334556ca3b0e53464fc.pdf": [
    "Large Language Models (LLMs)",
    "Edge Devices",
    "On-Premise Medical Assistance",
    "Resource-Constrained Systems",
    "Model Optimization",
    "Tiny-LLMs",
    "Low-Rank Adaptation (LoRA)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "LangChain Integration",
    "Privacy-Preserving AI",
    "Hallucination Mitigation",
    "Domain-Specific Fine-tuning",
    "Healthcare Accessibility"
  ],
  "fde0ffe77186561497ce15e4faca82db11dacd64.pdf": [
    "Reinforcement Learning with Verifiable Rewards (RLVR)",
    "superficial self-reflection",
    "RISE (Reinforcing Reasoning with Self-Verification)",
    "online Reinforcement Learning framework",
    "integrated problem-solving and self-verification",
    "dual use of verifiable rewards",
    "on-policy verification data generation",
    "joint optimization",
    "Large Language Models (LLMs)",
    "mathematical reasoning benchmarks",
    "Proximal Policy Optimization (PPO)",
    "robust self-verification skills",
    "unified RL objective",
    "self-aware LLM reasoners"
  ],
  "32fdc2cf900e1af1acc4264f312a55c1de5879d3.pdf": [
    "RLHF",
    "training data memorization",
    "k-approximate counterfactual memorization",
    "code completion models",
    "Large Language Models (LLMs)",
    "direct preference learning (IPO)",
    "reward model training",
    "sensitive data leakage",
    "memorization dynamics in RLHF",
    "empirical analysis",
    "initial fine-tuning memorization persistence",
    "reduced memorization (RLHF)",
    "increased memorization (IPO)"
  ],
  "85c961d5b3fea95b48f94c0461782e887a8b3b0f.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "reward inference",
    "Direct Preference Optimization (DPO)",
    "Zeroth-Order Policy Gradient (ZPG)",
    "Zeroth-Order Block-Coordinate Policy Gradient (ZBCPG)",
    "direct policy optimization without reward inference",
    "stochastic MDPs",
    "infinite state/action spaces",
    "provably efficient algorithms",
    "polynomial convergence rates",
    "local value function difference",
    "policy gradient estimation"
  ],
  "2530e6ecbd0198012bb8ee4359acb9241cefec95.pdf": [
    "Large Language Models (LLMs)",
    "LLM alignment",
    "Reward Model (RM)",
    "GazeReward framework",
    "eye-tracking (ET)",
    "ET prediction models",
    "synthetic ET features",
    "implicit feedback integration",
    "human preference modeling",
    "scalable implicit feedback",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "ablation study",
    "accuracy gains",
    "AI alignment"
  ],
  "ff5b0cc250d93b97fe60e1b0c2048708d6875595.pdf": [
    "Large Language Models (LLMs)",
    "AI alignment",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Reinforcement Learning from AI Feedback (RLAIF)",
    "HHH principle",
    "sociotechnical critique",
    "curse of flexibility",
    "AI safety",
    "comprehensive sociotechnical design",
    "limitations of RLHF/RLAIF",
    "ethical considerations",
    "jailbreaking LLMs",
    "system safety"
  ],
  "96f0afd55fb1b37fcd683c7e3aa1704d18b60a73.pdf": [],
  "77f0687571a213c784f0901a821f22b2a03f3ddd.pdf": [
    "Self-Play with Adversarial Critic (SPAC)",
    "LLM alignment",
    "offline preference optimization",
    "provable convergence guarantees",
    "scalable LLM alignment",
    "Stackelberg game formulation",
    "on-average pessimistic policy evaluation",
    "Direct Preference Optimization (DPO-like)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "single-policy concentrability",
    "reward hacking mitigation",
    "general function approximation",
    "competitive empirical performance"
  ],
  "612ec1fbb54cfe61de62bc5922346d20f15f5023.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Parameter-Efficient Reinforcement Learning from Human Feedback (PE-RLHF)",
    "Low-Rank Adaptation (LoRA)",
    "Large Language Models (LLMs)",
    "Vision-Language Models (VLMs)",
    "computational cost reduction",
    "memory usage reduction",
    "reward model training",
    "policy model optimization",
    "parameter-efficient fine-tuning (PEFT)",
    "model alignment",
    "empirical validation",
    "comparable performance",
    "scalability"
  ],
  "32608b3b06793a9b453fa742756b34c82afdb9d7.pdf": [
    "Efficient Sampling-based RL (ESRL)",
    "Sequence generation models",
    "Reinforcement Learning (RL)",
    "Computational inefficiency",
    "Two-stage sampling",
    "Dynamic sampling",
    "Memory consumption reduction",
    "Training time reduction",
    "Machine Translation",
    "Abstractive Summarization",
    "RL from Human Feedback (RLHF)",
    "Autoregressive sampling",
    "Model capability estimation",
    "Optimization fusion",
    "FIFO-based baseline reward"
  ],
  "103436cbc7509b306c2fe82e62c9e63d29064c95.pdf": [],
  "ba7fe3757a5343e73b7961b29fe5d65dbb0ef971.pdf": [
    "Large Language Models (LLMs)",
    "Optimization Modeling",
    "Solver-Informed Reinforcement Learning (SIRL)",
    "Reinforcement Learning with Verifiable Rewards (RLVR)",
    "Optimization Solvers",
    "Formal Correctness",
    "Solver-as-Oracle Paradigm",
    "Instance-Enhanced Self-Consistency",
    "Partial KL Surrogate Function",
    "Automated Optimization Modeling",
    "LLM Hallucinations",
    "LP Files",
    "State-of-the-art Performance",
    "Democratizing Optimization"
  ],
  "c9e4efa58fd42a07da27ae70254981715cc257d5.pdf": [
    "Reward over-optimization",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Extrapolation error",
    "Out-of-distribution (OOD) responses",
    "Behavior-Supported Policy Optimization (BSPO)",
    "Value regularization",
    "In-distribution (ID) region",
    "Behavior-supported Bellman operator",
    "OOD detection",
    "Theoretical convergence guarantees",
    "ScoreLM model",
    "Mitigating reward over-optimization",
    "Aligning LLMs with human preferences"
  ],
  "d37e78b26ca0333c92a7445e20bb9e859242d5e1.pdf": [],
  "33c611e6b1c071dee7a928b5263e5baf3b23ead6.pdf": [
    "Large Language Model (LLM) alignment",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Differential Privacy (DP)",
    "End-to-End DP Framework",
    "Differentially Private PPO (DPPPO)",
    "DP Reward Model training",
    "LoRA integration",
    "Privacy-utility trade-off",
    "Privacy leakage",
    "Mathematical privacy guarantees",
    "Instruction-following LLMs",
    "Privacy amplification by subsampling",
    "Parallel composition theorem",
    "Larger model sizes for DP"
  ],
  "d0ffb09a00b67365efb9e217c3fd45d804733810.pdf": [],
  "eb291a2e237774b162d9c51c21c4868795589e94.pdf": [],
  "cb660ea0c8c14097513a2a2199ed3a18799683be.pdf": [],
  "bd0ed34897bcf69d482caf16e7baab1b725d8b88.pdf": [],
  "adc8c71591ee5b043447a7d7db8ae09a8a9f1251.pdf": [
    "Interpretable Preferences",
    "Multi-Objective Reward Modeling",
    "Mixture-of-Experts (MoE) Scalarization",
    "Absolute-Rating Reward Model (ArmoRM)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "LLM Alignment",
    "Dynamic Context-Conditioned Gating",
    "Verbosity Bias Mitigation",
    "Reward Hacking",
    "State-of-the-art performance",
    "RewardBench",
    "Steerable Reward Models"
  ],
  "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "online vs. offline alignment",
    "on-policy sampling",
    "performance gap",
    "IPO loss",
    "KL divergence budget",
    "discriminative vs. generative capabilities",
    "AI alignment",
    "systematic empirical investigation",
    "generative quality gap",
    "reward model",
    "fundamental challenge for offline alignment"
  ],
  "d084517f14ee247883de0f4dd58bb923e418157d.pdf": [
    "LLaMA-Berry",
    "mathematical reasoning",
    "Large Language Models (LLMs)",
    "Olympiad-level problems",
    "Self-Refine Monte Carlo Tree Search (SR-MCTS)",
    "Pairwise Preference Reward Model (PPRM)",
    "Enhanced Borda Count (EBC)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "solution search efficiency",
    "local optima avoidance",
    "GPT-4 Turbo comparable performance",
    "open-source LLMs",
    "iterative self-refinement",
    "global quantile scores"
  ],
  "f21d0177e9374bb8579c1d9c71319f212f62b3d5.pdf": [
    "InferAligner",
    "inference-time alignment",
    "harmlessness alignment",
    "cross-model guidance",
    "conditional intervention",
    "Safety Steering Vectors (SSVs)",
    "Large Language Models (LLMs)",
    "Multimodal Large Language Models (MLLMs)",
    "activation engineering",
    "utility preservation",
    "training-free alignment",
    "MM-Harmful Bench",
    "guidance gate"
  ],
  "830c277b2992f59ec2f21982e245bd1e17dd85ca.pdf": [
    "Step-level Value Preference Optimization (SVPO)",
    "Mathematical Reasoning",
    "Multi-step Symbolic Reasoning",
    "Monte Carlo Tree Search (MCTS)",
    "Automatic Step-level Preference Annotation",
    "Explicit Value Model Integration",
    "Direct Preference Optimization (DPO) Limitations",
    "Fine-grained Preference Learning",
    "Step-level Beam Search (SBS)",
    "Novel Regularization Term",
    "State-of-the-art Performance",
    "Out-of-domain Generalization",
    "Large Language Models (LLMs)"
  ],
  "44162aa2763c88a384d9c51d60eafcc59277a1c9.pdf": [
    "Decoding-time Realignment (DeRa)",
    "Language Model Alignment",
    "KL Divergence Regularization",
    "Hyperparameter Tuning Acceleration",
    "Retraining-free Alignment",
    "Geometric Mixtures Theory",
    "Autoregressive Logit Combination",
    "Dynamic Alignment Control",
    "Reward Hacking Mitigation",
    "Computational Efficiency",
    "Human Preferences",
    "Summarization",
    "Hallucination Mitigation"
  ],
  "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290.pdf": [
    "Self-Exploring Language Models (SELM)",
    "active preference elicitation",
    "online LLM alignment",
    "optimism-based active exploration",
    "RM-free objective",
    "bilevel optimization objective",
    "guided exploration",
    "mitigation of DPO limitations",
    "iterative online algorithm",
    "policy gradient analysis",
    "significant performance gains",
    "computational efficiency"
  ],
  "613a32f18388958cc60dbb906d87fc7f206c0e66.pdf": [
    "SPIN-Diffusion",
    "self-play fine-tuning",
    "diffusion models",
    "text-to-image generation",
    "no paired preference data",
    "single image per text prompt",
    "full diffusion trajectory objective",
    "score function parameterization",
    "iterative self-improvement",
    "theoretical guarantees",
    "surpasses RLHF methods",
    "exceptional dataset utilization"
  ],
  "c08fa8d84104ec1a8304f75b72bed411100aaf5c.pdf": [
    "OPENTHINK IMG framework",
    "Visual Tool Reinforcement Learning (V-TOOLRL)",
    "Large Vision-Language Models (LVLMs)",
    "adaptive visual tool use",
    "Reinforcement Learning (RL)",
    "Supervised Fine-Tuning (SFT) limitations",
    "distributed tool deployment",
    "scalable trajectory generation",
    "chart reasoning tasks",
    "end-to-end framework",
    "dynamic tool invocation",
    "Group-wise Proximal Policy Optimization (GRPO)",
    "rule-based accuracy reward"
  ],
  "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d.pdf": [
    "Large Language Models (LLMs)",
    "Human alignment",
    "Direct Preference Optimization (DPO)",
    "Iterative self-alignment",
    "DICE algorithm",
    "DPO implicit reward model",
    "Bootstrapping preference data",
    "Length-regularized reward shaping",
    "Experience replay",
    "Catastrophic forgetting",
    "Length bias",
    "Cost-effective alignment",
    "Length-controlled win rate",
    "AlpacaEval 2"
  ],
  "33c445469aa9688837b0f76a2e55bcabe29dce47.pdf": [
    "Chain-of-thought (CoT) LLMs",
    "causal analysis framework",
    "Structural Causal Models (SCMs)",
    "LLM reasoning mechanisms",
    "causal interventions",
    "faithfulness and consistency",
    "reasoning vs. explaining",
    "in-context learning (ICL)",
    "post-training techniques (SFT",
    "RLHF)",
    "mathematical and logical reasoning",
    "model size impact",
    "underlying causal mechanism"
  ],
  "4ed96712afa0d0e82cddb3d669d4e9f60195aecb.pdf": [
    "LLM-empowered Recommender Systems",
    "Adversarial Attacks",
    "Safety Vulnerability",
    "Black-box Attack",
    "CheatAgent Framework",
    "LLM-based Attack Agent",
    "Adversarial Perturbations",
    "Insertion Positioning",
    "Self-Reflection Policy Optimization",
    "Prompt Tuning",
    "Novel Problem Formulation",
    "Untargeted Attack",
    "Attack Effectiveness"
  ],
  "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "alignment tax",
    "Online Merging Optimizer",
    "gradient steering",
    "model merging",
    "Supervised Fine-tuning (SFT)",
    "bonus-tax trade-off",
    "OnDARE optimizer",
    "catastrophic forgetting",
    "LLM alignment",
    "alignment reward",
    "foundational capabilities"
  ],
  "9123ec44f0026e70f8398b904e97a4224866bb36.pdf": [
    "Bi-Factorial Preference Optimization (BFPO)",
    "LLM safety-helpfulness alignment",
    "multi-objective RLHF re-parameterization",
    "supervised learning framework",
    "novel labeling function",
    "theoretical equivalence",
    "reduced red teaming reliance",
    "efficient harmlessness improvement",
    "preserving helpfulness",
    "Large Language Models (LLMs)",
    "Direct Preference Optimization (DPO)",
    "computational efficiency"
  ],
  "3d43594804af065c89d4f5be5d0a17957b633092.pdf": [
    "Reward overoptimization",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "lightweight reward uncertainty quantification",
    "last layer embeddings",
    "AdvPO (Adversarial Policy Optimization)",
    "distributionally robust optimization",
    "computational efficiency",
    "proxy reward model",
    "ensemble methods",
    "MaxMin objective",
    "closed-form solution",
    "human-assisted evaluations",
    "LLM alignment"
  ],
  "ea9809331f53bd9b3013f49cc10ac79965e40b2e.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "LLM creativity",
    "syntactic and semantic diversity",
    "diversity loss",
    "multi-faceted experimental framework",
    "attractor states",
    "mode collapse",
    "token entropy analysis",
    "sentence embeddings (SBERT)",
    "t-SNE visualization",
    "alignment-creativity trade-off",
    "marketing applications",
    "prompt engineering"
  ],
  "ca39d564e30c35ccc95546272903674f89e5ad0f.pdf": [],
  "5695f983699b36af61851d8025aab9caea970eae.pdf": [
    "Large Language Model (LLM) reasoning",
    "Reinforcement Learning from Internal Feedback (RLIF)",
    "unsupervised reward proxies",
    "entropy-based internal feedback signals",
    "policy entropy minimization",
    "GRPO algorithm",
    "math reasoning benchmarks",
    "Base LLMs",
    "instruction-tuned models",
    "RLIF performance degradation",
    "transitional words frequency",
    "theoretical analysis",
    "empirical validation",
    "conditional utility of RLIF"
  ],
  "b8f435d3b8202f1086be9d791857c20cb3a4a90a.pdf": [
    "Pareto Optimal Preference Learning (POPL)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "hidden context",
    "pluralistic alignment",
    "lexicase selection",
    "multi-objective optimization",
    "label-free alignment",
    "Pareto-optimal policies",
    "sequential RL tasks",
    "LLM fine-tuning",
    "robotics",
    "contradictory preferences",
    "robust personalization",
    "fairness in AI"
  ],
  "280598d6613a071db232422b914f613a37cf13d1.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Model (LLM) alignment",
    "training instability",
    "Direct Preference Optimization (DPO)",
    "Variational Alignment with Re-weighting (VAR)",
    "variational inference",
    "reward-driven re-weighted supervised fine-tuning",
    "non-negative variational weights",
    "in-batch normalization",
    "optimal RLHF policy",
    "stable optimization",
    "Helpful and Harmless Assistant Task (HHA)",
    "simplified RLHF pipeline",
    "unbounded loss landscape"
  ],
  "4146b447187e1a09b736564854007c403f986c69.pdf": [
    "Large Language Models (LLMs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Human preferences alignment",
    "ALARM framework",
    "Hierarchical Rewards Modeling",
    "Holistic rewards",
    "Aspect-specific rewards",
    "Reward selection mechanism",
    "Hierarchical reward combination",
    "Reward shaping",
    "Long-form question answering",
    "Machine translation",
    "Scalable oversight",
    "Inconsistent human supervision"
  ],
  "b1286763413a7b2309edeba1ed18884be429a941.pdf": [],
  "68981715a1e37c955329fc1a278aef59c9be4764.pdf": [
    "Dynamic Rewarding with Prompt Optimization (DRPO)",
    "tuning-free LLM alignment",
    "inference-time prompt optimization",
    "dynamic rewarding mechanism",
    "search-based optimization framework",
    "system prompts and ICL examples",
    "cost-effective and annotation-efficient",
    "self-improvement capabilities",
    "base models outperform tuned models",
    "automated prompt optimization",
    "helpfulness and harmlessness alignment",
    "superficial alignment hypothesis",
    "LLM-as-evaluator/optimizer"
  ],
  "e43fa63d5b79b65b8dc2abb7aa5bc903471be6f7.pdf": [],
  "1271cc5f6eaecebecd0489c23c727b30ee7f6089.pdf": [
    "Code Community Question Answering (CCQA)",
    "Large Language Models (LLMs)",
    "Multi-perspective user preferences",
    "Outdated answer mitigation",
    "ALMupQA framework",
    "Preference alignment",
    "Multi-perspective Preference Ranking Alignment (MPRA)",
    "List-wise contrastive loss",
    "Retrieval-augmented In-context Learning (RIL)",
    "StaCCQA dataset",
    "Supervised Fine-Tuning",
    "Improved performance metrics"
  ],
  "0d49552b54a1c2e064047d332018a898fcf6d9cb.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "credit assignment problem",
    "macro actions",
    "MA-RLHF framework",
    "temporal abstraction",
    "Proximal Policy Optimization (PPO)",
    "LLM alignment",
    "learning efficiency",
    "macro action termination conditions",
    "performance gains",
    "faster convergence",
    "scalability and robustness",
    "program synthesis",
    "de-tokenization perspective"
  ],
  "d3cdb7c701821509290b6428f7b445885440729b.pdf": [],
  "197c91461c4f0bfc19d775f329607492ac80912f.pdf": [
    "RLHF reward models",
    "Large Language Models (LLMs)",
    "embedding-based input",
    "computational cost reduction",
    "reproducibility framework",
    "lightweight reward models",
    "decoupled reward modeling",
    "scalable evaluation",
    "CPU-only research",
    "enhanced training stability",
    "LLM alignment research",
    "standardized evaluation datasets",
    "hyperparameter sensitivity"
  ],
  "503c85a9df91de5dace92d6c5ade8627701f08ac.pdf": [
    "Participatory Urban Planning (PUP)",
    "Large Language Models (LLMs)",
    "Multi-agent LLM Simulation",
    "LLM-empowered Participatory Framework",
    "Human-centric Urban Planning",
    "Scalable Participatory Simulation",
    "Community-level Land-use Redevelopment",
    "Inclusive Urban Solutions",
    "Resident Demands Integration",
    "Satisfaction and Inclusion Metrics",
    "Comparison with Human Experts and RL",
    "Transparent Rationale"
  ],
  "ba0a50c7eed827ff18adce2ff5248df65e5c1e06.pdf": [],
  "365b5f8439ccd558de22c7fbb229a380c8ea423f.pdf": [
    "Reasoning-Rendering-Visual-Feedback (RRVF)",
    "Visual Reinforcement Learning",
    "Multimodal Large Language Models (MLLMs)",
    "Deep Visual Reasoning",
    "Asymmetry of Verification principle",
    "Learning from raw images",
    "Closed-loop self-correction",
    "Group Relative Policy Optimization (GRPO)",
    "Hybrid Reward Function",
    "Image-to-code generation",
    "Visual Judge MLLM",
    "Outperforming judge model",
    "Semantic equivalence problem",
    "Data annotation bottlenecks"
  ],
  "72789bb011045e4230834b2df0e3922f2104f8fa.pdf": [
    "Verilog code generation",
    "Large Language Models (LLMs)",
    "functional correctness",
    "verification insights",
    "testbench feedback",
    "Direct Preference Optimization (DPO)",
    "automatic testbench generation",
    "Reinforcement Learning (RL)",
    "hardware design",
    "Verilog Compiler Simulator (VCS)",
    "preference pair collection",
    "state-of-the-art performance",
    "verification-aware LLM training"
  ],
  "d05777760939dd4566b0777a750401f008546539.pdf": [],
  "f52e2a8ec86afffb9ff4153fc14b3bd8b53d2c3c.pdf": [],
  "d47aae06d20ea0189adad9b2c8184b429fe38438.pdf": [],
  "aa64075f0c7a0977508a5dcb6a3c319952afcc20.pdf": [],
  "c3bb8d030a47d28c7a965ee5112a453d86e098ad.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "bypassing reward model inference",
    "optimal policy identification",
    "model-free RLHF",
    "Batched Sequential Action Dueling (BSAD)",
    "instance-dependent sample complexity",
    "backward action dueling",
    "batched human feedback",
    "Condorcet winner guarantee",
    "reward-free exploration",
    "episodic Markov Decision Processes (MDPs)",
    "Large Language Models (LLMs)"
  ],
  "914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c.pdf": [
    "Proxy-RLHF",
    "LLM alignment",
    "decoupled architecture",
    "parameter efficiency",
    "data efficiency",
    "proxy model",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Stable Knowledge-Aware Module (SKAM)",
    "novel MDP design",
    "computational cost reduction",
    "GPT-4 win rates",
    "helpfulness alignment"
  ],
  "b467036844e26c96ee94c466d771f1a5bf617204.pdf": [
    "Reinforcement Learning (RL)",
    "Large Language Models (LLMs)",
    "LLM alignment",
    "LLM reasoning",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Direct Preference Optimization (DPO)",
    "Reinforcement Learning from AI Feedback (RLAIF)",
    "Constitutional AI",
    "RL with Verifiable Rewards (RLVR)",
    "Implicit Language Q-Learning (ILQL)",
    "Policy optimization",
    "Reward hacking",
    "Discrete action space",
    "Comparative taxonomy",
    "Verifier-guided training"
  ],
  "5cb5453d2c54e1449f82fdf2e976cab04396b224.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "Distributionally Robust Optimization (DRO)",
    "prompt distribution shifts",
    "out-of-distribution (OOD) generalization",
    "Total Variation (TV) distance",
    "Direct Preference Optimization (DPO)",
    "robust reward estimation",
    "robust policy optimization",
    "uncertainty sets",
    "provable algorithms",
    "real-world LLM deployment"
  ],
  "a4338066b4d83cd9043a04eb4d5732041056a0f1.pdf": [],
  "4069e2f0eaf53a0e7086bb715e359e345e151abc.pdf": [
    "KL-regularized Reinforcement Learning",
    "online settings",
    "logarithmic regret",
    "sample efficiency",
    "contextual bandits",
    "Markov Decision Processes (MDPs)",
    "Reinforcement Learning from Human Feedback (RLHF)",
    "Large Language Models (LLMs)",
    "optimism in the face of uncertainty (OFU)",
    "policy suboptimality decomposition",
    "multi-step policy decomposition",
    "novel analytical framework",
    "theoretical guarantees",
    "eliminating strong coverage assumptions"
  ],
  "8216a2d3d897f63b46e80211bccb0931ab9fbda9.pdf": [
    "Large Language Models (LLMs)",
    "non-standard Unicode characters",
    "LLM vulnerabilities",
    "jailbreaks",
    "hallucinations",
    "comprehension errors",
    "guardrail bypass",
    "Reinforcement Learning Human Feedback (RLHF)",
    "prompt leakage",
    "systematic testing methodology",
    "training data gaps",
    "LLM safety and reliability",
    "Unicode character processing"
  ],
  "8171ff1da2605a410b99b82e2dbd0feb68d021ef.pdf": [
    "Reinforcement Learning from Human Feedback (RLHF)",
    "token-level rewards",
    "REward reDistribution (RED)",
    "sparse and delayed rewards",
    "credit assignment",
    "Large Language Models (LLMs)",
    "reward models",
    "optimal policy preservation",
    "potential-based reward shaping",
    "minimal computational costs",
    "empirical validation",
    "dense rewards effectiveness",
    "Attention-based Credits (ABC) ineffectiveness",
    "LLM alignment"
  ]
}