\subsection{DRL for Abstractive Summarization and Machine Translation}

Traditional sequence-to-sequence models for abstractive summarization and machine translation, primarily trained with Maximum Likelihood Estimation (MLE), often suffer from "exposure bias" and a fundamental mismatch between their training objective (token-level accuracy) and the holistic, non-differentiable metrics used for evaluation (e.g., ROUGE for summarization, BLEU for machine translation). Deep Reinforcement Learning (DRL) emerged as a powerful paradigm to address these limitations by directly optimizing for these sequence-level quality metrics, leading to more fluent, coherent, and human-like generated text.

Early work laid the groundwork for applying reinforcement learning to sequence generation. \cite{Ranzato2016} pioneered the use of policy gradient methods for sequence-to-sequence learning, demonstrating how models could be trained to directly optimize non-differentiable metrics like BLEU. This approach allowed the model to explore the output space and learn to generate sequences that were globally better, rather than just locally accurate. The pervasive issue of exposure bias, where models trained on ground truth sequences struggle when exposed to their own generated (and potentially erroneous) tokens during inference, was a key motivator for this shift. \cite{Li2016}, in the context of dialogue generation, further highlighted this problem and showcased how policy gradient methods could mitigate it by allowing the model to learn from its own generated sequences, thereby improving long-term coherence and relevance.

For abstractive summarization, DRL has been instrumental in enhancing the quality of generated summaries beyond mere extractive approaches. \cite{Paulus2017} introduced a deep reinforced model for abstractive summarization, leveraging an actor-critic architecture to directly optimize ROUGE scores. This method combined supervised pre-training with DRL fine-tuning, allowing the model to learn to generate more coherent and informative summaries by rewarding it for higher ROUGE scores, which are indicative of content overlap and fluency. Building on this, \cite{Wang2018} (Deep Reinforcement Learning for Abstractive Summarization) further explored DRL techniques to improve abstractive summarization, often focusing on refining reward functions and training stability to produce more factually consistent and readable outputs.

In the domain of neural machine translation (NMT), DRL has similarly been employed to bridge the gap between training and evaluation. \cite{Bahdanau2017} proposed an actor-critic algorithm specifically for sequence prediction, which is highly applicable to NMT. This method aimed to reduce the high variance associated with pure policy gradient methods by introducing a critic network to estimate the value function, thereby providing more stable and efficient learning signals. This stability is crucial for complex tasks like machine translation, where the output space is vast. Furthermore, \cite{Wang2018} (Reinforcement Learning for Neural Machine Translation with a Diversified Reward) explicitly applied reinforcement learning to NMT, designing diversified reward functions that not only considered BLEU scores but also encouraged diversity in translations, addressing the issue of generic or repetitive outputs. Another facet of DRL's utility in NMT was demonstrated by \cite{Wang2018} (A Reinforcement Learning Approach to Improve the Robustness of Neural Machine Translation), where policy gradient methods were used to train an agent to generate adversarial examples, which in turn improved the robustness of the NMT model. This application showcases DRL's capacity to enhance the overall reliability and quality of generated content, a critical step for real-world utility.

In conclusion, the application of DRL, particularly through policy gradients and actor-critic models, marked a significant advancement in abstractive summarization and machine translation. By directly optimizing for holistic, non-differentiable metrics like ROUGE and BLEU, DRL successfully addressed the limitations of MLE, such as exposure bias, leading to the generation of more fluent, coherent, and human-like text. Despite these successes, challenges persist, including the inherent high variance of RL training, the sample inefficiency for complex language tasks, and the difficulty of designing effective and generalizable reward functions that truly capture nuanced aspects of text quality beyond n-gram overlap. These unresolved tensions continue to drive research towards more stable, efficient, and sophisticated DRL paradigms for natural language generation.