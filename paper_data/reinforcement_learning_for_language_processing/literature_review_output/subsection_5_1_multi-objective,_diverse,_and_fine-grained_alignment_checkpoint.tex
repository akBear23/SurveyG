\subsection*{Multi-Objective, Diverse, and Fine-Grained Alignment}

Achieving comprehensive alignment of Large Language Models (LLMs) with human preferences necessitates moving beyond a singular, aggregated reward signal to embrace the inherent complexity, diversity, and multi-faceted nature of human values \cite{wang2024a3a, srivastava2025gfw}. This advanced alignment paradigm addresses the limitations of traditional Reinforcement Learning from Human Feedback (RLHF) methods, which often struggle to balance conflicting objectives, represent minority preferences, or exert fine-grained control over model outputs.

A core challenge in aligning LLMs stems from the diversity of human preferences, which a single reward model often fails to capture. \cite{chakraborty20247ew} formally demonstrates this limitation with an "impossibility result" for single-reward RLHF, particularly for minority groups. To address this, they introduce \textbf{MaxMin-RLHF}, an approach that learns a mixture of reward models representing distinct human sub-populations and then optimizes an egalitarian MaxMin objective to maximize the minimum utility across these groups. Complementing this, \cite{boldi2024d0s} tackles diverse preferences arising from "hidden context" (unobservable factors influencing preferences). Their \textbf{Pareto Optimal Preference Learning (POPL)} framework leverages lexicase selection to learn a *set* of Pareto-optimal reward functions or policies, effectively catering to distinct groups without requiring explicit group labels, thus offering a label-free approach to pluralistic alignment.

A critical aspect of multi-objective alignment is balancing conflicting goals, such as helpfulness and safety. Naively scaling safety data can lead to "over-safe" models that refuse benign queries, diminishing helpfulness \cite{tan2025lk0}. To mitigate this, \cite{tan2025lk0} proposes \textbf{Equilibrate RLHF}, employing a Fine-grained Data-centric (FDC) approach to categorize safety data and an Adaptive Message-wise Alignment (AMA) with gradient masking to selectively emphasize safety-critical segments during RL training. This data-centric and adaptive strategy helps maintain the helpfulness-safety trade-off. Similarly, \cite{zhang2024b6u} introduces \textbf{Bi-Factorial Preference Optimization (BFPO)}, a DPO-like re-parameterization that uses a novel labeling function to capture global preferences, prioritizing safety while preserving helpfulness, offering a computationally efficient solution to this trade-off. Expanding on multi-objective constrained optimization, \cite{xu20242yo} presents \textbf{Constrained Generative Policy Optimization (CGPO)} with a "Mixture of Judges" (MoJ). CGPO integrates rule-based and LLM-based judges to identify reward hacking patterns and employs task-specific optimization settings, allowing for robust, multi-task, and constrained alignment that significantly enhances performance across diverse objectives like chat, reasoning, and safety.

Beyond balancing high-level objectives, achieving fine-grained control over LLM outputs is paramount. \cite{lai2024ifx} addresses the inconsistency and sparsity of human feedback by introducing \textbf{ALaRM (Align Language Models via Hierarchical Rewards Modeling)}. This framework combines holistic rewards with proactively selected aspect-specific rewards, integrating them hierarchically to provide more precise and consistent guidance, especially for high-quality generations. For more granular control at the token level, \cite{zhong2024wch} proposes \textbf{Reinforced Token Optimization (RTO)}, which re-frames RLHF as a Markov Decision Process (MDP) and leverages Direct Preference Optimization (DPO) to derive dense, token-wise rewards for Proximal Policy Optimization (PPO) training. This addresses the sparse reward problem and enables more fine-grained credit assignment. In a similar vein, \cite{chan2024xig} offers "Dense Reward for Free" using \textbf{Attention Based Credit (ABC)}, which exploits the reward model's internal attention maps to redistribute sparse sequence rewards across individual tokens with minimal computational overhead. \cite{li2024h19} further contributes with \textbf{RED (REward reDistribution)}, which computes token-level rewards as the incremental impact of each token on the reward model's score, providing dense, immediate feedback without modifying the reward model. To improve credit assignment over longer sequences, \cite{chai2024qal} introduces \textbf{MA-RLHF (Reinforcement Learning from Human Feedback with Macro Actions)}, which optimizes at the level of token sequences (macro actions), reducing the temporal distance between actions and rewards for more efficient learning.

Ensuring the robustness and stability of these advanced alignment objectives is also crucial. The phenomenon of reward over-optimization, where models exploit imperfections in proxy reward models, is exacerbated in multi-objective settings. \cite{moskovitz2023slz} confronts this with \textbf{constrained RLHF}, dynamically learning Lagrange multipliers to enforce "proxy points" for individual reward components, preventing over-optimization of composite reward models. \cite{zhai20238xc} proposes \textbf{Uncertainty-Penalized RLHF (UP-RLHF)} with diverse reward LoRA ensembles. While primarily for general over-optimization, the use of diverse ensembles and uncertainty penalization can be seen as a way to handle ambiguous or diverse preferences by being conservative when the reward model is uncertain about a particular preference. \cite{fu2025hl3} introduces \textbf{Preference As Reward (PAR)}, a reward shaping technique that applies a sigmoid function to centered rewards, providing bounded rewards that stabilize critic training and minimize policy gradient variance, thus mitigating reward hacking in complex reward landscapes. Furthermore, \cite{dai2025ygq} addresses reward over-optimization caused by reward model extrapolation errors for out-of-distribution responses. Their \textbf{Behavior-Supported Policy Optimization (BSPO)} uses value regularization to restrict policy iteration to the in-distribution region, preventing the policy from optimizing for spurious rewards in unseen areas. The insights from \cite{rafailov2024ohd} on scaling laws for reward model over-optimization in direct alignment algorithms like DPO further underscore the pervasive nature of this problem, even in simplified alignment pipelines, highlighting the continuous need for robust solutions in multi-objective contexts.

In conclusion, the field is rapidly evolving to capture the richness of human values by developing sophisticated methodologies for multi-objective, diverse, and fine-grained alignment. Techniques ranging from learning mixtures of reward models and multi-objective constrained RL to data-centric approaches and hierarchical feedback mechanisms are producing more nuanced, fair, and controllable LLM behaviors. However, challenges remain in defining and evaluating complex, often conflicting, objectives, ensuring scalability to frontier models, and guaranteeing fairness and robustness across the full spectrum of diverse human preferences in real-world, dynamic environments.