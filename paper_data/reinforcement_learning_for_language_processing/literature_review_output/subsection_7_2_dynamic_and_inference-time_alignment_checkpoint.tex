\subsection{Dynamic and Inference-Time Alignment}
The prevailing paradigm for aligning large language models (LLMs) with human preferences and values, predominantly through methods like Reinforcement Learning from Human Feedback (RLHF), typically results in static models. Once aligned, these models exhibit inflexibility to diverse user needs, evolving contexts, or real-time behavioral adjustments, necessitating costly and time-consuming retraining for any modification. This inherent inflexibility presents a significant operational and scalability challenge, especially in rapidly changing deployment environments. Consequently, an emerging and critical research direction focuses on enabling dynamic adjustment and control of LLM behavior directly during inference. These innovative inference-time alignment strategies offer unparalleled flexibility, efficiency, and real-time adaptability without the need for extensive post-training fine-tuning, marking a significant shift from static, pre-computed alignment to adaptive, context-aware control.

These dynamic alignment approaches can be broadly categorized based on their mechanism: direct inference-time interventions that manipulate model outputs or internal states, and inference-time search and self-optimization strategies that leverage the LLM's own capabilities or auxiliary agents to refine behavior. Each category offers distinct advantages and challenges in dynamically controlling the policy learned, often, through RLHF.

\subsubsection{Direct Inference-Time Interventions}
This category encompasses methods that directly modify the model's decoding process or internal activations to steer its behavior towards desired alignment objectives. These interventions are typically lightweight, operate on the fly, and often serve to dynamically adjust or correct the behavior of an already RLHF-aligned policy.

A foundational approach in this area is \textit{Representation Engineering} or \textit{Control Vectors}, which directly manipulate the internal activation space of LLMs to elicit specific behaviors \cite{zou2023representation, subramani2023steering, li2023inference}. These methods identify "steering vectors" by contrasting activations from desired and undesired responses (e.g., helpful vs. unhelpful, safe vs. unsafe). By adding or subtracting these vectors to the hidden states of an LLM during inference, the model's subsequent generation can be steered towards the desired attribute. For instance, \cite{li2023inference} demonstrated that simple inference-time interventions on internal states can significantly improve truthfulness in LLMs. While powerful for fine-grained control, a critical challenge lies in the interpretability of these steering vectors and the potential for unintended side effects or over-steering, as their impact on complex, emergent behaviors is not always fully understood. Moreover, the effectiveness often depends on careful calibration and the quality of the contrasting examples used to derive the vectors. These techniques offer a direct way to modify the latent policy of an RLHF-aligned model without altering its weights.

\textit{Decoding-time Realignment (DeRa)} \cite{liu2024w47} provides a logit-level intervention for dynamic adjustment of alignment strength during inference. Grounded in the theoretical insight that models aligned with varying KL regularization strengths are effectively geometric mixtures of a reference model and a single aligned model, DeRa linearly combines the logits of these two models during decoding. This enables real-time exploration of the reward-regularization trade-off, allowing users to dynamically dial up or down the "alignedness" of a model's output (e.g., helpfulness, conciseness) without retraining. While offering fine-grained control over an RLHF-trained policy, DeRa incurs increased inference time and memory usage due to the need to compute logits from two models simultaneously. Its effectiveness also relies on the assumption that a linear combination of logits accurately captures the desired behavioral spectrum, which might not hold universally across all tasks or models.

Addressing the critical aspect of safety, \textit{InferAligner} \cite{wang2024w7p} introduced an inference-time solution for enhancing harmlessness through cross-model guidance. This method extracts "Safety Steering Vectors" (SSVs) from an already safety-aligned model (often trained with RLHF) and conditionally applies them to the target model's activations. InferAligner employs a "guidance gate" that activates this intervention only when harmful intent is detected in the input, preventing unnecessary shifts that could degrade performance on benign tasks. This conditional, activation-level steering effectively transfers safety knowledge without retraining the target model, providing a robust defense against "model hacking" attacks like Probability Manipulation (ProMan) \cite{zhang2023pbi} that can bypass static alignment efforts. A key limitation is its dependency on the quality and robustness of the "teacher" safety-aligned model and the accuracy of the harmful intent detection mechanism. Poor detection could lead to negative transfer or missed harmful outputs. Unlike DeRa which operates on output logits for general alignment, InferAligner focuses on internal activations for a specific safety objective, demonstrating different granularities and targets of intervention.

\subsubsection{Inference-Time Search and Self-Optimization}
This category focuses on methods where the LLM itself, or a lightweight auxiliary model, dynamically optimizes its prompts, reasoning paths, or objectives during inference to achieve better alignment or task performance. These approaches often involve an iterative search process and can be seen as alternatives or complements to traditional RLHF.

\textit{Dynamic Rewarding with Prompt Optimization (DRPO)} \cite{singla2024dom} exemplifies a tuning-free, search-based framework for self-alignment at inference time. DRPO allows LLMs to iteratively self-improve by crafting optimal alignment instructions, including system prompts and in-context learning (ICL) examples, without any additional training or human intervention. Its core innovation lies in a dynamic rewarding mechanism that adjusts LLM-based rewards based on specific queries, identifying and rectifying model-specific alignment weaknesses. This enables LLMs to adapt efficiently to diverse alignment challenges, demonstrating that base models aligned with DRPO can achieve higher average alignment scores than their SFT/RLHF-tuned counterparts, challenging the necessity of extensive fine-tuning for alignment. However, a significant critical point is the potential for self-deception or bias amplification, as the LLM acts as both the generator and the rewarder. The iterative search process can also incur substantial computational cost at inference time, limiting its real-time applicability for latency-sensitive applications.

Beyond general alignment, dynamic control is also being applied to guide complex reasoning processes. \textit{RL-of-Thoughts (RLoT)} \cite{hao2025lc8} is an inference-time technique that trains a lightweight "navigator model" using reinforcement learning to dynamically generate task-adaptive logical structures for LLM reasoning. By selecting human cognition-inspired "logic blocks" (e.g., "Decompose," "Debate," "Refine") based on the LLM's self-evaluation of its current reasoning status, RLoT enables adaptive and task-specific reasoning without modifying the base LLM. This allows smaller models to achieve reasoning performance comparable to much larger ones, showcasing the power of dynamic, RL-guided search in the thought space. The explicit use of RL to train the navigator model directly connects this method to the review's core theme. A limitation of RLoT is the complexity of designing effective "logic blocks" and the reliance on the LLM's self-evaluation accuracy, which can be imperfect. Unlike DRPO which optimizes the *input instructions*, RLoT optimizes the *internal reasoning process* itself, demonstrating different facets of inference-time self-optimization.

In conclusion, the shift towards dynamic and inference-time alignment marks a significant evolution in LLM deployment, moving beyond the inflexibility of static post-training alignment. These approaches, ranging from real-time adjustment of alignment strength via logit manipulation \cite{liu2024w47} and cross-model safety guidance through activation steering \cite{wang2024w7p, zou2023representation}, to autonomous self-improvement via prompt optimization \cite{singla2024dom} and dynamic reasoning navigation \cite{hao2025lc8}, offer unparalleled flexibility and efficiency. They address the core limitation of static alignment by allowing LLMs to adapt to diverse user needs and contexts in real-time, significantly reducing the operational overhead associated with maintaining multiple aligned models. Future research will likely focus on combining these diverse techniques, further reducing inference overhead, improving the robustness of self-evaluation and steering mechanisms, and broadening their applicability to a wider array of alignment objectives and complex, multi-modal scenarios, ultimately leading to more adaptable and context-aware AI systems.