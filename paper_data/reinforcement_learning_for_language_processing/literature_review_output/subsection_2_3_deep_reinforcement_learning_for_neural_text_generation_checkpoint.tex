\subsection{Deep Reinforcement Learning for Neural Text Generation}
Neural text generation, traditionally dominated by maximum likelihood estimation (MLE) training for sequence-to-sequence (seq2seq) models, faced significant challenges, including 'exposure bias' and the inability to directly optimize non-differentiable, task-specific metrics like ROUGE or BLEU. Exposure bias arises from the discrepancy between training, where the model is conditioned on ground truth tokens (teacher forcing), and inference, where it conditions on its own potentially erroneous outputs, leading to error accumulation. Deep reinforcement learning (DRL) emerged as a powerful paradigm to address these limitations by framing text generation as a sequential decision-making process, allowing direct optimization of desired output quality.

Early applications of DRL in text generation demonstrated its potential to move beyond token-level accuracy towards optimizing for overall sequence quality. For instance, \cite{Li2016} pioneered the use of deep reinforcement learning for open-domain dialogue generation. Their approach employed policy gradient methods (specifically REINFORCE) to train a generative dialogue model, using a composite reward function designed to encourage informativeness, coherence, and ease of answering, thereby moving beyond simple perplexity-based objectives. This work highlighted DRL's ability to optimize for long-term conversational quality, a complex, non-differentiable metric.

Building on these foundational ideas, \cite{Bahdanau2017} introduced a more general actor-critic algorithm specifically for sequence prediction, aiming to mitigate the pervasive exposure bias problem. By learning a critic to estimate the value of states (partial sequences) and an actor to improve the policy, their method provided more stable and efficient learning compared to pure policy gradient approaches, which often suffer from high variance. This framework offered a robust mechanism to directly optimize for sequence-level metrics, applicable across various generation tasks. The utility of DRL in overcoming exposure bias and directly optimizing for human-aligned metrics was further showcased in abstractive summarization. \cite{Paulus2017} applied a deep reinforcement learning approach to abstractive summarization, training a seq2seq model with an intra-attention mechanism using policy gradients. Their model was rewarded based on ROUGE scores, directly optimizing for summary quality, which is a non-differentiable metric, demonstrating a significant improvement over purely MLE-trained models.

The versatility of DRL extended to other complex generation tasks, such as machine translation and controlled text generation. \cite{Wang2018} explored deep reinforcement learning for syntactic neural machine translation, where rewards could be designed to encourage not only fluency and adequacy (e.g., via BLEU scores) but also adherence to syntactic structures, offering finer control over the generated output. Similarly, \cite{Luo2019} leveraged DRL for controlled text generation, specifically for generating reviews with specific aspects. By incorporating aspect-specific rewards, their method enabled the model to generate text that not only was fluent but also accurately reflected desired attributes, showcasing DRL's power in steering generation towards specific, complex objectives.

While DRL marked a significant step forward in neural text generation, enabling direct optimization of task-specific metrics and mitigating exposure bias, it introduced its own set of challenges. A primary limitation across these works is the inherent difficulty in designing reward functions that perfectly align with nuanced human judgment or complex task requirements. Often, proxy metrics like ROUGE or BLEU, while useful, do not fully capture human preferences for coherence, factual accuracy, or style. Furthermore, the training of DRL models for high-dimensional text generation tasks often suffers from instability, high variance, and sample inefficiency, requiring extensive computational resources and careful hyperparameter tuning. These challenges underscore the ongoing quest for more robust and human-aligned generative models, a direction that has since evolved towards leveraging human feedback to learn reward functions, as seen in more recent advancements in large language models \cite{sahoo2024dp6, yu2023xc4, thirunavukarasu2023enj, holk20243vd, wang2024lje}.