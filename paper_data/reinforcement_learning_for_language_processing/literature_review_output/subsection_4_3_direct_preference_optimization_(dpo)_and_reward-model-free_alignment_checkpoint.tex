\subsection{Direct Preference Optimization (DPO) and Reward-Model-Free Alignment}
The traditional Reinforcement Learning from Human Feedback (RLHF) pipeline, while effective for aligning large language models (LLMs) with human preferences, is notoriously complex, computationally intensive, and often unstable, typically involving a separate reward model and a reinforcement learning stage with algorithms like Proximal Policy Optimization (PPO) \cite{zheng2023c98}. This complexity spurred the development of more streamlined, reward-model-free alignment approaches, with Direct Preference Optimization (DPO) emerging as a pivotal simplification that democratizes access to preference-based fine-tuning \cite{wang2024a3a, srivastava2025gfw}.

Direct Preference Optimization (DPO), introduced by \textcite{rafailov20239ck}, fundamentally re-conceptualizes the RLHF objective. Instead of explicitly training a reward model and then using reinforcement learning to optimize the policy, DPO leverages a theoretical insight: the optimal policy for the standard KL-constrained RLHF objective can be derived in closed form by re-parameterizing the reward model. This allows for direct optimization of the language model policy from human preferences using a simple binary cross-entropy loss, effectively eliminating the need for an explicit reward model and complex reinforcement learning algorithms like PPO. DPO's benefits are substantial, including improved computational efficiency, enhanced training stability, and significantly simplified implementation, making preference-based alignment more accessible.

Beyond DPO, other reward-model-free approaches also aim to simplify the alignment process. \textcite{hejna2023vyy} proposed Contrastive Preference Learning (CPL), which directly learns an optimal policy from regret-based human preferences without an intermediate reward function or traditional RL. CPL leverages the bijection between the optimal advantage function and the optimal policy's log-probability in Maximum Entropy RL, transforming the problem into a supervised contrastive objective applicable to general sequential decision-making tasks, including high-dimensional robotics, thereby offering broad applicability beyond just LLMs. Building upon the DPO paradigm, \textcite{ji2024d5f} introduced Self-Play with Adversarial Critic (SPAC), a DPO-like single-timescale algorithm for offline LLM alignment that provides provable convergence guarantees under weak data assumptions, addressing the lack of theoretical robustness in many empirical methods while maintaining computational scalability.

Despite their simplicity, direct alignment algorithms (DAAs) like DPO are not immune to challenges such as reward overoptimization. \textcite{rafailov2024ohd} empirically demonstrated that DAAs exhibit similar performance degradation patterns to classical RLHF, where maximizing the implicit reward beyond a certain point leads to a decline in true human-judged quality. This highlights the need for robust mechanisms even in simplified pipelines. To mitigate such issues, \textcite{zhai20238xc} proposed Uncertainty-Penalized RLHF (UP-RLHF), which augments the objective with an uncertainty regularization term derived from a diverse reward LoRA ensemble, penalizing rewards based on the reward model's estimated uncertainty. Similarly, \textcite{miao2025ox0} introduced Energy loss-aware PPO (EPPO), which tackles reward hacking by penalizing the increase in "energy loss" in the LLM's final layer, offering a novel internal-mechanistic perspective on mitigating policy degeneration. \textcite{dai2025ygq} addressed reward over-optimization via Behavior-Supported Regularization, using value regularization to restrict policy iteration to the in-distribution region of the reward model, thereby preventing extrapolation errors. Furthermore, \textcite{zhang2024esn} proposed Adversarial Policy Optimization (AdvPO) with lightweight uncertainty estimation, a distributionally robust optimization procedure that efficiently quantifies reward uncertainty using only the last layer embeddings of a single reward model, offering a computationally efficient alternative to ensemble methods.

The DPO paradigm has also been extended to address more complex alignment objectives, particularly multi-objective alignment. \textcite{zhang2024b6u} introduced Bi-Factorial Preference Optimization (BFPO), a supervised learning framework that re-parameterizes a joint RLHF objective for both safety and helpfulness into a single supervised learning loss. This innovation efficiently balances conflicting objectives without the extensive computational and human labor typically required for multi-objective RLHF. \textcite{tan2025lk0} developed Equilibrate RLHF, which includes an Adaptive Message-wise Alignment (AMA) approach that uses gradient masking to selectively highlight safety-critical segments, adapting DPO (as Adaptive DPO, ADPO) for a more nuanced helpfulness-safety trade-off. \textcite{xu20242yo} proposed Constrained Generative Policy Optimization (CGPO) with a "Mixture of Judges," which includes DPO-like optimizers (Constrained Online DPO, CODPO) to address reward hacking and extreme multi-objective optimization by integrating rule-based and LLM-based judges to identify constraint violations. \textcite{boldi2024d0s} presented Pareto-Optimal Learning from Preferences with Hidden Context (POPL), a reward-model-free approach that leverages lexicase selection to learn a set of Pareto-optimal policies for diverse human preferences without requiring explicit group labels, thereby ensuring fairness and robust personalization.

While DPO and other reward-model-free methods significantly simplify the alignment pipeline, practical challenges persist. The "alignment tax," where fine-tuning for preferences can degrade foundational capabilities, remains a concern. \textcite{lu202435m} introduced Online Merging Optimizers to mitigate this by integrating model merging into each optimization step of RLHF, continuously steering gradients towards maximizing rewards while preserving pre-trained capabilities. This highlights that even with simplified objectives, ensuring comprehensive model performance requires sophisticated regularization and optimization strategies. The ongoing research in this area continues to focus on making preference-based fine-tuning more robust, scalable, and capable of handling complex, multi-faceted alignment objectives, moving towards more reliable and human-aligned LLMs.