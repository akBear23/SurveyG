\subsection*{Early RL for Structured Prediction and Search}

The application of reinforcement learning (RL) to structured prediction tasks in natural language processing (NLP) marked a pivotal shift from traditional supervised learning paradigms. Pioneering methods framed complex linguistic problems, such as sequence labeling, parsing, and information extraction, as sequential decision-making processes. In this framework, an agent learns a policy to make a series of local choices that collectively construct a globally optimal structured output, whether a sequence, tree, or graph. This approach was crucial for tasks where the output structure exhibited strong interdependencies, and local errors could propagate, making global optimization challenging for conventional methods.

The primary motivation for adopting RL-inspired policy learning stemmed from the inherent limitations of supervised learning for structured outputs. Maximum Likelihood Estimation (MLE) often suffers from "exposure bias," where models are trained on ground-truth prefixes but are exposed to their own generated (and potentially erroneous) prefixes during inference, leading to a compounding of errors. Furthermore, many desirable evaluation metrics for structured outputs (e.g., F1-score for sequence labeling, BLEU for machine translation) are non-differentiable and global, making direct optimization with supervised learning difficult. RL provided a natural framework to address these issues by allowing direct optimization of task-specific, non-decomposable global objectives through a reward signal.

A prominent paradigm in this era was "Learning to Search" (L2S), which conceptualized structured prediction as navigating a search space to construct an optimal output. L2S methods aimed to learn a cost-sensitive classifier that, at each step, makes a local decision to extend the partial output, guided by an expert policy. A seminal work in this area is SEARN (Search-based Structured Prediction) by \cite{daume2009search}, which introduced a general framework for reducing structured prediction to a sequence of cost-sensitive classification problems. SEARN iteratively learns a policy by observing an "expert" (either a gold-standard oracle or a strong heuristic) and then performing exploration to improve its decisions. This allowed for direct optimization of global loss functions, mitigating the exposure bias inherent in greedy supervised approaches.

Building upon this, DAgger (Dataset Aggregation) by \cite{ross2011reduction} further refined the L2S approach by iteratively collecting data from the learned policy's executions and aggregating it with the expert's demonstrations. This process effectively addresses the covariate shift problem, where the distribution of states encountered by the learned policy during inference differs from the training distribution. DAgger's iterative self-correction mechanism allowed policies to generalize better to unseen states, making it highly effective for tasks like dependency parsing and sequence tagging, where the model sequentially predicts parts of a linguistic structure. These methods demonstrated that by framing structured prediction as a search problem and learning a policy to navigate this search, models could achieve superior performance on complex, interdependent linguistic structures compared to purely supervised baselines.

While these early methods offered significant advantages, they also presented challenges. The computational cost of exploring large search spaces could be substantial, and defining effective local policies without perfect foresight remained a complex task. Moreover, the reliance on expert demonstrations, whether human-crafted or derived from strong heuristics, limited their scalability and adaptability to novel tasks where such experts were unavailable. Despite these limitations, the principles established by early RL for structured prediction laid crucial groundwork. The core idea of sequential decision-making, explicit policy learning, and direct optimization of global, non-differentiable metrics became foundational for subsequent advancements.

The conceptual legacy of these early approaches continues to resonate in modern language processing, particularly with the advent of large language models (LLMs). While the underlying architectures and scale have changed dramatically, the challenge of generating optimal, structured text sequences and refining a policy through feedback remains. For instance, modern techniques for enhancing LLM reasoning, such as Self-Refine Monte Carlo Tree Search (SR-MCTS) for mathematical problem-solving \cite{zhang2024q0e}, explicitly frame solution generation as a search problem within a Markov Decision Process, directly extending the "Learning to Search" paradigm. Similarly, token-level reinforcement learning approaches like Inverse-Q* \cite{xia2024rab} and macro-action RLHF (MA-RLHF) \cite{chai2024qal} address the credit assignment problem and temporal abstraction for structured outputs, echoing the challenges faced by early methods in assigning credit over long sequences. Even applications like RL for code optimization \cite{duan2023nle} or physics problem-solving \cite{anand2024rnl} leverage RL to guide the sequential generation of structured text (code or multi-step solutions) based on environmental feedback, demonstrating the enduring relevance of these foundational ideas.

In conclusion, early applications of RL for structured prediction and search in NLP were instrumental in overcoming the limitations of traditional supervised learning. By framing tasks as sequential decision-making problems and employing methods like SEARN and DAgger, researchers learned policies to construct complex linguistic outputs, directly optimizing for global objectives. These pioneering efforts, despite their own computational and data-dependency challenges, established a critical conceptual framework that continues to inform and inspire modern approaches to generative language models, laying the essential groundwork for later deep reinforcement learning applications in neural text generation and the sophisticated alignment techniques seen today.