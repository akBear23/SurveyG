\subsection{Enhancing LLM Reasoning with RL}
Large Language Models (LLMs) excel in language generation, yet cultivating genuine, robust reasoning—encompassing mathematical problem-solving, multi-step logical deduction, and complex decision-making—presents a significant challenge for purely supervised learning paradigms. Reinforcement Learning (RL) has emerged as a powerful approach to address this limitation, enabling LLMs to 'think' more effectively and reliably by learning from diverse forms of feedback. Initial research, such as the development of the Chain-of-Thought Hub, empirically demonstrated a strong correlation between model scale and reasoning capabilities, and, crucially, highlighted the significant impact of Reinforcement Learning from Human Feedback (RLHF) on top-tier models like GPT-4 \cite{fu2023pyr}. This foundational understanding motivated deeper exploration into how RL could be specifically tailored to enhance reasoning.

A key avenue for improving LLM reasoning involves leveraging search and planning-based RL methods that allow models to explore complex solution spaces. \textcite{havrilla2024m0y} conducted a systematic comparative study of Expert Iteration (EI), Proximal Policy Optimization (PPO), and Return-Conditioned RL (RCRL) on mathematical word problems. Their findings revealed that Expert Iteration consistently outperformed PPO, particularly in deterministic reasoning environments, by effectively distilling high-quality reasoning trajectories. This work critically identified limited exploration during RL training as a significant bottleneck, suggesting that models often struggle to venture beyond solutions already produced by supervised fine-tuning (SFT) models, thereby limiting the full potential of RL. Building on the power of search, \textcite{zhang2024q0e} introduced LLaMA-Berry, a framework designed for Olympiad-level mathematical reasoning. It integrates Self-Refine Monte Carlo Tree Search (SR-MCTS) for robust exploration of the solution space, treating entire solutions as states and self-refinement as an optimization action. This is combined with a Pairwise Preference Reward Model (PPRM) trained via Direct Preference Optimization (DPO) and an Enhanced Borda Count for aggregating preferences, enabling smaller LLMs to tackle highly complex problems by efficiently navigating the vast search space of potential solutions.

For multi-step reasoning tasks, the granularity of feedback is paramount. Approaches that provide fine-grained, process-based feedback have shown particular promise. \textcite{hao2025lc8} introduced RL-of-Thoughts (RLoT), a novel framework that models long-sequence reasoning as a Markov Decision Process (MDP). A lightweight "navigator model," trained with Double-Dueling-DQN, dynamically selects human cognition-inspired "logic blocks" (e.g., "Decompose," "Refine") based on the LLM's self-evaluation and a Process Reward Model. This inference-time guidance allows for adaptive, task-specific reasoning structures without modifying the underlying LLM, enabling smaller models to achieve performance comparable to much larger ones by dynamically adapting their reasoning strategy. Further advancing fine-grained feedback, \textcite{chen20244ev} proposed Step-level Value Preference Optimization (SVPO). This method leverages MCTS to *autonomously generate fine-grained step-level preferences* and explicitly integrates a value model into a novel DPO-like loss function. By providing more granular feedback than coarse solution-level preferences, SVPO significantly boosts mathematical reasoning, demonstrating that learning from the intermediate steps of a reasoning chain is crucial for robust performance. In a similar vein, \textcite{chen2025vp2} developed Solver-Informed Reinforcement Learning (SIRL) for optimization modeling. SIRL leverages external optimization solvers as objective verifiers, providing precise and comprehensive feedback signals—including syntax correctness, feasibility, and solution quality—as direct rewards. This application of Reinforcement Learning with Verifiable Rewards (RLVR) provides highly granular, objective feedback at each step of the model generation process, grounding LLMs for authentic optimization modeling and significantly improving their functional correctness.

Beyond external feedback, methods that enable LLMs to self-verify and learn from internal signals are critical for robust reasoning. \textcite{liu20251xv} introduced RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework that explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities. RISE leverages verifiable rewards not only to guide solution generation but also to align the model's self-verification ability on-the-fly, addressing the challenge of "superficial self-reflection" where LLMs struggle to robustly identify flaws in their own reasoning. However, the utility of purely internal feedback requires careful consideration. \textcite{zhang2025d44} critically examined Reinforcement Learning from Internal Feedback (RLIF), exploring self-certainty and entropy-based signals as reward proxies. Their theoretical and empirical analysis revealed that these internal signals primarily drive the model to minimize policy entropy, making the output distribution more deterministic. While RLIF can initially boost reasoning performance for *base LLMs*, this gain often degrades with continued training, and it shows little to no improvement for instruction-tuned models, highlighting a "no free lunch" scenario where the effectiveness of internal feedback is highly dependent on the model's initial state and training dynamics.

RL has also proven effective in enhancing reasoning in specific domains and necessitates specialized data infrastructure. \textcite{anand2024rnl} applied Reinforcement Learning with Human and Artificial Intelligence Feedback (RLHAIF) using PPO, DPO, and Remax optimization to improve LLM performance on physics problem-solving. Their MISTRAL-PPO model demonstrated marked improvements in reasoning and accuracy, showcasing RL's potential in complex STEM domains. To support such specialized applications, \textcite{albalak2025wyc} addressed the critical need for high-quality, task-specific data by creating Big-Math, the largest open-source dataset of over 250,000 math problems specifically curated for RL training. This rigorous human-in-the-loop filtering and novel reformulation of multiple-choice questions into open-ended formats bridge the quality-quantity gap, providing essential infrastructure for robust reasoning research.

In summary, the application of RL to LLM reasoning has evolved from empirical observations of RLHF's importance to sophisticated algorithmic interventions. These include search-based methods for robust exploration, fine-grained and process-based feedback mechanisms for multi-step tasks, and frameworks for self-verification and internal feedback. While significant progress has been made in enhancing mathematical, logical, and multi-step reasoning, critical challenges remain. The limited exploration identified by \textcite{havrilla2024m0y} persists, particularly in complex, open-ended reasoning environments. Furthermore, a crucial critical perspective from \textcite{bao2024wnc} suggests that post-training techniques like SFT and RLHF may inadvertently *weaken* the ideal causal structure of reasoning, potentially promoting "explaining" over genuine "reasoning." This highlights the need for future work to focus not only on improving performance but also on ensuring that RL-driven enhancements foster truly robust, causally sound, and interpretable reasoning processes in LLMs, rather than merely optimizing for superficial correctness. Future research will likely explore more sophisticated exploration techniques, integrate explicit value learning more robustly, and develop methods that explicitly preserve or strengthen the causal integrity of LLM reasoning.