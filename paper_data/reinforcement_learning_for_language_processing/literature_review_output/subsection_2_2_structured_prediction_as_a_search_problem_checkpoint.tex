\subsection{Structured Prediction as a Search Problem}

Traditional approaches to structured prediction in Natural Language Processing (NLP), such as parsing or sequence labeling, often relied on complex graphical models or dynamic programming algorithms to find the optimal global structure. However, these methods frequently struggled with the inherent non-local dependencies and the mismatch between local training objectives and global evaluation metrics. This challenge spurred a paradigm shift, reframing structured prediction as a sequential decision-making process, akin to a search problem, where a policy learns to construct the output incrementally. This perspective was crucial for applying reinforcement learning (RL)-like thinking to NLP before the widespread adoption of deep reinforcement learning.

The conceptual groundwork for learning policies that make sequential decisions can be traced back to foundational work in reinforcement learning. \textcite{Williams1992} introduced the REINFORCE algorithm, a seminal policy gradient method, demonstrating how to directly optimize a policy's parameters to maximize expected rewards without requiring a differentiable model of the environment. Building on this, \textcite{Sutton2000} further elaborated on policy gradient methods with function approximation, providing a robust theoretical framework for learning policies in complex, high-dimensional state spaces. While these works were general to RL, they established the core algorithmic toolkit for learning decision-making policies, which would later influence structured prediction.

In the context of NLP, early explorations began to frame structured output generation as a sequence of choices. For instance, \textcite{Ranzato2007} explored learning policies for sequence labeling in computer vision, demonstrating how to construct structured outputs step-by-step by learning a policy that makes local decisions. This work highlighted the potential of moving beyond static, one-shot predictions to a dynamic, generative process.

A pivotal contribution that formalized this conceptual bridge for NLP was the "learning to search" (L2S) framework, prominently articulated by \textcite{Daume2009}. This framework reframes complex structured prediction tasks, such as dependency parsing or machine translation, as a series of local classification decisions made by a learned policy. Instead of relying on a pre-defined search algorithm, L2S trains a policy to guide a search process through the space of possible output structures, aiming to find the globally optimal one. This is often achieved through imitation learning, where the policy is trained to mimic an expert's decisions during a search, effectively reducing structured prediction to a sequence of supervised classification problems. The core idea is to learn a policy that makes decisions at each step of the output construction, thereby addressing the sequential nature of linguistic outputs and the need for global optimization beyond local token-level accuracy.

The significance of the L2S framework and its broader implications for structured learning in NLP were further consolidated by comprehensive surveys. \textcite{Chang2015} provided a detailed overview of structured learning techniques for NLP, including the L2S paradigm, solidifying its importance as a method for tackling complex linguistic tasks. This work underscored how L2S offered a flexible and powerful way to handle the interdependencies inherent in structured outputs, moving beyond the limitations of independent local predictions.

Despite its innovative approach to global optimization, the L2S framework, particularly when relying on imitation learning, faced inherent limitations. The primary challenge was "exposure bias," where the learned policy, trained on expert demonstrations, might encounter states during inference that were never seen during training, leading to compounding errors. Furthermore, while L2S aimed for global optimization, it often did so indirectly by mimicking an expert, rather than directly optimizing for non-differentiable, task-specific evaluation metrics. These unresolved tensions, particularly the exposure bias and the indirect nature of optimization, laid the groundwork for the subsequent advent of deep reinforcement learning methods that sought to directly optimize for long-term, non-differentiable rewards in sequence generation tasks.