\subsection{RL for Code Generation and Software Engineering}
The application of Reinforcement Learning (RL) to Large Language Models (LLMs) is rapidly expanding beyond linguistic fluency to address the critical need for functional correctness in code generation, bug fixing, and other complex software engineering (SE) tasks. This frontier leverages high-fidelity feedback mechanisms, such as rule-based rewards or external verification tools, to enable LLMs to act as more autonomous and reliable software agents.

A foundational challenge in applying RL to complex domains is the design of effective reward functions. In a novel meta-application, \textcite{ma2023vyo} introduce EUREKA, a system that empowers LLMs to autonomously design human-level reward functions for reinforcement learning agents, particularly for intricate robotic manipulation tasks. By providing the LLM with environment source code as context, EUREKA leverages its coding and in-context improvement capabilities to evolve executable Python reward functions, demonstrating a powerful form of "coding LLMs" to generate feedback for other RL systems. This approach, which integrates evolutionary search and reward reflection, significantly outperforms expert-engineered rewards on a diverse suite of RL environments, showcasing the LLM's ability to create sophisticated, functionally-oriented feedback.

Building on the concept of external, functionally-driven feedback, \textcite{wang20252c7} tackle the crucial problem of generating functionally correct hardware description language (HDL) code, specifically Verilog, from natural language specifications. Their method integrates verification insights from testbenches directly into the LLM's training loop using Reinforcement Learning, specifically Direct Preference Optimization (DPO). They introduce an automatic testbench generation pipeline that leverages a Verilog Compiler Simulator (VCS) to create preference pairs, where code passing more test cases is preferred. This approach moves beyond structural similarity rewards, which are prone to "reward hacking," by directly optimizing for functional correctness, demonstrating significant improvements over state-of-the-art baselines in generating verifiable HDL.

Extending these advancements to the broader, often less structured domain of real-world software engineering, \textcite{wei2025v4d} present SWE-RL, an innovative RL framework for enhancing LLM reasoning in bug fixing and issue resolution. Recognizing the impracticality of execution-based rewards for complex SE tasks due to high costs and lack of executable environments, SWE-RL employs a lightweight rule-based reward function based on patch similarity using `difflib.SequenceMatcher`. This approach leverages massive software evolution data from GitHub Pull Requests, coupled with a novel data curation pipeline that predicts unmodified files to prevent LLM bias. Trained on this real-world data, Llama3-SWE-RL-70B achieves a 41.0\% solve rate on SWE-bench Verified, outperforming other medium-sized LLMs and rivaling proprietary models like GPT-4o. Crucially, \textcite{wei2025v4d} demonstrate that this specialized RL training leads to emergent generalized reasoning skills, improving performance on diverse out-of-domain tasks such as function coding, library use, and even mathematics, a significant finding that suggests specialized RL can foster broader cognitive abilities in LLMs.

These works collectively represent a new frontier for RL in language processing, shifting the focus from purely linguistic quality to functional correctness and generalized reasoning in software engineering. While \textcite{ma2023vyo} show the meta-capability of LLMs to design rewards for complex tasks, \textcite{wang20252c7} directly integrate external verification tools for high-assurance code generation. \textcite{wei2025v4d} then scale these principles to real-world software evolution data, demonstrating that even without expensive execution-based rewards, specialized RL frameworks can lead to emergent generalized reasoning skills, enabling LLMs to act as more autonomous and reliable software agents. Future directions will likely involve developing more sophisticated rule-based reward functions, integrating multi-modal feedback (e.g., from static analysis tools, user feedback), and exploring multi-agent RL systems where LLMs collaborate to solve complex software problems, further blurring the lines between language processing and autonomous software development.