\subsection{Implicit Human Signals and Distributionally Robust Alignment}

Achieving robust and nuanced human-AI alignment necessitates moving beyond simplistic feedback mechanisms to capture subtle human preferences, while simultaneously ensuring model resilience against shifts in real-world data distributions. This subsection explores innovative approaches that integrate implicit human signals into reward models and leverage distributionally robust optimization techniques to enhance the generalization capabilities of aligned Large Language Models (LLMs).

To address the inherent limitations of explicit human feedback, such as high cost, scalability issues, and potential inconsistencies, researchers are exploring implicit behavioral cues. \cite{lpezcardona20242pt} introduces \textit{GazeReward}, a pioneering framework that directly integrates implicit eye-tracking (ET) data into the Reward Model (RM) for LLM alignment. A key innovation in this work is the use of ET prediction models to generate \textit{synthetic} ET features from text. This synthetic generation effectively circumvents the practical challenges of collecting real human gaze data, making the integration of these subconscious preferences both scalable and cost-efficient, and demonstrating substantial performance gains in RM accuracy.

While richer feedback signals are crucial, the reward models themselves must be robust in interpreting these preferences. Traditional preference learning often assumes a linear scaling between preference strength and reward differences, an assumption that can lead to inflexible reward functions and misalignment between reward model accuracy and policy performance. \cite{hong2024mqe} addresses this by proposing an adaptive preference loss function, derived from a KL-constrained Distributionally Robust Optimization (DRO) formulation, which incorporates instance-specific scaling parameters. This allows the reward model to learn more nuanced relationships, assigning smaller scaling parameters to ambiguous preferences and larger ones to clear preferences, thereby enhancing the fidelity and internal robustness of the reward model's interpretation of human feedback.

Building on the need for robustness, a significant challenge for aligned LLMs in real-world deployment is their susceptibility to out-of-distribution (OOD) prompt shifts, which can severely degrade performance. To counter this, \cite{mandal2025qf5} introduces a comprehensive distributionally robust framework for Reinforcement Learning with Human Feedback (RLHF). This work applies DRO to the entire RLHF pipeline, including reward estimation, policy optimization, and Direct Preference Optimization (DPO), by defining uncertainty sets based on Total Variation (TV) distance. By explicitly accounting for shifts in the prompt distribution, this methodology aims to make aligned LLMs more resilient to unseen or OOD prompts, providing provable algorithms that enhance generalization capabilities with minimal alterations to existing pipelines.

Collectively, these advancements push the boundaries of human-AI alignment by leveraging richer, more subtle forms of feedback and ensuring robustness in diverse real-world scenarios. While \cite{lpezcardona20242pt} pioneers the use of implicit human signals through synthetic generation to enrich the feedback loop, \cite{hong2024mqe} enhances the internal robustness of reward models by adaptively interpreting preference strengths. Complementing these, \cite{mandal2025qf5} tackles external robustness, ensuring that the entire RLHF process and the resulting LLM generalize effectively to shifts in input data distributions. Despite these strides, challenges remain, particularly in bridging the fidelity gap between synthetic and real implicit signals, and in developing unified frameworks that seamlessly integrate implicit feedback with robust optimization techniques to address both internal preference ambiguity and external distribution shifts simultaneously. Further research is also needed to scale DRO techniques for even larger models and more complex, dynamic real-world environments.