\subsection*{Training Efficiency and Parameter-Efficient RLHF}

The application of Reinforcement Learning from Human Feedback (RLHF) has proven instrumental in aligning large language models (LLMs) with human preferences, yet its substantial computational cost and memory footprint pose significant barriers to broader accessibility and scalability \cite{kaufmann2023hlw}. Addressing this, research has focused on three primary directions: integrating parameter-efficient fine-tuning (PEFT) methods, developing efficient RLHF frameworks, and exploring alternative paradigms that simplify the RLHF pipeline.

A direct approach to mitigating the resource demands of RLHF involves the integration of Parameter-Efficient Fine-Tuning (PEFT) methods. \cite{sidahmed2024ikf} introduced Parameter Efficient Reinforcement Learning from Human Feedback (PE-RLHF), demonstrating that Low-Rank Adaptation (LoRA) can be effectively applied to both reward model (RM) training and the subsequent reinforcement learning of the policy model. This innovation significantly reduces memory usage (20-57\%) and speeds up training (1.05x-1.9x) by only updating a small fraction of parameters, making RLHF more practical without compromising performance.

Beyond parameter efficiency, advancements in RLHF frameworks and system optimizations are crucial for handling the complex, distributed nature of the training process. \cite{wang2023v62} proposed EfÔ¨Åcient Sampling-based RL (ESRL) for sequence generation, which is highly relevant to RLHF. ESRL employs a two-stage sampling process to decouple sequence sampling from gradient computation and dynamic sampling to adaptively adjust sampling size and temperature, leading to substantial reductions in memory consumption and training time. Building on the need for robust system architectures, \cite{sheng2024sf5} introduced HybridFlow, a flexible and efficient RLHF framework. HybridFlow leverages a hierarchical hybrid programming model and a specialized 3D-HybridEngine to manage heterogeneous workloads and complex parallelism strategies, achieving throughput improvements of 1.53x to 20.57x over existing baselines. Further optimizing resource utilization, \cite{mei2024eqt} presented ReaL, a system for efficient RLHF training that dynamically reallocates LLM parameters across the training cluster. ReaL uses an MCMC-based search algorithm to discover optimal execution plans for fine-grained resource allocation and parallelization, yielding speedups of up to 3.58x and significantly improving performance over static heuristic approaches.

Another promising avenue for efficiency lies in simplifying the RLHF pipeline itself by bypassing the computationally intensive reward model inference step. \cite{tang2023lop} explored zeroth-order optimization for direct policy learning from human ranking oracles, introducing ZO-RankSGD. This method directly uses human preferences to estimate descent directions for non-convex functions, offering a provably convergent approach without an explicit reward model. Extending this, \cite{zhang2024w99} proposed Zeroth-Order Policy Gradient (ZPG) and Zeroth-Order Block-Coordinate Policy Gradient (ZBCPG) for general RL problems. These algorithms estimate policy gradients from human preferences over trajectory pairs, providing provably efficient policy-based RLHF without reward inference, even in stochastic environments. Complementing this, \cite{zhang20248x9} developed Batched Sequential Action Dueling (BSAD), a model-free algorithm that directly identifies the optimal policy from human preference information in episodic MDPs. BSAD achieves instance-dependent sample complexity comparable to classic RL by employing backward action dueling and reward-free exploration, further demonstrating the feasibility of reward-model-free RLHF.

Collectively, these advancements represent a significant stride towards democratizing RLHF. While PEFT methods like LoRA offer immediate memory and speed benefits, sophisticated frameworks such as HybridFlow and ReaL tackle the systemic challenges of distributed training. Simultaneously, direct policy optimization methods provide alternative, more streamlined pipelines by eliminating the reward model. Future research will likely focus on integrating these diverse efficiency strategies, exploring more advanced PEFT and Representation Fine-Tuning (ReFT) techniques, and developing adaptive systems that can dynamically choose the most efficient combination of methods based on model size, task complexity, and available hardware.