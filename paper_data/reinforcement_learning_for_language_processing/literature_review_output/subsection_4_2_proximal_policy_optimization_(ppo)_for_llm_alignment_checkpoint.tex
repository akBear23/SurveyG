\subsection{Proximal Policy Optimization (PPO) for LLM Alignment}

Proximal Policy Optimization (PPO) stands as the predominant algorithm for the policy optimization stage in Reinforcement Learning from Human Feedback (RLHF), serving as a cornerstone for aligning Large Language Models (LLMs) with human preferences \cite{wang2024a3a, srivastava2025gfw}. PPO's widespread adoption stems from its balance of stability and sample efficiency, achieved through a clipped surrogate objective and multiple epochs of optimization on collected data, which helps prevent excessively large policy updates. However, applying PPO to the vast and complex action spaces of LLMs presents unique challenges, including training instability, hyperparameter sensitivity, and the intricate coordination of multiple models \cite{zheng2023c98}.

To address these inherent difficulties, significant advancements have been made in refining PPO for LLM alignment. \textcite{zheng2023c98} meticulously dissected PPO's inner workings and proposed **PPO-max**, a carefully calibrated collection of effective PPO implementations designed to ensure stable and efficient policy model training. Their work identified policy constraints as a key factor for effective implementation and introduced action space modeling metrics (e.g., perplexity, response length, KL divergence between policy and SFT model) as more informative indicators of stability than traditional reward and loss functions. This foundational work provided a robust PPO variant, making the RLHF pipeline more accessible and reliable.

Despite these algorithmic refinements, PPO-based RLHF remains susceptible to "reward hacking" or "overoptimization," where the policy exploits imperfections in the learned reward model rather than genuinely improving alignment. \textcite{singhal2023egk} empirically demonstrated that spurious correlations, such as length bias, can significantly drive reported RLHF improvements, revealing that reward models often learn shallow features. This issue is further explored by \textcite{rafailov2024ohd}, who established scaling laws for reward model overoptimization in Direct Alignment Algorithms (DAAs), showing that performance degradation can occur very early in training, even in methods that bypass explicit reward models, underscoring the fragility of preference-based optimization.

A wave of research has focused on mitigating these reward model imperfections and their impact on PPO training. \textcite{yu20249l0} enhanced reward model quality and interpretability by leveraging self-generated critiques, thereby providing a more robust signal for PPO to optimize. For scenarios involving composite reward models, \textcite{moskovitz2023slz} introduced constrained RLHF with dynamic proxy points, preventing PPO from overoptimizing individual reward components beyond their effective range. Directly modifying the reward signal for PPO, \textcite{fu2025hl3} proposed **Preference As Reward (PAR)**, a novel reward shaping technique that applies a sigmoid function to centered rewards. This method is theoretically grounded to stabilize critic training and minimize policy gradient variance, effectively mitigating reward hacking. Complementing this, \textcite{miao2025ox0} introduced **Energy loss-aware PPO (EPPO)**, a mechanistic approach that penalizes the increase in "energy loss" within the LLM's final layer, addressing reward hacking from an internal model dynamics perspective.

Further efforts to make PPO robust against reward model uncertainties include \textcite{zhai20238xc}'s **Uncertainty-Penalized RLHF (UP-RLHF)**, which uses diverse reward LoRA ensembles to quantify reward uncertainty and penalize rewards based on this estimate, preventing the policy from exploiting out-of-distribution (OOD) regions where the reward model is unreliable. Similarly, \textcite{dai2025ygq} proposed **Behavior-Supported Policy Optimization (BSPO)**, a value regularization technique that restricts PPO policy iteration to the in-distribution region of the reward model, thereby preventing extrapolation errors. More broadly, \textcite{zhang2024esn} developed **Adversarial Policy Optimization (AdvPO)**, a distributionally robust optimization for PPO that leverages lightweight uncertainty estimation from last layer embeddings to adversarially search for the most pessimistic reward within a confidence region, making PPO more resilient to reward model inaccuracies without the heavy computational cost of ensembles.

Beyond single-objective overoptimization, PPO for LLMs faces significant challenges in multi-objective alignment and practical deployment. The insufficiency of single reward models for diverse human preferences is highlighted by \textcite{chakraborty20247ew}, who proposed MaxMin-RLHF to ensure fairness and represent minority opinions, an objective that PPO could then optimize. The critical helpfulness-safety trade-off is a major concern, addressed by \textcite{tan2025lk0} with "Equilibrate RLHF," which includes an Adaptive Message-wise Alignment (AMA) approach that can be integrated with Adaptive PPO (APPO) to selectively emphasize safety-critical segments. \textcite{xu20242yo} introduced **Constrained Generative Policy Optimization (CGPO)** with a "Mixture of Judges" for multi-task learning, developing new primal-type constrained RLHF optimizers (CRPG, CODPO, CRRAFT) that offer robust alternatives or enhancements to PPO in complex multi-objective settings. Furthermore, \textcite{boldi2024d0s} proposed Pareto-Optimal Preference Learning (POPL) to learn a set of reward functions or policies that are Pareto-optimal for distinct hidden context groups, which could subsequently guide PPO optimization. To address sparse and inconsistent human feedback, \textcite{lai2024ifx} introduced ALaRM, a hierarchical reward modeling framework that provides more consistent and fine-grained signals for PPO-like algorithms.

Practical challenges like catastrophic forgetting and training instability in production environments have also driven PPO advancements. \textcite{hou2024tvy} detailed the ChatGLM-RLHF pipeline, offering practical solutions for large-scale LLMs, including strategies for PPO stability, reward debiasing (e.g., bucket-based length balancing), and mitigating catastrophic forgetting by incorporating next-token-prediction loss from SFT data into the RLHF objective. To further combat the "alignment tax" (degradation of pre-trained capabilities), \textcite{lu202435m} proposed **Online Merging Optimizers** (e.g., OnDARE, OnTIES) that integrate model merging into *each* optimization step of RLHF, dynamically steering PPO gradients to boost rewards while preserving foundational capabilities.

In conclusion, PPO remains a central algorithm for LLM alignment, with continuous research dedicated to enhancing its stability, robustness, and applicability to complex, multi-objective scenarios. The evolution from generic PPO to specialized variants like PPO-max, coupled with sophisticated regularization techniques and multi-objective frameworks, underscores a relentless effort to mitigate reward model imperfections, prevent catastrophic forgetting, and navigate the vast action spaces of LLMs, ensuring stable and effective alignment. Future directions will likely continue to explore more adaptive and data-efficient PPO variants, integrating deeper theoretical understandings of model internals and human preferences to achieve truly robust and trustworthy AI systems.