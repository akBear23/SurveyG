\subsection{Optimizing Non-Differentiable Metrics in NLP}

Traditional approaches to training sequence generation models in Natural Language Processing (NLP) predominantly rely on Maximum Likelihood Estimation (MLE). MLE optimizes the probability of generating the correct next token given previous tokens and the ground truth, proving effective for learning language distributions. However, this token-level optimization suffers from inherent limitations when the ultimate goal is to produce outputs that score well on holistic, non-differentiable, task-specific evaluation metrics such as BLEU for machine translation or ROUGE for summarization, or more broadly, human-aligned quality judgments \cite{Ranzato2016}. This fundamental discrepancy between the training objective (local token accuracy) and the evaluation objective (global sequence quality) often leads to issues like "exposure bias" \cite{Ranzato2016}. Exposure bias occurs because models are trained on ground-truth prefixes but are forced to generate subsequent tokens based on their own potentially erroneous outputs during inference, leading to a compounding of errors and a divergence from the desired output distribution.

Early research highlighted the critical need to bridge this gap. While recurrent neural networks (RNNs) demonstrated promise for sequence transduction tasks under supervised learning \cite{Ranzato2007}, their performance was fundamentally constrained by the MLE objective. The desire for direct metric optimization became evident with methods like Minimum Error Rate Training (MERT) \cite{Rennie2005}. MERT discriminatively tuned parameters for statistical machine translation systems to maximize BLEU scores. Critically, MERT operated by performing a line search in parameter space to directly optimize the non-differentiable metric, demonstrating the feasibility and importance of such direct optimization. However, MERT was computationally intensive, specific to feature-based translation models, and not readily adaptable to the end-to-end gradient-based training of deep neural networks. Concurrently, frameworks for structured prediction, such as "learning to search," conceptualized complex NLP tasks like parsing or sequence labeling as a sequence of local classification decisions \cite{DaumeIII2009}. While these methods often leveraged imitation learning to guide a search process, they underscored the sequential decision-making paradigm inherent in language generation and the need for global optimization beyond local accuracy. These precursors collectively identified the core problem: MLE's inability to directly optimize for human-aligned, non-differentiable metrics and the practical limitations of pre-deep learning discriminative methods.

This persistent challenge motivated the adoption of reinforcement learning (RL), which offers a powerful paradigm to directly optimize non-differentiable objectives without requiring explicit gradient information from the reward function itself. This capability enables models to generate text that is more aligned with human judgments of quality. A pivotal conceptual breakthrough was the application of policy gradient methods, particularly REINFORCE \cite{Ranzato2016}, to sequence-to-sequence learning. This approach reframed text generation as a sequential decision-making process, where each generated token is an action, and the entire generated sequence receives a scalar reward based on a non-differentiable metric (e.g., BLEU, ROUGE). By sampling sequences from the model's current policy and using the resulting metric score as a reward signal, REINFORCE allowed models to be trained to directly maximize these sequence-level metrics. This inherently mitigated exposure bias, as the model learned from its own generated sequences rather than solely from ground truth. Subsequent work also explored actor-critic algorithms for sequence prediction, aiming to reduce the high variance often associated with pure policy gradient methods and improve training stability \cite{Bahdanau2016}.

However, the practical application of these early RL methods for sequence generation was not without its challenges. While conceptually sound, policy gradient methods often suffered from high variance and sample inefficiency, requiring extensive exploration of the vast action space (vocabulary) and long sequences. The computational burden of sampling numerous sequences for reward estimation and gradient calculation, especially for long sequences and large vocabularies, posed a significant practical hurdle, impacting training time and memory consumption \cite{wang2023v62}. This inherent inefficiency limited the scalability and widespread adoption of RL for direct metric optimization in complex NLP tasks, even as the conceptual framework proved its merit. Despite these practical difficulties, the introduction of RL provided the foundational mechanism to bridge the gap between token-level training and sequence-level evaluation, laying the groundwork for later deep RL applications in language generation.