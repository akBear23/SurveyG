\subsection{Data-Efficient and Self-Supervised Alignment}

The prohibitive costs associated with human supervision and data collection for aligning Large Language Models (LLMs) pose a significant challenge to their scalability and accessibility. This subsection delves into a critical area of research focused on developing data-efficient and self-supervised alignment methods, aiming to drastically reduce reliance on costly human effort. These innovations are pivotal for democratizing LLM alignment and enabling its application across a broader range of contexts.

A fundamental shift towards minimizing human intervention is seen in **principle-driven self-alignment** and **iterative self-training**. \texttt{SELF-ALIGN} \cite{sun20238m7} introduced a framework enabling LLM alignment "from scratch" using fewer than 300 lines of human-written principles. This approach leverages in-context learning (ICL) to guide response generation based on explicit principles, which are then internalized through a process called \texttt{Principle Engraving}. This method critically addresses the high cost and potential biases of human data, offering a pathway to alignment with minimal direct human supervision. In contrast, \textbf{Reinforced Self-Training (ReST)} \cite{gulcehre2023hz8} provides an efficient offline RL paradigm that iteratively generates and filters high-quality data. ReST decouples data generation from policy improvement, allowing the model to continuously augment its training data with samples from an improving policy. While \texttt{SELF-ALIGN} aims for alignment with minimal *initial* human input by encoding principles, ReST leverages an *existing* reward model to iteratively refine its policy and generate higher-quality data, thereby overcoming the limitations of fixed offline datasets and enhancing data efficiency through self-supervision. Both methods represent distinct philosophical approaches to reducing human supervision: one by front-loading principles, the other by bootstrapping from a feedback mechanism.

Maximizing information gain from limited human labels is another crucial aspect of data-efficient alignment, addressed through optimal dataset design and active learning strategies. \cite{scheid20244oy} presents a theoretical framework for the **optimal offline design of reward model datasets** in RLHF, framing it as a pure exploration contextual dueling bandit problem. Their ODPO (Optimal Design for Preference Optimization) algorithm leverages optimal design theory to select the most informative pairs for human labeling upfront, offering theoretical guarantees on simple regret minimization. For dynamic, online settings, **Self-Exploring Language Models (SELM)** \cite{zhang2024lqf} introduces an optimism-based active exploration mechanism for online alignment. SELM employs a bilevel optimization objective that is optimistically biased towards potentially high-reward responses, actively guiding the LLM to explore diverse regions of the response space and achieving significant performance boosts without additional computational overhead. Furthermore, to enhance the reliability of human feedback itself, \cite{hao2024iyj} proposes an online weighted aggregation mechanism that dynamically adjusts human labeler weights based on their feedback accuracy, incentivizing truthful reporting from strategic agents and achieving sublinear regret. These methods collectively aim to make every human interaction count, extracting maximum value from scarce human resources by strategically selecting data for annotation (ODPO) or actively exploring to gather better feedback (SELM). Complementing these, \cite{chan2024xig}'s Attention Based Credit (ABC) method densifies the sparse reward signal in RLHF by leveraging the attention maps from the reward model itself. This approach provides "dense reward for free," requiring no additional human annotation or modeling, thereby maximizing the information extracted from the *existing* scalar reward signal and improving training stability and learning rates without altering the optimal policy.

The pursuit of data efficiency also extends to self-improvement mechanisms that leverage the model's internal representations or bypass explicit reward models entirely, further reducing dependence on external feedback. \cite{chen2024vkb} introduces **DICE (self-alignment with DPO Implicit rewards)**, an iterative self-alignment framework that bootstraps LLMs using the *implicit reward model* derived from a DPO-tuned LLM itself. This allows for continuous improvement without any external feedback in subsequent rounds, while also incorporating length-regularized reward shaping and experience replay to mitigate length bias and catastrophic forgetting. Pushing the boundaries of data independence, \cite{xia2024rab}'s **Inverse-Q*** enables token-level RL alignment *without* preference data or explicit reward models by directly estimating a conditionally optimal policy and using its probabilities for token-level credit assignment. This offers a more efficient and robust alternative to conventional RLHF, particularly in low-resource settings. Further reducing the reliance on a separate reward model, \cite{zhang20248x9} proposes Batched Sequential Action Dueling (BSAD), a model-free RLHF algorithm that directly identifies the optimal policy from human preference information, completely bypassing the reward model inference step. This approach addresses the challenges of reward model overfitting and generalization, offering provable instance-dependent sample complexity. Additionally, \cite{singla2024dom} proposes **Dynamic Rewarding with Prompt Optimization (DRPO)**, a tuning-free, search-based framework that allows LLMs to self-align and optimize alignment instructions (system prompts and ICL examples) at inference time. This dynamic rewarding mechanism adapts to specific queries, enabling cost-efficient and rapidly adaptable alignment without model retraining or human supervision. These diverse approaches, from bootstrapping implicit rewards to entirely model-free or inference-time adjustments, represent a spectrum of strategies for achieving greater autonomy and efficiency in alignment, each with its own trade-offs regarding stability and potential for bias.

In summary, the field is rapidly advancing towards making LLM alignment more scalable and accessible by drastically reducing the need for costly human supervision. Innovations span principle-driven self-alignment, iterative self-training, optimal dataset design, and active learning strategies. A core tension exists between the desire for full model autonomy and the risk of unguided model drift or the amplification of subtle internal biases without explicit external steering. Methods like \texttt{SELF-ALIGN} rely on the robustness of initial principles, which can be brittle if incomplete or conflicting, while iterative self-training methods like ReST and DICE risk amplifying biases present in the initial reward model or self-generated data. Reward-model-free approaches like BSAD and Inverse-Q* offer theoretical elegance but still depend on the quality and representativeness of the *initial* human preference data. Future research will likely focus on developing more robust self-correction mechanisms, theoretically grounded methods to guarantee alignment quality across diverse and complex tasks, and hybrid approaches that judiciously combine minimal human oversight with advanced self-supervision to balance efficiency, safety, and performance.