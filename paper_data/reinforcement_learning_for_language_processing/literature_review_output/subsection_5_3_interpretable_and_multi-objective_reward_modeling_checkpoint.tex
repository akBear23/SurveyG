\subsection{Interpretable and Multi-Objective Reward Modeling}
The foundational success of Reinforcement Learning from Human Feedback (RLHF) in aligning Large Language Models (LLMs) was initially built upon single-scalar, black-box reward models (RMs) \cite{rafailov20239ck, zheng2023c98}. While effective for initial alignment, this approach suffered from significant limitations: it offered minimal insight into *why* a particular response was preferred, struggled to capture the inherent complexity and subjectivity of human values, and was prone to issues like reward hacking and the exploitation of spurious correlations, such as verbosity bias \cite{singhal2023egk, rafailov2024ohd}. Critically, this reduction of diverse human preferences to a single scalar often leads to an "objective mismatch," where the numerical optimization targets diverge from true human intent, resulting in unintended behaviors like excessive refusal or "laziness" \cite{lambert2023c8q}. Furthermore, the opacity of these models raises concerns about whose values are encoded and the potential for unexamined biases \cite{lambert2023bty}. Consequently, research has increasingly focused on developing reward models that are not only accurate but also transparent, interpretable, and capable of handling multiple, potentially conflicting, human objectives (e.g., helpfulness, safety, conciseness) \cite{wang2024a3a, zhang20242mw, srivastava2025gfw}.

Addressing the inherent diversity and potential contradictions in human preferences has been a primary driver for multi-objective reward modeling. Early efforts moved beyond a monolithic reward by learning multiple reward functions or employing diverse evaluators. \textcite{chakraborty20247ew} introduced MaxMin-RLHF, which learns a mixture of reward models to represent distinct human sub-population preferences, thereby acknowledging the insufficiency of a single reward for diverse feedback. Extending this concept, \textcite{boldi2024d0s} proposed Pareto-Optimal Preference Learning (POPL), which leverages lexicase selection to learn a set of Pareto-optimal reward functions or policies. This framework effectively handles contradictory preferences arising from "hidden context" without requiring explicit group labels, offering a more generalized approach to preference diversity. For complex tasks characterized by sparse and inconsistent human feedback, \textcite{lai2024ifx} introduced ALaRM (Align Language Models via Hierarchical Rewards Modeling), which integrates holistic rewards with proactively selected aspect-specific rewards, providing more precise and consistent guidance by decomposing the overall reward into finer-grained components. These methods collectively aim to represent the multifaceted nature of human preferences more faithfully than a single-scalar approach.

Beyond modeling diverse preferences, a critical challenge lies in balancing conflicting objectives, such as helpfulness and safety, and mitigating specific biases. \textcite{tan2025lk0} addressed the "over-safe" phenomenon, where LLMs excessively refuse benign queries, through Equilibrate RLHF. This framework employs a Fine-grained Data-centric (FDC) approach to categorize safety data and an Adaptive Message-wise Alignment (AMA) method using gradient masking to selectively highlight safety-critical segments, allowing for nuanced control over safety responses. Complementing this, \textcite{zhang2024b6u} proposed Bi-Factorial Preference Optimization (BFPO), an efficient supervised learning framework that re-parameterizes the joint RLHF objective for safety and helpfulness into a single loss using a novel labeling function, significantly reducing the need for costly "red teaming" data while still managing multiple objectives. Similarly, \textcite{xu20242yo} developed Constrained Generative Policy Optimization (CGPO) with a "Mixture of Judges" (combining rule-based and LLM-based evaluators) to provide fine-grained control and enforce constraints, mitigating reward hacking in multi-task learning scenarios by integrating explicit objective-level feedback.

Mitigating specific biases, particularly verbosity, has also been a key focus. \textcite{hou2024tvy} incorporated "Bucket-Based Length Balancing" into their ChatGLM-RLHF pipeline, which involves grouping responses by length during reward model training to reduce the model's inclination to prefer longer responses in a production setting. Further, \textcite{chen2024vkb} introduced length-regularized reward shaping within an iterative DPO framework, which directly debiases the implicit DPO reward model to construct a length-unbiased preference dataset, offering an alternative to loss-function-based length penalties and avoiding their associated hyperparameter tuning. These explicit adjustments to reward objectives are crucial for ensuring that high scores genuinely reflect quality rather than superficial characteristics.

The pursuit of truly interpretable and steerable reward models has also led to frameworks that provide granular insights into preference rationales. \textcite{yu20249l0} enhanced reward model quality and interpretability with Critic-RM, which leverages self-generated, high-quality critiques to provide explicit rationales alongside scalar scores. This allows for a more transparent understanding of the reward model's judgment process. In a similar vein, \textcite{sun20238m7} introduced Principle-Driven Self-Alignment, where LLMs are guided by a small set of human-written principles (e.g., ethical, informative) and in-context exemplars during response generation. These principles are then "engraved" into the model's parameters through fine-tuning, enabling the model to directly generate aligned responses without needing explicit prompting with principles during inference. This approach offers a direct path to multi-objective and steerable alignment by internalizing human-defined values. Furthermore, \textcite{pignatelli2024ffp} demonstrated the potential of LLMs for automated, granular credit assignment and subgoal decomposition, providing more interpretable and aspect-specific reward signals for complex tasks. This aligns with the idea of multi-task reward modeling, where \textcite{hou202448j} trained a unified reward model using a multi-task objective, combining pairwise ranking loss for human preferences with cross-entropy loss for binary-labeled reasoning data, including process supervision for intermediate steps. This integration of diverse feedback types contributes to a more comprehensive and interpretable reward signal.

A significant advancement in this direction is the comprehensive framework by \textcite{wang20247pw}, ArmoRM, which moves beyond black-box single-scalar RMs by training on multi-dimensional *absolute-rating* data. Each dimension corresponds to a human-interpretable objective (e.g., helpfulness, correctness, verbosity, safety), providing decomposable explanations for reward scores and offering granular transparency into the model's preference judgments. To enable dynamic and context-aware preference modeling, ArmoRM integrates a Mixture-of-Experts (MoE) Scalarization mechanism. This involves a shallow MLP gating network that dynamically selects and weights the most suitable reward objectives based on the input prompt's context, allowing for flexible and steerable alignment. Crucially, \textcite{wang20247pw} also directly addressed the pervasive verbosity bias by explicitly adjusting each reward objective to ensure zero Spearman correlation with the verbosity objective, ensuring that high scores genuinely reflect quality rather than mere length. This framework achieved state-of-the-art performance on benchmarks like RewardBench, even surpassing much larger models and LLM-as-a-judge methods, demonstrating its efficiency and effectiveness in achieving nuanced and controllable alignment.

The evolution towards interpretable and multi-objective reward modeling represents a crucial step in developing more transparent, controllable, and robust LLMs that truly reflect the multifaceted nature of human preferences. While significant progress has been made in decomposing reward signals, dynamically weighting objectives, and mitigating specific biases, challenges remain. Future research needs to focus on the scalability of collecting high-quality, multi-dimensional absolute-rating data, developing more sophisticated theoretical guarantees for dynamic objective weighting, and establishing robust evaluation metrics for the interpretability and steerability of these complex reward models.