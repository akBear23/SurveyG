\subsection{LLM/VLM-Driven Automated Reward Design}

The manual design of reward functions remains a significant bottleneck in Reinforcement Learning (RL), often requiring extensive human effort, domain expertise, and iterative refinement to avoid suboptimal or unintended agent behaviors. Recent advancements in large language models (LLMs) and vision-language models (VLMs) offer promising avenues to automate this challenging process, bridging the gap between high-level human intent and low-level reward signals for autonomous agents.

Early efforts in this domain leveraged LLMs' code generation capabilities to synthesize executable reward functions. \textcite{ma2023vyo} introduced \textit{Eureka}, a pioneering method where an LLM (GPT-4) generates executable Python reward code by taking environment source code as context. This approach employs an evolutionary search and a novel "reward reflection" mechanism, which provides fine-grained feedback on policy dynamics to the LLM for targeted code editing, enabling human-level and even superhuman performance on complex dexterous manipulation tasks. Building upon the concept of LLM-driven reward evolution, \textcite{hazra2024wjp} proposed \textit{REvolve}, an evolutionary framework that uses LLMs as intelligent genetic operators for mutation, crossover, and selection of reward functions. Critically, \textit{REvolve} addresses a limitation of earlier iterative methods by employing human feedback (preferences over policy rollouts) as a direct, non-differentiable fitness function, making it particularly effective for tasks with subjective notions of "good" behavior and reducing the need for a pre-defined fitness metric.

A parallel and distinct line of research shifted from LLMs generating explicit code to VLMs directly inferring reward signals from visual observations, thereby eliminating the need for environment source code or low-level state access. \textcite{rocamonde2023o9z} demonstrated that pretrained VLMs, such as CLIP, can serve as \textit{zero-shot reward models (VLM-RMs)} for vision-based RL. Their method computes reward as the cosine similarity between a natural language task description's embedding and the visual observation's embedding, introducing "Goal-Baseline Regularization" to project out irrelevant information and shape the reward landscape more effectively. This approach highlighted a strong scaling effect, where larger VLMs yielded superior reward models for complex visual tasks. However, direct VLM scores can sometimes be noisy or high-variance, prompting further innovation.

Addressing these limitations, \textcite{wang2024n8c} introduced \textit{RL-VLM-F}, which leverages advanced VLMs (e.g., GPT-4V, Gemini) to provide automated *preference feedback* over pairs of visual observations based on a natural language goal. Instead of directly outputting reward scores, the VLM acts as an automated human annotator, generating preference labels from which a robust reward function is then learned. This method employs a novel two-stage VLM querying process for analysis and labeling, operating solely on visual observations and a text goal, making it highly applicable to complex visual tasks, including deformable object manipulation, where ground-truth state information is often unavailable. \textit{RL-VLM-F} significantly reduces human effort in preference-based RL and demonstrates superior performance over prior methods that directly use VLM scores for reward generation.

Collectively, these works represent a significant leap in automating reward design, moving from LLMs generating interpretable code that requires environment state access to VLMs directly interpreting visual observations and natural language goals. While LLM-driven code generation offers transparency and fine-grained control over reward components, VLM-driven inference provides a more natural, visual-centric approach, particularly for tasks where only pixel data is available. Future research could explore hybrid approaches that combine the interpretability of LLM-generated code with the visual grounding of VLM-inferred signals, or focus on improving the robustness of VLM-based rewards to abstract visual environments and enhancing their spatial reasoning capabilities for real-world deployment. The computational cost associated with iterative LLM interactions and extensive RL training runs, as well as ensuring the safety and alignment of autonomously designed rewards, remain critical areas for continued investigation.