\subsection{Reinforcement Learning Fundamentals for Language Tasks}

Reinforcement Learning (RL) offers a powerful paradigm for framing language generation and understanding as sequential decision-making problems, where an intelligent agent learns to produce optimal linguistic outputs by maximizing cumulative rewards. This foundational approach is crucial for addressing the inherent challenges of language tasks, such as optimizing non-differentiable metrics and managing long-term dependencies.

At its core, an RL system for language tasks comprises several key components. The \textit{agent} is typically a language model, often a neural network, responsible for generating text. This agent interacts with an \textit{environment}, which can be conceptualized as the current textual context, the preceding tokens, or the user's input prompt. The \textit{state} represents the current situation, such as the partial sequence of text generated so far combined with the initial input. Based on this state, the agent performs an \textit{action}, which in language generation corresponds to selecting the next word or token from its vocabulary. The quality of this action, and ultimately the generated sequence, is evaluated by a scalar \textit{reward} signal. This reward is critical for guiding the learning process, indicating how "good" a particular linguistic output or decision was. The agent's strategy for choosing actions is defined by its \textit{policy}, typically a probability distribution over possible next tokens given the current state. Finally, a \textit{value function} estimates the expected cumulative future rewards from a given state or state-action pair, enabling the agent to make decisions that lead to long-term optimal outcomes.

Early applications of RL in sequence generation, while not always directly in language, established the utility of sequential decision-making frameworks. For instance, initial work explored Q-learning for tasks like video shot classification, demonstrating how an agent could learn optimal sequences of actions by maximizing rewards \cite{Ranzato2007}. This principle was then extended to address the specific challenges of natural language processing, particularly in tasks where traditional supervised learning struggles with non-differentiable evaluation metrics and exposure bias. Exposure bias occurs when a model is trained on ground truth sequences but then generates its own sequences during inference, leading to a mismatch.

To mitigate these issues, researchers began employing policy gradient methods, such as REINFORCE, to directly optimize task-specific, non-differentiable metrics. For example, \cite{Li2016} applied deep reinforcement learning to dialogue generation, treating the generation of each turn as a sequential decision process and using rewards based on dialogue quality. Similarly, \cite{Paulus2017} and \cite{Luo2019} leveraged policy gradients for abstractive summarization, where the agent learns to generate summaries by maximizing metrics like ROUGE, which are not amenable to standard backpropagation. In machine translation, \cite{Wang2018} utilized deep reinforcement learning for syntactic neural machine translation, demonstrating how RL could improve translation quality by optimizing for metrics like BLEU. These approaches collectively highlighted RL's ability to train models to directly maximize desired outcomes, moving beyond token-level accuracy.

Further advancements introduced more sophisticated RL algorithms to enhance sequence prediction. \cite{Bahdanau2017} proposed a general actor-critic algorithm for sequence prediction, combining the strengths of policy-based and value-based methods. This framework allowed the actor to learn a policy for generating sequences while the critic evaluated the actions, leading to more stable and efficient learning compared to pure policy gradient methods. Despite these advancements, a persistent challenge in these early applications was the design of effective reward functions. Sparse and delayed rewards, often derived from proxy metrics that may not perfectly align with human judgment, made learning difficult and sample-inefficient, a problem broadly recognized in complex RL environments \cite{laleh2024wmr}.

The foundational understanding of RL for sequential language tasks has paved the way for more complex applications, notably in aligning large language models (LLMs) with human preferences and instructions. Modern techniques like Reinforcement Learning from Human Feedback (RLHF) build directly upon these fundamentals, using human preferences to train a reward model that then guides an RL agent (typically using Proximal Policy Optimization, PPO) to fine-tune an LLM \cite{Ziegler2019, Stiennon2020, Ouyang2022}. This paradigm shift, also incorporating AI-generated feedback (RLAIF) \cite{Bai2022}, addresses the challenge of reward engineering by learning a reward function that better captures nuanced human values and complex principles, as highlighted in discussions on integrating generative AI into various domains \cite{yu2023xc4}. While these advanced methods represent a significant evolution, they fundamentally rely on the core RL concepts of sequential decision-making, reward maximization, and policy learning established by earlier work. The ongoing challenge remains in scaling high-quality feedback and ensuring the learned reward functions accurately and consistently reflect desired complex linguistic behaviors.