\subsection{Core RLHF Algorithms and Architectures}
The alignment of large language models (LLMs) with human preferences and instructions is primarily achieved through Reinforcement Learning from Human Feedback (RLHF), a multi-stage pipeline that has become the standard for developing capable conversational AI \cite{srivastava2025gfw, wang2024a3a}. This subsection introduces the foundational RLHF pipeline, detailing its typical stages, the role of Proximal Policy Optimization (PPO), and the architectural considerations for its implementation.

The standard RLHF pipeline typically unfolds in three sequential stages. First, a pre-trained LLM undergoes \textit{supervised fine-tuning (SFT)} on a dataset of high-quality human-written demonstrations, often in an instruction-following format. This initial step grounds the model in desired behaviors and response styles \cite{srivastava2025gfw}. Second, a separate \textit{reward model (RM)} is trained to predict human preferences. This RM is typically a neural network, often a fine-tuned version of the LLM itself, which takes a prompt and a model-generated response as input and outputs a scalar score indicating its perceived quality or alignment with human values. The RM is trained on a dataset of human preference comparisons, where annotators rank or rate multiple model outputs for a given prompt \cite{wang2024a3a}. Finally, the SFT model, now termed the \textit{policy model}, is fine-tuned using reinforcement learning to maximize the scores assigned by the trained RM.

Proximal Policy Optimization (PPO) \cite{zheng2023c98} is the most widely adopted RL algorithm for this final fine-tuning stage due to its balance of sample efficiency, stability, and performance. PPO aims to update the policy in small steps to prevent large, destabilizing changes, ensuring that the new policy does not deviate too far from the previous one. The PPO objective typically includes a KL-divergence penalty term, which regularizes the policy's deviation from the initial SFT model (often referred to as the \textit{reference model}), preventing catastrophic forgetting and maintaining the model's general capabilities \cite{srivastava2025gfw}. The architectural setup for PPO-based RLHF involves coordinating four distinct models: the \textit{policy model} (the LLM being optimized), a \textit{value model} (often a separate head on the policy model or a distinct network, trained to predict the expected future reward), the \textit{reward model} (trained on human preferences), and the \textit{reference model} (a frozen copy of the SFT model). This complex interplay of models presents significant engineering and algorithmic challenges \cite{zheng2023c98}.

Despite its widespread adoption, PPO-based RLHF is notoriously complex, unstable, and highly sensitive to hyperparameters, leading to immense trial-and-error costs \cite{zheng2023c98}. To address these issues, \cite{zheng2023c98} introduced PPO-max, a carefully calibrated and robust version of PPO that ensures stable policy training and enables longer training steps. However, even with algorithmic refinements, the reliance on an imperfect reward model introduces further complexities. A significant challenge is "reward hacking," where the policy exploits flaws in the RM to maximize proxy rewards without genuinely improving human-desired behaviors. For instance, \cite{singhal2023egk} revealed that reward models often exhibit a strong bias towards longer responses, leading PPO-trained models to generate verbose outputs that do not necessarily reflect higher quality.

Various innovations have emerged to mitigate these PPO-specific and reward model-induced limitations. \cite{moskovitz2023slz} proposed constrained RLHF to prevent reward model overoptimization, particularly in composite reward models, by dynamically learning Lagrange multipliers to adapt component reward model influence. Similarly, \cite{fu2025hl3} introduced Preference As Reward (PAR), a reward shaping technique that applies a sigmoid function to centered rewards, theoretically justifying its ability to stabilize critic training and minimize policy gradient variance, thereby mitigating reward hacking. Practical challenges such as catastrophic forgetting, where RLHF can degrade the LLM's foundational abilities, have also been addressed. \cite{hou2024tvy} incorporated next-token-prediction loss from SFT data into the RLHF objective and introduced "Bucket-Based Length Balancing" for reward model debiasing. Complementing this, \cite{lu202435m} proposed "Online Merging Optimizers" to dynamically blend RLHF gradients with SFT model information at each step, effectively boosting rewards while mitigating the alignment tax.

Beyond these PPO-centric improvements, the inherent complexity and instability of the multi-model RLHF pipeline spurred the development of alternative alignment algorithms. Direct Preference Optimization (DPO) \cite{rafailov20239ck} offers a significant simplification by reformulating the RLHF problem as a classification loss. DPO directly optimizes the language model policy to align with human preferences without requiring an explicit reward model or reinforcement learning, deriving the optimal policy in closed form from a KL-constrained reward maximization objective. This approach eliminates the need for a separate reward model training phase and the computationally expensive sampling from the LLM during training, making it more stable and lightweight. However, even direct alignment algorithms like DPO are not immune to the fundamental problem of overoptimization, as demonstrated by \cite{rafailov2024ohd}, where performance can still degrade after a certain point due to the implicit reward model exploiting spurious correlations. The theoretical underpinnings of such KL-constrained preference learning for RLHF were further explored by \cite{xiong2023klt}, which provided finite-sample guarantees and proposed iterative DPO variants.

The architectural demands of RLHF, involving multiple large models and complex data flows, also necessitate robust system-level solutions. \cite{sheng2024sf5} introduced HybridFlow, a framework designed to improve the computational efficiency and flexibility of distributed RLHF training through a hybrid single/multi-controller paradigm and optimized GPU allocation. This addresses the practical challenges of scaling these multi-model pipelines.

In summary, the core RLHF pipeline, predominantly driven by PPO, establishes a powerful mechanism for aligning LLMs with human preferences by iteratively refining a policy against a learned reward signal. However, its inherent complexity, sensitivity to hyperparameters, and susceptibility to reward hacking (e.g., length bias \cite{singhal2023egk}) have necessitated continuous algorithmic and architectural innovations. The emergence of simplified alternatives like DPO \cite{rafailov20239ck} and robust PPO variants \cite{zheng2023c98, fu2025hl3} highlights the ongoing effort to make LLM alignment more stable and efficient. Yet, persistent challenges such as reward model imperfections, overoptimization \cite{rafailov2024ohd}, and the trade-off between generalization and diversity \cite{kirk20230it} underscore the need for further research into more robust, interpretable, and scalable alignment methods. These foundational algorithms and their challenges lay the critical groundwork for understanding the subsequent advancements and the complexities of achieving truly human-aligned AI.