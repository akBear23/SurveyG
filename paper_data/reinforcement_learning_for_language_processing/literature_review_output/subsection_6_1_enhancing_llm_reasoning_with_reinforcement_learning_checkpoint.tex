\subsection{Enhancing LLM Reasoning with Reinforcement Learning}

While Large Language Models (LLMs) excel at pattern recognition and fluent text generation, their ability to perform robust, multi-step reasoning in complex cognitive tasks, such as mathematical problem-solving, logical inference, and multi-step decision-making, often remains a significant challenge. Superficial pattern matching can lead to plausible but incorrect outputs. Reinforcement Learning (RL) offers a powerful paradigm to move LLMs beyond these limitations, enabling them to optimize for the correctness and coherence of their reasoning *process* and *outcome* by leveraging explicit feedback signals. This section delves into methodologies that apply RL to significantly enhance LLM reasoning, encompassing systematic comparisons of algorithms, novel reward schemes, and the strategic integration of search algorithms for self-correction and fine-grained preference optimization.

A foundational step in this direction involves systematically evaluating which RL algorithms are most effective for reasoning tasks. \cite{havrilla2024m0y} conducted a comprehensive study comparing Expert Iteration (EI), Proximal Policy Optimization (PPO), and Return-Conditioned RL (RCRL) for fine-tuning LLMs on mathematical reasoning. Their findings revealed that EI consistently achieved superior performance with competitive sample efficiency compared to PPO and RCRL in deterministic reasoning tasks. Crucially, this work highlighted that RL fine-tuning could simultaneously improve both greedy accuracy (\texttt{maj@1}) and best-of-K sampling (\texttt{pass@96}), resolving a trade-off often observed with purely supervised fine-tuning. The study also identified limited exploration as a critical bottleneck for RL in LLM reasoning, suggesting that while RL can refine existing reasoning abilities, it struggles to discover entirely novel reasoning paths without sufficient guidance or intrinsic motivation.

Effective RL-driven reasoning necessitates high-fidelity, task-specific data, especially for verifiable domains like mathematics. Addressing this data bottleneck, \cite{albalak2025wyc} introduced \texttt{Big-Math}, a large-scale, high-quality math dataset specifically curated for RL training. Comprising over 250,000 problems, this dataset emphasizes open-ended questions with uniquely verifiable solutions, which is critical for training models to reason rigorously rather than merely generating plausible-looking answers. Complementing this, \cite{yu2024p4z} proposed \texttt{CodePMP}, a scalable preference model pretraining pipeline that synthesizes millions of code-preference pairs from public source code. This approach significantly enhances the sample efficiency and robustness of reward models for mathematical and logical reasoning tasks by providing a vast source of verifiable, structured feedback, thereby reducing the prohibitive reliance on expensive human annotations.

For complex, multi-step reasoning, the sparse, sequence-level rewards typical of standard RLHF methods present a significant credit assignment problem: it is difficult to determine which specific intermediate step contributed to the final success or failure. To overcome this, researchers have developed innovative reward schemes that provide more fine-grained, token-level feedback. \cite{zhong2024wch} introduced Reinforced Token Optimization (RTO), which re-frames RLHF as a Markov Decision Process (MDP) and leverages Direct Preference Optimization (DPO) to derive token-wise reward signals. This provides PPO with dense, fine-grained feedback, significantly improving performance and data efficiency by allowing the model to learn from the granular correctness of each reasoning step. Similarly, \cite{li2024h19} proposed RED (REward reDistribution), a method that assigns token-level rewards by calculating the incremental impact of each token on the reward model's score. This approach seamlessly integrates into existing RLHF pipelines with minimal overhead, offering a practical way to densify rewards. Further, \cite{chan2024xig} explored Attention Based Credit (ABC), which aims to densify rewards by redistributing the final scalar reward using the reward model's internal attention maps, effectively offering a "dense reward for free" without additional computational cost. To address the challenge of long reasoning chains, \cite{chai2024qal} introduced MA-RLHF, which incorporates "macro actions"—sequences of tokens or higher-level language constructs—into the RLHF process. By reducing the temporal distance between actions and rewards, MA-RLHF accelerates learning efficiency and improves credit assignment for longer reasoning trajectories.

Beyond fine-grained rewards, integrating search algorithms has proven particularly effective for enabling self-correction and robust exploration in complex reasoning tasks. These methods allow LLMs to explore multiple solution paths and learn from their mistakes. \cite{zhang2024q0e} presented \texttt{LLaMA-Berry}, a framework designed for Olympiad-level mathematical reasoning. It combines Self-Refine Monte Carlo Tree Search (SR-MCTS) with a Pairwise Preference Reward Model (PPRM) trained using DPO. This integration allows for efficient exploration of vast solution spaces, where MCTS guides the generation of diverse reasoning trajectories, and the PPRM provides robust evaluation of multi-step solutions, enabling the model to learn from both successful and unsuccessful attempts. Building on this, \cite{chen20244ev} introduced Step-level Value Preference Optimization (SVPO) for mathematical reasoning. SVPO uses MCTS to autonomously generate step-level preferences and Q-values, providing explicit, fine-grained feedback that is then used to train both a policy and an explicit value model. This approach significantly enhances the model's ability to navigate complex reasoning paths by learning the value of intermediate steps, leading to more strategic and accurate problem-solving.

RL is also being applied to develop dynamic reasoning strategies and tackle complex, real-world problems. \cite{hao2025lc8} proposed RL-of-Thoughts (RLoT), an inference-time technique that trains a lightweight RL agent (a "navigator model") to dynamically generate task-adaptive logical structures for LLM reasoning. This navigator selects human cognition-inspired "logic blocks" (e.g., "Reason one step," "Decompose," "Refine") to adaptively steer the LLM's thought process. This method offers a flexible alternative to static Chain-of-Thought prompting, enabling smaller models to achieve reasoning capabilities comparable to much larger counterparts by optimizing the sequence of reasoning operations. In a practical application, \cite{wei2025v4d} introduced SWE-RL, an RL approach for advancing LLM reasoning in open software evolution tasks like bug fixing. By leveraging software evolution data and a lightweight rule-based reward function, SWE-RL demonstrated emergent generalized reasoning skills for bug diagnosis and repair, showcasing the potential of RL for complex, real-world cognitive tasks where functional correctness is paramount.

Finally, self-improvement and efficient exploration are critical for scaling RL to diverse and open-ended reasoning challenges, reducing the reliance on continuous external supervision. \cite{zhang2024lqf} proposed Self-Exploring Language Models (SELM), an online alignment algorithm that actively explores the response space using an optimistically biased objective. This approach prevents models from getting stuck in local optima by encouraging the discovery of novel, potentially better reasoning paths. \cite{chen2024vkb} introduced DICE, a method for bootstrapping DPO-tuned LLMs with their own implicit rewards for iterative self-alignment. This significantly reduces the need for external feedback and mitigates issues like length exploitation and catastrophic forgetting, allowing models to refine their reasoning capabilities autonomously. Complementing this, \cite{xia2024rab} developed Inverse-Q*, a framework for token-level RL that can align LLMs *without* preference data or explicit reward models, by directly estimating an optimal policy from single dialogue data. This is particularly valuable for reasoning tasks where explicit preferences for intermediate steps are difficult or impossible to obtain.

In conclusion, the application of reinforcement learning has profoundly advanced LLM reasoning, moving from general alignment to targeted enhancements in mathematical problem-solving, logical inference, and multi-step decision-making. Key developments include systematic comparisons of RL algorithms to identify optimal strategies, the creation of high-quality, verifiable datasets, the design of fine-grained reward schemes for improved credit assignment, and the integration of powerful search algorithms for self-correction and robust exploration. These innovations collectively push LLMs towards more strategic, verifiable, and adaptive reasoning. Despite this progress, significant challenges remain, particularly in developing more sophisticated exploration strategies for highly stochastic reasoning environments, ensuring the generalizability of these specialized techniques across an even broader spectrum of complex cognitive tasks, and theoretically understanding *how* RL induces robust reasoning rather than merely optimizing for surface-level correctness.