\subsection{Direct Preference Optimization (DPO) and Reward Model-Free Alignment}

The traditional Reinforcement Learning from Human Feedback (RLHF) pipeline, particularly when implemented with Proximal Policy Optimization (PPO), presents significant practical challenges due to its inherent complexity, training instability, and high computational demands \cite{zheng2023c98}. These issues arise from the multi-stage process involving the training of a separate reward model (RM) and the subsequent on-policy reinforcement learning required to fine-tune the language model. This complexity motivated the search for more streamlined and robust alignment methodologies, leading to the development of Direct Preference Optimization (DPO) as a pivotal advancement \cite{rafailov20239ck}.

DPO fundamentally simplifies the RLHF pipeline by reformulating the preference learning problem as a direct policy optimization, thereby eliminating the need for an explicit reward model and the intricate dynamics of traditional reinforcement learning algorithms. At its core, DPO leverages a reparameterization of the implicit reward model within the standard KL-constrained RLHF objective. This theoretical insight allows the optimal policy to be derived in a closed form, expressing the probability of preferring one response over another directly in terms of the policy being optimized and a reference policy \cite{rafailov20239ck}. The resulting DPO objective function is a simple binary cross-entropy loss, which transforms the complex RL problem into a stable, supervised-like fine-tuning process. This direct approach offers substantial advantages in terms of computational efficiency, training stability, and often matches or surpasses the performance of PPO-based RLHF across various alignment tasks, including sentiment control and summarization \cite{rafailov20239ck}. The theoretical foundation for such KL-regularized preference learning is further explored by \textcite{xiong2023klt}, who provide a rigorous analysis of the reverse-KL regularized contextual bandit formulation, underpinning the principled design of algorithms like DPO.

Building upon the concept of reward model-free alignment, other methodologies have emerged that pursue direct policy optimization from preference data. \textcite{tang2023lop} introduced Zeroth-Order Rank-SGD (ZO-RankSGD), a provably convergent zeroth-order optimization algorithm designed to learn directly from ranking oracles for general non-convex functions. This work demonstrated the broader feasibility of optimizing policies without explicit reward functions by directly interpreting comparative feedback. Extending these ideas to more general stochastic Markov Decision Processes (MDPs) with infinite state and action spaces, \textcite{zhang2024w99} proposed Zeroth-Order Policy Gradient (ZPG) and Zeroth-Order Block-Coordinate Policy Gradient (ZBCPG) for RLHF without requiring reward inference. Further solidifying the theoretical basis for model-free RLHF, \textcite{zhang20248x9} developed the Batched Sequential Action Dueling (BSAD) algorithm, which directly identifies the optimal policy with provable instance-dependent sample complexity, completely circumventing the need for reward model training. These approaches collectively underscore the growing interest and theoretical advancements in direct policy optimization from preferences, offering alternatives to DPO's specific reparameterization.

While DPO and similar direct optimization methods represent a significant step towards simplifying and stabilizing LLM alignment, the broader landscape of efficient alignment also includes approaches that reduce reliance on human feedback or streamline the overall pipeline, even if they don't entirely eliminate a reward model. For instance, Reinforced Self-Training (ReST) \cite{gulcehre2023hz8} employs an efficient offline RL framework that iteratively generates and filters high-quality data using a reward model. Although an RM is still present for filtering, ReST's iterative data generation and offline training reduce the need for complex online RL interactions. Similarly, Reinforcement Learning from AI Feedback (RLAIF) \cite{lee2023mrw} and Principle-Driven Self-Alignment \cite{sun20238m7} aim to scale alignment by leveraging AI-generated preferences or a small set of human-defined principles, respectively, thereby lessening the burden of expensive human annotation. These methods, while distinct from DPO's purely reward model-free formulation, share the common goal of making LLM alignment more scalable and accessible.

Despite their advantages in stability and efficiency, DPO and other direct alignment algorithms are not without their own set of challenges. \textcite{rafailov2024ohd} revealed that Direct Alignment Algorithms (DAAs), including DPO, are susceptible to reward over-optimization, a phenomenon where the policy's performance on true human preferences can degrade prematurely during training. This occurs because the implicit reward function, despite not being explicitly trained, can still be over-optimized, leading to policy collapse or a reduction in generalization, mirroring issues observed in traditional RLHF. This highlights that simplifying the pipeline does not inherently resolve all fundamental problems associated with optimizing a proxy objective. Furthermore, the preference objectives used in DPO, much like explicit reward models, can still be susceptible to learning superficial correlations from preference data, such as output length, rather than genuinely capturing deep human intent \cite{singhal2023egk}. This can lead to models that appear aligned but merely exploit shallow cues. Adding a critical perspective on other reward model-free approaches, \textcite{zhang2025d44} critically examined Reinforcement Learning from Internal Feedback (RLIF), a method that uses intrinsic model signals as reward proxies. They demonstrated that such approaches often primarily act as policy entropy regularizers, leading to initial performance gains in base LLMs but subsequent degradation, suggesting that not all internal signals are robust or sufficient for achieving deep, sustained alignment.

In summary, DPO and the broader class of reward model-free approaches represent a significant paradigm shift in LLM alignment, offering more stable, computationally efficient, and theoretically grounded alternatives to traditional RLHF. By simplifying the optimization landscape, they have democratized access to preference-based fine-tuning. However, the field continues to grapple with persistent challenges such as reward over-optimization, the potential for preference objectives to learn superficial biases, and the nuanced effectiveness of various internal feedback signals. These issues necessitate ongoing research into more robust preference learning techniques, principled regularization strategies, and a deeper understanding of the true underlying objectives in human preferences.