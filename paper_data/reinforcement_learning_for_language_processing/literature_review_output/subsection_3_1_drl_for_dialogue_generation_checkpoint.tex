\subsection*{DRL for Dialogue Generation}

The initial wave of dialogue system development largely relied on supervised learning, which, despite its efficacy in predicting next tokens or turn-level responses, inherently struggled to optimize for the holistic, long-term qualities crucial for engaging conversations, such as coherence, naturalness, and overall user satisfaction \cite{Li2016}. This limitation stemmed from issues like exposure bias, where models trained on ground-truth sequences diverge from optimal paths during self-generated inference, leading to generic, repetitive, or short-sighted responses. Deep Reinforcement Learning (DRL) emerged as a transformative paradigm, framing dialogue generation as a sequential decision-making process. This allowed agents to learn optimal policies by interacting with an environment (e.g., a simulated user) and optimizing for cumulative rewards over multiple turns, thereby directly addressing the shortcomings of purely supervised approaches. The conceptual foundation for optimizing non-differentiable, sequence-level metrics, crucial for DRL in language generation, was established in earlier works on structured prediction, as discussed in Section 2.

A seminal contribution to the application of DRL in dialogue generation was the work by \cite{Li2016}. This research pioneered the use of deep reinforcement learning to train neural dialogue agents, moving beyond simple next-utterance prediction to optimize for long-term conversational objectives. Their approach explicitly aimed to improve metrics such as ease of answering, information flow, and overall coherence, which are notoriously difficult to capture with conventional turn-level supervised losses. By framing dialogue as a Markov Decision Process, the authors employed policy gradient methods, specifically REINFORCE, to train an agent that could learn from interactions with a simulated user. The agent received sequence-level rewards designed to encourage desirable conversational traits, such as generating diverse and informative responses while avoiding repetition. This marked a significant departure from previous methods, demonstrating DRL's potential to mitigate exposure bias and foster more engaging, goal-oriented dialogues. However, this pioneering work, like many early applications of pure policy gradient methods, faced inherent challenges, including high variance during training and the significant difficulty of designing effective, non-sparse reward functions for complex, multi-turn interactions.

Following \cite{Li2016}, subsequent research further explored the application of DRL to dialogue, particularly focusing on refining reward mechanisms and improving training stability. Early efforts often distinguished between open-domain and task-oriented dialogue systems. In task-oriented settings, DRL was applied to learn optimal dialogue policies for achieving specific goals, such as booking flights or making reservations. Here, rewards could be more explicitly defined based on task success, user satisfaction (often simulated), and efficiency metrics like turn count \cite{Dhingra2017, Fatemi2016}. For instance, \cite{Dhingra2017} utilized DRL to learn dialogue policies for goal-oriented conversational agents, demonstrating improvements in task completion rates and user satisfaction by optimizing a policy network. These systems often leveraged simulated user models to provide a rich, interactive training environment, allowing agents to explore diverse dialogue strategies and learn from long-term consequences. In open-domain dialogue, where the goal is more about engaging and coherent conversation rather than task completion, reward design became more abstract and challenging. Researchers experimented with various forms of intrinsic and extrinsic rewards, including those based on diversity, informativeness, and avoiding repetition. Some approaches explored adversarial training, where a discriminator network provided a "naturalness" reward signal to the generator, pushing it to produce more human-like responses \cite{Li2017adversarial}. These methods aimed to overcome the limitations of simple n-gram based metrics and the difficulty of obtaining explicit human feedback for every interaction.

Despite these pioneering efforts, several fundamental challenges persisted in the early applications of DRL for dialogue generation. Reward sparsity remained a significant hurdle, as meaningful positive feedback often materialized only at the end of a multi-turn conversation, making it difficult for agents to attribute credit to individual actions. The vast, discrete action space of language also exacerbated the exploration-exploitation dilemma, making it computationally expensive for agents to discover optimal dialogue policies efficiently. Furthermore, the high variance associated with pure policy gradient methods often led to unstable training. These challenges underscored the critical need for more robust reward engineering and efficient learning from feedback. While initial DRL for dialogue often relied on hand-crafted or simple simulated rewards, the field progressively recognized the importance of more nuanced feedback mechanisms. For instance, the difficulty in defining explicit, non-sparse rewards for long, multi-turn interactions directly motivated later advancements in learning from conversation-level preferences. \cite{shani202491k}, for example, formalizes multi-turn preference optimization, where feedback is provided on *entire conversational trajectories* rather than individual turns. This approach, while a more recent development in the RLHF paradigm (as discussed in Section 4), directly addresses the long-standing problem of reward sparsity and the need to optimize for long-term conversational quality that was first highlighted in pioneering DRL for dialogue. Similarly, as reward models became more sophisticated, the challenge of "reward model overoptimization" emerged, where maximizing a proxy reward could lead to a decrease in actual human-judged quality. This problem, exemplified by work like \cite{moskovitz2023slz} in dialogue generation, highlights the continuous complexity of aligning learned rewards with true human preferences, a challenge that traces its roots back to the initial difficulties of designing effective reward functions for DRL-driven dialogue agents.

In summary, the pioneering applications of deep reinforcement learning to dialogue generation marked a crucial shift from turn-level accuracy to optimizing for long-term conversational quality and user satisfaction. By framing dialogue as a sequential decision-making problem, DRL enabled agents to learn adaptive policies from interactive feedback, moving beyond the generic responses of purely supervised models. While initial methods faced significant hurdles related to reward design, sparsity, and training stability, these early explorations laid the groundwork for future advancements in more robust feedback mechanisms and sophisticated policy optimization, continually pushing towards more engaging, coherent, and goal-oriented interactive language systems.