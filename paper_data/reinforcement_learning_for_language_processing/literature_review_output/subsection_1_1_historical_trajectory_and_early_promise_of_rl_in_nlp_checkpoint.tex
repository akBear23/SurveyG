The application of reinforcement learning (RL) principles to natural language processing (NLP) emerged from a fundamental challenge inherent in traditional supervised learning: the limitations of maximum likelihood estimation (MLE) and its inability to directly optimize non-differentiable, task-specific evaluation metrics. Supervised models, while effective for token-level prediction, often suffered from "exposure bias" during sequence generation. This phenomenon occurs when a model, trained on ground-truth sequences, is forced to generate tokens conditioned on its *own* potentially erroneous previous outputs during inference, leading to a compounding of errors and a drift from the desired output distribution \cite{Ranzato2016}. Such models frequently produced generic, repetitive, or grammatically plausible but semantically uninspired outputs, failing to capture the nuanced, holistic quality desired in human language.

This critical limitation motivated a paradigm shift: viewing many NLP tasks, particularly those involving structured outputs or generative sequences, as sequential decision-making processes. In this framework, an "agent" learns to construct an output by making a series of choices (e.g., selecting the next word or structural element), receiving delayed rewards based on the quality of the complete structure or generated sequence, rather than just local token-level accuracy. This ability to learn from delayed, global rewards offered a distinct advantage over MLE, which optimizes local, independent probabilities and struggles to account for long-term dependencies and overall output quality.

Early conceptualizations of RL in NLP primarily focused on structured prediction tasks such as parsing, semantic role labeling, and sequence tagging. Here, the objective was to optimize for global structural correctness, often measured by non-differentiable metrics like F1-score or exact match, rather than just individual token or constituent accuracy. A crucial intellectual bridge in this era was the "learning to search" (L2S) framework, exemplified by approaches like SEARN \cite{DaumeIII2009} and DAgger \cite{Ross2011}. These methods elegantly reframed complex structured prediction as a series of local decisions made by a learned policy, often leveraging imitation learning to guide the search process. While not pure RL in the contemporary sense, L2S provided a robust conceptual and algorithmic foundation for sequential decision-making in NLP, demonstrating how a policy could learn to navigate a complex output space to optimize for a global objective, thereby laying the groundwork for more direct RL applications. This perspective highlighted the sequential nature of constructing linguistic outputs and the need for global optimization beyond local token-level accuracy, a theme that would become central to later Deep Reinforcement Learning (DRL) applications.

As neural networks and deep learning gained prominence in the mid-2010s, the principles of RL were extended to deep models, giving rise to DRL for direct language generation. This era, preceding the widespread adoption of large language models (LLMs), saw initial demonstrations of DRL's potential to address the limitations of MLE in generative tasks such as dialogue generation, abstractive summarization, and neural machine translation. The core idea was to train neural sequence-to-sequence models to directly optimize for human-aligned, non-differentiable metrics (e.g., ROUGE for summarization, BLEU for translation, or task-completion rates for dialogue) by treating the generation process as a Markov Decision Process. Policy gradient methods, such as REINFORCE, were adapted to train recurrent neural networks to generate sequences by maximizing a reward signal derived from these holistic metrics \cite{Ranzato2016}. This allowed models to learn from the consequences of their entire generated sequence, rather than just the likelihood of individual tokens, leading to outputs that were often more fluent, coherent, and aligned with overall task objectives than their MLE-trained counterparts.

Despite this early promise, the application of DRL to NLP presented significant challenges that limited its widespread adoption before the LLM era. The high variance inherent in policy gradient methods, especially when operating in the immense discrete action spaces of natural language (i.e., the vocabulary size, often tens of thousands of tokens), frequently led to unstable and inefficient training. Furthermore, the problem of sparse rewards meant that meaningful feedback was often only available at the end of a long sequence, making credit assignment difficult across numerous sequential decisions. Designing effective, non-brittle reward functions for subjective qualities like fluency, coherence, or relevance proved to be a labor-intensive and often domain-specific engineering endeavor, requiring careful tuning and often proxy metrics that did not perfectly align with human judgment. The inherent sample inefficiency of RL algorithms also posed a significant hurdle, requiring extensive interaction with the environment (or a carefully constructed simulator) to learn an effective policy, a costly process for complex language tasks. These persistent issues—training instability, reward engineering complexity, and sample inefficiency exacerbated by the unique characteristics of language—underscored the need for more robust algorithms and sophisticated feedback mechanisms. These unresolved challenges ultimately set the stage for the next wave of RL innovations in NLP, particularly the development of more stable DRL algorithms and, crucially, the integration of RL with human feedback for aligning large language models.