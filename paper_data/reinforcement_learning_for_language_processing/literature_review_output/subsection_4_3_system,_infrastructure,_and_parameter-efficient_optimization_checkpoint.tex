\subsection{System, Infrastructure, and Parameter-Efficient Optimization}
The burgeoning scale of Large Language Models (LLMs) and the computational demands of Reinforcement Learning from Human Feedback (RLHF) necessitate robust engineering solutions to enhance efficiency, flexibility, and scalability. This subsection explores innovations in distributed frameworks, parameter-efficient fine-tuning (PEFT) methods, and efficient sampling strategies that are critical for making advanced alignment techniques practical for deployment.

Addressing the inherent inefficiency and inflexibility of distributed RLHF training, \cite{sheng2024sf5} introduced HybridFlow, a novel framework that combines single-controller and multi-controller paradigms. This hybrid approach, coupled with a 3D-HybridEngine for optimized actor model transitions and an intelligent GPU allocation algorithm, achieves significant throughput improvements ranging from 1.53x to 20.57x. Complementing this, \cite{mei2024eqt} proposed ReaL, a system that dynamically reallocates model parameters and parallelization strategies at a fine-grained level. By employing an MCMC search to discover optimal execution plans, ReaL yields substantial speedups of up to 3.58x, further optimizing resource utilization for complex RLHF workloads.

To mitigate the prohibitive memory usage and training time associated with fine-tuning billions of parameters, parameter-efficient fine-tuning (PEFT) methods have emerged as a cornerstone of efficient RLHF. \cite{sidahmed2024ikf} demonstrated Parameter-Efficient RLHF (PE-RLHF) by integrating Low-Rank Adaptation (LoRA) into both reward model training and policy optimization phases. This approach, which freezes the main model backbone and updates only small LoRA adapters, achieved comparable performance to full fine-tuning while reducing memory usage by 20-57\% and accelerating training by 1.05x-1.9x. In a different vein, \cite{zhu2024zs2} introduced Proxy-RLHF, which decouples the LLM's generation task from the alignment task by training a small, lightweight "proxy model" to oversee and guide generation. This innovative architecture achieves comparable alignment performance with less than 1\% of the trainable parameters required by traditional RLHF or DPO. Beyond explicit PEFT, methods that simplify the overall RLHF pipeline also contribute to efficiency. Direct Preference Optimization (DPO) by \cite{rafailov20239ck} streamlines the alignment process by directly optimizing the policy with a classification loss, eliminating the need for a separate reward model and complex reinforcement learning steps. Similarly, \cite{zheng2024voy} demonstrated that bypassing the Supervised Fine-Tuning (SFT) phase entirely and directly applying RLHF to base models can preserve general capabilities and enhance conversational skills, simplifying the pipeline. Further reducing complexity, zeroth-order optimization methods like ZO-RankSGD \cite{tang2023lop}, ZPG/ZBCPG \cite{zhang2024w99}, and BSAD \cite{zhang20248x9} aim to bypass reward model inference altogether. These approaches directly optimize policies from human preferences, offering provably efficient alternatives for general stochastic Markov Decision Processes (MDPs) and contributing to a more streamlined and efficient RLHF pipeline.

Finally, efficient sampling strategies are crucial for the computational feasibility of applying RL to large-scale sequence generation models. \cite{wang2023v62} proposed EfÔ¨Åcient Sampling-based RL (ESRL), which tackles the high memory and time costs of exploration through a two-stage sampling framework that decouples sequence sampling from gradient computation, and a dynamic sampling mechanism that adaptively adjusts sampling size and temperature. This resulted in memory reductions of 47\% and training time reductions of 39\% for machine translation tasks. Building on offline RL principles, \cite{gulcehre2023hz8} introduced Reinforced Self-Training (ReST), an iterative approach that decouples data generation from policy improvement. ReST continuously generates new, high-quality data from an improving policy and filters it based on a reward model, leading to compute and sample-efficient policy refinement. Additionally, \cite{cao2024lh3} enhanced sample efficiency by introducing the RELC framework, which leverages LLM critique to generate dense, intermediate-step intrinsic rewards. This method effectively overcomes the sparse reward problem in text generation, leading to more efficient and stable learning.

In conclusion, the relentless pursuit of efficiency and scalability in RLHF for LLMs has led to significant advancements across system architecture, parameter optimization, and sampling strategies. While these innovations have drastically reduced computational burdens and increased flexibility, the challenge remains in integrating these diverse solutions into a cohesive, universally applicable framework that balances efficiency with robust performance and ethical alignment across increasingly complex tasks and model scales.