\subsection*{Core RLHF Paradigm: Reward Modeling and Policy Optimization}

The evolution of large language models (LLMs) from sophisticated text generators to capable instruction followers and value-aligned agents marks a pivotal shift, largely driven by the Reinforcement Learning from Human Feedback (RLHF) paradigm. Prior deep reinforcement learning (DRL) methods for sequence generation, while innovative in optimizing non-differentiable metrics, often grappled with the inherent challenge of designing effective, hand-crafted reward functions that truly captured the nuanced, subjective aspects of human quality and preferences \cite{Li2016, Paulus2017, Wang2018}. These proxy rewards frequently failed to align model outputs with complex human judgments, leading to outputs that were grammatically correct but semantically or pragmatically misaligned.

The RLHF paradigm fundamentally addressed these limitations by introducing a multi-stage process that learns directly from human preferences. This approach, initially explored in foundational works such as \cite{Ziegler2019} and \cite{Stiennon2020}, begins with the collection of human preference data, typically through pairwise comparisons of different model outputs for a given prompt. For instance, annotators might be asked to select which of two generated summaries is better, or which dialogue response is more helpful. This human preference data is then used to train a separate reward model (RM), often a neural network itself, to predict human preferences. The reward model learns to assign a scalar score to any given text sequence, effectively encoding the complex and subjective aspects of human judgment that were previously intractable to define programmatically \cite{Ziegler2019, Stiennon2020}. This step is crucial as it transforms the qualitative, comparative human feedback into a quantitative, differentiable reward signal, thereby overcoming the bottleneck of hand-crafted or proxy rewards.

Following the training of the reward model, the second stage involves policy optimization. Here, the pre-trained language model, which serves as the policy, is fine-tuned using reinforcement learning algorithms, most commonly Proximal Policy Optimization (PPO) \cite{Schulman2017}. The reward model, now acting as the objective function, provides a scalar reward for the language model's generated outputs. The language model is then updated to maximize this learned reward, effectively learning to generate text that is highly rated by the reward model, and by extension, aligned with human preferences \cite{Ziegler2019, Stiennon2020}. This iterative process allows the LLM to refine its behavior, moving beyond mere statistical text generation to actively produce responses that are helpful, harmless, and honest, as demonstrated by the significant advancements in models like InstructGPT and its successor ChatGPT \cite{Ouyang2022}. The success of this approach is evident in its ability to enable LLMs to follow complex instructions and align with human values, a capability largely absent in earlier models that relied solely on pre-training and supervised fine-tuning. For example, GPT-3.5's ability to provide appropriate responses to diverse user queries was largely attributed to this RLHF finetuning process \cite{thirunavukarasu2023enj}.

This paradigm shift has not only enhanced the instruction-following capabilities of LLMs but has also opened avenues for addressing more complex alignment challenges. However, the RLHF paradigm is not without its limitations. The quality and scalability of human preference data remain critical concerns, as biased or insufficient data can lead to a reward model that misrepresents true human preferences, potentially resulting in "reward hacking" where the model optimizes for the reward signal without truly achieving the desired behavior. The persistent issue of hallucinations, where LLMs generate factually incorrect or nonsensical information, continues to be a challenge even with RLHF, as highlighted by recent analyses of models like ChatGPT and GPT-4 \cite{thirunavukarasu2023enj}. Recent efforts, such as those in autonomous driving, have begun to integrate RLHF with multimodal LLMs, leveraging iterative refinement loops and symmetric performance metrics (e.g., BLEU, ROUGE) to explicitly target hallucination reduction and optimize computational efficiency \cite{wang2024lje}. This suggests an ongoing evolution where RLHF is combined with other techniques to enhance robustness and address specific failure modes. Despite these advancements, ensuring robust, scalable, and truly aligned behavior across diverse and complex contexts remains a key area of ongoing research, particularly as LLMs are deployed in safety-critical applications.