\subsection{Actor-Critic Methods and Exposure Bias Mitigation}

The application of Deep Reinforcement Learning (DRL) to sequence generation tasks, particularly in natural language processing, has been driven by the need to overcome fundamental limitations of traditional supervised learning, most notably the "exposure bias" problem and the challenge of optimizing for non-differentiable, sequence-level metrics. Exposure bias arises when models are trained on ground truth sequences using maximum likelihood estimation (MLE) but are then exposed to their own generated tokens during inference, leading to a compounding of errors and degraded performance \cite{li2016deep}. DRL offers a paradigm shift by framing sequence generation as a sequential decision-making process, where an agent learns a policy to generate tokens that maximize a long-term, task-specific reward.

Early efforts to apply reinforcement learning to structured prediction and sequence generation, such as the work by \cite{ranzato2007video}, demonstrated the potential of policy gradient methods like REINFORCE to optimize for sequence-level objectives. Similarly, \cite{li2016deep} pioneered the use of policy gradient DRL for dialogue generation, explicitly highlighting the pervasive exposure bias problem in this context. \cite{ranzato2016sequence} further applied policy gradient methods to sequence-to-sequence learning, aiming to directly optimize non-differentiable metrics and mitigate exposure bias. While these pure policy gradient approaches offered a direct way to optimize for desired outcomes, they often suffered from high variance during training, making learning unstable and sample-inefficient, as detailed in foundational works on policy gradient methods with function approximation \cite{sutton2000policy}. The conceptual framework of "learning to search" \cite{daume2009search, chang2015structured} also provided a precursor, framing structured prediction as a sequence of local decisions, which DRL later leveraged for more complex generative tasks.

To address the high variance inherent in pure policy gradient methods and enhance training stability, actor-critic algorithms emerged as a crucial advancement. These methods combine a policy network (the "actor") that selects actions (tokens) with a value network (the "critic") that estimates the expected return of states or state-action pairs, thereby providing a lower-variance baseline for the policy gradient. \cite{bahdanau2017actor} introduced an actor-critic algorithm specifically designed for sequence prediction, demonstrating its effectiveness in reducing variance and improving the stability of DRL training for tasks like abstractive summarization. This approach allowed for more efficient learning by leveraging the critic's estimate to guide the actor's updates. Following this, \cite{paulus2017deep} successfully applied an actor-critic model to abstractive summarization, optimizing directly for ROUGE scores and further showcasing the method's ability to mitigate exposure bias by training on the actual evaluation metric rather than token-level accuracy. Other works, such as \cite{wang2018reinforcement}, also explored DRL with diversified rewards for neural machine translation, contributing to the broader effort of using RL to optimize for specific generation qualities beyond MLE.

The evolution of DRL for language generation also saw the development of various strategies to explicitly mitigate exposure bias. Beyond the inherent benefit of optimizing sequence-level rewards, methods often combined supervised pre-training with DRL fine-tuning, allowing the model to first learn basic generation capabilities from ground truth before refining its policy with RL to handle its own generated outputs. This hybrid approach, often employing actor-critic variants, became a standard practice. Comprehensive surveys such as \cite{gao2019drl, zhu2020drl, liu2022drl} consistently highlight exposure bias as a central challenge in DRL for natural language generation and extensively review the role of actor-critic methods and other DRL strategies in addressing it.

More recently, the principles of actor-critic methods have been scaled and refined in the context of large language models (LLMs) through Reinforcement Learning from Human Feedback (RLHF). Pioneering works like \cite{ziegler2019fine} and \cite{stiennon2020learning} introduced the concept of training a reward model from human preferences, which then serves as the critic to fine-tune the LLM (the actor) using policy optimization algorithms like Proximal Policy Optimization (PPO), an advanced actor-critic variant. This paradigm, famously scaled in \cite{ouyang2022training} to create InstructGPT, represents a sophisticated approach to aligning LLMs with complex human instructions and values. By optimizing for human-preferred outcomes, RLHF inherently addresses a form of exposure bias, where the model learns to generate outputs that are not just grammatically correct but also helpful, harmless, and honest, even when exposed to its own generated content during the iterative refinement process \cite{thirunavukarasu2023enj, yu2023xc4, sahoo2024dp6}. This demonstrates a significant leap in using actor-critic principles to mitigate the mismatch between training objectives and desired generation quality. Furthermore, preference-based RL, as seen in \cite{holk20243vd}, leverages LLMs to reason from human text prompts, further refining reward learning and policy shaping.

In conclusion, the journey from pure policy gradient methods to sophisticated actor-critic algorithms and RLHF has been pivotal in making DRL practical for complex language generation tasks. Actor-critic methods significantly reduced variance and improved training stability, directly addressing a key limitation of earlier DRL applications. This, coupled with direct optimization of sequence-level rewards, has been instrumental in mitigating exposure bias. However, challenges persist, including the complexity of designing effective and generalizable reward functions, the sample inefficiency inherent in RL, and ensuring true alignment with nuanced human preferences in large-scale RLHF applications. Future research continues to explore more robust and efficient ways to leverage these methods for high-quality and reliable language generation.