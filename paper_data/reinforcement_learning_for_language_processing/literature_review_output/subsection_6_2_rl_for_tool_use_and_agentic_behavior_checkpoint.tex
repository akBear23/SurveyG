\subsection{RL for Tool Use and Agentic Behavior}

Reinforcement learning (RL) is pivotal in transforming Large Language Models (LLMs) from static text generators into dynamic, interactive agents capable of effectively using external tools and exhibiting sophisticated agentic behaviors. This involves learning to orchestrate diverse tools, stabilizing multi-turn interactions, and adaptively planning actions in complex environments, often optimizing for conversation-level preferences.

A foundational challenge in building truly interactive and goal-oriented AI agents is managing multi-turn interactions and aligning them with human preferences over extended dialogues. \cite{shani202491k} addresses this by introducing Multi-turn Preference Optimization (MTPO), a novel policy optimization algorithm for multi-turn interactions. This work formalizes multi-turn preference-based RL within a Contextual Markov Decision Process (CMDP) framework, crucially utilizing *conversation-level* preference feedback rather than single-turn feedback, and proposing a novel preference-based Q-function that considers long-term consequences. MTPO provides the first theoretically grounded policy optimization algorithm with convergence guarantees for general multi-turn preference-based RL, enabling models to stabilize complex sequential interactions.

Building on the ability to manage multi-turn interactions, RL further empowers LLMs to interact with and orchestrate external tools. In the visual domain, \cite{su20257nq} presents \texttt{OpenThinkIMG}, a comprehensive framework that enables Large Vision-Language Models (LVLMs) to dynamically and adaptively utilize external visual tools. Their \texttt{V-TOOLRL} framework, which employs Group-wise Proximal Policy Optimization (GRPO), allows LVLMs to learn adaptive tool invocation policies, moving beyond the limitations of static supervised fine-tuning (SFT) by optimizing directly for task success using feedback. This approach significantly enhances an agent's ability to "think with images" and orchestrate diverse visual tools for complex problem-solving. While \cite{su20257nq} focuses on orchestrating existing tools, the challenge remains for models to generate and refine complex outputs through iterative tool use.

Addressing this, \cite{chen20253a4} introduces the Reasoning-Rendering-Visual-Feedback (RRVF) framework, which enables Multimodal Large Language Models (MLLMs) to learn complex generative tasks, such as image-to-code generation, solely from raw images without relying on expensive image-text supervision. RRVF leverages the "Asymmetry of Verification" principle, where verifying a rendered output against an image is easier than generating the code from scratch. The framework employs a closed-loop iterative process where the MLLM generates code, external tools render it, and a "Visual Judge" MLLM provides natural language feedback, all optimized end-to-end using GRPO with a hybrid reward function. This demonstrates a sophisticated form of agentic behavior through self-correction and iterative refinement, allowing models to adaptively plan actions to achieve a desired visual outcome. Both \cite{su20257nq} and \cite{chen20253a4} highlight the effectiveness of RL in enabling tool use, but they also implicitly rely on powerful external LLMs (like GPT-4o) for either initial action planning or reward signal generation, pointing to a bootstrapping dependency.

Beyond mere tool invocation, RL also facilitates LLMs in exhibiting more complex, strategic agentic behaviors, including interactions with other systems. \cite{ning2024rhw} explores this by presenting \texttt{CheatAgent}, a novel framework where LLMs act as intelligent, black-box adversarial agents to attack LLM-empowered recommender systems. Unlike traditional RL agents that struggle with complex textual inputs, \texttt{CheatAgent} leverages LLMs' inherent language comprehension and open-world knowledge. It optimizes an auxiliary LLM's "policy" (represented by a trainable prefix prompt) through self-reflection based on the victim system's loss, akin to policy gradient methods. This work demonstrates a distinct form of agentic behavior where LLMs strategically interact with their environment and other systems, moving beyond predefined tool use to exhibit adaptive planning for specific goals, even adversarial ones.

In conclusion, RL is instrumental in advancing LLMs beyond static text generation towards dynamic, interactive, and goal-oriented AI agents. Significant progress has been made in stabilizing multi-turn interactions and optimizing for conversation-level preferences through methods like MTPO \cite{shani202491k}. Furthermore, RL has enabled sophisticated tool orchestration, as seen in visual domains with \texttt{OpenThinkIMG} \cite{su20257nq} and iterative self-correction for generative tasks with RRVF \cite{chen20253a4}. The emergence of LLMs as strategic agents, exemplified by \texttt{CheatAgent} \cite{ning2024rhw}, underscores the breadth of agentic behaviors RL can foster. Unresolved challenges include bridging theoretical guarantees with practical deep RL implementations, reducing reliance on powerful external models for feedback or data generation, and further exploring the ethical implications of increasingly autonomous and strategic AI agents.