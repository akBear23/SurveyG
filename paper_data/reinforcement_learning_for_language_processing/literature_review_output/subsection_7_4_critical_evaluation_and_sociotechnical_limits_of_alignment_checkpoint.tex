\subsection*{Critical Evaluation and Sociotechnical Limits of Alignment}

Despite the remarkable successes of Reinforcement Learning from Human Feedback (RLHF) in aligning Large Language Models (LLMs) with human preferences, a growing body of literature reveals inherent limitations, biases, and unintended consequences that challenge the very foundations of current alignment paradigms. This critical examination extends beyond purely technical performance, delving into the broader sociotechnical implications of AI safety.

One significant limitation of RLHF is the trade-off between generalization and diversity. \textcite{kirk20230it} conducted a systematic analysis, demonstrating that while RLHF significantly improves out-of-distribution (OOD) generalization, it simultaneously leads to a substantial reduction in output diversity. Building on this observation, \textcite{mohammadi20241pk} provided a mechanistic explanation for this loss of diversity, specifically focusing on creativity. Their work empirically showed that aligned models gravitate towards distinct, limited "attractor states" in their output embedding space, exhibiting behavior akin to mode collapse and transforming LLMs into more deterministic algorithms, thereby sacrificing creative potential for consistency.

Beyond these inherent trade-offs, RLHF is susceptible to various biases and vulnerabilities. \textcite{saito2023zs7} identified a significant "verbosity bias" where LLMs, when used as evaluators in Reinforcement Learning from AI Feedback (RLAIF), tend to prefer longer, more verbose answers even if their quality is similar to shorter ones, highlighting a discrepancy with human preferences for conciseness. Furthermore, the integrity of RLHF can be compromised by malicious attacks. \textcite{baumgrtner2024gu4} demonstrated that RLHF is highly vulnerable to stealthy "preference poisoning" attacks, where a small fraction of naturalistic, poisoned preference data can manipulate the model to generate specific entities with desired sentiments, posing significant security risks.

A fundamental conceptual challenge underlying many of these issues is the "objective mismatch" problem. \textcite{lambert2023c8q} formalized this problem, arguing that the decoupling of numerical objectives across reward model training, policy optimization, and downstream evaluation leads to unintended behaviors such as excessive refusals, "laziness," or verbosity. This mismatch arises from erroneous assumptions about the correlation between proxy rewards and true human intent. Reinforcing this, \textcite{lambert2023bty} provided a critical historical and conceptual analysis of RLHF reward models, highlighting the opacity of their value encoding process and the risks of applying optimization stacks designed for clear control problems to the vague domain of human language and values. They argue that the foundational assumptions—that human preferences are quantifiable and that reward maximization leads to desired behaviors—are often ill-posed in complex ethical contexts.

The impact of RLHF can also extend to the very internal reasoning mechanisms of LLMs. Surprisingly, \textcite{bao2024wnc} employed a causal inference framework to diagnose LLM reasoning with Chain-of-Thought (CoT) and found that while in-context learning strengthens ideal causal reasoning structures, post-training methods like Supervised Fine-Tuning (SFT) and RLHF can actually *weaken* them. This suggests a potential trade-off where optimizing for external metrics through RLHF might inadvertently degrade the underlying causal fidelity indicative of genuine reasoning, challenging the assumption of universal benefits from alignment.

Ultimately, these technical and conceptual limitations point to broader sociotechnical limits of AI alignment. \textcite{lindstrm20253o2} offers a comprehensive critique, questioning the sufficiency of the widely adopted "helpful, harmless, honest" (HHH) principles. They argue that the operationalization of HHH often oversimplifies complex ethical considerations and can even tolerate harm. By applying "the curse of flexibility" from system safety literature, they explain that the generalist nature and immense power of LLMs make it inherently difficult to define and ensure safety requirements through purely model-centric technical design. This work advocates for a paradigm shift, proposing that AI safety must be approached as a comprehensive sociotechnical discipline, integrating ethical, institutional, and process considerations alongside purely technical design, acknowledging that alignment is not solely a technical problem solvable within algorithmic boundaries.

In conclusion, while RLHF has been instrumental in advancing LLM capabilities, a critical evaluation reveals fundamental trade-offs between generalization and diversity, a loss of creativity, susceptibility to biases and attacks, and a pervasive "objective mismatch" problem. Furthermore, RLHF may inadvertently compromise the internal reasoning fidelity of models. These issues, coupled with the inherent vagueness of ethical principles, underscore that AI alignment is a complex sociotechnical challenge that cannot be resolved through technical design alone, necessitating a multidisciplinary approach that integrates broader ethical, institutional, and process considerations.