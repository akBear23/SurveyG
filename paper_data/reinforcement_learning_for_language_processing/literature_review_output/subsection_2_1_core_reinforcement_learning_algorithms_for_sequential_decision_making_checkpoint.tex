\subsection{Core Reinforcement Learning Algorithms for Sequential Decision Making}

The inherent sequential nature of language generation, where each token choice influences subsequent possibilities and the overall quality of the generated text, necessitates a decision-making framework capable of optimizing for long-term, cumulative rewards. Reinforcement Learning (RL) provides this framework, training an agent to learn a policy that directly maps states to actions to maximize expected future returns. This section introduces the foundational RL algorithms—policy gradient methods and actor-critic architectures—that form the bedrock for sequential decision-making in language processing, laying the groundwork for more advanced applications.

Policy gradient methods represent a fundamental class of RL algorithms that directly optimize the parameters of a stochastic policy, $\pi_\theta(a|s)$, which defines the probability of taking action $a$ in state $s$. The core idea is to estimate the gradient of the expected return with respect to the policy parameters $\theta$ and update $\theta$ in the direction of increasing return. The seminal REINFORCE algorithm, introduced by \textcite{Williams1992}, is a Monte Carlo policy gradient method that estimates this gradient by sampling complete trajectories. For each trajectory, the policy parameters are updated proportionally to the product of the gradient of the log-probability of the taken actions and the total return received from that point onward. Intuitively, the log-derivative trick, $\nabla_\theta \log \pi_\theta(a|s) = \frac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)}$, allows the gradient of the expected return to be expressed as an expectation over the policy's actions. This means that actions leading to high returns are reinforced (made more probable), while actions leading to low returns are discouraged. Mathematically, the update for a single trajectory $\tau = (s_0, a_0, r_1, s_1, \dots, s_{T-1}, a_{T-1}, r_T)$ is given by $\Delta\theta \propto \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) G_t$, where $G_t$ is the cumulative discounted return from time step $t$. This direct optimization approach is particularly advantageous for discrete action spaces, characteristic of token generation in language, and for optimizing non-differentiable, sequence-level rewards, which are common in NLP evaluation metrics. \textcite{Sutton2000} further formalized and generalized policy gradient methods with function approximation, laying the theoretical groundwork for their integration with deep neural networks. A significant challenge of pure policy gradient methods like REINFORCE, however, is the high variance of their gradient estimates, which can lead to unstable and inefficient training, especially in complex environments with long horizons. This high variance stems from the reliance on full trajectory returns ($G_t$), which can fluctuate significantly between samples.

To address the high variance inherent in pure policy gradient methods, actor-critic architectures emerged as a more stable and efficient alternative. These methods combine the strengths of both policy-based (actor) and value-based (critic) RL. The "actor" is responsible for learning the policy, i.e., how to choose actions, while the "critic" learns a value function (e.g., state-value function $V(s)$ or state-action value function $Q(s,a)$) to estimate the expected future reward from a given state or state-action pair. A key innovation of actor-critic methods is the use of bootstrapping, where the critic learns its value estimates by updating them based on other learned value estimates, rather than waiting for full episode returns. This allows for continuous learning and often faster convergence compared to Monte Carlo methods. The critic's value estimates serve as a learned baseline for the actor's policy updates. By subtracting a baseline (typically the state-value function $V(s)$) from the return $G_t$ to form an advantage estimate $A_t = G_t - V(s_t)$, the variance of the policy gradient estimate is significantly reduced without changing its expectation. This is because the baseline term, being independent of the action taken, does not affect the expected gradient but effectively normalizes the reward signal, leading to more stable learning. Early foundational work on actor-critic methods, such as those by \textcite{Barto1983} and further developed by \textcite{Konda1999}, established their theoretical underpinnings and practical benefits. The evolution of these methods saw the development of algorithms like Asynchronous Advantage Actor-Critic (A3C) and its synchronous variant A2C, which further improved stability and sample efficiency by leveraging parallel environments and more robust advantage estimation techniques. These advancements were crucial in making actor-critic methods practical for complex, high-dimensional problems, including those in language processing.

In summary, policy gradient methods, exemplified by REINFORCE, and the more stable actor-critic architectures, represent the fundamental algorithmic toolkit for training agents in sequential decision-making tasks critical to language processing. They enable direct optimization for long-term, non-differentiable rewards, addressing key limitations of supervised learning. While pure policy gradients offer conceptual simplicity, actor-critic methods provide a crucial advancement in stability and efficiency through variance reduction and bootstrapping. These foundational algorithms, continuously refined and adapted, remain indispensable for developing sophisticated and human-aligned language AI. The principles established here, particularly the actor-critic paradigm, were later extended and refined into powerful algorithms like Proximal Policy Optimization (PPO), which became central to modern large language model alignment through Reinforcement Learning from Human Feedback (RLHF), a topic that will be explored in detail in Section 4.