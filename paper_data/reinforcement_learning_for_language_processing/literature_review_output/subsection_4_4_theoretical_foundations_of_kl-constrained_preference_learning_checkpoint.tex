\subsection{Theoretical Foundations of KL-Constrained Preference Learning}

The alignment of large language models (LLMs) with human preferences, predominantly through Reinforcement Learning from Human Feedback (RLHF), necessitates robust theoretical foundations to ensure stability, performance, and a deeper understanding of the underlying optimization problem. A critical aspect of this theoretical grounding involves the use of Kullback-Leibler (KL) divergence constraints, which are instrumental in preventing catastrophic forgetting, maintaining output diversity, and bridging the gap between empirical success and theoretical robustness in LLM alignment.

A pivotal contribution to this field is the formalization of RLHF as a reverse-KL regularized contextual bandit problem by \cite{xiong2023klt}. This work provides the first rigorous theoretical analysis of this formulation, offering finite-sample guarantees for offline, online, and hybrid learning settings. By explicitly incorporating a KL-divergence constraint to keep the fine-tuned policy close to a reference model, \cite{xiong2023klt} ensures output diversity and fidelity, mitigating issues like reward hacking and the "alignment tax" prevalent in earlier empirical methods. Building upon this, \cite{zhao202532z} further strengthens the theoretical understanding of online KL-regularized reinforcement learning by achieving groundbreaking logarithmic regret bounds for both contextual bandits and Markov Decision Processes. This work provides the first theoretical justification for the superior sample efficiency observed in practical KL-regularized RLHF applications, demonstrating that such regularization fundamentally improves learning efficiency without requiring strong coverage assumptions.

While KL-regularization is crucial for stability, it can also introduce subtle algorithmic biases. \cite{xiao2024ro4} identifies "preference collapse" as an inherent algorithmic bias in standard KL-regularized RLHF, where the model disproportionately favors dominant preferences, even with an oracle reward model. To address this, they propose Preference Matching (PM) RLHF, which introduces a novel regularizer derived from an ordinary differential equation to provably align LLMs with the preference distribution of the reward model, thereby ensuring that the policy accurately reflects diverse human preferences and maintains output diversity.

The practical implementation of KL-constrained learning often involves iterative algorithms and efficient reward signal generation. \cite{gulcehre2023hz8} introduces Reinforced Self-Training (ReST), an iterative, decoupled approach that continuously generates new, higher-quality data from an improving policy and filters it based on a learned reward model. While ReST's core objective is not explicitly KL-constrained, the reward models it leverages are typically trained using preference data that implicitly or explicitly derive from KL-regularized objectives, highlighting the importance of efficient iterative learning. Extending this, \cite{chen2024vkb} proposes DICE, an iterative self-alignment method that bootstraps new preference data using the implicit reward model derived from a DPO-tuned LLM (which itself is based on a KL-regularized objective). This approach enables cost-effective iterative refinement, mitigating catastrophic forgetting and length bias without external feedback. Similarly, \cite{zhang2024lqf} introduces Self-Exploring Language Models (SELM), an online direct alignment algorithm that uses an optimistically biased, RM-free objective with a KL-divergence constraint to actively explore the response space, preventing models from getting stuck in local optima and promoting guided exploration.

The interplay between online and offline learning, and the robustness of KL-constrained methods, is also a significant area of theoretical inquiry. \cite{cen2024nef} presents Value-Incentivized Preference Optimization (VPO), a unified framework for online and offline RLHF that implicitly handles uncertainty (optimism for online, pessimism for offline) through value function regularization. VPO provides theoretical regret guarantees for both settings, matching standard RL rates while maintaining a DPO-like simplicity, thereby reinforcing the theoretical robustness of KL-constrained approaches in diverse data collection paradigms. Complementing this, \cite{tang2024wt3} empirically dissects the performance gap between online and offline alignment algorithms, emphasizing the pivotal role of on-policy sampling in achieving high generative quality, a challenge that offline KL-regularized methods must overcome. To address this, \cite{zhou202469n} proposes Weighted Preference Optimization (WPO), which enhances off-policy DPO by reweighting preference pairs based on their probability under the current policy, effectively simulating on-policy learning and improving the stability and efficiency of KL-regularized DPO. Furthermore, \cite{ji2024d5f} introduces Self-Play with Adversarial Critic (SPAC), a provable and scalable offline alignment method for LLMs. SPAC leverages a Stackelberg game formulation with an adversarial critic and "on-average pessimism" to achieve theoretical convergence guarantees under weak data assumptions, while its practical DPO-like implementation bridges the gap between theoretical robustness and computational scalability for KL-constrained objectives.

Beyond the core optimization, fine-grained control and reward redistribution are crucial for effective KL-constrained RLHF. \cite{zhong2024wch} introduces Reinforced Token Optimization (RTO), which reframes RLHF as a Markov Decision Process (MDP) with token-wise rewards. It innovatively uses DPO to extract these fine-grained token-level signals for subsequent PPO training, addressing the sparsity of rewards while operating within the context of KL-regularized objectives. Similarly, \cite{chan2024xig} proposes Attention Based Credit (ABC) and \cite{li2024h19} introduces RED (REward reDistribution), both aiming to generate dense, token-level reward signals from sparse, holistic feedback by leveraging the reward model's internal states or incremental score changes. These methods improve credit assignment and training stability in RLHF, providing a richer signal for the policy optimization that is often regularized by KL divergence.

While not explicitly KL-constrained, \cite{hejna2023vyy}'s Contrastive Preference Learning (CPL) offers an alternative theoretical paradigm by directly learning policies from regret-based preferences without traditional RL. Its foundation in Maximum Entropy RL, which often involves entropy regularization, is conceptually related to the principles underlying KL-constrained objectives, demonstrating a broader theoretical exploration of preference modeling beyond direct reward maximization.

In conclusion, the theoretical foundations of KL-constrained preference learning have significantly advanced, moving from initial empirical successes to rigorous mathematical formulations and provable algorithms. Works like \cite{xiong2023klt} and \cite{zhao202532z} provide the core theoretical understanding and guarantees for stability and efficiency. Subsequent research addresses critical challenges such as algorithmic bias \cite{xiao2024ro4}, iterative learning efficiency \cite{gulcehre2023hz8, chen2024vkb, zhang2024lqf}, the online-offline gap \cite{cen2024nef, tang2024wt3, zhou202469n, ji2024d5f}, and fine-grained reward assignment \cite{zhong2024wch, chan2024xig, li2024h19}. Despite these advancements, challenges remain in fully generalizing theoretical guarantees to the complex, non-linear dynamics of all LLM architectures, dynamically adapting KL coefficients to varying tasks, and ensuring robustness to imperfections in reward models. Future work will likely focus on these areas to further solidify the theoretical robustness and practical efficacy of KL-constrained alignment.