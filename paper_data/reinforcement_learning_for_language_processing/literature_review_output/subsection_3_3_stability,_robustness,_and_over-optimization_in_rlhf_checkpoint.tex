\subsection{Stability, Robustness, and Over-optimization in RLHF}

Reinforcement Learning from Human Feedback (RLHF) is pivotal for aligning large language models (LLMs) with human preferences, yet it grapples with critical challenges such as training instability, reward hacking, and over-optimization. These issues cause models to exploit imperfections in the reward function or training dynamics, leading to performance degradation and unintended behaviors rather than genuine alignment \cite{srivastava2025gfw, wang2024a3a}. Addressing these technical hurdles is crucial for building reliable and trustworthy alignment systems.

Early investigations into RLHF revealed its inherent instability and sensitivity to hyperparameters, particularly with Proximal Policy Optimization (PPO) \cite{zheng2023c98}. A pervasive problem identified across various alignment paradigms, including Direct Alignment Algorithms (DAAs) like Direct Preference Optimization (DPO) \cite{rafailov20239ck}, is reward over-optimization \cite{rafailov2024ohd}. This phenomenon occurs when models maximize a proxy reward function beyond a point where it accurately reflects true human preferences, leading to performance degradation. Diagnostics have shown that reward models can exploit shallow correlations, such as output length, to achieve high scores without genuine quality improvement \cite{singhal2023egk}. Further probing into the internal mechanisms of LLMs during RLHF revealed the "Energy Loss Phenomenon," where an excessive increase in energy loss in the final layer correlates with reward hacking and reduced contextual relevance \cite{miao2025ox0}.

To combat these issues, a range of mitigation strategies have emerged, often focusing on improving the reward model itself or regularizing the policy optimization process. Enhancing reward model robustness is a key direction; for instance, \cite{yu20249l0} proposed Critic-RM, a framework that leverages self-generated, high-quality critiques to improve reward model accuracy and interpretability without relying on stronger teacher LLMs. Complementing this, \cite{frick20248mv} introduced Preference Proxy Evaluations (PPE), a benchmark explicitly correlated with downstream RLHF performance, providing a more reliable way to evaluate reward models and accelerate their development.

Uncertainty quantification in reward models offers a powerful approach to prevent over-optimization. \cite{zhai20238xc} introduced Uncertainty-Penalized RLHF (UP-RLHF), which penalizes rewards based on the estimated uncertainty of a diverse ensemble of Low-Rank Adaptation (LoRA) reward models, thereby guiding the policy away from out-of-distribution (OOD) samples where the reward model is less reliable. Building on this, \cite{zhang2024esn} developed a more efficient approach using lightweight uncertainty estimation from a single reward model's last layer embeddings, coupled with Adversarial Policy Optimization (AdvPO) for distributionally robust optimization. This method aims to be less pessimistic than sample-wise penalization, leading to more effective policy improvement. Further, \cite{cen2024nef} proposed Value-Incentivized Preference Optimization (VPO), a unified framework for online and offline RLHF that implicitly incorporates optimism or pessimism through value function regularization, bypassing the intractable explicit confidence interval construction for LLMs.

Constrained reinforcement learning (CRL) and novel reward shaping methods also play a crucial role. \cite{moskovitz2023slz} addressed over-optimization in composite reward models by introducing constrained RLHF, which dynamically adapts component reward model influence using Lagrange multipliers to prevent exceeding "proxy points" where individual components become unreliable. \cite{dai2025ygq} proposed Behavior-Supported Policy Optimization (BSPO), which uses value regularization to restrict policy iteration to the in-distribution region of the reward model, effectively penalizing OOD responses without affecting in-distribution ones. In multi-task settings, \cite{xu20242yo} introduced Constrained Generative Policy Optimization (CGPO) with a "Mixture of Judges" (rule-based and LLM-based) to identify and mitigate reward hacking patterns, allowing for task-specific optimization strategies.

Reward shaping techniques aim to provide more stable and informative signals. \cite{fu2025hl3} proposed Preference As Reward (PAR), a novel shaping method that applies a sigmoid function to centered rewards, theoretically proven to stabilize critic training and minimize policy gradient variance. To address the sparse reward problem and improve credit assignment, \cite{li2024h19} introduced RED (REward reDistribution), which assigns token-level rewards by leveraging the incremental impact of each token on the reward model's score. Similarly, \cite{zhong2024wch} developed Reinforced Token Optimization (RTO) which uses DPO to derive token-wise reward signals for PPO, significantly improving performance and data efficiency. \cite{chan2024xig} presented Attention Based Credit (ABC), a method to densify rewards by redistributing the final scalar reward across tokens using the reward model's internal attention maps, offering "dense reward for free" without additional computation. Furthermore, \cite{chai2024qal} introduced MA-RLHF, which uses macro actions (sequences of tokens) to reduce the credit assignment problem, leading to faster convergence and substantial performance gains across various tasks.

Beyond direct reward optimization, addressing algorithmic biases and broader robustness concerns is vital. \cite{xiao2024ro4} identified an inherent algorithmic bias in KL-regularized RLHF that leads to "preference collapse," where minority preferences are disregarded, and proposed Preference Matching RLHF to provably align models with the preference distribution. Practical implementations like ChatGLM-RLHF \cite{hou2024tvy} offer solutions for large-scale training stability, length bias reduction (via bucket-based balancing), and catastrophic forgetting mitigation (by incorporating next-token-prediction loss). The "alignment tax," or degradation of foundational abilities, is also tackled by \cite{lu202435m} with Online Merging Optimizers, which dynamically blend RLHF gradients with SFT model information to boost rewards while mitigating forgetting. The theoretical foundations of KL-regularized RLHF have also been strengthened, with \cite{xiong2023klt} providing comprehensive theoretical analyses and \cite{zhao202532z} achieving the first logarithmic regret bounds for online settings, explaining the empirical efficiency of such methods.

The challenge of balancing conflicting objectives, such as helpfulness and safety, also presents a form of over-optimization, where models can become "over-safe" and refuse benign queries. \cite{tan2025lk0} proposed Equilibrate RLHF, which uses a fine-grained data-centric approach and adaptive message-wise alignment with gradient masking to balance these trade-offs. Similarly, \cite{zhang2024b6u} introduced Bi-Factorial Preference Optimization (BFPO), an efficient supervised learning framework that re-parameterizes a joint RLHF objective for safety and helpfulness. For diverse human preferences, \cite{boldi2024d0s} presented Pareto-Optimal Preference Learning (POPL), which leverages lexicase selection to learn a set of policies optimal for distinct hidden context groups without requiring explicit group labels, enhancing fairness and robustness. \cite{lai2024ifx} introduced ALaRM, which aligns LLMs via hierarchical reward modeling, integrating holistic and proactively selected aspect-specific rewards for more consistent and fine-grained supervision. Finally, \cite{wang20247pw} proposed ArmoRM with Mixture-of-Experts (MoE) scalarization for interpretable, multi-objective reward modeling, mitigating issues like verbosity bias and enabling steerable alignment.

Beyond training-time solutions, inference-time strategies also contribute to robustness. \cite{wang2024w7p} introduced InferAligner, an inference-time alignment method for harmlessness that uses cross-model guidance from a safety-aligned model and conditional intervention to modify activations, achieving robust safety without retraining. The field is also exploring self-alignment and tuning-free approaches to reduce reliance on costly and potentially biased human feedback. \cite{singla2024dom} proposed Dynamic Rewarding with Prompt Optimization (DRPO), a tuning-free, search-based framework that enables LLMs to iteratively self-improve and craft optimal alignment instructions at inference time. \cite{chen2024vkb} introduced DICE, an iterative self-alignment framework that bootstraps DPO-tuned LLMs using their own implicit reward models, addressing length bias and catastrophic forgetting without external feedback. Furthermore, \cite{zhang2024lqf} developed Self-Exploring Language Models (SELM), an online DPO variant with an optimistically biased objective to actively explore the response space, preventing models from getting stuck in local optima. \cite{xia2024rab} presented Inverse-Q*, a token-level RL approach that aligns LLMs *without* preference data or explicit reward models by directly estimating an optimal policy.

Despite these advancements, challenges persist. Many solutions introduce new hyperparameters or rely on assumptions about reward model behavior, and their generalizability across diverse tasks and model scales remains an ongoing challenge. The dynamic nature of human preferences and the potential for strategic misreporting by human labelers, as highlighted by \cite{hao2024iyj}, continue to pose threats to reward stability. Future research must continue to focus on developing more adaptive, theoretically grounded, and computationally efficient methods to ensure LLMs genuinely align with complex human preferences, moving beyond merely maximizing a proxy reward.