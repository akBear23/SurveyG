\subsection{Interpretable Reward Models and Preference Control}

The efficacy of Reinforcement Learning from Human Feedback (RLHF) in aligning Large Language Models (LLMs) with human preferences is critically dependent on the underlying reward models (RMs). However, conventional RMs often function as opaque, single-scalar black boxes, making it challenging to understand *why* a model prefers certain responses and hindering precise control over LLM behavior. This lack of transparency impedes debugging, fosters biases, and erodes trust in the alignment process.

A significant challenge stemming from opaque reward models is the emergence of undesirable biases. For instance, \cite{singhal2023egk} rigorously demonstrated a pervasive "verbosity bias" in RMs, where longer outputs are often favored regardless of their actual quality, leading LLMs to generate verbose rather than genuinely helpful responses. This highlights how RMs can optimize for shallow correlations rather than true human intent. Beyond specific biases, the black-box nature of RMs makes it difficult to diagnose trade-offs observed in aligned models; for example, \cite{kirk20230it} found that while RLHF improves out-of-distribution generalization, it often reduces output diversity, a phenomenon whose root cause within the reward signal is hard to pinpoint. Furthermore, algorithmic biases in how preferences are aggregated and optimized can lead to "preference collapse," where minority viewpoints are disregarded even with a perfectly specified reward model, as theorized by \cite{xiao2024ro4}. Such issues underscore the urgent need for RMs that offer greater transparency and steerability.

To address these limitations, recent research has focused on developing RMs that provide decomposable and understandable preference scores for individual objectives. A notable advancement is the \textit{Absolute-Rating Multi-Objective Reward Model (ArmoRM)} introduced by \cite{wang20247pw}. Unlike traditional RMs that rely on binarized pairwise comparisons, ArmoRM is trained on multi-dimensional absolute-rating data, allowing it to output distinct scores for human-interpretable objectives such as helpfulness, correctness, and safety. This decomposable output provides a clear window into the RM's decision-making, enabling developers to understand *why* a response was preferred. Building on this, \cite{wang20247pw} further proposes a \textit{Mixture-of-Experts (MoE) Scalarization} mechanism, where a gating network dynamically weights these individual objective scores based on the input context. This dynamic weighting allows for flexible and steerable preference control, mitigating issues like the verbosity bias identified by \cite{singhal2023egk} through a specific decorrelation technique. The ArmoRM with MoE scalarization not only achieved state-of-the-art performance on RewardBench but also matched much larger models, demonstrating that interpretability can be achieved without sacrificing performance.

Beyond multi-objective decomposition, efforts to provide more granular reward signals contribute to interpretability and control. Several works have explored deriving dense, token-level rewards from sparse, sequence-level feedback to improve credit assignment. For instance, \cite{li2024h19} introduced RED (REward reDistribution), which assigns token-level rewards by calculating the incremental impact of each token on the reward model's score, thereby providing fine-grained feedback without modifying the RM. Similarly, \cite{zhong2024wch} developed Reinforced Token Optimization (RTO), which leverages Direct Preference Optimization (DPO) to extract token-wise reward signals, effectively bridging the gap between sentence-level preferences and step-wise RL. Complementing this, \cite{chan2024xig} proposed Attention Based Credit (ABC), a method that reuses the reward model's internal attention maps to redistribute the final scalar reward across tokens, offering "dense reward for free" with minimal computational overhead. These token-level approaches enhance the ability to debug and understand local generation choices. Furthermore, the concept of LLMs autonomously designing reward functions, as demonstrated by \cite{ma2023vyo} with Eureka for robotic tasks, showcases a meta-level of preference control. Eureka's "reward reflection" mechanism, which provides fine-grained feedback on individual reward components, aligns with the goal of decomposable and understandable preference signals. The challenges of designing effective reward signals for complex tasks, as highlighted by \cite{havrilla2024m0y} in the context of improving LLM reasoning, further underscore the value of such interpretable and steerable reward mechanisms.

In conclusion, the evolution of reward modeling is moving decisively towards enhancing transparency, steerability, and interpretability. The shift from opaque, single-scalar reward outputs to decomposable, multi-objective scores, dynamically weighted by context, represents a significant step forward. While methods like ArmoRM with MoE scalarization offer a robust framework for understanding *why* an LLM prefers certain responses and mitigating biases like verbosity, the challenge remains in scaling these fine-grained approaches to diverse, complex, and open-ended tasks without introducing new forms of bias or computational overhead. Future research will likely continue to explore novel ways to integrate human values more precisely into reward signals, balance interpretability with performance, and develop more robust mechanisms for dynamic, context-aware preference control to ensure truly aligned and trustworthy LLM behavior.