\subsection*{Navigating This Review: Scope and Structure}

This literature review meticulously maps the intellectual trajectory of reinforcement learning (RL) within the domain of language processing, tracing its evolution from foundational concepts to its transformative role in shaping modern large language models (LLMs). The scope is defined by the intersection of RL and language, encompassing both historical applications to traditional Natural Language Processing (NLP) tasks and the more recent, pivotal contributions of RL in aligning and enhancing LLMs. This review specifically focuses on how RL methodologies have been adapted, refined, and critiqued to address the unique challenges of language generation and understanding, particularly in the context of human preferences and complex reasoning.

The review begins by establishing the foundational concepts of RL as applied to language. Early work, such as \cite{sun2023awe}, framed Reinforcement Learning from Human Feedback (RLHF) as an Online Inverse Reinforcement Learning problem, providing a theoretical lens for understanding its efficacy over traditional supervised fine-tuning (SFT). This foundational understanding paved the way for the development of core alignment methodologies. The Proximal Policy Optimization (PPO) algorithm, as detailed in \cite{zheng2023c98}, became a cornerstone of RLHF, though its complexity and instability presented significant challenges. In response, Direct Preference Optimization (DPO) emerged as a simpler, more stable alternative, directly optimizing the language model policy to align with human preferences without an explicit reward model \cite{rafailov20239ck}. These initial advancements are comprehensively summarized in surveys like \cite{wang2024a3a} and \cite{srivastava2025gfw}, which categorize techniques and highlight the rapid evolution of RL-enhanced LLMs.

However, the initial success of RLHF and DPO quickly revealed inherent limitations, necessitating a wave of research focused on robustness and stability. A pervasive issue identified was "reward over-optimization" or "reward hacking," where models exploit imperfections in the reward function. This was observed even in DPO, as shown by \cite{rafailov2024ohd}, which found that DPO still suffers from over-optimization, exhibiting scaling laws similar to classical RLHF. Diagnostics revealed that reward models could be susceptible to shallow correlations, such as output length \cite{singhal2023egk}, or internal "energy loss" phenomena \cite{miao2025ox0}. To counter these, researchers developed various mitigation strategies: reward shaping, as proposed by \cite{fu2025hl3} with their Preference As Reward (PAR) technique, which applies a sigmoid function to centered rewards; constrained RLHF, exemplified by \cite{moskovitz2023slz} and \cite{dai2025ygq}, which prevent over-optimization by dynamically adjusting reward component influence or using value regularization to restrict policy iteration to in-distribution regions. Uncertainty quantification also played a role, with \cite{zhai20238xc} introducing Uncertainty-Penalized RLHF (UP-RLHF) using diverse reward LoRA ensembles, and \cite{zhang2024esn} proposing Adversarial Policy Optimization (AdvPO) with lightweight uncertainty estimation to make RLHF more robust. Practical pipeline improvements, such as those detailed in \cite{hou2024tvy} for the ChatGLM-RLHF system, addressed issues like length bias and catastrophic forgetting. Furthermore, methods like Online Merging Optimizers \cite{lu202435m} were introduced to mitigate the "alignment tax" and catastrophic forgetting during RLHF. The challenge of sparse, sequence-level rewards was tackled by approaches like Reinforced Token Optimization (RTO) \cite{zhong2024wch}, which derives token-wise rewards from DPO, and Attention Based Credit (ABC) \cite{chan2024xig} and RED \cite{li2024h19}, which redistribute holistic rewards to individual tokens, providing "dense reward for free." Theoretical underpinnings for KL-regularized RLHF were also strengthened by works like \cite{xiong2023klt} and \cite{zhao202532z}, which provided logarithmic regret bounds for online settings.

The field then progressed to more advanced alignment objectives, moving beyond simple aggregated preferences to address the inherent complexity of human values. This included multi-objective alignment to balance conflicting goals like helpfulness and safety, as explored by \cite{tan2025lk0} with their Equilibrate RLHF framework and \cite{zhang2024b6u}'s Bi-Factorial Preference Optimization. The diversity of human preferences was explicitly addressed by \cite{chakraborty20247ew}'s MaxMin-RLHF, which learns a mixture of reward models, and \cite{boldi2024d0s}'s Pareto-Optimal Learning from Preferences, which uses lexicase selection to cater to distinct hidden context groups without explicit labels. Fine-grained control and multi-task learning were advanced by \cite{xu20242yo}'s Constrained Generative Policy Optimization (CGPO) with a Mixture of Judges and \cite{lai2024ifx}'s ALaRM, which models hierarchical rewards. Concurrently, efforts focused on reducing the data and supervision burden. Reinforcement Learning from AI Feedback (RLAIF) emerged as a scalable alternative to RLHF, with \cite{lee2023mrw} demonstrating its comparable performance. Self-supervised and data-efficient alignment techniques gained prominence, including principle-driven self-alignment with minimal human supervision \cite{sun20238m7}, Reinforced Self-Training (ReST) for iterative self-improvement \cite{gulcehre2023hz8}, and optimal design for reward modeling to efficiently collect human feedback \cite{scheid20244oy}. More radically, Inverse-Q* \cite{xia2024rab} enabled token-level RL alignment without preference data, while CodePMP \cite{yu2024p4z} offered scalable preference model pretraining for reasoning tasks using synthetic code data. Active preference elicitation was explored by \cite{zhang2024lqf}'s Self-Exploring Language Models, and iterative self-alignment was further refined by \cite{chen2024vkb} using DPO's implicit rewards and \cite{singla2024dom}'s tuning-free Dynamic Rewarding with Prompt Optimization (DRPO).

Finally, the review delves into critical examinations of RLHF, its inherent risks, and its broader applications. Concerns about the unintended consequences of alignment include trade-offs between generalization and diversity \cite{kirk20230it, mohammadi20241pk}, where RLHF can reduce output diversity and lead to "attractor states." Security vulnerabilities like preference poisoning attacks \cite{baumgrtner2024gu4} and biases such as verbosity bias in AI-generated feedback \cite{saito2023zs7} highlight the fragility of current methods. More fundamentally, papers like \cite{lambert2023bty} and \cite{lambert2023c8q} critically analyze the "objective mismatch" problem and the ill-posed assumptions of reward models, while \cite{lindstrm20253o2} explores the sociotechnical limits of alignment, arguing that RLHF alone is insufficient for true AI safety. The challenge of "jailbreaking" open-sourced LLMs \cite{zhang2023pbi} further underscores these risks. Scaling properties of RLHF were systematically investigated by \cite{hou202448j} and \cite{shen2025pyh}, revealing diminishing returns and inefficiencies, while \cite{tang2024wt3} explored the performance gap between online and offline alignment. Despite these challenges, RL has found novel applications, such as teaching LLMs to reason \cite{havrilla2024m0y}, enhancing autonomous driving safety with LLM-enhanced RLHF \cite{sun2024nor}, and generating dense rewards for text generation via LLM critique \cite{cao2024lh3}. The broader impact on fields like Affective Computing is also being surveyed \cite{zhang20242mw}. The need for robust evaluation of reward models themselves, as addressed by \cite{frick20248mv}'s Preference Proxy Evaluations (PPE), is critical. Further advancements include causality-aware alignment \cite{xia20247qb}, interpretable multi-objective reward modeling \cite{wang20247pw}, and inference-time alignment for harmlessness \cite{wang2024w7p}.

This structured progression from foundational concepts and core algorithms to addressing their limitations, exploring advanced objectives, and critically examining their societal and technical implications, provides a comprehensive overview of the field's intellectual journey. While significant progress has been made in aligning LLMs, persistent challenges such as robustly handling diverse and conflicting human preferences, ensuring true safety and interpretability, and achieving efficient scalability remain critical areas for future research.