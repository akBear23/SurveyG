\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Historical Trajectory and Early Promise of RL in NLP}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Contemporary Motivations: Aligning Large Language Models with Human Intent}{5}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Scope and Structure of the Review}{8}{subsection.1.3}%
\contentsline {section}{\numberline {2}Foundational Concepts: Bridging RL and Language}{11}{section.2}%
\contentsline {subsection}{\numberline {2.1}Core Reinforcement Learning Algorithms for Sequential Decision Making}{11}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Structured Prediction as a Search Problem}{13}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Optimizing Non-Differentiable Metrics in NLP}{15}{subsection.2.3}%
\contentsline {section}{\numberline {3}Deep Reinforcement Learning for Direct Language Generation}{17}{section.3}%
\contentsline {subsection}{\numberline {3.1}DRL for Dialogue Generation}{17}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}DRL for Abstractive Summarization and Machine Translation}{20}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Actor-Critic Methods and Exposure Bias Mitigation}{22}{subsection.3.3}%
\contentsline {section}{\numberline {4}The Rise of Reinforcement Learning from Human Feedback (RLHF)}{25}{section.4}%
\contentsline {subsection}{\numberline {4.1}Core RLHF Paradigm: Reward Modeling and Policy Optimization}{25}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Proximal Policy Optimization (PPO) for LLM Alignment}{27}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Direct Preference Optimization (DPO) and Reward-Model-Free Alignment}{30}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Theoretical Foundations of KL-Constrained Preference Learning}{33}{subsection.4.4}%
\contentsline {section}{\numberline {5}Advanced Feedback Mechanisms and Robust Alignment}{36}{section.5}%
\contentsline {subsection}{\numberline {5.1}Reinforcement Learning from AI Feedback (RLAIF)}{36}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Mitigating Reward Model Imperfections and Overoptimization}{39}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Interpretable and Multi-Objective Reward Modeling}{41}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Implicit Human Signals and Distributionally Robust Alignment}{44}{subsection.5.4}%
\contentsline {section}{\numberline {6}RL for Enhanced LLM Capabilities and Specialized Applications}{46}{section.6}%
\contentsline {subsection}{\numberline {6.1}Enhancing LLM Reasoning with RL}{46}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}RL for Tool Use and Agentic Behavior}{49}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}LLM/VLM-Driven Automated Reward Design}{52}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Domain-Specific Applications: Software Engineering and Code Generation}{54}{subsection.6.4}%
\contentsline {section}{\numberline {7}Efficiency, Scalability, and Responsible AI in RL for Language Processing}{55}{section.7}%
\contentsline {subsection}{\numberline {7.1}Training Efficiency and Parameter-Efficient RLHF}{55}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Dynamic and Inference-Time Alignment}{57}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}Direct Inference-Time Interventions}{58}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Inference-Time Search and Self-Optimization}{60}{subsubsection.7.2.2}%
\contentsline {subsection}{\numberline {7.3}Privacy, Security, and Robustness of Aligned LLMs}{61}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Critical Evaluation and Sociotechnical Limits of Alignment}{63}{subsection.7.4}%
\contentsline {section}{\numberline {8}Conclusion and Future Directions}{65}{section.8}%
\contentsline {subsection}{\numberline {8.1}Summary of Key Advancements and Intellectual Trajectories}{65}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Unresolved Tensions and Theoretical Gaps}{68}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Practical Challenges and Ethical Considerations}{71}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Promising Future Directions}{74}{subsection.8.4}%
\contentsline {section}{References}{79}{section*.2}%
