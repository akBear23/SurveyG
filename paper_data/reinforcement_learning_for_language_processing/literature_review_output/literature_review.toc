\contentsline {section}{\numberline {1}Introduction}{5}{section.1}%
\contentsline {subsection}{\numberline {1.1}Historical Trajectory and Early Promise of RL in NLP}{5}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Contemporary Motivations: Aligning Large Language Models with Human Intent}{7}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Scope and Structure of the Review}{10}{subsection.1.3}%
\contentsline {section}{\numberline {2}Foundational Concepts: Bridging RL and Language}{13}{section.2}%
\contentsline {subsection}{\numberline {2.1}Core Reinforcement Learning Algorithms for Sequential Decision Making}{13}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Structured Prediction as a Search Problem}{15}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Optimizing Non-Differentiable Metrics in NLP}{17}{subsection.2.3}%
\contentsline {section}{\numberline {3}Deep Reinforcement Learning for Direct Language Generation}{19}{section.3}%
\contentsline {subsection}{\numberline {3.1}DRL for Dialogue Generation}{19}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}DRL for Abstractive Summarization and Machine Translation}{22}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Actor-Critic Methods and Exposure Bias Mitigation}{24}{subsection.3.3}%
\contentsline {section}{\numberline {4}The Rise of Reinforcement Learning from Human Feedback (RLHF)}{27}{section.4}%
\contentsline {subsection}{\numberline {4.1}Core RLHF Paradigm: Reward Modeling and Policy Optimization}{27}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Proximal Policy Optimization (PPO) for LLM Alignment}{29}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Direct Preference Optimization (DPO) and Reward-Model-Free Alignment}{32}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Theoretical Foundations of KL-Constrained Preference Learning}{35}{subsection.4.4}%
\contentsline {section}{\numberline {5}Advanced Feedback Mechanisms and Robust Alignment}{38}{section.5}%
\contentsline {subsection}{\numberline {5.1}Reinforcement Learning from AI Feedback (RLAIF)}{38}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Mitigating Reward Model Imperfections and Overoptimization}{41}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Interpretable and Multi-Objective Reward Modeling}{43}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Implicit Human Signals and Distributionally Robust Alignment}{46}{subsection.5.4}%
\contentsline {section}{\numberline {6}RL for Enhanced LLM Capabilities and Specialized Applications}{48}{section.6}%
\contentsline {subsection}{\numberline {6.1}Enhancing LLM Reasoning with RL}{48}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}RL for Tool Use and Agentic Behavior}{51}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}LLM/VLM-Driven Automated Reward Design}{54}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Domain-Specific Applications: Software Engineering and Code Generation}{56}{subsection.6.4}%
\contentsline {section}{\numberline {7}Efficiency, Scalability, and Responsible AI in RL for Language Processing}{57}{section.7}%
\contentsline {subsection}{\numberline {7.1}Training Efficiency and Parameter-Efficient RLHF}{57}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Dynamic and Inference-Time Alignment}{59}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}Direct Inference-Time Interventions}{60}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Inference-Time Search and Self-Optimization}{62}{subsubsection.7.2.2}%
\contentsline {subsection}{\numberline {7.3}Privacy, Security, and Robustness of Aligned LLMs}{63}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Critical Evaluation and Sociotechnical Limits of Alignment}{65}{subsection.7.4}%
\contentsline {section}{\numberline {8}Conclusion and Future Directions}{67}{section.8}%
\contentsline {subsection}{\numberline {8.1}Summary of Key Advancements and Intellectual Trajectories}{67}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Unresolved Tensions and Theoretical Gaps}{70}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Practical Challenges and Ethical Considerations}{73}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Promising Future Directions}{76}{subsection.8.4}%
\contentsline {section}{References}{81}{section*.2}%
