\subsection{Core RLHF Mechanisms and Early Breakthroughs}

The advent of Reinforcement Learning from Human Feedback (RLHF) marked a pivotal shift in Large Language Model (LLM) development, offering a robust solution to the inherent limitations of direct Deep Reinforcement Learning (DRL) for language generation, such as exposure bias and the difficulty of optimizing non-differentiable, human-centric metrics. This breakthrough enabled LLMs to move beyond mere statistical language modeling to reliably follow complex instructions, generate helpful and harmless responses, and align with nuanced human values, fundamentally enhancing their controllability and user-friendliness. The foundational RLHF process typically unfolds in three distinct stages.

The first stage, **Supervised Fine-Tuning (SFT)**, is crucial for imbuing the pre-trained LLM with basic instruction-following capabilities. In this phase, a pre-trained LLM is fine-tuned on a dataset of high-quality human-written demonstrations, where prompts are paired with desired responses. This initial supervised learning step provides a baseline policy that can generate reasonable and coherent text, serving as a stable starting point for the subsequent reinforcement learning phase and mitigating the challenges of sparse rewards in an open-ended generation environment.

The second stage involves training a **Reward Model (RM)**, which acts as a scalable proxy for nuanced human values and preferences. Instead of directly optimizing for complex, often subjective human criteria, a separate model is trained to predict human preferences. This is achieved by collecting a dataset of human comparisons, where annotators evaluate and rank multiple responses generated by the LLM for a given prompt \cite{Ziegler2019, Stiennon2020}. For instance, \cite{Ziegler2019} demonstrated the effectiveness of training a reward model on human preferences to fine-tune language models, while \cite{Stiennon2020} further applied this concept to abstractive summarization, showing how a reward model learned from human feedback could significantly improve summary quality. The RM, typically another LLM, learns to assign a scalar score to a generated response, reflecting its alignment with human-desired attributes like helpfulness, harmlessness, and honesty. This approach elegantly bypasses the challenge of explicitly defining and hand-crafting complex reward functions for every desired behavior.

Finally, the core of RLHF involves **fine-tuning the LLM policy using reinforcement learning algorithms**, most commonly Proximal Policy Optimization (PPO), guided by the trained reward model. In this stage, the LLM (now acting as the policy) generates responses to new prompts, and these responses are then scored by the RM. The policy is updated to maximize this reward signal, effectively learning to generate outputs that the RM predicts humans would prefer. The PPO algorithm, known for its stability and sample efficiency, is particularly well-suited for this task, allowing the LLM to iteratively refine its generation strategy \cite{Ouyang2022}. This iterative optimization process, as demonstrated by \cite{Lewis2020} for fine-tuning text-to-text transformers on various task-specific and non-differentiable metrics, enables the LLM to robustly follow complex instructions and align with human values. The culmination of this three-stage process was famously showcased by \cite{Ouyang2022} with InstructGPT (and subsequently ChatGPT), which leveraged RLHF to train LLMs that could follow instructions and exhibit helpful, harmless, and honest behavior, even outperforming much larger models fine-tuned purely with supervised methods.

This foundational RLHF paradigm marked a significant development in making LLMs more controllable, user-friendly, and ethically responsible. By integrating human preferences into the learning loop, RLHF effectively addressed the limitations of direct DRL, where designing explicit reward functions for subjective qualities was intractable. However, these early breakthroughs also highlighted new challenges, such as the high cost and potential biases of human data collection, the inherent difficulty of ensuring true alignment across diverse contexts, and the risk of "reward hacking," where the model exploits imperfections in the proxy reward model rather than genuinely improving its behavior. These emergent challenges subsequently became central to the next wave of research in RLHF.