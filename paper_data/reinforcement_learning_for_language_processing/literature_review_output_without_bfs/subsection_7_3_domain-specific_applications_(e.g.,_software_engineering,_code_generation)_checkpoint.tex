\subsection{Domain-Specific Applications (e.g., Software Engineering, Code Generation)}

Reinforcement Learning (RL) is increasingly being leveraged to align Large Language Models (LLMs) with the stringent requirements of highly specialized domains, where outputs demand strict functional correctness, adherence to domain-specific rules, or complex problem-solving. This marks a crucial intellectual trajectory in RL for language processing, shifting towards "grounded" or "executable" language generation by integrating non-textual, domain-specific feedback for rigorous and verifiable performance.

A significant challenge in applying RL to complex, real-world engineering tasks is the impracticality of obtaining direct execution-based rewards. \cite{wei2025v4d} addresses this in the context of software engineering (SE) with \textbf{SWE-RL}, an innovative framework designed to enhance LLM reasoning for bug fixing and issue resolution. Instead of costly execution-based feedback, SWE-RL leverages a massive dataset of GitHub Pull Requests and employs a lightweight, rule-based reward function based on patch similarity using \texttt{difflib.SequenceMatcher}. This approach, the first to apply RL with rule-based rewards to real-world software evolution data, enabled a medium-sized LLM (Llama3-SWE-RL-70B) to achieve a 41.0\% solve rate on the SWE-bench Verified benchmark, rivaling larger proprietary models. Crucially, \cite{wei2025v4d} demonstrated that RL on this single, complex in-domain task led to emergent generalized reasoning skills across diverse out-of-domain tasks, suggesting that specialized training can foster broader cognitive benefits. However, the reliance on a proxy reward function, while practical, might not perfectly capture true functional equivalence, and the `Agentless Mini` scaffold used for evaluation simplifies some real-world complexities.

Complementing this, other research has focused on domains where direct external verification is feasible and critical for functional correctness. \cite{wang20252c7} exemplifies this by training a Verilog generation LLM with RL using rigorous testbench feedback. Recognizing that Supervised Fine-Tuning (SFT) alone cannot guarantee functional correctness for Hardware Description Languages (HDLs), this work combines SFT with Direct Preference Optimization (DPO). The core innovation lies in a novel automatic testbench generation pipeline that leverages real Verilog Compiler Simulator (VCS) feedback to collect preference pairs. By designating code passing more test cases as "preferred," \cite{wang20252c7} aligns the LLM directly with functional correctness, mitigating the "reward hacking" problem common in explicit reward-based RL. This approach, the first to integrate verification insights and DPO for Verilog generation, consistently outperforms state-of-the-art baselines in generating functionally correct HDL code. A limitation, however, is that automatic testbench generation cannot guarantee coverage of all functional cases, and the reliance on external simulators introduces computational overhead.

Both \cite{wei2025v4d} and \cite{wang20252c7} push the boundaries of LLM utility in critical engineering fields, albeit through different strategies for feedback integration. \cite{wei2025v4d} demonstrates the power of scalable, lightweight, rule-based proxy rewards when direct execution feedback is prohibitive, while \cite{wang20252c7} showcases how to effectively integrate rigorous, non-textual, domain-specific external verification for achieving strict functional correctness. The unresolved tension lies in balancing the practicality and scalability of proxy reward functions with their fidelity to true task objectives, and in scaling computationally intensive verification processes to match the ever-growing size and complexity of LLMs and their diverse applications. Future directions will likely explore hybrid approaches that combine the efficiency of proxy rewards with targeted, rigorous verification, or develop more efficient and comprehensive external verification methods.