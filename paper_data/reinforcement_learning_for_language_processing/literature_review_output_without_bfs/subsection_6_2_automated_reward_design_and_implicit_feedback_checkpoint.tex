\subsection{Automated Reward Design and Implicit Feedback}
The efficacy of Reinforcement Learning (RL) agents is profoundly dependent on well-designed reward functions, which are notoriously difficult and time-consuming to engineer manually. This challenge has spurred research into leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs) to automate reward design or infer preference signals from implicit human behaviors, thereby moving beyond costly explicit human labeling. These advanced techniques aim to create more sophisticated, interpretable, and scalable reward mechanisms that capture nuanced preferences and task objectives.

One prominent direction involves using LLMs to generate executable reward code. \textcite{ma2023vyo} introduced Eureka, a framework where an LLM (GPT-4) autonomously designs executable Python reward functions for complex robotic tasks. Eureka employs an evolutionary optimization process, guided by "environment as context" and "reward reflection," an automated feedback mechanism that analyzes policy training dynamics, enabling the LLM to achieve superhuman performance on various dexterous manipulation tasks. Building upon this, \textcite{hazra2024wjp} proposed REvolve, which also leverages LLMs as intelligent genetic operators within an evolutionary framework to evolve executable Python reward code. A key innovation in REvolve is the integration of human preference feedback on policy rollouts as the fitness function, allowing for the design of more nuanced and human-aligned rewards in subjective domains like autonomous driving, thereby addressing the limitations of purely automated feedback for complex, tacit human knowledge.

Beyond generating explicit code, VLMs have emerged as powerful tools for directly inferring reward signals from visual observations. \textcite{rocamonde2023o9z} demonstrated that pretrained VLMs, such as CLIP, can serve as zero-shot reward models (VLM-RMs). This method computes rewards as the cosine similarity between the language embedding of a natural language task description and the image embedding of the current state, enabling agents to learn complex tasks like humanoid locomotion from simple text prompts. While effective, direct VLM scores can sometimes be noisy. To address this, \textcite{wang2024n8c} introduced RL-VLM-F, which leverages VLMs (e.g., GPT-4V, Gemini) to provide preference feedback over pairs of visual observations based on a text goal. A reward function is then learned from these VLM-generated preference labels, offering a more robust and scalable approach to inferring rewards from visual data, particularly for complex manipulation tasks involving deformable objects, without requiring low-level state access.

A complementary line of research focuses on how LLMs and VLMs can interpret complex or implicit signals to construct reward functions. \textcite{cao2024lh3} tackled the sparse reward problem in text generation by proposing RELC (Rewards from Language model Critique). This framework utilizes an LLM as a critic to generate dense, intermediate-step intrinsic rewards during RL training, effectively automating reward shaping and significantly improving sample efficiency and performance across tasks like sentiment control and detoxification. Extending this to real-world systems, \textcite{sun2024nor} presented a human-centric approach for optimizing autonomous driving safety using LLM-enhanced RLHF. Their method employs an LLM to interpret multimodal sensor data, including physical (e.g., acceleration) and physiological (e.g., heart rate) feedback from human drivers, translating these complex, often implicit, signals into preferences for the autonomous driving reward function. This innovative approach aims to capture human comfort and safety, moving beyond explicit labels. Finally, directly integrating truly implicit human signals, \textcite{lpezcardona20242pt} introduced GazeReward, a framework that incorporates synthetic eye-tracking (ET) data into the Reward Model for LLM alignment. By using ET prediction models to generate synthetic gaze features from text, GazeReward provides a scalable and cost-effective way to infer human preferences from subconscious behavioral cues, demonstrating substantial accuracy gains in reward model predictions.

In conclusion, the integration of LLMs and VLMs into reward design and feedback mechanisms marks a significant paradigm shift in RL. These methods collectively demonstrate the potential to automate the creation of executable reward functions, infer preferences directly from visual and linguistic cues, and even translate complex implicit human signals into actionable reward components. While these advancements promise more scalable, interpretable, and human-aligned RL systems, challenges remain in ensuring the fidelity of synthetic implicit signals, guaranteeing the generalizability and safety of LLM-generated reward code, and establishing robust ethical frameworks for inferring preferences from subtle human behaviors. Future research will likely focus on enhancing the robustness of these models, exploring multimodal implicit signals more broadly, and developing hybrid approaches that combine the strengths of explicit and implicit feedback for truly sophisticated AI alignment.