\subsection{Improving Multi-Step Reasoning and Problem Solving}

Large Language Models (LLMs) often struggle with complex, multi-step reasoning and problem-solving tasks, particularly in challenging domains like mathematics and logical deduction, where superficial pattern matching is insufficient. Advanced reinforcement learning (RL) techniques, especially those leveraging preference-based feedback and sophisticated search algorithms, are proving instrumental in moving LLMs towards more robust, verifiable, and genuinely compositional reasoning processes.

The critical role of Reinforcement Learning from Human Feedback (RLHF) in enhancing LLM reasoning capabilities was empirically highlighted by \cite{fu2023pyr}. Their Chain-of-Thought Hub demonstrated that RLHF significantly impacts the performance of top-tier models on diverse reasoning benchmarks, underscoring the necessity for more sophisticated RL-driven approaches to achieve state-of-the-art performance. Building on this, some research focuses on dynamic guidance during inference. For instance, \cite{hao2025lc8} introduced RL-of-Thoughts (RLoT), an inference-time technique that trains a lightweight "navigator model" using Double-Dueling-DQN. This navigator dynamically selects human cognition-inspired "logic blocks" (e.g., "Reason one step," "Decompose," "Refine") to guide the LLM's reasoning path, adapting to the problem's evolving state through self-evaluation and a Process Reward Model. This approach enables smaller LLMs to achieve performance comparable to much larger models without modifying their base parameters, offering an efficient external control mechanism for complex tasks.

However, for deeper, more intrinsic improvements in compositional reasoning, methods that fine-tune the LLM with fine-grained, step-level feedback are crucial. A prominent direction involves combining preference-based RL with powerful search algorithms like Monte Carlo Tree Search (MCTS) to optimize for correct solution paths. \cite{zhang2024q0e} proposed LLaMA-Berry, a framework specifically designed for Olympiad-level mathematical reasoning. LLaMA-Berry integrates Self-Refine MCTS to robustly explore the solution space, treating entire solutions as states and self-refinement as an optimization action. It employs a Pairwise Preference Reward Model (PPRM), trained with Direct Preference Optimization (DPO), to evaluate solutions, and an Enhanced Borda Count method to aggregate these preferences into global scores. This synergistic combination enables smaller LLMs to tackle highly complex mathematical problems effectively by providing solution-level preference signals.

A significant advancement in providing truly fine-grained feedback comes from \cite{chen20244ev} with their Step-level Value Preference Optimization (SVPO). Recognizing that solution-level preferences are insufficient for multi-step reasoning, SVPO leverages MCTS not just for search, but to *autonomously generate fine-grained step-level preferences*. By analyzing Q-values at each node in the MCTS tree, SVPO identifies potential reasoning errors at specific steps, eliminating the need for costly human or GPT-4 annotations. It then integrates an explicit value model into a novel DPO-like loss function, trained directly from these MCTS-derived Q-values and step-level preferences. This allows the LLM to learn *why* certain steps are better than others, significantly boosting mathematical reasoning performance and enabling the model to navigate complex reasoning paths more effectively during inference with techniques like Step-level Beam Search. Such advancements are further supported by the availability of high-quality, large-scale datasets like Big-Math \cite{albalak2025wyc}, which provides meticulously curated, open-ended mathematical problems suitable for RL training, bridging the gap between data quality and quantity.

In conclusion, the integration of advanced RL techniques, particularly preference-based methods combined with sophisticated search algorithms like MCTS, is driving LLMs towards more robust multi-step reasoning. The evolution from observing RLHF's impact to developing inference-time guidance and, crucially, fine-tuning methods that provide fine-grained, step-level feedback marks a significant shift. Future research will likely focus on scaling these fine-grained feedback mechanisms to broader logical domains, improving the efficiency of MCTS-based data generation, and integrating these compositional reasoning capabilities more deeply into foundational LLM architectures, ultimately enabling verifiable and genuinely intelligent problem-solving.