\subsection{Mitigating Reward Model Imperfections and Overoptimization}

The efficacy of Reinforcement Learning from Human Feedback (RLHF) hinges on the fidelity of its proxy reward models (RMs) to true human preferences. A critical challenge in LLM alignment is "reward hacking" or "overoptimization," where models exploit flaws, spurious correlations, or unintended shortcuts in these imperfect RMs, leading to misaligned or undesirable behaviors \cite{lambert2023c8q}. This section explores diagnostic insights and a range of mitigation strategies designed to ensure that aligned policies genuinely reflect complex human preferences rather than merely optimizing a flawed proxy.

Initial diagnostic studies revealed pervasive issues. \cite{singhal2023egk} empirically demonstrated that output length is a surprisingly significant spurious correlation, with reward models often favoring longer responses regardless of genuine quality, and that simple length-only optimization can reproduce most RLHF "improvements." Extending this, \cite{rafailov2024ohd} showed that even in Direct Alignment Algorithms (DAAs) like DPO \cite{rafailov20239ck}, performance degradation due to overoptimization occurs early in training, highlighting the brittleness of these methods. \cite{saito2023zs7} further identified a verbosity bias in AI-generated preferences, a common source of RM imperfection. Conceptually, \cite{lambert2023c8q} framed these issues as an "objective mismatch," where the numerical objectives of RM training, policy optimization, and downstream evaluation become decoupled, leading to unintended behaviors. Moreover, \cite{kirk20230it} and \cite{mohammadi20241pk} observed that while RLHF improves alignment, it can inadvertently reduce output diversity and creativity, suggesting an overoptimization towards a narrow, "safe" behavioral mode.

To counter these imperfections, research has focused on improving reward model quality and robustness. \cite{yu20249l0} enhanced RM quality and interpretability by leveraging self-generated, filtered critiques, moving beyond black-box scalar scores. Building on this, \cite{wang20247pw} introduced ArmoRM, a multi-objective reward model with a Mixture-of-Experts (MoE) scalarization, providing interpretable and steerable reward scores while explicitly mitigating verbosity bias. For complex tasks, \cite{lai2024ifx} proposed ALaRM, a hierarchical reward modeling framework that combines holistic and proactively selected aspect-specific rewards to address sparse and inconsistent human feedback. Addressing data scarcity for robust RMs in reasoning tasks, \cite{yu2024p4z} developed CodePMP, a scalable preference model pretraining pipeline that synthesizes high-quality code-preference pairs. Crucially, \cite{frick20248mv} introduced the Preference Proxy Evaluations (PPE) benchmark, which directly correlates RM performance on proxy tasks with real downstream human preference scores of RLHF-tuned LLMs, providing a vital tool to ensure RM improvements translate to actual policy gains. In a practical deployment context, \cite{hou2024tvy} incorporated "Bucket-Based Length Balancing" and L2 regularization during RM training to reduce length bias and stabilize training.

Beyond improving RMs, a significant body of work focuses on sophisticated regularization and constrained RLHF methods at the policy optimization stage to prevent models from exploiting RM flaws. \cite{moskovitz2023slz} proposed constrained RLHF, using dynamically identified "proxy points" to prevent overoptimization of individual components within composite reward models. \cite{fu2025hl3} introduced Preference As Reward (PAR), a theoretically grounded reward shaping technique that applies a sigmoid function to centered rewards, thereby bounding rewards and stabilizing training against hacking. Taking an internal-mechanistic approach, \cite{miao2025ox0} identified the "energy loss phenomenon" in the LLM's final layer during RLHF and proposed Energy loss-aware PPO (EPPO) to penalize excessive increases in this metric, mitigating reward hacking by preserving contextual relevance.

Uncertainty estimation in reward predictions has emerged as a powerful strategy. \cite{zhai20238xc} developed Uncertainty-Penalized RLHF (UP-RLHF), which uses diverse Reward LoRA Ensembles to quantify and penalize reward uncertainty, preventing the policy from generating out-of-distribution (OOD) responses where the RM is unreliable. Building on this, \cite{dai2025ygq} introduced Behavior-Supported Policy Optimization (BSPO), which employs value regularization to restrict policy iteration to the in-distribution region of the reward model, effectively penalizing OOD values without affecting in-distribution ones. Further enhancing efficiency, \cite{zhang2024esn} proposed Adversarial Policy Optimization (AdvPO) with a lightweight uncertainty estimation method based on last-layer embeddings, making RLHF more robust to overoptimization without the computational overhead of ensembles. \cite{cen2024nef} presented Value-Incentivized Preference Optimization (VPO), a unified framework for online and offline RLHF that implicitly incorporates optimism or pessimism through value function regularization, bypassing explicit uncertainty estimation.

Other regularization techniques address specific algorithmic biases and trade-offs. \cite{xiao2024ro4} identified "preference collapse" as an inherent algorithmic bias in standard KL-regularized RLHF, where minority preferences are disregarded, and proposed Preference Matching (PM) RLHF to ensure diversity. \cite{tan2025lk0} tackled the helpfulness-safety trade-off with Equilibrate RLHF, using a fine-grained data-centric approach and Adaptive Message-wise Alignment to prevent models from becoming "over-safe" and unhelpful. \cite{xu20242yo} introduced Constrained Generative Policy Optimization (CGPO) with a "Mixture of Judges" to mitigate reward hacking and multi-objective conflicts in complex multi-task settings. Furthermore, to combat the "alignment tax" (loss of general capabilities), \cite{lu202435m} developed Online Merging Optimizers that dynamically blend RLHF gradients with SFT model information, preventing overoptimization towards new preferences at the expense of foundational knowledge. Addressing the fundamental limitation of single reward models for diverse user groups, \cite{chakraborty20247ew} proposed MaxMin-RLHF, and \cite{boldi2024d0s} introduced Pareto-Optimal Learning from Preferences with Hidden Context, both aiming for multi-objective or Pareto-optimal solutions that do not overoptimize for an aggregated, potentially biased, preference. Finally, to improve credit assignment and make reward hacking more difficult, methods like \cite{li2024h19} (RED), \cite{chan2024xig} (ABC), and \cite{zhong2024wch} (RTO) provide dense, token-level rewards by redistributing holistic feedback, guiding the learning process with greater precision.

Despite these advancements, perfectly capturing complex, nuanced human preferences remains an open challenge. The inherent subjectivity and context-dependency of human values mean that proxy reward models will always be imperfect. Future research must continue to explore more robust and adaptive reward modeling techniques, develop more sophisticated regularization methods that account for diverse and potentially conflicting objectives, and integrate advanced uncertainty quantification to ensure that LLMs are aligned with genuine human intentions rather than merely optimizing a flawed proxy.