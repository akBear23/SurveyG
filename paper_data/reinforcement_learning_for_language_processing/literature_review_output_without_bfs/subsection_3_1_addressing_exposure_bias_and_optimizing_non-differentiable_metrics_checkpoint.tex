\subsection{Addressing Exposure Bias and Optimizing Non-Differentiable Metrics}

The application of deep reinforcement learning (DRL) marked a pivotal shift in sequence generation tasks, moving beyond the limitations of purely supervised learning to directly optimize for end-to-end, non-differentiable metrics that better reflect overall generation quality. Supervised sequence-to-sequence models, typically trained with maximum likelihood estimation (MLE), suffer from a critical discrepancy known as 'exposure bias' \cite{Ranzato2016, Li2016}. During training, these models are exposed only to ground truth sequences (teacher forcing), but at inference time, they must generate tokens conditioned on their own potentially erroneous previous outputs. This discrepancy leads to a compounding of errors and degraded performance in unseen contexts. Furthermore, token-level accuracy, the typical supervised objective, often fails to correlate with global sequence quality metrics like BLEU, ROUGE, or human judgments of coherence and relevance \cite{Papineni2002, Lin2004}.

To address these challenges, researchers began to frame sequence generation as a sequential decision-making problem, where a policy learns to select the next token to maximize a long-term, non-differentiable reward. Early work on directly optimizing non-differentiable metrics for structured prediction, such as Minimum Error Rate Training (MERT) for machine translation \cite{Och2003}, highlighted the need for such approaches. Conceptually, this aligned with the "learning to search" paradigm for structured prediction \cite{DaumeIII2009}, where a policy makes local decisions to construct a global output. Foundational reinforcement learning algorithms, such as REINFORCE \cite{Williams1992} and more general policy gradient methods with function approximation \cite{Sutton2000}, provided the theoretical bedrock for these applications. An early application of reinforcement learning to structured output prediction, using policy gradient methods to optimize task-specific metrics, further demonstrated the potential of this paradigm beyond traditional control problems \cite{Ranzato2007}.

The advent of deep learning enabled the integration of these RL principles with powerful neural architectures, leading to Deep Reinforcement Learning (DRL) for direct language generation. A significant contribution was the application of policy gradient methods to sequence-to-sequence models, directly optimizing non-differentiable metrics like BLEU \cite{Ranzato2016}. This approach explicitly mitigated exposure bias by allowing the model to explore its own generated sequences during training, thereby learning to recover from its own mistakes. Often, these methods employed a curriculum learning strategy, such as MIXER \cite{Ranzato2016}, which gradually transitioned from MLE to full RL training to stabilize the learning process and prevent catastrophic forgetting. Similarly, deep reinforcement learning was pioneered for dialogue generation, where policy gradient methods were employed to optimize for long-term, human-like dialogue quality metrics such as ease of answering, information flow, and coherence, which are inherently non-differentiable \cite{Li2016}. This work demonstrated how DRL could enable agents to generate more engaging and relevant responses by considering the overall conversational flow rather than just local token accuracy.

The utility of DRL extended to abstractive summarization, where models were trained using policy gradients to maximize ROUGE scores, a standard metric for summarization quality \cite{Paulus2017}. By combining supervised pre-training with reinforcement learning fine-tuning, these models could generate more coherent and relevant summaries, moving beyond extractive methods and improving the overall quality of the generated text. In neural machine translation, DRL fine-tuning was also adopted to directly optimize for BLEU scores, leading to improvements in translation quality in high-profile systems \cite{Wu2016}. Further applications of DRL for abstractive summarization and neural machine translation also explored the use of diversified reward functions to encourage more varied and high-quality outputs, addressing the issue of generic responses often produced by MLE-trained models \cite{Wang2018}. These methods collectively underscored DRL's capacity to generate text that is more globally coherent, relevant, and human-like by directly optimizing for end-to-end quality.

While policy gradient methods offered a powerful solution, they often suffered from high variance during training, which could impede learning stability. To address this, actor-critic algorithms were introduced for sequence prediction \cite{Bahdanau2017}. These methods combine a policy network (actor) with a value network (critic) that estimates the expected reward, thereby reducing variance and improving the stability and efficiency of training. A particularly influential method was Self-Critical Sequence Training (SCST) \cite{Rennie2017}, which used the reward of a baseline sequence (e.g., sampled greedily) to normalize the rewards of other sampled sequences, significantly reducing variance and making policy gradient methods more robust for optimizing non-differentiable metrics like BLEU and ROUGE. This allowed for more robust optimization and further enhanced the ability to mitigate exposure bias in various sequence generation tasks.

Despite these advancements, several significant challenges persisted within these early DRL applications for text generation. Policy gradient methods, including early actor-critic variants, were notoriously difficult to train, often exhibiting high variance and sensitivity to hyperparameters, which could lead to unstable learning and policy collapse \cite{zheng2023c98}. The computational burden of exploration was also substantial, requiring extensive sampling over vast action spaces (vocabulary) and long sequences, leading to excessive memory consumption and prolonged training times \cite{wang2023v62}. Furthermore, a fundamental hurdle was the problem of reward sparsity: meaningful rewards were typically only available at the end of a long sequence, making it difficult for the model to assign credit to individual tokens or intermediate decisions. This "temporal credit assignment problem" hindered efficient learning and made it challenging to discern which specific parts of a generation contributed positively or negatively to the final holistic reward \cite{li2024h19, cao2024lh3}. The inherent difficulty of designing effective, task-specific reward functions that truly captured nuanced aspects of human-like generation quality (e.g., creativity, factual correctness, safety) also remained a significant practical hurdle. These severe limitations in training stability, computational efficiency, and reward engineering ultimately motivated the search for more robust reward signals and training paradigms, paving the way for the subsequent, more sophisticated alignment techniques discussed in later sections.