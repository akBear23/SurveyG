\subsection{Simplified Alignment: Direct Preference Optimization (DPO) and Variants}

The initial success of Reinforcement Learning from Human Feedback (RLHF) in aligning Large Language Models (LLMs) was often hampered by its inherent complexity, computational expense, and training instability, particularly when relying on Proximal Policy Optimization (PPO) \cite{zheng2023c98}. This spurred a significant research direction towards more efficient, stable, and theoretically grounded alignment methods, with Direct Preference Optimization (DPO) emerging as a pivotal advancement \cite{wang2024a3a, srivastava2025gfw}.

\textbf{Direct Preference Optimization (DPO)} \cite{rafailov20239ck} revolutionized LLM alignment by re-parameterizing the RLHF objective into a simple classification loss. This elegant reformulation eliminated the need for an explicit reward model and the complex, multi-stage PPO training process, which typically involves sampling from the LLM during training. DPO achieves this by leveraging a novel parameterization of the reward model within the KL-constrained reward maximization objective, allowing the optimal policy to be extracted in closed form. The resulting objective is a straightforward binary cross-entropy loss, significantly reducing computational overhead, improving training stability, and making preference-based fine-tuning more accessible.

Despite DPO's simplification, the challenge of "reward overoptimization" or "reward hacking" persists, where models exploit imperfections in the implicit reward function, leading to performance degradation \cite{rafailov2024ohd}. Spurious correlations, such as length bias, can still influence DPO's implicit reward, causing models to generate longer, but not necessarily better, responses \cite{singhal2023egk}. To address these robustness concerns, several variants and alternative simplified approaches have emerged. Methods like \cite{zhai20238xc} introduce uncertainty-penalized RLHF using diverse reward LoRA ensembles to prevent policies from venturing into out-of-distribution regions where reward signals are unreliable. Similarly, \cite{dai2025ygq} proposes Behavior-Supported Regularization, which uses value regularization to restrict policy iteration to the in-distribution region of the reward model, thereby mitigating extrapolation errors. \cite{zhang2024esn} further improves this by introducing Adversarial Policy Optimization (AdvPO) with lightweight uncertainty estimation, a distributionally robust approach that avoids the computational overhead of ensemble methods.

Beyond robustness, other methods aim for greater theoretical grounding or broader applicability. \textbf{Self-Play with Adversarial Critic (SPAC)} \cite{ji2024d5f} offers a DPO-like single-timescale algorithm that provides provable convergence guarantees under weak data assumptions, addressing the lack of theoretical robustness in purely empirical methods. \textbf{Value-Incentivized Preference Optimization (VPO)} \cite{cen2024nef} presents a unified framework for both online and offline RLHF, implicitly incorporating uncertainty estimation into a DPO-like direct policy optimization, a feature often missing in DPO. Further simplifying the pipeline, \textbf{Variational Alignment with Re-weighting (VAR)} \cite{du2025zfp} reformulates RLHF as a variational inference problem, leading to a reward-driven re-weighted Supervised Fine-Tuning (SFT) loss. VAR ensures non-negative weights and a stable optimization landscape, directly comparing its enhanced stability and effectiveness against DPO.

The core DPO framework has also been extended to tackle more complex alignment objectives. While basic DPO handles single-objective alignment, real-world applications often demand balancing conflicting goals like helpfulness and safety. \textbf{Bi-Factorial Preference Optimization (BFPO)} \cite{zhang2024b6u} re-parameterizes a multi-objective RLHF problem (safety-helpfulness) into a single supervised learning objective, offering an efficient DPO-like solution for complex trade-offs. The issue of "preference collapse," where KL-regularized RLHF (including DPO) disproportionately favors dominant preferences, is identified by \cite{xiao2024ro4}, who propose Preference Matching to ensure diversity. For enhancing DPO's efficiency and adaptability, \textbf{Weighted Preference Optimization (WPO)} \cite{zhou202469n} improves DPO by dynamically reweighting preference pairs based on their likelihood under the current policy, effectively simulating on-policy learning with off-policy data and addressing the distributional gap.

The pursuit of efficiency and fine-grained control has also led to approaches that bridge DPO's simplicity with the token-level granularity of PPO. \textbf{Reinforced Token Optimization (RTO)} \cite{zhong2024wch} innovatively uses DPO to extract token-wise reward signals, which are then fed into a subsequent PPO training stage. This provides PPO with dense, fine-grained rewards, addressing the sparsity issue of traditional PPO implementations in RLHF. Similarly, \textbf{RED (REward reDistribution)} \cite{li2024h19} assigns token-level rewards by leveraging an off-the-shelf reward model to predict scores at each timestep, transforming sparse sequence rewards into dense, immediate token-level signals without modifying the reward model.

Other methods push the boundaries of simplification even further, moving towards tuning-free or inference-time alignment. \textbf{Contrastive Preference Learning (CPL)} \cite{hejna2023vyy} offers a broader "RL-free" approach, directly learning optimal policies from regret-based preferences using a supervised contrastive objective, applicable to general sequential decision-making. \textbf{Inverse-Q*} \cite{xia2024rab} achieves token-level RL *without preference data or explicit reward models* by directly estimating an optimal policy, representing an extreme simplification. For iterative self-improvement, \textbf{DICE (self-alignment with DPO Implicit rEwards)} \cite{chen2024vkb} enables DPO-tuned models to iteratively self-align without external feedback by bootstrapping with their own implicit reward models. \textbf{Self-Exploring Language Models (SELM)} \cite{zhang2024lqf} introduces an optimistically biased objective for active exploration in online DPO, tackling the limitation of passive exploration.

These simplified and unified approaches collectively represent a significant advancement in the practical application of LLM alignment. While DPO laid the groundwork for efficient preference-based fine-tuning, subsequent research has focused on enhancing its robustness, theoretical guarantees, and applicability to multi-objective and fine-grained control, often by integrating implicit uncertainty, dynamic weighting, or novel re-parameterizations. The ongoing challenge lies in achieving these simplifications without sacrificing the nuanced understanding of human preferences, especially in complex, open-ended generative tasks, and ensuring that models remain robust against various forms of reward hacking and miscalibration.