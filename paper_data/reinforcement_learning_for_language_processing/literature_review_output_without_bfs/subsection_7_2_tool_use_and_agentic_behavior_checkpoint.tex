\subsection{Tool Use and Agentic Behavior}

The development of Large Language Models (LLMs) as intelligent, autonomous agents capable of interacting with external environments and utilizing diverse tools represents a significant frontier in AI research. Reinforcement Learning (RL) is pivotal in enabling these models to learn adaptive strategies, explore complex action spaces, and optimize for task success in dynamic, open-ended environments, thereby fostering more versatile AI systems that extend beyond pure language generation. This subsection explores recent advancements in applying RL to empower LLMs with sophisticated tool-use capabilities, particularly in multimodal contexts and code generation.

A key direction involves enabling Large Vision-Language Models (LVLMs) to dynamically orchestrate visual tools for complex problem-solving. \cite{su20257nq} introduces \textbf{OPENTHINK IMG}, a comprehensive, open-source framework that standardizes visual tool interfaces and facilitates scalable data generation for tool-augmented LVLMs. Central to their approach is \textbf{V-TOOLRL}, a reinforcement learning framework leveraging Group-wise Proximal Policy Optimization (GRPO) to learn adaptive tool invocation policies. This methodology allows LVLMs to move beyond static supervised fine-tuning (SFT) by autonomously exploring and optimizing for task success using feedback from visual tool interactions, demonstrating significant performance gains over SFT baselines and even closed-source models in chart reasoning tasks. However, the initial action planning in their data generation pipeline still relies on powerful external LLMs like GPT-4o, indicating a bootstrapping dependency for high-quality initial trajectories.

Complementing the orchestration of visual tools, another critical area is the RL-driven generation of code that interacts with external solvers and simulators, enabling deep visual reasoning and self-correction. \cite{chen20253a4} addresses the challenge of training Multimodal Large Language Models (MLLMs) for complex visual generative tasks without extensive image-text supervision. Their proposed \textbf{Reasoning-Rendering-Visual-Feedback (RRVF)} framework leverages the "Asymmetry of Verification" principle: it is easier to verify a rendered output against an image than to generate the code from scratch. This closed-loop iterative process involves the MLLM generating code, external tools rendering it, and a "Visual Judge" MLLM providing natural language feedback, all optimized end-to-end using GRPO with a hybrid reward function that includes visual similarity, format correctness, and adaptive tool-use. This innovative approach allows MLLMs to learn complex generative logic directly from pixels through self-correction, circumventing data annotation bottlenecks and even surpassing the performance of the judge model itself. Similar to \cite{su20257nq}, a powerful MLLM (GPT-4o) is critical for generating the visual similarity reward, highlighting a shared reliance on frontier models to bootstrap the learning process.

The papers collectively highlight a significant intellectual trajectory in applying reinforcement learning to enhance the agentic capabilities of multimodal LLMs. Both \cite{su20257nq} and \cite{chen20253a4} represent a paradigm shift from static, supervised fine-tuning towards dynamic, feedback-driven learning for MLLMs, demonstrating that RL (specifically GRPO) can enable more adaptive and robust behaviors. While \cite{su20257nq} focuses on learning how to orchestrate diverse visual tools for problem-solving, \cite{chen20253a4} tackles learning how to generate precise code from visual inputs through iterative self-correction. This synergy allows LLMs to interact with and learn from their environments in increasingly sophisticated ways, fostering emergent reasoning patterns and problem-solving strategies.

Despite these advancements, a common thread of unresolved issues pertains to the initial bootstrapping and feedback generation mechanisms. The reliance on powerful MLLMs (like GPT-4o) for either data generation or reward signal generation across both works points to an interesting tension: while RL enables models to learn autonomously, the initial quality and diversity of the learning signal often still depend on the capabilities of other advanced models. Future directions should explore methods to reduce this dependency, perhaps through more generalized self-supervised or self-generated feedback mechanisms, or by developing more robust and efficient ways to synthesize diverse, high-quality interaction data without relying on frontier models. Further research is also needed to generalize these RL-driven tool-use paradigms to a broader array of complex, open-ended environments and tool types, moving towards truly autonomous and versatile AI agents.