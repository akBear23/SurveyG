\subsection{Theoretical Foundations and Algorithmic Advancements}

The empirical success of Reinforcement Learning from Human Feedback (RLHF) in aligning Large Language Models (LLMs) with complex human preferences has spurred significant theoretical and algorithmic innovations. This subsection delves into the rigorous justifications, formal guarantees, and novel policy optimization techniques that underpin and refine the RLHF paradigm, addressing its inherent challenges in efficiency, stability, and bias. These advancements can be broadly categorized into formalizing RLHF objectives and convergence, developing direct policy optimization methods, and enhancing core RLHF mechanisms through reward shaping, bias mitigation, and stability improvements.

A growing body of work provides theoretical justifications for RLHF's empirical effectiveness and clarifies its relationship to established RL paradigms. \cite{sun2023awe} re-frames RLHF as an Online Inverse Reinforcement Learning (IRL) problem with known transition dynamics, arguing that this property mitigates compounding errors and distributional shifts inherent in purely offline methods like Supervised Fine-Tuning (SFT), thereby explaining RLHF's observed superiority. Further connecting RLHF to fundamental RL theory, \cite{su2025mld} establishes a unified framework (UDRRA) that links various RLHF algorithms, including Direct Preference Optimization (DPO), to actor-critic-based PPO, clarifying their loss function construction, target policy distributions, and the impact of key components on convergence. This theoretical grounding helps demystify DPO's effectiveness despite its seemingly supervised nature. Complementing this, \cite{hejna2023vyy} introduces Contrastive Preference Learning (CPL), a direct policy learning approach grounded in a regret-based preference model and Maximum Entropy RL, offering theoretical guarantees for recovering the optimal policy and demonstrating improved efficiency by transforming the problem into a supervised objective, bypassing explicit reward modeling and traditional RL optimization.

The quest for direct policy optimization from human feedback, circumventing explicit reward model inference, has led to several provably efficient algorithms. \cite{tang2023lop} proposes ZO-RankSGD, the first provably convergent zeroth-order optimization algorithm that learns directly from human ranking oracles for general non-convex functions, achieving a convergence rate of $O(\sqrt{d/T})$. Extending this to more general RL settings, \cite{zhang2024w99} develops Zeroth-Order Policy Gradient (ZPG) and its block-coordinate variant (ZBCPG) for stochastic Markov Decision Processes (MDPs), provably identifying optimal policies from human preferences without reward inference. These methods offer a compelling alternative to reward modeling, particularly when reward function design is challenging, though they often entail higher sample complexity or rely on specific oracle access. For model-free RLHF in episodic MDPs, \cite{zhang20248x9} introduces Batched Sequential Action Dueling (BSAD), which directly identifies optimal policies from batched human preferences, achieving instance-dependent sample complexity without explicit reward inference. For offline LLM alignment, \cite{ji2024d5f} presents Self-Play with Adversarial Critic (SPAC), which offers provable convergence guarantees under weak data assumptions by formulating the problem as a Stackelberg game with an "on-average pessimistic" critic, and importantly, provides a practical DPO-like single-timescale implementation.

Formal convergence guarantees for KL-constrained preference learning have also seen significant advancements. \cite{xiong2023klt} provides a rigorous theoretical analysis of the reverse-KL regularized contextual bandit problem for RLHF, delivering finite-sample guarantees for offline, online, and hybrid settings, and leading to iterative DPO and pessimism-based algorithms. This work highlights the importance of strategic online exploration for performance improvement. Building on this, \cite{zhao202532z} achieves a breakthrough by establishing the first logarithmic regret bounds for online KL-regularized contextual bandits and MDPs, significantly improving theoretical efficiency guarantees (from $O(\sqrt{T})$ to $O(\log T)$) without relying on strong coverage assumptions. This theoretical advancement provides a strong justification for the observed empirical efficiency of KL-regularized RL in LLM alignment, demonstrating its ability to mitigate the "alignment tax" and enhance training stability.

Beyond theoretical foundations, novel policy optimization techniques have emerged to enhance efficiency, stability, and address specific algorithmic biases. A critical challenge in RLHF is the sparse reward signal, where a single scalar reward is given at the end of a long sequence. To combat this, \cite{zhong2024wch} introduces Reinforced Token Optimization (RTO), which re-frames RLHF as an MDP and leverages DPO to extract dense, token-wise reward signals, leading to substantial improvements in PPO's data efficiency and performance (e.g., 7.5-point gain on AlpacaEval 2). Similarly, \cite{chan2024xig} proposes Attention Based Credit (ABC), a method to densify sparse rewards by redistributing the final scalar reward using the reward model's internal attention maps. This approach is notable for being "dense reward for free," requiring no additional modeling or significant computation, and is theoretically proven to be equivalent to potential-based reward shaping, ensuring optimal policy preservation. \cite{li2024h19} further contributes to dense reward generation with REward reDistribution (RED), assigning token-level rewards based on the incremental impact of each token on the reward model's score. While these reward densification methods improve credit assignment, they rely heavily on the internal states of a potentially flawed reward model, risking the propagation of its biases to the token level. Addressing the credit assignment problem from a different angle, \cite{chai2024qal} introduces Reinforcement Learning from Human Feedback with Macro Actions (MA-RLHF), which allows optimization at a higher level of abstraction by treating sequences of tokens as single decision units, accelerating learning and improving performance.

Algorithmic biases in RLHF have also been rigorously analyzed and addressed. \cite{xiao2024ro4} identifies "preference collapse," an inherent algorithmic bias in standard KL-regularized RLHF that disproportionately favors dominant preferences. They propose Preference Matching (PM) RLHF, a novel algorithm that provably aligns LLMs with the full diversity of preference distributions by deriving a new regularizer from an ordinary differential equation, achieving significant improvements in alignment (e.g., 29-41\% reduction in Preference Matching Divergence). To enhance the stability and efficiency of off-policy RLHF, \cite{zhou202469n} introduces Weighted Preference Optimization (WPO), which mitigates the "distributional gap" by dynamically reweighting preference pairs based on their probability under the current policy, effectively simulating on-policy learning and achieving state-of-the-art results (76.7\% win rate on AlpacaEval 2). \cite{cen2024nef} presents Value-Incentivized Preference Optimization (VPO), a unified framework for online and offline RLHF that implicitly incorporates optimism or pessimism through value function regularization, bypassing the intractable explicit uncertainty estimation for LLMs and demonstrating improved empirical performance. Furthermore, \cite{tang2024wt3} empirically dissects the performance gap between online and offline alignment algorithms, concluding that on-policy sampling plays a pivotal role in achieving high generative quality, providing crucial insights into the limitations of offline methods.

Addressing the critical issue of "overoptimization" where LLMs exploit flaws in reward models, \cite{zhai20238xc} proposes Uncertainty-Penalized RLHF (UP-RLHF). This method augments the RLHF objective with an uncertainty regularization term, penalizing rewards based on the estimated uncertainty of a diverse reward LoRA ensemble. This approach effectively mitigates overoptimization and improves alignment with human preferences. Another significant challenge is the "alignment tax," where RLHF can degrade foundational capabilities. \cite{lu202435m} introduces the Online Merging Optimizer, which integrates model merging into each optimization step of RLHF, continuously steering gradients to maximize rewards while maintaining SFT capabilities, thereby mitigating the alignment tax more effectively than offline merging or simple regularization.

Beyond the core RLHF pipeline, advancements also include methods for active exploration and data-agnostic alignment. \cite{zhang2024lqf} proposes Self-Exploring Language Models (SELM), an online direct alignment algorithm with an optimistically biased objective that actively explores the response space, improving instruction-following capabilities without additional computational overhead. This addresses the limitation of passive exploration in DPO-like methods that can lead to local optima. \cite{chen2024vkb} introduces DICE, an iterative self-alignment framework that bootstraps DPO-tuned LLMs using their own implicit reward models, enabling continuous improvement without external feedback and mitigating length bias. Finally, \cite{singla2024dom} presents Dynamic Rewarding with Prompt Optimization (DRPO), a tuning-free, search-based framework for self-alignment at inference time, dynamically adjusting LLM-based rewards for rapid adaptability. Even the initial data collection for reward models is being optimized; \cite{scheid20244oy} proposes Optimal Design for Preference Optimization (ODPO), an offline framework for selecting the most informative pairs for human labeling using optimal design theory, offering theoretical guarantees for minimizing simple regret in reward model training. This highlights a shift towards optimizing the entire RLHF pipeline, from data acquisition to policy optimization.

Despite these significant advancements, several challenges persist. The trade-off between exploration and exploitation remains complex, particularly in the vast and discrete action spaces of LLMs, necessitating continued research into robust active exploration strategies. The inherent algorithmic biases, such as preference collapse \cite{xiao2024ro4}, necessitate continued research into robust regularization techniques that can handle diverse and potentially conflicting preferences. Moreover, while methods like direct policy optimization offer theoretical elegance, their practical implementation often introduces new complexities or computational demands that need careful consideration. Future work will likely focus on developing more adaptive, context-aware, and theoretically sound algorithms that can balance alignment with diverse human preferences, maintain model capabilities, and scale efficiently to novel and complex tasks while offering stronger guarantees of safety and robustness.