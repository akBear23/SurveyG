\subsection{Core Reinforcement Learning Principles}

Reinforcement Learning (RL) provides a formal mathematical framework for an autonomous \textit{agent} to learn optimal behavior through trial-and-error interaction with an \textit{environment}. This sequential decision-making process is typically modeled as a Markov Decision Process (MDP), defined by a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ \cite{sutton2018reinforcement}. Here, $\mathcal{S}$ represents the set of possible \textit{states} describing the environment at any given time, and $\mathcal{A}$ is the set of \textit{actions} the agent can take. $\mathcal{P}(s'|s,a)$ denotes the transition probability from state $s$ to $s'$ upon taking action $a$, and $\mathcal{R}(s,a,s')$ is the immediate numerical \textit{reward signal} received after such a transition. The discount factor $\gamma \in [0,1)$ balances the importance of immediate versus future rewards. In the context of language generation, for instance, the state $s$ can be the sequence of tokens generated thus far, an action $a$ is the choice of the next token, and the reward $\mathcal{R}$ might be a score like ROUGE or human feedback received only upon completion of the full sequence. The agent's overarching objective is to learn an optimal \textit{policy} $\pi(a|s)$, which maps states to probabilities of selecting each action, such that the expected cumulative discounted reward, or \textit{return}, is maximized over time.

Fundamental to understanding RL algorithms are value functions, which quantify the desirability of states or state-action pairs. The state-value function $V^\pi(s)$ represents the expected return starting from state $s$ and following policy $\pi$, while the action-value function $Q^\pi(s,a)$ represents the expected return starting from state $s$, taking action $a$, and then following policy $\pi$. These value functions are intrinsically linked by the Bellman equations, which express a recursive relationship between the value of a state and the values of its successor states, forming the bedrock for many RL algorithms \cite{sutton2018reinforcement}.

Based on these value functions, RL algorithms broadly fall into value-based, policy-based, and actor-critic categories. \textit{Value-based methods}, such as Q-learning \cite{watkins1992q} and SARSA, aim to directly learn the optimal action-value function $Q^*(s,a)$, from which an optimal policy can be derived by selecting actions greedily. While highly effective in tabular settings or with discrete, finite state-action spaces, these methods struggle with the continuous or vast state-action spaces characteristic of modern language models, where explicitly representing or learning all Q-values becomes intractable.

For such complex environments, \textit{policy gradient methods} are often preferred, as they directly optimize the policy parameters by estimating the gradient of the expected return. The REINFORCE algorithm, introduced by \textcite{Williams1992}, is a seminal example. It estimates the policy gradient using Monte Carlo rollouts, where the gradient is proportional to the product of the policy's log-probability and the observed return from that point onward. While theoretically sound, REINFORCE often suffers from high variance in its gradient estimates, which can lead to unstable and slow learning, especially in environments with long horizons or sparse rewards. This high variance stems from using the raw cumulative return as the only feedback signal for each action.

To mitigate the high variance inherent in pure policy gradient methods, \textit{actor-critic architectures} emerged as a powerful alternative. These methods combine two components: an "actor" that learns and updates the policy, and a "critic" that learns a value function to evaluate the actor's actions. The critic's role is to provide a baseline for the policy gradient, typically by estimating the state-value function $V^\pi(s)$ or the action-value function $Q^\pi(s,a)$. By subtracting this baseline from the observed return, the actor can compute an \textit{advantage function}, $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$, which measures how much better an action $a$ is compared to the average action taken in state $s$ under policy $\pi$. This advantage function serves as a more stable and informative signal for updating the policy, significantly reducing the variance of the gradient estimates and leading to more stable and efficient learning \cite{sutton2018reinforcement}. Advanced actor-critic algorithms, such as Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, have become standard in modern deep RL applications due to their balance of stability and performance. PPO achieves this by employing a clipped surrogate objective function that constrains policy updates, preventing destructively large changes and improving training stability. The relationship between such policy optimization methods and direct preference optimization (DPO) has also been a subject of recent analysis, highlighting their shared underlying principles \cite{su2025mld}.

The application of RL principles is inherently challenged by several fundamental problems. The \textit{exploration-exploitation dilemma} requires the agent to balance trying out new, potentially better actions (exploration) with leveraging actions known to yield high rewards (exploitation). An agent that explores too little might get stuck in sub-optimal local optima, while one that explores too much might waste valuable resources. This trade-off is particularly acute in large, complex state-action spaces, such as those encountered in generating diverse and coherent language. Theoretical work on KL-regularized RL, often used in LLM alignment, has shown promise in achieving logarithmic regret, indicating improved sample efficiency in online settings by leveraging the benign optimization landscape induced by KL-regularization \cite{zhao202532z}.

Another critical challenge is the \textit{credit assignment problem}, which arises when rewards are sparse or delayed \cite{sutton2018reinforcement}. In language generation, for instance, a single reward (e.g., a BLEU score or human preference) is often provided only at the end of a long sequence of generated tokens. This makes it difficult to determine which specific tokens or phrases contributed positively or negatively to the final outcome. For example, in a multi-turn dialogue, a single poor word choice early on can derail an entire conversation, but a final conversation-level reward signal provides no direct information on which of the dozens of preceding actions was at fault \cite{shani202491k}.

To address the credit assignment problem, general RL techniques like \textit{reward shaping} and \textit{options} have been developed. Reward shaping involves augmenting the environment's sparse reward function with additional, more frequent, and informative auxiliary rewards. When designed carefully, using potential-based functions, reward shaping can provably preserve the optimal policy of the original MDP, ensuring that the agent's learning objective remains aligned with the true task \cite{ng1999potential}. Recent work has explored leveraging Large Language Models (LLMs) themselves to automate reward shaping by providing dense, intermediate-step rewards \cite{cao2024jcn} or by redistributing holistic sequence-level rewards to token-level signals, interpreting this as a form of potential-based reward shaping \cite{li2024h19}. Similarly, LLMs have been used to decompose tasks into subgoals and assess their achievement, thereby generating auxiliary rewards and facilitating credit assignment in a zero-shot manner \cite{pignatelli2024ffp}.

Options, on the other hand, allow an agent to learn and execute temporally extended actions, effectively creating a hierarchical structure that can simplify the decision-making process and provide more immediate sub-goal feedback \cite{sutton1999between}. This concept has been adapted for language models through "macro actions" (MA-RLHF), where the policy optimization occurs at the level of sequences of tokens rather than individual tokens, significantly reducing the temporal distance between actions and rewards and improving learning efficiency \cite{chai2024qal}. Furthermore, novel approaches like Inverse-Q* aim to perform token-level RL without explicit reward models or preference data, by directly estimating an optimal policy and using its probabilities for fine-grained credit assignment \cite{xia2024rab}. These mechanisms are crucial for making RL tractable in environments where direct feedback is scarce and long-term dependencies are prevalent.

In summary, the core principles of reinforcement learning, encompassing the MDP framework, value functions, policy gradient methods, and actor-critic architectures, provide a robust foundation for sequential decision-making. However, the inherent challenges of exploration-exploitation and credit assignment necessitate sophisticated algorithmic solutions like variance reduction techniques (e.g., advantage functions, PPO's clipped objective) and reward engineering strategies (e.g., reward shaping, macro actions). These foundational concepts and their associated challenges form the essential theoretical and algorithmic background for understanding how RL is applied to complex domains, including language tasks, which often present vast state spaces, long action sequences, and nuanced reward signals.