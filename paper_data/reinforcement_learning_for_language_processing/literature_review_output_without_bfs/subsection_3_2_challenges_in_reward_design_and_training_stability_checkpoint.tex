\subsection*{Challenges in Reward Design and Training Stability}

Early applications of deep reinforcement learning (DRL) to language generation tasks faced significant and persistent challenges, primarily stemming from the inherent difficulty of designing effective reward functions, the problem of reward sparsity, and the instability of policy gradient training methods. These hurdles ultimately motivated the subsequent development of human-centric feedback mechanisms.

The theoretical foundations of reinforcement learning, particularly policy gradient methods like REINFORCE \cite{Williams1992} and their extension with function approximation \cite{Sutton2000}, provided a powerful framework for optimizing policies directly. However, these methods are notoriously susceptible to high variance, making training processes difficult and brittle, especially in complex, high-dimensional action spaces like language generation. Early attempts to apply RL to structured prediction, such as framing sequence labeling as a sequential decision-making problem using REINFORCE \cite{Ranzato2007}, explicitly highlighted these issues, often suffering from high variance and sample inefficiency. Similarly, the "learning to search" paradigm \cite{Daume2009, Chang2015}, which conceptualized structured prediction in NLP as a search process guided by a learned policy, frequently relied on imitation learning from expert demonstrations. While innovative, this approach often struggled to directly optimize for true long-term, non-differentiable rewards and could suffer from exposure bias, where the model performs poorly when encountering states not seen in expert demonstrations.

As DRL began to be directly applied to generative NLP tasks, these challenges became even more pronounced. Pioneering work in dialogue generation using DRL \cite{Li2016} explicitly identified reward sparsity, high variance during training, and the inherent difficulty of designing effective, task-specific reward functions as major obstacles. For instance, defining a differentiable and non-sparse reward signal that accurately captures nuanced qualities like conversational coherence, informativeness, or creativity in a long dialogue sequence proved exceedingly difficult. Feedback was often only available at the end of a generated sequence or dialogue turn, leading to sparse rewards that made credit assignment challenging for the learning agent.

Subsequent efforts to apply DRL to tasks like abstractive summarization \cite{Paulus2017} and further dialogue generation \cite{Li2017} continued to grapple with these issues. While these works demonstrated DRL's potential to optimize non-differentiable metrics like ROUGE scores, the reward functions themselves were often hand-crafted and could be incomplete or misaligned with human perceptions of quality. The challenge of designing rewards that accurately reflect complex output qualities such as factual correctness, safety, or stylistic preferences remained a significant bottleneck. To mitigate the instability of pure policy gradient methods, actor-critic algorithms were introduced for sequence prediction \cite{Bahdanau2017}, aiming to reduce variance and improve training stability by learning a critic to estimate the value function. Despite these advancements, the core problem of reward engineering persisted. For example, optimizing Neural Machine Translation (NMT) with a diversified reward function required careful construction of multi-faceted reward signals to encourage desired output properties beyond simple BLEU scores \cite{Wang2018}.

Comprehensive surveys of DRL in NLP \cite{Gao2019, Zhu2020, Liu2022} consistently highlighted these issues as central to the field's progress. They underscored that exposure bias, reward sparsity, the complexity of reward design, and the inherent training instability of DRL algorithms were pervasive problems across various language generation tasks. These challenges meant that DRL models often required extensive hyperparameter tuning, were sensitive to initialization, and could produce outputs that, while locally coherent, lacked global consistency or failed to meet complex human-centric criteria.

In conclusion, the early applications of deep reinforcement learning to language generation were significantly hampered by the intrinsic difficulties of crafting effective, non-sparse, and differentiable reward functions, especially for subjective qualities like creativity or safety. Coupled with the high variance and instability inherent in policy gradient methods, these practical hurdles made DRL training processes arduous and often brittle. The persistent nature of these challenges ultimately underscored the limitations of purely algorithmic reward design and paved the way for the exploration of more robust and human-aligned reward signals, such as those derived from human feedback, which are now central to the development of modern large language models \cite{yu2023xc4, sahoo2024dp6, holk20243vd}.