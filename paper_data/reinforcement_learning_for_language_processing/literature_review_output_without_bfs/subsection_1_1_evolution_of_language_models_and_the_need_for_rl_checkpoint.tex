\subsection{Evolution of Language Models and the Need for RL}

The landscape of natural language processing has undergone a profound transformation, evolving from rudimentary statistical methods to highly sophisticated neural architectures. This evolution has progressively endowed language models with remarkable generative capabilities, yet it has simultaneously exposed a critical gap between raw generative power and the nuanced, human-centric objectives essential for real-world utility and safety.

Early language models were predominantly statistical, relying on N-gram probabilities to predict the next word based on a limited preceding context. While foundational, these models suffered from severe sparsity issues and an inability to capture long-range dependencies. The advent of Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs), marked a significant leap, enabling models to process sequential data and learn more complex contextual patterns. However, RNNs struggled with parallelization and very long sequences. The paradigm truly shifted with the introduction of the Transformer architecture \cite{community_16}, which leveraged self-attention mechanisms to process entire sequences in parallel and capture arbitrarily long-range dependencies efficiently. This innovation paved the way for the development of massive pre-trained language models (LLMs) like GPT-3 \cite{community_28}, which exhibited emergent capabilities such as few-shot learning, where models can perform new tasks with minimal examples, and in-context reasoning, allowing them to follow complex instructions provided within the prompt \cite{layer_1}.

Despite these monumental advancements, traditional training objectives, primarily maximum likelihood estimation (MLE) through next-token prediction, inherently optimize for fluency and coherence based on the statistical properties of the training data distribution. While effective for generating grammatically correct and contextually plausible text, this objective does not intrinsically align LLMs with subjective human preferences such as helpfulness, harmlessness, honesty, or precise instruction-following \cite{layer_1, community_28}. The core limitation lies in the fact that MLE aims to reproduce the observed data distribution, which may contain biases, undesirable patterns, or simply lack explicit signals for complex human values. Consequently, an LLM trained solely on MLE might generate fluent but factually incorrect information (hallucinations), produce biased or toxic content, or fail to adhere to nuanced user instructions, even when its generative capacity is high. The escalating scale and complexity of LLMs, coupled with their increasing deployment in user-facing applications, magnified this "alignment problem." Larger models, while more capable, also possess a greater potential to generate convincing but misaligned outputs, making the discrepancy between raw generative power and desired behavioral alignment critically apparent. Achieving objectives like "be helpful and harmless" or "answer truthfully" requires optimizing for criteria that are often subjective, non-differentiable, and cannot be easily encoded into a simple next-token prediction loss. This critical need necessitated a paradigm shift from mere next-token prediction towards optimizing for these nuanced, often non-differentiable, human-centric objectives.

This fundamental mismatch between the capabilities of large generative models and the intricate, often implicit, expectations of human users set the stage for the integration of reinforcement learning (RL) as a mechanism to bridge this critical gap. RL provides a powerful framework for optimizing policies against arbitrary reward signals, even those derived from complex human feedback or non-differentiable metrics. By allowing models to learn through trial and error, guided by a reward function that encapsulates desired behaviors, RL offers a pathway to instill human values and precise instruction-following capabilities into LLMs, thereby enhancing their real-world utility, safety, and trustworthiness. This foundational shift from purely generative optimization to behavior-driven alignment marks a pivotal moment in the development of language AI.