\subsection{Reinforcement Learning from AI Feedback (RLAIF) and Self-Alignment}

The burgeoning capabilities of Large Language Models (LLMs) have been significantly propelled by Reinforcement Learning from Human Feedback (RLHF), yet its reliance on costly and unscalable human annotation presents a major bottleneck \cite{lambert2023bty, lee2023mrw}. This challenge has spurred extensive research into methods that replace or augment human feedback with AI-generated feedback, such as Reinforcement Learning from AI Feedback (RLAIF), and self-alignment strategies where models learn from their own generated content or internal mechanisms. The overarching goal is to create more autonomous and scalable alignment pipelines that reduce human supervision while maintaining or enhancing alignment quality and robustness.

A direct approach to scaling alignment is **Reinforcement Learning from AI Feedback (RLAIF)**, where LLMs themselves act as evaluators. \textcite{lee2023mrw} provided foundational work, demonstrating that RLAIF can achieve performance comparable to RLHF across various tasks, even enabling self-improvement using same-size or same-checkpoint AI labelers. They introduced Direct-RLAIF (d-RLAIF), which bypasses explicit reward model training by having an LLM directly provide reward scores during the RL phase, mitigating "reward model staleness." However, relying on AI for feedback introduces its own set of challenges. \textcite{saito2023zs7} critically identified "verbosity bias" in LLM preference labeling, showing that AI evaluators often prefer longer responses regardless of quality, a discrepancy with human preferences that can lead to suboptimal alignment. To address such limitations, \textcite{li2024ev4} proposed Hybrid Reinforcement Learning from AI Feedback (HRLAIF), a multi-stage AI preference labeling framework. HRLAIF guides AI to make more accurate judgments on complex attributes like factual correctness through structured processes, such as correctness verification and AI-driven red teaming, thereby improving both helpfulness and harmlessness while maintaining cost-efficiency.

Beyond directly replacing human annotators, **self-alignment strategies** aim to make models learn from their own generated content or internal signals, further reducing human supervision. \textcite{sun20238m7} pioneered principle-driven self-alignment from scratch with minimal human supervision. Their `SELF-ALIGN` framework uses a small set of human-written principles and in-context exemplars to guide an LLM's internal reasoning, followed by a "principle engraving" step that internalizes these rules into the model's parameters, enabling aligned generation without explicit prompting.

Another avenue for self-alignment involves leveraging models' internal feedback mechanisms. \textcite{cao2024lh3} addressed the sparse reward problem in text generation by using LLM critique to generate dense, token-level intrinsic rewards (RELC), directly improving the RL policy optimization step within the RLHF pipeline. Expanding on this, \textcite{liu20251xv} introduced RISE (Reinforcing Reasoning with Self-Verification), an online RL framework that simultaneously trains LLMs in problem-solving and self-verification using verifiable rewards. This enables models to learn to critique their own reasoning processes, enhancing robustness. However, the efficacy of naive internal feedback is not without limits. \textcite{zhang2025d44} critically examined Reinforcement Learning from Internal Feedback (RLIF) using signals like self-certainty and entropy. They theoretically and empirically demonstrated that these internal feedback signals primarily minimize policy entropy, which can initially boost base models but leads to performance degradation with prolonged training due to a reduction in "transitional words" crucial for multi-step reasoning. This highlights that RLIF is not a "free lunch" and requires careful application.

Further advancements in self-alignment leverage implicit rewards and active exploration. \textcite{chen2024vkb} introduced DICE (self-alignment with DPO Implicit Rewards), an iterative self-alignment method that bootstraps DPO-tuned LLMs by using their *own implicit reward models* to generate new preference data. This eliminates the need for external feedback in subsequent alignment rounds, making iterative DPO more self-contained. Complementing this, \textcite{zhang2024lqf} proposed Self-Exploring Language Models (SELM), an active preference elicitation method for online alignment. SELM uses an optimistically biased, reward model-free objective to guide LLMs to actively explore potentially high-reward regions of the response space, addressing the limitations of passive exploration in traditional online alignment. Taking autonomy a step further, \textcite{singla2024dom} presented Dynamic Rewarding with Prompt Optimization (DRPO), a tuning-free, search-based framework where LLMs dynamically adjust rewards and optimize prompts at inference time for self-alignment, entirely without model training or human supervision.

Despite these advancements, several challenges persist. The robustness of AI-generated feedback and self-alignment to adversarial attacks remains a concern. \textcite{baumgrtner2024gu4} demonstrated RLHF's vulnerability to "preference poisoning" using naturalistic, stealthy data, a risk that RLAIF or self-alignment might inherit if the underlying "AI judge" or self-evaluation mechanism is compromised. Similarly, \textcite{zhang2023pbi} showed that alignment mechanisms do not prevent the misuse of *open-sourced* LLMs through "model hacking," forcing the generation of undesired content, which poses a fundamental security challenge for autonomous alignment. Moreover, alignment processes, whether human or AI-driven, often introduce trade-offs. \textcite{kirk20230it} found that RLHF improves out-of-distribution generalization but reduces output diversity, while \textcite{mohammadi20241pk} empirically showed that debiasing LLMs with RLHF can significantly reduce their creativity and lead to "attractor states" in their output space. This "objective mismatch," where numerical objectives decouple from true human intent, can lead to unintended behaviors like refusals or verbosity, as highlighted by \textcite{lambert2023c8q}.

Addressing these issues requires a deeper understanding of the underlying mechanisms and more robust methodologies. \textcite{xiao2024ro4} identified "preference collapse" as an algorithmic bias in standard KL-regularized RLHF, where minority preferences are disregarded, proposing Preference Matching to ensure diversity. \textcite{wang20247pw} tackled reward model interpretability and steerability through multi-objective reward modeling and Mixture-of-Experts, mitigating issues like reward hacking and verbosity bias, which is crucial for reliable AI feedback. Furthermore, exploring novel feedback signals, such as implicit human cues like gaze data, which \textcite{lpezcardona20242pt} showed can be integrated into reward models using synthetic features for scalability, offers new avenues for human-like feedback without direct human effort. The application of LLM-enhanced RLHF is also extending beyond language to complex domains like autonomous driving, as demonstrated by \textcite{sun2024nor}, showcasing the broad potential of feedback-driven alignment.

In conclusion, the shift towards RLAIF and self-alignment represents a critical intellectual trajectory in LLM alignment, driven by the need for scalability and autonomy. While significant progress has been made in developing AI-driven feedback mechanisms and self-improving models, the field continues to grapple with fundamental challenges related to bias, robustness, the trade-off between alignment and creativity, and the computational efficiency of these complex systems. Future research must focus on developing more theoretically grounded, robust, and interpretable autonomous alignment strategies that can consistently deliver high-quality, safe, and diverse LLM behaviors across an ever-expanding range of applications.