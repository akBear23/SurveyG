[
  {
    "success": true,
    "doc_id": "31b6f759917ac52bfb8c523c7e10a575",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Convolution-based Graph Neural Networks (GNNs) characteristically suffer from several fundamental technical problems: limited expressiveness (cannot exceed Weisfeiler-Lehman (WL) isomorphism test, struggle with cycle sizes and diameters), over-smoothing (node representations become indistinguishable with more layers), and over-squashing (difficulty learning long-range dependencies due to information bottleneck).\n    *   These issues limit their applicability and performance in complex social and physical modeling tasks, and they also require specialized sparse kernels for efficient computation.\n\n*   **Related Work & Positioning**\n    *   **Walk-based GNNs**: Existing walk-based GNNs (e.g., RAW-GNN, CRaWl, AWE, AgentNet) also use random walks for node neighborhood representation or structural encoding. However, many are de facto convolutional, incorporate walks only as features in message passing, or focus primarily on expressiveness without addressing over-smoothing and over-squashing. Most are not applicable to or tested on node-level tasks.\n    *   **Random Walk Kernel GNNs**: Methods like RWK and GSN use subgraph counting or kernel representations, often arguing for superior expressiveness in specific cases, but are typically much slower and also do not address over-smoothing/squashing.\n    *   **Methods to alleviate over-smoothing**: Approaches like DropEdge (stochastic regularization), graph rewiring (e.g., GPR-GNN), constant-energy methods, and Residual GNNs (e.g., GCNII) attempt to mitigate over-smoothing. However, they often have similar or compromised expressiveness compared to basic GCNs, can be less robust, or the non-decreasing energy doesn't always translate to better performance.\n    *   **Positioning**: \\cite{wang2024oi8} proposes a *non-convolutional* GNN that jointly remedies the issues of limited expressiveness, over-smoothing, and over-squashing, and is shown to be faster and more scalable than existing convolutional and many walk-based GNNs, while achieving competitive performance on both node- and graph-level tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper introduces the **Random Walk with Unifying Memory (RUM) neural network**, an entirely convolution-free graph learning module.\n    *   **Random Walks**: For each node, RUM stochastically samples a finite-length random walk. The trajectory of node embeddings along this walk forms the \"semantic representation\" ($\\omega_x(w)$).\n    *   **Anonymous Experiment**: A novel function, termed \"anonymous experiment\" ($\\omega_u(w)$), is used to describe the *topological environment* of a walk. It records the first unique occurrence of a node in a walk, effectively labeling nodes based on their unique traversal order.\n    *   **Unifying Memory**: An RNN (specifically, GRU) is employed to merge these semantic ($\\omega_x(w)$) and topological ($\\omega_u(w)$) features into a \"unifying memory\" representation $h(w)$. This RNN naturally encodes the inductive bias that nodes closer to the origin have a stronger impact.\n    *   **Node/Graph Representations**: Node representations ($\\psi(v)$) are formed by averaging the unifying memories of random walks terminating at that node. Graph representations ($\\Psi(G)$) are then summed from node representations.\n    *   **Non-Convolutional Nature**: Unlike most GNNs, RUM does not use any convolution operators (Equation 1) or iteratively pool representations within local neighborhoods. It relies solely on random walk trajectories and their processing by RNNs.\n\n*   **Key Technical Contributions**\n    *   **Novel Architecture**: Introduction of RUM, a fundamentally non-convolutional GNN architecture that processes graph information via random walks and RNNs.\n    *   **Anonymous Experiment**: A novel method for encoding the topological environment of a random walk, crucial for RUM's expressiveness.\n    *   **Theoretical Insights**:\n        *   **Enhanced Expressiveness**: Theoretically shown to be more expressive than the Weisfeiler-Lehman (WL) isomorphism test and thus convolutional GNNs, capable of distinguishing non-isomorphic graphs that WL-equivalent GNNs cannot (e.g., cycle sizes, radius).\n        *   **Alleviation of Over-smoothing**: Theoretically and experimentally verified that RUM attenuates over-smoothing, with its expected Dirichlet energy not diminishing even with long walks, unlike convolutional GNNs.\n        *   **Alleviation of Over-squashing**: Shown to decay slower in inter-node Jacobian compared to convolutional counterparts, mitigating the over-squashing problem.\n    *   **System Design**: Parameter sharing is natural with RNNs, leading to a constant number of parameters regardless of neighborhood size. RUM is naturally compatible with mini-batching and scalable to huge graphs without requiring all neighbors to be present.\n\n*   **Experimental Validation**\n    *   **Tasks**: Evaluated on a variety of real-world node-level (e.g., classification, regression) and graph-level (e.g., classification, regression) tasks.\n    *   **Performance**: Achieves competitive performance against state-of-the-art convolutional GNNs and outperforms existing walk-based GNNs on most graph-level tasks.\n    *   **Robustness**: Demonstrated to be robust.\n    *   **Efficiency**: Memory-efficient and scalable, capable of handling large graphs.\n    *   **Speed**: Empirically verified to be faster than even the simplest convolutional GNNs (Figure 4), with runtime complexity of O(|V|lkD) which is agnostic to the number of edges |E|.\n    *   **Illustrative Experiments**: Confirmed theoretical claims regarding expressiveness (Figure 3) and alleviation of over-smoothing (Figure 2).\n\n*   **Limitations & Scope**\n    *   **Theoretical Assumptions**: Theoretical proofs for expressiveness and over-smoothing alleviation rely on assumptions such as universal and injective functions for $\\phi_x, \\phi_u, f$, and connected, unweighted, undirected graphs.\n    *   **Non-contractive Mapping**: While the theory suggests non-contractive mappings for $f$ would guarantee alleviation of over-smoothing, the GRU is used \"out-of-box without constraining it to be explicitly non-contractive.\"\n    *   **Over-squashing**: While alleviating over-squashing, RUM does not fully address the information bottleneck with exponentially growing reception fields, but rather improves the gradient flow from the aggregation function.\n    *   **Scope**: The current analysis focuses on finite-long random walks.\n\n*   **Technical Significance**\n    *   \\cite{wang2024oi8} presents a significant advancement by designing a GNN architecture \"entirely free of convolution operators,\" offering a compelling alternative to prevalent convolution-based GNNs.\n    *   It provides a joint remedy to the long-standing fundamental pathologies of GNNs: limited expressiveness, over-smoothing, and over-squashing.\n    *   The introduction of the \"anonymous experiment\" and the use of RNNs to unify semantic and topological features from random walks represents a novel paradigm for graph representation learning.\n    *   Its demonstrated competitive performance, robustness, scalability, and speed, combined with strong theoretical underpinnings, position RUM as a promising direction for future research in graph representation learning, especially for applications requiring deep GNNs or handling large, complex graphs.",
    "intriguing_abstract": "Convolutional Graph Neural Networks (GNNs) are fundamentally constrained by limited expressiveness (bounded by the Weisfeiler-Lehman test), over-smoothing, and over-squashing, hindering their application to complex, deep graph learning tasks. We introduce the **Random Walk with Unifying Memory (RUM) neural network**, a novel, entirely *non-convolutional* GNN architecture that jointly remedies these pervasive issues.\n\nRUM leverages stochastic finite-length random walks, processing their semantic trajectories and a novel \"anonymous experiment\" topological encoding through a Recurrent Neural Network (RNN) to form a unifying memory representation. This unique approach theoretically demonstrates superior expressiveness beyond the WL test, effectively attenuates over-smoothing by maintaining non-diminishing expected Dirichlet energy, and mitigates over-squashing by improving inter-node gradient flow.\n\nEmpirically, RUM achieves competitive performance on diverse node- and graph-level tasks, while proving significantly faster, more memory-efficient, and scalable than existing convolutional and walk-based GNNs. RUM represents a paradigm shift in graph representation learning, offering a robust and efficient solution for deep learning on large-scale graphs.",
    "keywords": [
      "Random Walk with Unifying Memory (RUM) neural network",
      "non-convolutional GNN",
      "limited expressiveness",
      "over-smoothing",
      "over-squashing",
      "anonymous experiment",
      "random walks",
      "RNNs (GRU)",
      "Weisfeiler-Lehman (WL) isomorphism test",
      "enhanced expressiveness",
      "alleviation of over-smoothing",
      "alleviation of over-squashing",
      "scalability and speed",
      "node-level and graph-level tasks",
      "joint remedy for GNN pathologies"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf",
    "citation_key": "wang2024oi8",
    "metadata": {
      "title": "Non-convolutional Graph Neural Networks",
      "authors": [
        "Yuanqing Wang",
        "Kyunghyun Cho"
      ],
      "published_date": "2024",
      "abstract": "Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Convolution-based Graph Neural Networks (GNNs) characteristically suffer from several fundamental technical problems: limited expressiveness (cannot exceed Weisfeiler-Lehman (WL) isomorphism test, struggle with cycle sizes and diameters), over-smoothing (node representations become indistinguishable with more layers), and over-squashing (difficulty learning long-range dependencies due to information bottleneck).\n    *   These issues limit their applicability and performance in complex social and physical modeling tasks, and they also require specialized sparse kernels for efficient computation.\n\n*   **Related Work & Positioning**\n    *   **Walk-based GNNs**: Existing walk-based GNNs (e.g., RAW-GNN, CRaWl, AWE, AgentNet) also use random walks for node neighborhood representation or structural encoding. However, many are de facto convolutional, incorporate walks only as features in message passing, or focus primarily on expressiveness without addressing over-smoothing and over-squashing. Most are not applicable to or tested on node-level tasks.\n    *   **Random Walk Kernel GNNs**: Methods like RWK and GSN use subgraph counting or kernel representations, often arguing for superior expressiveness in specific cases, but are typically much slower and also do not address over-smoothing/squashing.\n    *   **Methods to alleviate over-smoothing**: Approaches like DropEdge (stochastic regularization), graph rewiring (e.g., GPR-GNN), constant-energy methods, and Residual GNNs (e.g., GCNII) attempt to mitigate over-smoothing. However, they often have similar or compromised expressiveness compared to basic GCNs, can be less robust, or the non-decreasing energy doesn't always translate to better performance.\n    *   **Positioning**: \\cite{wang2024oi8} proposes a *non-convolutional* GNN that jointly remedies the issues of limited expressiveness, over-smoothing, and over-squashing, and is shown to be faster and more scalable than existing convolutional and many walk-based GNNs, while achieving competitive performance on both node- and graph-level tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper introduces the **Random Walk with Unifying Memory (RUM) neural network**, an entirely convolution-free graph learning module.\n    *   **Random Walks**: For each node, RUM stochastically samples a finite-length random walk. The trajectory of node embeddings along this walk forms the \"semantic representation\" ($\\omega_x(w)$).\n    *   **Anonymous Experiment**: A novel function, termed \"anonymous experiment\" ($\\omega_u(w)$), is used to describe the *topological environment* of a walk. It records the first unique occurrence of a node in a walk, effectively labeling nodes based on their unique traversal order.\n    *   **Unifying Memory**: An RNN (specifically, GRU) is employed to merge these semantic ($\\omega_x(w)$) and topological ($\\omega_u(w)$) features into a \"unifying memory\" representation $h(w)$. This RNN naturally encodes the inductive bias that nodes closer to the origin have a stronger impact.\n    *   **Node/Graph Representations**: Node representations ($\\psi(v)$) are formed by averaging the unifying memories of random walks terminating at that node. Graph representations ($\\Psi(G)$) are then summed from node representations.\n    *   **Non-Convolutional Nature**: Unlike most GNNs, RUM does not use any convolution operators (Equation 1) or iteratively pool representations within local neighborhoods. It relies solely on random walk trajectories and their processing by RNNs.\n\n*   **Key Technical Contributions**\n    *   **Novel Architecture**: Introduction of RUM, a fundamentally non-convolutional GNN architecture that processes graph information via random walks and RNNs.\n    *   **Anonymous Experiment**: A novel method for encoding the topological environment of a random walk, crucial for RUM's expressiveness.\n    *   **Theoretical Insights**:\n        *   **Enhanced Expressiveness**: Theoretically shown to be more expressive than the Weisfeiler-Lehman (WL) isomorphism test and thus convolutional GNNs, capable of distinguishing non-isomorphic graphs that WL-equivalent GNNs cannot (e.g., cycle sizes, radius).\n        *   **Alleviation of Over-smoothing**: Theoretically and experimentally verified that RUM attenuates over-smoothing, with its expected Dirichlet energy not diminishing even with long walks, unlike convolutional GNNs.\n        *   **Alleviation of Over-squashing**: Shown to decay slower in inter-node Jacobian compared to convolutional counterparts, mitigating the over-squashing problem.\n    *   **System Design**: Parameter sharing is natural with RNNs, leading to a constant number of parameters regardless of neighborhood size. RUM is naturally compatible with mini-batching and scalable to huge graphs without requiring all neighbors to be present.\n\n*   **Experimental Validation**\n    *   **Tasks**: Evaluated on a variety of real-world node-level (e.g., classification, regression) and graph-level (e.g., classification, regression) tasks.\n    *   **Performance**: Achieves competitive performance against state-of-the-art convolutional GNNs and outperforms existing walk-based GNNs on most graph-level tasks.\n    *   **Robustness**: Demonstrated to be robust.\n    *   **Efficiency**: Memory-efficient and scalable, capable of handling large graphs.\n    *   **Speed**: Empirically verified to be faster than even the simplest convolutional GNNs (Figure 4), with runtime complexity of O(|V|lkD) which is agnostic to the number of edges |E|.\n    *   **Illustrative Experiments**: Confirmed theoretical claims regarding expressiveness (Figure 3) and alleviation of over-smoothing (Figure 2).\n\n*   **Limitations & Scope**\n    *   **Theoretical Assumptions**: Theoretical proofs for expressiveness and over-smoothing alleviation rely on assumptions such as universal and injective functions for $\\phi_x, \\phi_u, f$, and connected, unweighted, undirected graphs.\n    *   **Non-contractive Mapping**: While the theory suggests non-contractive mappings for $f$ would guarantee alleviation of over-smoothing, the GRU is used \"out-of-box without constraining it to be explicitly non-contractive.\"\n    *   **Over-squashing**: While alleviating over-squashing, RUM does not fully address the information bottleneck with exponentially growing reception fields, but rather improves the gradient flow from the aggregation function.\n    *   **Scope**: The current analysis focuses on finite-long random walks.\n\n*   **Technical Significance**\n    *   \\cite{wang2024oi8} presents a significant advancement by designing a GNN architecture \"entirely free of convolution operators,\" offering a compelling alternative to prevalent convolution-based GNNs.\n    *   It provides a joint remedy to the long-standing fundamental pathologies of GNNs: limited expressiveness, over-smoothing, and over-squashing.\n    *   The introduction of the \"anonymous experiment\" and the use of RNNs to unify semantic and topological features from random walks represents a novel paradigm for graph representation learning.\n    *   Its demonstrated competitive performance, robustness, scalability, and speed, combined with strong theoretical underpinnings, position RUM as a promising direction for future research in graph representation learning, especially for applications requiring deep GNNs or handling large, complex graphs.",
      "keywords": [
        "Random Walk with Unifying Memory (RUM) neural network",
        "non-convolutional GNN",
        "limited expressiveness",
        "over-smoothing",
        "over-squashing",
        "anonymous experiment",
        "random walks",
        "RNNs (GRU)",
        "Weisfeiler-Lehman (WL) isomorphism test",
        "enhanced expressiveness",
        "alleviation of over-smoothing",
        "alleviation of over-squashing",
        "scalability and speed",
        "node-level and graph-level tasks",
        "joint remedy for GNN pathologies"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **presents new methods, algorithms, or systems:** the abstract explicitly states, \"here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (rum) neural network...\" this is the core characteristic of a technical paper.\n2.  **discusses technical problem and proposed solution:** the introduction details the \"common pathologies\" (technical problems) of existing convolution-based gnns, setting the stage for the proposed rum neural network as a solution.\n3.  **supporting evidence:** while the paper also includes strong theoretical analysis (\"theoretically show and experimentally verify that rum attenuates the aforementioned symptoms and is more expressive than the weisfeiler-lehman (wl) isomorphism test\") and empirical evaluation (\"on a variety of node- and graph-level classification and regression tasks, rum not only achieves competitive performance...\"), these are typically components used to validate and demonstrate the effectiveness of a *new technical contribution*. the primary contribution is the design and development of the rum neural network."
    },
    "file_name": "9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf"
  },
  {
    "success": true,
    "doc_id": "bdb83174545fa6f7851b98c43a4441e3",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Graph Neural Networks (GNNs) perform poorly on graphs exhibiting *heterophily* (where connected nodes tend to have dissimilar features or labels), due to their inherent homophily assumption and smoothing operations.\n    *   **Importance & Challenge**: Heterophilous graphs are common in real-world scenarios (e.g., protein structures, ecological food webs). Existing solutions that amplify a node's neighborhood with multi-hop neighbors face significant challenges:\n        *   Determining personalized optimal neighborhood sizes for different nodes.\n        *   Ignoring homophilous nodes that fall outside the chosen multi-hop neighborhood.\n        *   A straightforward global aggregation approach would lead to prohibitive quadratic or cubic time complexity and could inadvertently include more heterophilous nodes.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   Some methods leverage both low-pass and high-pass convolutional filters (e.g., GPR-GNN, ACM-GCN) to adapt to homophily/heterophily.\n        *   Other methods enlarge the node neighborhood size to include more distant homophilous nodes (e.g., H2GCN, WRGAT), often focusing on higher-order neighbors or transforming the graph.\n    *   **Limitations of Previous Solutions**: These methods struggle with adaptively setting neighborhood sizes and may still miss globally distant but homophilous nodes. They often don't fully leverage the *global* homophily present across the entire graph.\n    *   **Positioning**: GloGNN differentiates itself by performing node neighborhood aggregation from the *whole set of nodes* in the graph, aiming to capture global homophily, which is distinct from merely expanding to multi-hop neighbors.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes GloGNN and GloGNN++ models that generate node embeddings by aggregating information from *all global nodes* in the graph.\n        *   In each layer, a **coefficient matrix Z(l)** is learned to capture correlations between nodes. Z(l)ij describes the importance of node xj to xi.\n        *   This matrix is derived from an optimization problem that characterizes each node by all other nodes, inspired by the linear subspace model.\n        *   The initial node embedding H(0) is decoupled from aggregation and generated by MLPs from both node features and adjacency matrix, then combined.\n        *   A skip connection is used: H(l) = (1-beta)Z(l)H(l) + H(0) + O(l).\n        *   Z(l) is regularized by nodes' multi-hop reachabilities (sum of k * A^k) to incorporate both feature and topology similarity.\n    *   **Novelty/Difference**:\n        *   **Global Aggregation with Learned Coefficients**: Instead of fixed neighborhoods, GloGNN learns a coefficient matrix for *all* nodes, effectively finding global homophily.\n        *   **Signed Coefficient Matrix**: Z(l) allows signed values, enabling it to assign large positive coefficients to homophilous nodes and small positive or negative coefficients to heterophilous ones, implicitly combining low-pass and high-pass filtering.\n        *   **Closed-Form Solution**: The optimization problem for Z(l) has a closed-form solution, simplifying its computation.\n        *   **Linear Time Complexity Acceleration**: A crucial innovation is transforming the aggregation equation to avoid direct computation of Z(l) and reordering matrix multiplications. This reduces the time complexity from cubic/quadratic (O(n^3) or O(n^2c)) to *linear* (O(k2n)) by leveraging matrix properties, the small number of labels (c), and the sparsity of the adjacency matrix.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of GloGNN and GloGNN++ models for effective and efficient GNNs on heterophilous graphs.\n    *   **Theoretical Insights**: Proof that both the learned coefficient matrix Z(l) and the generated node embedding matrix H(l+1) exhibit a **grouping effect**. This means nodes with similar features and local structures (even if distant) will have similar coefficient vectors and representation vectors, explaining the model's effectiveness.\n    *   **System Design/Architectural Innovations**: A novel framework that decouples feature transformation, learns global node correlations via a signed coefficient matrix, and performs aggregation with linear time complexity.\n    *   **Algorithmic Efficiency**: Development of an acceleration technique that reduces the computational complexity of global aggregation from cubic/quadratic to linear time.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to compare GloGNN and GloGNN++ against other methods.\n    *   **Competitors**: Compared against 11 other state-of-the-art GNN models.\n    *   **Datasets**: Evaluated on 15 benchmark datasets, covering a wide range of domains, scales, and graph heterophilies.\n    *   **Key Performance Metrics & Results**: The experimental results demonstrate that GloGNN and GloGNN++ achieve **superior performance** (implying higher accuracy or relevant task metrics) and are also **very efficient** (due to the linear time complexity).\n\n6.  **Limitations & Scope**\n    *   The paper primarily focuses on addressing the limitations of *previous* methods (e.g., difficulty in setting personalized neighborhood sizes, ignoring distant homophilous nodes, computational cost of naive global aggregation).\n    *   The provided text does not explicitly state technical limitations or assumptions *of GloGNN itself*, beyond the general scope of node classification on heterophilous graphs. The model's effectiveness is theoretically explained by the grouping effect, and its efficiency is a key design goal.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GloGNN significantly advances GNNs for heterophilous graphs by providing a principled and efficient way to leverage *global homophily*. It moves beyond local or multi-hop aggregation limitations by considering all nodes while maintaining computational feasibility.\n    *   **Potential Impact**:\n        *   Enables the application of GNNs to a broader range of real-world heterophilous datasets where traditional GNNs struggle.\n        *   The theoretical proof of the grouping effect provides a deeper understanding of why such global aggregation strategies can be effective.\n        *   The linear time complexity for global aggregation opens avenues for scaling GNNs to very large graphs, which is a critical challenge in the field.\n        *   The concept of a learned, signed coefficient matrix could inspire future research into more adaptive and expressive graph convolution operations.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) notoriously struggle with heterophilous graphs, where connected nodes often exhibit dissimilar features, due to their inherent homophily bias and localized aggregation. Existing multi-hop solutions are limited by rigid neighborhood definitions and fail to capture crucial globally distant homophilous nodes, often at prohibitive computational expense. We introduce GloGNN and GloGNN++, a novel framework that fundamentally redefines GNN aggregation by leveraging *global homophily* across the entire graph.\n\nOur models learn a dynamic, *signed coefficient matrix* for all nodes, effectively capturing intricate global correlations and implicitly performing both low-pass and high-pass filtering. This innovative approach is theoretically underpinned by a proven \"grouping effect,\" demonstrating that nodes with similar features and local structures converge to similar representations, regardless of their direct connectivity. Crucially, we achieve this global aggregation with a breakthrough *linear time complexity* (O(k2n)), transforming what would typically be a quadratic or cubic operation. Extensive experiments across 15 diverse datasets confirm GloGNN's superior performance and efficiency against state-of-the-art methods. This work significantly expands GNN applicability to complex real-world heterophilous networks, offering a scalable and principled solution for robust node classification.",
    "keywords": [
      "Heterophilous graphs",
      "Graph Neural Networks (GNNs)",
      "Homophily assumption",
      "GloGNN",
      "Global aggregation",
      "Learned coefficient matrix",
      "Signed coefficient matrix",
      "Linear time complexity",
      "Grouping effect",
      "Node embeddings",
      "Computational efficiency",
      "Superior performance",
      "State-of-the-art advancement"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf",
    "citation_key": "li2022315",
    "metadata": {
      "title": "Finding Global Homophily in Graph Neural Networks When Meeting Heterophily",
      "authors": [
        "Xiang Li",
        "Renyu Zhu",
        "Yao Cheng",
        "Caihua Shan",
        "Siqiang Luo",
        "Dongsheng Li",
        "Wei Qian"
      ],
      "published_date": "2022",
      "abstract": "We investigate graph neural networks on graphs with heterophily. Some existing methods amplify a node's neighborhood with multi-hop neighbors to include more nodes with homophily. However, it is a significant challenge to set personalized neighborhood sizes for different nodes. Further, for other homophilous nodes excluded in the neighborhood, they are ignored for information aggregation. To address these problems, we propose two models GloGNN and GloGNN++, which generate a node's embedding by aggregating information from global nodes in the graph. In each layer, both models learn a coefficient matrix to capture the correlations between nodes, based on which neighborhood aggregation is performed. The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution. We further accelerate neighborhood aggregation and derive a linear time complexity. We theoretically explain the models' effectiveness by proving that both the coefficient matrix and the generated node embedding matrix have the desired grouping effect. We conduct extensive experiments to compare our models against 11 other competitors on 15 benchmark datasets in a wide range of domains, scales and graph heterophilies. Experimental results show that our methods achieve superior performance and are also very efficient.",
      "file_path": "paper_data/Graph_Neural_Networks/b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Graph Neural Networks (GNNs) perform poorly on graphs exhibiting *heterophily* (where connected nodes tend to have dissimilar features or labels), due to their inherent homophily assumption and smoothing operations.\n    *   **Importance & Challenge**: Heterophilous graphs are common in real-world scenarios (e.g., protein structures, ecological food webs). Existing solutions that amplify a node's neighborhood with multi-hop neighbors face significant challenges:\n        *   Determining personalized optimal neighborhood sizes for different nodes.\n        *   Ignoring homophilous nodes that fall outside the chosen multi-hop neighborhood.\n        *   A straightforward global aggregation approach would lead to prohibitive quadratic or cubic time complexity and could inadvertently include more heterophilous nodes.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   Some methods leverage both low-pass and high-pass convolutional filters (e.g., GPR-GNN, ACM-GCN) to adapt to homophily/heterophily.\n        *   Other methods enlarge the node neighborhood size to include more distant homophilous nodes (e.g., H2GCN, WRGAT), often focusing on higher-order neighbors or transforming the graph.\n    *   **Limitations of Previous Solutions**: These methods struggle with adaptively setting neighborhood sizes and may still miss globally distant but homophilous nodes. They often don't fully leverage the *global* homophily present across the entire graph.\n    *   **Positioning**: GloGNN differentiates itself by performing node neighborhood aggregation from the *whole set of nodes* in the graph, aiming to capture global homophily, which is distinct from merely expanding to multi-hop neighbors.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes GloGNN and GloGNN++ models that generate node embeddings by aggregating information from *all global nodes* in the graph.\n        *   In each layer, a **coefficient matrix Z(l)** is learned to capture correlations between nodes. Z(l)ij describes the importance of node xj to xi.\n        *   This matrix is derived from an optimization problem that characterizes each node by all other nodes, inspired by the linear subspace model.\n        *   The initial node embedding H(0) is decoupled from aggregation and generated by MLPs from both node features and adjacency matrix, then combined.\n        *   A skip connection is used: H(l) = (1-beta)Z(l)H(l) + H(0) + O(l).\n        *   Z(l) is regularized by nodes' multi-hop reachabilities (sum of k * A^k) to incorporate both feature and topology similarity.\n    *   **Novelty/Difference**:\n        *   **Global Aggregation with Learned Coefficients**: Instead of fixed neighborhoods, GloGNN learns a coefficient matrix for *all* nodes, effectively finding global homophily.\n        *   **Signed Coefficient Matrix**: Z(l) allows signed values, enabling it to assign large positive coefficients to homophilous nodes and small positive or negative coefficients to heterophilous ones, implicitly combining low-pass and high-pass filtering.\n        *   **Closed-Form Solution**: The optimization problem for Z(l) has a closed-form solution, simplifying its computation.\n        *   **Linear Time Complexity Acceleration**: A crucial innovation is transforming the aggregation equation to avoid direct computation of Z(l) and reordering matrix multiplications. This reduces the time complexity from cubic/quadratic (O(n^3) or O(n^2c)) to *linear* (O(k2n)) by leveraging matrix properties, the small number of labels (c), and the sparsity of the adjacency matrix.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of GloGNN and GloGNN++ models for effective and efficient GNNs on heterophilous graphs.\n    *   **Theoretical Insights**: Proof that both the learned coefficient matrix Z(l) and the generated node embedding matrix H(l+1) exhibit a **grouping effect**. This means nodes with similar features and local structures (even if distant) will have similar coefficient vectors and representation vectors, explaining the model's effectiveness.\n    *   **System Design/Architectural Innovations**: A novel framework that decouples feature transformation, learns global node correlations via a signed coefficient matrix, and performs aggregation with linear time complexity.\n    *   **Algorithmic Efficiency**: Development of an acceleration technique that reduces the computational complexity of global aggregation from cubic/quadratic to linear time.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to compare GloGNN and GloGNN++ against other methods.\n    *   **Competitors**: Compared against 11 other state-of-the-art GNN models.\n    *   **Datasets**: Evaluated on 15 benchmark datasets, covering a wide range of domains, scales, and graph heterophilies.\n    *   **Key Performance Metrics & Results**: The experimental results demonstrate that GloGNN and GloGNN++ achieve **superior performance** (implying higher accuracy or relevant task metrics) and are also **very efficient** (due to the linear time complexity).\n\n6.  **Limitations & Scope**\n    *   The paper primarily focuses on addressing the limitations of *previous* methods (e.g., difficulty in setting personalized neighborhood sizes, ignoring distant homophilous nodes, computational cost of naive global aggregation).\n    *   The provided text does not explicitly state technical limitations or assumptions *of GloGNN itself*, beyond the general scope of node classification on heterophilous graphs. The model's effectiveness is theoretically explained by the grouping effect, and its efficiency is a key design goal.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GloGNN significantly advances GNNs for heterophilous graphs by providing a principled and efficient way to leverage *global homophily*. It moves beyond local or multi-hop aggregation limitations by considering all nodes while maintaining computational feasibility.\n    *   **Potential Impact**:\n        *   Enables the application of GNNs to a broader range of real-world heterophilous datasets where traditional GNNs struggle.\n        *   The theoretical proof of the grouping effect provides a deeper understanding of why such global aggregation strategies can be effective.\n        *   The linear time complexity for global aggregation opens avenues for scaling GNNs to very large graphs, which is a critical challenge in the field.\n        *   The concept of a learned, signed coefficient matrix could inspire future research into more adaptive and expressive graph convolution operations.",
      "keywords": [
        "Heterophilous graphs",
        "Graph Neural Networks (GNNs)",
        "Homophily assumption",
        "GloGNN",
        "Global aggregation",
        "Learned coefficient matrix",
        "Signed coefficient matrix",
        "Linear time complexity",
        "Grouping effect",
        "Node embeddings",
        "Computational efficiency",
        "Superior performance",
        "State-of-the-art advancement"
      ],
      "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   the abstract explicitly states: \"we **propose two models glognn and glognn++**,\" and describes how they \"generate a node’s embedding by aggregating information from global nodes.\" it also mentions learning a \"coefficient matrix\" and deriving \"linear time complexity,\" all of which point to the development of new methods and algorithms.\n*   the introduction sets up a problem with existing graph neural networks (gnns) in the context of heterophily, which is a common setup for introducing a *new technical solution*.\n*   while the paper also includes strong **theoretical** elements (\"theoretically explain the models’ effectiveness by proving...\") and **empirical** elements (\"conduct extensive experiments to compare our models against 11 other competitors... experimental results show that our methods achieve superior performance...\"), these aspects serve to support and validate the *new methods* being proposed. the core contribution is the development of glognn and glognn++."
    },
    "file_name": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf"
  },
  {
    "success": true,
    "doc_id": "26f52e314db211053bb7d1c51e1f15d3",
    "summary": "Here's a focused summary of the paper \"UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND\" \\cite{kang2024fsk} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Traditional continuous Graph Neural Networks (GNNs) rely on integer-order differential equations, which model instantaneous, local changes (Markovian updates) in node features. This approach struggles to capture long-term dependencies and memory effects inherent in many real-world graph dynamics.\n    *   **Importance & Challenge:** Many real-world graphs (e.g., social, biological, internet networks) exhibit non-local, memory-dependent behaviors and scale-free hierarchical (fractal) structures. Integer-order models are insufficient to accurately describe these complex dynamics, potentially leading to limitations like oversmoothing and suboptimal graph representation learning.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior continuous GNNs (e.g., GRAND, GRAND++, GraphCON, CDE, GREAD) leverage integer-order Ordinary Differential Equations (ODEs) for information propagation, typically using first or second-order derivatives.\n    *   **Limitations of Previous Solutions:** These models are restricted to integer-order derivatives, implying Markovian update mechanisms where feature evolution depends only on the present state. This prevents them from inherently capturing the non-local properties and memory-dependent dynamics crucial for systems with self-similarity or anomalous transport. Other works using fractional calculus either apply it to graph shift operators with integer-order ODEs or to gradient propagation during training, not to the core node feature updating process itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the FRactional-Order graph Neural Dynamical network (FROND) framework \\cite{kang2024fsk}, which replaces the integer-order differential operator in continuous GNNs with the Caputo fractional derivative (DβtX(t) = F(W,X(t)), β > 0).\n    *   **Novelty:**\n        *   **Generalization of Continuous GNNs:** FROND generalizes existing integer-order continuous GNNs by allowing the derivative order β to be any positive real number, effectively subsuming them as special cases when β is an integer.\n        *   **Memory-Dependent Dynamics:** By employing the Caputo fractional derivative, FROND inherently integrates the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics.\n        *   **Non-Markovian Random Walk Interpretation:** For the fractional linear diffusion model (F-GRAND-l), the paper provides an interpretation from a non-Markovian random walk perspective, where the walker's complete path history influences future steps, contrasting with the Markovian walks of traditional models.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Proposed a novel, generalized continuous GNN framework (FROND) that incorporates non-local fractional derivatives, laying the groundwork for a new class of GNNs with learnable memory-dependent feature-updating processes \\cite{kang2024fsk}.\n    *   **System Design/Architectural Innovations:** Demonstrated the seamless compatibility of FROND, showing how it can be integrated to augment the performance of existing integer-order continuous GNNs (e.g., F-GRAND, F-GRAND++, F-GREAD, F-CDE, F-GraphCON).\n    *   **Theoretical Insights/Analysis:**\n        *   Analytically established that the non-Markovian random walk in FROND leads to a slow algebraic rate of convergence to stationarity, which inherently mitigates oversmoothing, unlike the exponentially swift convergence in Markovian integer-order models.\n        *   Suggested a connection between the optimal fractional order β and the inherent \"fractality\" of graph datasets, offering a potential avenue for deeper structural insights.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The FROND framework was validated through extensive experiments comparing fractional adaptations of various established integer-order continuous GNNs (GRAND, GRAND++, GraphCON, CDE, GREAD) on diverse datasets \\cite{kang2024fsk}.\n    *   **Key Performance Metrics & Comparison Results:** The fractional adaptations consistently demonstrated improved performance compared to their integer-order counterparts. Detailed ablation studies were performed to provide insights into the choice of numerical schemes and parameters, underscoring the framework’s potential as an effective extension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on β ∈ (0,1] for initial conditions, though the broader definition for β > 0 is mentioned. The framework relies on numerical FDE solvers, which may introduce computational considerations.\n    *   **Scope of Applicability:** FROND is designed to enhance continuous GNNs for graph representation learning, particularly beneficial for datasets exhibiting non-local, memory-dependent behaviors, or fractal structures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FROND significantly advances the technical state-of-the-art by introducing a principled and generalized approach to incorporate memory-dependent dynamics and non-local interactions into continuous GNNs, overcoming the limitations of purely Markovian integer-order models \\cite{kang2024fsk}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new research directions for designing GNNs that can model more complex, memory-dependent feature-updating processes.\n        *   Provides a robust mechanism for mitigating oversmoothing in GNNs through its algebraic convergence properties.\n        *   Offers a novel tool for exploring the underlying \"fractality\" and self-similarity of graph datasets by optimizing the fractional order β.\n        *   Serves as a powerful and compatible extension to enhance the performance of existing continuous GNN architectures.",
    "intriguing_abstract": "Traditional Graph Neural Networks (GNNs) often model node feature evolution using integer-order differential equations, inherently limiting them to instantaneous, Markovian updates. This approach struggles to capture the pervasive non-local, memory-dependent dynamics and scale-free hierarchical structures characteristic of real-world graphs, frequently leading to issues like oversmoothing. We introduce FROND (FRactional-Order graph Neural Dynamical network), a novel framework that revolutionizes continuous GNNs by integrating the **Caputo fractional derivative**.\n\nFROND generalizes existing continuous GNNs, enabling feature updates to inherently depend on their entire historical trajectory, thus capturing complex **non-Markovian dynamics**. We analytically demonstrate that this **fractional-order** propagation leads to an algebraic rate of convergence to stationarity, fundamentally mitigating **oversmoothing**—a stark contrast to the exponential convergence of integer-order models. Furthermore, FROND offers a unique lens to explore the intrinsic \"**fractality**\" of graph datasets, with the optimal fractional order potentially revealing deeper structural insights. Extensive experiments validate FROND's ability to seamlessly enhance the performance of various state-of-the-art **continuous GNNs**, paving the way for a new generation of GNNs capable of modeling richer, memory-aware graph dynamics and unlocking unprecedented understanding of complex network systems.",
    "keywords": [
      "FROND framework",
      "Fractional Calculus",
      "Graph Neural Networks",
      "Caputo fractional derivative",
      "Memory-dependent dynamics",
      "Non-local interactions",
      "Oversmoothing mitigation",
      "Non-Markovian random walk",
      "Generalized continuous GNNs",
      "Graph representation learning",
      "Fractal graph structures",
      "Algebraic convergence",
      "Learnable memory-dependent processes"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf",
    "citation_key": "kang2024fsk",
    "metadata": {
      "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
      "authors": [
        "Qiyu Kang",
        "Kai Zhao",
        "Qinxu Ding",
        "Feng Ji",
        "Xuhao Li",
        "Wenfei Liang",
        "Yang Song",
        "Wee Peng Tay"
      ],
      "published_date": "2024",
      "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
      "file_path": "paper_data/Graph_Neural_Networks/900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND\" \\cite{kang2024fsk} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Traditional continuous Graph Neural Networks (GNNs) rely on integer-order differential equations, which model instantaneous, local changes (Markovian updates) in node features. This approach struggles to capture long-term dependencies and memory effects inherent in many real-world graph dynamics.\n    *   **Importance & Challenge:** Many real-world graphs (e.g., social, biological, internet networks) exhibit non-local, memory-dependent behaviors and scale-free hierarchical (fractal) structures. Integer-order models are insufficient to accurately describe these complex dynamics, potentially leading to limitations like oversmoothing and suboptimal graph representation learning.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior continuous GNNs (e.g., GRAND, GRAND++, GraphCON, CDE, GREAD) leverage integer-order Ordinary Differential Equations (ODEs) for information propagation, typically using first or second-order derivatives.\n    *   **Limitations of Previous Solutions:** These models are restricted to integer-order derivatives, implying Markovian update mechanisms where feature evolution depends only on the present state. This prevents them from inherently capturing the non-local properties and memory-dependent dynamics crucial for systems with self-similarity or anomalous transport. Other works using fractional calculus either apply it to graph shift operators with integer-order ODEs or to gradient propagation during training, not to the core node feature updating process itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the FRactional-Order graph Neural Dynamical network (FROND) framework \\cite{kang2024fsk}, which replaces the integer-order differential operator in continuous GNNs with the Caputo fractional derivative (DβtX(t) = F(W,X(t)), β > 0).\n    *   **Novelty:**\n        *   **Generalization of Continuous GNNs:** FROND generalizes existing integer-order continuous GNNs by allowing the derivative order β to be any positive real number, effectively subsuming them as special cases when β is an integer.\n        *   **Memory-Dependent Dynamics:** By employing the Caputo fractional derivative, FROND inherently integrates the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics.\n        *   **Non-Markovian Random Walk Interpretation:** For the fractional linear diffusion model (F-GRAND-l), the paper provides an interpretation from a non-Markovian random walk perspective, where the walker's complete path history influences future steps, contrasting with the Markovian walks of traditional models.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Proposed a novel, generalized continuous GNN framework (FROND) that incorporates non-local fractional derivatives, laying the groundwork for a new class of GNNs with learnable memory-dependent feature-updating processes \\cite{kang2024fsk}.\n    *   **System Design/Architectural Innovations:** Demonstrated the seamless compatibility of FROND, showing how it can be integrated to augment the performance of existing integer-order continuous GNNs (e.g., F-GRAND, F-GRAND++, F-GREAD, F-CDE, F-GraphCON).\n    *   **Theoretical Insights/Analysis:**\n        *   Analytically established that the non-Markovian random walk in FROND leads to a slow algebraic rate of convergence to stationarity, which inherently mitigates oversmoothing, unlike the exponentially swift convergence in Markovian integer-order models.\n        *   Suggested a connection between the optimal fractional order β and the inherent \"fractality\" of graph datasets, offering a potential avenue for deeper structural insights.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The FROND framework was validated through extensive experiments comparing fractional adaptations of various established integer-order continuous GNNs (GRAND, GRAND++, GraphCON, CDE, GREAD) on diverse datasets \\cite{kang2024fsk}.\n    *   **Key Performance Metrics & Comparison Results:** The fractional adaptations consistently demonstrated improved performance compared to their integer-order counterparts. Detailed ablation studies were performed to provide insights into the choice of numerical schemes and parameters, underscoring the framework’s potential as an effective extension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on β ∈ (0,1] for initial conditions, though the broader definition for β > 0 is mentioned. The framework relies on numerical FDE solvers, which may introduce computational considerations.\n    *   **Scope of Applicability:** FROND is designed to enhance continuous GNNs for graph representation learning, particularly beneficial for datasets exhibiting non-local, memory-dependent behaviors, or fractal structures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FROND significantly advances the technical state-of-the-art by introducing a principled and generalized approach to incorporate memory-dependent dynamics and non-local interactions into continuous GNNs, overcoming the limitations of purely Markovian integer-order models \\cite{kang2024fsk}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new research directions for designing GNNs that can model more complex, memory-dependent feature-updating processes.\n        *   Provides a robust mechanism for mitigating oversmoothing in GNNs through its algebraic convergence properties.\n        *   Offers a novel tool for exploring the underlying \"fractality\" and self-similarity of graph datasets by optimizing the fractional order β.\n        *   Serves as a powerful and compatible extension to enhance the performance of existing continuous GNN architectures.",
      "keywords": [
        "FROND framework",
        "Fractional Calculus",
        "Graph Neural Networks",
        "Caputo fractional derivative",
        "Memory-dependent dynamics",
        "Non-local interactions",
        "Oversmoothing mitigation",
        "Non-Markovian random walk",
        "Generalized continuous GNNs",
        "Graph representation learning",
        "Fractal graph structures",
        "Algebraic convergence",
        "Learnable memory-dependent processes"
      ],
      "paper_type": "this paper introduces a **new continuous graph neural network (gnn) framework** called frond, which utilizes caputo fractional derivatives. it explicitly states:\n\n*   **abstract:** \"we introduce the fractional-order graph neural dynamical network (frond), a new continuous graph neural network (gnn) framework.\" and \"we demonstrate analytically that oversmoothing can be mitigated in this setting. experimentally, we validate the frond framework...\"\n*   **introduction:** \"we introduce the fractional-order graph neural dynamical network (frond) framework, a new approach that broadens the capabilities of traditional integer-order continuous gnns by incorporating fractional calculus.\" and \"our objective in this paper is to formulate a generalized fractional-order continuous gnn framework. our key contributions are summarized as follows: •we propose a novel, generalized continuous gnn framework...\"\n\nthe paper proposes a novel method/system, provides theoretical analysis (theorems, random walk interpretation, oversmoothing mitigation), and validates its performance through extensive experiments. while it contains elements of theoretical and empirical research, its primary contribution is the **development and presentation of a new system/method**.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf"
  },
  {
    "success": true,
    "doc_id": "9fac8603e374f6f0842ea0de6807ab3d",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The challenge of designing appropriate Graph Neural Network (GNN) methods for the diverse and varied real-world recommendation tasks.\n    *   **Importance and Challenge**: GNNs have emerged as state-of-the-art for recommendation due to their ability to handle structured data and explore high-order information. However, the complexity and diversity of real-world recommendation scenarios make it difficult to effectively adapt and apply GNNs, necessitating a structured understanding of challenges and solutions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a tutorial that extensively reviews and synthesizes existing GNN-based recommendation models. It positions itself by providing an \"extensive background of recommender systems and graph neural networks\" and discussing \"recent advances.\"\n    *   **Limitations of Previous Solutions (as identified by the tutorial)**: The paper highlights four critical challenges in existing GNN-based recommendation methods: graph construction, network design, optimization, and computation efficiency.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic review and taxonomy of GNN-based recommendation models. It identifies critical challenges and elaborates on how recent advances address these.\n    *   **Novelty/Difference**: The innovation lies in providing a \"systematic taxonomy from four critical perspectives: stages, scenarios, objectives, and applications\" for GNN-based recommendation models, coupled with a structured discussion of the four key challenge areas and their potential solutions.\n\n*   **Key Technical Contributions**\n    *   **Systematic Taxonomy**: Presents a novel, systematic taxonomy for GNN-based recommendation models, categorizing them across stages, scenarios, objectives, and applications.\n    *   **Challenge Identification**: Clearly articulates and discusses four critical technical challenges in GNN-based recommendation: graph construction, network design, optimization, and computation efficiency.\n    *   **Solution Elaboration**: Elaborates on recent technical advances that address these identified challenges, providing a structured overview of the current state-of-the-art.\n\n*   **Experimental Validation**\n    *   As a tutorial and survey paper, \\cite{gao2022f3h} does not present its own novel experimental validation or empirical results. Instead, it synthesizes and discusses the experimental findings and performance metrics from the various GNN-based recommendation models it reviews.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper's scope is limited to GNN-based recommendation systems. As a tutorial, its depth for any single model or technique is constrained by the breadth of its coverage. It assumes a foundational understanding of recommender systems and GNNs.\n    *   **Scope of Applicability**: The insights and taxonomy are applicable to researchers and practitioners working on designing, implementing, or understanding GNNs for various recommendation tasks.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{gao2022f3h} significantly advances the technical state-of-the-art by providing a comprehensive, structured, and critical overview of the rapidly evolving field of GNN-based recommendation. It consolidates fragmented knowledge, identifies key technical hurdles, and categorizes solutions.\n    *   **Potential Impact on Future Research**: It serves as a foundational resource, guiding future research by highlighting open problems, critical challenges, and promising directions (e.g., in graph construction, network design, optimization, and computational efficiency), thereby fostering more targeted and impactful innovations in the field.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized recommender systems, offering unparalleled capabilities in modeling complex user-item interactions and high-order information. However, the sheer diversity and evolving complexity of real-world recommendation tasks present significant hurdles in designing and deploying effective GNN-based solutions. Navigating this rapidly expanding landscape demands a structured understanding of both existing paradigms and persistent challenges.\n\nThis paper provides a comprehensive and systematic tutorial, offering a novel taxonomy that categorizes GNN-based recommendation models across stages, scenarios, objectives, and applications. We meticulously identify and elaborate on four critical technical challenges: robust graph construction, innovative network design, efficient optimization strategies, and scalable computation efficiency. By synthesizing recent advances, we illuminate how cutting-edge research addresses these hurdles. This work serves as an indispensable resource, consolidating fragmented knowledge, guiding researchers through the intricacies of GNN-based recommendation, and charting promising directions for future innovation in this dynamic field.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Recommendation systems",
      "GNN-based recommendation models",
      "Systematic review",
      "GNN-based recommendation taxonomy",
      "GNN graph construction",
      "GNN network design",
      "GNN optimization",
      "GNN computation efficiency",
      "GNN recommendation challenges",
      "High-order information",
      "Real-world recommendation tasks",
      "GNN recommendation state-of-the-art",
      "GNN recommendation open problems"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/4fa31616b834c377c4995c346a2b17464f25692a.pdf",
    "citation_key": "gao2022f3h",
    "metadata": {
      "title": "Graph Neural Networks for Recommender System",
      "authors": [
        "Chen Gao",
        "Xiang Wang",
        "Xiangnan He",
        "Yong Li"
      ],
      "published_date": "2022",
      "abstract": "Recently, graph neural network (GNN) has become the new state-of-the-art approach in many recommendation problems, with its strong ability to handle structured data and to explore high-order information. However, as the recommendation tasks are diverse and various in the real world, it is quite challenging to design proper GNN methods for specific problems. In this tutorial, we focus on the critical challenges of GNN-based recommendation and the potential solutions. Specifically, we start from an extensive background of recommender systems and graph neural networks. Then we fully discuss why GNNs are required in recommender systems and the four parts of challenges, including graph construction, network design, optimization, and computation efficiency. Then, we discuss how to address these challenges by elaborating on the recent advances of GNN-based recommendation models, with a systematic taxonomy from four critical perspectives: stages, scenarios, objectives, and applications. Last, we finalize this tutorial with conclusions and discuss important future directions.",
      "file_path": "paper_data/Graph_Neural_Networks/4fa31616b834c377c4995c346a2b17464f25692a.pdf",
      "venue": "Web Search and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The challenge of designing appropriate Graph Neural Network (GNN) methods for the diverse and varied real-world recommendation tasks.\n    *   **Importance and Challenge**: GNNs have emerged as state-of-the-art for recommendation due to their ability to handle structured data and explore high-order information. However, the complexity and diversity of real-world recommendation scenarios make it difficult to effectively adapt and apply GNNs, necessitating a structured understanding of challenges and solutions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a tutorial that extensively reviews and synthesizes existing GNN-based recommendation models. It positions itself by providing an \"extensive background of recommender systems and graph neural networks\" and discussing \"recent advances.\"\n    *   **Limitations of Previous Solutions (as identified by the tutorial)**: The paper highlights four critical challenges in existing GNN-based recommendation methods: graph construction, network design, optimization, and computation efficiency.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic review and taxonomy of GNN-based recommendation models. It identifies critical challenges and elaborates on how recent advances address these.\n    *   **Novelty/Difference**: The innovation lies in providing a \"systematic taxonomy from four critical perspectives: stages, scenarios, objectives, and applications\" for GNN-based recommendation models, coupled with a structured discussion of the four key challenge areas and their potential solutions.\n\n*   **Key Technical Contributions**\n    *   **Systematic Taxonomy**: Presents a novel, systematic taxonomy for GNN-based recommendation models, categorizing them across stages, scenarios, objectives, and applications.\n    *   **Challenge Identification**: Clearly articulates and discusses four critical technical challenges in GNN-based recommendation: graph construction, network design, optimization, and computation efficiency.\n    *   **Solution Elaboration**: Elaborates on recent technical advances that address these identified challenges, providing a structured overview of the current state-of-the-art.\n\n*   **Experimental Validation**\n    *   As a tutorial and survey paper, \\cite{gao2022f3h} does not present its own novel experimental validation or empirical results. Instead, it synthesizes and discusses the experimental findings and performance metrics from the various GNN-based recommendation models it reviews.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper's scope is limited to GNN-based recommendation systems. As a tutorial, its depth for any single model or technique is constrained by the breadth of its coverage. It assumes a foundational understanding of recommender systems and GNNs.\n    *   **Scope of Applicability**: The insights and taxonomy are applicable to researchers and practitioners working on designing, implementing, or understanding GNNs for various recommendation tasks.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{gao2022f3h} significantly advances the technical state-of-the-art by providing a comprehensive, structured, and critical overview of the rapidly evolving field of GNN-based recommendation. It consolidates fragmented knowledge, identifies key technical hurdles, and categorizes solutions.\n    *   **Potential Impact on Future Research**: It serves as a foundational resource, guiding future research by highlighting open problems, critical challenges, and promising directions (e.g., in graph construction, network design, optimization, and computational efficiency), thereby fostering more targeted and impactful innovations in the field.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Recommendation systems",
        "GNN-based recommendation models",
        "Systematic review",
        "GNN-based recommendation taxonomy",
        "GNN graph construction",
        "GNN network design",
        "GNN optimization",
        "GNN computation efficiency",
        "GNN recommendation challenges",
        "High-order information",
        "Real-world recommendation tasks",
        "GNN recommendation state-of-the-art",
        "GNN recommendation open problems"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states, \"in this **tutorial**, we focus on the critical challenges of gnn-based recommendation and the potential solutions.\"\n*   it mentions starting \"from an **extensive background** of recommender systems and graph neural networks.\"\n*   it discusses \"fully discuss why gnns are required... and the four parts of challenges... then, we discuss how to address these.\" this indicates a comprehensive overview and analysis of existing work, problems, and solutions.\n*   the introduction snippet reinforces the \"tutorial\" aspect and mentions discussing \"important future directions,\" which is a common component of survey papers.\n*   the phrase \"state-of-the-art approach\" in the first sentence also suggests a review of current progress.\n\nthese elements strongly align with the criteria for a **survey** paper, which \"reviews existing literature comprehensively\" and often discusses \"literature organization, classification schemes\" (implied by discussing challenges in parts) and \"state-of-the-art.\"\n\n**classification: survey**"
    },
    "file_name": "4fa31616b834c377c4995c346a2b17464f25692a.pdf"
  },
  {
    "success": true,
    "doc_id": "fe2d60eeddbb4953d65ffe2989d0b04f",
    "summary": "This paper, `Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking` by Li et al. \\cite{li2023o4c}, critically examines the evaluation practices of Graph Neural Networks (GNNs) for link prediction and proposes a new, more realistic benchmarking framework.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses significant pitfalls in the current evaluation of GNN-based link prediction methods, which lead to unreliable comparisons and hinder progress.\n    *   **Importance and Challenge**: These pitfalls include: (1) underreported performance of existing baselines due to poor hyperparameter tuning or non-standard settings; (2) a lack of unified data splits and evaluation metrics across different studies and datasets; and (3) an unrealistic evaluation setting that uses \"easy\" negative samples, which do not reflect real-world scenarios (e.g., negative samples often lack common neighbors, making them trivial to classify). This problem is crucial because it obscures the true capabilities of GNN models and makes it difficult to identify genuinely superior approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself not by proposing a new GNN architecture, but by scrutinizing and improving the *evaluation methodology* for existing and future GNN-based link prediction models. It acknowledges the proliferation of GNN methods for link prediction but highlights the absence of a comprehensive, critical examination of their evaluation.\n    *   **Limitations of Previous Solutions**: The paper directly addresses the limitations of previous *evaluation practices*, which are the three pitfalls mentioned above. It argues that current benchmarking efforts are inconsistent and often misleading, preventing fair comparisons and accurate assessment of model performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        1.  **Reproducible and Fair Comparison**: The authors first conduct a rigorous re-evaluation of 17 prominent link prediction methods (including heuristics, embedding methods, GNNs, and GNN+Pairwise Info models) across 7 diverse datasets (Cora, Citeseer, Pubmed, ogbl-collab, ogbl-ddi, ogbl-ppa, ogbl-citation2). This involves unified data splits, consistent evaluation metrics (AUC, MRR, Hits@K), and a comprehensive hyperparameter search for all models.\n        2.  **New Evaluation Setting with Heuristic Related Sampling Technique (HeaRT)**: To address the unrealistic negative sampling, the paper proposes a novel evaluation strategy. This strategy tailors negative samples to each positive sample by restricting them to be \"corruptions\" (i.e., sharing one node with the positive sample). Within this more realistic pool, a **Heuristic Related Sampling Technique (HeaRT)** is introduced to select *hard* negative samples based on a combination of multiple structural heuristics.\n    *   **Novelty**: The primary novelty lies in two aspects: (1) the systematic and comprehensive re-benchmarking effort itself, which corrects widespread misrepresentations of model performance and establishes a unified evaluation framework; and (2) the introduction of HeaRT, a novel negative sampling technique that generates challenging, realistic negative samples, thereby creating a more robust and real-world aligned evaluation task for link prediction.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A standardized and reproducible benchmarking methodology for GNN-based link prediction, including unified data splits, metrics, and rigorous hyperparameter tuning.\n        *   HeaRT: A novel negative sampling technique that generates hard, heuristic-related negative samples for link prediction evaluation, making the task more challenging and realistic.\n    *   **System Design/Architectural Innovations**: While not proposing a new GNN architecture, the paper significantly innovates the *evaluation framework* for GNNs in link prediction.\n    *   **Theoretical Insights/Analysis**: Provides empirical insights into the actual performance of GNNs and the profound impact of evaluation settings on reported results.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A \"fair comparison\" of 17 methods on 7 datasets under existing, but unified, evaluation settings.\n        *   Evaluation of these methods under the proposed new evaluation setting utilizing HeaRT.\n    *   **Key Performance Metrics**: AUC, MRR, and Hits@K (with K varying based on dataset).\n    *   **Comparison Results (from fair comparison under existing settings)**:\n        *   **Better than Reported Performance**: Many models, including GCN, GAE, and Neo-GNN, achieved significantly higher performance than previously reported when properly tuned and evaluated under consistent settings. For example, Neo-GNN's Hits@50 on `ogbl-collab` increased from 57.52 to 66.13 \\cite{li2023o4c}. Heuristic methods on `ogbl-citation2` also saw substantial improvements (e.g., MRR around 75% vs. 50%) by treating the graph as undirected, consistent with GNN evaluations.\n        *   **Divergence from Reported Results**: Some methods, like BUDDY on `ogbl-ddi`, showed lower performance than previously reported, attributed to differences in training negative sampling strategies, further highlighting inconsistencies in prior work.\n        *   The new HeaRT evaluation setting is designed to expose new challenges and opportunities by aligning evaluation with real-world situations, though specific results from this setting are not detailed in the provided abstract and introduction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on homogeneous graphs. For large OGB datasets, hyperparameter search was constrained due to computational cost. The HeaRT method, while more realistic, still samples *K* negatives due to the prohibitive cost of considering all possible corruptions.\n    *   **Scope of Applicability**: The findings and proposed benchmark are most directly applicable to GNN-based link prediction on homogeneous graphs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art by establishing a robust, reproducible, and more realistic benchmarking framework for GNN-based link prediction. It corrects previous misrepresentations of model performance, providing a clearer picture of current capabilities.\n    *   **Potential Impact**: It provides a reliable foundation for future research, enabling fair comparisons and accelerating the development of more robust and generalizable link prediction models. By exposing the limitations of current evaluation practices and offering a more challenging setting, it encourages the community to develop models that perform better in real-world applications, thereby fostering more impactful research.",
    "intriguing_abstract": "The landscape of Graph Neural Networks (GNNs) for link prediction is plagued by unreliable evaluations, hindering true progress and fair comparisons. Current benchmarks suffer from inconsistent data splits, poor hyperparameter tuning, and critically, unrealistic \"easy\" negative samples that inflate reported performance. This paper uncovers these pervasive pitfalls, presenting a rigorous re-evaluation of 17 prominent link prediction methods across 7 diverse datasets. Our comprehensive benchmarking, employing unified settings and meticulous hyperparameter optimization, reveals that many GNN models, including GCN and GAE, achieve significantly higher performance than previously reported, correcting widespread misconceptions. To address the fundamental flaw of trivial negative samples, we introduce a novel, more realistic evaluation setting featuring the **Heuristic Related Sampling Technique (HeaRT)**. HeaRT generates challenging, structurally-informed negative samples, pushing models to learn more robust representations. This work establishes a much-needed, reproducible, and challenging benchmark, providing a clearer picture of the state-of-the-art and paving the way for the development of truly impactful GNNs for real-world link prediction tasks.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "link prediction",
      "evaluation pitfalls",
      "benchmarking framework",
      "reproducible comparison",
      "hyperparameter tuning",
      "unified data splits",
      "negative sampling",
      "Heuristic Related Sampling Technique (HeaRT)",
      "hard negative samples",
      "realistic evaluation setting",
      "model performance misrepresentation",
      "standardized methodology"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/f442378ead6282024cf5b9046daa10422fe9fc5f.pdf",
    "citation_key": "li2023o4c",
    "metadata": {
      "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
      "authors": [
        "Juanhui Li",
        "Harry Shomer",
        "Haitao Mao",
        "Shenglai Zeng",
        "Yao Ma",
        "Neil Shah",
        "Jiliang Tang",
        "Dawei Yin"
      ],
      "published_date": "2023",
      "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT",
      "file_path": "paper_data/Graph_Neural_Networks/f442378ead6282024cf5b9046daa10422fe9fc5f.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, `Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking` by Li et al. \\cite{li2023o4c}, critically examines the evaluation practices of Graph Neural Networks (GNNs) for link prediction and proposes a new, more realistic benchmarking framework.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses significant pitfalls in the current evaluation of GNN-based link prediction methods, which lead to unreliable comparisons and hinder progress.\n    *   **Importance and Challenge**: These pitfalls include: (1) underreported performance of existing baselines due to poor hyperparameter tuning or non-standard settings; (2) a lack of unified data splits and evaluation metrics across different studies and datasets; and (3) an unrealistic evaluation setting that uses \"easy\" negative samples, which do not reflect real-world scenarios (e.g., negative samples often lack common neighbors, making them trivial to classify). This problem is crucial because it obscures the true capabilities of GNN models and makes it difficult to identify genuinely superior approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself not by proposing a new GNN architecture, but by scrutinizing and improving the *evaluation methodology* for existing and future GNN-based link prediction models. It acknowledges the proliferation of GNN methods for link prediction but highlights the absence of a comprehensive, critical examination of their evaluation.\n    *   **Limitations of Previous Solutions**: The paper directly addresses the limitations of previous *evaluation practices*, which are the three pitfalls mentioned above. It argues that current benchmarking efforts are inconsistent and often misleading, preventing fair comparisons and accurate assessment of model performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        1.  **Reproducible and Fair Comparison**: The authors first conduct a rigorous re-evaluation of 17 prominent link prediction methods (including heuristics, embedding methods, GNNs, and GNN+Pairwise Info models) across 7 diverse datasets (Cora, Citeseer, Pubmed, ogbl-collab, ogbl-ddi, ogbl-ppa, ogbl-citation2). This involves unified data splits, consistent evaluation metrics (AUC, MRR, Hits@K), and a comprehensive hyperparameter search for all models.\n        2.  **New Evaluation Setting with Heuristic Related Sampling Technique (HeaRT)**: To address the unrealistic negative sampling, the paper proposes a novel evaluation strategy. This strategy tailors negative samples to each positive sample by restricting them to be \"corruptions\" (i.e., sharing one node with the positive sample). Within this more realistic pool, a **Heuristic Related Sampling Technique (HeaRT)** is introduced to select *hard* negative samples based on a combination of multiple structural heuristics.\n    *   **Novelty**: The primary novelty lies in two aspects: (1) the systematic and comprehensive re-benchmarking effort itself, which corrects widespread misrepresentations of model performance and establishes a unified evaluation framework; and (2) the introduction of HeaRT, a novel negative sampling technique that generates challenging, realistic negative samples, thereby creating a more robust and real-world aligned evaluation task for link prediction.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A standardized and reproducible benchmarking methodology for GNN-based link prediction, including unified data splits, metrics, and rigorous hyperparameter tuning.\n        *   HeaRT: A novel negative sampling technique that generates hard, heuristic-related negative samples for link prediction evaluation, making the task more challenging and realistic.\n    *   **System Design/Architectural Innovations**: While not proposing a new GNN architecture, the paper significantly innovates the *evaluation framework* for GNNs in link prediction.\n    *   **Theoretical Insights/Analysis**: Provides empirical insights into the actual performance of GNNs and the profound impact of evaluation settings on reported results.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A \"fair comparison\" of 17 methods on 7 datasets under existing, but unified, evaluation settings.\n        *   Evaluation of these methods under the proposed new evaluation setting utilizing HeaRT.\n    *   **Key Performance Metrics**: AUC, MRR, and Hits@K (with K varying based on dataset).\n    *   **Comparison Results (from fair comparison under existing settings)**:\n        *   **Better than Reported Performance**: Many models, including GCN, GAE, and Neo-GNN, achieved significantly higher performance than previously reported when properly tuned and evaluated under consistent settings. For example, Neo-GNN's Hits@50 on `ogbl-collab` increased from 57.52 to 66.13 \\cite{li2023o4c}. Heuristic methods on `ogbl-citation2` also saw substantial improvements (e.g., MRR around 75% vs. 50%) by treating the graph as undirected, consistent with GNN evaluations.\n        *   **Divergence from Reported Results**: Some methods, like BUDDY on `ogbl-ddi`, showed lower performance than previously reported, attributed to differences in training negative sampling strategies, further highlighting inconsistencies in prior work.\n        *   The new HeaRT evaluation setting is designed to expose new challenges and opportunities by aligning evaluation with real-world situations, though specific results from this setting are not detailed in the provided abstract and introduction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on homogeneous graphs. For large OGB datasets, hyperparameter search was constrained due to computational cost. The HeaRT method, while more realistic, still samples *K* negatives due to the prohibitive cost of considering all possible corruptions.\n    *   **Scope of Applicability**: The findings and proposed benchmark are most directly applicable to GNN-based link prediction on homogeneous graphs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art by establishing a robust, reproducible, and more realistic benchmarking framework for GNN-based link prediction. It corrects previous misrepresentations of model performance, providing a clearer picture of current capabilities.\n    *   **Potential Impact**: It provides a reliable foundation for future research, enabling fair comparisons and accelerating the development of more robust and generalizable link prediction models. By exposing the limitations of current evaluation practices and offering a more challenging setting, it encourages the community to develop models that perform better in real-world applications, thereby fostering more impactful research.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "link prediction",
        "evaluation pitfalls",
        "benchmarking framework",
        "reproducible comparison",
        "hyperparameter tuning",
        "unified data splits",
        "negative sampling",
        "Heuristic Related Sampling Technique (HeaRT)",
        "hard negative samples",
        "realistic evaluation setting",
        "model performance misrepresentation",
        "standardized methodology"
      ],
      "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n1.  **focus on evaluation and benchmarking:** the title \"evaluating graph neural networks for link prediction: current pitfalls and new benchmarking\" directly indicates an empirical focus. the abstract explicitly states the goal is to \"properly evaluate these new methods\" and to \"conduct a fair comparison across prominent methods and datasets.\" it also mentions creating a \"new evaluation setting\" and \"new benchmarking.\" these are all hallmarks of empirical research.\n\n2.  **data-driven studies:** the abstract mentions \"conduct a fair comparison across prominent methods and datasets,\" which involves running experiments and analyzing data. the \"new benchmarking\" is inherently a data-driven study.\n\n3.  **methodology and findings:** the paper identifies \"pitfalls\" in current evaluation practices, which are observations from existing empirical studies. it then proposes a new methodology (including heart) to overcome these pitfalls and conducts a new, improved empirical study.\n\n4.  **technical component serves empirical goal:** while the paper introduces a \"heuristic related sampling technique (heart),\" which is a technical contribution (a new method/algorithm for sampling negative samples), this technique is presented as a *component* of the \"more practical evaluation setting.\" its purpose is to enable a more robust and realistic *empirical evaluation*. the technical contribution serves the overarching empirical goal of better benchmarking.\n\n5.  **alignment with criteria:**\n    *   **empirical:** abstract mentions: \"study\", \"experiment\", \"data\", \"statistical\", \"findings\" (implied by \"fair comparison\" and \"new benchmarking\"). introduction discusses: research questions (implied by identifying \"pitfalls\"), methodology (new evaluation setting, heart). this aligns well.\n    *   **technical:** while it \"creates\" a new technique (heart), the primary contribution isn't just the technique itself, but its application within a comprehensive empirical evaluation framework. the paper isn't proposing a new gnn model, but a new way to *evaluate* gnn models.\n\nthe core of the paper is about how to conduct better data-driven studies (evaluations and benchmarks) for link prediction, identifying flaws in current practices, and then performing such an improved study."
    },
    "file_name": "f442378ead6282024cf5b9046daa10422fe9fc5f.pdf"
  },
  {
    "success": true,
    "doc_id": "05c143a81d2b8f84d6e6d1bc9ae2401a",
    "summary": "Here's a focused summary of the paper \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" \\cite{michel2023hc4} for a literature review:\n\n*   **CITATION**: \\cite{michel2023hc4}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressive power, being no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in distinguishing non-isomorphic graphs \\cite{michel2023hc4}.\n    *   This limitation restricts their ability to capture complex structural information, hindering performance on various graph learning tasks.\n    *   The problem is challenging because finding all paths in a graph is NP-hard, requiring efficient methods to leverage path information without incurring prohibitive computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Path-based GNNs**: Previous works incorporated shortest path distances as features (e.g., Graphormer, PEGN, SP-MPNN, Geodesic GNN) or aggregated nodes at specific shortest path distances.\n    *   **Limitations of Previous Path-based GNNs**: Many either use path information indirectly or focus on specific path types/lengths. The most related work, PathNet (Sun et al., 2022), samples paths, is only evaluated on node classification, and lacks an extensive study of expressive power, which PathNNs \\cite{michel2023hc4} explicitly address.\n    *   **Expressive GNNs**: Other approaches to enhance GNN expressive power include higher-order WL variants, k-order graph networks (some achieving 3-WL power), and methods using subgraphs or vertex identifiers. PathNNs \\cite{michel2023hc4} offer a distinct path-centric approach to achieve higher expressive power.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea**: Path Neural Networks (PathNNs) \\cite{michel2023hc4} update node representations by aggregating information from various paths emanating from each node.\n    *   **Path Encoding**: For each path length, a recurrent layer is used to encode paths into vectors.\n    *   **Aggregation**: The representations of all relevant paths emanating from a node are aggregated to produce the node's new representation.\n    *   **Three Variants**:\n        *   **Single Shortest Paths (SP)**: Aggregates a single shortest path for all possible node pair combinations.\n        *   **All Shortest Paths (SP+)**: Aggregates all possible shortest paths between every node pair combination.\n        *   **All Simple Paths (AP)**: Aggregates all simple paths (not necessarily shortest) up to a fixed length K.\n    *   **Key Innovation: Annotated Sets of Paths**: While initial `Path-Trees` (analogous to `WL-Trees`) were shown to be insufficient for `SP` and `SP+` to surpass 1-WL, the core innovation lies in operating on \"annotated sets of paths\" (`˜SP`, `˜SP+`, `˜AP`). Nodes within these paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, creating a richer, hierarchical structural encoding.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of Path Neural Networks (PathNNs) \\cite{michel2023hc4} as a novel GNN architecture that explicitly leverages path information for node representation learning.\n        *   Development of three distinct PathNN variants (SP, SP+, AP) based on different path collection strategies.\n        *   The concept and implementation of \"annotated sets of paths\" for recursively enriching path information, which is crucial for achieving higher expressive power.\n    *   **Theoretical Insights/Analysis**:\n        *   Formal definition of `Path-Trees` as a path-based analogue to `WL-Trees`.\n        *   Proof that `AP-Trees` are strictly more powerful than `WL-Trees` in distinguishing non-isomorphic graphs (Theorem 3.3).\n        *   Crucially, the theoretical demonstration that by operating on **annotated sets of paths**, two of the PathNN variants (`˜SP`, `˜SP+`) are strictly more powerful than the 1-WL algorithm, and the most expressive variant (`˜AP`) can even distinguish graphs indistinguishable by the 3-WL algorithm (Theorem 3.5, as implied by abstract/intro).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation on synthetic datasets specifically designed to measure expressive power, testing the ability to distinguish non-isomorphic graphs.\n        *   Performance evaluation on real-world graph classification and graph regression datasets.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Expressive Power**: PathNNs \\cite{michel2023hc4} successfully distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL. The most expressive PathNN variant (`˜AP`) demonstrated the ability to distinguish graphs that are even 3-WL indistinguishable, empirically validating the theoretical claims.\n        *   **Real-world Tasks**: On graph classification and regression datasets, the different PathNN variants achieved high levels of performance, outperforming baseline methods in most cases.\n\n6.  **Limitations & Scope**\n    *   **Computational Complexity**: Finding all simple paths (`AP` variant) in a graph is NP-hard. The model addresses this by considering paths only up to a fixed length `K`.\n    *   **Scalability of AP-Trees**: `AP-Trees` (and by extension, the `˜AP` variant) can grow exponentially with height `K` and graph density, potentially limiting the practical maximum path length `K` that can be used.\n    *   **Scope of Applicability**: The paper focuses on undirected graphs and does not explicitly discuss applicability to directed graphs or other graph types (e.g., hypergraphs).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PathNNs \\cite{michel2023hc4} significantly advance the technical state-of-the-art by providing GNN architectures that provably surpass the expressive power of the 1-WL algorithm, a known bottleneck for many standard GNNs. The ability to distinguish 3-WL indistinguishable graphs with a path-based approach is a notable achievement.\n    *   **Potential Impact on Future Research**: This work opens new avenues for designing more powerful GNNs by effectively leveraging path information. It highlights the importance of recursive, hierarchical structural encoding (via annotated paths) for enhancing expressive power. Future research could explore more efficient path enumeration/sampling strategies, adaptive path length selection, or the integration of path information with other structural elements (e.g., subgraphs) to further push the boundaries of GNN expressiveness and performance.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their expressive power is fundamentally limited by the 1-Weisfeiler-Leman (1-WL) algorithm, hindering their ability to capture intricate structural information crucial for complex graph learning tasks. We introduce **Path Neural Networks (PathNNs)**, a novel architecture designed to overcome this bottleneck by explicitly leveraging comprehensive path information. Our core innovation lies in operating on *annotated sets of paths*, where nodes are recursively enriched with hashes of their shorter path sets, creating a hierarchical structural encoding. We present three variants—Single Shortest Paths (SP), All Shortest Paths (SP+), and All Simple Paths (AP)—and theoretically demonstrate that our annotated versions (˜SP, ˜SP+) provably surpass the 1-WL algorithm. Remarkably, the ˜AP variant can even distinguish graphs that are indistinguishable by the 3-WL algorithm, a significant theoretical breakthrough. Extensive experiments on synthetic datasets confirm this superior expressive power, while evaluations on real-world graph classification and regression tasks show PathNNs achieving state-of-the-art performance. PathNNs pave the way for a new generation of GNNs capable of deeper structural understanding, pushing the boundaries of graph representation learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "expressive power",
      "Weisfeiler-Leman (WL) algorithm",
      "Path Neural Networks (PathNNs)",
      "path-based GNNs",
      "annotated sets of paths",
      "path encoding",
      "PathNN variants (SP",
      "SP+",
      "AP)",
      "surpassing 1-WL expressiveness",
      "distinguishing 3-WL indistinguishable graphs",
      "node representation learning",
      "graph classification",
      "graph regression"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf",
    "citation_key": "michel2023hc4",
    "metadata": {
      "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks",
      "authors": [
        "Gaspard Michel",
        "Giannis Nikolentzos",
        "J. Lutzeyer",
        "M. Vazirgiannis"
      ],
      "published_date": "2023",
      "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.",
      "file_path": "paper_data/Graph_Neural_Networks/20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" \\cite{michel2023hc4} for a literature review:\n\n*   **CITATION**: \\cite{michel2023hc4}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressive power, being no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in distinguishing non-isomorphic graphs \\cite{michel2023hc4}.\n    *   This limitation restricts their ability to capture complex structural information, hindering performance on various graph learning tasks.\n    *   The problem is challenging because finding all paths in a graph is NP-hard, requiring efficient methods to leverage path information without incurring prohibitive computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Path-based GNNs**: Previous works incorporated shortest path distances as features (e.g., Graphormer, PEGN, SP-MPNN, Geodesic GNN) or aggregated nodes at specific shortest path distances.\n    *   **Limitations of Previous Path-based GNNs**: Many either use path information indirectly or focus on specific path types/lengths. The most related work, PathNet (Sun et al., 2022), samples paths, is only evaluated on node classification, and lacks an extensive study of expressive power, which PathNNs \\cite{michel2023hc4} explicitly address.\n    *   **Expressive GNNs**: Other approaches to enhance GNN expressive power include higher-order WL variants, k-order graph networks (some achieving 3-WL power), and methods using subgraphs or vertex identifiers. PathNNs \\cite{michel2023hc4} offer a distinct path-centric approach to achieve higher expressive power.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea**: Path Neural Networks (PathNNs) \\cite{michel2023hc4} update node representations by aggregating information from various paths emanating from each node.\n    *   **Path Encoding**: For each path length, a recurrent layer is used to encode paths into vectors.\n    *   **Aggregation**: The representations of all relevant paths emanating from a node are aggregated to produce the node's new representation.\n    *   **Three Variants**:\n        *   **Single Shortest Paths (SP)**: Aggregates a single shortest path for all possible node pair combinations.\n        *   **All Shortest Paths (SP+)**: Aggregates all possible shortest paths between every node pair combination.\n        *   **All Simple Paths (AP)**: Aggregates all simple paths (not necessarily shortest) up to a fixed length K.\n    *   **Key Innovation: Annotated Sets of Paths**: While initial `Path-Trees` (analogous to `WL-Trees`) were shown to be insufficient for `SP` and `SP+` to surpass 1-WL, the core innovation lies in operating on \"annotated sets of paths\" (`˜SP`, `˜SP+`, `˜AP`). Nodes within these paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, creating a richer, hierarchical structural encoding.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of Path Neural Networks (PathNNs) \\cite{michel2023hc4} as a novel GNN architecture that explicitly leverages path information for node representation learning.\n        *   Development of three distinct PathNN variants (SP, SP+, AP) based on different path collection strategies.\n        *   The concept and implementation of \"annotated sets of paths\" for recursively enriching path information, which is crucial for achieving higher expressive power.\n    *   **Theoretical Insights/Analysis**:\n        *   Formal definition of `Path-Trees` as a path-based analogue to `WL-Trees`.\n        *   Proof that `AP-Trees` are strictly more powerful than `WL-Trees` in distinguishing non-isomorphic graphs (Theorem 3.3).\n        *   Crucially, the theoretical demonstration that by operating on **annotated sets of paths**, two of the PathNN variants (`˜SP`, `˜SP+`) are strictly more powerful than the 1-WL algorithm, and the most expressive variant (`˜AP`) can even distinguish graphs indistinguishable by the 3-WL algorithm (Theorem 3.5, as implied by abstract/intro).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation on synthetic datasets specifically designed to measure expressive power, testing the ability to distinguish non-isomorphic graphs.\n        *   Performance evaluation on real-world graph classification and graph regression datasets.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Expressive Power**: PathNNs \\cite{michel2023hc4} successfully distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL. The most expressive PathNN variant (`˜AP`) demonstrated the ability to distinguish graphs that are even 3-WL indistinguishable, empirically validating the theoretical claims.\n        *   **Real-world Tasks**: On graph classification and regression datasets, the different PathNN variants achieved high levels of performance, outperforming baseline methods in most cases.\n\n6.  **Limitations & Scope**\n    *   **Computational Complexity**: Finding all simple paths (`AP` variant) in a graph is NP-hard. The model addresses this by considering paths only up to a fixed length `K`.\n    *   **Scalability of AP-Trees**: `AP-Trees` (and by extension, the `˜AP` variant) can grow exponentially with height `K` and graph density, potentially limiting the practical maximum path length `K` that can be used.\n    *   **Scope of Applicability**: The paper focuses on undirected graphs and does not explicitly discuss applicability to directed graphs or other graph types (e.g., hypergraphs).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PathNNs \\cite{michel2023hc4} significantly advance the technical state-of-the-art by providing GNN architectures that provably surpass the expressive power of the 1-WL algorithm, a known bottleneck for many standard GNNs. The ability to distinguish 3-WL indistinguishable graphs with a path-based approach is a notable achievement.\n    *   **Potential Impact on Future Research**: This work opens new avenues for designing more powerful GNNs by effectively leveraging path information. It highlights the importance of recursive, hierarchical structural encoding (via annotated paths) for enhancing expressive power. Future research could explore more efficient path enumeration/sampling strategies, adaptive path length selection, or the integration of path information with other structural elements (e.g., subgraphs) to further push the boundaries of GNN expressiveness and performance.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "expressive power",
        "Weisfeiler-Leman (WL) algorithm",
        "Path Neural Networks (PathNNs)",
        "path-based GNNs",
        "annotated sets of paths",
        "path encoding",
        "PathNN variants (SP",
        "SP+",
        "AP)",
        "surpassing 1-WL expressiveness",
        "distinguishing 3-WL indistinguishable graphs",
        "node representation learning",
        "graph classification",
        "graph regression"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this paper, we **propose path neural networks (pathnns), a model** that updates node representations...\" and \"we **derive three different variants** of the pathnn model...\" this directly aligns with the \"technical\" classification criterion of presenting new methods, algorithms, or systems.\n*   while the paper also mentions \"we **prove** that two of these variants are strictly more powerful...\" (suggesting theoretical) and \"we **experimentally validate** our theoretical results\" and \"the different pathnn variants are also **evaluated on graph classification and graph regression datasets**...\" (suggesting empirical), the primary contribution is the **development and proposal of a new model (pathnns)**. the theoretical analysis and empirical evaluation serve to validate and demonstrate the capabilities of this new technical contribution.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf"
  },
  {
    "success": true,
    "doc_id": "56a2f477ec8e8b26f1daca00cad21c6b",
    "summary": "This paper, \"\\cite{chen2022mmu}\", provides a comprehensive survey of Graph Neural Networks (GNNs) and Graph Transformers (GTs) in computer vision, adopting a novel task-oriented perspective.\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the lack of a systematic, comprehensive, and timely literature review on the application and advancements of GNNs and GTs in computer vision. While deep learning (especially CNNs) excels with regular grid data, visual information with irregular topologies (e.g., object relations, point clouds, meshes) is crucial for representation learning but understudied by traditional methods.\n    *   **Importance & Challenge:** Understanding and leveraging relational information and topological structures is vital for complex vision tasks (e.g., scene understanding, learning from limited data). GNNs and GTs offer powerful mechanisms for modeling such irregular data and relationships. However, the rapid proliferation of GNN/GT-based methods across diverse computer vision tasks necessitates a structured overview to synthesize insights, identify limitations, and guide future research.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper acknowledges previous GNN surveys (e.g., \\cite{defferrard2016convolutional, kipf2016semi, velickovic2017graph}) that introduced GNN development, and some surveys that reviewed GNN applications in *certain* vision tasks.\n    *   **Limitations of Previous Solutions:** Existing surveys are deemed less comprehensive, lack a detailed examination of GNNs and GTs specifically in computer vision, and do not offer a sufficiently refined taxonomy or in-depth discussions on insights, limitations, and future directions from a task-oriented perspective.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's core approach is a novel, task-oriented taxonomy for organizing and reviewing GNN and GT applications in computer vision. It systematically categorizes applications first by input data modality (2D natural images, videos, 3D data, vision + language, medical images) and then by specific vision tasks within each modality.\n    *   **Novelty/Difference:** This task-oriented perspective is innovative as it allows for a granular examination of how different GNN-based approaches tackle specific vision problems, their performance, and their unique challenges. It also explicitly includes Graph Transformers, recognizing their architectural similarities and growing importance in vision. The survey provides definitions, challenges, representative approaches, and discussions for each task.\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy:** Introduction of a comprehensive, task-oriented taxonomy for GNNs and GTs in computer vision, structured by input data modality and specific vision tasks.\n    *   **In-depth Review:** Provides detailed coverage of representative GNN and GT approaches, including recurrent GNNs, spectral and spatial convolutional GNNs (e.g., ChebNet \\cite{defferrard2016convolutional}, GCNs \\cite{kipf2016semi}, GraphSAGE \\cite{hamilton2017inductive}, GAT \\cite{velickovic2017graph}), and new techniques like deeper GNNs \\cite{li2019deepgcn, rong2019dropedge}, graph pooling \\cite{ying2018hierarchical, ma2019graph}, and Vision GNNs (ViG) \\cite{chen2022vig}.\n    *   **Graph Transformer Analysis:** Dedicated review of Graph Transformers, particularly for 3D data (e.g., Point Transformer \\cite{zhao2021point}, Mesh Graphormer \\cite{lin2021meshgraphormer}), and a comparative analysis distinguishing GNNs from Vision Transformers (ViTs) based on their relational inductive biases.\n    *   **Insights and Future Directions:** Offers systematic discussions on insights, limitations, and potential future research directions for GNNs and GTs across various computer vision domains.\n\n5.  **Experimental Validation**\n    *   As a survey paper, \\cite{chen2022mmu} does not conduct its own experiments. Instead, it *reviews* and *reports* on the experimental validation and performance of the surveyed GNN and GT methods.\n    *   **Key Performance Metrics and Comparison Results (as reported from surveyed works):**\n        *   The paper highlights examples like ViG \\cite{chen2022vig} outperforming DeiT by 1.7% (top-1 accuracy) on ImageNet classification and Swin-T by 0.3% (mAP) on MSCOCO object detection, demonstrating the efficacy of GNN-based approaches.\n        *   It discusses how DeepGCNs \\cite{li2019deepgcn} achieve deeper architectures (e.g., 56-layer GCN for point cloud semantic segmentation) by incorporating CNN concepts like residual connections.\n        *   It notes that methods like Li et al. \\cite{li2020training} enable training extremely deep and wide GNNs (e.g., 1001 layers) through reversible connections, improving memory and parameter efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on the GNN aspects even when GNN-based vision approaches use a mixture of CNNs and GNNs. It assumes readers have some familiarity with basic GNN concepts, though it provides a recap.\n    *   **Scope of Applicability:** The survey's scope is limited to GNNs and Graph Transformers within the domain of computer vision. While comprehensive within this scope, it does not delve into GNN applications in other fields like data mining or NLP, except as general context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{chen2022mmu} significantly advances the technical state-of-the-art by providing the first comprehensive, task-oriented overview of GNNs and GTs in computer vision. It synthesizes a vast and rapidly growing body of literature, making it accessible and structured.\n    *   **Potential Impact on Future Research:** This survey serves as a crucial reference for researchers and practitioners, offering a clear understanding of current trends, effective methodologies, and open challenges. Its detailed taxonomy and discussions on insights, limitations, and future directions are invaluable for identifying promising research avenues, fostering interdisciplinary connections, and accelerating the development of more robust and intelligent vision systems leveraging graph-structured data.",
    "intriguing_abstract": "While convolutional neural networks have revolutionized computer vision, they often struggle with the inherent irregular topologies and relational complexities of real-world visual data, such as object interactions, point clouds, and meshes. Graph Neural Networks (GNNs) and the emerging Graph Transformers (GTs) offer powerful paradigms to model these crucial structural relationships. This paper presents the first comprehensive and timely survey of GNNs and GTs specifically tailored for computer vision, adopting a novel task-oriented perspective.\n\nWe introduce a systematic taxonomy, categorizing applications by input data modality and specific vision tasks, providing an unparalleled granular examination of diverse approaches. Beyond traditional GNNs (e.g., GCNs, GAT, GraphSAGE), we extensively review advanced techniques like deeper GNNs, graph pooling, and Vision GNNs (ViG), alongside a dedicated analysis of Graph Transformers (e.g., Point Transformer, Mesh Graphormer) and their distinct relational inductive biases compared to Vision Transformers. This survey synthesizes a rapidly expanding field, offering critical insights into current limitations and charting promising future research directions. It serves as an indispensable resource for researchers, accelerating the development of more intelligent and robust vision systems capable of understanding complex visual structures.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Graph Transformers (GTs)",
      "Computer Vision",
      "Task-oriented Taxonomy",
      "Irregular Data Modeling",
      "Relational Information",
      "3D Data Processing",
      "Vision GNNs (ViG)",
      "Deep GNN Architectures",
      "Scene Understanding",
      "Comprehensive Survey",
      "Future Research Directions",
      "Representation Learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/741a7faf9dbefd418cda878c61c5b839ecc02977.pdf",
    "citation_key": "chen2022mmu",
    "metadata": {
      "title": "A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective",
      "authors": [
        "Chaoqi Chen",
        "Yushuang Wu",
        "Qiyuan Dai",
        "Hong-Yu Zhou",
        "Mutian Xu",
        "Sibei Yang",
        "Xiaoguang Han",
        "Yizhou Yu"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (e.g., social network analysis and recommender systems), computer vision (e.g., object detection and point cloud learning), and natural language processing (e.g., relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, i.e., 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.",
      "file_path": "paper_data/Graph_Neural_Networks/741a7faf9dbefd418cda878c61c5b839ecc02977.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, \"\\cite{chen2022mmu}\", provides a comprehensive survey of Graph Neural Networks (GNNs) and Graph Transformers (GTs) in computer vision, adopting a novel task-oriented perspective.\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the lack of a systematic, comprehensive, and timely literature review on the application and advancements of GNNs and GTs in computer vision. While deep learning (especially CNNs) excels with regular grid data, visual information with irregular topologies (e.g., object relations, point clouds, meshes) is crucial for representation learning but understudied by traditional methods.\n    *   **Importance & Challenge:** Understanding and leveraging relational information and topological structures is vital for complex vision tasks (e.g., scene understanding, learning from limited data). GNNs and GTs offer powerful mechanisms for modeling such irregular data and relationships. However, the rapid proliferation of GNN/GT-based methods across diverse computer vision tasks necessitates a structured overview to synthesize insights, identify limitations, and guide future research.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper acknowledges previous GNN surveys (e.g., \\cite{defferrard2016convolutional, kipf2016semi, velickovic2017graph}) that introduced GNN development, and some surveys that reviewed GNN applications in *certain* vision tasks.\n    *   **Limitations of Previous Solutions:** Existing surveys are deemed less comprehensive, lack a detailed examination of GNNs and GTs specifically in computer vision, and do not offer a sufficiently refined taxonomy or in-depth discussions on insights, limitations, and future directions from a task-oriented perspective.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's core approach is a novel, task-oriented taxonomy for organizing and reviewing GNN and GT applications in computer vision. It systematically categorizes applications first by input data modality (2D natural images, videos, 3D data, vision + language, medical images) and then by specific vision tasks within each modality.\n    *   **Novelty/Difference:** This task-oriented perspective is innovative as it allows for a granular examination of how different GNN-based approaches tackle specific vision problems, their performance, and their unique challenges. It also explicitly includes Graph Transformers, recognizing their architectural similarities and growing importance in vision. The survey provides definitions, challenges, representative approaches, and discussions for each task.\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy:** Introduction of a comprehensive, task-oriented taxonomy for GNNs and GTs in computer vision, structured by input data modality and specific vision tasks.\n    *   **In-depth Review:** Provides detailed coverage of representative GNN and GT approaches, including recurrent GNNs, spectral and spatial convolutional GNNs (e.g., ChebNet \\cite{defferrard2016convolutional}, GCNs \\cite{kipf2016semi}, GraphSAGE \\cite{hamilton2017inductive}, GAT \\cite{velickovic2017graph}), and new techniques like deeper GNNs \\cite{li2019deepgcn, rong2019dropedge}, graph pooling \\cite{ying2018hierarchical, ma2019graph}, and Vision GNNs (ViG) \\cite{chen2022vig}.\n    *   **Graph Transformer Analysis:** Dedicated review of Graph Transformers, particularly for 3D data (e.g., Point Transformer \\cite{zhao2021point}, Mesh Graphormer \\cite{lin2021meshgraphormer}), and a comparative analysis distinguishing GNNs from Vision Transformers (ViTs) based on their relational inductive biases.\n    *   **Insights and Future Directions:** Offers systematic discussions on insights, limitations, and potential future research directions for GNNs and GTs across various computer vision domains.\n\n5.  **Experimental Validation**\n    *   As a survey paper, \\cite{chen2022mmu} does not conduct its own experiments. Instead, it *reviews* and *reports* on the experimental validation and performance of the surveyed GNN and GT methods.\n    *   **Key Performance Metrics and Comparison Results (as reported from surveyed works):**\n        *   The paper highlights examples like ViG \\cite{chen2022vig} outperforming DeiT by 1.7% (top-1 accuracy) on ImageNet classification and Swin-T by 0.3% (mAP) on MSCOCO object detection, demonstrating the efficacy of GNN-based approaches.\n        *   It discusses how DeepGCNs \\cite{li2019deepgcn} achieve deeper architectures (e.g., 56-layer GCN for point cloud semantic segmentation) by incorporating CNN concepts like residual connections.\n        *   It notes that methods like Li et al. \\cite{li2020training} enable training extremely deep and wide GNNs (e.g., 1001 layers) through reversible connections, improving memory and parameter efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on the GNN aspects even when GNN-based vision approaches use a mixture of CNNs and GNNs. It assumes readers have some familiarity with basic GNN concepts, though it provides a recap.\n    *   **Scope of Applicability:** The survey's scope is limited to GNNs and Graph Transformers within the domain of computer vision. While comprehensive within this scope, it does not delve into GNN applications in other fields like data mining or NLP, except as general context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{chen2022mmu} significantly advances the technical state-of-the-art by providing the first comprehensive, task-oriented overview of GNNs and GTs in computer vision. It synthesizes a vast and rapidly growing body of literature, making it accessible and structured.\n    *   **Potential Impact on Future Research:** This survey serves as a crucial reference for researchers and practitioners, offering a clear understanding of current trends, effective methodologies, and open challenges. Its detailed taxonomy and discussions on insights, limitations, and future directions are invaluable for identifying promising research avenues, fostering interdisciplinary connections, and accelerating the development of more robust and intelligent vision systems leveraging graph-structured data.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Graph Transformers (GTs)",
        "Computer Vision",
        "Task-oriented Taxonomy",
        "Irregular Data Modeling",
        "Relational Information",
        "3D Data Processing",
        "Vision GNNs (ViG)",
        "Deep GNN Architectures",
        "Scene Understanding",
        "Comprehensive Survey",
        "Future Research Directions",
        "Representation Learning"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "741a7faf9dbefd418cda878c61c5b839ecc02977.pdf"
  },
  {
    "success": true,
    "doc_id": "36d452d87498c9e8a500308af22a148b",
    "summary": "Here's a focused summary of the paper \\cite{yuan2021pgd} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the black-box predictions of Graph Neural Networks (GNNs).\n    *   **Importance and Challenge**: GNNs' lack of explainability hinders trust and application in critical domains. Existing GNN explanation methods primarily focus on node, edge, or node feature importance, neglecting the more intuitive and human-intelligible concept of *subgraphs*. Graph data's non-grid-like structure and crucial structural information make direct application of image/text explanation methods unsuitable.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Acknowledges prior GNN explanation methods like GNNExplainer, PGExplainer, and PGM-Explainer, as well as XGNN.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods invariably focus on node, edge, or node feature levels, only considering subgraphs indirectly (e.g., via regularization terms or post-processing).\n        *   Important nodes/edges identified by previous methods are not guaranteed to be connected, making them less intuitive as \"subgraphs.\"\n        *   These methods often ignore the crucial interactions among different nodes and edges.\n        *   XGNN provides general graph patterns but lacks input-dependent, precise explanations.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **SubgraphX**, a novel method to explain GNN predictions by directly identifying important subgraphs.\n    *   **Novelty/Difference**:\n        *   **Direct Subgraph Identification**: It is the first attempt to explain GNNs by explicitly and directly identifying connected subgraphs, rather than inferring them from node/edge importance.\n        *   **Efficient Subgraph Exploration**: Employs Monte Carlo Tree Search (MCTS) to efficiently explore the vast space of possible subgraphs.\n        *   **Interaction-Aware Importance Scoring**: Utilizes Shapley values from cooperative game theory to measure subgraph importance, which inherently captures interactions among different graph structures.\n        *   **Graph-Inspired Efficient Computations**: Proposes efficient approximation schemes for Shapley values tailored for graph data, by considering interactions primarily within the GNN's L-hop information aggregation range and employing Monte-Carlo sampling.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The SubgraphX framework, integrating MCTS for subgraph exploration and Shapley values for importance scoring.\n        *   A novel application of MCTS to guide the search for important *connected* subgraphs by defining pruning actions and using subgraph importance as rewards.\n        *   Adaptation of Shapley values as a game-theoretical scoring function to quantify subgraph importance, ensuring fairness and capturing structural interactions.\n        *   Development of graph-specific approximation schemes for Shapley values, leveraging the L-hop receptive field of GNNs and Monte-Carlo sampling to expedite computation.\n    *   **Theoretical Insights**: Highlights that Shapley values are the only solution satisfying desirable axioms (efficiency, symmetry, linearity, and dummy axiom), ensuring the correctness and fairness of the explanations.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Both qualitative and quantitative experiments were performed to evaluate the effectiveness and efficiency of SubgraphX.\n    *   **Key Performance Metrics and Comparison Results**: Experimental results demonstrate that SubgraphX achieves \"significantly improved explanations\" and provides \"better explanations for a variety of GNN models\" compared to existing methods. It maintains a \"reasonable computational cost\" despite its superior performance.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The brute-force computation of Shapley values is intractable for large graphs, necessitating the proposed approximation schemes.\n        *   The method considers connected subgraphs for human intelligibility.\n        *   An upper bound (`Nmin`) on the size of subgraphs is used to ensure succinct explanations.\n    *   **Scope of Applicability**: Primarily demonstrated for graph classification models, but the authors state it can be easily extended to other search algorithms and scoring functions.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: SubgraphX represents a significant step forward by being the first to directly and explicitly identify subgraphs for GNN explanations, moving beyond node/edge-level interpretations. This provides more intuitive and human-intelligible explanations.\n    *   **Potential Impact on Future Research**: This work lays a foundation for subgraph-level explainability in GNNs, potentially fostering new research directions in understanding complex graph models, improving trust, and enabling their deployment in sensitive applications where interpretability is paramount. The integration of game theory and efficient search algorithms for graph explainability is a notable contribution.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) offer unparalleled power in analyzing complex graph data, yet their inherent black-box nature severely impedes trust and limits deployment in critical, high-stakes domains. Existing GNN explainability methods predominantly focus on fragmented node or edge importance, often failing to provide intuitive, human-intelligible explanations that capture the crucial structural interactions within a graph.\n\nWe introduce **SubgraphX**, a pioneering framework that directly identifies the most important *connected subgraphs* responsible for a GNN's prediction, moving beyond indirect inference from individual component importance. SubgraphX ingeniously combines Monte Carlo Tree Search (MCTS) for efficient exploration of the vast subgraph space with a novel application of Shapley values. This game-theoretic approach rigorously quantifies subgraph importance, inherently capturing complex interactions among graph components and ensuring fair, accurate attribution. To address computational intractability, we propose efficient, graph-specific approximation schemes for Shapley values, tailored to the GNN's receptive field.\n\nExtensive experiments demonstrate that SubgraphX delivers significantly improved and more intuitive explanations across various GNN models, all while maintaining reasonable computational costs. By providing direct, interaction-aware subgraph explanations, SubgraphX not only advances the state-of-the-art in GNN interpretability but also paves the way for greater trust, broader adoption, and new research into understanding complex graph-structured data.",
    "keywords": [
      "GNN explainability",
      "SubgraphX",
      "direct subgraph identification",
      "Monte Carlo Tree Search (MCTS)",
      "Shapley values",
      "interaction-aware importance scoring",
      "graph-specific approximation schemes",
      "human-intelligible explanations",
      "black-box GNN predictions",
      "L-hop receptive field",
      "graph classification",
      "computational efficiency"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/123139463809b5acf98b95d4c8e958be334a32b5.pdf",
    "citation_key": "yuan2021pgd",
    "metadata": {
      "title": "On Explainability of Graph Neural Networks via Subgraph Explorations",
      "authors": [
        "Hao Yuan",
        "Haiyang Yu",
        "Jie Wang",
        "Kang Li",
        "Shuiwang Ji"
      ],
      "published_date": "2021",
      "abstract": "We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.",
      "file_path": "paper_data/Graph_Neural_Networks/123139463809b5acf98b95d4c8e958be334a32b5.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{yuan2021pgd} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the black-box predictions of Graph Neural Networks (GNNs).\n    *   **Importance and Challenge**: GNNs' lack of explainability hinders trust and application in critical domains. Existing GNN explanation methods primarily focus on node, edge, or node feature importance, neglecting the more intuitive and human-intelligible concept of *subgraphs*. Graph data's non-grid-like structure and crucial structural information make direct application of image/text explanation methods unsuitable.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Acknowledges prior GNN explanation methods like GNNExplainer, PGExplainer, and PGM-Explainer, as well as XGNN.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods invariably focus on node, edge, or node feature levels, only considering subgraphs indirectly (e.g., via regularization terms or post-processing).\n        *   Important nodes/edges identified by previous methods are not guaranteed to be connected, making them less intuitive as \"subgraphs.\"\n        *   These methods often ignore the crucial interactions among different nodes and edges.\n        *   XGNN provides general graph patterns but lacks input-dependent, precise explanations.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **SubgraphX**, a novel method to explain GNN predictions by directly identifying important subgraphs.\n    *   **Novelty/Difference**:\n        *   **Direct Subgraph Identification**: It is the first attempt to explain GNNs by explicitly and directly identifying connected subgraphs, rather than inferring them from node/edge importance.\n        *   **Efficient Subgraph Exploration**: Employs Monte Carlo Tree Search (MCTS) to efficiently explore the vast space of possible subgraphs.\n        *   **Interaction-Aware Importance Scoring**: Utilizes Shapley values from cooperative game theory to measure subgraph importance, which inherently captures interactions among different graph structures.\n        *   **Graph-Inspired Efficient Computations**: Proposes efficient approximation schemes for Shapley values tailored for graph data, by considering interactions primarily within the GNN's L-hop information aggregation range and employing Monte-Carlo sampling.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The SubgraphX framework, integrating MCTS for subgraph exploration and Shapley values for importance scoring.\n        *   A novel application of MCTS to guide the search for important *connected* subgraphs by defining pruning actions and using subgraph importance as rewards.\n        *   Adaptation of Shapley values as a game-theoretical scoring function to quantify subgraph importance, ensuring fairness and capturing structural interactions.\n        *   Development of graph-specific approximation schemes for Shapley values, leveraging the L-hop receptive field of GNNs and Monte-Carlo sampling to expedite computation.\n    *   **Theoretical Insights**: Highlights that Shapley values are the only solution satisfying desirable axioms (efficiency, symmetry, linearity, and dummy axiom), ensuring the correctness and fairness of the explanations.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Both qualitative and quantitative experiments were performed to evaluate the effectiveness and efficiency of SubgraphX.\n    *   **Key Performance Metrics and Comparison Results**: Experimental results demonstrate that SubgraphX achieves \"significantly improved explanations\" and provides \"better explanations for a variety of GNN models\" compared to existing methods. It maintains a \"reasonable computational cost\" despite its superior performance.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The brute-force computation of Shapley values is intractable for large graphs, necessitating the proposed approximation schemes.\n        *   The method considers connected subgraphs for human intelligibility.\n        *   An upper bound (`Nmin`) on the size of subgraphs is used to ensure succinct explanations.\n    *   **Scope of Applicability**: Primarily demonstrated for graph classification models, but the authors state it can be easily extended to other search algorithms and scoring functions.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: SubgraphX represents a significant step forward by being the first to directly and explicitly identify subgraphs for GNN explanations, moving beyond node/edge-level interpretations. This provides more intuitive and human-intelligible explanations.\n    *   **Potential Impact on Future Research**: This work lays a foundation for subgraph-level explainability in GNNs, potentially fostering new research directions in understanding complex graph models, improving trust, and enabling their deployment in sensitive applications where interpretability is paramount. The integration of game theory and efficient search algorithms for graph explainability is a notable contribution.",
      "keywords": [
        "GNN explainability",
        "SubgraphX",
        "direct subgraph identification",
        "Monte Carlo Tree Search (MCTS)",
        "Shapley values",
        "interaction-aware importance scoring",
        "graph-specific approximation schemes",
        "human-intelligible explanations",
        "black-box GNN predictions",
        "L-hop receptive field",
        "graph classification",
        "computational efficiency"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose a novel method**, known as subgraphx...\", \"we **propose to use shapley values**...\", \"we **propose efﬁcient approximation schemes**...\".\n*   it describes the mechanism of this new method: \"efﬁciently exploring different subgraphs with monte carlo tree search.\"\n*   the introduction sets up a technical problem: gnns are \"black boxes\" and \"lack explanations,\" raising \"the need of investigating the explainability of deep graph models.\"\n*   the paper's core contribution is the development and presentation of a new method/algorithm to solve this technical problem. while it mentions \"experimental results show that our subgraphx achieves significantly improved explanations,\" this is the validation of the proposed method, not the primary focus of the paper as an empirical study itself.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
    },
    "file_name": "123139463809b5acf98b95d4c8e958be334a32b5.pdf"
  },
  {
    "success": true,
    "doc_id": "fb41078d59dda491e65c3421ec8a4df7",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Conventional Graph Neural Networks (GNNs) often lack fairness considerations, particularly at the individual level \\cite{dong202183w}. While some research addresses group fairness (fairness across predefined subgroups), there's a critical need to ensure *individual fairness* – that similar individuals receive similar prediction results.\n    *   **Importance & Challenge**: Individual fairness is crucial for high-stake decision-making scenarios. Traditional definitions of individual fairness, often based on Lipschitz conditions, face challenges with Lipschitz constant specification and distance calibration, making them difficult to apply effectively \\cite{dong202183w}.\n\n2.  **Related Work & Positioning**\n    *   **Relation**: This work builds upon existing research in GNN fairness but shifts focus from *group fairness* (fairness for subgroups based on protected attributes like gender or race) to *individual fairness* (fairness at the node level for similar individuals) \\cite{dong202183w}.\n    *   **Limitations of Previous Solutions**: Prior individual fairness definitions based on Lipschitz conditions are hampered by practical difficulties in specifying the Lipschitz constant and calibrating distances, which REDRESS aims to overcome \\cite{dong202183w}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes REDRESS, a novel ranking-based framework designed to enhance the individual fairness of GNNs \\cite{dong202183w}.\n    *   **Novelty**:\n        *   It refines the notion of individual fairness from a *ranking perspective*, which naturally addresses the issues of Lipschitz constant specification and distance calibration inherent in conventional Lipschitz-based definitions \\cite{dong202183w}.\n        *   REDRESS formulates a *ranking-based individual fairness promotion problem*.\n        *   The framework encapsulates GNN model utility maximization and ranking-based individual fairness promotion in a *joint, end-to-end training framework* \\cite{dong202183w}.\n        *   It is designed as a *plug-and-play framework*, making it easily generalizable to any prevalent GNN architecture.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of REDRESS, a novel ranking-based framework for individual fairness in GNNs \\cite{dong202183w}.\n    *   **Theoretical Insights/Analysis**: Refinement of the individual fairness notion from a ranking perspective, leading to a new formulation that bypasses challenges of Lipschitz conditions \\cite{dong202183w}.\n    *   **System Design/Architectural Innovations**: A joint optimization framework that simultaneously maximizes model utility and promotes ranking-based individual fairness, designed to be plug-and-play with existing GNN architectures \\cite{dong202183w}.\n\n5.  **Experimental Validation**\n    *   **Experiments**: Extensive experiments were conducted on *multiple real-world graphs* \\cite{dong202183w}.\n    *   **Performance Metrics & Results**: The results demonstrate the *superiority of REDRESS* in achieving a good balance between maximizing model utility and promoting individual fairness \\cite{dong202183w}.\n\n6.  **Limitations & Scope**\n    *   **Scope**: The paper presents an \"initial investigation\" into enhancing individual fairness of GNNs \\cite{dong202183w}. While the framework is plug-and-play and generalizable, specific technical limitations are not detailed in the provided abstract.\n\n7.  **Technical Significance**\n    *   **Advancement**: This work significantly advances the technical state-of-the-art by moving beyond group fairness to address the more granular and challenging problem of individual fairness in GNNs \\cite{dong202183w}.\n    *   **Potential Impact**: By providing a practical, generalizable, and effective ranking-based framework, REDRESS offers a new paradigm for designing fair GNNs, potentially impacting future research in trustworthy AI and fair machine learning on graph-structured data. Its plug-and-play nature facilitates broader adoption and integration into existing GNN pipelines \\cite{dong202183w}.",
    "intriguing_abstract": "The pervasive deployment of Graph Neural Networks (GNNs) in high-stakes applications urgently demands robust fairness guarantees, particularly at the individual level. While group fairness has garnered attention, ensuring *individual fairness*—that similar individuals receive similar predictions—remains a formidable challenge. Traditional Lipschitz-based definitions, plagued by issues of constant specification and distance calibration, have severely hindered practical implementation.\n\nWe introduce REDRESS, a groundbreaking *ranking-based framework* that fundamentally redefines and promotes individual fairness in GNNs. Moving beyond the limitations of conventional approaches, REDRESS reframes individual fairness from a novel *ranking perspective*, inherently sidestepping the complexities of Lipschitz conditions. Our framework formulates a *ranking-based individual fairness promotion problem* and integrates it with GNN utility maximization into a *joint, end-to-end training pipeline*. Designed as a *plug-and-play* module, REDRESS seamlessly integrates with any prevalent GNN architecture, offering unprecedented generalizability. Extensive experiments on *multiple real-world graphs* demonstrate REDRESS's superior ability to achieve an optimal balance between model utility and individual fairness. This work pioneers a new paradigm for building *trustworthy AI* on *graph-structured data*, making fair, high-stakes decision-making a practical reality.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Individual fairness",
      "REDRESS framework",
      "Ranking-based individual fairness",
      "Lipschitz conditions",
      "Joint optimization framework",
      "Plug-and-play framework",
      "Model utility maximization",
      "Graph-structured data",
      "Fair machine learning",
      "Trustworthy AI",
      "High-stake decision-making"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf",
    "citation_key": "dong202183w",
    "metadata": {
      "title": "Individual Fairness for Graph Neural Networks: A Ranking based Approach",
      "authors": [
        "Yushun Dong",
        "Jian Kang",
        "H. Tong",
        "Jundong Li"
      ],
      "published_date": "2021",
      "abstract": "Recent years have witnessed the pivotal role of Graph Neural Networks (GNNs) in various high-stake decision-making scenarios due to their superior learning capability. Close on the heels of the successful adoption of GNNs in different application domains has been the increasing societal concern that conventional GNNs often do not have fairness considerations. Although some research progress has been made to improve the fairness of GNNs, these works mainly focus on the notion of group fairness regarding different subgroups defined by a protected attribute such as gender, age, and race. Beyond that, it is also essential to study the GNN fairness at a much finer granularity (i.e., at the node level) to ensure that GNNs render similar prediction results for similar individuals to achieve the notion of individual fairness. Toward this goal, in this paper, we make an initial investigation to enhance the individual fairness of GNNs and propose a novel ranking based framework---REDRESS. Specifically, we refine the notion of individual fairness from a ranking perspective, and formulate the ranking based individual fairness promotion problem. This naturally addresses the issue of Lipschitz constant specification and distance calibration resulted from the Lipschitz condition in the conventional individual fairness definition. Our proposed framework REDRESS encapsulates the GNN model utility maximization and the ranking-based individual fairness promotion in a joint framework to enable end-to-end training. It is noteworthy mentioning that REDRESS is a plug-and-play framework and can be easily generalized to any prevalent GNN architectures. Extensive experiments on multiple real-world graphs demonstrate the superiority of REDRESS in achieving a good balance between model utility maximization and individual fairness promotion. Our open source code can be found here: https://github.com/yushundong/REDRESS.",
      "file_path": "paper_data/Graph_Neural_Networks/fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Conventional Graph Neural Networks (GNNs) often lack fairness considerations, particularly at the individual level \\cite{dong202183w}. While some research addresses group fairness (fairness across predefined subgroups), there's a critical need to ensure *individual fairness* – that similar individuals receive similar prediction results.\n    *   **Importance & Challenge**: Individual fairness is crucial for high-stake decision-making scenarios. Traditional definitions of individual fairness, often based on Lipschitz conditions, face challenges with Lipschitz constant specification and distance calibration, making them difficult to apply effectively \\cite{dong202183w}.\n\n2.  **Related Work & Positioning**\n    *   **Relation**: This work builds upon existing research in GNN fairness but shifts focus from *group fairness* (fairness for subgroups based on protected attributes like gender or race) to *individual fairness* (fairness at the node level for similar individuals) \\cite{dong202183w}.\n    *   **Limitations of Previous Solutions**: Prior individual fairness definitions based on Lipschitz conditions are hampered by practical difficulties in specifying the Lipschitz constant and calibrating distances, which REDRESS aims to overcome \\cite{dong202183w}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes REDRESS, a novel ranking-based framework designed to enhance the individual fairness of GNNs \\cite{dong202183w}.\n    *   **Novelty**:\n        *   It refines the notion of individual fairness from a *ranking perspective*, which naturally addresses the issues of Lipschitz constant specification and distance calibration inherent in conventional Lipschitz-based definitions \\cite{dong202183w}.\n        *   REDRESS formulates a *ranking-based individual fairness promotion problem*.\n        *   The framework encapsulates GNN model utility maximization and ranking-based individual fairness promotion in a *joint, end-to-end training framework* \\cite{dong202183w}.\n        *   It is designed as a *plug-and-play framework*, making it easily generalizable to any prevalent GNN architecture.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of REDRESS, a novel ranking-based framework for individual fairness in GNNs \\cite{dong202183w}.\n    *   **Theoretical Insights/Analysis**: Refinement of the individual fairness notion from a ranking perspective, leading to a new formulation that bypasses challenges of Lipschitz conditions \\cite{dong202183w}.\n    *   **System Design/Architectural Innovations**: A joint optimization framework that simultaneously maximizes model utility and promotes ranking-based individual fairness, designed to be plug-and-play with existing GNN architectures \\cite{dong202183w}.\n\n5.  **Experimental Validation**\n    *   **Experiments**: Extensive experiments were conducted on *multiple real-world graphs* \\cite{dong202183w}.\n    *   **Performance Metrics & Results**: The results demonstrate the *superiority of REDRESS* in achieving a good balance between maximizing model utility and promoting individual fairness \\cite{dong202183w}.\n\n6.  **Limitations & Scope**\n    *   **Scope**: The paper presents an \"initial investigation\" into enhancing individual fairness of GNNs \\cite{dong202183w}. While the framework is plug-and-play and generalizable, specific technical limitations are not detailed in the provided abstract.\n\n7.  **Technical Significance**\n    *   **Advancement**: This work significantly advances the technical state-of-the-art by moving beyond group fairness to address the more granular and challenging problem of individual fairness in GNNs \\cite{dong202183w}.\n    *   **Potential Impact**: By providing a practical, generalizable, and effective ranking-based framework, REDRESS offers a new paradigm for designing fair GNNs, potentially impacting future research in trustworthy AI and fair machine learning on graph-structured data. Its plug-and-play nature facilitates broader adoption and integration into existing GNN pipelines \\cite{dong202183w}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Individual fairness",
        "REDRESS framework",
        "Ranking-based individual fairness",
        "Lipschitz conditions",
        "Joint optimization framework",
        "Plug-and-play framework",
        "Model utility maximization",
        "Graph-structured data",
        "Fair machine learning",
        "Trustworthy AI",
        "High-stake decision-making"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the paper identifies a problem: conventional gnns lack individual fairness.\n*   it explicitly states: \"we refine the notion of individual fairness from a ranking perspective, and formulate the ranking based individual fairness promotion problem.\"\n*   it proposes a \"framework redress\" which \"encapsulates the gnn model utility maximization and the ranking-based individual fairness promotion in a joint framework.\"\n*   it highlights that redress is \"plug-and-play\" and \"can be easily generalized.\"\n*   it mentions \"extensive experiments on multiple real-world graphs demonstrate the superiority of redress.\"\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems (in this case, a new framework and approach for individual fairness in gnns). while it includes empirical validation, the core contribution described is the *development* of this new solution.\n\n**classification: technical**"
    },
    "file_name": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf"
  },
  {
    "success": true,
    "doc_id": "b7fff345aa6bf149b75871d9faf5cd63",
    "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n**CITATION REQUIREMENTS**: Always use \"\\cite{cappart2021xrp}\" when referencing this paper.\n\n---\n\n### Technical Paper Analysis: Combinatorial Optimization and Reasoning with Graph Neural Networks \\cite{cappart2021xrp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the challenge of effectively applying machine learning (ML), particularly Graph Neural Networks (GNNs), to Combinatorial Optimization (CO) problems. Traditionally, CO methods solve problem instances in isolation, despite many practical instances stemming from related data distributions.\n    *   **Why is this problem important and challenging?** CO problems are often NP-hard and computationally intensive. Exploiting common patterns across instances using ML could lead to faster and more efficient algorithms for real-world applications (e.g., vehicle routing, scheduling). Key challenges for ML in CO include:\n        *   **Permutation invariance:** Graphs have no unique representation, requiring methods invariant to node reordering.\n        *   **Scalability and sparsity:** Real-world CO instances are often large and sparse, demanding scalable and sparsity-aware ML models.\n        *   **Expressivity:** Models must be expressive enough to detect and exploit critical structural patterns.\n        *   **Handling side information:** Incorporating auxiliary information like objective functions and user-defined constraints.\n        *   **Data efficiency:** Reducing the reliance on large amounts of labeled training data (which often means solving many hard CO instances).\n        *   **Generalization:** The ability to transfer learned knowledge to unseen instances, including those of different sizes or distributions.\n\n2.  **Related Work & Positioning**\n    *   **How does this work relate to existing approaches?** This paper is a conceptual review that synthesizes recent advancements in using GNNs for CO. It positions itself against:\n        *   **Traditional CO solvers:** Which focus on individual instances without leveraging data distributions.\n        *   **Early ML for CO:** Such as Hopfield networks and self-organizing maps, which were often applied to single instances rather than being trained over datasets.\n        *   **Other contemporary surveys:** It differentiates itself by providing a comprehensive, structured overview specifically on GNNs in CO, covering both heuristic and exact algorithms, and end-to-end algorithmic reasoning. Other surveys often have a broader ML focus, concentrate on specific ML paradigms (e.g., reinforcement learning), or miss recent GNN developments \\cite{cappart2021xrp}.\n    *   **Limitations of previous solutions:**\n        *   Classical CO algorithms heavily rely on human-made pre-processing and feature engineering, which can be costly, error-prone, and introduce biases that may not align with real-world data \\cite{cappart2021xrp}.\n        *   Earlier neural network applications for CO had limited success due to their instance-specific nature, lacking generalization across problem distributions \\cite{cappart2021xrp}.\n        *   Existing surveys, while valuable, often lack the specific focus and depth on GNNs' unique advantages and challenges within the CO domain, particularly regarding algorithmic reasoning \\cite{cappart2021xrp}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper reviews the application of Graph Neural Networks (GNNs) as the core technical method for addressing CO problems. GNNs compute vectorial representations for each node by iteratively aggregating features from neighboring nodes. This process is trained end-to-end to adapt to specific data distributions.\n    *   **What makes this approach novel or different?** The paper itself is a *review*, and its innovation lies in providing a novel, structured, and comprehensive overview of how GNNs are being used in CO. It highlights GNNs' inherent properties that make them suitable:\n        *   **Permutation invariance and equivariance:** GNNs are designed to automatically exploit symmetries in graph-structured data.\n        *   **Sparsity awareness:** Their local aggregation mechanism naturally handles sparse inputs, leading to more scalable models.\n        *   **Feature integration:** GNNs can incorporate multi-dimensional node and edge features, naturally exploiting cost and objective function information \\cite{cappart2021xrp}.\n        *   **End-to-end reasoning:** The review explores GNNs' potential to go beyond imitating classical algorithms, facilitating end-to-end algorithmic reasoning from raw input processing to solving abstracted CO problems \\cite{cappart2021xrp}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:** The paper *surveys* novel GNN-based algorithms and methods developed by others, categorizing their applications in CO. It does not propose new algorithms itself.\n    *   **System design or architectural innovations:** It reviews how GNNs are integrated into CO pipelines, either as direct solvers (e.g., predicting solutions) or as components enhancing existing exact solvers (e.g., guiding branch-and-bound).\n    *   **Theoretical insights or analysis:** The paper provides a conceptual analysis of GNNs' inductive bias, explaining how their permutation invariance and sparsity awareness effectively encode combinatorial and relational input. It also discusses the inherent trade-offs between scalability, expressivity, and generalization in GNN applications for CO \\cite{cappart2021xrp}.\n    *   **The paper's *own* contributions as a review are:**\n        1.  A complete, structured overview of GNN applications to CO, covering both heuristic and exact algorithms \\cite{cappart2021xrp}.\n        2.  A survey of recent progress in using GNN-based end-to-end algorithmic reasoners \\cite{cappart2021xrp}.\n        3.  Highlighting the shortcomings of GNNs in CO and offering guidelines and recommendations for addressing them \\cite{cappart2021xrp}.\n        4.  Providing a list of open research directions to stimulate future research in the field \\cite{cappart2021xrp}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted?** As a review paper, \\cite{cappart2021xrp} does not present new experimental validation. Instead, it references successful applications from the literature to illustrate the efficacy of GNNs in CO.\n    *   **Key performance metrics and comparison results:** The paper highlights examples like the work by Mirhoseini et al. (2021) on chip placement, where GNNs were used with reinforcement learning to generate optimized placements for Google's TPU accelerators. This approach demonstrated the ability to quickly generalize to unseen netlists and optimize power, performance, and area, showcasing the practical impact of GNNs in CO \\cite{cappart2021xrp}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The review identifies several limitations of current GNN approaches in CO:\n        *   **Data efficiency:** GNNs often require large amounts of labeled training data, which can be prohibitive in CO as it means solving many potentially hard instances \\cite{cappart2021xrp}.\n        *   **Scalability:** While GNNs scale linearly with the number of edges and parameters, scalability remains a challenge for extremely large real-world instances.\n        *   **Trade-offs:** There is an inherent trade-off between scalability, expressivity, and generalization that current GNN models must navigate \\cite{cappart2021xrp}.\n    *   **Scope of applicability:** The review focuses on the application of GNNs within the CO context, covering their use for finding primal solutions (heuristics), enhancing dual methods (exact solvers), and facilitating end-to-end algorithmic reasoning. It targets both optimization and machine learning researchers \\cite{cappart2021xrp}.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art?** By providing a comprehensive and structured overview, \\cite{cappart2021xrp} synthesizes the rapidly evolving field of GNNs for CO. It clarifies how GNNs' inherent properties (permutation invariance, sparsity awareness) make them uniquely suited for graph-structured combinatorial problems. The paper's systematic categorization of applications (heuristic, exact, algorithmic reasoning) helps to organize existing knowledge and identify key trends.\n    *   **Potential impact on future research:** The paper explicitly outlines shortcomings of current GNN approaches in CO and provides concrete guidelines and recommendations for tackling them. Crucially, it lists open research directions, aiming to stimulate and guide future research at the intersection of GNNs and CO, thereby facilitating further innovation in this emerging area \\cite{cappart2021xrp}.",
    "intriguing_abstract": "Combinatorial Optimization (CO) problems, ubiquitous in real-world applications from logistics to chip design, remain notoriously NP-hard, often requiring computationally intensive, instance-specific solvers. This paper \\cite{cappart2021xrp} presents a timely and comprehensive review, illuminating the transformative potential of **Graph Neural Networks (GNNs)** to revolutionize CO by leveraging underlying data distributions. We delve into how GNNs, with their inherent **permutation invariance** and **sparsity awareness**, are uniquely positioned to learn complex structural patterns and facilitate **end-to-end algorithmic reasoning**.\n\nThis review systematically categorizes GNN applications across **heuristic algorithms** and **exact solvers**, showcasing their ability to enhance solution quality and accelerate discovery. Beyond merely surveying, we critically analyze GNNs' strengths, such as improved **generalization** and **scalability**, while also addressing persistent challenges like **data efficiency**. By synthesizing recent advancements and outlining crucial open research directions, this work provides a vital roadmap for researchers. It aims to inspire novel approaches that push the boundaries of what's possible in solving intractable CO problems, paving the way for more efficient and intelligent decision-making systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Combinatorial Optimization (CO)",
      "Machine learning for CO",
      "Algorithmic reasoning",
      "Permutation invariance",
      "Scalability challenges",
      "Generalization capabilities",
      "Data efficiency",
      "Heuristic and exact algorithms",
      "Structured review",
      "Open research directions",
      "NP-hard problems",
      "End-to-end learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf",
    "citation_key": "cappart2021xrp",
    "metadata": {
      "title": "Combinatorial optimization and reasoning with graph neural networks",
      "authors": [
        "Quentin Cappart",
        "D. Chételat",
        "Elias Boutros Khalil",
        "Andrea Lodi",
        "Christopher Morris",
        "Petar Velickovic"
      ],
      "published_date": "2021",
      "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have mostly focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing the former. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at researchers in both optimization and machine learning.",
      "file_path": "paper_data/Graph_Neural_Networks/d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n**CITATION REQUIREMENTS**: Always use \"\\cite{cappart2021xrp}\" when referencing this paper.\n\n---\n\n### Technical Paper Analysis: Combinatorial Optimization and Reasoning with Graph Neural Networks \\cite{cappart2021xrp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the challenge of effectively applying machine learning (ML), particularly Graph Neural Networks (GNNs), to Combinatorial Optimization (CO) problems. Traditionally, CO methods solve problem instances in isolation, despite many practical instances stemming from related data distributions.\n    *   **Why is this problem important and challenging?** CO problems are often NP-hard and computationally intensive. Exploiting common patterns across instances using ML could lead to faster and more efficient algorithms for real-world applications (e.g., vehicle routing, scheduling). Key challenges for ML in CO include:\n        *   **Permutation invariance:** Graphs have no unique representation, requiring methods invariant to node reordering.\n        *   **Scalability and sparsity:** Real-world CO instances are often large and sparse, demanding scalable and sparsity-aware ML models.\n        *   **Expressivity:** Models must be expressive enough to detect and exploit critical structural patterns.\n        *   **Handling side information:** Incorporating auxiliary information like objective functions and user-defined constraints.\n        *   **Data efficiency:** Reducing the reliance on large amounts of labeled training data (which often means solving many hard CO instances).\n        *   **Generalization:** The ability to transfer learned knowledge to unseen instances, including those of different sizes or distributions.\n\n2.  **Related Work & Positioning**\n    *   **How does this work relate to existing approaches?** This paper is a conceptual review that synthesizes recent advancements in using GNNs for CO. It positions itself against:\n        *   **Traditional CO solvers:** Which focus on individual instances without leveraging data distributions.\n        *   **Early ML for CO:** Such as Hopfield networks and self-organizing maps, which were often applied to single instances rather than being trained over datasets.\n        *   **Other contemporary surveys:** It differentiates itself by providing a comprehensive, structured overview specifically on GNNs in CO, covering both heuristic and exact algorithms, and end-to-end algorithmic reasoning. Other surveys often have a broader ML focus, concentrate on specific ML paradigms (e.g., reinforcement learning), or miss recent GNN developments \\cite{cappart2021xrp}.\n    *   **Limitations of previous solutions:**\n        *   Classical CO algorithms heavily rely on human-made pre-processing and feature engineering, which can be costly, error-prone, and introduce biases that may not align with real-world data \\cite{cappart2021xrp}.\n        *   Earlier neural network applications for CO had limited success due to their instance-specific nature, lacking generalization across problem distributions \\cite{cappart2021xrp}.\n        *   Existing surveys, while valuable, often lack the specific focus and depth on GNNs' unique advantages and challenges within the CO domain, particularly regarding algorithmic reasoning \\cite{cappart2021xrp}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper reviews the application of Graph Neural Networks (GNNs) as the core technical method for addressing CO problems. GNNs compute vectorial representations for each node by iteratively aggregating features from neighboring nodes. This process is trained end-to-end to adapt to specific data distributions.\n    *   **What makes this approach novel or different?** The paper itself is a *review*, and its innovation lies in providing a novel, structured, and comprehensive overview of how GNNs are being used in CO. It highlights GNNs' inherent properties that make them suitable:\n        *   **Permutation invariance and equivariance:** GNNs are designed to automatically exploit symmetries in graph-structured data.\n        *   **Sparsity awareness:** Their local aggregation mechanism naturally handles sparse inputs, leading to more scalable models.\n        *   **Feature integration:** GNNs can incorporate multi-dimensional node and edge features, naturally exploiting cost and objective function information \\cite{cappart2021xrp}.\n        *   **End-to-end reasoning:** The review explores GNNs' potential to go beyond imitating classical algorithms, facilitating end-to-end algorithmic reasoning from raw input processing to solving abstracted CO problems \\cite{cappart2021xrp}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:** The paper *surveys* novel GNN-based algorithms and methods developed by others, categorizing their applications in CO. It does not propose new algorithms itself.\n    *   **System design or architectural innovations:** It reviews how GNNs are integrated into CO pipelines, either as direct solvers (e.g., predicting solutions) or as components enhancing existing exact solvers (e.g., guiding branch-and-bound).\n    *   **Theoretical insights or analysis:** The paper provides a conceptual analysis of GNNs' inductive bias, explaining how their permutation invariance and sparsity awareness effectively encode combinatorial and relational input. It also discusses the inherent trade-offs between scalability, expressivity, and generalization in GNN applications for CO \\cite{cappart2021xrp}.\n    *   **The paper's *own* contributions as a review are:**\n        1.  A complete, structured overview of GNN applications to CO, covering both heuristic and exact algorithms \\cite{cappart2021xrp}.\n        2.  A survey of recent progress in using GNN-based end-to-end algorithmic reasoners \\cite{cappart2021xrp}.\n        3.  Highlighting the shortcomings of GNNs in CO and offering guidelines and recommendations for addressing them \\cite{cappart2021xrp}.\n        4.  Providing a list of open research directions to stimulate future research in the field \\cite{cappart2021xrp}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted?** As a review paper, \\cite{cappart2021xrp} does not present new experimental validation. Instead, it references successful applications from the literature to illustrate the efficacy of GNNs in CO.\n    *   **Key performance metrics and comparison results:** The paper highlights examples like the work by Mirhoseini et al. (2021) on chip placement, where GNNs were used with reinforcement learning to generate optimized placements for Google's TPU accelerators. This approach demonstrated the ability to quickly generalize to unseen netlists and optimize power, performance, and area, showcasing the practical impact of GNNs in CO \\cite{cappart2021xrp}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The review identifies several limitations of current GNN approaches in CO:\n        *   **Data efficiency:** GNNs often require large amounts of labeled training data, which can be prohibitive in CO as it means solving many potentially hard instances \\cite{cappart2021xrp}.\n        *   **Scalability:** While GNNs scale linearly with the number of edges and parameters, scalability remains a challenge for extremely large real-world instances.\n        *   **Trade-offs:** There is an inherent trade-off between scalability, expressivity, and generalization that current GNN models must navigate \\cite{cappart2021xrp}.\n    *   **Scope of applicability:** The review focuses on the application of GNNs within the CO context, covering their use for finding primal solutions (heuristics), enhancing dual methods (exact solvers), and facilitating end-to-end algorithmic reasoning. It targets both optimization and machine learning researchers \\cite{cappart2021xrp}.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art?** By providing a comprehensive and structured overview, \\cite{cappart2021xrp} synthesizes the rapidly evolving field of GNNs for CO. It clarifies how GNNs' inherent properties (permutation invariance, sparsity awareness) make them uniquely suited for graph-structured combinatorial problems. The paper's systematic categorization of applications (heuristic, exact, algorithmic reasoning) helps to organize existing knowledge and identify key trends.\n    *   **Potential impact on future research:** The paper explicitly outlines shortcomings of current GNN approaches in CO and provides concrete guidelines and recommendations for tackling them. Crucially, it lists open research directions, aiming to stimulate and guide future research at the intersection of GNNs and CO, thereby facilitating further innovation in this emerging area \\cite{cappart2021xrp}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Combinatorial Optimization (CO)",
        "Machine learning for CO",
        "Algorithmic reasoning",
        "Permutation invariance",
        "Scalability challenges",
        "Generalization capabilities",
        "Data efficiency",
        "Heuristic and exact algorithms",
        "Structured review",
        "Open research directions",
        "NP-hard problems",
        "End-to-end learning"
      ],
      "paper_type": "**survey**\n\n**reasoning:**\n\nthe abstract explicitly states: \"this paper presents a conceptual **review** of recent key advancements in this emerging field...\" this directly aligns with the classification criteria for a **survey** paper, which \"reviews existing literature comprehensively\" and often uses terms like \"review\" or \"state-of-the-art.\" the introduction further sets the stage for the field of combinatorial optimization and the recent integration of machine learning, particularly gnns, which is typical for a review paper that aims to provide an overview of a developing area."
    },
    "file_name": "d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf"
  },
  {
    "success": true,
    "doc_id": "752f33240689ea4a6d3e3ffe3fdba113",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) can make discriminatory decisions towards certain demographic groups in high-stakes applications (e.g., online fraud detection). Existing fair GNN approaches are model-specific, requiring costly fine-tuning for each GNN variant \\cite{dong2021qcg}.\n    *   **Importance and Challenge:** Mitigating bias in GNNs is crucial. The challenge lies in developing a *model-agnostic* debiasing approach that can universally benefit various GNNs and tasks, rather than being tied to a specific GNN architecture or its outcome \\cite{dong2021qcg}. This requires addressing:\n        *   **Data Bias Modeling:** How to appropriately model bias directly in the input attributed network, independent of GNN outcomes.\n        *   **Multi-Modality Debiasing:** How to debias both node attributes and network structure, as bias can exist in diverse formats across these modalities.\n        *   **Model-Agnostic Debiasing:** How to formulate a debiasing objective that is universally applicable to different GNNs while still ensuring fair GNN outcomes \\cite{dong2021qcg}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to existing approaches:** The work acknowledges existing efforts in mitigating bias in graph mining algorithms (e.g., random walk modifications, adversarial learning for node embeddings) and recent explorations on fair GNNs that debias GNN *outcomes* \\cite{dong2021qcg}.\n    *   **Limitations of previous solutions:** Existing GNN debiasing approaches are \"tailored for a specific GNN model\" and \"costly to fine-tune\" for the myriad of GNN variants \\cite{dong2021qcg}. Unlike these model-specific methods, this paper proposes to debias the *input attributed network* itself, a step not taken by prior literature \\cite{dong2021qcg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes EDITS (modEling an DmItigating daTa biaS), a novel framework to mitigate bias in attributed networks *before* they are fed into GNNs \\cite{dong2021qcg}. This is achieved by defining and measuring bias directly in the input data and then optimizing to reduce it.\n    *   **Novelty/Difference:**\n        *   **Data-centric Debiasing:** EDITS shifts the focus from debiasing GNN *models* or *outcomes* to debiasing the *input attributed network* in a model-agnostic manner \\cite{dong2021qcg}.\n        *   **Novel Bias Definitions & Metrics:** It introduces formal definitions for \"attribute bias\" and \"structural bias\" and corresponding quantitative metrics to measure them in attributed networks \\cite{dong2021qcg}.\n        *   **Multi-Modality Consideration:** The approach explicitly considers and addresses bias in both node attributes and network structure, based on preliminary analysis showing how each modality contributes to bias during GNN information propagation \\cite{dong2021qcg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Formal definitions of \"attribute bias\" and \"structural bias\" for attributed networks \\cite{dong2021qcg}.\n        *   Quantitative bias metrics based on Wasserstein-1 distance:\n            *   `b_attr`: Measures the average Wasserstein-1 distance between attribute value distributions of different demographic groups across all dimensions \\cite{dong2021qcg}.\n            *   `b_stru`: Measures the average Wasserstein-1 distance between aggregated reachable attribute value distributions of different groups after information propagation, using a defined propagation matrix `M_H` \\cite{dong2021qcg}.\n        *   The EDITS framework, which formulates an optimization objective to mitigate both `b_attr` and `b_stru` while maintaining downstream task performance \\cite{dong2021qcg}.\n    *   **System Design or Architectural Innovations:** EDITS is designed as a pre-processing framework, making it model-agnostic and universally applicable to any GNN, thereby offering a \"one-size-fits-all\" solution for data debiasing \\cite{dong2021qcg}.\n    *   **Theoretical Insights or Analysis:** Preliminary analysis (Section 2) demonstrates how bias in node attributes and network structure independently affects information propagation in GNNs, highlighting the necessity of multi-modality debiasing \\cite{dong2021qcg}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experiments were conducted on both synthetic and real-world datasets \\cite{dong2021qcg}. Synthetic datasets were used to illustrate how biased attributes and structures introduce bias during GNN information propagation \\cite{dong2021qcg}.\n    *   **Key Performance Metrics and Comparison Results:** Experiments \"demonstrate the validity of the proposed bias metrics\" and \"the superiority of EDITS on both bias mitigation and utility maintenance\" \\cite{dong2021qcg}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** For simplicity, the initial formulation of bias metrics focuses on binary sensitive attributes, with generalization to non-binary cases left for the Appendix \\cite{dong2021qcg}.\n    *   **Scope of Applicability:** EDITS is designed for undirected attributed networks and is model-agnostic, meaning it can be applied as a pre-processing step for any GNN \\cite{dong2021qcg}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** This paper makes an initial investigation into the novel problem of debiasing attributed networks for GNNs, shifting the paradigm from model-specific outcome debiasing to model-agnostic input data debiasing \\cite{dong2021qcg}. It provides a principled way to model and quantify data bias in graph data.\n    *   **Potential Impact on Future Research:** EDITS offers a more efficient, generalizable, and cost-effective solution for achieving fairness in GNNs across diverse applications \\cite{dong2021qcg}. It opens new research directions in data-centric fairness for graph machine learning, potentially inspiring further work on pre-processing techniques for fair GNNs.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their susceptibility to discriminatory decisions in high-stakes applications poses a critical challenge. Existing fair GNN approaches are predominantly model-specific, demanding costly fine-tuning for each architectural variant. We introduce EDITS (modEling an DmItigating daTa biaS), a novel, model-agnostic framework that fundamentally shifts the paradigm: instead of debiasing GNN *outcomes*, EDITS directly mitigates bias within the *input attributed network* itself.\n\nEDITS pioneers a data-centric approach by formally defining and quantifying \"attribute bias\" and \"structural bias\" using Wasserstein-1 distance, addressing inherent inequities in both node features and network topology. Our framework optimizes these novel metrics while preserving downstream task utility, offering a \"one-size-fits-all\" pre-processing solution universally applicable to any GNN. Comprehensive experiments on synthetic and real-world datasets validate our bias metrics and demonstrate EDITS' superior performance in simultaneously achieving significant bias mitigation and utility maintenance. This work advances the state-of-the-art in data-centric fairness for graph machine learning, providing an efficient, generalizable, and cost-effective pathway to robustly fair GNN deployments and opening new research frontiers.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Bias mitigation",
      "Model-agnostic debiasing",
      "Attributed networks",
      "EDITS framework",
      "Data-centric debiasing",
      "Attribute bias",
      "Structural bias",
      "Wasserstein-1 distance",
      "Multi-modality debiasing",
      "Pre-processing framework",
      "Fairness in GNNs",
      "Information propagation",
      "Utility maintenance"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/cd551790992d16148fe2e5ff2cc76861195e2191.pdf",
    "citation_key": "dong2021qcg",
    "metadata": {
      "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
      "authors": [
        "Yushun Dong",
        "Ninghao Liu",
        "B. Jalaeian",
        "Jundong Li"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) have shown superior performance in analyzing attributed networks in various web-based applications such as social recommendation and web search. Nevertheless, in high-stake decision-making scenarios such as online fraud detection, there is an increasing societal concern that GNNs could make discriminatory decisions towards certain demographic groups. Despite recent explorations on fair GNNs, these works are tailored for a specific GNN model. However, myriads of GNN variants have been proposed for different applications, and it is costly to fine-tune existing debiasing algorithms for each specific GNN architecture. Different from existing works that debias GNN models, we aim to debias the input attributed network to achieve fairer GNNs through feeding GNNs with less biased data. Specifically, we propose novel definitions and metrics to measure the bias in an attributed network, which leads to the optimization objective to mitigate bias. We then develop a framework EDITS to mitigate the bias in attributed networks while maintaining the performance of GNNs in downstream tasks. EDITS works in a model-agnostic manner, i.e., it is independent of any specific GNN. Experiments demonstrate the validity of the proposed bias metrics and the superiority of EDITS on both bias mitigation and utility maintenance. Open-source implementation: https://github.com/yushundong/EDITS.",
      "file_path": "paper_data/Graph_Neural_Networks/cd551790992d16148fe2e5ff2cc76861195e2191.pdf",
      "venue": "The Web Conference",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) can make discriminatory decisions towards certain demographic groups in high-stakes applications (e.g., online fraud detection). Existing fair GNN approaches are model-specific, requiring costly fine-tuning for each GNN variant \\cite{dong2021qcg}.\n    *   **Importance and Challenge:** Mitigating bias in GNNs is crucial. The challenge lies in developing a *model-agnostic* debiasing approach that can universally benefit various GNNs and tasks, rather than being tied to a specific GNN architecture or its outcome \\cite{dong2021qcg}. This requires addressing:\n        *   **Data Bias Modeling:** How to appropriately model bias directly in the input attributed network, independent of GNN outcomes.\n        *   **Multi-Modality Debiasing:** How to debias both node attributes and network structure, as bias can exist in diverse formats across these modalities.\n        *   **Model-Agnostic Debiasing:** How to formulate a debiasing objective that is universally applicable to different GNNs while still ensuring fair GNN outcomes \\cite{dong2021qcg}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to existing approaches:** The work acknowledges existing efforts in mitigating bias in graph mining algorithms (e.g., random walk modifications, adversarial learning for node embeddings) and recent explorations on fair GNNs that debias GNN *outcomes* \\cite{dong2021qcg}.\n    *   **Limitations of previous solutions:** Existing GNN debiasing approaches are \"tailored for a specific GNN model\" and \"costly to fine-tune\" for the myriad of GNN variants \\cite{dong2021qcg}. Unlike these model-specific methods, this paper proposes to debias the *input attributed network* itself, a step not taken by prior literature \\cite{dong2021qcg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes EDITS (modEling an DmItigating daTa biaS), a novel framework to mitigate bias in attributed networks *before* they are fed into GNNs \\cite{dong2021qcg}. This is achieved by defining and measuring bias directly in the input data and then optimizing to reduce it.\n    *   **Novelty/Difference:**\n        *   **Data-centric Debiasing:** EDITS shifts the focus from debiasing GNN *models* or *outcomes* to debiasing the *input attributed network* in a model-agnostic manner \\cite{dong2021qcg}.\n        *   **Novel Bias Definitions & Metrics:** It introduces formal definitions for \"attribute bias\" and \"structural bias\" and corresponding quantitative metrics to measure them in attributed networks \\cite{dong2021qcg}.\n        *   **Multi-Modality Consideration:** The approach explicitly considers and addresses bias in both node attributes and network structure, based on preliminary analysis showing how each modality contributes to bias during GNN information propagation \\cite{dong2021qcg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Formal definitions of \"attribute bias\" and \"structural bias\" for attributed networks \\cite{dong2021qcg}.\n        *   Quantitative bias metrics based on Wasserstein-1 distance:\n            *   `b_attr`: Measures the average Wasserstein-1 distance between attribute value distributions of different demographic groups across all dimensions \\cite{dong2021qcg}.\n            *   `b_stru`: Measures the average Wasserstein-1 distance between aggregated reachable attribute value distributions of different groups after information propagation, using a defined propagation matrix `M_H` \\cite{dong2021qcg}.\n        *   The EDITS framework, which formulates an optimization objective to mitigate both `b_attr` and `b_stru` while maintaining downstream task performance \\cite{dong2021qcg}.\n    *   **System Design or Architectural Innovations:** EDITS is designed as a pre-processing framework, making it model-agnostic and universally applicable to any GNN, thereby offering a \"one-size-fits-all\" solution for data debiasing \\cite{dong2021qcg}.\n    *   **Theoretical Insights or Analysis:** Preliminary analysis (Section 2) demonstrates how bias in node attributes and network structure independently affects information propagation in GNNs, highlighting the necessity of multi-modality debiasing \\cite{dong2021qcg}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experiments were conducted on both synthetic and real-world datasets \\cite{dong2021qcg}. Synthetic datasets were used to illustrate how biased attributes and structures introduce bias during GNN information propagation \\cite{dong2021qcg}.\n    *   **Key Performance Metrics and Comparison Results:** Experiments \"demonstrate the validity of the proposed bias metrics\" and \"the superiority of EDITS on both bias mitigation and utility maintenance\" \\cite{dong2021qcg}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** For simplicity, the initial formulation of bias metrics focuses on binary sensitive attributes, with generalization to non-binary cases left for the Appendix \\cite{dong2021qcg}.\n    *   **Scope of Applicability:** EDITS is designed for undirected attributed networks and is model-agnostic, meaning it can be applied as a pre-processing step for any GNN \\cite{dong2021qcg}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** This paper makes an initial investigation into the novel problem of debiasing attributed networks for GNNs, shifting the paradigm from model-specific outcome debiasing to model-agnostic input data debiasing \\cite{dong2021qcg}. It provides a principled way to model and quantify data bias in graph data.\n    *   **Potential Impact on Future Research:** EDITS offers a more efficient, generalizable, and cost-effective solution for achieving fairness in GNNs across diverse applications \\cite{dong2021qcg}. It opens new research directions in data-centric fairness for graph machine learning, potentially inspiring further work on pre-processing techniques for fair GNNs.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Bias mitigation",
        "Model-agnostic debiasing",
        "Attributed networks",
        "EDITS framework",
        "Data-centric debiasing",
        "Attribute bias",
        "Structural bias",
        "Wasserstein-1 distance",
        "Multi-modality debiasing",
        "Pre-processing framework",
        "Fairness in GNNs",
        "Information propagation",
        "Utility maintenance"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose** novel definitions and metrics to measure the bias in an attributed network,\" and \"we then **develop a framework edits** to mitigate the bias in attributed networks.\"\n*   it discusses a \"proposed solution\" (edits) to a \"technical problem\" (data bias in gnns).\n*   it mentions \"experiments demonstrate the validity of the proposed bias metrics and the superiority of edits,\" indicating that empirical validation is part of the paper, but the core contribution is the *development* of the new method/framework.\n\nthese points strongly align with the criteria for a **technical** paper.\n\n**classification: technical**"
    },
    "file_name": "cd551790992d16148fe2e5ff2cc76861195e2191.pdf"
  },
  {
    "success": true,
    "doc_id": "cbd511c51092f886ab92c27a3a6f182b",
    "summary": "Here's a focused summary of the paper \"Rethinking Fair Graph Neural Networks from Re-balancing\" \\cite{li20245zy} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing unfairness in Graph Neural Networks (GNNs) that arises from distribution disparities between different demographic groups in real-world datasets. This leads to GNNs favoring privileged groups and making biased decisions in high-stake applications.\n    *   **Importance and Challenge**: Fairness is crucial in automated decision-making systems (e.g., credit risk, fraud detection). Existing fair GNN methods often require significant architectural changes or additional complex loss functions with extensive hyper-parameter tuning. Simple re-balancing methods, while promising, have inherent shortcomings when applied directly to graph-structured data (e.g., information loss, over-fitting, difficulty connecting new nodes).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous fair GNNs primarily employ regularization terms during optimization or modify training data to eliminate sensitive attribute information \\cite{li20245zy}.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods often overlook the fundamental issue of attribute imbalance, where underprivileged groups are underrepresented \\cite{li20245zy}.\n        *   They typically involve significant architectural modifications or complex loss functions requiring more hyper-parameter tuning \\cite{li20245zy}.\n        *   Simple re-balancing methods (re-sampling, re-weighting) are competitive but suffer from:\n            *   Down-sampling: Loss of beneficial information, jeopardizing model utility \\cite{li20245zy}.\n            *   Over-sampling: Over-fitting and difficulty in connecting adjacent edges of newly generated nodes due to the non-i.i.d. nature of graphs \\cite{li20245zy}.\n            *   Re-weighting: Can lead to over-fitting and increased false positives for major nodes \\cite{li20245zy}.\n        *   Prior work on class imbalance in node classification (e.g., GraphENS, TAM) did not consider sub-groups within classes or address unfairness in GNNs from a re-balancing perspective \\cite{li20245zy}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes FairGB (FairGraph Neural Network via re-Balancing), a novel approach that mitigates GNN unfairness through group balancing. FairGB consists of two mutually reinforcing modules: Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL) \\cite{li20245zy}.\n    *   **Novelty**:\n        *   **Re-balancing for GNN Fairness**: It rethinks GNN fairness from a re-balancing perspective, demonstrating that simple re-balancing can be highly effective, and then building a robust method upon this insight \\cite{li20245zy}.\n        *   **Counterfactual Node Mixup (CNM)**: Generates new, unbiased ego-networks by interpolating node attributes and neighbor distributions of \"counterfactual pairs.\" This includes:\n            *   *Inter-domain mixup*: Samples with the same target label but different sensitive attributes, promoting domain-invariant features \\cite{li20245zy}.\n            *   *Inter-class mixup*: Samples with different target labels but the same sensitive attribute, smoothing decision surfaces and reducing bias dependency \\cite{li20245zy}.\n            *   *Neighborhood mixup*: Interpolates neighbor distributions to integrate new nodes into the graph structure while preserving original degree distribution characteristics \\cite{li20245zy}.\n        *   **Contribution Alignment Loss (CAL)**: A gradient-based re-weighting method that balances the contribution of each demographic group during training, addressing the limitations of quantity-based balancing \\cite{li20245zy}. It calculates contributions of mixed nodes to each original group based on gradient norms.\n        *   **Hybrid Model**: FairGB effectively combines re-sampling (CNM) and re-weighting (CAL) strategies, allowing them to mutually alleviate the shortcomings of simple re-balancing methods \\cite{li20245zy}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   FairGB framework, integrating Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL) \\cite{li20245zy}.\n        *   Counterfactual Node Mixup: A novel data augmentation technique for graphs that generates unbiased ego-networks by interpolating node features and neighbor distributions across inter-domain and inter-class counterfactual pairs \\cite{li20245zy}.\n        *   Contribution Alignment Loss: A gradient-aware re-weighting mechanism that dynamically adjusts group contributions during training to enhance fairness beyond simple quantity balancing \\cite{li20245zy}.\n    *   **Theoretical Insights/Analysis**:\n        *   Provides a causal view of GNN prediction, analyzing paths between sensitive attributes and target labels (S→X→Z→Y, S→A→Z→Y, S↔C→Y) to motivate the need for debiasing both attributes and topology, and severing spurious correlations \\cite{li20245zy}.\n        *   Theoretically proves that balanced and consistent bias distribution within each class (achieved by CNM) makes sensitive attributes statistically independent from target labels (P(Y=y|S=s)=P(Y=y)) \\cite{li20245zy}.\n    *   **System Design/Architectural Innovations**: FairGB integrates these modules with minimal architectural changes to existing GNNs, requiring only one additional hyper-parameter \\cite{li20245zy}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on benchmark datasets commonly used for fairness evaluation in GNNs, including German, Bail, and Credit datasets \\cite{li20245zy}.\n    *   **Key Performance Metrics**: Both utility (e.g., F1-score) and fairness metrics (e.g., Demographic Parity difference, Δsp) were used for evaluation \\cite{li20245zy}.\n    *   **Comparison Results**:\n        *   FairGB achieves state-of-the-art results concerning both utility and fairness metrics, outperforming vanilla GNNs and existing fair GNN models \\cite{li20245zy}.\n        *   Preliminary analysis showed that simple re-balancing methods could already match or surpass existing fair GNNs, providing a strong baseline for FairGB's development \\cite{li20245zy}.\n        *   The F1-Δsp trade-off plots (e.g., Figure 1) visually demonstrate FairGB's superior performance in achieving a better balance between utility and fairness \\cite{li20245zy}.\n        *   Observations indicate that the decision boundaries of target labels and sensitive attributes become roughly orthogonal, suggesting their independence, which aligns with the theoretical debiasing goal \\cite{li20245zy}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on binary classification and binary sensitive attributes. While FairGB addresses the shortcomings of *simple* re-balancing methods, the specific limitations of FairGB itself are not explicitly detailed beyond its effectiveness. The method assumes the availability of sensitive attribute labels for training.\n    *   **Scope of Applicability**: FairGB is designed for fair node classification tasks on attributed graphs where sensitive attributes are known. Its applicability to other graph learning tasks (e.g., link prediction, graph classification) or scenarios with unknown sensitive attributes is not discussed.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li20245zy} significantly advances the technical state-of-the-art in fair GNNs by providing a new, effective perspective rooted in re-balancing. It demonstrates that addressing group imbalance directly, rather than solely focusing on sensitive attribute removal, can lead to superior fairness and utility.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into re-balancing techniques for complex graph data, potentially inspiring simpler yet more effective fairness interventions in GNNs. The theoretical analysis of counterfactual mixup from both causal and statistical views provides a strong foundation for future debiasing methods in graph learning.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are revolutionizing data analysis, yet their deployment in high-stakes applications is severely hampered by inherent biases stemming from demographic disparities in real-world graphs. Current fair GNN methods often involve complex architectural modifications or struggle with the unique challenges of graph-structured data when attempting simple re-balancing. We rethink GNN fairness from a fundamental re-balancing perspective, introducing FairGB, a novel framework that synergistically combines Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL). CNM innovatively generates unbiased ego-networks by interpolating node features and *neighbor distributions* across counterfactual pairs, effectively addressing the non-i.i.d. nature of graphs. Complementing this, CAL employs a *gradient-based re-weighting* mechanism to dynamically align group contributions, overcoming the limitations of traditional quantity-based approaches. This hybrid strategy, supported by a causal theoretical analysis, achieves state-of-the-art fairness and utility with minimal architectural changes. Extensive experiments demonstrate FairGB's superior performance, significantly improving *demographic parity* while maintaining high *F1-scores* on benchmark datasets. FairGB offers a powerful, elegant solution to GNN unfairness, paving the way for more robust and equitable AI systems in critical domains.",
    "keywords": [
      "Fair Graph Neural Networks",
      "GNN unfairness",
      "distribution disparities",
      "re-balancing",
      "FairGB",
      "Counterfactual Node Mixup (CNM)",
      "Contribution Alignment Loss (CAL)",
      "hybrid re-sampling and re-weighting",
      "causal view of GNN prediction",
      "node classification",
      "utility-fairness trade-off",
      "Demographic Parity",
      "state-of-the-art fairness",
      "ego-networks"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf",
    "citation_key": "li20245zy",
    "metadata": {
      "title": "Rethinking Fair Graph Neural Networks from Re-balancing",
      "authors": [
        "Zhixun Li",
        "Yushun Dong",
        "Qiang Liu",
        "Jeffrey Xu Yu"
      ],
      "published_date": "2024",
      "abstract": "Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.",
      "file_path": "paper_data/Graph_Neural_Networks/458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Rethinking Fair Graph Neural Networks from Re-balancing\" \\cite{li20245zy} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing unfairness in Graph Neural Networks (GNNs) that arises from distribution disparities between different demographic groups in real-world datasets. This leads to GNNs favoring privileged groups and making biased decisions in high-stake applications.\n    *   **Importance and Challenge**: Fairness is crucial in automated decision-making systems (e.g., credit risk, fraud detection). Existing fair GNN methods often require significant architectural changes or additional complex loss functions with extensive hyper-parameter tuning. Simple re-balancing methods, while promising, have inherent shortcomings when applied directly to graph-structured data (e.g., information loss, over-fitting, difficulty connecting new nodes).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous fair GNNs primarily employ regularization terms during optimization or modify training data to eliminate sensitive attribute information \\cite{li20245zy}.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods often overlook the fundamental issue of attribute imbalance, where underprivileged groups are underrepresented \\cite{li20245zy}.\n        *   They typically involve significant architectural modifications or complex loss functions requiring more hyper-parameter tuning \\cite{li20245zy}.\n        *   Simple re-balancing methods (re-sampling, re-weighting) are competitive but suffer from:\n            *   Down-sampling: Loss of beneficial information, jeopardizing model utility \\cite{li20245zy}.\n            *   Over-sampling: Over-fitting and difficulty in connecting adjacent edges of newly generated nodes due to the non-i.i.d. nature of graphs \\cite{li20245zy}.\n            *   Re-weighting: Can lead to over-fitting and increased false positives for major nodes \\cite{li20245zy}.\n        *   Prior work on class imbalance in node classification (e.g., GraphENS, TAM) did not consider sub-groups within classes or address unfairness in GNNs from a re-balancing perspective \\cite{li20245zy}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes FairGB (FairGraph Neural Network via re-Balancing), a novel approach that mitigates GNN unfairness through group balancing. FairGB consists of two mutually reinforcing modules: Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL) \\cite{li20245zy}.\n    *   **Novelty**:\n        *   **Re-balancing for GNN Fairness**: It rethinks GNN fairness from a re-balancing perspective, demonstrating that simple re-balancing can be highly effective, and then building a robust method upon this insight \\cite{li20245zy}.\n        *   **Counterfactual Node Mixup (CNM)**: Generates new, unbiased ego-networks by interpolating node attributes and neighbor distributions of \"counterfactual pairs.\" This includes:\n            *   *Inter-domain mixup*: Samples with the same target label but different sensitive attributes, promoting domain-invariant features \\cite{li20245zy}.\n            *   *Inter-class mixup*: Samples with different target labels but the same sensitive attribute, smoothing decision surfaces and reducing bias dependency \\cite{li20245zy}.\n            *   *Neighborhood mixup*: Interpolates neighbor distributions to integrate new nodes into the graph structure while preserving original degree distribution characteristics \\cite{li20245zy}.\n        *   **Contribution Alignment Loss (CAL)**: A gradient-based re-weighting method that balances the contribution of each demographic group during training, addressing the limitations of quantity-based balancing \\cite{li20245zy}. It calculates contributions of mixed nodes to each original group based on gradient norms.\n        *   **Hybrid Model**: FairGB effectively combines re-sampling (CNM) and re-weighting (CAL) strategies, allowing them to mutually alleviate the shortcomings of simple re-balancing methods \\cite{li20245zy}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   FairGB framework, integrating Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL) \\cite{li20245zy}.\n        *   Counterfactual Node Mixup: A novel data augmentation technique for graphs that generates unbiased ego-networks by interpolating node features and neighbor distributions across inter-domain and inter-class counterfactual pairs \\cite{li20245zy}.\n        *   Contribution Alignment Loss: A gradient-aware re-weighting mechanism that dynamically adjusts group contributions during training to enhance fairness beyond simple quantity balancing \\cite{li20245zy}.\n    *   **Theoretical Insights/Analysis**:\n        *   Provides a causal view of GNN prediction, analyzing paths between sensitive attributes and target labels (S→X→Z→Y, S→A→Z→Y, S↔C→Y) to motivate the need for debiasing both attributes and topology, and severing spurious correlations \\cite{li20245zy}.\n        *   Theoretically proves that balanced and consistent bias distribution within each class (achieved by CNM) makes sensitive attributes statistically independent from target labels (P(Y=y|S=s)=P(Y=y)) \\cite{li20245zy}.\n    *   **System Design/Architectural Innovations**: FairGB integrates these modules with minimal architectural changes to existing GNNs, requiring only one additional hyper-parameter \\cite{li20245zy}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on benchmark datasets commonly used for fairness evaluation in GNNs, including German, Bail, and Credit datasets \\cite{li20245zy}.\n    *   **Key Performance Metrics**: Both utility (e.g., F1-score) and fairness metrics (e.g., Demographic Parity difference, Δsp) were used for evaluation \\cite{li20245zy}.\n    *   **Comparison Results**:\n        *   FairGB achieves state-of-the-art results concerning both utility and fairness metrics, outperforming vanilla GNNs and existing fair GNN models \\cite{li20245zy}.\n        *   Preliminary analysis showed that simple re-balancing methods could already match or surpass existing fair GNNs, providing a strong baseline for FairGB's development \\cite{li20245zy}.\n        *   The F1-Δsp trade-off plots (e.g., Figure 1) visually demonstrate FairGB's superior performance in achieving a better balance between utility and fairness \\cite{li20245zy}.\n        *   Observations indicate that the decision boundaries of target labels and sensitive attributes become roughly orthogonal, suggesting their independence, which aligns with the theoretical debiasing goal \\cite{li20245zy}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on binary classification and binary sensitive attributes. While FairGB addresses the shortcomings of *simple* re-balancing methods, the specific limitations of FairGB itself are not explicitly detailed beyond its effectiveness. The method assumes the availability of sensitive attribute labels for training.\n    *   **Scope of Applicability**: FairGB is designed for fair node classification tasks on attributed graphs where sensitive attributes are known. Its applicability to other graph learning tasks (e.g., link prediction, graph classification) or scenarios with unknown sensitive attributes is not discussed.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li20245zy} significantly advances the technical state-of-the-art in fair GNNs by providing a new, effective perspective rooted in re-balancing. It demonstrates that addressing group imbalance directly, rather than solely focusing on sensitive attribute removal, can lead to superior fairness and utility.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into re-balancing techniques for complex graph data, potentially inspiring simpler yet more effective fairness interventions in GNNs. The theoretical analysis of counterfactual mixup from both causal and statistical views provides a strong foundation for future debiasing methods in graph learning.",
      "keywords": [
        "Fair Graph Neural Networks",
        "GNN unfairness",
        "distribution disparities",
        "re-balancing",
        "FairGB",
        "Counterfactual Node Mixup (CNM)",
        "Contribution Alignment Loss (CAL)",
        "hybrid re-sampling and re-weighting",
        "causal view of GNN prediction",
        "node classification",
        "utility-fairness trade-off",
        "Demographic Parity",
        "state-of-the-art fairness",
        "ego-networks"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this paper, we **propose fairgb**, fairgraph neural network via re- balancing, which mitigates the unfairness of gnns by group balancing. **technically, fairgb consists of two modules**...\"\n*   it describes the components of this new method: \"counterfactual node mixup\" and \"contribution alignment loss.\"\n*   it mentions theoretical analysis to support the method: \"prove that our strategy can make sensitive attributes statistically independent.\"\n*   it mentions empirical validation: \"experimental results on benchmark datasets show that our method can achieve state-of-the-art results.\"\n*   the introduction sets up a problem (fairness in gnns) and critiques existing solutions, leading to the need for their proposed approach.\n\nthe core of the paper is the **proposal and development of a new method/system (fairgb)**. while it includes theoretical analysis and empirical evaluation, these are in support of presenting the novel technical solution.\n\ntherefore, the paper type is **technical**."
    },
    "file_name": "458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf"
  },
  {
    "success": true,
    "doc_id": "2d9c7d7cd28b102832200cf34318ca8f",
    "summary": "Here's a focused summary of the paper \\cite{zhao2020bmj} for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"Data Augmentation for Graph Neural Networks\" \\cite{zhao2020bmj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of effective data augmentation techniques for Graph Neural Networks (GNNs), particularly for improving semi-supervised node classification.\n    *   **Importance and Challenge**: Data augmentation is crucial for improving model generalizability in machine learning (e.g., CV, NLP), but its application to graphs is challenging due to their complex, non-Euclidean structure. Standard augmentation operations (like rotation, cropping) have no direct analogs for graphs, and defining new, meaningful graph manipulations (beyond simple node/edge addition/removal) is difficult without introducing noise or reducing data.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing graph augmentation methods are limited:\n        *   **DROPEDGE** \\cite{rong2019dropedge} randomly removes edges, akin to dropout, but cannot add beneficial edges.\n        *   **ADAEDGE** \\cite{chen2019adaedge} iteratively adds/removes edges based on high-confidence label predictions, but is ad-hoc, prone to error propagation, and highly dependent on training size.\n        *   **BGCN** \\cite{zhang2019bgcn} iteratively trains a stochastic block model with GCN predictions to produce denoised graphs, also risking error propagation.\n    *   **Limitations of Previous Solutions**: These prior methods either rely on random perturbations (which can hurt performance), or iterative, ad-hoc approaches that are susceptible to error propagation and may not generalize well. \\cite{zhao2020bmj} positions itself by proposing a more principled, learning-based approach to strategically manipulate edges.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces the **GAUG** framework, which leverages neural edge predictors (specifically, a Graph Auto-Encoder (GAE) \\cite{kipf2016variational}) to learn and encode class-homophilic structure within a graph. This allows for strategic addition of \"missing\" intra-class edges and removal of \"noisy\" inter-class edges.\n    *   **Novelty/Difference**:\n        *   **Learning-based Edge Manipulation**: Unlike random or confidence-based iterative methods, GAUG uses a pre-trained edge predictor to deterministically identify which edges to add or remove based on learned probabilities, effectively \"denoising\" and \"completing\" the graph structure.\n        *   **Two Augmentation Settings**: GAUG proposes two distinct strategies based on inference feasibility:\n            *   **GAUG-M (Modified-Graph Setting)**: The input graph is fundamentally modified (edges added/removed) and this modified graph (`Gm`) is used for *both* training and inference. This is suitable when the graph is static.\n            *   **GAUG-O (Original-Graph Setting)**: Multiple augmented graph variants (`G_m^i`) are generated for training, but inference is performed *only* on the original graph (`G`). This is for dynamic or latency-sensitive scenarios where modifying the inference graph is impractical.\n        *   **Theoretical Motivation**: The paper provides a theoretical basis (Theorem 1) demonstrating that GNNs can easily differentiate classes and generate equivalent representations for same-class nodes on an \"ideal\" class-homophilic graph (fully connected components per class). GAUG aims to approximate this ideal.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of the GAUG framework (GAUG-M and GAUG-O) for graph data augmentation via learned edge manipulation.\n    *   **Insight into Edge Predictors**: Demonstrating that neural edge predictors (like GAE) can effectively learn and encode class-homophilic tendencies, enabling the promotion of intra-class edges and demotion of inter-class edges.\n    *   **Strategic Edge Manipulation**: A method to deterministically add top-scoring non-edges and remove lowest-scoring existing edges based on learned probabilities from an edge predictor.\n    *   **Modular Design**: The framework is designed to be modular, allowing flexible application to various GNN architectures.\n    *   **Formalization of Augmentation Settings**: Clearly distinguishing between \"modified-graph\" and \"original-graph\" settings for graph augmentation, addressing practical inference constraints.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on multiple benchmark datasets (e.g., Cora) and across various GNN architectures (e.g., GCN). The evaluation focused on semi-supervised node classification tasks.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metric**: Test micro-F1 score.\n        *   **GAUG-M Performance**: Showed significant improvements, achieving up to **17% absolute F1 performance improvements** over GNNs without augmentation, and up to **16% over baseline augmentation strategies**.\n        *   **GAUG-O Performance**: Demonstrated consistent improvements, with up to **9% absolute F1 performance improvements** over GNNs without augmentation, and up to **9% over baseline augmentation strategies**.\n        *   **Qualitative Analysis (Figure 2)**: Illustrated that GAUG-M's learned edge additions/removals lead to a much steeper growth/slower decrease of intra-class edges and a slower increase/steeper decrease of inter-class edges compared to random perturbations, directly correlating with improved classification performance. Random modifications were shown to hurt performance.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While GAUG-M shows strong performance, the paper notes that \"learned removal improves performance over part of the range (until 20%)\" (Figure 2), suggesting that excessive removal might eventually degrade performance. The theoretical discussion also implicitly acknowledges the risk of overfitting if modifications are \"too contrived,\" though GAUG aims to mitigate this.\n    *   **Scope of Applicability**: The framework is primarily evaluated and designed for semi-supervised node classification tasks. Its applicability to other graph tasks (e.g., link prediction, graph classification) or different graph types (e.g., dynamic graphs where the underlying structure changes rapidly) is not explicitly explored in depth.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GAUG significantly advances the state-of-the-art in graph data augmentation by providing a principled, learning-based method for structural manipulation, moving beyond random or ad-hoc approaches.\n    *   **Potential Impact on Future Research**:\n        *   Opens avenues for more sophisticated learned graph transformations beyond simple edge addition/removal.\n        *   Encourages further exploration of how latent graph properties (like class-homophily) can be leveraged for improving GNN performance.\n        *   Provides a robust baseline for future research in graph data augmentation, particularly for semi-supervised learning settings.\n        *   The distinction between modified-graph and original-graph settings offers a practical framework for designing augmentation strategies based on real-world deployment constraints.",
    "intriguing_abstract": "Data augmentation is a cornerstone of robust deep learning, yet its principled application to the complex, non-Euclidean structures of Graph Neural Networks (GNNs) remains a formidable challenge. Existing methods often rely on random perturbations or ad-hoc iterative schemes, risking performance degradation or error propagation. We introduce **GAUG**, a novel, learning-based framework that fundamentally redefines graph data augmentation for semi-supervised node classification.\n\nGAUG leverages neural edge predictors, specifically a Graph Auto-Encoder (GAE), to strategically identify and manipulate edges based on learned class-homophilic structures. This allows for the intelligent addition of missing intra-class connections and removal of noisy inter-class links, effectively 'denoising' and 'completing' the graph. We propose two distinct settings: GAUG-M, which modifies the graph for both training and inference, and GAUG-O, which augments only during training for scenarios requiring an unmodified inference graph. Theoretically grounded and modular, GAUG achieves unprecedented gains, yielding up to **17% absolute F1 performance improvements** over unaugmented GNNs and significantly outperforming baseline strategies on benchmark datasets. GAUG offers a robust, principled solution to a critical GNN challenge, paving the way for more generalizable and powerful graph learning models.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Graph data augmentation",
      "Semi-supervised node classification",
      "GAUG framework",
      "Learning-based edge manipulation",
      "Graph Auto-Encoder (GAE)",
      "Class-homophilic structure",
      "Strategic edge addition/removal",
      "Modified-Graph Setting (GAUG-M)",
      "Original-Graph Setting (GAUG-O)",
      "Neural edge predictors",
      "Performance improvements",
      "Model generalizability"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf",
    "citation_key": "zhao2020bmj",
    "metadata": {
      "title": "Data Augmentation for Graph Neural Networks",
      "authors": [
        "Tong Zhao",
        "Yozen Liu",
        "Leonardo Neves",
        "Oliver J. Woodford",
        "Meng Jiang",
        "Neil Shah"
      ],
      "published_date": "2020",
      "abstract": "Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{zhao2020bmj} for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"Data Augmentation for Graph Neural Networks\" \\cite{zhao2020bmj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of effective data augmentation techniques for Graph Neural Networks (GNNs), particularly for improving semi-supervised node classification.\n    *   **Importance and Challenge**: Data augmentation is crucial for improving model generalizability in machine learning (e.g., CV, NLP), but its application to graphs is challenging due to their complex, non-Euclidean structure. Standard augmentation operations (like rotation, cropping) have no direct analogs for graphs, and defining new, meaningful graph manipulations (beyond simple node/edge addition/removal) is difficult without introducing noise or reducing data.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing graph augmentation methods are limited:\n        *   **DROPEDGE** \\cite{rong2019dropedge} randomly removes edges, akin to dropout, but cannot add beneficial edges.\n        *   **ADAEDGE** \\cite{chen2019adaedge} iteratively adds/removes edges based on high-confidence label predictions, but is ad-hoc, prone to error propagation, and highly dependent on training size.\n        *   **BGCN** \\cite{zhang2019bgcn} iteratively trains a stochastic block model with GCN predictions to produce denoised graphs, also risking error propagation.\n    *   **Limitations of Previous Solutions**: These prior methods either rely on random perturbations (which can hurt performance), or iterative, ad-hoc approaches that are susceptible to error propagation and may not generalize well. \\cite{zhao2020bmj} positions itself by proposing a more principled, learning-based approach to strategically manipulate edges.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces the **GAUG** framework, which leverages neural edge predictors (specifically, a Graph Auto-Encoder (GAE) \\cite{kipf2016variational}) to learn and encode class-homophilic structure within a graph. This allows for strategic addition of \"missing\" intra-class edges and removal of \"noisy\" inter-class edges.\n    *   **Novelty/Difference**:\n        *   **Learning-based Edge Manipulation**: Unlike random or confidence-based iterative methods, GAUG uses a pre-trained edge predictor to deterministically identify which edges to add or remove based on learned probabilities, effectively \"denoising\" and \"completing\" the graph structure.\n        *   **Two Augmentation Settings**: GAUG proposes two distinct strategies based on inference feasibility:\n            *   **GAUG-M (Modified-Graph Setting)**: The input graph is fundamentally modified (edges added/removed) and this modified graph (`Gm`) is used for *both* training and inference. This is suitable when the graph is static.\n            *   **GAUG-O (Original-Graph Setting)**: Multiple augmented graph variants (`G_m^i`) are generated for training, but inference is performed *only* on the original graph (`G`). This is for dynamic or latency-sensitive scenarios where modifying the inference graph is impractical.\n        *   **Theoretical Motivation**: The paper provides a theoretical basis (Theorem 1) demonstrating that GNNs can easily differentiate classes and generate equivalent representations for same-class nodes on an \"ideal\" class-homophilic graph (fully connected components per class). GAUG aims to approximate this ideal.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of the GAUG framework (GAUG-M and GAUG-O) for graph data augmentation via learned edge manipulation.\n    *   **Insight into Edge Predictors**: Demonstrating that neural edge predictors (like GAE) can effectively learn and encode class-homophilic tendencies, enabling the promotion of intra-class edges and demotion of inter-class edges.\n    *   **Strategic Edge Manipulation**: A method to deterministically add top-scoring non-edges and remove lowest-scoring existing edges based on learned probabilities from an edge predictor.\n    *   **Modular Design**: The framework is designed to be modular, allowing flexible application to various GNN architectures.\n    *   **Formalization of Augmentation Settings**: Clearly distinguishing between \"modified-graph\" and \"original-graph\" settings for graph augmentation, addressing practical inference constraints.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on multiple benchmark datasets (e.g., Cora) and across various GNN architectures (e.g., GCN). The evaluation focused on semi-supervised node classification tasks.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metric**: Test micro-F1 score.\n        *   **GAUG-M Performance**: Showed significant improvements, achieving up to **17% absolute F1 performance improvements** over GNNs without augmentation, and up to **16% over baseline augmentation strategies**.\n        *   **GAUG-O Performance**: Demonstrated consistent improvements, with up to **9% absolute F1 performance improvements** over GNNs without augmentation, and up to **9% over baseline augmentation strategies**.\n        *   **Qualitative Analysis (Figure 2)**: Illustrated that GAUG-M's learned edge additions/removals lead to a much steeper growth/slower decrease of intra-class edges and a slower increase/steeper decrease of inter-class edges compared to random perturbations, directly correlating with improved classification performance. Random modifications were shown to hurt performance.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While GAUG-M shows strong performance, the paper notes that \"learned removal improves performance over part of the range (until 20%)\" (Figure 2), suggesting that excessive removal might eventually degrade performance. The theoretical discussion also implicitly acknowledges the risk of overfitting if modifications are \"too contrived,\" though GAUG aims to mitigate this.\n    *   **Scope of Applicability**: The framework is primarily evaluated and designed for semi-supervised node classification tasks. Its applicability to other graph tasks (e.g., link prediction, graph classification) or different graph types (e.g., dynamic graphs where the underlying structure changes rapidly) is not explicitly explored in depth.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GAUG significantly advances the state-of-the-art in graph data augmentation by providing a principled, learning-based method for structural manipulation, moving beyond random or ad-hoc approaches.\n    *   **Potential Impact on Future Research**:\n        *   Opens avenues for more sophisticated learned graph transformations beyond simple edge addition/removal.\n        *   Encourages further exploration of how latent graph properties (like class-homophily) can be leveraged for improving GNN performance.\n        *   Provides a robust baseline for future research in graph data augmentation, particularly for semi-supervised learning settings.\n        *   The distinction between modified-graph and original-graph settings offers a practical framework for designing augmentation strategies based on real-world deployment constraints.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Graph data augmentation",
        "Semi-supervised node classification",
        "GAUG framework",
        "Learning-based edge manipulation",
        "Graph Auto-Encoder (GAE)",
        "Class-homophilic structure",
        "Strategic edge addition/removal",
        "Modified-Graph Setting (GAUG-M)",
        "Original-Graph Setting (GAUG-O)",
        "Neural edge predictors",
        "Performance improvements",
        "Model generalizability"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"our main contribution introduces the gaug graph data augmentation framework, which leverages these insights to improve performance in gnn-based node classification via edge prediction.\"\n*   it also mentions: \"our work studies graph data augmentation for graph neural networks (gnns)... we discuss practical and theoretical motivations, considerations and strategies for graph data augmentation.\"\n*   the introduction sets up a technical problem: \"despite the complementary nature of gnns and data augmentation, few works present strategies for combining the two. one major obstacle is that, in contrast to...\"\n\nthese points strongly indicate the paper is presenting a new method or framework to solve a technical problem. while it includes \"extensive experiments\" (empirical aspect), the primary focus and contribution is the development and presentation of the \"gaug graph data augmentation framework.\"\n\ntherefore, the paper type is **technical**."
    },
    "file_name": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf"
  },
  {
    "success": true,
    "doc_id": "b3fc910d073dece3277384406186df5e",
    "summary": "Here's a focused summary of the paper by \\cite{joshi20239d0} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: On the Expressive Power of Geometric Graph Neural Networks \\cite{joshi20239d0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Characterizing and understanding the expressive power of Graph Neural Networks (GNNs) for *geometric graphs* embedded in Euclidean space. Standard GNNs and the Weisfeiler-Leman (WL) test, widely used for non-geometric graphs, are inapplicable to these systems.\n    *   **Importance & Challenge**: Geometric graphs, such as biomolecules, materials, and physical simulations, inherently possess both relational structure and geometric attributes (e.g., 3D coordinates, velocities). These geometric attributes must respect physical symmetries (permutations, rotation, reflection, translation). Standard GNNs fail to account for these symmetries, leading to a loss of physical meaning and transformation behavior. The challenge lies in developing a theoretical framework that can assess how well geometric GNNs can distinguish between geometrically non-isomorphic graphs while respecting these symmetries.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the established Weisfeiler-Leman (WL) graph isomorphism test, which has been a powerful tool for analyzing the expressive power of non-geometric GNNs.\n    *   **Limitations of Previous Solutions**: The standard WL framework and non-geometric GNNs do not directly apply to geometric graphs because they lack the ability to account for spatial symmetries. Geometric graphs exhibit a stronger notion of \"geometric isomorphism\" that requires invariance or equivariance to Euclidean transformations, which the WL test does not capture. Existing geometric GNNs (both G-equivariant and G-invariant) have shown empirical success, but their theoretical expressive power remained largely uncharacterized.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Geometric Weisfeiler-Leman (GWL) test**, a novel generalization of the WL test designed for discriminating geometric graphs while respecting physical symmetries.\n        *   GWL iteratively updates node \"colors\" and auxiliary \"geometric objects\" (`g(t)_i`) that aggregate geometric information from progressively larger t-hop neighborhoods.\n        *   It employs a **G-orbit injective and G-invariant function (I-HASH)** to compute scalar node colors, ensuring that neighborhoods identical up to a group action receive the same color.\n        *   Crucially, the aggregation of geometric objects (`g(t)_i`) is designed to be **injective and G-equivariant**, preserving local geometric orientation and information.\n    *   **Novelty/Difference**:\n        *   **Symmetry-Aware Isomorphism Test**: GWL is the first theoretical framework to extend the WL test to geometric graphs, explicitly incorporating invariance/equivariance to permutations, rotations, reflections, and translations.\n        *   **Granular Expressivity Analysis**: Unlike binary universal approximation, GWL provides a discrimination-based perspective, offering a more granular and practically insightful lens into geometric GNN expressivity by linking it to the ability to distinguish geometric (sub-)graphs.\n        *   **Unified Framework**: It provides a theoretical upper bound for the expressive power of both G-equivariant and G-invariant geometric GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Geometric Weisfeiler-Leman (GWL) test** as a theoretical upper bound for the expressive power of geometric GNNs.\n    *   **Theoretical Insights/Analysis**:\n        *   **Characterization of Expressivity**: GWL formally characterizes how key design choices influence geometric GNN expressivity:\n            *   **Invariant Layers' Limitations**: Invariant GNNs (and the proposed Invariant GWL, IGWL) have limited expressivity; they cannot distinguish \"1-hop identical\" geometric graphs and fail to compute non-local geometric properties (e.g., volume, centroid).\n            *   **Equivariant Layers' Power**: Equivariant GNNs (and GWL) distinguish a larger class of graphs by propagating geometric information beyond local neighborhoods through stacking equivariant layers.\n            *   **Role of Higher-Order Tensors/Scalarization**: Higher-order tensors and scalarization (e.g., `IGWL(k)` with higher body order `k`) are shown to enable maximally powerful geometric GNNs by providing more complete descriptors of local geometry (e.g., beyond just distances and angles).\n            *   **Depth**: Increasing depth (number of layers) allows GWL and equivariant GNNs to aggregate information from larger k-hop neighborhoods, distinguishing k-hop distinct graphs.\n        *   **Equivalence to Universal Approximation**: The paper proves an equivalence between a model's ability to discriminate geometric graphs (via GWL) and its universal approximation capabilities for G-invariant functions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Synthetic experiments were performed to demonstrate the practical implications of the theoretical findings. These experiments are available at `https://github.com/chaitjo/geometric-gnn-dojo`.\n    *   **Key Performance Metrics & Comparison Results**: The experiments focused on illustrating:\n        *   **Geometric Oversquashing with Increased Depth**: Demonstrating how increasing depth in geometric GNNs can lead to oversquashing, a phenomenon where distinct geometric information becomes indistinguishable.\n        *   **Utility of Higher Order Spherical Tensors**: Providing counterexamples that highlight the necessity and utility of higher-order spherical tensors for distinguishing complex local geometries that lower-order invariants (like distances and angles) cannot resolve. This empirically supports the theoretical claim about the importance of scalarization body order.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   GWL assumes finite-sized geometric graphs and features drawn from countable datasets.\n        *   The `HASH` and `I-HASH` functions are idealized injective and G-orbit injective maps, respectively, which are not necessarily continuous in practical implementations.\n        *   The theoretical analysis focuses on the *upper bound* of expressivity, and achieving this bound in practical GNNs requires specific conditions on aggregation, update, and readout functions (e.g., injectivity, equivariance).\n    *   **Scope of Applicability**: The framework is applicable to geometric graphs embedded in Euclidean space, particularly relevant for systems in biochemistry, material science, and physical simulations where physical symmetries are crucial. It primarily focuses on distinguishing geometric graphs and characterizing the expressive power of GNNs in this context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the theoretical understanding of geometric GNNs by providing the first principled framework (GWL) to analyze their expressive power in a symmetry-aware manner. It extends the foundational WL test to a critical domain where standard GNNs fall short.\n    *   **Potential Impact on Future Research**:\n        *   **Guiding Architecture Design**: The insights derived from GWL (e.g., the importance of equivariant layers, higher-order tensors, and understanding depth limitations) can directly guide the design of more powerful and expressive geometric GNN architectures.\n        *   **Benchmarking and Evaluation**: GWL provides a theoretical benchmark against which the expressive power of new geometric GNN models can be rigorously evaluated.\n        *   **Deeper Theoretical Understanding**: It offers a more granular and practically relevant perspective on expressivity compared to universal approximation, fostering deeper theoretical investigations into the capabilities and limitations of geometric deep learning models.",
    "intriguing_abstract": "Unlocking the full potential of Graph Neural Networks (GNNs) for complex physical systems like molecules and materials demands a deep understanding of their ability to process *geometric graphs* while respecting fundamental *physical symmetries*. Traditional GNNs and the Weisfeiler-Leman (WL) test fall short, failing to account for crucial *invariance* and *equivariance* to rotations, reflections, and translations.\n\nThis paper introduces the **Geometric Weisfeiler-Leman (GWL) test**, a novel, symmetry-aware generalization of the WL test, providing the first principled framework to rigorously characterize the *expressive power* of *geometric GNNs*. GWL unveils how design choices, such as the use of *equivariant layers* and *higher-order tensors*, are critical for distinguishing geometrically non-isomorphic graphs and achieving maximal expressivity. We demonstrate that while *invariant GNNs* have inherent limitations, *equivariant GNNs* can propagate rich geometric information, proving an equivalence between geometric graph discrimination and *universal approximation* for G-invariant functions. Our theoretical insights, supported by empirical validation, offer profound implications for guiding the design of more powerful and robust geometric GNN architectures, setting a new benchmark for understanding their capabilities in scientific discovery.",
    "keywords": [
      "Geometric Graph Neural Networks",
      "Geometric Weisfeiler-Leman (GWL) test",
      "expressive power",
      "physical symmetries",
      "G-equivariant layers",
      "G-invariant layers",
      "higher-order tensors",
      "geometric isomorphism",
      "discrimination-based expressivity",
      "geometric oversquashing",
      "biomolecules and materials",
      "WL test generalization",
      "universal approximation equivalence"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/5e6db511e736f77f844bbeebaa2b177427abada1.pdf",
    "citation_key": "joshi20239d0",
    "metadata": {
      "title": "On the Expressive Power of Geometric Graph Neural Networks",
      "authors": [
        "Chaitanya K. Joshi",
        "Simon V. Mathis"
      ],
      "published_date": "2023",
      "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
      "file_path": "paper_data/Graph_Neural_Networks/5e6db511e736f77f844bbeebaa2b177427abada1.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper by \\cite{joshi20239d0} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: On the Expressive Power of Geometric Graph Neural Networks \\cite{joshi20239d0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Characterizing and understanding the expressive power of Graph Neural Networks (GNNs) for *geometric graphs* embedded in Euclidean space. Standard GNNs and the Weisfeiler-Leman (WL) test, widely used for non-geometric graphs, are inapplicable to these systems.\n    *   **Importance & Challenge**: Geometric graphs, such as biomolecules, materials, and physical simulations, inherently possess both relational structure and geometric attributes (e.g., 3D coordinates, velocities). These geometric attributes must respect physical symmetries (permutations, rotation, reflection, translation). Standard GNNs fail to account for these symmetries, leading to a loss of physical meaning and transformation behavior. The challenge lies in developing a theoretical framework that can assess how well geometric GNNs can distinguish between geometrically non-isomorphic graphs while respecting these symmetries.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the established Weisfeiler-Leman (WL) graph isomorphism test, which has been a powerful tool for analyzing the expressive power of non-geometric GNNs.\n    *   **Limitations of Previous Solutions**: The standard WL framework and non-geometric GNNs do not directly apply to geometric graphs because they lack the ability to account for spatial symmetries. Geometric graphs exhibit a stronger notion of \"geometric isomorphism\" that requires invariance or equivariance to Euclidean transformations, which the WL test does not capture. Existing geometric GNNs (both G-equivariant and G-invariant) have shown empirical success, but their theoretical expressive power remained largely uncharacterized.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Geometric Weisfeiler-Leman (GWL) test**, a novel generalization of the WL test designed for discriminating geometric graphs while respecting physical symmetries.\n        *   GWL iteratively updates node \"colors\" and auxiliary \"geometric objects\" (`g(t)_i`) that aggregate geometric information from progressively larger t-hop neighborhoods.\n        *   It employs a **G-orbit injective and G-invariant function (I-HASH)** to compute scalar node colors, ensuring that neighborhoods identical up to a group action receive the same color.\n        *   Crucially, the aggregation of geometric objects (`g(t)_i`) is designed to be **injective and G-equivariant**, preserving local geometric orientation and information.\n    *   **Novelty/Difference**:\n        *   **Symmetry-Aware Isomorphism Test**: GWL is the first theoretical framework to extend the WL test to geometric graphs, explicitly incorporating invariance/equivariance to permutations, rotations, reflections, and translations.\n        *   **Granular Expressivity Analysis**: Unlike binary universal approximation, GWL provides a discrimination-based perspective, offering a more granular and practically insightful lens into geometric GNN expressivity by linking it to the ability to distinguish geometric (sub-)graphs.\n        *   **Unified Framework**: It provides a theoretical upper bound for the expressive power of both G-equivariant and G-invariant geometric GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Geometric Weisfeiler-Leman (GWL) test** as a theoretical upper bound for the expressive power of geometric GNNs.\n    *   **Theoretical Insights/Analysis**:\n        *   **Characterization of Expressivity**: GWL formally characterizes how key design choices influence geometric GNN expressivity:\n            *   **Invariant Layers' Limitations**: Invariant GNNs (and the proposed Invariant GWL, IGWL) have limited expressivity; they cannot distinguish \"1-hop identical\" geometric graphs and fail to compute non-local geometric properties (e.g., volume, centroid).\n            *   **Equivariant Layers' Power**: Equivariant GNNs (and GWL) distinguish a larger class of graphs by propagating geometric information beyond local neighborhoods through stacking equivariant layers.\n            *   **Role of Higher-Order Tensors/Scalarization**: Higher-order tensors and scalarization (e.g., `IGWL(k)` with higher body order `k`) are shown to enable maximally powerful geometric GNNs by providing more complete descriptors of local geometry (e.g., beyond just distances and angles).\n            *   **Depth**: Increasing depth (number of layers) allows GWL and equivariant GNNs to aggregate information from larger k-hop neighborhoods, distinguishing k-hop distinct graphs.\n        *   **Equivalence to Universal Approximation**: The paper proves an equivalence between a model's ability to discriminate geometric graphs (via GWL) and its universal approximation capabilities for G-invariant functions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Synthetic experiments were performed to demonstrate the practical implications of the theoretical findings. These experiments are available at `https://github.com/chaitjo/geometric-gnn-dojo`.\n    *   **Key Performance Metrics & Comparison Results**: The experiments focused on illustrating:\n        *   **Geometric Oversquashing with Increased Depth**: Demonstrating how increasing depth in geometric GNNs can lead to oversquashing, a phenomenon where distinct geometric information becomes indistinguishable.\n        *   **Utility of Higher Order Spherical Tensors**: Providing counterexamples that highlight the necessity and utility of higher-order spherical tensors for distinguishing complex local geometries that lower-order invariants (like distances and angles) cannot resolve. This empirically supports the theoretical claim about the importance of scalarization body order.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   GWL assumes finite-sized geometric graphs and features drawn from countable datasets.\n        *   The `HASH` and `I-HASH` functions are idealized injective and G-orbit injective maps, respectively, which are not necessarily continuous in practical implementations.\n        *   The theoretical analysis focuses on the *upper bound* of expressivity, and achieving this bound in practical GNNs requires specific conditions on aggregation, update, and readout functions (e.g., injectivity, equivariance).\n    *   **Scope of Applicability**: The framework is applicable to geometric graphs embedded in Euclidean space, particularly relevant for systems in biochemistry, material science, and physical simulations where physical symmetries are crucial. It primarily focuses on distinguishing geometric graphs and characterizing the expressive power of GNNs in this context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the theoretical understanding of geometric GNNs by providing the first principled framework (GWL) to analyze their expressive power in a symmetry-aware manner. It extends the foundational WL test to a critical domain where standard GNNs fall short.\n    *   **Potential Impact on Future Research**:\n        *   **Guiding Architecture Design**: The insights derived from GWL (e.g., the importance of equivariant layers, higher-order tensors, and understanding depth limitations) can directly guide the design of more powerful and expressive geometric GNN architectures.\n        *   **Benchmarking and Evaluation**: GWL provides a theoretical benchmark against which the expressive power of new geometric GNN models can be rigorously evaluated.\n        *   **Deeper Theoretical Understanding**: It offers a more granular and practically relevant perspective on expressivity compared to universal approximation, fostering deeper theoretical investigations into the capabilities and limitations of geometric deep learning models.",
      "keywords": [
        "Geometric Graph Neural Networks",
        "Geometric Weisfeiler-Leman (GWL) test",
        "expressive power",
        "physical symmetries",
        "G-equivariant layers",
        "G-invariant layers",
        "higher-order tensors",
        "geometric isomorphism",
        "discrimination-based expressivity",
        "geometric oversquashing",
        "biomolecules and materials",
        "WL test generalization",
        "universal approximation equivalence"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose a geometric version of the wl test (gwl)\"**: this indicates the development of a new formal model or test. the weisfeiler-leman test is a theoretical framework for graph isomorphism. proposing a geometric version of it falls under developing a formal model.\n2.  **\"we use gwl to characterise the expressive power of geometric gnns\"**: \"expressive power\" is a fundamental concept in theoretical computer science and machine learning, often analyzed through formal models and mathematical proofs. \"characterise\" implies a deep analytical understanding of capabilities and limitations.\n3.  **the findings (1)-(4)**: these points describe analytical properties and limitations of different geometric gnn architectures, derived from the gwl framework. for example, \"invariant layers have limited expressivity,\" \"equivariant layers distinguish a larger class of graphs,\" and \"gwl’s discrimination-based perspective is equivalent to universal approximation.\" the equivalence to \"universal approximation\" is a strong indicator of a theoretical contribution, as it involves proving fundamental capabilities.\n4.  **\"mathematical analysis, proofs, formal models\"**: the gwl itself is a formal model, and its application to characterize expressivity involves theoretical analysis.\n\nwhile the paper \"proposes\" a new test (which could be seen as technical), the primary goal and outcome are the **mathematical analysis** and **characterization of expressive power** using this formal model. the focus is on understanding the fundamental properties and limits of geometric gnns, which aligns perfectly with the definition of a **theoretical** paper. the mention of \"synthetic experiments supplementing our results\" suggests that the empirical work supports the theoretical findings rather than being the main contribution.\n\n**classification: theoretical**"
    },
    "file_name": "5e6db511e736f77f844bbeebaa2b177427abada1.pdf"
  },
  {
    "success": true,
    "doc_id": "548a023e13ba59c44e1cf940ff41a762",
    "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of supervised Graph Neural Network (GNN) training, which notoriously requires large amounts of labeled data. While transfer learning (pre-training GNNs and fine-tuning) is a promising solution, existing self-supervised pre-training methods suffer from a significant \"inherent training objective gap\" between the pretext task and the downstream task.\n    *   **Importance & Challenge**: This task gap necessitates costly and tedious fine-tuning, which prevents efficient elicitation of pre-trained knowledge, often leading to poor downstream performance. In some cases, naive pre-training can even deteriorate downstream tasks, undermining the reliability of transfer learning for graph data \\cite{sun2022d18}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the concept of self-supervised pre-training for GNNs, which aims to encode universal graph knowledge.\n    *   **Limitations of Previous Solutions**: Existing self-supervised pre-training methods for GNNs largely overlook the critical training objective gap between the pre-training pretext task and the downstream task. This oversight leads to inefficient knowledge transfer, requiring extensive fine-tuning, and can even result in performance degradation on downstream tasks \\cite{sun2022d18}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Graph Pre-training and Prompt Tuning (GPPT) \\cite{sun2022d18}.\n        *   **Pre-training Phase**: GNNs are initially pre-trained using the masked edge prediction task, chosen for its simplicity and popularity.\n        *   **Prompt Tuning Phase**: A novel \"graph prompting function\" is introduced. This function modifies a standalone node (for downstream node classification) into a \"token pair.\"\n        *   **Token Pair Composition**: Each token pair consists of a \"candidate label class\" and the \"node entity.\"\n        *   **Task Reformulation**: The downstream node classification task is ingeniously reformulated to mimic the structure of the edge prediction task used during pre-training.\n    *   **Novelty**: The core innovation lies in bridging the inherent training objective gap by reformulating the downstream task to align with the pre-training task. This allows the pre-trained GNNs to be applied *without tedious fine-tuning* to evaluate the \"linking probability\" of the token pair, directly yielding node classification decisions \\cite{sun2022d18}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The Graph Pre-training and Prompt Tuning (GPPT) paradigm itself, which explicitly addresses the task gap in GNN transfer learning \\cite{sun2022d18}.\n        *   The \"graph prompting function\" that transforms individual nodes into token pairs (candidate label class + node entity).\n        *   The reformulation of downstream node classification into an \"edge prediction-like\" task, enabling direct application of pre-trained models without fine-tuning.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on eight benchmark datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   GPPT demonstrated superior performance, delivering an average improvement of **4.29%** in few-shot graph analysis.\n        *   It significantly accelerated model convergence, achieving up to **4.32X** faster convergence \\cite{sun2022d18}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided text does not explicitly state technical limitations or assumptions beyond the focus on node classification and few-shot scenarios. The method is designed for scenarios where the downstream task can be reformulated to resemble the pre-training task (e.g., node classification as a form of \"linking\" between a node and a label).\n    *   **Scope of Applicability**: The method is primarily applicable to transfer learning for GNNs, particularly in few-shot graph analysis and node classification tasks, where the goal is to leverage pre-trained knowledge efficiently without extensive fine-tuning \\cite{sun2022d18}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: GPPT significantly advances the technical state-of-the-art in GNN transfer learning by effectively bridging the critical training objective gap between pre-training and downstream tasks. This leads to more efficient and effective knowledge transfer \\cite{sun2022d18}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into prompt-based learning for GNNs, potentially inspiring similar task reformulation strategies for other graph-related problems (e.g., link prediction, graph classification) to minimize fine-tuning costs and improve few-shot performance. It highlights the importance of aligning pre-training and downstream task objectives.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are revolutionizing data analysis, yet their reliance on vast amounts of labeled data for supervised training remains a significant bottleneck. While self-supervised pre-training offers a promising avenue for transfer learning, existing methods are plagued by a critical \"inherent training objective gap\" between pretext tasks and downstream applications. This gap necessitates costly and tedious fine-tuning, often hindering knowledge transfer and even degrading performance.\n\nWe introduce Graph Pre-training and Prompt Tuning (GPPT), a novel paradigm that explicitly bridges this objective gap. GPPT pre-trains GNNs using masked edge prediction. Crucially, it then employs a unique \"graph prompting function\" to reformulate downstream node classification tasks. By transforming individual nodes into \"token pairs\" (candidate label + node entity), GPPT ingeniously aligns the downstream task structure with the pre-training objective. This allows pre-trained GNNs to be applied directly, *without tedious fine-tuning*, to evaluate \"linking probabilities\" for classification. Extensive experiments on eight benchmarks demonstrate GPPT's superiority, achieving an average 4.29% improvement in few-shot graph analysis and up to 4.32X faster convergence. GPPT significantly advances GNN transfer learning, offering a highly efficient and effective solution for data-scarce scenarios and opening new frontiers for prompt-based graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "transfer learning",
      "self-supervised pre-training",
      "training objective gap",
      "Graph Pre-training and Prompt Tuning (GPPT)",
      "prompt tuning",
      "graph prompting function",
      "task reformulation",
      "masked edge prediction",
      "node classification",
      "few-shot graph analysis",
      "efficient knowledge transfer",
      "accelerated model convergence",
      "no tedious fine-tuning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf",
    "citation_key": "sun2022d18",
    "metadata": {
      "title": "GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks",
      "authors": [
        "Mingchen Sun",
        "Kaixiong Zhou",
        "Xingbo He",
        "Ying Wang",
        "Xin Wang"
      ],
      "published_date": "2022",
      "abstract": "Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis and accelerating the model convergence up to 4.32X. The code is available in: https://github.com/MingChen-Sun/GPPT.",
      "file_path": "paper_data/Graph_Neural_Networks/e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of supervised Graph Neural Network (GNN) training, which notoriously requires large amounts of labeled data. While transfer learning (pre-training GNNs and fine-tuning) is a promising solution, existing self-supervised pre-training methods suffer from a significant \"inherent training objective gap\" between the pretext task and the downstream task.\n    *   **Importance & Challenge**: This task gap necessitates costly and tedious fine-tuning, which prevents efficient elicitation of pre-trained knowledge, often leading to poor downstream performance. In some cases, naive pre-training can even deteriorate downstream tasks, undermining the reliability of transfer learning for graph data \\cite{sun2022d18}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the concept of self-supervised pre-training for GNNs, which aims to encode universal graph knowledge.\n    *   **Limitations of Previous Solutions**: Existing self-supervised pre-training methods for GNNs largely overlook the critical training objective gap between the pre-training pretext task and the downstream task. This oversight leads to inefficient knowledge transfer, requiring extensive fine-tuning, and can even result in performance degradation on downstream tasks \\cite{sun2022d18}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Graph Pre-training and Prompt Tuning (GPPT) \\cite{sun2022d18}.\n        *   **Pre-training Phase**: GNNs are initially pre-trained using the masked edge prediction task, chosen for its simplicity and popularity.\n        *   **Prompt Tuning Phase**: A novel \"graph prompting function\" is introduced. This function modifies a standalone node (for downstream node classification) into a \"token pair.\"\n        *   **Token Pair Composition**: Each token pair consists of a \"candidate label class\" and the \"node entity.\"\n        *   **Task Reformulation**: The downstream node classification task is ingeniously reformulated to mimic the structure of the edge prediction task used during pre-training.\n    *   **Novelty**: The core innovation lies in bridging the inherent training objective gap by reformulating the downstream task to align with the pre-training task. This allows the pre-trained GNNs to be applied *without tedious fine-tuning* to evaluate the \"linking probability\" of the token pair, directly yielding node classification decisions \\cite{sun2022d18}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The Graph Pre-training and Prompt Tuning (GPPT) paradigm itself, which explicitly addresses the task gap in GNN transfer learning \\cite{sun2022d18}.\n        *   The \"graph prompting function\" that transforms individual nodes into token pairs (candidate label class + node entity).\n        *   The reformulation of downstream node classification into an \"edge prediction-like\" task, enabling direct application of pre-trained models without fine-tuning.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on eight benchmark datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   GPPT demonstrated superior performance, delivering an average improvement of **4.29%** in few-shot graph analysis.\n        *   It significantly accelerated model convergence, achieving up to **4.32X** faster convergence \\cite{sun2022d18}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided text does not explicitly state technical limitations or assumptions beyond the focus on node classification and few-shot scenarios. The method is designed for scenarios where the downstream task can be reformulated to resemble the pre-training task (e.g., node classification as a form of \"linking\" between a node and a label).\n    *   **Scope of Applicability**: The method is primarily applicable to transfer learning for GNNs, particularly in few-shot graph analysis and node classification tasks, where the goal is to leverage pre-trained knowledge efficiently without extensive fine-tuning \\cite{sun2022d18}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: GPPT significantly advances the technical state-of-the-art in GNN transfer learning by effectively bridging the critical training objective gap between pre-training and downstream tasks. This leads to more efficient and effective knowledge transfer \\cite{sun2022d18}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into prompt-based learning for GNNs, potentially inspiring similar task reformulation strategies for other graph-related problems (e.g., link prediction, graph classification) to minimize fine-tuning costs and improve few-shot performance. It highlights the importance of aligning pre-training and downstream task objectives.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "transfer learning",
        "self-supervised pre-training",
        "training objective gap",
        "Graph Pre-training and Prompt Tuning (GPPT)",
        "prompt tuning",
        "graph prompting function",
        "task reformulation",
        "masked edge prediction",
        "node classification",
        "few-shot graph analysis",
        "efficient knowledge transfer",
        "accelerated model convergence",
        "no tedious fine-tuning"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **strong indicators for \"technical\"**:\n    *   the abstract identifies a problem (\"inherent training objective gap\") and the introduction immediately states: \"to bridge the task gap, we **propose a novel transfer learning paradigm** to generalize gnns, namely graph pre-training and prompt tuning (gppt).\"\n    *   it further elaborates: \"we **propose the graph prompting function** to modify the standalone node into a token pair...\"\n    *   keywords like \"propose,\" \"novel transfer learning paradigm,\" and \"graph prompting function\" directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n    *   the title itself, \"gppt: graph pre-training and prompt tuning to generalize graph neural networks,\" indicates the introduction of a new system/method.\n\n*   **presence of \"empirical\" elements**:\n    *   the introduction mentions: \"the extensive **experiments on eight benchmark datasets demonstrate the superiority of gppt**, delivering an average improvement of 4.29%... and accelerating the model convergence up to 4.32x.\" this clearly indicates an empirical evaluation.\n    *   however, these experiments are conducted *to demonstrate the superiority* of the *proposed* method. the empirical work serves to validate the technical contribution, rather than being the sole or primary focus of the paper (e.g., analyzing existing data without proposing a new method).\n\nwhile the paper includes significant empirical work, its core contribution, as highlighted in the abstract and introduction, is the **development and proposal of a new method/system (gppt)**. the experiments are the means to validate this technical innovation."
    },
    "file_name": "e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf"
  },
  {
    "success": true,
    "doc_id": "75a571f6d4389ab2f01a0e9388142ad3",
    "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of Estimated Time of Arrival (ETA) prediction in large-scale transportation networks, specifically for web mapping services like Google Maps \\cite{derrowpinion2021mwn}.\n    *   This problem is important due to its impact on user experience, informed decision-making for traffic participants, and enterprise applications (e.g., food delivery). It is challenging because it requires accounting for complex spatiotemporal interactions, modeling the topological properties of the road network, and anticipating future events (like rush hours) that are not immediately obvious from current road states \\cite{derrowpinion2021mwn}.\n\n*   **Related Work & Positioning**\n    *   This work builds upon advancements in graph representation learning, particularly Graph Neural Networks (GNNs), and the Graph Network framework, following an encode-process-decode paradigm \\cite{derrowpinion2021mwn}.\n    *   It relates to existing approaches in travel-time prediction and spatiotemporal traffic forecasting, which often use GNNs combined with recurrent mechanisms or leverage historical travel times.\n    *   A key limitation of previous production solutions, including Google Maps' prior baseline, was their reliance on segment-level linear regression models that did not leverage the inherent graph structure of the road network, leading to less accurate predictions \\cite{derrowpinion2021mwn}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is a Graph Neural Network (GNN) estimator deployed in production at Google Maps for ETA prediction \\cite{derrowpinion2021mwn}.\n    *   The approach models the road network using \"supersegments\" (sequences of connected road segments) and predicts travel times for multiple fixed time horizons into the future. At serving time, predictions for sequential supersegments are interpolated to provide accurate ETAs \\cite{derrowpinion2021mwn}.\n    *   The GNN architecture comprises three Graph Network (GN) blocks (encoder, processor, decoder) and learns separate models for each prediction horizon. It predicts supersegment, segment, and cumulative segment travel times, with the supersegment-level output used in production \\cite{derrowpinion2021mwn}.\n    *   Novel aspects include:\n        *   **Sophisticated Featurization**: Utilizing both real-time and historical traffic data (speeds, times) at segment and supersegment levels, along with learnable embedding vectors for segments and supersegments \\cite{derrowpinion2021mwn}.\n        *   **Robust Training Regimes**: Employing MetaGradients to dynamically tune the learning rate and stabilize training across uneven query batches, making the model production-ready. It also incorporates semi-supervised training methods (graph auto-encoders, deep graph infomax) to discover and compress road network motifs \\cite{derrowpinion2021mwn}.\n        *   **Aggregator Combinations**: Investigating and leveraging various combinations of GNN aggregation functions (e.g., summing and averaging) for improved performance in different modeling scenarios \\cite{derrowpinion2021mwn}.\n\n*   **Key Technical Contributions**\n    *   **Novel GNN Model Design**: A robust GNN estimator built from standard Graph Network blocks, specifically tailored for ETA prediction by operating on \"supersegments\" and predicting across multiple future horizons \\cite{derrowpinion2021mwn}.\n    *   **Advanced Training Methodologies**: Integration of MetaGradients for dynamic learning rate tuning and stability, alongside semi-supervised techniques (graph auto-encoders, deep graph infomax) for enhanced representation learning \\cite{derrowpinion2021mwn}.\n    *   **Innovative Data Featurization**: A comprehensive approach to featurizing road network data, combining real-time and historical traffic patterns with learnable embeddings at both segment and supersegment levels \\cite{derrowpinion2021mwn}.\n    *   **Production Deployment**: Successful deployment of a GNN-based ETA predictor in a large-scale, real-world system (Google Maps), demonstrating its stability and performance under production conditions \\cite{derrowpinion2021mwn}.\n\n*   **Experimental Validation**\n    *   **Deployment in Google Maps**: The GNN model was deployed in production within Google Maps, serving real-world users \\cite{derrowpinion2021mwn}.\n    *   **Key Performance Metric**: The primary metric for evaluation was the reduction in \"negative ETA outcomes,\" defined as instances where the ETA error from the observed travel duration exceeds a certain threshold \\cite{derrowpinion2021mwn}.\n    *   **Comparison Results**: The deployed GNN significantly reduced negative ETA outcomes compared to the previous production baseline (which used segment-level linear regression). Specific improvements included over 40% reduction in cities like Sydney and substantial gains across all trips worldwide \\cite{derrowpinion2021mwn}.\n    *   **Offline Ablations**: Extensive offline studies were conducted to ablate various architectural decisions (e.g., aggregator combinations) and training regimes, providing prescriptive advice for practitioners \\cite{derrowpinion2021mwn}.\n    *   **Qualitative Analyses**: Visualizations of specific traffic situations were performed to demonstrate where the GNN model provided a clear advantage over the prior baseline \\cite{derrowpinion2021mwn}.\n\n*   **Limitations & Scope**\n    *   The GNN model detailed in the paper processes static inputs and does not directly forecast future speeds, although having estimates about future traffic flow is considered meaningful for ETA prediction \\cite{derrowpinion2021mwn}.\n    *   The training conditions for ETA prediction in production can be unstable across many batches of queries, particularly over routes of different scales, which necessitated the use of stabilizing methods like MetaGradients \\cite{derrowpinion2021mwn}.\n    *   The models are region-specific, with data constructed for each region to build region-specific GNN models \\cite{derrowpinion2021mwn}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by demonstrating the first large-scale, real-world deployment of a GNN for ETA prediction in a critical, user-facing application like Google Maps \\cite{derrowpinion2021mwn}.\n    *   It provides a blueprint for constructing powerful and stable GNN estimators for complex spatiotemporal tasks, offering insights into featurization, model design, and robust training regimes (e.g., MetaGradients) essential for production readiness \\cite{derrowpinion2021mwn}.\n    *   The observed substantial reductions in ETA errors (e.g., 40%+ in Sydney) highlight the practical efficacy and superiority of graph-aware models over traditional approaches, paving the way for future research in dynamic graph representation learning and more sophisticated spatiotemporal forecasting in transportation networks \\cite{derrowpinion2021mwn}.",
    "intriguing_abstract": "Accurate Estimated Time of Arrival (ETA) prediction in vast, dynamic transportation networks is a critical, yet notoriously challenging, problem impacting millions of users daily. This paper introduces a groundbreaking Graph Neural Network (GNN) estimator, now successfully deployed at scale within Google Maps, that fundamentally transforms real-time ETA prediction. Moving beyond traditional segment-level models, our novel approach leverages the inherent topological structure of road networks by operating on \"supersegments\" and predicting across multiple future horizons. We detail sophisticated featurization, combining real-time and historical traffic data with learnable embeddings, and pioneer robust training regimes. Notably, we integrate MetaGradients for dynamic learning rate tuning and employ semi-supervised techniques like graph auto-encoders and deep graph infomax to uncover latent road network motifs, ensuring production stability and superior representation learning. The impact is profound: deployed globally, this GNN model has achieved a remarkable reduction in negative ETA outcomes, exceeding 40% in major cities like Sydney and delivering substantial worldwide improvements. This work not only sets a new state-of-the-art for large-scale, real-world ETA prediction but also provides a robust blueprint for deploying powerful GNNs in complex spatiotemporal applications, paving the way for more intelligent and reliable urban mobility systems.",
    "keywords": [
      "ETA prediction",
      "Graph Neural Networks (GNNs)",
      "Large-scale transportation networks",
      "Google Maps",
      "Supersegments",
      "MetaGradients",
      "Semi-supervised training",
      "Sophisticated featurization",
      "Production deployment",
      "Negative ETA outcomes reduction",
      "Spatiotemporal traffic forecasting",
      "Graph Network framework",
      "Learnable embeddings"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/5822490cf59df7f7ccb92b8901f244850b867a66.pdf",
    "citation_key": "derrowpinion2021mwn",
    "metadata": {
      "title": "ETA Prediction with Graph Neural Networks in Google Maps",
      "authors": [
        "Austin Derrow-Pinion",
        "Jennifer She",
        "David Wong",
        "Oliver Lange",
        "Todd Hester",
        "L. Perez",
        "Marc Nunkesser",
        "Seongjae Lee",
        "Xueying Guo",
        "Brett Wiltshire",
        "P. Battaglia",
        "Vishal Gupta",
        "Ang Li",
        "Zhongwen Xu",
        "Alvaro Sanchez-Gonzalez",
        "Yujia Li",
        "Petar Velivckovi'c"
      ],
      "published_date": "2021",
      "abstract": "Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events---such as rush hours---that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",
      "file_path": "paper_data/Graph_Neural_Networks/5822490cf59df7f7ccb92b8901f244850b867a66.pdf",
      "venue": "International Conference on Information and Knowledge Management",
      "citationCount": 0,
      "score": 0,
      "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of Estimated Time of Arrival (ETA) prediction in large-scale transportation networks, specifically for web mapping services like Google Maps \\cite{derrowpinion2021mwn}.\n    *   This problem is important due to its impact on user experience, informed decision-making for traffic participants, and enterprise applications (e.g., food delivery). It is challenging because it requires accounting for complex spatiotemporal interactions, modeling the topological properties of the road network, and anticipating future events (like rush hours) that are not immediately obvious from current road states \\cite{derrowpinion2021mwn}.\n\n*   **Related Work & Positioning**\n    *   This work builds upon advancements in graph representation learning, particularly Graph Neural Networks (GNNs), and the Graph Network framework, following an encode-process-decode paradigm \\cite{derrowpinion2021mwn}.\n    *   It relates to existing approaches in travel-time prediction and spatiotemporal traffic forecasting, which often use GNNs combined with recurrent mechanisms or leverage historical travel times.\n    *   A key limitation of previous production solutions, including Google Maps' prior baseline, was their reliance on segment-level linear regression models that did not leverage the inherent graph structure of the road network, leading to less accurate predictions \\cite{derrowpinion2021mwn}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is a Graph Neural Network (GNN) estimator deployed in production at Google Maps for ETA prediction \\cite{derrowpinion2021mwn}.\n    *   The approach models the road network using \"supersegments\" (sequences of connected road segments) and predicts travel times for multiple fixed time horizons into the future. At serving time, predictions for sequential supersegments are interpolated to provide accurate ETAs \\cite{derrowpinion2021mwn}.\n    *   The GNN architecture comprises three Graph Network (GN) blocks (encoder, processor, decoder) and learns separate models for each prediction horizon. It predicts supersegment, segment, and cumulative segment travel times, with the supersegment-level output used in production \\cite{derrowpinion2021mwn}.\n    *   Novel aspects include:\n        *   **Sophisticated Featurization**: Utilizing both real-time and historical traffic data (speeds, times) at segment and supersegment levels, along with learnable embedding vectors for segments and supersegments \\cite{derrowpinion2021mwn}.\n        *   **Robust Training Regimes**: Employing MetaGradients to dynamically tune the learning rate and stabilize training across uneven query batches, making the model production-ready. It also incorporates semi-supervised training methods (graph auto-encoders, deep graph infomax) to discover and compress road network motifs \\cite{derrowpinion2021mwn}.\n        *   **Aggregator Combinations**: Investigating and leveraging various combinations of GNN aggregation functions (e.g., summing and averaging) for improved performance in different modeling scenarios \\cite{derrowpinion2021mwn}.\n\n*   **Key Technical Contributions**\n    *   **Novel GNN Model Design**: A robust GNN estimator built from standard Graph Network blocks, specifically tailored for ETA prediction by operating on \"supersegments\" and predicting across multiple future horizons \\cite{derrowpinion2021mwn}.\n    *   **Advanced Training Methodologies**: Integration of MetaGradients for dynamic learning rate tuning and stability, alongside semi-supervised techniques (graph auto-encoders, deep graph infomax) for enhanced representation learning \\cite{derrowpinion2021mwn}.\n    *   **Innovative Data Featurization**: A comprehensive approach to featurizing road network data, combining real-time and historical traffic patterns with learnable embeddings at both segment and supersegment levels \\cite{derrowpinion2021mwn}.\n    *   **Production Deployment**: Successful deployment of a GNN-based ETA predictor in a large-scale, real-world system (Google Maps), demonstrating its stability and performance under production conditions \\cite{derrowpinion2021mwn}.\n\n*   **Experimental Validation**\n    *   **Deployment in Google Maps**: The GNN model was deployed in production within Google Maps, serving real-world users \\cite{derrowpinion2021mwn}.\n    *   **Key Performance Metric**: The primary metric for evaluation was the reduction in \"negative ETA outcomes,\" defined as instances where the ETA error from the observed travel duration exceeds a certain threshold \\cite{derrowpinion2021mwn}.\n    *   **Comparison Results**: The deployed GNN significantly reduced negative ETA outcomes compared to the previous production baseline (which used segment-level linear regression). Specific improvements included over 40% reduction in cities like Sydney and substantial gains across all trips worldwide \\cite{derrowpinion2021mwn}.\n    *   **Offline Ablations**: Extensive offline studies were conducted to ablate various architectural decisions (e.g., aggregator combinations) and training regimes, providing prescriptive advice for practitioners \\cite{derrowpinion2021mwn}.\n    *   **Qualitative Analyses**: Visualizations of specific traffic situations were performed to demonstrate where the GNN model provided a clear advantage over the prior baseline \\cite{derrowpinion2021mwn}.\n\n*   **Limitations & Scope**\n    *   The GNN model detailed in the paper processes static inputs and does not directly forecast future speeds, although having estimates about future traffic flow is considered meaningful for ETA prediction \\cite{derrowpinion2021mwn}.\n    *   The training conditions for ETA prediction in production can be unstable across many batches of queries, particularly over routes of different scales, which necessitated the use of stabilizing methods like MetaGradients \\cite{derrowpinion2021mwn}.\n    *   The models are region-specific, with data constructed for each region to build region-specific GNN models \\cite{derrowpinion2021mwn}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by demonstrating the first large-scale, real-world deployment of a GNN for ETA prediction in a critical, user-facing application like Google Maps \\cite{derrowpinion2021mwn}.\n    *   It provides a blueprint for constructing powerful and stable GNN estimators for complex spatiotemporal tasks, offering insights into featurization, model design, and robust training regimes (e.g., MetaGradients) essential for production readiness \\cite{derrowpinion2021mwn}.\n    *   The observed substantial reductions in ETA errors (e.g., 40%+ in Sydney) highlight the practical efficacy and superiority of graph-aware models over traditional approaches, paving the way for future research in dynamic graph representation learning and more sophisticated spatiotemporal forecasting in transportation networks \\cite{derrowpinion2021mwn}.",
      "keywords": [
        "ETA prediction",
        "Graph Neural Networks (GNNs)",
        "Large-scale transportation networks",
        "Google Maps",
        "Supersegments",
        "MetaGradients",
        "Semi-supervised training",
        "Sophisticated featurization",
        "Production deployment",
        "Negative ETA outcomes reduction",
        "Spatiotemporal traffic forecasting",
        "Graph Network framework",
        "Learnable embeddings"
      ],
      "paper_type": "the paper should be classified as **case_study**.\n\nhere's the reasoning:\n\n1.  **strong emphasis on a specific application:** the title itself, \"eta prediction with graph neural networks **in google maps**,\" immediately highlights the specific context. the abstract repeatedly mentions \"deployed in production at google maps,\" \"real-world situations,\" and specific results for \"cities like sydney.\" the introduction also features \"google maps\" prominently and a figure showing \"google maps estimated time-of-arrival (eta) prediction improvements.\" this focus on a particular, real-world system and its deployment is a hallmark of a case study.\n\n2.  **analysis of practical experience:** the abstract details \"prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge.\" this indicates a detailed analysis of the system's performance and behavior within its specific operational environment.\n\n3.  **deployment and impact:** the statement \"our gnn proved powerful when deployed, significantly reducing negative eta outcomes in several regions compared to the previous production baseline (40+% in cities like sydney)\" directly speaks to the practical impact and experience of using the system in a real-world setting.\n\nwhile the paper also presents a \"graph neural network estimator\" and \"details the usage of training schedule methods such as meta-gradients\" (elements of a **technical** paper) and provides quantitative results and comparisons (elements of an **empirical** paper), the overarching narrative and the most prominent distinguishing feature is the detailed analysis of its application and performance within the specific context of google maps' production environment. the abstract even notes that the \"main architecture consists of standard gnn building blocks,\" suggesting the novelty is less in the fundamental gnn itself and more in its successful adaptation, robustification, and deployment at scale for this specific application. a case study often integrates technical descriptions and empirical evaluations within its specific application context."
    },
    "file_name": "5822490cf59df7f7ccb92b8901f244850b867a66.pdf"
  },
  {
    "success": true,
    "doc_id": "d089fc97e249ebbc2e9e320ac17ee3fa",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are powerful for learning node embeddings but rely on the availability and quality of graph-structured data.\n    *   The intrinsic graph structures in real-world applications are often not optimal for downstream tasks because:\n        *   Raw graphs are constructed from original feature spaces, which may not reflect the \"true\" topology after feature transformation.\n        *   Real-world graphs can be noisy or incomplete due to data measurement/collection errors.\n        *   Many applications (e.g., NLP) only have sequential or feature matrix data, requiring graph construction from scratch.\n    *   The paper addresses the challenge of learning a better graph structure that is optimized for downstream prediction tasks, especially when the initial graph is noisy, incomplete, or unavailable.\n\n*   **Related Work & Positioning**\n    *   Existing GNN methods typically assume a perfect input graph topology, which is often not true `\\cite{chen2020bvl}`.\n    *   Some works reweight neighborhood importance using self-attention but still assume noiseless original graph connectivity `\\cite{chen2020bvl}`.\n    *   Previous graph learning methods that optimize an adjacency matrix (e.g., `[25]`) often do so without explicitly considering the downstream task `\\cite{chen2020bvl}`.\n    *   Methods modeling graph learning as learning a joint discrete probability distribution on edges (e.g., `[15]`) struggle with inductive settings (new nodes at test time) because they assume known graph nodes `\\cite{chen2020bvl}`.\n    *   `\\cite{chen2020bvl}` positions itself by proposing an end-to-end, iterative framework that jointly learns graph structure and GNN parameters, explicitly optimizing for downstream tasks, and handling inductive settings.\n\n*   **Technical Approach & Innovation**\n    *   **Iterative Deep Graph Learning (IDGL)**: An end-to-end framework that jointly and iteratively learns graph structure and GNN parameters. The core idea is to learn a better graph structure based on better node embeddings, and vice versa.\n    *   **Dynamic Stopping Criterion**: The iterative process dynamically stops when the learned graph structure approaches close enough to the graph optimized for the downstream prediction task, rather than using a fixed number of iterations.\n    *   **Graph Learning as Similarity Metric Learning**: The problem of graph structure learning is cast as a similarity metric learning problem, allowing for handling inductive settings.\n        *   Uses a multi-head weighted cosine similarity function to compute similarity scores between nodes (or node-anchor pairs).\n        *   Applies epsilon-neighborhood sparsification to extract a symmetric sparse non-negative adjacency matrix.\n    *   **Adaptive Graph Regularization**: Incorporates a graph regularization loss (`L_G`) that controls the smoothness, connectivity, and sparsity of the learned graph. This loss combines Dirichlet energy (for smoothness) with terms penalizing disconnected graphs (logarithmic barrier) and large degrees (Frobenius norm).\n    *   **Hybrid Loss Function**: Minimizes a hybrid loss `L = L_pred + L_G`, combining the task-specific prediction loss (`L_pred`) and the graph regularization loss (`L_G`).\n    *   **Graph Combination**: The final graph structure `rA` is a linear combination of the initial graph `A^(0)` and the learned graph `A^(t)` (or `A^(1)` from raw features), allowing the learned structure to augment the initial one.\n    *   **Scalable Version (IDGL-ANCH)**: To address `O(n^2)` complexity for large graphs, an anchor-based approximation technique is introduced.\n        *   Learns a node-anchor affinity matrix `R` (size `n x s`, where `s` is the number of anchors, `s << n`).\n        *   Recovers node-to-node message passing by decomposing it into node-to-anchor and anchor-to-node message passing, reducing complexity to `O(ns)`.\n        *   Applies graph regularization to the *anchor graph* (derived from `R`) to control the quality of the learned node-anchor affinity.\n\n*   **Key Technical Contributions**\n    *   Proposed IDGL, the first framework to introduce iterative learning for joint graph structure and embedding learning, with a dynamic stopping criterion `\\cite{chen2020bvl}`.\n    *   Formulated graph learning as a similarity metric learning problem, enabling end-to-end optimization with downstream tasks and inductive capabilities `\\cite{chen2020bvl}`.\n    *   Developed IDGL-ANCH, a scalable version achieving linear complexity `O(ns)` in both time and memory by leveraging anchor-based approximation and node-anchor message passing `\\cite{chen2020bvl}`.\n    *   Introduced a comprehensive graph regularization strategy that controls smoothness, connectivity, and sparsity of the learned graph, applicable to both full and anchor-based graphs `\\cite{chen2020bvl}`.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments on nine benchmarks for node classification tasks.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   IDGL models consistently outperform or match state-of-the-art baselines across various downstream tasks `\\cite{chen2020bvl}`.\n        *   Demonstrated improved robustness to adversarial graph examples `\\cite{chen2020bvl}`.\n        *   Showed capability to cope with both transductive and inductive learning settings `\\cite{chen2020bvl}`.\n        *   IDGL-ANCH achieved significant reductions in time and space complexity without compromising performance `\\cite{chen2020bvl}`.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The current formulation assumes that graph noise is only from graph topology (adjacency matrix), and the node feature matrix `X` is noiseless `\\cite{chen2020bvl}`.\n    *   **Scope of Applicability**: Primarily demonstrated for node-level prediction tasks (node classification). While the problem formulation mentions graph-level tasks, the main experimental validation is on node classification. The framework is agnostic to specific GNN architectures, using a 2-layer GCN as an example.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art by providing a robust and effective solution for learning optimal graph structures for GNNs, especially in scenarios with noisy, incomplete, or absent initial graphs `\\cite{chen2020bvl}`.\n    *   The iterative joint learning paradigm and dynamic stopping criterion offer a novel approach to graph structure optimization.\n    *   The scalability of IDGL-ANCH makes deep graph learning applicable to much larger real-world graphs, overcoming a significant practical bottleneck `\\cite{chen2020bvl}`.\n    *   The framework's ability to handle inductive settings and adversarial graphs enhances the practical utility and reliability of GNNs `\\cite{chen2020bvl}`.\n    *   Potential impact on future research includes exploring graph learning when both topology and node features are noisy, and extending the framework to other graph-level tasks or more complex GNN architectures.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized representation learning, yet their efficacy critically hinges on the quality and availability of the underlying graph structure. Real-world graphs are often noisy, incomplete, or entirely absent, posing a significant bottleneck for GNN deployment. We introduce Iterative Deep Graph Learning (IDGL), a novel, end-to-end framework that jointly and iteratively optimizes graph structure and GNN parameters for downstream prediction tasks.\n\nUnlike prior methods, IDGL casts graph learning as a similarity metric learning problem, enabling robust performance in inductive settings and employing a dynamic stopping criterion based on task-specific convergence. A comprehensive adaptive graph regularization strategy ensures learned graphs are smooth, connected, and sparse. Crucially, for large-scale applications, we propose IDGL-ANCH, an anchor-based approximation that achieves linear time and memory complexity ($O(ns)$) by decomposing node-to-node message passing into node-to-anchor interactions. Extensive experiments on node classification benchmarks demonstrate IDGL consistently outperforms state-of-the-art baselines, exhibiting superior robustness to adversarial graphs and seamless adaptation to both transductive and inductive learning scenarios. This work significantly advances the practical utility and scalability of GNNs, making them more adaptable for real-world challenges where optimal graph structures are elusive.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "graph structure learning",
      "Iterative Deep Graph Learning (IDGL)",
      "joint graph structure and embedding learning",
      "dynamic stopping criterion",
      "similarity metric learning",
      "inductive learning",
      "adaptive graph regularization",
      "scalable graph learning",
      "IDGL-ANCH",
      "anchor-based approximation",
      "node classification",
      "robustness to adversarial graphs",
      "optimal graph structures"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf",
    "citation_key": "chen2020bvl",
    "metadata": {
      "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
      "authors": [
        "Yu Chen",
        "Lingfei Wu",
        "Mohammed J. Zaki"
      ],
      "published_date": "2020",
      "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
      "file_path": "paper_data/Graph_Neural_Networks/ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are powerful for learning node embeddings but rely on the availability and quality of graph-structured data.\n    *   The intrinsic graph structures in real-world applications are often not optimal for downstream tasks because:\n        *   Raw graphs are constructed from original feature spaces, which may not reflect the \"true\" topology after feature transformation.\n        *   Real-world graphs can be noisy or incomplete due to data measurement/collection errors.\n        *   Many applications (e.g., NLP) only have sequential or feature matrix data, requiring graph construction from scratch.\n    *   The paper addresses the challenge of learning a better graph structure that is optimized for downstream prediction tasks, especially when the initial graph is noisy, incomplete, or unavailable.\n\n*   **Related Work & Positioning**\n    *   Existing GNN methods typically assume a perfect input graph topology, which is often not true `\\cite{chen2020bvl}`.\n    *   Some works reweight neighborhood importance using self-attention but still assume noiseless original graph connectivity `\\cite{chen2020bvl}`.\n    *   Previous graph learning methods that optimize an adjacency matrix (e.g., `[25]`) often do so without explicitly considering the downstream task `\\cite{chen2020bvl}`.\n    *   Methods modeling graph learning as learning a joint discrete probability distribution on edges (e.g., `[15]`) struggle with inductive settings (new nodes at test time) because they assume known graph nodes `\\cite{chen2020bvl}`.\n    *   `\\cite{chen2020bvl}` positions itself by proposing an end-to-end, iterative framework that jointly learns graph structure and GNN parameters, explicitly optimizing for downstream tasks, and handling inductive settings.\n\n*   **Technical Approach & Innovation**\n    *   **Iterative Deep Graph Learning (IDGL)**: An end-to-end framework that jointly and iteratively learns graph structure and GNN parameters. The core idea is to learn a better graph structure based on better node embeddings, and vice versa.\n    *   **Dynamic Stopping Criterion**: The iterative process dynamically stops when the learned graph structure approaches close enough to the graph optimized for the downstream prediction task, rather than using a fixed number of iterations.\n    *   **Graph Learning as Similarity Metric Learning**: The problem of graph structure learning is cast as a similarity metric learning problem, allowing for handling inductive settings.\n        *   Uses a multi-head weighted cosine similarity function to compute similarity scores between nodes (or node-anchor pairs).\n        *   Applies epsilon-neighborhood sparsification to extract a symmetric sparse non-negative adjacency matrix.\n    *   **Adaptive Graph Regularization**: Incorporates a graph regularization loss (`L_G`) that controls the smoothness, connectivity, and sparsity of the learned graph. This loss combines Dirichlet energy (for smoothness) with terms penalizing disconnected graphs (logarithmic barrier) and large degrees (Frobenius norm).\n    *   **Hybrid Loss Function**: Minimizes a hybrid loss `L = L_pred + L_G`, combining the task-specific prediction loss (`L_pred`) and the graph regularization loss (`L_G`).\n    *   **Graph Combination**: The final graph structure `rA` is a linear combination of the initial graph `A^(0)` and the learned graph `A^(t)` (or `A^(1)` from raw features), allowing the learned structure to augment the initial one.\n    *   **Scalable Version (IDGL-ANCH)**: To address `O(n^2)` complexity for large graphs, an anchor-based approximation technique is introduced.\n        *   Learns a node-anchor affinity matrix `R` (size `n x s`, where `s` is the number of anchors, `s << n`).\n        *   Recovers node-to-node message passing by decomposing it into node-to-anchor and anchor-to-node message passing, reducing complexity to `O(ns)`.\n        *   Applies graph regularization to the *anchor graph* (derived from `R`) to control the quality of the learned node-anchor affinity.\n\n*   **Key Technical Contributions**\n    *   Proposed IDGL, the first framework to introduce iterative learning for joint graph structure and embedding learning, with a dynamic stopping criterion `\\cite{chen2020bvl}`.\n    *   Formulated graph learning as a similarity metric learning problem, enabling end-to-end optimization with downstream tasks and inductive capabilities `\\cite{chen2020bvl}`.\n    *   Developed IDGL-ANCH, a scalable version achieving linear complexity `O(ns)` in both time and memory by leveraging anchor-based approximation and node-anchor message passing `\\cite{chen2020bvl}`.\n    *   Introduced a comprehensive graph regularization strategy that controls smoothness, connectivity, and sparsity of the learned graph, applicable to both full and anchor-based graphs `\\cite{chen2020bvl}`.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments on nine benchmarks for node classification tasks.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   IDGL models consistently outperform or match state-of-the-art baselines across various downstream tasks `\\cite{chen2020bvl}`.\n        *   Demonstrated improved robustness to adversarial graph examples `\\cite{chen2020bvl}`.\n        *   Showed capability to cope with both transductive and inductive learning settings `\\cite{chen2020bvl}`.\n        *   IDGL-ANCH achieved significant reductions in time and space complexity without compromising performance `\\cite{chen2020bvl}`.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The current formulation assumes that graph noise is only from graph topology (adjacency matrix), and the node feature matrix `X` is noiseless `\\cite{chen2020bvl}`.\n    *   **Scope of Applicability**: Primarily demonstrated for node-level prediction tasks (node classification). While the problem formulation mentions graph-level tasks, the main experimental validation is on node classification. The framework is agnostic to specific GNN architectures, using a 2-layer GCN as an example.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art by providing a robust and effective solution for learning optimal graph structures for GNNs, especially in scenarios with noisy, incomplete, or absent initial graphs `\\cite{chen2020bvl}`.\n    *   The iterative joint learning paradigm and dynamic stopping criterion offer a novel approach to graph structure optimization.\n    *   The scalability of IDGL-ANCH makes deep graph learning applicable to much larger real-world graphs, overcoming a significant practical bottleneck `\\cite{chen2020bvl}`.\n    *   The framework's ability to handle inductive settings and adversarial graphs enhances the practical utility and reliability of GNNs `\\cite{chen2020bvl}`.\n    *   Potential impact on future research includes exploring graph learning when both topology and node features are noisy, and extending the framework to other graph-level tasks or more complex GNN architectures.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "graph structure learning",
        "Iterative Deep Graph Learning (IDGL)",
        "joint graph structure and embedding learning",
        "dynamic stopping criterion",
        "similarity metric learning",
        "inductive learning",
        "adaptive graph regularization",
        "scalable graph learning",
        "IDGL-ANCH",
        "anchor-based approximation",
        "node classification",
        "robustness to adversarial graphs",
        "optimal graph structures"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose** an end-to-end graph learning framework, namely iterative deepgraph learning (idgl)\". it then details the \"key rationale,\" \"iterative method,\" \"adaptive graph regularization,\" and a \"scalable version of idgl.\"\n*   the introduction identifies limitations of existing gnns and states: \"to address these limitations, we **propose** an end-to-end graph learning framework, namely iterative deepgraph learning (idgl), for jointly and iteratively learning the graph structure and the gnn parameters...\"\n*   the paper describes a new system/framework (idgl) and its components, and then validates it with experiments.\n\nthis aligns perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and uses keywords like \"propose,\" \"develop,\" \"present,\" \"algorithm,\" and \"method.\" while it includes empirical results, these results are presented to validate the proposed technical framework, making the core contribution technical.\n\n**classification: technical**"
    },
    "file_name": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf"
  },
  {
    "success": true,
    "doc_id": "2a29343eeb2fb26aacdf1fa40798bd85",
    "summary": "Here's a focused summary of the paper \\cite{zeng2022jhz} for a literature review:\n\n### Technical Paper Analysis: Decoupling the Depth and Scope of Graph Neural Networks \\cite{zeng2022jhz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: State-of-the-art Graph Neural Networks (GNNs) face significant scalability limitations with increasing graph and model sizes, particularly when increasing model depth.\n    *   **Importance and Challenge**:\n        *   **Expressivity Challenge (Oversmoothing)**: Deep GNNs iteratively mix neighbor features, causing embedding vectors of different nodes to collapse into a low-dimensional subspace, leading to degraded expressivity and loss of local information.\n        *   **Scalability Challenge (Neighbor Explosion)**: Recursive expansion of multi-hop neighborhoods results in an exponentially growing receptive field size, leading to prohibitive computation and hardware costs on large graphs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Expressivity**: Most remedies focus on neural architecture exploration, such as proposing more expressive aggregation functions \\cite{44, 12, 49, 29} or using residue-style designs like skip-connections \\cite{50, 28, 18} or multi-hop message passing within a single layer \\cite{34, 1, 33, 31}.\n        *   **Scalability**: Sampling methods (e.g., importance-based layer-wise sampling \\cite{8, 7, 61}, subgraph-based sampling \\cite{54, 9, 55}) are used to improve training speed and efficiency.\n    *   **Limitations of Previous Solutions**:\n        *   These solutions are often partial and do not address the root cause.\n        *   Existing approaches typically take a \"global view\" of the graph, where the GNN's depth (number of layers) and scope (receptive field) are tightly coupled. This coupling limits design space exploration.\n        *   Sampling methods for scalability often cannot be naturally generalized to inference without accuracy loss.\n        *   Deep GNNs, even with architectural tricks, still suffer from accuracy drops, indicating that the GCN-style propagation itself might be problematic.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel design principle to **decouple the depth and scope of GNNs**.\n        *   For a target entity (node or edge), a **localized subgraph** `G[v]` is first extracted as a bounded-size scope using an `EXTRACT` function.\n        *   Then, a GNN of **arbitrary depth** (`L'`) is applied *only* on this extracted subgraph `G[v]`, treating it as the new full graph.\n        *   The key is that the GNN depth `L'` can be significantly greater than the subgraph's depth `L` (i.e., `L' > L`).\n    *   **Novelty/Difference**:\n        *   This approach introduces a new dimension in GNN design space by treating scope extraction and GNN depth as independently tuned parameters.\n        *   It shifts from a \"global view\" to a \"local view\" of the graph, where the GNN smooths the local neighborhood into informative representations rather than oversmoothing the global graph.\n        *   The practical implementation, **SHADOW-GNN (Decoupled GNN on a SHAllow subgraDh)**, uses shallow yet informative subgraphs (e.g., 2- or 3-hop neighbors) and applies deeper GNNs (e.g., `L'=5`) on them.\n\n4.  **Key Technical Contributions**\n    *   **Novel Design Principle**: Introduction of the \"decoupling depth and scope\" principle for GNNs, which fundamentally rethinks how GNNs process information on large graphs \\cite{zeng2022jhz}.\n    *   **Theoretical Insights**:\n        *   **Graph Signal Processing (SHADOW-GCN)**: Proves that decoupled-GCN performs \"local-smoothing\" rather than oversmoothing. `Proposition 3.1` shows that aggregation converges to a linear combination of local features `X[v]`, preserving local feature and structural information, unlike normal GCNs which oversmooth to a single point. `Theorem 3.2` and its corollaries formally demonstrate that SHADOW-GCN does not oversmooth, ensuring distinct aggregations for nodes with different subgraphs.\n        *   **Function Approximation (SHADOW-SAGE)**: Shows that SHADOW-SAGE is more expressive than GraphSAGE. `Theorem 3.3` demonstrates that SHADOW-SAGE can approximate certain target functions (which GraphSAGE cannot accurately learn) with error decaying exponentially with depth.\n        *   **Topological Learning (SHADOW-GIN)**: Argues that decoupling improves the discriminativeness of GNNs beyond the 1-Dimensional Weisfeiler-Lehman (1-WL) test. By operating on non-regular subgraphs of a regular graph, SHADOW-GIN can break symmetries that 1-WL (and normal GNNs) fail to distinguish.\n    *   **System Design/Architectural Innovations**:\n        *   **SHADOW-GNN Framework**: A practical implementation that integrates subgraph extraction functions and deep GNNs.\n        *   **Subgraph Extraction Functions**: Proposes various methods for efficient construction of shallow, informative subgraphs on commodity hardware.\n        *   **Neural Architecture Extensions**: Introduces pooling and ensemble techniques to better utilize subgraph node embeddings after deep message passing.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated SHADOW-GNNs on two graph learning tasks (node classification and link prediction) across seven benchmark graphs, including very large ones (up to `ogbn-papers100M` with 111M nodes).\n    *   **Backbone Architectures**: Tested with six different backbone GNN architectures.\n    *   **Key Performance Metrics & Results**:\n        *   **Accuracy Improvement**: Achieved significant accuracy gains compared to the original GNN models.\n        *   **Resource Reduction**: Demonstrated orders of magnitude reduction in computation and hardware costs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The theoretical analysis for GCN primarily ignores non-linear activation and bias parameters to focus on the aggregation matrix's asymptotic behavior, consistent with prior oversmoothing literature.\n        *   The effectiveness heavily relies on the `EXTRACT` function's ability to identify \"critical neighbors\" and exclude \"irrelevant ones\" within the bounded scope.\n    *   **Scope of Applicability**:\n        *   Primarily demonstrated for node classification and link prediction tasks.\n        *   Applicable to large-scale graphs where traditional deep GNNs suffer from oversmoothing and neighbor explosion.\n        *   The principle is general and can be applied to various GNN architectures (GCN, GraphSAGE, GIN, etc.).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   Provides a fundamental design principle that resolves the long-standing trade-off between GNN depth (expressivity) and scope (scalability) \\cite{zeng2022jhz}.\n        *   Offers a principled way to build deeper and more expressive GNNs without incurring prohibitive computational costs or oversmoothing.\n        *   Theoretically enhances the expressive power of GNNs from multiple perspectives (graph signal processing, function approximation, topological learning), surpassing limitations of prior models like 1-WL.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for designing highly scalable and expressive GNNs for extremely large graphs.\n        *   Encourages further research into optimal subgraph extraction strategies and how to best leverage deep GNNs on localized scopes.\n        *   Could lead to the development of more robust and efficient GNNs for real-world applications in social networks, knowledge graphs, and drug discovery.",
    "intriguing_abstract": "Deep Graph Neural Networks (GNNs) are powerful, yet their potential is crippled by a fundamental dilemma: increasing depth for enhanced expressivity inevitably leads to debilitating oversmoothing and prohibitive 'neighbor explosion' on large-scale graphs. We introduce a paradigm-shifting solution: **decoupling the depth and scope of GNNs.**\n\nOur novel framework, **SHADOW-GNN (Decoupled GNN on a SHAllow subgraDh)**, redefines GNN design. Instead of a global graph view, it first extracts a shallow, localized subgraph for each target entity. Critically, a GNN of *arbitrary depth* is then applied *only* within this bounded scope. This approach fundamentally resolves the long-standing depth-scalability trade-off. Theoretically, we prove SHADOW-GNN performs 'local-smoothing,' preventing oversmoothing and significantly enhancing expressivity beyond GraphSAGE and the 1-Dimensional Weisfeiler-Lehman (1-WL) test. Empirically, SHADOW-GNN achieves significant accuracy gains on node classification and link prediction across diverse benchmarks, including `ogbn-papers100M`, while reducing computation and hardware costs by orders of magnitude. By enabling truly deep and expressive GNNs for massive graphs, SHADOW-GNN opens unprecedented avenues for scalable graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "oversmoothing",
      "neighbor explosion",
      "decoupling depth and scope",
      "localized subgraph",
      "SHADOW-GNN",
      "subgraph extraction",
      "local-smoothing",
      "GNN expressivity",
      "1-Dimensional Weisfeiler-Lehman test",
      "scalability",
      "node classification",
      "link prediction",
      "large-scale graphs",
      "resource reduction"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/b4895de425a02af87713bd78ed1a29fe425753af.pdf",
    "citation_key": "zeng2022jhz",
    "metadata": {
      "title": "Decoupling the Depth and Scope of Graph Neural Networks",
      "authors": [
        "Hanqing Zeng",
        "Muhan Zhang",
        "Yinglong Xia",
        "Ajitesh Srivastava",
        "Andrey Malevich",
        "R. Kannan",
        "V. Prasanna",
        "Long Jin",
        "Ren Chen"
      ],
      "published_date": "2022",
      "abstract": "State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -- to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into\"white noise\". Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.",
      "file_path": "paper_data/Graph_Neural_Networks/b4895de425a02af87713bd78ed1a29fe425753af.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{zeng2022jhz} for a literature review:\n\n### Technical Paper Analysis: Decoupling the Depth and Scope of Graph Neural Networks \\cite{zeng2022jhz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: State-of-the-art Graph Neural Networks (GNNs) face significant scalability limitations with increasing graph and model sizes, particularly when increasing model depth.\n    *   **Importance and Challenge**:\n        *   **Expressivity Challenge (Oversmoothing)**: Deep GNNs iteratively mix neighbor features, causing embedding vectors of different nodes to collapse into a low-dimensional subspace, leading to degraded expressivity and loss of local information.\n        *   **Scalability Challenge (Neighbor Explosion)**: Recursive expansion of multi-hop neighborhoods results in an exponentially growing receptive field size, leading to prohibitive computation and hardware costs on large graphs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Expressivity**: Most remedies focus on neural architecture exploration, such as proposing more expressive aggregation functions \\cite{44, 12, 49, 29} or using residue-style designs like skip-connections \\cite{50, 28, 18} or multi-hop message passing within a single layer \\cite{34, 1, 33, 31}.\n        *   **Scalability**: Sampling methods (e.g., importance-based layer-wise sampling \\cite{8, 7, 61}, subgraph-based sampling \\cite{54, 9, 55}) are used to improve training speed and efficiency.\n    *   **Limitations of Previous Solutions**:\n        *   These solutions are often partial and do not address the root cause.\n        *   Existing approaches typically take a \"global view\" of the graph, where the GNN's depth (number of layers) and scope (receptive field) are tightly coupled. This coupling limits design space exploration.\n        *   Sampling methods for scalability often cannot be naturally generalized to inference without accuracy loss.\n        *   Deep GNNs, even with architectural tricks, still suffer from accuracy drops, indicating that the GCN-style propagation itself might be problematic.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel design principle to **decouple the depth and scope of GNNs**.\n        *   For a target entity (node or edge), a **localized subgraph** `G[v]` is first extracted as a bounded-size scope using an `EXTRACT` function.\n        *   Then, a GNN of **arbitrary depth** (`L'`) is applied *only* on this extracted subgraph `G[v]`, treating it as the new full graph.\n        *   The key is that the GNN depth `L'` can be significantly greater than the subgraph's depth `L` (i.e., `L' > L`).\n    *   **Novelty/Difference**:\n        *   This approach introduces a new dimension in GNN design space by treating scope extraction and GNN depth as independently tuned parameters.\n        *   It shifts from a \"global view\" to a \"local view\" of the graph, where the GNN smooths the local neighborhood into informative representations rather than oversmoothing the global graph.\n        *   The practical implementation, **SHADOW-GNN (Decoupled GNN on a SHAllow subgraDh)**, uses shallow yet informative subgraphs (e.g., 2- or 3-hop neighbors) and applies deeper GNNs (e.g., `L'=5`) on them.\n\n4.  **Key Technical Contributions**\n    *   **Novel Design Principle**: Introduction of the \"decoupling depth and scope\" principle for GNNs, which fundamentally rethinks how GNNs process information on large graphs \\cite{zeng2022jhz}.\n    *   **Theoretical Insights**:\n        *   **Graph Signal Processing (SHADOW-GCN)**: Proves that decoupled-GCN performs \"local-smoothing\" rather than oversmoothing. `Proposition 3.1` shows that aggregation converges to a linear combination of local features `X[v]`, preserving local feature and structural information, unlike normal GCNs which oversmooth to a single point. `Theorem 3.2` and its corollaries formally demonstrate that SHADOW-GCN does not oversmooth, ensuring distinct aggregations for nodes with different subgraphs.\n        *   **Function Approximation (SHADOW-SAGE)**: Shows that SHADOW-SAGE is more expressive than GraphSAGE. `Theorem 3.3` demonstrates that SHADOW-SAGE can approximate certain target functions (which GraphSAGE cannot accurately learn) with error decaying exponentially with depth.\n        *   **Topological Learning (SHADOW-GIN)**: Argues that decoupling improves the discriminativeness of GNNs beyond the 1-Dimensional Weisfeiler-Lehman (1-WL) test. By operating on non-regular subgraphs of a regular graph, SHADOW-GIN can break symmetries that 1-WL (and normal GNNs) fail to distinguish.\n    *   **System Design/Architectural Innovations**:\n        *   **SHADOW-GNN Framework**: A practical implementation that integrates subgraph extraction functions and deep GNNs.\n        *   **Subgraph Extraction Functions**: Proposes various methods for efficient construction of shallow, informative subgraphs on commodity hardware.\n        *   **Neural Architecture Extensions**: Introduces pooling and ensemble techniques to better utilize subgraph node embeddings after deep message passing.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated SHADOW-GNNs on two graph learning tasks (node classification and link prediction) across seven benchmark graphs, including very large ones (up to `ogbn-papers100M` with 111M nodes).\n    *   **Backbone Architectures**: Tested with six different backbone GNN architectures.\n    *   **Key Performance Metrics & Results**:\n        *   **Accuracy Improvement**: Achieved significant accuracy gains compared to the original GNN models.\n        *   **Resource Reduction**: Demonstrated orders of magnitude reduction in computation and hardware costs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The theoretical analysis for GCN primarily ignores non-linear activation and bias parameters to focus on the aggregation matrix's asymptotic behavior, consistent with prior oversmoothing literature.\n        *   The effectiveness heavily relies on the `EXTRACT` function's ability to identify \"critical neighbors\" and exclude \"irrelevant ones\" within the bounded scope.\n    *   **Scope of Applicability**:\n        *   Primarily demonstrated for node classification and link prediction tasks.\n        *   Applicable to large-scale graphs where traditional deep GNNs suffer from oversmoothing and neighbor explosion.\n        *   The principle is general and can be applied to various GNN architectures (GCN, GraphSAGE, GIN, etc.).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   Provides a fundamental design principle that resolves the long-standing trade-off between GNN depth (expressivity) and scope (scalability) \\cite{zeng2022jhz}.\n        *   Offers a principled way to build deeper and more expressive GNNs without incurring prohibitive computational costs or oversmoothing.\n        *   Theoretically enhances the expressive power of GNNs from multiple perspectives (graph signal processing, function approximation, topological learning), surpassing limitations of prior models like 1-WL.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for designing highly scalable and expressive GNNs for extremely large graphs.\n        *   Encourages further research into optimal subgraph extraction strategies and how to best leverage deep GNNs on localized scopes.\n        *   Could lead to the development of more robust and efficient GNNs for real-world applications in social networks, knowledge graphs, and drug discovery.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "oversmoothing",
        "neighbor explosion",
        "decoupling depth and scope",
        "localized subgraph",
        "SHADOW-GNN",
        "subgraph extraction",
        "local-smoothing",
        "GNN expressivity",
        "1-Dimensional Weisfeiler-Lehman test",
        "scalability",
        "node classification",
        "link prediction",
        "large-scale graphs",
        "resource reduction"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"**we propose a design principle to decouple the depth and scope of gnns** – to generate representation of a target entity..., we first extract a localized subgraph..., and then apply a gnn of arbitrary depth...\" this clearly indicates the presentation of a new method or system.\n*   the introduction discusses a technical problem (\"how to effectively scale up gnns,\" \"expressivity challenge,\" \"scalability challenge\") and sets the stage for a proposed solution.\n*   the theoretical and empirical aspects mentioned in the abstract are used to support and validate this proposed design principle, rather than being the sole focus of the paper.\n\ntherefore, this paper best fits the **technical** classification."
    },
    "file_name": "b4895de425a02af87713bd78ed1a29fe425753af.pdf"
  },
  {
    "success": true,
    "doc_id": "fcdb8d433a2f55cb3fa9aa663c555b2c",
    "summary": "Here's a focused summary of the technical paper \\cite{yuan20208v3} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) are powerful but operate as \"black-boxes,\" lacking human-intelligible explanations for their predictions.\n    *   **Importance & Challenge:**\n        *   **Trust and Adoption:** The lack of interpretability prevents GNNs from being fully trusted and deployed in critical application domains (e.g., fairness, privacy, safety, drug discovery) where understanding the underlying decision-making is crucial.\n        *   **Verification:** Without explanations, it's impossible to verify if GNNs' learned rules align with real-world domain knowledge (e.g., chemical rules for molecular graphs).\n        *   **Limitations of Existing Methods:** Prior GNN interpretation efforts are primarily \"example-level\" (explaining a single prediction), which is time-consuming and impractical for understanding the general behavior of an entire model. \"Model-level\" interpretations (explaining general patterns) are largely unexplored for GNNs.\n        *   **Graph Data Challenges:** Traditional model-level interpretation methods (like input optimization for images) cannot be directly applied to graphs due due to their discrete structural information (adjacency matrix), difficulty in visualizing abstract graph patterns, and the need to incorporate non-differentiable graph rules (e.g., chemical valency).\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Example-level GNN Interpretations:** Previous work like GNN Explainer \\cite{yuan20208v3} and methods applying sensitivity analysis (SA), guided backpropagation (GBP), and layer-wise relevance propagation (LRP) to graphs focus on explaining individual predictions by identifying important edges or node features for a given input.\n        *   **Model-level Interpretations (Non-Graph):** Input optimization methods \\cite{yuan20208v3} exist for image and text data, generating optimized inputs that maximize a certain model behavior.\n    *   **Limitations of Previous Solutions:**\n        *   **Example-level:** Requires checking explanations for numerous examples to understand the model's general behavior, which is time-consuming and often infeasible for humans.\n        *   **Model-level (Non-Graph):** Cannot be directly applied to GNNs due to the discrete nature of graph structures (non-differentiable adjacency matrix), the challenge of visualizing abstract graph patterns, and the necessity of incorporating non-differentiable domain-specific graph rules (e.g., chemical valency).\n    *   **Positioning:** \\cite{yuan20208v3} proposes XGNN as a novel approach to provide *model-level* interpretations for GNNs, addressing the gap in existing literature.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** XGNN interprets GNNs by training a **graph generator** to produce graph patterns that maximize a specific prediction score of the target GNN model. This reveals the general graph patterns that the GNN associates with a particular class.\n    *   **Formulation as Reinforcement Learning (RL):** The graph generation process is formulated as an RL problem to handle the non-differentiable nature of graph structure generation.\n        *   **State:** The partially generated graph at each step.\n        *   **Action:** Adding an edge to the current graph, either between existing nodes or by adding a new node from a candidate set and connecting it.\n        *   **Policy:** A GNN-based graph generator $\\partial_\\theta(\\cdot)$ that takes the current graph and candidate set as input and outputs probabilities for possible actions.\n        *   **Reward:** Composed of two parts:\n            1.  **GNN Guidance:** Feedback from the pre-trained GNN model, encouraging the generated graph to maximize the target class's prediction score.\n            2.  **Graph Validity Rules:** Rewards for adhering to domain-specific graph rules (e.g., no multiple edges, chemical valency limits for atoms) to ensure generated graphs are valid and human-intelligible.\n    *   **Innovation:**\n        *   **First Model-Level Interpretation for GNNs:** Addresses a critical gap in GNN interpretability.\n        *   **Graph Generation for Interpretability:** Leverages recent advances in graph generation, but specifically adapts it for the purpose of explaining GNNs.\n        *   **Reinforcement Learning for Discrete Graph Optimization:** Effectively tackles the challenge of optimizing discrete graph structures by formulating it as an RL problem.\n        *   **Incorporation of Graph Rules:** Integrates non-differentiable domain knowledge into the reward function, leading to more realistic and meaningful explanations.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Framework (XGNN):** A general framework for model-level interpretation of GNNs, which can be adapted with different graph generation methods.\n    *   **RL Formulation for Interpretability:** Pioneering the use of reinforcement learning to generate explanatory graph patterns for GNNs.\n    *   **Policy Network for Graph Generation:** Employing GNNs themselves as the policy network within the RL framework to guide graph construction.\n    *   **Hybrid Reward Function:** Designing a reward function that combines feedback from the target GNN (for maximizing prediction) with domain-specific graph rules (for validity and interpretability).\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated XGNN on both synthetic and real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Demonstrated that XGNN can successfully find \"desired graph patterns\" that maximize specific GNN predictions.\n        *   Showed that the generated graphs help in understanding and verifying the trained GNN models.\n        *   Indicated that the generated graph patterns can provide guidance on how to improve the trained GNNs.\n        *   The generated graphs are \"small-scale and less complex,\" reducing the cost of manual analysis compared to analyzing large datasets.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The specific graph generator implementation (adding one edge at a time) might be limited in generating highly complex or diverse graph structures efficiently.\n        *   The quality of explanations is dependent on the chosen graph rules and the effectiveness of the RL training.\n        *   The \"small-scale and less complex\" nature of generated graphs, while aiding manual analysis, might not capture all nuances of very complex patterns a GNN might learn.\n    *   **Scope of Applicability:** Primarily demonstrated for graph classification tasks. While the framework is general, its application to other GNN tasks (e.g., node classification, link prediction) would require adaptation.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** XGNN represents a significant advancement by being the first method to provide model-level explanations for GNNs, moving beyond example-level interpretations.\n    *   **Potential Impact on Future Research:**\n        *   **Increased Trust and Adoption:** Enhances the trustworthiness of GNNs, paving the way for their wider adoption in critical domains.\n        *   **Model Debugging and Improvement:** Provides a mechanism to understand GNNs' internal workings, enabling verification against domain knowledge and offering insights for model improvement.\n        *   **New Research Direction:** Opens up a new research avenue for GNN interpretability, particularly in leveraging generative models and reinforcement learning for explanation.\n        *   **Facilitates Human Understanding:** Generates concise, high-level, and valid graph patterns that are easier for humans to analyze and comprehend.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are transforming data science, yet their inherent \"black-box\" nature severely impedes trust and widespread adoption in critical domains like drug discovery and safety. Existing interpretability methods primarily offer example-level explanations, failing to reveal the general decision-making logic of an entire GNN model. We introduce XGNN, the *first framework* to provide **model-level interpretations** for GNNs, fundamentally advancing the field.\n\nXGNN tackles the challenge of discrete graph structures by formulating graph pattern discovery as a **reinforcement learning (RL)** problem. It trains a **graph generator** to synthesize canonical **graph patterns** that maximally activate specific GNN predictions. Crucially, our novel hybrid reward function integrates both GNN guidance and non-differentiable **domain knowledge** (e.g., chemical valency), ensuring generated explanations are not only predictive but also valid and human-intelligible. This groundbreaking approach enables researchers to verify GNNs against real-world rules, debug model biases, and gain unprecedented insights into their learned representations, paving the way for more trustworthy and impactful GNN applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN interpretability",
      "model-level interpretations",
      "XGNN framework",
      "graph generator",
      "Reinforcement Learning (RL)",
      "discrete graph optimization",
      "hybrid reward function",
      "domain-specific graph rules",
      "trust and adoption of GNNs",
      "model debugging and improvement",
      "human-intelligible explanations"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf",
    "citation_key": "yuan20208v3",
    "metadata": {
      "title": "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
      "authors": [
        "Haonan Yuan",
        "Jiliang Tang",
        "Xia Hu",
        "Shuiwang Ji"
      ],
      "published_date": "2020",
      "abstract": "Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{yuan20208v3} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) are powerful but operate as \"black-boxes,\" lacking human-intelligible explanations for their predictions.\n    *   **Importance & Challenge:**\n        *   **Trust and Adoption:** The lack of interpretability prevents GNNs from being fully trusted and deployed in critical application domains (e.g., fairness, privacy, safety, drug discovery) where understanding the underlying decision-making is crucial.\n        *   **Verification:** Without explanations, it's impossible to verify if GNNs' learned rules align with real-world domain knowledge (e.g., chemical rules for molecular graphs).\n        *   **Limitations of Existing Methods:** Prior GNN interpretation efforts are primarily \"example-level\" (explaining a single prediction), which is time-consuming and impractical for understanding the general behavior of an entire model. \"Model-level\" interpretations (explaining general patterns) are largely unexplored for GNNs.\n        *   **Graph Data Challenges:** Traditional model-level interpretation methods (like input optimization for images) cannot be directly applied to graphs due due to their discrete structural information (adjacency matrix), difficulty in visualizing abstract graph patterns, and the need to incorporate non-differentiable graph rules (e.g., chemical valency).\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Example-level GNN Interpretations:** Previous work like GNN Explainer \\cite{yuan20208v3} and methods applying sensitivity analysis (SA), guided backpropagation (GBP), and layer-wise relevance propagation (LRP) to graphs focus on explaining individual predictions by identifying important edges or node features for a given input.\n        *   **Model-level Interpretations (Non-Graph):** Input optimization methods \\cite{yuan20208v3} exist for image and text data, generating optimized inputs that maximize a certain model behavior.\n    *   **Limitations of Previous Solutions:**\n        *   **Example-level:** Requires checking explanations for numerous examples to understand the model's general behavior, which is time-consuming and often infeasible for humans.\n        *   **Model-level (Non-Graph):** Cannot be directly applied to GNNs due to the discrete nature of graph structures (non-differentiable adjacency matrix), the challenge of visualizing abstract graph patterns, and the necessity of incorporating non-differentiable domain-specific graph rules (e.g., chemical valency).\n    *   **Positioning:** \\cite{yuan20208v3} proposes XGNN as a novel approach to provide *model-level* interpretations for GNNs, addressing the gap in existing literature.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** XGNN interprets GNNs by training a **graph generator** to produce graph patterns that maximize a specific prediction score of the target GNN model. This reveals the general graph patterns that the GNN associates with a particular class.\n    *   **Formulation as Reinforcement Learning (RL):** The graph generation process is formulated as an RL problem to handle the non-differentiable nature of graph structure generation.\n        *   **State:** The partially generated graph at each step.\n        *   **Action:** Adding an edge to the current graph, either between existing nodes or by adding a new node from a candidate set and connecting it.\n        *   **Policy:** A GNN-based graph generator $\\partial_\\theta(\\cdot)$ that takes the current graph and candidate set as input and outputs probabilities for possible actions.\n        *   **Reward:** Composed of two parts:\n            1.  **GNN Guidance:** Feedback from the pre-trained GNN model, encouraging the generated graph to maximize the target class's prediction score.\n            2.  **Graph Validity Rules:** Rewards for adhering to domain-specific graph rules (e.g., no multiple edges, chemical valency limits for atoms) to ensure generated graphs are valid and human-intelligible.\n    *   **Innovation:**\n        *   **First Model-Level Interpretation for GNNs:** Addresses a critical gap in GNN interpretability.\n        *   **Graph Generation for Interpretability:** Leverages recent advances in graph generation, but specifically adapts it for the purpose of explaining GNNs.\n        *   **Reinforcement Learning for Discrete Graph Optimization:** Effectively tackles the challenge of optimizing discrete graph structures by formulating it as an RL problem.\n        *   **Incorporation of Graph Rules:** Integrates non-differentiable domain knowledge into the reward function, leading to more realistic and meaningful explanations.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Framework (XGNN):** A general framework for model-level interpretation of GNNs, which can be adapted with different graph generation methods.\n    *   **RL Formulation for Interpretability:** Pioneering the use of reinforcement learning to generate explanatory graph patterns for GNNs.\n    *   **Policy Network for Graph Generation:** Employing GNNs themselves as the policy network within the RL framework to guide graph construction.\n    *   **Hybrid Reward Function:** Designing a reward function that combines feedback from the target GNN (for maximizing prediction) with domain-specific graph rules (for validity and interpretability).\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated XGNN on both synthetic and real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Demonstrated that XGNN can successfully find \"desired graph patterns\" that maximize specific GNN predictions.\n        *   Showed that the generated graphs help in understanding and verifying the trained GNN models.\n        *   Indicated that the generated graph patterns can provide guidance on how to improve the trained GNNs.\n        *   The generated graphs are \"small-scale and less complex,\" reducing the cost of manual analysis compared to analyzing large datasets.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The specific graph generator implementation (adding one edge at a time) might be limited in generating highly complex or diverse graph structures efficiently.\n        *   The quality of explanations is dependent on the chosen graph rules and the effectiveness of the RL training.\n        *   The \"small-scale and less complex\" nature of generated graphs, while aiding manual analysis, might not capture all nuances of very complex patterns a GNN might learn.\n    *   **Scope of Applicability:** Primarily demonstrated for graph classification tasks. While the framework is general, its application to other GNN tasks (e.g., node classification, link prediction) would require adaptation.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** XGNN represents a significant advancement by being the first method to provide model-level explanations for GNNs, moving beyond example-level interpretations.\n    *   **Potential Impact on Future Research:**\n        *   **Increased Trust and Adoption:** Enhances the trustworthiness of GNNs, paving the way for their wider adoption in critical domains.\n        *   **Model Debugging and Improvement:** Provides a mechanism to understand GNNs' internal workings, enabling verification against domain knowledge and offering insights for model improvement.\n        *   **New Research Direction:** Opens up a new research avenue for GNN interpretability, particularly in leveraging generative models and reinforcement learning for explanation.\n        *   **Facilitates Human Understanding:** Generates concise, high-level, and valid graph patterns that are easier for humans to analyze and comprehend.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN interpretability",
        "model-level interpretations",
        "XGNN framework",
        "graph generator",
        "Reinforcement Learning (RL)",
        "discrete graph optimization",
        "hybrid reward function",
        "domain-specific graph rules",
        "trust and adoption of GNNs",
        "model debugging and improvement",
        "human-intelligible explanations"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\n**reasoning:**\n\n1.  **\"propose\", \"develop\", \"present\", \"algorithm\", \"method\"**: the abstract explicitly states, \"we **propose** a novel approach, known as xgnn, to interpret gnns at the model-level.\" it further details this by saying, \"we **propose** to explain gnns by training a graph generator,\" and describes the technical formulation: \"we formulate the graph generation as a reinforcement learning task,\" and \"the graph generator is trained via a policy gradient method.\" these phrases are strong indicators of presenting a new method or system.\n2.  **technical problem, proposed solution**: the introduction clearly identifies a technical problem: gnns are \"black-boxes and lack human-intelligible explanations,\" which prevents their use in critical applications. the paper then immediately moves to present xgnn as a solution to this problem.\n3.  **empirical component as validation**: while the abstract mentions \"experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained gnns,\" this empirical evaluation serves to validate the effectiveness of the *newly proposed technical method*, rather than being the primary focus of the paper itself (e.g., studying a phenomenon or testing a hypothesis with existing methods). the core contribution is the creation of xgnn."
    },
    "file_name": "75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf"
  },
  {
    "success": true,
    "doc_id": "bdbcd527e9dd9cea82554883db24668d",
    "summary": "Here's a focused summary of the technical paper \\cite{xie2021n52} for a literature review:\n\n---\n\n### Analysis of \"Self-Supervised Learning of Graph Neural Networks: A Uniﬁed Review\" \\cite{xie2021n52}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of applying deep learning models, particularly Graph Neural Networks (GNNs), when labeled data is scarce, expensive, or unavailable. Supervised training, which requires extensive labeled samples, becomes impractical in many real-world graph-related scenarios.\n    *   **Importance and Challenge:**\n        *   **Importance:** Self-supervised learning (SSL) offers a paradigm to leverage vast amounts of unlabeled graph data, enabling representation learning, pre-training for downstream tasks, or auxiliary learning to enhance supervised models. This is crucial for domains like molecular graphs where unlabeled data is abundant but labels are costly.\n        *   **Challenge:** Extending SSL's success from natural language and image domains to graph data is non-trivial due to the unique, non-Euclidean structure of graphs. Key challenges include effectively capturing both node attributes and structural topology, designing appropriate \"views\" for contrastive learning, selecting suitable graph encoders, and generating meaningful self-supervision labels for predictive models.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the general success of SSL in other domains (NLP, computer vision) and adapts these concepts to GNNs. It acknowledges that many SSL methods for GNNs are inspired by image-domain techniques (e.g., DGI, graph autoencoders).\n    *   **Limitations of Previous Solutions (and how this survey addresses them):**\n        *   Existing general SSL surveys (e.g., \\cite{liu2021self}) often have taxonomies mostly oriented by image-specific SSL methods, which may not fully apply or provide deep insights into graph-specific challenges (e.g., view generation for graphs, graph-specific predictive tasks).\n        *   Concurrent graph-specific SSL surveys (e.g., \\cite{you2020graph}) might offer broader coverage but may lack the unified theoretical grounding and timely insights provided by \\cite{xie2021n52}.\n        *   \\cite{xie2021n52} distinguishes itself by providing a *unified, graph-specific* review, theoretically grounding all contrastive methods in mutual information maximization, and offering a framework that clarifies similarities and differences among diverse methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm (of the survey):** The paper's core approach is a comprehensive, unified review and categorization of existing SSL methods for GNNs. It proposes two main categories:\n        *   **Contrastive Models:** These learn by discriminating between positive (similar) and negative (dissimilar) pairs of \"views\" generated from graph data. The paper unifies these methods under a framework where the objective is related to mutual information maximization, and differences arise from how views are generated (e.g., identical, subgraphs, structural/feature transformations) and the specific mutual information estimators used (e.g., Jensen-Shannon, InfoNCE).\n        *   **Predictive Models:** These are trained in a supervised fashion using self-generated labels. The paper categorizes these based on the nature of the self-generated labels, including graph reconstruction (e.g., autoencoders, autoregressive), property prediction (e.g., k-hop connectivity, contextual properties), self-training, and invariance regularization.\n    *   **Novelty/Difference of this Approach (as a survey):**\n        *   **Unified Frameworks:** It provides a novel, unified framework for both contrastive and predictive SSL methods for GNNs, highlighting commonalities and distinctions in their components (view generation, objective functions, label generation strategies).\n        *   **Mutual Information Perspective:** It theoretically grounds all contrastive methods in mutual information maximization, offering a fresh and coherent perspective.\n        *   **Graph-Specific Focus:** Unlike general SSL surveys, it specifically addresses the unique challenges and solutions for graph-structured data.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (of the survey):**\n        *   **Unified Contrastive Learning Framework:** A general framework for GNN contrastive learning, categorizing methods by view generation strategies (e.g., identical, subgraphs, structural/feature transformations) and mutual information estimators (e.g., Jensen-Shannon, InfoNCE).\n        *   **Unified Predictive Learning Categorization:** A systematic categorization of predictive SSL methods based on how self-generated labels are obtained (graph reconstruction, property prediction, self-training, invariance regularization).\n    *   **System Design or Architectural Innovations (of the survey):**\n        *   **Standardized Testbed:** Development of an open-source, standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics. This facilitates consistent empirical comparison and methodological development.\n    *   **Theoretical Insights or Analysis (of the survey):**\n        *   **Mutual Information as Unifying Principle:** Emphasizing mutual information maximization as the theoretical grounding for all contrastive learning methods, providing a deeper understanding of their objectives.\n        *   **Insights into Component Choices:** Providing insights to guide the choice of components within the unified frameworks (e.g., view generation strategies, objective functions, graph encoders).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted (by the survey):** As a review paper, \\cite{xie2021n52} does not present novel experimental results for new SSL methods. Instead, its primary \"validation\" contribution is:\n        *   **Development of a Standardized Testbed:** The authors developed and released a standardized testbed, including implementations of common baseline methods, datasets, and evaluation metrics. This resource is designed to *enable* future empirical comparisons and methodological development by the research community.\n    *   **Key Performance Metrics and Comparison Results (summarized from reviewed papers):** The survey summarizes common settings of SSL tasks (unsupervised representation learning, unsupervised pre-training, auxiliary learning) and commonly used datasets across various categories. It implicitly covers the performance metrics and comparison results reported in the *reviewed* literature by discussing the different methods and their underlying principles, setting the stage for how such comparisons *should* be conducted using their testbed.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):**\n        *   **Focus on GNNs:** The review is specifically tailored to GNNs, meaning some general SSL concepts or methods from other domains that don't directly translate to graphs are not discussed in detail.\n        *   **Exclusion of Unsupervised Domain Adaptation:** The paper explicitly states it does not discuss unsupervised domain adaptation in detail, as this paradigm is not yet specifically studied in the graph domain.\n        *   **Timeliness vs. Exhaustive Coverage:** While aiming for an up-to-date review, it prioritizes a unified framework and timely insights over exhaustive coverage of every single existing SSL method for GNNs (as acknowledged in comparison to \\cite{you2020graph}).\n    *   **Scope of Applicability:** The review primarily covers SSL methods for static, attributed, undirected graphs. While it mentions discussing heterogeneous and dynamic graphs in an appendix, the core frameworks are presented for the more common graph types.\n\n7.  **Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:**\n        *   **Clarification and Unification:** By providing unified frameworks and a mutual information perspective, \\cite{xie2021n52} significantly clarifies the landscape of SSL for GNNs, making it easier for researchers to understand the relationships between diverse methods.\n        *   **Foundation for New Methods:** The structured categorization and insights into component choices (e.g., view generation, objective functions) provide a strong foundation and inspiration for developing novel and more effective SSL algorithms for GNNs.\n        *   **Facilitating Reproducibility and Comparison:** The development of a standardized testbed is a crucial contribution for the field, promoting reproducibility, fair empirical comparisons, and accelerating research progress.\n    *   **Potential Impact on Future Research:**\n        *   **Accelerated Development:** The unified frameworks and testbed will likely accelerate the development of new SSL methods for GNNs by providing clear guidelines and a common platform for evaluation.\n        *   **Improved Understanding:** Researchers will gain a deeper theoretical and practical understanding of SSL principles applied to graphs, leading to more principled method design.\n        *   **Broader Adoption:** By making SSL for GNNs more accessible and understandable, the paper can encourage broader adoption of these techniques in various graph-related applications where labeled data is a bottleneck.\n\n---",
    "intriguing_abstract": "The pervasive challenge of labeled data scarcity severely limits the application of Graph Neural Networks (GNNs) in critical domains. Self-supervised learning (SSL) offers a powerful paradigm to unlock the potential of vast unlabeled graph data, yet its extension to complex non-Euclidean graph structures presents unique hurdles. This paper provides a timely and comprehensive review, offering a novel, unified framework that demystifies the diverse landscape of SSL for GNNs.\n\nWe systematically categorize existing methods into two core paradigms: **contrastive learning**, theoretically grounded in mutual information maximization, and **predictive learning**, driven by self-generated labels. Our framework clarifies the intricate interplay of view generation strategies, objective functions, and graph encoders. Beyond theoretical unification, we introduce a crucial open-source, standardized testbed, complete with baseline implementations and datasets, to foster reproducibility and accelerate empirical comparisons. This work not only illuminates the foundational principles of graph representation learning but also provides indispensable tools and insights, paving the way for principled development of next-generation SSL algorithms for GNNs and their broader impact across science and industry.",
    "keywords": [
      "Self-supervised learning (SSL)",
      "Graph Neural Networks (GNNs)",
      "Representation learning",
      "Unlabeled graph data",
      "Mutual information maximization",
      "Contrastive learning",
      "Predictive models",
      "Unified framework",
      "View generation",
      "Standardized testbed",
      "Graph reconstruction",
      "Molecular graphs",
      "Non-Euclidean structure"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf",
    "citation_key": "xie2021n52",
    "metadata": {
      "title": "Self-Supervised Learning of Graph Neural Networks: A Unified Review",
      "authors": [
        "Yaochen Xie",
        "Zhao Xu",
        "Zhengyang Wang",
        "Shuiwang Ji"
      ],
      "published_date": "2021",
      "abstract": "Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.",
      "file_path": "paper_data/Graph_Neural_Networks/8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{xie2021n52} for a literature review:\n\n---\n\n### Analysis of \"Self-Supervised Learning of Graph Neural Networks: A Uniﬁed Review\" \\cite{xie2021n52}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of applying deep learning models, particularly Graph Neural Networks (GNNs), when labeled data is scarce, expensive, or unavailable. Supervised training, which requires extensive labeled samples, becomes impractical in many real-world graph-related scenarios.\n    *   **Importance and Challenge:**\n        *   **Importance:** Self-supervised learning (SSL) offers a paradigm to leverage vast amounts of unlabeled graph data, enabling representation learning, pre-training for downstream tasks, or auxiliary learning to enhance supervised models. This is crucial for domains like molecular graphs where unlabeled data is abundant but labels are costly.\n        *   **Challenge:** Extending SSL's success from natural language and image domains to graph data is non-trivial due to the unique, non-Euclidean structure of graphs. Key challenges include effectively capturing both node attributes and structural topology, designing appropriate \"views\" for contrastive learning, selecting suitable graph encoders, and generating meaningful self-supervision labels for predictive models.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the general success of SSL in other domains (NLP, computer vision) and adapts these concepts to GNNs. It acknowledges that many SSL methods for GNNs are inspired by image-domain techniques (e.g., DGI, graph autoencoders).\n    *   **Limitations of Previous Solutions (and how this survey addresses them):**\n        *   Existing general SSL surveys (e.g., \\cite{liu2021self}) often have taxonomies mostly oriented by image-specific SSL methods, which may not fully apply or provide deep insights into graph-specific challenges (e.g., view generation for graphs, graph-specific predictive tasks).\n        *   Concurrent graph-specific SSL surveys (e.g., \\cite{you2020graph}) might offer broader coverage but may lack the unified theoretical grounding and timely insights provided by \\cite{xie2021n52}.\n        *   \\cite{xie2021n52} distinguishes itself by providing a *unified, graph-specific* review, theoretically grounding all contrastive methods in mutual information maximization, and offering a framework that clarifies similarities and differences among diverse methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm (of the survey):** The paper's core approach is a comprehensive, unified review and categorization of existing SSL methods for GNNs. It proposes two main categories:\n        *   **Contrastive Models:** These learn by discriminating between positive (similar) and negative (dissimilar) pairs of \"views\" generated from graph data. The paper unifies these methods under a framework where the objective is related to mutual information maximization, and differences arise from how views are generated (e.g., identical, subgraphs, structural/feature transformations) and the specific mutual information estimators used (e.g., Jensen-Shannon, InfoNCE).\n        *   **Predictive Models:** These are trained in a supervised fashion using self-generated labels. The paper categorizes these based on the nature of the self-generated labels, including graph reconstruction (e.g., autoencoders, autoregressive), property prediction (e.g., k-hop connectivity, contextual properties), self-training, and invariance regularization.\n    *   **Novelty/Difference of this Approach (as a survey):**\n        *   **Unified Frameworks:** It provides a novel, unified framework for both contrastive and predictive SSL methods for GNNs, highlighting commonalities and distinctions in their components (view generation, objective functions, label generation strategies).\n        *   **Mutual Information Perspective:** It theoretically grounds all contrastive methods in mutual information maximization, offering a fresh and coherent perspective.\n        *   **Graph-Specific Focus:** Unlike general SSL surveys, it specifically addresses the unique challenges and solutions for graph-structured data.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (of the survey):**\n        *   **Unified Contrastive Learning Framework:** A general framework for GNN contrastive learning, categorizing methods by view generation strategies (e.g., identical, subgraphs, structural/feature transformations) and mutual information estimators (e.g., Jensen-Shannon, InfoNCE).\n        *   **Unified Predictive Learning Categorization:** A systematic categorization of predictive SSL methods based on how self-generated labels are obtained (graph reconstruction, property prediction, self-training, invariance regularization).\n    *   **System Design or Architectural Innovations (of the survey):**\n        *   **Standardized Testbed:** Development of an open-source, standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics. This facilitates consistent empirical comparison and methodological development.\n    *   **Theoretical Insights or Analysis (of the survey):**\n        *   **Mutual Information as Unifying Principle:** Emphasizing mutual information maximization as the theoretical grounding for all contrastive learning methods, providing a deeper understanding of their objectives.\n        *   **Insights into Component Choices:** Providing insights to guide the choice of components within the unified frameworks (e.g., view generation strategies, objective functions, graph encoders).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted (by the survey):** As a review paper, \\cite{xie2021n52} does not present novel experimental results for new SSL methods. Instead, its primary \"validation\" contribution is:\n        *   **Development of a Standardized Testbed:** The authors developed and released a standardized testbed, including implementations of common baseline methods, datasets, and evaluation metrics. This resource is designed to *enable* future empirical comparisons and methodological development by the research community.\n    *   **Key Performance Metrics and Comparison Results (summarized from reviewed papers):** The survey summarizes common settings of SSL tasks (unsupervised representation learning, unsupervised pre-training, auxiliary learning) and commonly used datasets across various categories. It implicitly covers the performance metrics and comparison results reported in the *reviewed* literature by discussing the different methods and their underlying principles, setting the stage for how such comparisons *should* be conducted using their testbed.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):**\n        *   **Focus on GNNs:** The review is specifically tailored to GNNs, meaning some general SSL concepts or methods from other domains that don't directly translate to graphs are not discussed in detail.\n        *   **Exclusion of Unsupervised Domain Adaptation:** The paper explicitly states it does not discuss unsupervised domain adaptation in detail, as this paradigm is not yet specifically studied in the graph domain.\n        *   **Timeliness vs. Exhaustive Coverage:** While aiming for an up-to-date review, it prioritizes a unified framework and timely insights over exhaustive coverage of every single existing SSL method for GNNs (as acknowledged in comparison to \\cite{you2020graph}).\n    *   **Scope of Applicability:** The review primarily covers SSL methods for static, attributed, undirected graphs. While it mentions discussing heterogeneous and dynamic graphs in an appendix, the core frameworks are presented for the more common graph types.\n\n7.  **Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:**\n        *   **Clarification and Unification:** By providing unified frameworks and a mutual information perspective, \\cite{xie2021n52} significantly clarifies the landscape of SSL for GNNs, making it easier for researchers to understand the relationships between diverse methods.\n        *   **Foundation for New Methods:** The structured categorization and insights into component choices (e.g., view generation, objective functions) provide a strong foundation and inspiration for developing novel and more effective SSL algorithms for GNNs.\n        *   **Facilitating Reproducibility and Comparison:** The development of a standardized testbed is a crucial contribution for the field, promoting reproducibility, fair empirical comparisons, and accelerating research progress.\n    *   **Potential Impact on Future Research:**\n        *   **Accelerated Development:** The unified frameworks and testbed will likely accelerate the development of new SSL methods for GNNs by providing clear guidelines and a common platform for evaluation.\n        *   **Improved Understanding:** Researchers will gain a deeper theoretical and practical understanding of SSL principles applied to graphs, leading to more principled method design.\n        *   **Broader Adoption:** By making SSL for GNNs more accessible and understandable, the paper can encourage broader adoption of these techniques in various graph-related applications where labeled data is a bottleneck.\n\n---",
      "keywords": [
        "Self-supervised learning (SSL)",
        "Graph Neural Networks (GNNs)",
        "Representation learning",
        "Unlabeled graph data",
        "Mutual information maximization",
        "Contrastive learning",
        "Predictive models",
        "Unified framework",
        "View generation",
        "Standardized testbed",
        "Graph reconstruction",
        "Molecular graphs",
        "Non-Euclidean structure"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf"
  },
  {
    "success": true,
    "doc_id": "ba7c705efb6015b93fd5f1e6adfb0895",
    "summary": "This paper \\cite{mitra2024x43} provides a comprehensive survey on the application of Graph Neural Networks (GNNs) in enhancing defensive cyber operations, structured around the Lockheed Martin Cyber Kill Chain (CKC).\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively defending against diverse, rapidly evolving cyber-attacks that constantly sabotage the confidentiality, integrity, and availability of digital systems. Traditional signature-based and rule-based defensive systems struggle to keep pace with the rapid mutation and polymorphism of modern threats.\n*   **Importance and Challenge:** Cyber-attacks are increasingly complex, targeting critical infrastructures and sensitive information in an interconnected world. The problem is important because effective cyber defense is crucial for safeguarding digital society. It is challenging due to the relentless evolution of attack patterns, the vast knowledge space required, and the need for adaptive, intelligent defense mechanisms that can predict and prevent attacks rather than merely react to them.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** The work positions GNNs as a promising Machine Learning (ML) approach that offers an adaptive and intelligent defense mechanism, contrasting them with conventional cybersecurity methods (e.g., signature-based detection) that are often outpaced by modern cyber threats.\n*   **Limitations of Previous Solutions:** Traditional methods often fail to capture nuanced patterns and dependencies within diverse datasets, leading to missed insights. They struggle with the rapid mutation and polymorphism of modern cyber threats, making them less effective in proactive defense.\n*   **Positioning:** This paper claims to be the first survey that specifically concentrates on the influence of GNNs in *overall cyber defense operations*, using the Lockheed Martin Cyber Kill Chain (CKC) as a comprehensive taxonomy to structure the analysis of GNN applications across the entire attack life cycle.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper surveys the application of Graph Neural Networks (GNNs) to enhance defensive cyber operations. GNNs are utilized for their ability to process and learn from heterogeneous cyber threat data by representing entities (nodes) and their relationships (edges) as graphs. The fundamental GNN mechanism involves iterative message passing, where nodes aggregate information from their neighbors and update their own feature representations.\n*   **Novelty or Difference:** As a survey, the paper's innovation lies in systematically mapping and analyzing existing GNN frameworks and their applications to each stage of the Lockheed Martin Cyber Kill Chain (CKC) for defensive purposes. It highlights how GNNs, by inherently recognizing the interconnectedness of entities in cyber threat landscapes, offer an adaptive and intelligent defense mechanism that can uncover hidden insights beyond conventional methods. The paper details various GNN variants, including:\n    *   **Graph Convolutional Networks (GCNs) \\cite{mitra2024x43}:** Employ convolution functions on graph data for semi-supervised classification, aggregating information from all neighbors.\n    *   **GraphSAGE-mean \\cite{mitra2024x43}:** Uses a random sampling strategy for neighbor aggregation, making it suitable for large graphs and heterogeneous data.\n    *   **Graph Attention Networks (GATs) \\cite{mitra2024x43}:** Incorporate attention mechanisms to assign different weights to neighbors, learning the importance of each node for fine-grained representations.\n    *   **Gated Graph Neural Networks (GGNNs) \\cite{mitra2024x43}:** Utilize recurrent neural architectures (GRUs) to learn node representations, capable of handling dynamic graphs and longer-range dependencies.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques (surveyed):** The paper highlights the capabilities of GNNs to capture complex relationships and nuanced patterns in cyber threat data, enabling adaptive and intelligent defense. It details the distinct aggregation and update functions of various GNN architectures (GCN, GraphSAGE, GAT, GGNN), showcasing their specific strengths (e.g., GAT's attention mechanism for importance weighting, GGNN's ability to handle dynamic graphs).\n*   **System Design or Architectural Innovations:** While not proposing new architectures, the paper implicitly contributes by demonstrating how GNNs provide a powerful graph-based analytical framework that can underpin innovative defensive system designs across the entire cyber kill chain.\n*   **Theoretical Insights or Analysis:** The paper provides a foundational overview of GNNs, explaining their core principles (aggregation and update functions) and categorizing their application based on the CKC. It offers insights into the theoretical advantages and trade-offs of different GNN variants, such as GCN's simplicity versus GAT's fine-grained representations or GGNN's dynamic graph handling.\n*   **Primary Contribution (as a survey):** A comprehensive summary of state-of-the-art research articles utilizing GNNs, systematically grouped according to their application in each phase of the defensive cyber operations life-cycle as defined by the Cyber Kill Chain.\n\n### 5. Experimental Validation\n*   The provided excerpt is from the introductory and background sections of a survey paper. It does not present original experimental validation conducted by the authors of this paper. Instead, it refers to \"Existing vast literature in GNN\" and \"Recent literature\" as providing evidence for GNNs' benefits and capabilities in solving challenging tasks on graph-structured data. The full survey would presumably detail the experimental validations from the papers it reviews.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions (of GNNs discussed):**\n    *   **GCNs \\cite{mitra2024x43}:** Can incur significant computational overhead for very large graphs due to considering all neighbors.\n    *   **GATs \\cite{mitra2024x43}:** While powerful, they can be more computationally expensive than simpler GCNs.\n    *   **GGNNs \\cite{mitra2024x43}:** Also noted to be potentially more computationally expensive than GCNs, especially for complex dynamic graphs.\n    *   **General GNNs:** Their effective utilization in cybersecurity relies on the availability of comprehensive, rich, and complete underlying data to construct meaningful graphs.\n*   **Scope of Applicability:** The paper's scope is focused on the application of GNNs specifically in aiding *defensive cyber operations*, with a particular emphasis on how they can be leveraged to address each stage of the Lockheed Martin Cyber Kill Chain.\n\n### 7. Technical Significance\n*   **Advance the Technical State-of-the-Art:** By systematically surveying and categorizing the diverse applications of GNNs across the entire cyber kill chain, the paper significantly advances the understanding of how these advanced ML techniques can be leveraged for proactive and adaptive cyber defense. It highlights GNNs' potential to move beyond traditional reactive, signature-based defenses towards more intelligent and predictive security measures.\n*   **Potential Impact on Future Research:** The paper explicitly identifies \"open research areas and further improvement scopes\" and aims to \"equip researchers with the capabilities and possible future research avenues in concretizing cyber defense operations through GNN.\" It provides a structured, CKC-based taxonomy that can guide future research efforts, helping to identify gaps and opportunities for GNN integration and development in each phase of defensive cyber operations.",
    "intriguing_abstract": "The relentless evolution of cyber threats demands a paradigm shift from reactive, signature-based defenses to intelligent, adaptive strategies. This paper presents the first comprehensive survey systematically exploring the transformative potential of Graph Neural Networks (GNNs) in bolstering defensive cyber operations across the entire Lockheed Martin Cyber Kill Chain (CKC). We delve into how GNNs, including Graph Convolutional Networks (GCNs), GraphSAGE, Graph Attention Networks (GATs), and Gated Graph Neural Networks (GGNNs), inherently capture complex relationships within heterogeneous cyber threat data, offering unprecedented capabilities for proactive threat detection, prediction, and mitigation.\n\nBy mapping diverse GNN applications to each CKC stage, from reconnaissance to actions on objectives, this work illuminates how these advanced machine learning techniques can uncover hidden insights and build resilient cyber defenses. This survey not only consolidates state-of-the-art research but also identifies critical open challenges and future research avenues, equipping researchers to concretize intelligent cyber defense and safeguard our interconnected digital world.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Defensive cyber operations",
      "Lockheed Martin Cyber Kill Chain (CKC)",
      "Adaptive and intelligent cyber defense",
      "Evolving cyber threats",
      "Graph-based analytical framework",
      "Graph Convolutional Networks (GCNs)",
      "Graph Attention Networks (GATs)",
      "Gated Graph Neural Networks (GGNNs)",
      "Message passing",
      "Complex relationship capture",
      "Proactive security measures",
      "Cyber threat data processing",
      "Comprehensive survey"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/b20589941cd52d199ba381b92e092ba7fb36d689.pdf",
    "citation_key": "mitra2024x43",
    "metadata": {
      "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
      "authors": [
        "Shaswata Mitra",
        "Trisha Chakraborty",
        "Subash Neupane",
        "Aritran Piplai",
        "Sudip Mittal"
      ],
      "published_date": "2024",
      "abstract": "In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint. Furthermore, We also discuss open research areas and further improvement scopes.",
      "file_path": "paper_data/Graph_Neural_Networks/b20589941cd52d199ba381b92e092ba7fb36d689.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper \\cite{mitra2024x43} provides a comprehensive survey on the application of Graph Neural Networks (GNNs) in enhancing defensive cyber operations, structured around the Lockheed Martin Cyber Kill Chain (CKC).\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively defending against diverse, rapidly evolving cyber-attacks that constantly sabotage the confidentiality, integrity, and availability of digital systems. Traditional signature-based and rule-based defensive systems struggle to keep pace with the rapid mutation and polymorphism of modern threats.\n*   **Importance and Challenge:** Cyber-attacks are increasingly complex, targeting critical infrastructures and sensitive information in an interconnected world. The problem is important because effective cyber defense is crucial for safeguarding digital society. It is challenging due to the relentless evolution of attack patterns, the vast knowledge space required, and the need for adaptive, intelligent defense mechanisms that can predict and prevent attacks rather than merely react to them.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** The work positions GNNs as a promising Machine Learning (ML) approach that offers an adaptive and intelligent defense mechanism, contrasting them with conventional cybersecurity methods (e.g., signature-based detection) that are often outpaced by modern cyber threats.\n*   **Limitations of Previous Solutions:** Traditional methods often fail to capture nuanced patterns and dependencies within diverse datasets, leading to missed insights. They struggle with the rapid mutation and polymorphism of modern cyber threats, making them less effective in proactive defense.\n*   **Positioning:** This paper claims to be the first survey that specifically concentrates on the influence of GNNs in *overall cyber defense operations*, using the Lockheed Martin Cyber Kill Chain (CKC) as a comprehensive taxonomy to structure the analysis of GNN applications across the entire attack life cycle.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper surveys the application of Graph Neural Networks (GNNs) to enhance defensive cyber operations. GNNs are utilized for their ability to process and learn from heterogeneous cyber threat data by representing entities (nodes) and their relationships (edges) as graphs. The fundamental GNN mechanism involves iterative message passing, where nodes aggregate information from their neighbors and update their own feature representations.\n*   **Novelty or Difference:** As a survey, the paper's innovation lies in systematically mapping and analyzing existing GNN frameworks and their applications to each stage of the Lockheed Martin Cyber Kill Chain (CKC) for defensive purposes. It highlights how GNNs, by inherently recognizing the interconnectedness of entities in cyber threat landscapes, offer an adaptive and intelligent defense mechanism that can uncover hidden insights beyond conventional methods. The paper details various GNN variants, including:\n    *   **Graph Convolutional Networks (GCNs) \\cite{mitra2024x43}:** Employ convolution functions on graph data for semi-supervised classification, aggregating information from all neighbors.\n    *   **GraphSAGE-mean \\cite{mitra2024x43}:** Uses a random sampling strategy for neighbor aggregation, making it suitable for large graphs and heterogeneous data.\n    *   **Graph Attention Networks (GATs) \\cite{mitra2024x43}:** Incorporate attention mechanisms to assign different weights to neighbors, learning the importance of each node for fine-grained representations.\n    *   **Gated Graph Neural Networks (GGNNs) \\cite{mitra2024x43}:** Utilize recurrent neural architectures (GRUs) to learn node representations, capable of handling dynamic graphs and longer-range dependencies.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques (surveyed):** The paper highlights the capabilities of GNNs to capture complex relationships and nuanced patterns in cyber threat data, enabling adaptive and intelligent defense. It details the distinct aggregation and update functions of various GNN architectures (GCN, GraphSAGE, GAT, GGNN), showcasing their specific strengths (e.g., GAT's attention mechanism for importance weighting, GGNN's ability to handle dynamic graphs).\n*   **System Design or Architectural Innovations:** While not proposing new architectures, the paper implicitly contributes by demonstrating how GNNs provide a powerful graph-based analytical framework that can underpin innovative defensive system designs across the entire cyber kill chain.\n*   **Theoretical Insights or Analysis:** The paper provides a foundational overview of GNNs, explaining their core principles (aggregation and update functions) and categorizing their application based on the CKC. It offers insights into the theoretical advantages and trade-offs of different GNN variants, such as GCN's simplicity versus GAT's fine-grained representations or GGNN's dynamic graph handling.\n*   **Primary Contribution (as a survey):** A comprehensive summary of state-of-the-art research articles utilizing GNNs, systematically grouped according to their application in each phase of the defensive cyber operations life-cycle as defined by the Cyber Kill Chain.\n\n### 5. Experimental Validation\n*   The provided excerpt is from the introductory and background sections of a survey paper. It does not present original experimental validation conducted by the authors of this paper. Instead, it refers to \"Existing vast literature in GNN\" and \"Recent literature\" as providing evidence for GNNs' benefits and capabilities in solving challenging tasks on graph-structured data. The full survey would presumably detail the experimental validations from the papers it reviews.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions (of GNNs discussed):**\n    *   **GCNs \\cite{mitra2024x43}:** Can incur significant computational overhead for very large graphs due to considering all neighbors.\n    *   **GATs \\cite{mitra2024x43}:** While powerful, they can be more computationally expensive than simpler GCNs.\n    *   **GGNNs \\cite{mitra2024x43}:** Also noted to be potentially more computationally expensive than GCNs, especially for complex dynamic graphs.\n    *   **General GNNs:** Their effective utilization in cybersecurity relies on the availability of comprehensive, rich, and complete underlying data to construct meaningful graphs.\n*   **Scope of Applicability:** The paper's scope is focused on the application of GNNs specifically in aiding *defensive cyber operations*, with a particular emphasis on how they can be leveraged to address each stage of the Lockheed Martin Cyber Kill Chain.\n\n### 7. Technical Significance\n*   **Advance the Technical State-of-the-Art:** By systematically surveying and categorizing the diverse applications of GNNs across the entire cyber kill chain, the paper significantly advances the understanding of how these advanced ML techniques can be leveraged for proactive and adaptive cyber defense. It highlights GNNs' potential to move beyond traditional reactive, signature-based defenses towards more intelligent and predictive security measures.\n*   **Potential Impact on Future Research:** The paper explicitly identifies \"open research areas and further improvement scopes\" and aims to \"equip researchers with the capabilities and possible future research avenues in concretizing cyber defense operations through GNN.\" It provides a structured, CKC-based taxonomy that can guide future research efforts, helping to identify gaps and opportunities for GNN integration and development in each phase of defensive cyber operations.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Defensive cyber operations",
        "Lockheed Martin Cyber Kill Chain (CKC)",
        "Adaptive and intelligent cyber defense",
        "Evolving cyber threats",
        "Graph-based analytical framework",
        "Graph Convolutional Networks (GCNs)",
        "Graph Attention Networks (GATs)",
        "Gated Graph Neural Networks (GGNNs)",
        "Message passing",
        "Complex relationship capture",
        "Proactive security measures",
        "Cyber threat data processing",
        "Comprehensive survey"
      ],
      "paper_type": "based on the provided abstract and introduction snippet, the paper can be classified as a **position** paper.\n\nhere's why:\n\n*   **problem identification:** the text clearly outlines a significant and escalating problem: \"the ceaseless barrage of cyber threats,\" \"complexity of cyber attacks,\" and the need for a \"paradigm shift in defensive strategies.\"\n*   **proposed direction/argument:** it argues that \"the marriage of machine learning (ml) and cybersecurity has yielded innovative solutions\" and specifically highlights graph neural networks (gnns) as a \"promising contender\" for navigating and analyzing complex relationships in cyber defense data. this is an argument for the potential and relevance of gnns in this domain.\n*   **lack of specific novel contribution:** the abstract does not mention \"propose,\" \"develop,\" \"present a new algorithm,\" or \"conduct an experiment.\" instead, it states that gnns \"have emerged\" as a promising solution, suggesting an advocacy for an existing or emerging technology rather than a novel technical contribution by the authors themselves.\n*   **focus on vision/future:** the discussion about \"getting ahead of the attacker to predict its possible next move\" and the identification of gnns as a \"cutting-edge ml solution\" points towards a vision for future defensive cyber operations.\n\nthe paper is arguing for the importance and utility of gnns in addressing the evolving challenges of defensive cyber operations, aligning well with the characteristics of a position paper."
    },
    "file_name": "b20589941cd52d199ba381b92e092ba7fb36d689.pdf"
  },
  {
    "success": true,
    "doc_id": "48fc059120a4554e69eca4c720278aa0",
    "summary": "Here's a focused summary of the paper \"Nested Graph Neural Networks\" by Zhang and Li for a literature review:\n\n*   **CITATION**: \\cite{zhang2021kc7}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) and the 1-Weisfeiler-Lehman (1-WL) algorithm primarily encode *rooted subtrees* around nodes through iterative neighbor aggregation \\cite{zhang2021kc7}.\n    *   This approach has limited expressiveness for representing general non-tree graphs, especially those containing cycles, leading to failures in distinguishing structurally different graphs (e.g., two triangles vs. a hexagon, as shown in Figure 1) \\cite{zhang2021kc7}.\n    *   The problem is important because accurate graph representation is crucial for tasks like graph classification, molecule modeling, and drug discovery, where distinguishing subtle structural differences is key \\cite{zhang2021kc7}.\n\n2.  **Related Work & Positioning**\n    *   GNNs are positioned as state-of-the-art for graph representation learning, building upon message passing schemes that mimic the 1-WL test \\cite{zhang2021kc7}.\n    *   This work acknowledges the success of GNNs due to their alignment with the inductive bias that similar graphs share common substructures (rooted subtrees) \\cite{zhang2021kc7}.\n    *   **Limitations of previous solutions (message passing GNNs/1-WL):**\n        *   Bounded by the expressive power of the 1-WL test, meaning they cannot discriminate certain non-isomorphic graphs (e.g., all n-node r-regular graphs) \\cite{zhang2021kc7}.\n        *   Rooted subtrees are a specific substructure and not general enough to represent arbitrary subgraphs, particularly those with cycles \\cite{zhang2021kc7}.\n        *   Standard GNNs do not allow for the use of root-node-specific structural features (e.g., distance from the root) to enhance node representations \\cite{zhang2021kc7}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method:** Nested Graph Neural Networks (NGNNs) introduce a two-level GNN architecture to encode *rooted subgraphs* instead of rooted subtrees \\cite{zhang2021kc7}.\n    *   **Mechanism:**\n        1.  For each node in the original graph, a local *rooted subgraph* (induced by nodes within `h` hops) is extracted \\cite{zhang2021kc7}.\n        2.  A *base GNN* (e.g., a message passing GNN) is applied *independently* to each extracted rooted subgraph to learn intermediate node representations within that subgraph \\cite{zhang2021kc7}.\n        3.  A *subgraph pooling layer* aggregates these intermediate node representations into a single *subgraph representation*, which then serves as the final representation for the root node of that subgraph in the original graph \\cite{zhang2021kc7}.\n        4.  An *outer GNN* (e.g., a simple graph pooling layer) aggregates these root node representations (which are now subgraph representations) to form a representation for the entire graph \\cite{zhang2021kc7}.\n    *   **Novelty/Difference:**\n        *   Shifts the \"receptive field\" from rooted subtrees to more general rooted subgraphs, capturing richer local structural information, especially cycles \\cite{zhang2021kc7}.\n        *   The base GNNs operate on *copied* subgraphs, allowing a node to have different representations and initial features (e.g., augmented with distance to the root) depending on its specific role within different subgraphs \\cite{zhang2021kc7}. This is a key distinction from standard GNNs where a node has a single representation at any given time.\n        *   It's a \"GNN of GNNs\" framework, where inner GNNs learn subgraph features and an outer GNN learns graph features from these \\cite{zhang2021kc7}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of the Nested Graph Neural Network (NGNN) framework, a two-level GNN architecture for learning graph representations based on rooted subgraphs \\cite{zhang2021kc7}.\n        *   A mechanism for extracting and independently processing local rooted subgraphs using shared-parameter base GNNs \\cite{zhang2021kc7}.\n        *   The use of a subgraph pooling layer to condense subgraph-level information into node-level representations for the outer GNN \\cite{zhang2021kc7}.\n    *   **Theoretical Insights/Analysis:**\n        *   Rigorous proof (Theorem 1) demonstrating that NGNN is *strictly more powerful* than the 1-WL test and standard message passing GNNs \\cite{zhang2021kc7}.\n        *   Specifically, NGNN can discriminate *almost all* r-regular graphs, a class of graphs where 1-WL and standard GNNs consistently fail \\cite{zhang2021kc7}.\n        *   The theory shows that NGNN achieves this enhanced power with relatively small height subgraphs and few base GNN layers, aided by subgraph pooling \\cite{zhang2021kc7}.\n    *   **System Design/Architectural Innovations:**\n        *   A \"plug-and-play\" framework that can integrate various existing GNNs as base GNNs, enhancing their expressive power \\cite{zhang2021kc7}.\n        *   Enables augmenting initial node features with subgraph-specific structural information (e.g., distance encoding from the root node), which is not possible in standard GNNs \\cite{zhang2021kc7}.\n    *   **Efficiency:**\n        *   Maintains linear time and space complexity with respect to graph size, similar to standard message passing GNNs, unlike other more powerful but computationally expensive higher-order GNNs \\cite{zhang2021kc7}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on both synthetic and real-world graph classification and regression datasets \\cite{zhang2021kc7}.\n        *   Tested with different base GNNs (e.g., GCN, GIN) to demonstrate its plug-and-play nature and consistent improvements \\cite{zhang2021kc7}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   On synthetic datasets, NGNN demonstrated higher expressive power than 1-WL, aligning well with the theoretical findings \\cite{zhang2021kc7}.\n        *   On real-world benchmark datasets, NGNN consistently improved the performance of a wide range of base GNNs \\cite{zhang2021kc7}.\n        *   Achieved highly competitive performance across all tested datasets, showcasing its practical effectiveness \\cite{zhang2021kc7}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   While significantly more powerful than 1-WL, NGNN's ability to discriminate is still bounded, specifically proven for \"almost all\" r-regular graphs, not *all* graphs or all types of graph isomorphism problems \\cite{zhang2021kc7}.\n        *   The framework relies on the base GNNs and pooling layers being \"injective\" (Definition 2) for its theoretical guarantees, which assumes sufficient capacity of the neural networks \\cite{zhang2021kc7}.\n    *   **Scope of Applicability:**\n        *   Primarily focused on graph-level tasks such as graph classification and regression \\cite{zhang2021kc7}.\n        *   Applicable to scenarios where distinguishing local subgraph structures (including cycles) is crucial for overall graph understanding \\cite{zhang2021kc7}.\n        *   The framework is general and can be combined with various existing GNN architectures as base GNNs \\cite{zhang2021kc7}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:**\n        *   Breaks the 1-WL expressiveness barrier for GNNs while maintaining computational efficiency, addressing a long-standing limitation in the field \\cite{zhang2021kc7}.\n        *   Provides a principled way to incorporate richer local structural information (rooted subgraphs) into GNN representations, moving beyond simple rooted subtrees \\cite{zhang2021kc7}.\n        *   Offers a flexible, plug-and-play framework that can immediately enhance existing GNN models \\cite{zhang2021kc7}.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for designing more powerful and expressive GNN architectures by leveraging nested computations and subgraph-level features \\cite{zhang2021kc7}.\n        *   Encourages further exploration into how to effectively define, extract, and learn from various types of local graph substructures \\cite{zhang2021kc7}.\n        *   Could lead to improved performance in applications requiring fine-grained structural understanding, such as drug discovery, material science, and social network analysis \\cite{zhang2021kc7}.",
    "intriguing_abstract": "Despite their remarkable success, standard Graph Neural Networks (GNNs) and the 1-Weisfeiler-Lehman (1-WL) test are fundamentally limited in their expressive power, often failing to distinguish structurally distinct graphs, especially those with intricate cycles or regular patterns. This bottleneck hinders progress in critical applications like molecular modeling and drug discovery, where subtle structural differences are paramount.\n\nWe introduce **Nested Graph Neural Networks (NGNNs)**, a novel two-level GNN architecture that dramatically enhances expressive power by encoding *rooted subgraphs* instead of mere rooted subtrees. NGNNs operate as a 'GNN of GNNs,' where inner GNNs learn rich, context-aware features from local subgraphs, which are then aggregated by an outer GNN to form comprehensive graph representations.\n\nThis paradigm shift is backed by rigorous theoretical proofs demonstrating that NGNNs are *strictly more powerful* than the 1-WL test and conventional message passing GNNs, capable of discriminating almost all r-regular graphs while maintaining linear time and space complexity. Empirically, NGNNs consistently outperform state-of-the-art models across diverse benchmarks, offering a plug-and-play framework to elevate existing GNN architectures, unlocking unprecedented capabilities for fine-grained graph understanding.",
    "keywords": [
      "Nested Graph Neural Networks (NGNNs)",
      "Graph Neural Networks (GNNs)",
      "1-Weisfeiler-Lehman (1-WL) test",
      "rooted subgraphs",
      "enhanced expressive power",
      "two-level GNN architecture",
      "subgraph pooling layer",
      "discriminates r-regular graphs",
      "linear time and space complexity",
      "graph representation learning",
      "plug-and-play framework",
      "graph classification",
      "drug discovery",
      "subgraph-specific structural information"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/775a6e0f9104b282ed867871d743e3afd1e66d96.pdf",
    "citation_key": "zhang2021kc7",
    "metadata": {
      "title": "Nested Graph Neural Networks",
      "authors": [
        "Muhan Zhang",
        "Pan Li"
      ],
      "published_date": "2021",
      "abstract": "Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant-factor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/775a6e0f9104b282ed867871d743e3afd1e66d96.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Nested Graph Neural Networks\" by Zhang and Li for a literature review:\n\n*   **CITATION**: \\cite{zhang2021kc7}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) and the 1-Weisfeiler-Lehman (1-WL) algorithm primarily encode *rooted subtrees* around nodes through iterative neighbor aggregation \\cite{zhang2021kc7}.\n    *   This approach has limited expressiveness for representing general non-tree graphs, especially those containing cycles, leading to failures in distinguishing structurally different graphs (e.g., two triangles vs. a hexagon, as shown in Figure 1) \\cite{zhang2021kc7}.\n    *   The problem is important because accurate graph representation is crucial for tasks like graph classification, molecule modeling, and drug discovery, where distinguishing subtle structural differences is key \\cite{zhang2021kc7}.\n\n2.  **Related Work & Positioning**\n    *   GNNs are positioned as state-of-the-art for graph representation learning, building upon message passing schemes that mimic the 1-WL test \\cite{zhang2021kc7}.\n    *   This work acknowledges the success of GNNs due to their alignment with the inductive bias that similar graphs share common substructures (rooted subtrees) \\cite{zhang2021kc7}.\n    *   **Limitations of previous solutions (message passing GNNs/1-WL):**\n        *   Bounded by the expressive power of the 1-WL test, meaning they cannot discriminate certain non-isomorphic graphs (e.g., all n-node r-regular graphs) \\cite{zhang2021kc7}.\n        *   Rooted subtrees are a specific substructure and not general enough to represent arbitrary subgraphs, particularly those with cycles \\cite{zhang2021kc7}.\n        *   Standard GNNs do not allow for the use of root-node-specific structural features (e.g., distance from the root) to enhance node representations \\cite{zhang2021kc7}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method:** Nested Graph Neural Networks (NGNNs) introduce a two-level GNN architecture to encode *rooted subgraphs* instead of rooted subtrees \\cite{zhang2021kc7}.\n    *   **Mechanism:**\n        1.  For each node in the original graph, a local *rooted subgraph* (induced by nodes within `h` hops) is extracted \\cite{zhang2021kc7}.\n        2.  A *base GNN* (e.g., a message passing GNN) is applied *independently* to each extracted rooted subgraph to learn intermediate node representations within that subgraph \\cite{zhang2021kc7}.\n        3.  A *subgraph pooling layer* aggregates these intermediate node representations into a single *subgraph representation*, which then serves as the final representation for the root node of that subgraph in the original graph \\cite{zhang2021kc7}.\n        4.  An *outer GNN* (e.g., a simple graph pooling layer) aggregates these root node representations (which are now subgraph representations) to form a representation for the entire graph \\cite{zhang2021kc7}.\n    *   **Novelty/Difference:**\n        *   Shifts the \"receptive field\" from rooted subtrees to more general rooted subgraphs, capturing richer local structural information, especially cycles \\cite{zhang2021kc7}.\n        *   The base GNNs operate on *copied* subgraphs, allowing a node to have different representations and initial features (e.g., augmented with distance to the root) depending on its specific role within different subgraphs \\cite{zhang2021kc7}. This is a key distinction from standard GNNs where a node has a single representation at any given time.\n        *   It's a \"GNN of GNNs\" framework, where inner GNNs learn subgraph features and an outer GNN learns graph features from these \\cite{zhang2021kc7}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of the Nested Graph Neural Network (NGNN) framework, a two-level GNN architecture for learning graph representations based on rooted subgraphs \\cite{zhang2021kc7}.\n        *   A mechanism for extracting and independently processing local rooted subgraphs using shared-parameter base GNNs \\cite{zhang2021kc7}.\n        *   The use of a subgraph pooling layer to condense subgraph-level information into node-level representations for the outer GNN \\cite{zhang2021kc7}.\n    *   **Theoretical Insights/Analysis:**\n        *   Rigorous proof (Theorem 1) demonstrating that NGNN is *strictly more powerful* than the 1-WL test and standard message passing GNNs \\cite{zhang2021kc7}.\n        *   Specifically, NGNN can discriminate *almost all* r-regular graphs, a class of graphs where 1-WL and standard GNNs consistently fail \\cite{zhang2021kc7}.\n        *   The theory shows that NGNN achieves this enhanced power with relatively small height subgraphs and few base GNN layers, aided by subgraph pooling \\cite{zhang2021kc7}.\n    *   **System Design/Architectural Innovations:**\n        *   A \"plug-and-play\" framework that can integrate various existing GNNs as base GNNs, enhancing their expressive power \\cite{zhang2021kc7}.\n        *   Enables augmenting initial node features with subgraph-specific structural information (e.g., distance encoding from the root node), which is not possible in standard GNNs \\cite{zhang2021kc7}.\n    *   **Efficiency:**\n        *   Maintains linear time and space complexity with respect to graph size, similar to standard message passing GNNs, unlike other more powerful but computationally expensive higher-order GNNs \\cite{zhang2021kc7}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on both synthetic and real-world graph classification and regression datasets \\cite{zhang2021kc7}.\n        *   Tested with different base GNNs (e.g., GCN, GIN) to demonstrate its plug-and-play nature and consistent improvements \\cite{zhang2021kc7}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   On synthetic datasets, NGNN demonstrated higher expressive power than 1-WL, aligning well with the theoretical findings \\cite{zhang2021kc7}.\n        *   On real-world benchmark datasets, NGNN consistently improved the performance of a wide range of base GNNs \\cite{zhang2021kc7}.\n        *   Achieved highly competitive performance across all tested datasets, showcasing its practical effectiveness \\cite{zhang2021kc7}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   While significantly more powerful than 1-WL, NGNN's ability to discriminate is still bounded, specifically proven for \"almost all\" r-regular graphs, not *all* graphs or all types of graph isomorphism problems \\cite{zhang2021kc7}.\n        *   The framework relies on the base GNNs and pooling layers being \"injective\" (Definition 2) for its theoretical guarantees, which assumes sufficient capacity of the neural networks \\cite{zhang2021kc7}.\n    *   **Scope of Applicability:**\n        *   Primarily focused on graph-level tasks such as graph classification and regression \\cite{zhang2021kc7}.\n        *   Applicable to scenarios where distinguishing local subgraph structures (including cycles) is crucial for overall graph understanding \\cite{zhang2021kc7}.\n        *   The framework is general and can be combined with various existing GNN architectures as base GNNs \\cite{zhang2021kc7}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:**\n        *   Breaks the 1-WL expressiveness barrier for GNNs while maintaining computational efficiency, addressing a long-standing limitation in the field \\cite{zhang2021kc7}.\n        *   Provides a principled way to incorporate richer local structural information (rooted subgraphs) into GNN representations, moving beyond simple rooted subtrees \\cite{zhang2021kc7}.\n        *   Offers a flexible, plug-and-play framework that can immediately enhance existing GNN models \\cite{zhang2021kc7}.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for designing more powerful and expressive GNN architectures by leveraging nested computations and subgraph-level features \\cite{zhang2021kc7}.\n        *   Encourages further exploration into how to effectively define, extract, and learn from various types of local graph substructures \\cite{zhang2021kc7}.\n        *   Could lead to improved performance in applications requiring fine-grained structural understanding, such as drug discovery, material science, and social network analysis \\cite{zhang2021kc7}.",
      "keywords": [
        "Nested Graph Neural Networks (NGNNs)",
        "Graph Neural Networks (GNNs)",
        "1-Weisfeiler-Lehman (1-WL) test",
        "rooted subgraphs",
        "enhanced expressive power",
        "two-level GNN architecture",
        "subgraph pooling layer",
        "discriminates r-regular graphs",
        "linear time and space complexity",
        "graph representation learning",
        "plug-and-play framework",
        "graph classification",
        "drug discovery",
        "subgraph-specific structural information"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **core contribution:** the abstract explicitly states, \"we propose nested graph neural networks (ngnns).\" it then proceeds to describe *how* ngnns work (\"ngnn represents a graph with rooted subgraphs...\", \"ngnn extracts a local subgraph around each node and applies a base gnn...\"). this focus on introducing a new method/system is the hallmark of a technical paper.\n2.  **supporting elements:** while the paper includes \"rigorous theoretical analysis\" and \"proved that ngnn can discriminate almost all r-regular graphs\" (theoretical aspects) and \"we test ngnn with different base gnns on several benchmark datasets\" with \"highly competitive performance\" (empirical aspects), these are presented as validation and analysis *of the proposed ngnn method*. a strong technical paper often includes both theoretical justification and empirical evaluation of its new method.\n3.  **absence of other primary classifications:**\n    *   it's not a **survey** as its primary goal is not to review existing literature comprehensively.\n    *   it's not purely **theoretical** because the theory is applied to a *new proposed system*, not just abstract mathematical analysis.\n    *   it's not purely **empirical** because the experiments are testing a *new method* rather than just exploring data or phenomena.\n    *   it's not a **case_study**, **position**, or **short** paper based on the content.\n\nthe primary purpose is to present a new method (ngnn), making it a **technical** paper."
    },
    "file_name": "775a6e0f9104b282ed867871d743e3afd1e66d96.pdf"
  },
  {
    "success": true,
    "doc_id": "6899471f9d23f4642b9ac9691467145d",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) often perform poorly on tasks based on sets of nodes (e.g., link prediction, motif prediction, relation prediction) compared to single-node or whole-graph tasks \\cite{wang2022p2r}.\n    *   This failure stems from GNNs' intrinsic inability to distinguish nodes that are matched under graph automorphism, leading to a loss of node identities \\cite{wang2022p2r}.\n    *   Existing solutions using augmented node features (random features or deterministic distance encoding) suffer from slow convergence, inaccurate predictions, high computational complexity, or lack of generalizability and stability \\cite{wang2022p2r}.\n    *   Specifically, previous Positional Encoding (PE) methods, which use absolute node positions, struggle to guarantee permutation equivariance and stability, especially when graphs have multiple eigenvalues or small eigengaps, making them unreliable for practical networks \\cite{wang2022p2r}.\n\n*   **Related Work & Positioning**\n    *   **Random Features (RF)**: Augment GNNs with random node features to distinguish nodes and guarantee permutation equivariance. Limitations: often hard to converge, noisy, and inaccurate due to injected randomness \\cite{wang2022p2r}.\n    *   **Deterministic Distance Encoding (DE)**: Define extra features based on distances from a node to a target node set. Limitations: theoretically sound and empirically strong, but introduces huge memory and time consumption as features are sample-specific and cannot be shared \\cite{wang2022p2r}.\n    *   **Previous Positional Encoding (PE) Methods**:\n        *   Randomize PE (e.g., distances to random anchor nodes) to ensure permutation equivariance. Limitations: slow convergence, subpar performance \\cite{wang2022p2r}.\n        *   Use eigenvectors of randomly permuted graph Laplacian, or perturb signs of eigenvectors. Limitations: problematic when the Laplacian matrix has multiple eigenvalues (common in real-world graphs), and unstable even with distinct eigenvalues if eigengaps are small (sensitivity depends on the inverse of the smallest eigengap) \\cite{wang2022p2r}.\n    *   **Positioning**: The work by \\cite{wang2022p2r} proposes a principled solution to address the instability and lack of equivariance in GNNs using PE, particularly for large graphs and smaller training sets, where previous methods are conjectured to fail more severely.\n\n*   **Technical Approach & Innovation**\n    *   **Core Idea**: `PEG` (Positional Encoding GNN) uses separate channels to update original node features and positional features \\cite{wang2022p2r}.\n    *   **Equivariance Properties**: `PEG` imposes permutation equivariance with respect to the original node features and simultaneously imposes O(p) (orthogonal group) equivariance with respect to the p-dimensional positional features \\cite{wang2022p2r}. This O(p) equivariance allows for rotation and reflection of positional features without changing their meaning, addressing the non-uniqueness of eigenvectors.\n    *   **Stability Mechanism**: For Laplacian Eigenmap (LE) as PE, `PEG` is provably stable. Its sensitivity to graph perturbation depends only on the gap between the p-th and (p+1)-th eigenvalues of the graph Laplacian, rather than the smallest gap between any two consecutive eigenvalues (which is the case for previous methods) \\cite{wang2022p2r}. This significantly improves stability, especially when small eigengaps exist among the first `p` eigenvalues.\n    *   **Generalizability**: The approach applies to a broad range of PE techniques formulated as matrix factorization (e.g., Laplacian Eigenmap, Deepwalk) \\cite{wang2022p2r}.\n\n*   **Key Technical Contributions**\n    *   **Novel GNN Layer (`PEG`)**: Introduction of a new class of GNN layers that explicitly handles positional features in separate channels, ensuring both permutation equivariance for node features and O(p) equivariance for positional features \\cite{wang2022p2r}.\n    *   **Rigorous Mathematical Analysis**: Provides a principled study of equivariance and stability issues in GNNs with PE, including a formal definition of PE-stability and PE-equivariance \\cite{wang2022p2r}.\n    *   **Provable Stability**: Mathematical proof that `PEG` is provably stable, with its sensitivity to graph perturbations depending on a much larger eigengap (between `p` and `p+1` eigenvalues) compared to previous methods (which depend on the smallest eigengap among the first `p+1` eigenvalues) \\cite{wang2022p2r}.\n    *   **Addressing Eigenvector Ambiguity**: The O(p) equivariance directly addresses the non-uniqueness of eigenvectors when multiple eigenvalues exist, a critical limitation of prior PE methods \\cite{wang2022p2r}.\n\n*   **Experimental Validation**\n    *   **Task**: Extensive link prediction experiments \\cite{wang2022p2r}.\n    *   **Datasets**: Evaluated over 8 real-world networks \\cite{wang2022p2r}.\n    *   **Performance Metrics**: Not explicitly stated in the provided text, but implied to be accuracy/performance for link prediction.\n    *   **Key Results**:\n        *   `PEG` achieves comparable performance to strong baselines based on Deterministic Distance Encoding (DE) while having significantly lower training and inference complexity \\cite{wang2022p2r}.\n        *   `PEG` significantly outperforms other baselines that do not use DE \\cite{wang2022p2r}.\n        *   The performance gap is further enlarged in domain-shift link prediction scenarios (training and testing on networks from different domains), demonstrating `PEG`'s strong generalization and transferability \\cite{wang2022p2r}.\n\n*   **Limitations & Scope**\n    *   **Experiment Setting**: The paper conjectures that the instability issues of previous models become more severe for larger graphs and smaller training sets, which is the focus of their experiments. An extensive study of this point is left for future work \\cite{wang2022p2r}.\n    *   **PE Techniques**: While applicable to matrix factorization-based PE (LE, Deepwalk), the generalizability to *all* PE techniques is not explicitly stated \\cite{wang2022p2r}.\n    *   **Node-Set Tasks**: The primary focus is on node-set-based tasks, with link prediction as the main experimental validation \\cite{wang2022p2r}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `PEG` provides a principled and provably stable method for incorporating positional encoding into GNNs, overcoming critical limitations of previous approaches related to equivariance and stability, especially for graphs with complex eigenvalue structures \\cite{wang2022p2r}.\n    *   **Improved Generalization and Scalability**: Demonstrates superior generalization and transferability, particularly in domain-shift scenarios, and achieves competitive performance with significantly lower computational cost compared to DE-based methods \\cite{wang2022p2r}.\n    *   **Potential Impact**: Opens avenues for more reliable and powerful GNN applications in node-set-based tasks, which are prevalent in real-world scenarios (e.g., drug discovery, social network analysis). The mathematical framework for stability analysis of GNNs with PE is a significant theoretical contribution \\cite{wang2022p2r}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) often struggle with node-set tasks like link prediction, primarily due to their intrinsic inability to distinguish automorphic nodes and the inherent instability of existing Positional Encoding (PE) methods. Prior PE approaches, frequently relying on graph Laplacian eigenvectors, suffer from unreliable permutation equivariance and become particularly problematic in real-world graphs characterized by multiple eigenvalues or small eigengaps.\n\nWe introduce `PEG` (Positional Encoding GNN), a novel GNN layer that fundamentally redefines how positional information is integrated. `PEG` employs separate channels to maintain permutation equivariance for original node features while simultaneously enforcing O(p) equivariance for p-dimensional positional features. This innovative O(p) equivariance elegantly resolves the critical issue of eigenvector ambiguity, a major limitation of previous methods. Crucially, `PEG` is provably stable, with its sensitivity to graph perturbations depending on a significantly larger eigengap (between the p-th and (p+1)-th eigenvalues) than prior approaches. Extensive link prediction experiments across 8 real-world networks demonstrate `PEG`'s competitive performance against complex Deterministic Distance Encoding baselines, but with vastly reduced computational complexity. Furthermore, `PEG` exhibits superior generalization and transferability in domain-shift scenarios. This work offers a principled, robust, and scalable solution, paving the way for more reliable GNN applications in crucial node-set tasks.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Positional Encoding (PE)",
      "Node-set tasks",
      "Permutation equivariance",
      "GNN stability",
      "PEG (Positional Encoding GNN)",
      "O(p) equivariance",
      "Eigenvector ambiguity",
      "Provable stability",
      "Eigengap dependency",
      "Separate channels",
      "Link prediction",
      "Computational complexity",
      "Domain-shift generalization",
      "Mathematical analysis"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf",
    "citation_key": "wang2022p2r",
    "metadata": {
      "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
      "authors": [
        "Hongya Wang",
        "Haoteng Yin",
        "Muhan Zhang",
        "Pan Li"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and imposes $O(p)$ (orthogonal group) equivariance w.r.t. the positional features simultaneously, where $p$ is the dimension of used positional features. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.",
      "file_path": "paper_data/Graph_Neural_Networks/facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) often perform poorly on tasks based on sets of nodes (e.g., link prediction, motif prediction, relation prediction) compared to single-node or whole-graph tasks \\cite{wang2022p2r}.\n    *   This failure stems from GNNs' intrinsic inability to distinguish nodes that are matched under graph automorphism, leading to a loss of node identities \\cite{wang2022p2r}.\n    *   Existing solutions using augmented node features (random features or deterministic distance encoding) suffer from slow convergence, inaccurate predictions, high computational complexity, or lack of generalizability and stability \\cite{wang2022p2r}.\n    *   Specifically, previous Positional Encoding (PE) methods, which use absolute node positions, struggle to guarantee permutation equivariance and stability, especially when graphs have multiple eigenvalues or small eigengaps, making them unreliable for practical networks \\cite{wang2022p2r}.\n\n*   **Related Work & Positioning**\n    *   **Random Features (RF)**: Augment GNNs with random node features to distinguish nodes and guarantee permutation equivariance. Limitations: often hard to converge, noisy, and inaccurate due to injected randomness \\cite{wang2022p2r}.\n    *   **Deterministic Distance Encoding (DE)**: Define extra features based on distances from a node to a target node set. Limitations: theoretically sound and empirically strong, but introduces huge memory and time consumption as features are sample-specific and cannot be shared \\cite{wang2022p2r}.\n    *   **Previous Positional Encoding (PE) Methods**:\n        *   Randomize PE (e.g., distances to random anchor nodes) to ensure permutation equivariance. Limitations: slow convergence, subpar performance \\cite{wang2022p2r}.\n        *   Use eigenvectors of randomly permuted graph Laplacian, or perturb signs of eigenvectors. Limitations: problematic when the Laplacian matrix has multiple eigenvalues (common in real-world graphs), and unstable even with distinct eigenvalues if eigengaps are small (sensitivity depends on the inverse of the smallest eigengap) \\cite{wang2022p2r}.\n    *   **Positioning**: The work by \\cite{wang2022p2r} proposes a principled solution to address the instability and lack of equivariance in GNNs using PE, particularly for large graphs and smaller training sets, where previous methods are conjectured to fail more severely.\n\n*   **Technical Approach & Innovation**\n    *   **Core Idea**: `PEG` (Positional Encoding GNN) uses separate channels to update original node features and positional features \\cite{wang2022p2r}.\n    *   **Equivariance Properties**: `PEG` imposes permutation equivariance with respect to the original node features and simultaneously imposes O(p) (orthogonal group) equivariance with respect to the p-dimensional positional features \\cite{wang2022p2r}. This O(p) equivariance allows for rotation and reflection of positional features without changing their meaning, addressing the non-uniqueness of eigenvectors.\n    *   **Stability Mechanism**: For Laplacian Eigenmap (LE) as PE, `PEG` is provably stable. Its sensitivity to graph perturbation depends only on the gap between the p-th and (p+1)-th eigenvalues of the graph Laplacian, rather than the smallest gap between any two consecutive eigenvalues (which is the case for previous methods) \\cite{wang2022p2r}. This significantly improves stability, especially when small eigengaps exist among the first `p` eigenvalues.\n    *   **Generalizability**: The approach applies to a broad range of PE techniques formulated as matrix factorization (e.g., Laplacian Eigenmap, Deepwalk) \\cite{wang2022p2r}.\n\n*   **Key Technical Contributions**\n    *   **Novel GNN Layer (`PEG`)**: Introduction of a new class of GNN layers that explicitly handles positional features in separate channels, ensuring both permutation equivariance for node features and O(p) equivariance for positional features \\cite{wang2022p2r}.\n    *   **Rigorous Mathematical Analysis**: Provides a principled study of equivariance and stability issues in GNNs with PE, including a formal definition of PE-stability and PE-equivariance \\cite{wang2022p2r}.\n    *   **Provable Stability**: Mathematical proof that `PEG` is provably stable, with its sensitivity to graph perturbations depending on a much larger eigengap (between `p` and `p+1` eigenvalues) compared to previous methods (which depend on the smallest eigengap among the first `p+1` eigenvalues) \\cite{wang2022p2r}.\n    *   **Addressing Eigenvector Ambiguity**: The O(p) equivariance directly addresses the non-uniqueness of eigenvectors when multiple eigenvalues exist, a critical limitation of prior PE methods \\cite{wang2022p2r}.\n\n*   **Experimental Validation**\n    *   **Task**: Extensive link prediction experiments \\cite{wang2022p2r}.\n    *   **Datasets**: Evaluated over 8 real-world networks \\cite{wang2022p2r}.\n    *   **Performance Metrics**: Not explicitly stated in the provided text, but implied to be accuracy/performance for link prediction.\n    *   **Key Results**:\n        *   `PEG` achieves comparable performance to strong baselines based on Deterministic Distance Encoding (DE) while having significantly lower training and inference complexity \\cite{wang2022p2r}.\n        *   `PEG` significantly outperforms other baselines that do not use DE \\cite{wang2022p2r}.\n        *   The performance gap is further enlarged in domain-shift link prediction scenarios (training and testing on networks from different domains), demonstrating `PEG`'s strong generalization and transferability \\cite{wang2022p2r}.\n\n*   **Limitations & Scope**\n    *   **Experiment Setting**: The paper conjectures that the instability issues of previous models become more severe for larger graphs and smaller training sets, which is the focus of their experiments. An extensive study of this point is left for future work \\cite{wang2022p2r}.\n    *   **PE Techniques**: While applicable to matrix factorization-based PE (LE, Deepwalk), the generalizability to *all* PE techniques is not explicitly stated \\cite{wang2022p2r}.\n    *   **Node-Set Tasks**: The primary focus is on node-set-based tasks, with link prediction as the main experimental validation \\cite{wang2022p2r}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `PEG` provides a principled and provably stable method for incorporating positional encoding into GNNs, overcoming critical limitations of previous approaches related to equivariance and stability, especially for graphs with complex eigenvalue structures \\cite{wang2022p2r}.\n    *   **Improved Generalization and Scalability**: Demonstrates superior generalization and transferability, particularly in domain-shift scenarios, and achieves competitive performance with significantly lower computational cost compared to DE-based methods \\cite{wang2022p2r}.\n    *   **Potential Impact**: Opens avenues for more reliable and powerful GNN applications in node-set-based tasks, which are prevalent in real-world scenarios (e.g., drug discovery, social network analysis). The mathematical framework for stability analysis of GNNs with PE is a significant theoretical contribution \\cite{wang2022p2r}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Positional Encoding (PE)",
        "Node-set tasks",
        "Permutation equivariance",
        "GNN stability",
        "PEG (Positional Encoding GNN)",
        "O(p) equivariance",
        "Eigenvector ambiguity",
        "Provable stability",
        "Eigengap dependency",
        "Separate channels",
        "Link prediction",
        "Computational complexity",
        "Domain-shift generalization",
        "Mathematical analysis"
      ],
      "paper_type": "the paper should be classified as **theoretical**.\n\nhere's why:\n\n1.  **strong emphasis on \"provable solution\" and \"rigorous mathematical analysis\":** the abstract explicitly states, \"we... propose a provable solution, a class of gnn layers termed peg with rigorous mathematical analysis.\" this language directly aligns with the \"theoretical\" criteria of \"prove\", \"analysis\", and \"mathematical\".\n2.  **focus on formal properties:** the paper discusses \"permutation equivariance\" and \"o(p)(orthogonal group) equivariance\" as key characteristics of their proposed gnn layers (peg). these are formal mathematical properties, indicating a deep theoretical foundation.\n3.  **problem framed in terms of stability and generalizability:** the introduction highlights that existing gnns \"are not generalizable to unseen graphs (inductive) or stable.\" the proposed solution, peg, aims to address these issues in a \"principled way\" through its provable properties, which is a theoretical approach to solving a practical problem.\n4.  while the paper *proposes a new method* (peg gnn layers), which could suggest \"technical,\" the core contribution is framed around the *mathematical analysis and provable properties* that make this method robust and effective. the empirical experiments mentioned are to \"demonstrate the advantages\" of a solution that is fundamentally justified by its theoretical underpinnings."
    },
    "file_name": "facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf"
  },
  {
    "success": true,
    "doc_id": "8fabada499c391c89beb1974528fda16",
    "summary": "Here's a focused summary of the technical paper \\cite{lu20213kr} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses a \"significant gap\" in conventional Graph Neural Network (GNN) pre-training, which follows a two-step paradigm: 1) pre-training on abundant unlabeled data, and 2) fine-tuning on downstream labeled data. This gap arises from the divergence of optimization objectives between the two steps.\n    *   **Importance and Challenge**: Training GNNs typically requires extensive labeled data, which is often scarce and costly. While pre-training aims to learn transferable knowledge from unlabeled data, the objective mismatch between pre-training (optimizing for intrinsic graph properties) and fine-tuning (optimizing for downstream task performance) significantly hurts the generalization ability and quick adaptability of pre-trained GNN models. A key challenge is to develop a pre-training strategy that can simultaneously capture both local (node-level) and global (graph-level) information using *completely unlabeled* data, as existing methods either focus on node-level or require supervised information for graph-level pre-training.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{lu20213kr} positions itself against conventional GNN pre-training methods and draws inspiration from meta-learning, particularly Model-Agnostic Meta-Learning (MAML).\n    *   **Limitations of Previous Solutions**:\n        *   Prior GNN pre-training strategies (e.g., \\cite{hu2019graph}, \\cite{navarin2018graph}) are decoupled from the fine-tuning process, leading to a suboptimal initialization that is not inherently optimized for rapid adaptation to new tasks.\n        *   Existing methods for learning graph-level representations often require supervised information (e.g., \\cite{hu2020strategies}), failing to provide a fully self-supervised approach for both node and graph levels simultaneously.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{lu20213kr} proposes L2P-GNN (Learning to Pre-train GNNs), a self-supervised pre-training strategy that explicitly learns *how to fine-tune* during the pre-training process. This is achieved by framing the pre-training as an optimization-based meta-learning problem (similar to MAML). The pre-training objective is to find initial GNN parameters that, after a few gradient updates on a *simulated* support set (sampled from the pre-training graph), perform optimally on a *simulated* query set (also from the same pre-training graph).\n    *   **Novelty**:\n        *   The core innovation is the application of a meta-learning paradigm to GNN pre-training, directly optimizing for the model's ability to quickly adapt to new tasks, thereby alleviating the objective divergence.\n        *   It introduces a novel *dual adaptation mechanism* for completely self-supervised learning of both node-level and graph-level representations:\n            *   **Node-level adaptation**: Utilizes a self-supervised link prediction task, where the GNN learns to distinguish existing edges from randomly sampled non-edges.\n            *   **Graph-level adaptation**: Employs a contrastive learning approach, where the GNN is trained to maximize the mutual information between the representation of a whole graph and its sampled sub-structures, ensuring global context is preserved.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The first exploration of \"learning to pre-train GNNs\" to explicitly bridge the gap between pre-training and fine-tuning objectives \\cite{lu20213kr}.\n        *   A novel, completely self-supervised GNN pre-training strategy that simultaneously learns effective representations at both node and graph levels.\n        *   The design of a dual adaptation mechanism combining node-level link prediction and graph-level contrastive learning within a meta-learning framework.\n    *   **System Design or Architectural Innovations**: Integration of the MAML framework with self-supervised tasks specifically designed for graph-structured data.\n    *   **Theoretical Insights or Analysis**: Provides an analysis demonstrating the divergence in optimization objectives of conventional GNN pre-training and formally introduces a meta-learning objective to address it.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A systematic empirical study was performed to evaluate L2P-GNN's effectiveness across various GNN models.\n    *   **Key Performance Metrics and Comparison Results**: The evaluation involved pre-training on a public collection of protein graphs and a newly compiled large-scale bibliographic graph dataset. Experimental results demonstrate that L2P-GNN learns \"effective and transferable prior knowledge\" that yields \"powerful representations for downstream tasks,\" consistently and significantly outperforming state-of-the-art baselines.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: While not explicitly detailed in the provided text, meta-learning approaches like MAML can be computationally intensive due to the need for second-order gradients. The effectiveness of L2P-GNN relies on the quality and diversity of the simulated tasks constructed during pre-training.\n    *   **Scope of Applicability**: L2P-GNN is broadly applicable to various GNN architectures and is particularly beneficial in scenarios where labeled data for downstream tasks is scarce, but abundant unlabeled graph data is available for pre-training. It supports both node-level and graph-level downstream tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of the Technical State-of-the-Art**: \\cite{lu20213kr} significantly advances GNN pre-training by shifting the paradigm from decoupled pre-training to a meta-learning approach that directly optimizes for transferability and rapid adaptation. This addresses a fundamental limitation of previous two-step pre-training strategies.\n    *   **Potential Impact on Future Research**: This work opens new research directions for GNNs, encouraging further exploration of meta-learning techniques for graph representation learning, the development of more sophisticated self-supervised tasks for multi-level graph understanding, and the creation of more robust and generalizable GNN models for real-world applications with limited supervision.",
    "intriguing_abstract": "Conventional Graph Neural Network (GNN) pre-training faces a critical challenge: the inherent objective divergence between pre-training on abundant unlabeled data and fine-tuning on scarce labeled data. This mismatch severely impedes rapid adaptation and generalization, limiting GNNs' potential in real-world applications.\n\nWe introduce L2P-GNN (Learning to Pre-train GNNs), a novel self-supervised meta-learning framework that fundamentally redefines GNN pre-training by explicitly optimizing for *transferability* and *rapid adaptation*. Inspired by Model-Agnostic Meta-Learning (MAML), L2P-GNN learns *how to fine-tune* during pre-training, finding initial GNN parameters that quickly adapt to simulated downstream tasks. A core innovation is its dual adaptation mechanism, enabling completely self-supervised learning of both node-level and graph-level representations. This is achieved through a node-level link prediction task and a graph-level contrastive learning objective, ensuring comprehensive knowledge capture from unlabeled data.\n\nEmpirical results demonstrate L2P-GNN's significant superiority over state-of-the-art baselines, yielding powerful, transferable representations across diverse downstream tasks. This work paves the way for more robust and generalizable GNNs, crucial for advancing graph representation learning in scenarios with limited supervision.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN pre-training",
      "objective divergence",
      "meta-learning",
      "L2P-GNN",
      "self-supervised learning",
      "dual adaptation mechanism",
      "node-level representations",
      "graph-level representations",
      "link prediction",
      "contrastive learning",
      "transferable knowledge",
      "rapid adaptation"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/94497472eecb7530a2b75c564548c540ebd61e9b.pdf",
    "citation_key": "lu20213kr",
    "metadata": {
      "title": "Learning to Pre-train Graph Neural Networks",
      "authors": [
        "Yuanfu Lu",
        "Xunqiang Jiang",
        "Yuan Fang",
        "C. Shi"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) have become the defacto standard for representation learning on graphs, which derive effective node representations by recursively aggregating information from graph neighborhoods. \nWhile GNNs can be trained from scratch, pre-training GNNs to learn transferable knowledge for downstream tasks has recently been demonstrated to improve the state of the art. \nHowever, conventional GNN pre-training methods follow a two-step paradigm: 1) pre-training on abundant unlabeled data and 2) fine-tuning on downstream labeled data, between which there exists a significant gap due to the divergence of optimization objectives in the two steps. \nIn this paper, we conduct an analysis to show the divergence between pre-training and fine-tuning, and to alleviate such divergence, we propose L2P-GNN, a self-supervised pre-training strategy for GNNs. \nThe key insight is that L2P-GNN attempts to learn how to fine-tune during the pre-training process in the form of transferable prior knowledge. To encode both local and global information into the prior, L2P-GNN is further designed with a dual adaptation mechanism at both node and graph levels. \nFinally, we conduct a systematic empirical study on the pre-training of various GNN models, using both a public collection of protein graphs and a new compilation of bibliographic graphs for pre-training. Experimental results show that L2P-GNN is capable of learning effective and transferable prior knowledge that yields powerful representations for downstream tasks. \n(Code and datasets are available at https://github.com/rootlu/L2P-GNN.)",
      "file_path": "paper_data/Graph_Neural_Networks/94497472eecb7530a2b75c564548c540ebd61e9b.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{lu20213kr} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses a \"significant gap\" in conventional Graph Neural Network (GNN) pre-training, which follows a two-step paradigm: 1) pre-training on abundant unlabeled data, and 2) fine-tuning on downstream labeled data. This gap arises from the divergence of optimization objectives between the two steps.\n    *   **Importance and Challenge**: Training GNNs typically requires extensive labeled data, which is often scarce and costly. While pre-training aims to learn transferable knowledge from unlabeled data, the objective mismatch between pre-training (optimizing for intrinsic graph properties) and fine-tuning (optimizing for downstream task performance) significantly hurts the generalization ability and quick adaptability of pre-trained GNN models. A key challenge is to develop a pre-training strategy that can simultaneously capture both local (node-level) and global (graph-level) information using *completely unlabeled* data, as existing methods either focus on node-level or require supervised information for graph-level pre-training.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{lu20213kr} positions itself against conventional GNN pre-training methods and draws inspiration from meta-learning, particularly Model-Agnostic Meta-Learning (MAML).\n    *   **Limitations of Previous Solutions**:\n        *   Prior GNN pre-training strategies (e.g., \\cite{hu2019graph}, \\cite{navarin2018graph}) are decoupled from the fine-tuning process, leading to a suboptimal initialization that is not inherently optimized for rapid adaptation to new tasks.\n        *   Existing methods for learning graph-level representations often require supervised information (e.g., \\cite{hu2020strategies}), failing to provide a fully self-supervised approach for both node and graph levels simultaneously.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{lu20213kr} proposes L2P-GNN (Learning to Pre-train GNNs), a self-supervised pre-training strategy that explicitly learns *how to fine-tune* during the pre-training process. This is achieved by framing the pre-training as an optimization-based meta-learning problem (similar to MAML). The pre-training objective is to find initial GNN parameters that, after a few gradient updates on a *simulated* support set (sampled from the pre-training graph), perform optimally on a *simulated* query set (also from the same pre-training graph).\n    *   **Novelty**:\n        *   The core innovation is the application of a meta-learning paradigm to GNN pre-training, directly optimizing for the model's ability to quickly adapt to new tasks, thereby alleviating the objective divergence.\n        *   It introduces a novel *dual adaptation mechanism* for completely self-supervised learning of both node-level and graph-level representations:\n            *   **Node-level adaptation**: Utilizes a self-supervised link prediction task, where the GNN learns to distinguish existing edges from randomly sampled non-edges.\n            *   **Graph-level adaptation**: Employs a contrastive learning approach, where the GNN is trained to maximize the mutual information between the representation of a whole graph and its sampled sub-structures, ensuring global context is preserved.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The first exploration of \"learning to pre-train GNNs\" to explicitly bridge the gap between pre-training and fine-tuning objectives \\cite{lu20213kr}.\n        *   A novel, completely self-supervised GNN pre-training strategy that simultaneously learns effective representations at both node and graph levels.\n        *   The design of a dual adaptation mechanism combining node-level link prediction and graph-level contrastive learning within a meta-learning framework.\n    *   **System Design or Architectural Innovations**: Integration of the MAML framework with self-supervised tasks specifically designed for graph-structured data.\n    *   **Theoretical Insights or Analysis**: Provides an analysis demonstrating the divergence in optimization objectives of conventional GNN pre-training and formally introduces a meta-learning objective to address it.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A systematic empirical study was performed to evaluate L2P-GNN's effectiveness across various GNN models.\n    *   **Key Performance Metrics and Comparison Results**: The evaluation involved pre-training on a public collection of protein graphs and a newly compiled large-scale bibliographic graph dataset. Experimental results demonstrate that L2P-GNN learns \"effective and transferable prior knowledge\" that yields \"powerful representations for downstream tasks,\" consistently and significantly outperforming state-of-the-art baselines.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: While not explicitly detailed in the provided text, meta-learning approaches like MAML can be computationally intensive due to the need for second-order gradients. The effectiveness of L2P-GNN relies on the quality and diversity of the simulated tasks constructed during pre-training.\n    *   **Scope of Applicability**: L2P-GNN is broadly applicable to various GNN architectures and is particularly beneficial in scenarios where labeled data for downstream tasks is scarce, but abundant unlabeled graph data is available for pre-training. It supports both node-level and graph-level downstream tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of the Technical State-of-the-Art**: \\cite{lu20213kr} significantly advances GNN pre-training by shifting the paradigm from decoupled pre-training to a meta-learning approach that directly optimizes for transferability and rapid adaptation. This addresses a fundamental limitation of previous two-step pre-training strategies.\n    *   **Potential Impact on Future Research**: This work opens new research directions for GNNs, encouraging further exploration of meta-learning techniques for graph representation learning, the development of more sophisticated self-supervised tasks for multi-level graph understanding, and the creation of more robust and generalizable GNN models for real-world applications with limited supervision.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN pre-training",
        "objective divergence",
        "meta-learning",
        "L2P-GNN",
        "self-supervised learning",
        "dual adaptation mechanism",
        "node-level representations",
        "graph-level representations",
        "link prediction",
        "contrastive learning",
        "transferable knowledge",
        "rapid adaptation"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract keywords:** \"we propose l2p-gnn, a self-supervised pre-training strategy for gnns,\" \"l2p-gnn is further designed with a dual adaptation mechanism.\" these phrases directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n*   **introduction focus:** the introduction sets up a problem with existing gnn pre-training methods and implicitly leads to the need for a new solution, which the abstract then explicitly states is proposed in the paper.\n*   **primary contribution:** the core contribution is the development and proposal of a novel pre-training strategy (l2p-gnn). while it includes an \"empirical study\" to validate the proposed method, the *creation* of the method itself is the central focus. many technical papers include empirical evaluations to demonstrate the effectiveness of their proposed solutions."
    },
    "file_name": "94497472eecb7530a2b75c564548c540ebd61e9b.pdf"
  },
  {
    "success": true,
    "doc_id": "e92cf2c13a676fe98e20d3405f3591eb",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Graph Neural Networks (GNNs) learn spurious correlations between input graphs and labels when trained on biased datasets, leading to poor generalization capability on unseen, unbiased data \\cite{fan2022m67}. This is particularly pronounced in \"severe bias\" scenarios where bias substructures are easier for GNNs to learn than true causal substructures.\n    *   **Importance and Challenge**: Graph classification tasks often rely on identifying relevant \"causal substructures\" (e.g., functional groups in molecules, digit subgraphs in images). However, real-world graph data is often inevitably biased, containing \"meaningless substructures spuriously correlated with labels\" \\cite{fan2022m67}. The challenge lies in disentangling these causal and bias substructures from entangled graphs, especially when bias is severe and dominates GNN learning, and then ensuring GNNs only utilize the causal information for stable prediction.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work relates to GNN generalization in Out-of-Distribution (OOD) environments, disentangled GNNs, and general debiasing methods.\n    *   **Limitations of Previous Solutions**:\n        *   Existing OOD GNN methods (e.g., StableGNN, OOD-GNN, DIR) are not specifically designed for datasets with *severe bias*, which presents a more challenging scenario for generalization \\cite{fan2022m67}.\n        *   Prior disentangled GNNs (e.g., DisenGCN, IPGDN, FactorGCN) focus on node-level or general graph factorization but do not explicitly consider disentangling *causal and bias information* \\cite{fan2022m67}.\n        *   General debiasing methods, often designed for image datasets, cannot effectively extract causal substructures from graph data \\cite{fan2022m67}. This paper is distinct in addressing the severe bias problem specifically for graph data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes DisC (Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure), a general disentangled GNN framework.\n        1.  **Causal and Bias Substructure Generator**: A parameterized edge mask generator (an MLP) explicitly splits the input graph into causal and bias subgraphs by assigning probabilities to each edge for being part of the causal subgraph. This generator takes a global view across all edges in a graph and across the entire graph population to identify statistical causal/bias information \\cite{fan2022m67}.\n        2.  **Learning Disentangled Representations**: Two separate GNN modules (a \"Causal GNN\" and a \"Bias GNN\") are trained on the respective masked subgraphs.\n            *   The Bias GNN is supervised by a \"bias-aware\" Generalized Cross Entropy (GCE) loss to amplify the bias, making it easier to learn the bias substructure.\n            *   The Causal GNN is supervised by a \"causal-aware\" (weighted cross-entropy) loss, focusing on samples where the Bias GNN struggles, thereby encouraging it to learn the true causal signals \\cite{fan2022m67}.\n        3.  **Decorrelating Causal and Bias Variables**: After training, the disentangled bias representations from different training graphs are randomly permuted to synthesize counterfactual unbiased training samples in the embedding space. This process removes the spurious correlation between causal and bias variables, allowing the model to concentrate on the true correlation between causal subgraphs and labels \\cite{fan2022m67}.\n    *   **Novelty**: The approach is novel in explicitly disentangling causal and bias substructures in graphs using a parameterized edge mask generator, employing a dual GNN architecture with specialized loss functions to amplify/mitigate bias, and further decorrelating these disentangled representations through counterfactual sample generation in the embedding space \\cite{fan2022m67}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel GNN framework, DisC, for disentangling causal substructures from biased graphs.\n        *   A parameterized edge mask generator that explicitly splits graphs into causal and bias subgraphs, offering a global view for edge importance and substructure identification \\cite{fan2022m67}.\n        *   A dual GNN training strategy using \"causal-aware\" (weighted CE) and \"bias-aware\" (GCE) loss functions to guide the disentanglement process \\cite{fan2022m67}.\n        *   A method for synthesizing counterfactual unbiased samples by permuting bias representations to decorrelate causal and bias variables in the embedding space \\cite{fan2022m67}.\n    *   **Theoretical Insights/Analysis**: Systematic analysis of bias impact on GNNs from both experimental study and a causal perspective (using Structural Causal Models), identifying two unblocked paths that induce spurious correlation and proposing solutions to intercept them \\cite{fan2022m67}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments to demonstrate the impact of severe bias on GNN generalization and to validate the effectiveness of DisC.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**: Three new graph datasets with controllable bias degrees were constructed (e.g., CMNIST-75sp, which converts biased MNIST images into superpixel graphs) to benchmark the severe bias problem \\cite{fan2022m67}.\n        *   **Results**: DisC achieved superior generalization performance over existing baselines, showing \"large margins\" of improvement (from 4.47% to 169.17% average improvements) \\cite{fan2022m67}.\n        *   **Interpretability and Transferability**: The learned edge mask provides appealing interpretability, demonstrating that the model could discover and leverage causal substructures for prediction. The model also exhibits transferability.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method assumes that causal and bias substructures can be disentangled via edge masking and that their correlation can be removed through permutation in the latent space. The effectiveness relies on the design of the edge mask generator and the specific loss functions.\n    *   **Scope of Applicability**: The framework is designed for graph classification tasks where a \"relevant substructure\" determines the label and \"meaningless substructures spuriously correlate\" with labels \\cite{fan2022m67}. It is particularly aimed at scenarios with *severe bias*.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work is the first to systematically study the generalization problem of GNNs in the challenging scenario of *severe bias* \\cite{fan2022m67}. It provides a robust framework for GNNs to learn stable causal relationships, significantly improving generalization ability in biased environments.\n    *   **Potential Impact**: DisC offers a flexible, interpretable, robust, and transferable solution for debiasing GNNs, which is crucial for real-world applications where data bias is prevalent. It paves the way for future research into more robust and causally-aware GNN architectures and training strategies, especially in critical domains like drug discovery or social network analysis where identifying true causal patterns is paramount.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) often learn spurious correlations from biased datasets, severely hindering their generalization to Out-of-Distribution (OOD) data. This problem is particularly acute under *severe bias*, where irrelevant substructures dominate GNN learning. We introduce DisC (Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure), a novel framework to robustly disentangle causal and bias information within graphs.\n\nDisC leverages a parameterized **edge mask generator** to explicitly split input graphs into causal and bias subgraphs. A dual GNN architecture learns specialized representations, guided by 'causal-aware' and 'bias-aware' loss functions. Crucially, we synthesize **counterfactual unbiased samples** by permuting bias representations, systematically decorrelating spurious associations and focusing the model on genuine causal signals.\n\nExtensive experiments on severely biased graph datasets demonstrate DisC's superior **generalization performance**, achieving up to 169% improvement over state-of-the-art baselines. Our approach provides appealing interpretability by highlighting causal substructures and offers a robust, transferable solution for building trustworthy GNNs in real-world applications. DisC paves the way for truly causally-aware GNNs.",
    "keywords": [
      "Severe bias in Graph Neural Networks",
      "causal substructure disentanglement",
      "spurious correlations",
      "DisC framework",
      "parameterized edge mask generator",
      "dual GNN architecture",
      "counterfactual sample generation",
      "debiasing GNNs",
      "Out-of-Distribution generalization",
      "graph classification tasks",
      "interpretability",
      "Structural Causal Models"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/24b2aed0f130e5278325b5055711de44d247460e.pdf",
    "citation_key": "fan2022m67",
    "metadata": {
      "title": "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure",
      "authors": [
        "Shaohua Fan",
        "Xiao Wang",
        "Yanhu Mo",
        "Chuan Shi",
        "Jian Tang"
      ],
      "published_date": "2022",
      "abstract": "Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability. By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspiring by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly, we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability. Code and data are available at: https://github.com/googlebaba/DisC.",
      "file_path": "paper_data/Graph_Neural_Networks/24b2aed0f130e5278325b5055711de44d247460e.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Graph Neural Networks (GNNs) learn spurious correlations between input graphs and labels when trained on biased datasets, leading to poor generalization capability on unseen, unbiased data \\cite{fan2022m67}. This is particularly pronounced in \"severe bias\" scenarios where bias substructures are easier for GNNs to learn than true causal substructures.\n    *   **Importance and Challenge**: Graph classification tasks often rely on identifying relevant \"causal substructures\" (e.g., functional groups in molecules, digit subgraphs in images). However, real-world graph data is often inevitably biased, containing \"meaningless substructures spuriously correlated with labels\" \\cite{fan2022m67}. The challenge lies in disentangling these causal and bias substructures from entangled graphs, especially when bias is severe and dominates GNN learning, and then ensuring GNNs only utilize the causal information for stable prediction.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work relates to GNN generalization in Out-of-Distribution (OOD) environments, disentangled GNNs, and general debiasing methods.\n    *   **Limitations of Previous Solutions**:\n        *   Existing OOD GNN methods (e.g., StableGNN, OOD-GNN, DIR) are not specifically designed for datasets with *severe bias*, which presents a more challenging scenario for generalization \\cite{fan2022m67}.\n        *   Prior disentangled GNNs (e.g., DisenGCN, IPGDN, FactorGCN) focus on node-level or general graph factorization but do not explicitly consider disentangling *causal and bias information* \\cite{fan2022m67}.\n        *   General debiasing methods, often designed for image datasets, cannot effectively extract causal substructures from graph data \\cite{fan2022m67}. This paper is distinct in addressing the severe bias problem specifically for graph data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes DisC (Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure), a general disentangled GNN framework.\n        1.  **Causal and Bias Substructure Generator**: A parameterized edge mask generator (an MLP) explicitly splits the input graph into causal and bias subgraphs by assigning probabilities to each edge for being part of the causal subgraph. This generator takes a global view across all edges in a graph and across the entire graph population to identify statistical causal/bias information \\cite{fan2022m67}.\n        2.  **Learning Disentangled Representations**: Two separate GNN modules (a \"Causal GNN\" and a \"Bias GNN\") are trained on the respective masked subgraphs.\n            *   The Bias GNN is supervised by a \"bias-aware\" Generalized Cross Entropy (GCE) loss to amplify the bias, making it easier to learn the bias substructure.\n            *   The Causal GNN is supervised by a \"causal-aware\" (weighted cross-entropy) loss, focusing on samples where the Bias GNN struggles, thereby encouraging it to learn the true causal signals \\cite{fan2022m67}.\n        3.  **Decorrelating Causal and Bias Variables**: After training, the disentangled bias representations from different training graphs are randomly permuted to synthesize counterfactual unbiased training samples in the embedding space. This process removes the spurious correlation between causal and bias variables, allowing the model to concentrate on the true correlation between causal subgraphs and labels \\cite{fan2022m67}.\n    *   **Novelty**: The approach is novel in explicitly disentangling causal and bias substructures in graphs using a parameterized edge mask generator, employing a dual GNN architecture with specialized loss functions to amplify/mitigate bias, and further decorrelating these disentangled representations through counterfactual sample generation in the embedding space \\cite{fan2022m67}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel GNN framework, DisC, for disentangling causal substructures from biased graphs.\n        *   A parameterized edge mask generator that explicitly splits graphs into causal and bias subgraphs, offering a global view for edge importance and substructure identification \\cite{fan2022m67}.\n        *   A dual GNN training strategy using \"causal-aware\" (weighted CE) and \"bias-aware\" (GCE) loss functions to guide the disentanglement process \\cite{fan2022m67}.\n        *   A method for synthesizing counterfactual unbiased samples by permuting bias representations to decorrelate causal and bias variables in the embedding space \\cite{fan2022m67}.\n    *   **Theoretical Insights/Analysis**: Systematic analysis of bias impact on GNNs from both experimental study and a causal perspective (using Structural Causal Models), identifying two unblocked paths that induce spurious correlation and proposing solutions to intercept them \\cite{fan2022m67}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments to demonstrate the impact of severe bias on GNN generalization and to validate the effectiveness of DisC.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**: Three new graph datasets with controllable bias degrees were constructed (e.g., CMNIST-75sp, which converts biased MNIST images into superpixel graphs) to benchmark the severe bias problem \\cite{fan2022m67}.\n        *   **Results**: DisC achieved superior generalization performance over existing baselines, showing \"large margins\" of improvement (from 4.47% to 169.17% average improvements) \\cite{fan2022m67}.\n        *   **Interpretability and Transferability**: The learned edge mask provides appealing interpretability, demonstrating that the model could discover and leverage causal substructures for prediction. The model also exhibits transferability.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method assumes that causal and bias substructures can be disentangled via edge masking and that their correlation can be removed through permutation in the latent space. The effectiveness relies on the design of the edge mask generator and the specific loss functions.\n    *   **Scope of Applicability**: The framework is designed for graph classification tasks where a \"relevant substructure\" determines the label and \"meaningless substructures spuriously correlate\" with labels \\cite{fan2022m67}. It is particularly aimed at scenarios with *severe bias*.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work is the first to systematically study the generalization problem of GNNs in the challenging scenario of *severe bias* \\cite{fan2022m67}. It provides a robust framework for GNNs to learn stable causal relationships, significantly improving generalization ability in biased environments.\n    *   **Potential Impact**: DisC offers a flexible, interpretable, robust, and transferable solution for debiasing GNNs, which is crucial for real-world applications where data bias is prevalent. It paves the way for future research into more robust and causally-aware GNN architectures and training strategies, especially in critical domains like drug discovery or social network analysis where identifying true causal patterns is paramount.",
      "keywords": [
        "Severe bias in Graph Neural Networks",
        "causal substructure disentanglement",
        "spurious correlations",
        "DisC framework",
        "parameterized edge mask generator",
        "dual GNN architecture",
        "counterfactual sample generation",
        "debiasing GNNs",
        "Out-of-Distribution generalization",
        "graph classification tasks",
        "interpretability",
        "Structural Causal Models"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"**we propose a general disentangled gnn framework**\", \"**we design a parameterized edge mask generator**\", and \"**we synthesize the counterfactual unbiased training samples**\". these phrases directly indicate the development and presentation of new methods and systems.\n*   the introduction sets up a technical problem (gnns suffering from bias and exploring spurious correlations) that the proposed framework aims to solve.\n*   while the paper also mentions \"**experimental results** well demonstrate that our approach achieves superior generalization performance\" and \"**we construct three new graph datasets**\", indicating an empirical component, the primary contribution described is the *creation and design* of a new method/framework. empirical results are used to validate this technical contribution.\n\ntherefore, the paper best fits the **technical** classification."
    },
    "file_name": "24b2aed0f130e5278325b5055711de44d247460e.pdf"
  },
  {
    "success": true,
    "doc_id": "63fecd4b0d7cdb5f45a03a677604c12b",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the problem of backdoor attacks against Graph Neural Networks (GNNs), specifically in the context of graph classification.\n    *   **Importance and Challenge**: GNNs are increasingly used in security-critical applications like fraud and malware detection. An attacker could exploit vulnerabilities to evade detection. Existing adversarial attacks on GNNs primarily focus on node classification or require optimized, per-instance perturbations (adversarial examples), which are less stealthy and effective than backdoor attacks. Backdoor attacks, while extensively studied in image domains, are unexplored for GNNs due due to the unique structural properties of graphs (e.g., no Cartesian coordinate system, varying sizes).\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work proposes the *first* backdoor attack to GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Most prior adversarial GNN research focused on node classification, not graph classification.\n        *   Existing adversarial examples for GNN-based graph classification \\cite{zhang2020b0m} require different, optimized perturbations for each testing graph and have limited success when the target GNN is unknown.\n        *   Backdoor attacks, common in image processing, had not been adapted to the unique challenges of graph data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a subgraph-based backdoor attack for GNN-based graph classification.\n        *   **Trigger Design**: A predefined subgraph pattern serves as the backdoor trigger.\n        *   **Trigger Synthesis**: Triggers are generated as random subgraphs using models like Erdős-Rényi (ER), Small World (SW), or Preferential Attachment (PA) to ensure uniqueness and stealth. ER is empirically found to be the most effective.\n        *   **Poisoning Phase**: An attacker injects this trigger subgraph into a subset of training graphs (controlled by `poisoning intensity` $\\gamma$) and changes their labels to an attacker-chosen `target label`.\n        *   **Training**: A GNN is trained on this `backdoored training dataset`, learning to associate the trigger with the target label.\n        *   **Attack Execution**: During inference, if the same trigger is injected into a testing graph, the `backdoored GNN` is highly likely to predict the target label.\n    *   **Novelty**: This is the first systematic study and practical implementation of backdoor attacks against GNNs. It introduces a novel method for designing and injecting subgraph triggers into graph data, along with parameters (`trigger size`, `trigger density`, `trigger synthesis method`, `poisoning intensity`) to characterize the attack.\n\n*   **Key Technical Contributions**\n    *   **Novel Attack Methodology**: Proposed the first subgraph-based backdoor attack for GNNs in graph classification, defining key parameters for trigger design and poisoning.\n    *   **Trigger Synthesis Exploration**: Investigated different generative models (ER, SW, PA) for creating stealthy and effective subgraph triggers, empirically demonstrating ER's superior performance.\n    *   **Defense Generalization**: Generalized a state-of-the-art certified defense, randomized smoothing (specifically randomized subsampling for binary data), to defend against these GNN backdoor attacks.\n    *   **Empirical Analysis of Defense Limitations**: Provided empirical evidence that while the generalized defense can be effective in some scenarios, it is notably ineffective against larger triggers, highlighting the critical need for new, GNN-specific defense mechanisms.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The attacks and the generalized defense were evaluated on three real-world graph datasets: Bitcoin (fraudulent transaction detection), Twitter (fake user detection), and COLLAB (scientific collaboration).\n    *   **Key Performance Metrics**:\n        *   **Attack Effectiveness**: Measured by the attack success rate (percentage of triggered testing graphs misclassified to the target label).\n        *   **Attack Stealthiness**: Measured by the impact on the GNN's prediction accuracy for *clean* testing graphs.\n        *   **Defense Effectiveness**: Measured by the reduction in attack success rate and the `certified trigger size` (provable robustness threshold).\n    *   **Comparison Results**:\n        *   **Attack**: Demonstrated high attack success rates (e.g., 90% on Twitter with a trigger size 30% of average nodes) with minimal impact on clean accuracy (e.g., 0.03 accuracy drop on Twitter) \\cite{zhang2020b0m}. ER-generated triggers were more effective than SW and PA.\n        *   **Defense**: Randomized subsampling could reduce attack success rates (e.g., 0.39 drop on Twitter with 0.02 clean accuracy drop for 20% trigger size). However, it was ineffective against larger triggers (e.g., <0.01 reduction on Twitter for 30% trigger size) due to achieving only small certified trigger sizes.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The generalized randomized smoothing defense is shown to be ineffective against larger triggers, achieving only small certified trigger sizes. The attack primarily focuses on subgraph injection and graph classification.\n    *   **Scope of Applicability**: The proposed attack and defense generalization are primarily applicable to GNNs used for graph classification tasks. The methods are described for undirected graphs but can be extended to directed graphs.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This paper makes a significant advancement by introducing the first systematic study of backdoor attacks against GNNs, opening a new critical area of research in GNN security.\n    *   **Potential Impact on Future Research**: It highlights a crucial vulnerability in GNNs, especially given their deployment in sensitive applications. The demonstrated limitations of existing certified defenses against these attacks strongly motivate future research into novel, robust, and GNN-specific defense mechanisms and detection strategies.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are increasingly deployed in security-critical domains like fraud and malware detection, yet their vulnerability to insidious backdoor attacks remains largely unexplored. This paper presents the *first systematic study* of backdoor attacks against GNNs in graph classification, revealing a profound and stealthy threat. Unlike conventional adversarial examples, our novel methodology embeds a predefined subgraph trigger into a subset of training graphs, poisoning the GNN to misclassify any future graph containing this trigger to an attacker-chosen target label.\n\nWe introduce a sophisticated subgraph-based poisoning strategy, exploring various generative models (e.g., Erdős-Rényi) to synthesize unique and stealthy triggers. Our extensive empirical validation on real-world datasets demonstrates high attack success rates (up to 90%) with negligible impact on the GNN's clean accuracy. Furthermore, we generalize a state-of-the-art certified defense, randomized smoothing, to the GNN context. Critically, our findings show this defense offers only limited provable robustness, proving ineffective against larger triggers. This pioneering work exposes a significant, previously unaddressed vulnerability in GNNs, demanding urgent research into robust, GNN-specific defense mechanisms and advancing the frontier of graph machine learning security.",
    "keywords": [
      "Backdoor attacks",
      "Graph Neural Networks (GNNs)",
      "Graph classification",
      "Subgraph-based backdoor attack",
      "Trigger design and synthesis",
      "Poisoning attacks",
      "Erdős-Rényi (ER) triggers",
      "Randomized smoothing",
      "Certified defense limitations",
      "GNN security vulnerability",
      "Attack success rate",
      "Security-critical applications",
      "Novel attack methodology"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf",
    "citation_key": "zhang2020b0m",
    "metadata": {
      "title": "Backdoor Attacks to Graph Neural Networks",
      "authors": [
        "Zaixi Zhang",
        "Jinyuan Jia",
        "Binghui Wang",
        "N. Gong"
      ],
      "published_date": "2020",
      "abstract": "In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a subgraph based backdoor attack to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
      "file_path": "paper_data/Graph_Neural_Networks/bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf",
      "venue": "ACM Symposium on Access Control Models and Technologies",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the problem of backdoor attacks against Graph Neural Networks (GNNs), specifically in the context of graph classification.\n    *   **Importance and Challenge**: GNNs are increasingly used in security-critical applications like fraud and malware detection. An attacker could exploit vulnerabilities to evade detection. Existing adversarial attacks on GNNs primarily focus on node classification or require optimized, per-instance perturbations (adversarial examples), which are less stealthy and effective than backdoor attacks. Backdoor attacks, while extensively studied in image domains, are unexplored for GNNs due due to the unique structural properties of graphs (e.g., no Cartesian coordinate system, varying sizes).\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work proposes the *first* backdoor attack to GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Most prior adversarial GNN research focused on node classification, not graph classification.\n        *   Existing adversarial examples for GNN-based graph classification \\cite{zhang2020b0m} require different, optimized perturbations for each testing graph and have limited success when the target GNN is unknown.\n        *   Backdoor attacks, common in image processing, had not been adapted to the unique challenges of graph data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a subgraph-based backdoor attack for GNN-based graph classification.\n        *   **Trigger Design**: A predefined subgraph pattern serves as the backdoor trigger.\n        *   **Trigger Synthesis**: Triggers are generated as random subgraphs using models like Erdős-Rényi (ER), Small World (SW), or Preferential Attachment (PA) to ensure uniqueness and stealth. ER is empirically found to be the most effective.\n        *   **Poisoning Phase**: An attacker injects this trigger subgraph into a subset of training graphs (controlled by `poisoning intensity` $\\gamma$) and changes their labels to an attacker-chosen `target label`.\n        *   **Training**: A GNN is trained on this `backdoored training dataset`, learning to associate the trigger with the target label.\n        *   **Attack Execution**: During inference, if the same trigger is injected into a testing graph, the `backdoored GNN` is highly likely to predict the target label.\n    *   **Novelty**: This is the first systematic study and practical implementation of backdoor attacks against GNNs. It introduces a novel method for designing and injecting subgraph triggers into graph data, along with parameters (`trigger size`, `trigger density`, `trigger synthesis method`, `poisoning intensity`) to characterize the attack.\n\n*   **Key Technical Contributions**\n    *   **Novel Attack Methodology**: Proposed the first subgraph-based backdoor attack for GNNs in graph classification, defining key parameters for trigger design and poisoning.\n    *   **Trigger Synthesis Exploration**: Investigated different generative models (ER, SW, PA) for creating stealthy and effective subgraph triggers, empirically demonstrating ER's superior performance.\n    *   **Defense Generalization**: Generalized a state-of-the-art certified defense, randomized smoothing (specifically randomized subsampling for binary data), to defend against these GNN backdoor attacks.\n    *   **Empirical Analysis of Defense Limitations**: Provided empirical evidence that while the generalized defense can be effective in some scenarios, it is notably ineffective against larger triggers, highlighting the critical need for new, GNN-specific defense mechanisms.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The attacks and the generalized defense were evaluated on three real-world graph datasets: Bitcoin (fraudulent transaction detection), Twitter (fake user detection), and COLLAB (scientific collaboration).\n    *   **Key Performance Metrics**:\n        *   **Attack Effectiveness**: Measured by the attack success rate (percentage of triggered testing graphs misclassified to the target label).\n        *   **Attack Stealthiness**: Measured by the impact on the GNN's prediction accuracy for *clean* testing graphs.\n        *   **Defense Effectiveness**: Measured by the reduction in attack success rate and the `certified trigger size` (provable robustness threshold).\n    *   **Comparison Results**:\n        *   **Attack**: Demonstrated high attack success rates (e.g., 90% on Twitter with a trigger size 30% of average nodes) with minimal impact on clean accuracy (e.g., 0.03 accuracy drop on Twitter) \\cite{zhang2020b0m}. ER-generated triggers were more effective than SW and PA.\n        *   **Defense**: Randomized subsampling could reduce attack success rates (e.g., 0.39 drop on Twitter with 0.02 clean accuracy drop for 20% trigger size). However, it was ineffective against larger triggers (e.g., <0.01 reduction on Twitter for 30% trigger size) due to achieving only small certified trigger sizes.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The generalized randomized smoothing defense is shown to be ineffective against larger triggers, achieving only small certified trigger sizes. The attack primarily focuses on subgraph injection and graph classification.\n    *   **Scope of Applicability**: The proposed attack and defense generalization are primarily applicable to GNNs used for graph classification tasks. The methods are described for undirected graphs but can be extended to directed graphs.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This paper makes a significant advancement by introducing the first systematic study of backdoor attacks against GNNs, opening a new critical area of research in GNN security.\n    *   **Potential Impact on Future Research**: It highlights a crucial vulnerability in GNNs, especially given their deployment in sensitive applications. The demonstrated limitations of existing certified defenses against these attacks strongly motivate future research into novel, robust, and GNN-specific defense mechanisms and detection strategies.",
      "keywords": [
        "Backdoor attacks",
        "Graph Neural Networks (GNNs)",
        "Graph classification",
        "Subgraph-based backdoor attack",
        "Trigger design and synthesis",
        "Poisoning attacks",
        "Erdős-Rényi (ER) triggers",
        "Randomized smoothing",
        "Certified defense limitations",
        "GNN security vulnerability",
        "Attack success rate",
        "Security-critical applications",
        "Novel attack methodology"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose\"**: the abstract explicitly states, \"in this work, we **propose** the first backdoor attack to graph neural networks (gnn). specifically, we **propose** a subgraph based backdoor attack...\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems.\n2.  **new method/system**: the core contribution is the development of a novel backdoor attack mechanism for gnns.\n3.  **empirical validation**: while the abstract mentions \"our empirical results on three real-world graph datasets show...\", this is the validation of the *proposed* technical solution. many technical papers include empirical sections to demonstrate the effectiveness and performance of their new methods. the empirical results are in service of proving the efficacy of the proposed attack and defense generalization, rather than being the sole focus of studying an existing phenomenon."
    },
    "file_name": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf"
  },
  {
    "success": true,
    "doc_id": "f9724ab304666568e46777d945870c2f",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### BrainGB: A Benchmark for Brain Network Analysis with Graph Neural Networks \\cite{cui2022mjr}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: There is a lack of systematic study and a public benchmark for designing and evaluating effective Graph Neural Networks (GNNs) for brain network analysis \\cite{cui2022mjr}. Existing GNN-based models for brain analysis are often tested on specific, private datasets, leading to irreproducibility \\cite{cui2022mjr}. Shallow models are inadequate for the sophisticated connectome structures of brain networks \\cite{cui2022mjr}.\n    *   **Importance and Challenge**: Understanding brain networks is crucial for neurobiological research, mental disorder therapy, and AI \\cite{cui2022mjr}. GNNs show great promise for complex networked data, but brain networks have unique characteristics (e.g., lack of useful initial node features, real-valued/signed connection weights, fixed ROI identities) that require customized GNN designs \\cite{cui2022mjr}. Challenges include restricted data accessibility, complex and modality-specific neuroimaging preprocessing pipelines, and the absence of standardized evaluation settings \\cite{cui2022mjr}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: BrainGB builds upon the general framework of GNNs for graph-structured data and previous efforts in brain network analysis using both shallow models and pioneering deep learning approaches \\cite{cui2022mjr}.\n    *   **Limitations of Previous Solutions**:\n        *   **Shallow Models**: Traditional graph theory and tensor factorization models are often inadequate for capturing the complex, sophisticated connectome structures of brain networks \\cite{cui2022mjr}.\n        *   **Pioneering Deep Models (GNNs)**: While some GNNs (e.g., BrainGNN, BrainNetCNN) have been proposed for brain disease prediction, they are typically experimented on specific, often private, datasets with undisclosed preprocessing details, making their results irreproducible and hindering broader research \\cite{cui2022mjr}.\n        *   **Lack of Standardization**: There is no unified public benchmark platform, standardized preprocessing pipelines, or fair experimental settings for evaluating GNNs in neuroimaging, which impedes methodological development \\cite{cui2022mjr}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: BrainGB is a novel benchmark platform designed to standardize and facilitate GNN-based brain network analysis \\cite{cui2022mjr}. Its core approach involves:\n        *   **Standardized Brain Network Construction**: Summarizing and providing detailed, step-by-step preprocessing and construction pipelines for both functional (fMRI) and structural (dMRI) brain networks from raw neuroimaging data \\cite{cui2022mjr}. This includes recommended software and quality control steps \\cite{cui2022mjr}.\n        *   **Modular GNN Design Space**: Decomposing the design space of GNNs for brain networks into four key modules: (a) node features, (b) message passing mechanisms, (c) attention mechanisms, and (d) pooling strategies \\cite{cui2022mjr}. This allows for systematic combination and evaluation of different GNN components.\n        *   **Unified, Reproducible Framework**: Establishing a unified, modular, scalable, and reproducible framework for fair evaluation, complete with accessible datasets, standard settings, and baseline models \\cite{cui2022mjr}.\n    *   **Novelty or Difference**:\n        *   **First Comprehensive Public Benchmark**: BrainGB is presented as the first comprehensive public benchmark for GNN-based brain network analysis, addressing a critical gap in the field \\cite{cui2022mjr}.\n        *   **Bridging Communities**: It explicitly aims to bridge the gap between the neuroimaging and deep learning communities by standardizing complex data preparation and GNN evaluation \\cite{cui2022mjr}.\n        *   **Systematic Design Exploration**: The modular decomposition of GNN components enables systematic exploration and identification of effective GNN architectures tailored to the unique characteristics of brain network data \\cite{cui2022mjr}.\n        *   **Open Science Initiative**: Provides open-source code, an out-of-box Python package, detailed tutorials, and a hosted website to support open and reproducible research, including a leaderboard for community contributions \\cite{cui2022mjr}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A unified, modular, scalable, and reproducible framework for GNN-based brain network analysis \\cite{cui2022mjr}.\n        *   Standardized and publicly documented preprocessing and construction pipelines for both functional and structural brain networks, making complex neuroimaging data accessible for deep learning research \\cite{cui2022mjr}.\n        *   A systematic decomposition of the GNN design space into four key modules (node features, message passing, attention, pooling) to guide model development and evaluation \\cite{cui2022mjr}.\n    *   **System Design or Architectural Innovations**:\n        *   The BrainGB platform itself, as a structured environment for GNN development and evaluation in neuroimaging, represents a significant system design contribution \\cite{cui2022mjr}.\n        *   Provision of an out-of-box Python package and a hosted website (`https://braingb.us`) with models, tutorials, and examples to facilitate ease of use and community engagement \\cite{cui2022mjr}.\n    *   **Theoretical Insights or Analysis**: The paper primarily focuses on empirical evidence and practical recipes for GNN design, rather than theoretical insights \\cite{cui2022mjr}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted extensive experiments on datasets across various cohorts and modalities \\cite{cui2022mjr}.\n    *   **Key Performance Metrics and Comparison Results**: Based on these experiments, BrainGB recommends a set of general recipes for effective GNN designs on brain networks \\cite{cui2022mjr}. (Specific metrics and detailed comparison results are not provided in the abstract/introduction but are implied to be part of the full paper's experimental section.)\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The benchmark primarily focuses on MRI-derived brain networks (fMRI and dMRI) \\cite{cui2022mjr}.\n        *   Brain networks often lack useful initial node (ROI) features, requiring specific GNN design considerations \\cite{cui2022mjr}.\n        *   Edge weights are real-valued, potentially dense, and noisy, which GNNs must handle \\cite{cui2022mjr}.\n        *   ROI identities and their orders are fixed across subjects within a dataset, a unique characteristic compared to other graph data \\cite{cui2022mjr}.\n    *   **Scope of Applicability**: BrainGB is applicable to GNN-based brain network analysis tasks, such as classifying individuals or predicting clinical outcomes (e.g., neurological diseases, gender) using functional and structural brain connectivity \\cite{cui2022mjr}.\n\n7.  **Technical Significance**\n    *   **Advances the Technical State-of-the-Art**:\n        *   Establishes the first comprehensive and public benchmark for GNNs in brain network analysis, addressing a critical gap in reproducibility and systematic evaluation \\cite{cui2022mjr}.\n        *   Significantly lowers the barrier for deep learning researchers to engage with complex neuroimaging data by providing standardized preprocessing pipelines and a modular GNN framework \\cite{cui2022mjr}.\n        *   Offers empirical evidence and \"general recipes\" for effective GNN designs, guiding future model development in this specialized domain \\cite{cui2022mjr}.\n    *   **Potential Impact on Future Research**:\n        *   Accelerates research and development of GNN-based methods for neuroimaging by providing a common, reproducible platform for comparison and innovation \\cite{cui2022mjr}.\n        *   Fosters greater collaboration and knowledge exchange between the neuroimaging and deep learning communities \\cite{cui2022mjr}.\n        *   Facilitates the discovery of neurological biomarkers, early diagnosis of disorders, and a deeper understanding of brain function and structure \\cite{cui2022mjr}.",
    "intriguing_abstract": "Unlocking the mysteries of the human brain through its intricate connectome is a grand challenge, with Graph Neural Networks (GNNs) showing immense promise for analyzing complex brain networks. However, the field of GNN-based neuroimaging has been plagued by irreproducibility and a critical lack of standardized evaluation, hindering systematic progress and the discovery of robust biomarkers.\n\nWe introduce **BrainGB**, the first comprehensive public benchmark designed to revolutionize GNN-based brain network analysis. BrainGB provides meticulously standardized preprocessing pipelines for both functional (fMRI) and structural (dMRI) neuroimaging data, transforming raw scans into ready-to-use brain networks. Crucially, it decomposes the GNN design space into modular components—node features, message passing mechanisms, attention mechanisms, and pooling strategies—enabling systematic exploration and tailored model development. This unified, open-source framework, complete with an out-of-box Python package and a community leaderboard, bridges the gap between neuroimaging and deep learning. BrainGB accelerates reproducible research, fosters innovation in brain disease prediction, and offers empirical \"recipes\" for effective GNN architectures, serving as an indispensable resource for advancing our understanding of brain function and pathology.",
    "keywords": [
      "BrainGB benchmark",
      "Graph Neural Networks (GNNs)",
      "brain network analysis",
      "standardized preprocessing pipelines",
      "functional and structural brain networks (fMRI/dMRI)",
      "modular GNN design space",
      "reproducible research framework",
      "connectome structures",
      "neuroimaging-deep learning bridge",
      "GNN design modules",
      "neurological disorder prediction",
      "open science platform",
      "empirical GNN design recipes"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf",
    "citation_key": "cui2022mjr",
    "metadata": {
      "title": "BrainGB: A Benchmark for Brain Network Analysis With Graph Neural Networks",
      "authors": [
        "Hejie Cui",
        "Wei Dai",
        "Yanqiao Zhu",
        "Xuan Kan",
        "Antonio Aodong Chen Gu",
        "Joshua Lukemire",
        "L. Zhan",
        "Lifang He",
        "Ying Guo",
        "Carl Yang"
      ],
      "published_date": "2022",
      "abstract": "Mapping the connectome of the human brain using structural or functional connectivity has become one of the most pervasive paradigms for neuroimaging analysis. Recently, Graph Neural Networks (GNNs) motivated from geometric deep learning have attracted broad interest due to their established power for modeling complex networked data. Despite their superior performance in many fields, there has not yet been a systematic study of how to design effective GNNs for brain network analysis. To bridge this gap, we present BrainGB, a benchmark for brain network analysis with GNNs. BrainGB standardizes the process by (1) summarizing brain network construction pipelines for both functional and structural neuroimaging modalities and (2) modularizing the implementation of GNN designs. We conduct extensive experiments on datasets across cohorts and modalities and recommend a set of general recipes for effective GNN designs on brain networks. To support open and reproducible research on GNN-based brain network analysis, we host the BrainGB website at https://braingb.us with models, tutorials, examples, as well as an out-of-box Python package. We hope that this work will provide useful empirical evidence and offer insights for future research in this novel and promising direction.",
      "file_path": "paper_data/Graph_Neural_Networks/3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf",
      "venue": "IEEE Transactions on Medical Imaging",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### BrainGB: A Benchmark for Brain Network Analysis with Graph Neural Networks \\cite{cui2022mjr}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: There is a lack of systematic study and a public benchmark for designing and evaluating effective Graph Neural Networks (GNNs) for brain network analysis \\cite{cui2022mjr}. Existing GNN-based models for brain analysis are often tested on specific, private datasets, leading to irreproducibility \\cite{cui2022mjr}. Shallow models are inadequate for the sophisticated connectome structures of brain networks \\cite{cui2022mjr}.\n    *   **Importance and Challenge**: Understanding brain networks is crucial for neurobiological research, mental disorder therapy, and AI \\cite{cui2022mjr}. GNNs show great promise for complex networked data, but brain networks have unique characteristics (e.g., lack of useful initial node features, real-valued/signed connection weights, fixed ROI identities) that require customized GNN designs \\cite{cui2022mjr}. Challenges include restricted data accessibility, complex and modality-specific neuroimaging preprocessing pipelines, and the absence of standardized evaluation settings \\cite{cui2022mjr}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: BrainGB builds upon the general framework of GNNs for graph-structured data and previous efforts in brain network analysis using both shallow models and pioneering deep learning approaches \\cite{cui2022mjr}.\n    *   **Limitations of Previous Solutions**:\n        *   **Shallow Models**: Traditional graph theory and tensor factorization models are often inadequate for capturing the complex, sophisticated connectome structures of brain networks \\cite{cui2022mjr}.\n        *   **Pioneering Deep Models (GNNs)**: While some GNNs (e.g., BrainGNN, BrainNetCNN) have been proposed for brain disease prediction, they are typically experimented on specific, often private, datasets with undisclosed preprocessing details, making their results irreproducible and hindering broader research \\cite{cui2022mjr}.\n        *   **Lack of Standardization**: There is no unified public benchmark platform, standardized preprocessing pipelines, or fair experimental settings for evaluating GNNs in neuroimaging, which impedes methodological development \\cite{cui2022mjr}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: BrainGB is a novel benchmark platform designed to standardize and facilitate GNN-based brain network analysis \\cite{cui2022mjr}. Its core approach involves:\n        *   **Standardized Brain Network Construction**: Summarizing and providing detailed, step-by-step preprocessing and construction pipelines for both functional (fMRI) and structural (dMRI) brain networks from raw neuroimaging data \\cite{cui2022mjr}. This includes recommended software and quality control steps \\cite{cui2022mjr}.\n        *   **Modular GNN Design Space**: Decomposing the design space of GNNs for brain networks into four key modules: (a) node features, (b) message passing mechanisms, (c) attention mechanisms, and (d) pooling strategies \\cite{cui2022mjr}. This allows for systematic combination and evaluation of different GNN components.\n        *   **Unified, Reproducible Framework**: Establishing a unified, modular, scalable, and reproducible framework for fair evaluation, complete with accessible datasets, standard settings, and baseline models \\cite{cui2022mjr}.\n    *   **Novelty or Difference**:\n        *   **First Comprehensive Public Benchmark**: BrainGB is presented as the first comprehensive public benchmark for GNN-based brain network analysis, addressing a critical gap in the field \\cite{cui2022mjr}.\n        *   **Bridging Communities**: It explicitly aims to bridge the gap between the neuroimaging and deep learning communities by standardizing complex data preparation and GNN evaluation \\cite{cui2022mjr}.\n        *   **Systematic Design Exploration**: The modular decomposition of GNN components enables systematic exploration and identification of effective GNN architectures tailored to the unique characteristics of brain network data \\cite{cui2022mjr}.\n        *   **Open Science Initiative**: Provides open-source code, an out-of-box Python package, detailed tutorials, and a hosted website to support open and reproducible research, including a leaderboard for community contributions \\cite{cui2022mjr}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A unified, modular, scalable, and reproducible framework for GNN-based brain network analysis \\cite{cui2022mjr}.\n        *   Standardized and publicly documented preprocessing and construction pipelines for both functional and structural brain networks, making complex neuroimaging data accessible for deep learning research \\cite{cui2022mjr}.\n        *   A systematic decomposition of the GNN design space into four key modules (node features, message passing, attention, pooling) to guide model development and evaluation \\cite{cui2022mjr}.\n    *   **System Design or Architectural Innovations**:\n        *   The BrainGB platform itself, as a structured environment for GNN development and evaluation in neuroimaging, represents a significant system design contribution \\cite{cui2022mjr}.\n        *   Provision of an out-of-box Python package and a hosted website (`https://braingb.us`) with models, tutorials, and examples to facilitate ease of use and community engagement \\cite{cui2022mjr}.\n    *   **Theoretical Insights or Analysis**: The paper primarily focuses on empirical evidence and practical recipes for GNN design, rather than theoretical insights \\cite{cui2022mjr}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted extensive experiments on datasets across various cohorts and modalities \\cite{cui2022mjr}.\n    *   **Key Performance Metrics and Comparison Results**: Based on these experiments, BrainGB recommends a set of general recipes for effective GNN designs on brain networks \\cite{cui2022mjr}. (Specific metrics and detailed comparison results are not provided in the abstract/introduction but are implied to be part of the full paper's experimental section.)\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The benchmark primarily focuses on MRI-derived brain networks (fMRI and dMRI) \\cite{cui2022mjr}.\n        *   Brain networks often lack useful initial node (ROI) features, requiring specific GNN design considerations \\cite{cui2022mjr}.\n        *   Edge weights are real-valued, potentially dense, and noisy, which GNNs must handle \\cite{cui2022mjr}.\n        *   ROI identities and their orders are fixed across subjects within a dataset, a unique characteristic compared to other graph data \\cite{cui2022mjr}.\n    *   **Scope of Applicability**: BrainGB is applicable to GNN-based brain network analysis tasks, such as classifying individuals or predicting clinical outcomes (e.g., neurological diseases, gender) using functional and structural brain connectivity \\cite{cui2022mjr}.\n\n7.  **Technical Significance**\n    *   **Advances the Technical State-of-the-Art**:\n        *   Establishes the first comprehensive and public benchmark for GNNs in brain network analysis, addressing a critical gap in reproducibility and systematic evaluation \\cite{cui2022mjr}.\n        *   Significantly lowers the barrier for deep learning researchers to engage with complex neuroimaging data by providing standardized preprocessing pipelines and a modular GNN framework \\cite{cui2022mjr}.\n        *   Offers empirical evidence and \"general recipes\" for effective GNN designs, guiding future model development in this specialized domain \\cite{cui2022mjr}.\n    *   **Potential Impact on Future Research**:\n        *   Accelerates research and development of GNN-based methods for neuroimaging by providing a common, reproducible platform for comparison and innovation \\cite{cui2022mjr}.\n        *   Fosters greater collaboration and knowledge exchange between the neuroimaging and deep learning communities \\cite{cui2022mjr}.\n        *   Facilitates the discovery of neurological biomarkers, early diagnosis of disorders, and a deeper understanding of brain function and structure \\cite{cui2022mjr}.",
      "keywords": [
        "BrainGB benchmark",
        "Graph Neural Networks (GNNs)",
        "brain network analysis",
        "standardized preprocessing pipelines",
        "functional and structural brain networks (fMRI/dMRI)",
        "modular GNN design space",
        "reproducible research framework",
        "connectome structures",
        "neuroimaging-deep learning bridge",
        "GNN design modules",
        "neurological disorder prediction",
        "open science platform",
        "empirical GNN design recipes"
      ],
      "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n*   **strong emphasis on experiments and data:** the abstract explicitly states, \"we conduct extensive experiments on datasets across cohorts and modalities and recommend a set of general recipes for effective gnn designs on brain networks.\" it also mentions, \"we hope that this work will provide useful empirical evidence and offer insights for future research...\"\n*   **data-driven findings:** the goal is to \"recommend a set of general recipes\" and provide \"empirical evidence,\" which are direct outcomes of data-driven studies and statistical analysis.\n*   **methodology:** while it presents a benchmark (braingb), the benchmark itself is a tool to facilitate the \"systematic study\" and \"extensive experiments\" to derive findings about gnn designs for brain networks. the core contribution highlighted is the *results* from these experiments.\n\nwhile the development of braingb has technical aspects (presenting a new system/benchmark), the primary focus and the highlighted contributions in the abstract are the data-driven studies, experiments, and the empirical evidence and insights derived from them."
    },
    "file_name": "3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf"
  },
  {
    "success": true,
    "doc_id": "bbe6bf6de9054d8f897ff8459b895c44",
    "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Analysis of \"Explaining Graph Neural Networks via Structure-aware Interaction Index\" \\cite{bui2024zy9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Shapley-based explainability methods for Graph Neural Networks (GNNs) fail to adequately account for the inherent graph structure and high-order interactions among nodes.\n    *   **Importance & Challenge**:\n        *   GNNs are widely used in high-stakes domains, making their explainability crucial for trust and understanding.\n        *   Shapley values, while theoretically sound, create \"pathological\" or Out-Of-Distribution (OOD) perturbed graphs when applied naively to GNNs, leading to biased attribution scores.\n        *   Most methods focus solely on individual node/edge importance, neglecting complex interactions (motifs) and failing to identify groups of nodes that collectively influence predictions.\n        *   Existing explainers primarily identify positive contributions, overlooking structures that negatively affect predictions (crucial for counterfactual reasoning).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Positions itself within black-box, post-hoc, perturbation-based GNN explainers, similar to methods like SubgraphX and SAME that use Shapley values.\n        *   Generalizes both the Myerson value (which incorporates graph structure for node-wise importance) and the Shapley-Taylor index (which captures high-order interactions but is structure-agnostic).\n    *   **Limitations of Previous Solutions**:\n        *   **Shapley-based methods (e.g., Duval & Malliaros, Yuan et al., Ye et al.)**: Neglect graph structure during perturbation, potentially evaluating GNNs on disconnected or OOD inputs, leading to biased attributions.\n        *   **Node-wise importance focus (e.g., Zhang et al., Duval & Malliaros)**: Primarily attribute importance to individual nodes/edges, failing to capture complex interactions or \"motifs\" where groups of nodes act together. Greedy aggregation of node-wise scores often yields disconnected or unintuitive explanations.\n        *   **Lack of negative motif identification**: Most methods only identify substructures that positively affect predictions, missing those that hinder confidence or provide counterfactual insights.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   Introduces the **Myerson-Taylor interaction index**, a novel allocation rule that generalizes both the Myerson value and the Shapley-Taylor index. It incorporates graph structure by using an \"interaction-restricted function\" `f|E(T)` which evaluates disconnected subgraphs as the sum of their connected components, preventing OOD evaluations.\n        *   Proposes **Myerson-Taylor Structure-Aware Graph Explainer (MAGE)**, a two-phase explainer:\n            1.  **Interaction Matrix Computation**: Uses the second-order Myerson-Taylor index (Ψ²) to compute pairwise interaction scores between all nodes, forming an interaction matrix `B`.\n            2.  **Motif Search Optimization**: Solves an optimization problem to identify `m` non-overlapping, connected subgraphs (motifs) of total size at most `M`. This optimization maximizes the sum of *absolute* group attribution scores (GrAttr(S)), allowing for the discovery of both positive and negative influential motifs.\n    *   **Novelty/Differentiation**:\n        *   **Structure-Awareness**: Explicitly integrates graph connectivity into the attribution process via `f|E`, mitigating OOD bias inherent in structure-agnostic Shapley perturbations.\n        *   **High-Order Interactions**: Captures interactions beyond individual nodes, enabling the identification of meaningful motifs.\n        *   **Axiomatic Foundation**: Proves that the Myerson-Taylor index is the unique one satisfying five natural axioms accounting for graph structure and high-order interactions.\n        *   **Dual Motif Identification**: Capable of identifying both positively and negatively contributing motifs, providing a more comprehensive understanding of GNN predictions.\n        *   **Decoupled Computation**: Separates attribution calculation from subgraph search, allowing for parallel computation of the interaction matrix and potentially more tractable motif extraction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Myerson-Taylor interaction index** (Ψᵏ) for graph inputs, which simultaneously accounts for graph structure and high-order node interactions.\n    *   **Theoretical Insights**: Proof of the uniqueness of the Myerson-Taylor index based on a system of five axioms.\n    *   **System Design**: Development of **MAGE**, a two-phase explainer that leverages the second-order Myerson-Taylor index to construct an interaction matrix, followed by an optimization model for identifying multiple, non-overlapping, connected explanatory motifs (both positive and negative).\n    *   **Problem Formulation**: Formulation of the motif search as a quadratic multiple knapsack problem variant, which can be solved via linear relaxations and MILP solvers.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate MAGE's effectiveness in explaining GNN predictions.\n    *   **Datasets & Models**: Tested on ten diverse graph datasets (molecular prediction, image, sentiment classification) and three different GNN models.\n    *   **Performance Metrics & Comparison**:\n        *   Compared against seven popular, state-of-the-art baselines.\n        *   MAGE consistently provided superior subgraph explanations.\n        *   Achieved up to a **27.55% increase in explanation accuracy** compared to the best baseline.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   The number of connected subgraphs is still exponential in `|V|`, though significantly reduced for sparse graphs compared to all possible subgraphs.\n        *   Relies on Monte Carlo sampling for approximating the Myerson-Taylor index, similar to other game-based explainers, which introduces approximation errors.\n        *   Requires computing connected components for evaluated subsets, adding `O(|V|)` complexity per evaluation.\n    *   **Scope of Applicability**: Primarily designed for black-box GNN models and graph-structured inputs where identifying influential subgraphs (motifs) is desired. The framework does not explicitly exploit node or edge feature vectors, focusing on structural importance.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{bui2024zy9} significantly advances GNN explainability by providing a theoretically grounded, structure-aware, and interaction-capturing attribution method. It overcomes key limitations of prior Shapley-based approaches by explicitly integrating graph topology and enabling the discovery of complex, multi-node motifs.\n    *   **Potential Impact**:\n        *   Enables more accurate and intuitive explanations for GNN predictions, particularly in domains where structural patterns are critical (e.g., chemistry, biology).\n        *   Facilitates counterfactual reasoning by identifying both positive and negative influential motifs.\n        *   The axiomatic foundation provides strong theoretical guarantees for the proposed index.\n        *   The decoupled approach for attribution and motif search could inspire more efficient explainability frameworks for complex structured data.",
    "intriguing_abstract": "Explaining Graph Neural Networks (GNNs) is paramount for trust in high-stakes applications, yet existing Shapley-based methods often yield biased explanations by neglecting inherent graph structure and complex high-order interactions. These approaches struggle to identify meaningful *motifs*—groups of nodes collectively influencing predictions—and overlook crucial substructures that negatively impact outcomes, leading to Out-Of-Distribution (OOD) perturbations.\n\nWe introduce the **Myerson-Taylor interaction index**, a novel, axiomatically-grounded allocation rule that uniquely generalizes both the Myerson value and the Shapley-Taylor index. This breakthrough explicitly integrates graph connectivity, preventing OOD evaluations, while simultaneously capturing intricate high-order node interactions. Leveraging this, we propose **MAGE (Myerson-Taylor Structure-Aware Graph Explainer)**, a two-phase framework. MAGE computes a comprehensive interaction matrix and then optimizes for multiple, non-overlapping, *connected motifs*, crucially identifying both positively and negatively contributing substructures. Extensive experiments across diverse datasets and GNN models demonstrate MAGE's superior performance, achieving up to a 27.55% increase in explanation accuracy. Our work provides a robust, theoretically sound, and practically effective solution for unlocking the black-box of GNNs, enabling deeper understanding and crucial counterfactual reasoning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN explainability",
      "Myerson-Taylor interaction index",
      "structure-aware attribution",
      "high-order interactions",
      "explanatory motifs",
      "MAGE (Graph Explainer)",
      "axiomatic foundation",
      "Out-Of-Distribution (OOD) perturbations",
      "positive and negative motifs",
      "subgraph explanations",
      "explanation accuracy"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf",
    "citation_key": "bui2024zy9",
    "metadata": {
      "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
      "authors": [
        "Ngoc Bui",
        "Hieu Trung Nguyen",
        "Viet Anh Nguyen",
        "Rex Ying"
      ],
      "published_date": "2024",
      "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
      "file_path": "paper_data/Graph_Neural_Networks/a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Analysis of \"Explaining Graph Neural Networks via Structure-aware Interaction Index\" \\cite{bui2024zy9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Shapley-based explainability methods for Graph Neural Networks (GNNs) fail to adequately account for the inherent graph structure and high-order interactions among nodes.\n    *   **Importance & Challenge**:\n        *   GNNs are widely used in high-stakes domains, making their explainability crucial for trust and understanding.\n        *   Shapley values, while theoretically sound, create \"pathological\" or Out-Of-Distribution (OOD) perturbed graphs when applied naively to GNNs, leading to biased attribution scores.\n        *   Most methods focus solely on individual node/edge importance, neglecting complex interactions (motifs) and failing to identify groups of nodes that collectively influence predictions.\n        *   Existing explainers primarily identify positive contributions, overlooking structures that negatively affect predictions (crucial for counterfactual reasoning).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Positions itself within black-box, post-hoc, perturbation-based GNN explainers, similar to methods like SubgraphX and SAME that use Shapley values.\n        *   Generalizes both the Myerson value (which incorporates graph structure for node-wise importance) and the Shapley-Taylor index (which captures high-order interactions but is structure-agnostic).\n    *   **Limitations of Previous Solutions**:\n        *   **Shapley-based methods (e.g., Duval & Malliaros, Yuan et al., Ye et al.)**: Neglect graph structure during perturbation, potentially evaluating GNNs on disconnected or OOD inputs, leading to biased attributions.\n        *   **Node-wise importance focus (e.g., Zhang et al., Duval & Malliaros)**: Primarily attribute importance to individual nodes/edges, failing to capture complex interactions or \"motifs\" where groups of nodes act together. Greedy aggregation of node-wise scores often yields disconnected or unintuitive explanations.\n        *   **Lack of negative motif identification**: Most methods only identify substructures that positively affect predictions, missing those that hinder confidence or provide counterfactual insights.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   Introduces the **Myerson-Taylor interaction index**, a novel allocation rule that generalizes both the Myerson value and the Shapley-Taylor index. It incorporates graph structure by using an \"interaction-restricted function\" `f|E(T)` which evaluates disconnected subgraphs as the sum of their connected components, preventing OOD evaluations.\n        *   Proposes **Myerson-Taylor Structure-Aware Graph Explainer (MAGE)**, a two-phase explainer:\n            1.  **Interaction Matrix Computation**: Uses the second-order Myerson-Taylor index (Ψ²) to compute pairwise interaction scores between all nodes, forming an interaction matrix `B`.\n            2.  **Motif Search Optimization**: Solves an optimization problem to identify `m` non-overlapping, connected subgraphs (motifs) of total size at most `M`. This optimization maximizes the sum of *absolute* group attribution scores (GrAttr(S)), allowing for the discovery of both positive and negative influential motifs.\n    *   **Novelty/Differentiation**:\n        *   **Structure-Awareness**: Explicitly integrates graph connectivity into the attribution process via `f|E`, mitigating OOD bias inherent in structure-agnostic Shapley perturbations.\n        *   **High-Order Interactions**: Captures interactions beyond individual nodes, enabling the identification of meaningful motifs.\n        *   **Axiomatic Foundation**: Proves that the Myerson-Taylor index is the unique one satisfying five natural axioms accounting for graph structure and high-order interactions.\n        *   **Dual Motif Identification**: Capable of identifying both positively and negatively contributing motifs, providing a more comprehensive understanding of GNN predictions.\n        *   **Decoupled Computation**: Separates attribution calculation from subgraph search, allowing for parallel computation of the interaction matrix and potentially more tractable motif extraction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Myerson-Taylor interaction index** (Ψᵏ) for graph inputs, which simultaneously accounts for graph structure and high-order node interactions.\n    *   **Theoretical Insights**: Proof of the uniqueness of the Myerson-Taylor index based on a system of five axioms.\n    *   **System Design**: Development of **MAGE**, a two-phase explainer that leverages the second-order Myerson-Taylor index to construct an interaction matrix, followed by an optimization model for identifying multiple, non-overlapping, connected explanatory motifs (both positive and negative).\n    *   **Problem Formulation**: Formulation of the motif search as a quadratic multiple knapsack problem variant, which can be solved via linear relaxations and MILP solvers.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate MAGE's effectiveness in explaining GNN predictions.\n    *   **Datasets & Models**: Tested on ten diverse graph datasets (molecular prediction, image, sentiment classification) and three different GNN models.\n    *   **Performance Metrics & Comparison**:\n        *   Compared against seven popular, state-of-the-art baselines.\n        *   MAGE consistently provided superior subgraph explanations.\n        *   Achieved up to a **27.55% increase in explanation accuracy** compared to the best baseline.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   The number of connected subgraphs is still exponential in `|V|`, though significantly reduced for sparse graphs compared to all possible subgraphs.\n        *   Relies on Monte Carlo sampling for approximating the Myerson-Taylor index, similar to other game-based explainers, which introduces approximation errors.\n        *   Requires computing connected components for evaluated subsets, adding `O(|V|)` complexity per evaluation.\n    *   **Scope of Applicability**: Primarily designed for black-box GNN models and graph-structured inputs where identifying influential subgraphs (motifs) is desired. The framework does not explicitly exploit node or edge feature vectors, focusing on structural importance.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{bui2024zy9} significantly advances GNN explainability by providing a theoretically grounded, structure-aware, and interaction-capturing attribution method. It overcomes key limitations of prior Shapley-based approaches by explicitly integrating graph topology and enabling the discovery of complex, multi-node motifs.\n    *   **Potential Impact**:\n        *   Enables more accurate and intuitive explanations for GNN predictions, particularly in domains where structural patterns are critical (e.g., chemistry, biology).\n        *   Facilitates counterfactual reasoning by identifying both positive and negative influential motifs.\n        *   The axiomatic foundation provides strong theoretical guarantees for the proposed index.\n        *   The decoupled approach for attribution and motif search could inspire more efficient explainability frameworks for complex structured data.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN explainability",
        "Myerson-Taylor interaction index",
        "structure-aware attribution",
        "high-order interactions",
        "explanatory motifs",
        "MAGE (Graph Explainer)",
        "axiomatic foundation",
        "Out-Of-Distribution (OOD) perturbations",
        "positive and negative motifs",
        "subgraph explanations",
        "explanation accuracy"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **proposes new methods/systems:** the abstract explicitly states, \"this paper introduces the myerson-taylor interaction index\" and \"we propose myerson-taylor structure-aware graph explainer (mage), a novel explainer\". this directly aligns with the \"technical\" criteria: \"presents new methods, algorithms, or systems.\"\n2.  **addresses a technical problem:** the introduction highlights the challenge of \"understanding the rationale behind the prediction of gnns\" and the \"lack of explainability\" as a growing concern, setting up a technical problem that the paper aims to solve.\n3.  **includes theoretical foundation:** the abstract mentions, \"we prove that the myerson-taylor index is the unique one that satisfies a system of five natural axioms.\" this indicates a strong theoretical component.\n4.  **includes empirical validation:** the abstract concludes with, \"extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.\" this shows an empirical component.\n\nwhile the paper has strong theoretical and empirical elements, its primary contribution is the **development and proposal of a novel method/system (mage and the myerson-taylor index) to solve a specific technical problem (gnn explainability)**. the theoretical analysis provides the foundation for this new method, and the experiments validate its effectiveness. therefore, the overarching classification is **technical**.\n\n**classification: technical**"
    },
    "file_name": "a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf"
  },
  {
    "success": true,
    "doc_id": "72f04da4783e119ca0d3fa4b7639eec1",
    "summary": "This paper, \"Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities\" by Liu et al., provides a comprehensive survey of graph pooling methods for Graph Neural Networks (GNNs) \\cite{liu2022a5y}.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Graph Neural Networks (GNNs) are highly effective for various graph-level tasks (e.g., graph classification, regression, generation), but these tasks require a holistic, fixed-size graph-level representation from variable-sized graph inputs \\cite{liu2022a5y}. Graph pooling is an essential mechanism to condense the input graph into a smaller graph or a single vector.\n    *   **Importance & Challenge**: Despite the proliferation of diverse graph pooling methods, there has been a significant lack of systematic summarization, comprehensive evaluation, and a clear outline of the progress, challenges, and future opportunities in this rapidly evolving field \\cite{liu2022a5y}. This gap hinders new practitioners and researchers from gaining a comprehensive understanding and identifying promising research directions.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper *is* a systematic review, categorizing and analyzing a wide array of existing graph pooling methods \\cite{liu2022a5y}. It positions itself as the first comprehensive survey to systematically summarize these works.\n    *   **Limitations of previous solutions (as identified by the survey)**:\n        *   **Flat Pooling**: Often fails to consider the intrinsic hierarchical structures of graphs, leading to information loss and degraded performance \\cite{liu2022a5y}.\n        *   **Node Clustering Pooling**: Suffers from high time and storage complexity (typically O(n^2) space) due to the computation of dense cluster assignment matrices \\cite{liu2022a5y}.\n        *   **Node Drop Pooling**: While more efficient and scalable, it inherently suffers from inevitable information loss due to node removal \\cite{liu2022a5y}.\n        *   **General**: Prior works lacked a unified framework for understanding, comparing, and evaluating the diverse pooling strategies.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core \"method\" is its systematic review and proposed taxonomy of graph pooling techniques. It classifies methods into Flat Pooling and Hierarchical Pooling (further divided into Node Clustering Pooling, Node Drop Pooling, and Other Pooling) \\cite{liu2022a5y}.\n    *   **Novelty/Difference**:\n        *   Proposes a novel, comprehensive taxonomy of existing graph pooling methods, accompanied by mathematical summaries for each category \\cite{liu2022a5y}.\n        *   Introduces universal and modularized frameworks for Hierarchical Pooling, specifically deconstructing Node Clustering Pooling into \"Cluster Assignment Matrix (CAM) Generator\" and \"Graph Coarsening\" modules, and Node Drop Pooling into \"Score Generator,\" \"Node Selector,\" and \"Graph Coarsening\" modules \\cite{liu2022a5y}. This modularity aids in understanding and comparing different approaches.\n        *   Provides an overview of related libraries, commonly used benchmark datasets, model architectures for downstream tasks, and open-source implementations \\cite{liu2022a5y}.\n        *   Outlines various applications where graph pooling is utilized across diverse domains \\cite{liu2022a5y}.\n        *   Discusses critical challenges facing current studies and offers insights into future research directions \\cite{liu2022a5y}.\n\n*   **Key Technical Contributions**\n    *   **Novel Frameworks/Taxonomy**: The primary contribution is the introduction of a systematic and comprehensive taxonomy for graph pooling methods, along with modularized frameworks for hierarchical pooling strategies \\cite{liu2022a5y}.\n    *   **Structured Analysis**: Provides a structured analysis of how various representative methods fit into these proposed frameworks, highlighting their design choices for components like CAM generation, score generation, node selection, and graph coarsening \\cite{liu2022a5y}.\n    *   **Resource Compilation**: Compiles and summarizes essential resources for the field, including benchmark datasets (e.g., TU dataset, OGB dataset), common GNN architectures, and open-source libraries (e.g., PyTorch Geometric, DGL) \\cite{liu2022a5y}.\n    *   **Future Directions**: Identifies and articulates key challenges and promising future research opportunities in graph pooling \\cite{liu2022a5y}.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work *does not conduct its own experiments*.\n    *   Instead, it *summarizes the experimental validation practices* within the field of graph pooling \\cite{liu2022a5y}:\n        *   Lists commonly used benchmark datasets (e.g., D&D, PROTEINS, NCI1, MUTAG, REDDIT, IMDB, COLLAB, QM9) from repositories like TU dataset and Open Graph Benchmark (OGB) \\cite{liu2022a5y}. These datasets cover various categories (protein, molecule, social networks) and graph sizes.\n        *   Mentions that these methods are typically evaluated on graph-level tasks such as graph classification, regression, and generation \\cite{liu2022a5y}.\n        *   Highlights the availability of open-source implementations for many methods, facilitating reproducibility and comparison \\cite{liu2022a5y}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of the field, as identified by the paper)**:\n        *   Flat pooling's inability to capture hierarchical graph structures \\cite{liu2022a5y}.\n        *   High computational and memory complexity of node clustering pooling, limiting its applicability to large graphs \\cite{liu2022a5y}.\n        *   Inherent information loss in node drop pooling methods \\cite{liu2022a5y}.\n        *   The need for more sophisticated score generators and graph coarsening strategies to mitigate information loss in node drop methods \\cite{liu2022a5y}.\n    *   **Scope of Applicability**: The survey focuses on graph pooling methods primarily for graph-level tasks within GNNs, with applications spanning chemistry, biology, social networks, computer vision, natural language processing, and recommendation systems \\cite{liu2022a5y}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the *understanding and organization* of the state-of-the-art in graph pooling by providing the first systematic and comprehensive review \\cite{liu2022a5y}. It fills a critical gap in the literature by structuring the diverse landscape of existing methods.\n    *   **Potential Impact**:\n        *   Serves as a foundational resource for new researchers and practitioners, offering a clear entry point and comprehensive understanding of graph pooling \\cite{liu2022a5y}.\n        *   Informs experienced researchers about the latest advancements, critical challenges, and promising future directions, thereby stimulating and guiding future research efforts in the field \\cite{liu2022a5y}.\n        *   Facilitates the development of novel graph pooling methods by providing a structured framework for analysis and comparison \\cite{liu2022a5y}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph-level tasks, yet effectively condensing variable-sized graph inputs into fixed-size representations remains a critical challenge. Graph pooling is the indispensable mechanism addressing this, but its rapidly evolving landscape has lacked systematic summarization and a unified understanding. This paper presents the first comprehensive survey of graph pooling methods, introducing a novel, systematic taxonomy that demystifies their diverse architectures.\n\nWe propose universal, modularized frameworks for hierarchical pooling, deconstructing complex strategies like Node Clustering and Node Drop into intuitive components such as Cluster Assignment, Node Selection, and Graph Coarsening. This rigorous analysis clarifies the design choices, strengths, and inherent limitations (e.g., information loss, computational complexity) of existing methods. By compiling essential resources, benchmark datasets, and applications, and outlining critical challenges and promising future directions, this work serves as a foundational guide, empowering researchers to navigate this dynamic field and catalyze the development of next-generation graph pooling techniques.",
    "keywords": [
      "Graph Pooling",
      "Graph Neural Networks (GNNs)",
      "Graph-level tasks",
      "Systematic survey",
      "Comprehensive taxonomy",
      "Hierarchical Pooling",
      "Node Clustering Pooling",
      "Node Drop Pooling",
      "Modularized frameworks",
      "Cluster Assignment Matrix (CAM)",
      "Graph Coarsening",
      "Research challenges",
      "Future opportunities",
      "Information loss",
      "Computational complexity"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3db15a5534050ab2cfc1d09dd772d032395515e1.pdf",
    "citation_key": "liu2022a5y",
    "metadata": {
      "title": "Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities",
      "authors": [
        "Chuang Liu",
        "Yibing Zhan",
        "Chang Li",
        "Bo Du",
        "Jia Wu",
        "Wenbin Hu",
        "Tongliang Liu",
        "Dacheng Tao"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks have emerged as a leading architecture for many graph-level tasks, such as graph classification and graph generation. As an essential component of the architecture, graph pooling is indispensable for obtaining a holistic graph-level representation of the whole graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these works. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods for graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods with a mathematical summary for each category; 2) then, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) next, we further outline the applications that incorporate the idea of graph pooling in a variety of domains; 4) finally, we discuss certain critical challenges facing current studies and share our insights on future potential directions for research on the improvement of graph pooling.",
      "file_path": "paper_data/Graph_Neural_Networks/3db15a5534050ab2cfc1d09dd772d032395515e1.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, \"Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities\" by Liu et al., provides a comprehensive survey of graph pooling methods for Graph Neural Networks (GNNs) \\cite{liu2022a5y}.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Graph Neural Networks (GNNs) are highly effective for various graph-level tasks (e.g., graph classification, regression, generation), but these tasks require a holistic, fixed-size graph-level representation from variable-sized graph inputs \\cite{liu2022a5y}. Graph pooling is an essential mechanism to condense the input graph into a smaller graph or a single vector.\n    *   **Importance & Challenge**: Despite the proliferation of diverse graph pooling methods, there has been a significant lack of systematic summarization, comprehensive evaluation, and a clear outline of the progress, challenges, and future opportunities in this rapidly evolving field \\cite{liu2022a5y}. This gap hinders new practitioners and researchers from gaining a comprehensive understanding and identifying promising research directions.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper *is* a systematic review, categorizing and analyzing a wide array of existing graph pooling methods \\cite{liu2022a5y}. It positions itself as the first comprehensive survey to systematically summarize these works.\n    *   **Limitations of previous solutions (as identified by the survey)**:\n        *   **Flat Pooling**: Often fails to consider the intrinsic hierarchical structures of graphs, leading to information loss and degraded performance \\cite{liu2022a5y}.\n        *   **Node Clustering Pooling**: Suffers from high time and storage complexity (typically O(n^2) space) due to the computation of dense cluster assignment matrices \\cite{liu2022a5y}.\n        *   **Node Drop Pooling**: While more efficient and scalable, it inherently suffers from inevitable information loss due to node removal \\cite{liu2022a5y}.\n        *   **General**: Prior works lacked a unified framework for understanding, comparing, and evaluating the diverse pooling strategies.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core \"method\" is its systematic review and proposed taxonomy of graph pooling techniques. It classifies methods into Flat Pooling and Hierarchical Pooling (further divided into Node Clustering Pooling, Node Drop Pooling, and Other Pooling) \\cite{liu2022a5y}.\n    *   **Novelty/Difference**:\n        *   Proposes a novel, comprehensive taxonomy of existing graph pooling methods, accompanied by mathematical summaries for each category \\cite{liu2022a5y}.\n        *   Introduces universal and modularized frameworks for Hierarchical Pooling, specifically deconstructing Node Clustering Pooling into \"Cluster Assignment Matrix (CAM) Generator\" and \"Graph Coarsening\" modules, and Node Drop Pooling into \"Score Generator,\" \"Node Selector,\" and \"Graph Coarsening\" modules \\cite{liu2022a5y}. This modularity aids in understanding and comparing different approaches.\n        *   Provides an overview of related libraries, commonly used benchmark datasets, model architectures for downstream tasks, and open-source implementations \\cite{liu2022a5y}.\n        *   Outlines various applications where graph pooling is utilized across diverse domains \\cite{liu2022a5y}.\n        *   Discusses critical challenges facing current studies and offers insights into future research directions \\cite{liu2022a5y}.\n\n*   **Key Technical Contributions**\n    *   **Novel Frameworks/Taxonomy**: The primary contribution is the introduction of a systematic and comprehensive taxonomy for graph pooling methods, along with modularized frameworks for hierarchical pooling strategies \\cite{liu2022a5y}.\n    *   **Structured Analysis**: Provides a structured analysis of how various representative methods fit into these proposed frameworks, highlighting their design choices for components like CAM generation, score generation, node selection, and graph coarsening \\cite{liu2022a5y}.\n    *   **Resource Compilation**: Compiles and summarizes essential resources for the field, including benchmark datasets (e.g., TU dataset, OGB dataset), common GNN architectures, and open-source libraries (e.g., PyTorch Geometric, DGL) \\cite{liu2022a5y}.\n    *   **Future Directions**: Identifies and articulates key challenges and promising future research opportunities in graph pooling \\cite{liu2022a5y}.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work *does not conduct its own experiments*.\n    *   Instead, it *summarizes the experimental validation practices* within the field of graph pooling \\cite{liu2022a5y}:\n        *   Lists commonly used benchmark datasets (e.g., D&D, PROTEINS, NCI1, MUTAG, REDDIT, IMDB, COLLAB, QM9) from repositories like TU dataset and Open Graph Benchmark (OGB) \\cite{liu2022a5y}. These datasets cover various categories (protein, molecule, social networks) and graph sizes.\n        *   Mentions that these methods are typically evaluated on graph-level tasks such as graph classification, regression, and generation \\cite{liu2022a5y}.\n        *   Highlights the availability of open-source implementations for many methods, facilitating reproducibility and comparison \\cite{liu2022a5y}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of the field, as identified by the paper)**:\n        *   Flat pooling's inability to capture hierarchical graph structures \\cite{liu2022a5y}.\n        *   High computational and memory complexity of node clustering pooling, limiting its applicability to large graphs \\cite{liu2022a5y}.\n        *   Inherent information loss in node drop pooling methods \\cite{liu2022a5y}.\n        *   The need for more sophisticated score generators and graph coarsening strategies to mitigate information loss in node drop methods \\cite{liu2022a5y}.\n    *   **Scope of Applicability**: The survey focuses on graph pooling methods primarily for graph-level tasks within GNNs, with applications spanning chemistry, biology, social networks, computer vision, natural language processing, and recommendation systems \\cite{liu2022a5y}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the *understanding and organization* of the state-of-the-art in graph pooling by providing the first systematic and comprehensive review \\cite{liu2022a5y}. It fills a critical gap in the literature by structuring the diverse landscape of existing methods.\n    *   **Potential Impact**:\n        *   Serves as a foundational resource for new researchers and practitioners, offering a clear entry point and comprehensive understanding of graph pooling \\cite{liu2022a5y}.\n        *   Informs experienced researchers about the latest advancements, critical challenges, and promising future directions, thereby stimulating and guiding future research efforts in the field \\cite{liu2022a5y}.\n        *   Facilitates the development of novel graph pooling methods by providing a structured framework for analysis and comparison \\cite{liu2022a5y}.",
      "keywords": [
        "Graph Pooling",
        "Graph Neural Networks (GNNs)",
        "Graph-level tasks",
        "Systematic survey",
        "Comprehensive taxonomy",
        "Hierarchical Pooling",
        "Node Clustering Pooling",
        "Node Drop Pooling",
        "Modularized frameworks",
        "Cluster Assignment Matrix (CAM)",
        "Graph Coarsening",
        "Research challenges",
        "Future opportunities",
        "Information loss",
        "Computational complexity"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly states: \"we attempt to fill this gap by providing a broad review of recent methods for graph pooling.\" it also mentions: \"systematically summarize these works,\" \"propose a taxonomy of existing graph pooling methods,\" \"provide an overview of the libraries related to graph pooling,\" \"outline the applications,\" and \"discuss certain critical challenges... and share our insights on future potential directions.\"\n*   the **introduction** discusses the importance of graph pooling and mentions that \"many designs of graph pooling have been proposed, which could be roughly divided into flat pooling (section 3.1 ) and hierarchical pooling ( section 3.2 ).\" this indicates an organization and classification of existing literature.\n\nthese points directly align with the criteria for a **survey** paper: \"reviews existing literature comprehensively,\" \"abstract mentions: 'survey', 'review', 'comprehensive analysis', 'state-of-the-art',\" and \"introduction discusses: literature organization, classification schemes.\"\n\ntherefore, the paper type is: **survey**"
    },
    "file_name": "3db15a5534050ab2cfc1d09dd772d032395515e1.pdf"
  },
  {
    "success": true,
    "doc_id": "9105b5ea823026cff0808deb6c384452",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Traditional time series analysis methods (e.g., SVR, GBDT, VAR, ARIMA, CNN, RNN, Transformers) struggle to explicitly model complex inter-temporal and inter-variable relationships, particularly spatial relations in non-Euclidean data, which are prevalent in multivariate time series.\n    *   **Importance/Challenge**: Time series data is ubiquitous and critical across various domains (e.g., cloud computing, transportation, IoT). Accurately analyzing this data requires capturing intricate spatial-temporal dependencies, which existing methods often fail to do effectively, leading to less accurate results. Graph Neural Networks (GNNs) offer a promising avenue to address this by explicitly modeling these non-Euclidean relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper is a comprehensive survey that reviews the burgeoning field of GNNs for time series analysis (GNN4TS).\n    *   **Limitations of previous solutions**: Prior time series analysis methods (traditional and deep learning) lack explicit modeling of non-Euclidean spatial relationships. Existing surveys on GNNs or spatial-temporal data are often limited in scope, focusing on specific domains (e.g., traffic, urban computing) or tasks (e.g., forecasting), and do not provide a holistic view of GNNs across the full spectrum of time series analysis tasks.\n    *   **Positioning**: This survey \\cite{jin2023ijy} aims to fill this gap by providing the *first comprehensive and up-to-date review* of GNN4TS, encompassing four fundamental tasks: forecasting, classification, anomaly detection, and imputation, without restricting to specific domains or tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper systematically reviews and categorizes GNN-based approaches for time series analysis. It defines key concepts like spatial-temporal graphs and GNN operations (AGGREGATE, COMBINE). A significant part of its technical approach is detailing how graph structures are generated for time series data.\n    *   **Innovation (of the survey)**:\n        *   **Unified and Structured Taxonomy**: Presents a novel framework to categorize existing GNN4TS works from both task-oriented (forecasting, classification, anomaly detection, imputation) and methodology-oriented perspectives (spatial/temporal dependency modeling, model architecture).\n        *   **Detailed Graph Construction Methods**: Provides a comprehensive discussion and categorization of strategies for generating graph structures when not readily available, including:\n            *   **Heuristic-based Graphs**: Spatial Proximity (e.g., geographical distance, Gaussian kernel), Pairwise Connectivity (e.g., transportation networks), Pairwise Similarity (e.g., cosine similarity, Pearson correlation, DTW), and Functional Dependence (e.g., Granger causality, transfer entropy).\n            *   **Learning-based Graphs**: Approaches that learn graph structures directly from data end-to-end with the downstream task, often using embedding comparisons or attention mechanisms, with sparsification techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Taxonomy**: Introduces a comprehensive, task-oriented taxonomy for GNN4TS, covering forecasting, classification, anomaly detection, and imputation, and a methodology-oriented classification based on spatial/temporal dependency modeling and model architecture.\n    *   **Comprehensive Review of Graph Construction**: Systematically outlines and categorizes various heuristic-based and learning-based methods for constructing graph structures from time series data, which is crucial for applying GNNs.\n    *   **Consolidation of Knowledge**: For the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities.\n    *   **Future Research Agenda**: Identifies and discusses potential future research directions in the field of GNN4TS.\n\n*   **Experimental Validation**\n    *   As a survey, this paper does not present new experimental results. Instead, it synthesizes findings from the reviewed literature.\n    *   **Key Performance Metrics and Comparison Results (from reviewed papers)**: The survey highlights that GNN-based approaches \"demonstrate promising results\" and \"significant advantages\" in modeling real-world time series data compared to traditional methods. It notes that GNNs enable the capture of \"diverse and intricate relationships\" and have shown \"promising outcomes\" across forecasting, classification, anomaly detection, and imputation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The survey \\cite{jin2023ijy} focuses specifically on GNNs for time series analysis (GNN4TS) across four core tasks: forecasting, classification, anomaly detection, and imputation. It primarily discusses spatial GNNs.\n    *   **Technical Limitations (of the field, as identified by the survey)**: The field still faces challenges in graph structure generation (especially for complex, dynamic relationships), scalability for very large time series, and interpretability of GNN models.\n\n*   **Technical Significance**\n    *   **Advances the technical state-of-the-art**: By providing the first comprehensive and structured overview of GNN4TS, the survey consolidates fragmented knowledge, making the field more accessible and understandable for researchers and practitioners. It clarifies the landscape of GNN applications in time series.\n    *   **Potential impact on future research**: The survey \\cite{jin2023ijy} serves as a foundational resource, guiding future research by identifying open questions, emerging trends, and promising directions in GNN4TS. It underscores the power of GNNs in explicitly modeling complex spatial-temporal dependencies, thereby encouraging further innovation in this area.",
    "intriguing_abstract": "Multivariate time series analysis is critical across diverse domains, yet traditional methods often falter in explicitly modeling the intricate inter-temporal and non-Euclidean spatial dependencies inherent in such data. Graph Neural Networks (GNNs) offer a powerful paradigm to overcome these limitations. This paper presents the *first comprehensive and up-to-date survey* of GNNs for Time Series (GNN4TS), unifying fragmented knowledge and illuminating the field's rapid advancements. We introduce a novel, structured taxonomy categorizing GNN4TS approaches across four fundamental tasks: forecasting, classification, anomaly detection, and imputation. Crucially, we provide an in-depth review of graph construction strategies, from heuristic-based methods utilizing spatial proximity or functional dependence to advanced learning-based techniques that dynamically infer graph structures. By consolidating foundational concepts, practical applications, and identifying pressing future research directions, this survey serves as an indispensable resource. It underscores GNNs' significant advantages in capturing complex spatial-temporal relationships, paving the way for more accurate and robust time series analysis across science and industry.",
    "keywords": [
      "GNNs for time series analysis (GNN4TS)",
      "multivariate time series",
      "spatial-temporal dependencies",
      "non-Euclidean spatial relationships",
      "graph construction methods",
      "heuristic-based graphs",
      "learning-based graphs",
      "unified taxonomy",
      "time series forecasting",
      "classification",
      "anomaly detection",
      "imputation",
      "GNN scalability and interpretability",
      "comprehensive survey"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf",
    "citation_key": "jin2023ijy",
    "metadata": {
      "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
      "authors": [
        "Ming Jin",
        "Huan Yee Koh",
        "Qingsong Wen",
        "Daniele Zambon",
        "C. Alippi",
        "G. I. Webb",
        "Irwin King",
        "Shirui Pan"
      ],
      "published_date": "2023",
      "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
      "file_path": "paper_data/Graph_Neural_Networks/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Traditional time series analysis methods (e.g., SVR, GBDT, VAR, ARIMA, CNN, RNN, Transformers) struggle to explicitly model complex inter-temporal and inter-variable relationships, particularly spatial relations in non-Euclidean data, which are prevalent in multivariate time series.\n    *   **Importance/Challenge**: Time series data is ubiquitous and critical across various domains (e.g., cloud computing, transportation, IoT). Accurately analyzing this data requires capturing intricate spatial-temporal dependencies, which existing methods often fail to do effectively, leading to less accurate results. Graph Neural Networks (GNNs) offer a promising avenue to address this by explicitly modeling these non-Euclidean relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper is a comprehensive survey that reviews the burgeoning field of GNNs for time series analysis (GNN4TS).\n    *   **Limitations of previous solutions**: Prior time series analysis methods (traditional and deep learning) lack explicit modeling of non-Euclidean spatial relationships. Existing surveys on GNNs or spatial-temporal data are often limited in scope, focusing on specific domains (e.g., traffic, urban computing) or tasks (e.g., forecasting), and do not provide a holistic view of GNNs across the full spectrum of time series analysis tasks.\n    *   **Positioning**: This survey \\cite{jin2023ijy} aims to fill this gap by providing the *first comprehensive and up-to-date review* of GNN4TS, encompassing four fundamental tasks: forecasting, classification, anomaly detection, and imputation, without restricting to specific domains or tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper systematically reviews and categorizes GNN-based approaches for time series analysis. It defines key concepts like spatial-temporal graphs and GNN operations (AGGREGATE, COMBINE). A significant part of its technical approach is detailing how graph structures are generated for time series data.\n    *   **Innovation (of the survey)**:\n        *   **Unified and Structured Taxonomy**: Presents a novel framework to categorize existing GNN4TS works from both task-oriented (forecasting, classification, anomaly detection, imputation) and methodology-oriented perspectives (spatial/temporal dependency modeling, model architecture).\n        *   **Detailed Graph Construction Methods**: Provides a comprehensive discussion and categorization of strategies for generating graph structures when not readily available, including:\n            *   **Heuristic-based Graphs**: Spatial Proximity (e.g., geographical distance, Gaussian kernel), Pairwise Connectivity (e.g., transportation networks), Pairwise Similarity (e.g., cosine similarity, Pearson correlation, DTW), and Functional Dependence (e.g., Granger causality, transfer entropy).\n            *   **Learning-based Graphs**: Approaches that learn graph structures directly from data end-to-end with the downstream task, often using embedding comparisons or attention mechanisms, with sparsification techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Taxonomy**: Introduces a comprehensive, task-oriented taxonomy for GNN4TS, covering forecasting, classification, anomaly detection, and imputation, and a methodology-oriented classification based on spatial/temporal dependency modeling and model architecture.\n    *   **Comprehensive Review of Graph Construction**: Systematically outlines and categorizes various heuristic-based and learning-based methods for constructing graph structures from time series data, which is crucial for applying GNNs.\n    *   **Consolidation of Knowledge**: For the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities.\n    *   **Future Research Agenda**: Identifies and discusses potential future research directions in the field of GNN4TS.\n\n*   **Experimental Validation**\n    *   As a survey, this paper does not present new experimental results. Instead, it synthesizes findings from the reviewed literature.\n    *   **Key Performance Metrics and Comparison Results (from reviewed papers)**: The survey highlights that GNN-based approaches \"demonstrate promising results\" and \"significant advantages\" in modeling real-world time series data compared to traditional methods. It notes that GNNs enable the capture of \"diverse and intricate relationships\" and have shown \"promising outcomes\" across forecasting, classification, anomaly detection, and imputation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The survey \\cite{jin2023ijy} focuses specifically on GNNs for time series analysis (GNN4TS) across four core tasks: forecasting, classification, anomaly detection, and imputation. It primarily discusses spatial GNNs.\n    *   **Technical Limitations (of the field, as identified by the survey)**: The field still faces challenges in graph structure generation (especially for complex, dynamic relationships), scalability for very large time series, and interpretability of GNN models.\n\n*   **Technical Significance**\n    *   **Advances the technical state-of-the-art**: By providing the first comprehensive and structured overview of GNN4TS, the survey consolidates fragmented knowledge, making the field more accessible and understandable for researchers and practitioners. It clarifies the landscape of GNN applications in time series.\n    *   **Potential impact on future research**: The survey \\cite{jin2023ijy} serves as a foundational resource, guiding future research by identifying open questions, emerging trends, and promising directions in GNN4TS. It underscores the power of GNNs in explicitly modeling complex spatial-temporal dependencies, thereby encouraging further innovation in this area.",
      "keywords": [
        "GNNs for time series analysis (GNN4TS)",
        "multivariate time series",
        "spatial-temporal dependencies",
        "non-Euclidean spatial relationships",
        "graph construction methods",
        "heuristic-based graphs",
        "learning-based graphs",
        "unified taxonomy",
        "time series forecasting",
        "classification",
        "anomaly detection",
        "imputation",
        "GNN scalability and interpretability",
        "comprehensive survey"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"a survey on graph neural networks for time series: forecasting, classification, imputation, and anomaly detection\" explicitly uses the word \"survey\".\n*   **abstract:**\n    *   \"in this survey, we provide a comprehensive review of graph neural networks for time series analysis (gnn4ts)...\"\n    *   \"our aim is to guide designers and practitioners to understand, build applications, and advance research of gnn4ts.\"\n    *   \"at first, we provide a comprehensive task-oriented taxonomy of gnn4ts.\"\n    *   \"then, we present and discuss representative research works and introduce mainstream applications of gnn4ts.\"\n    *   \"a comprehensive discussion of potential future research directions completes the survey.\"\n    *   \"this survey, for the first time, brings together a vast array of knowledge on gnn-based time series research...\"\n*   **introduction:**\n    *   \"to fill the gap, this survey offers a comprehensive and up-to-date review of graph neural networks for time series analysis...\"\n    *   \"specifically, we first provide two broad views to classify and discuss existing works from the task- and methodology-oriented perspectives.\"\n    *   \"the key contributions of our survey are summarized as follows: first comprehensive survey. unified and structured taxonomy. detailed and current overview. broadening applications. future research directions.\"\n    *   table 1 is titled \"comparison between our survey and other related surveys.\"\n\nall these points directly match the criteria for a **survey** paper: it reviews existing literature comprehensively, discusses literature organization and classification schemes, and aims to provide a comprehensive overview of the state-of-the-art."
    },
    "file_name": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf"
  },
  {
    "success": true,
    "doc_id": "871e92a0b9bc91401fb93bc39b52ffc1",
    "summary": "Here's a focused summary of the paper \"GNNExplainer: Generating Explanations for Graph Neural Networks\" by Ying et al. \\cite{ying2019rza}, tailored for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) are powerful for machine learning on graphs but lack transparency, making their predictions difficult to explain.\n    *   **Importance & Challenge**: Understanding GNN predictions is crucial for increasing trust, ensuring transparency in decision-critical applications (fairness, privacy), and enabling practitioners to debug models and identify systematic errors. The challenge lies in GNNs' complex combination of rich relational graph structure and node feature information, which existing interpretability methods for other neural networks fail to address.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous interpretability methods for non-graph neural networks include:\n        *   Locally approximating models with simpler surrogate models (e.g., LIME-like approaches).\n        *   Identifying relevant features through gradients, backpropagation of neuron contributions, or saliency maps.\n        *   Post-hoc interpretability methods treating models as black boxes.\n        *   Attention mechanisms in some GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods for non-graph NNs do not incorporate relational information, which is fundamental to graph data.\n        *   Saliency maps can be misleading and suffer from gradient saturation, especially with discrete inputs like graph adjacency matrices.\n        *   Attention mechanisms in GNNs often provide global importance (same values for all predictions) and typically do not jointly consider both graph structure and node features for *instance-specific* explanations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{ying2019rza} proposes GNNEXPLAINER, a general, model-agnostic approach that, given a trained GNN and its prediction, identifies a compact subgraph structure and a small subset of node features that are most influential for that prediction.\n    *   **Novelty**:\n        *   Formulates the explanation task as an optimization problem that maximizes the mutual information between the GNN's prediction and the distribution of possible subgraph structures and node features.\n        *   Employs a mean-field variational approximation to learn a real-valued graph mask (for subgraph selection) and simultaneously learns a feature mask (for node feature selection).\n        *   Uses a continuous relaxation of the adjacency matrix and a sigmoid function to mask the computation graph, optimizing a cross-entropy objective via gradient descent.\n        *   Jointly optimizes for both graph structural and node feature importance, which is a key innovation for GNN interpretability.\n        *   Utilizes a reparameterization trick to backpropagate gradients through the binary feature mask.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of GNNEXPLAINER, the first general, model-agnostic framework for providing interpretable, instance-specific explanations for predictions made by *any* GNN model on *any* graph-based machine learning task (node classification, link prediction, graph classification).\n    *   **Joint Optimization**: A novel formulation that jointly identifies both the most important subgraph structure and the most relevant node features contributing to a GNN's prediction.\n    *   **Theoretical Insight/Analysis**: Leveraging mutual information maximization and mean-field variational approximation to tractably optimize for explanations in a complex, non-convex GNN landscape.\n    *   **Scope**: Ability to generate consistent and concise explanations for single instances or entire classes of instances, and to incorporate domain-specific constraints through regularization.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated on synthetic graphs with \"planted\" network motifs (ground truth structures determining labels).\n        *   Tested on real-world datasets, including molecular graphs (e.g., MUTAG, BA-2Motifs) and social interaction networks (Reddit).\n    *   **Key Performance Metrics & Results**:\n        *   **Explanation Accuracy**: On synthetic graphs, GNNEXPLAINER accurately identified the planted subgraphs/motifs and node features, outperforming alternative baseline approaches by up to 43.0% in explanation accuracy.\n        *   **Domain Insights**: On real-world data, it robustly identified important domain-specific graph structures (e.g., NO2 chemical groups, ring structures in molecules; star structures in Reddit threads) and node features that influence GNN predictions.\n        *   Demonstrated consistent and concise explanations across different GNN models and machine learning tasks on graphs.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The approach relies on a continuous relaxation of discrete graph structures and a mean-field variational approximation, which, while empirically effective, involves assumptions (e.g., convexity for Jensen's inequality) that do not strictly hold for non-convex neural networks. The optimization finds a local minimum.\n    *   **Scope of Applicability**: GNNEXPLAINER is designed to be general and model-agnostic, applicable to any GNN architecture and any graph-based machine learning task. It can provide both single-instance and multi-instance explanations.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ying2019rza} significantly advances the technical state-of-the-art by providing the first general, model-agnostic framework for explaining GNN predictions, uniquely addressing the joint importance of graph structure and node features.\n    *   **Potential Impact**:\n        *   Enhances trust and transparency in GNNs, particularly in high-stakes applications.\n        *   Empowers practitioners to gain deeper insights into GNN behavior, debug models, and identify systematic errors before deployment.\n        *   Facilitates scientific discovery by visualizing semantically relevant structures and providing domain-specific insights from GNN predictions.\n        *   Lays foundational groundwork for future research in explainable AI for graph-structured data.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized machine learning on graph-structured data, yet their inherent black-box nature severely impedes trust and adoption in critical applications. Understanding *why* a GNN makes a specific prediction is paramount for debugging, ensuring fairness, and driving scientific discovery. Existing interpretability methods fall short, failing to jointly consider the intricate interplay of graph topology and node features.\n\nWe introduce GNNEXPLAINER, the first general, model-agnostic framework designed to provide interpretable, instance-specific explanations for *any* GNN prediction. GNNEXPLAINER uniquely formulates the explanation task as an optimization problem maximizing the mutual information between the GNN's prediction and a compact subgraph structure alongside a minimal set of node features. Through a novel mean-field variational approximation and continuous relaxation, our method precisely identifies the most influential structural patterns and feature subsets. Extensive experiments on synthetic and real-world datasets demonstrate GNNEXPLAINER's superior accuracy in pinpointing ground-truth explanations, outperforming baselines by up to 43%. This work significantly advances Explainable AI for graphs, fostering transparency, enabling robust GNN development, and unlocking deeper insights into complex relational data.",
    "keywords": [
      "GNNEXPLAINER",
      "Graph Neural Networks (GNNs)",
      "GNN interpretability",
      "instance-specific explanations",
      "joint optimization of graph structure and node features",
      "model-agnostic framework",
      "mutual information maximization",
      "mean-field variational approximation",
      "subgraph structure identification",
      "node feature importance",
      "continuous relaxation",
      "explanation accuracy",
      "graph-based machine learning tasks",
      "transparency and trust in GNNs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/00358a3f17821476d93461192b9229fe7d92bb3f.pdf",
    "citation_key": "ying2019rza",
    "metadata": {
      "title": "GNNExplainer: Generating Explanations for Graph Neural Networks",
      "authors": [
        "Rex Ying",
        "Dylan Bourgeois",
        "Jiaxuan You",
        "M. Zitnik",
        "J. Leskovec"
      ],
      "published_date": "2019",
      "abstract": "Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/00358a3f17821476d93461192b9229fe7d92bb3f.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"GNNExplainer: Generating Explanations for Graph Neural Networks\" by Ying et al. \\cite{ying2019rza}, tailored for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) are powerful for machine learning on graphs but lack transparency, making their predictions difficult to explain.\n    *   **Importance & Challenge**: Understanding GNN predictions is crucial for increasing trust, ensuring transparency in decision-critical applications (fairness, privacy), and enabling practitioners to debug models and identify systematic errors. The challenge lies in GNNs' complex combination of rich relational graph structure and node feature information, which existing interpretability methods for other neural networks fail to address.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous interpretability methods for non-graph neural networks include:\n        *   Locally approximating models with simpler surrogate models (e.g., LIME-like approaches).\n        *   Identifying relevant features through gradients, backpropagation of neuron contributions, or saliency maps.\n        *   Post-hoc interpretability methods treating models as black boxes.\n        *   Attention mechanisms in some GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Existing methods for non-graph NNs do not incorporate relational information, which is fundamental to graph data.\n        *   Saliency maps can be misleading and suffer from gradient saturation, especially with discrete inputs like graph adjacency matrices.\n        *   Attention mechanisms in GNNs often provide global importance (same values for all predictions) and typically do not jointly consider both graph structure and node features for *instance-specific* explanations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{ying2019rza} proposes GNNEXPLAINER, a general, model-agnostic approach that, given a trained GNN and its prediction, identifies a compact subgraph structure and a small subset of node features that are most influential for that prediction.\n    *   **Novelty**:\n        *   Formulates the explanation task as an optimization problem that maximizes the mutual information between the GNN's prediction and the distribution of possible subgraph structures and node features.\n        *   Employs a mean-field variational approximation to learn a real-valued graph mask (for subgraph selection) and simultaneously learns a feature mask (for node feature selection).\n        *   Uses a continuous relaxation of the adjacency matrix and a sigmoid function to mask the computation graph, optimizing a cross-entropy objective via gradient descent.\n        *   Jointly optimizes for both graph structural and node feature importance, which is a key innovation for GNN interpretability.\n        *   Utilizes a reparameterization trick to backpropagate gradients through the binary feature mask.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of GNNEXPLAINER, the first general, model-agnostic framework for providing interpretable, instance-specific explanations for predictions made by *any* GNN model on *any* graph-based machine learning task (node classification, link prediction, graph classification).\n    *   **Joint Optimization**: A novel formulation that jointly identifies both the most important subgraph structure and the most relevant node features contributing to a GNN's prediction.\n    *   **Theoretical Insight/Analysis**: Leveraging mutual information maximization and mean-field variational approximation to tractably optimize for explanations in a complex, non-convex GNN landscape.\n    *   **Scope**: Ability to generate consistent and concise explanations for single instances or entire classes of instances, and to incorporate domain-specific constraints through regularization.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated on synthetic graphs with \"planted\" network motifs (ground truth structures determining labels).\n        *   Tested on real-world datasets, including molecular graphs (e.g., MUTAG, BA-2Motifs) and social interaction networks (Reddit).\n    *   **Key Performance Metrics & Results**:\n        *   **Explanation Accuracy**: On synthetic graphs, GNNEXPLAINER accurately identified the planted subgraphs/motifs and node features, outperforming alternative baseline approaches by up to 43.0% in explanation accuracy.\n        *   **Domain Insights**: On real-world data, it robustly identified important domain-specific graph structures (e.g., NO2 chemical groups, ring structures in molecules; star structures in Reddit threads) and node features that influence GNN predictions.\n        *   Demonstrated consistent and concise explanations across different GNN models and machine learning tasks on graphs.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The approach relies on a continuous relaxation of discrete graph structures and a mean-field variational approximation, which, while empirically effective, involves assumptions (e.g., convexity for Jensen's inequality) that do not strictly hold for non-convex neural networks. The optimization finds a local minimum.\n    *   **Scope of Applicability**: GNNEXPLAINER is designed to be general and model-agnostic, applicable to any GNN architecture and any graph-based machine learning task. It can provide both single-instance and multi-instance explanations.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ying2019rza} significantly advances the technical state-of-the-art by providing the first general, model-agnostic framework for explaining GNN predictions, uniquely addressing the joint importance of graph structure and node features.\n    *   **Potential Impact**:\n        *   Enhances trust and transparency in GNNs, particularly in high-stakes applications.\n        *   Empowers practitioners to gain deeper insights into GNN behavior, debug models, and identify systematic errors before deployment.\n        *   Facilitates scientific discovery by visualizing semantically relevant structures and providing domain-specific insights from GNN predictions.\n        *   Lays foundational groundwork for future research in explainable AI for graph-structured data.",
      "keywords": [
        "GNNEXPLAINER",
        "Graph Neural Networks (GNNs)",
        "GNN interpretability",
        "instance-specific explanations",
        "joint optimization of graph structure and node features",
        "model-agnostic framework",
        "mutual information maximization",
        "mean-field variational approximation",
        "subgraph structure identification",
        "node feature importance",
        "continuous relaxation",
        "explanation accuracy",
        "graph-based machine learning tasks",
        "transparency and trust in GNNs"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **technical** paper.\n\nhere's why:\n\n*   **abstract mentions:** \"here we **propose gnnexplainer**, the ﬁrst general, model-agnostic **approach** for providing interpretable explanations...\", \"we **formulate gnnexplainer as an optimization task**...\", \"experiments... show that **our approach** can identify important graph structures...\"\n*   **introduction discusses:** the problem of gnns lacking transparency, the importance of explainability, and the lack of existing methods for explaining gnns, setting the stage for the proposed solution.\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it includes empirical evaluation, the core contribution is the development and proposal of the gnnexplainer method itself."
    },
    "file_name": "00358a3f17821476d93461192b9229fe7d92bb3f.pdf"
  },
  {
    "success": true,
    "doc_id": "e7fecd38f96af8097db79de09f87fe4f",
    "summary": "Here's a focused summary of the paper \\cite{liu2020w3t} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer from significant performance degradation when stacking multiple layers to increase their receptive fields. This limits their ability to capture long-range dependencies in graphs.\n    *   **Importance & Challenge:** Graphs are ubiquitous, and GNNs have shown great success. However, the inability to build deeper GNNs restricts their power, especially for tasks requiring information from distant nodes or in semi-supervised settings with limited labeled data where training signals need to propagate widely. The challenge lies in understanding *why* performance degrades and developing architectures that can effectively leverage larger receptive fields.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper acknowledges that current GNNs (e.g., GCN \\cite{liu2020w3t} [11], GraphSAGE \\cite{liu2020w3t} [9], GAT \\cite{liu2020w3t} [30], GIN \\cite{liu2020w3t} [32]) typically perform neighborhood aggregation. It builds upon the existing understanding of the \"over-smoothing\" issue, which posits that repeated propagation makes node representations indistinguishable \\cite{liu2020w3t} [3, 15, 33].\n    *   **Limitations of Previous Solutions:** Previous works attribute performance degradation primarily to over-smoothing. Solutions include smoothness regularizers \\cite{liu2020w3t} [3], layer-aggregation mechanisms (e.g., Jumping Knowledge Network \\cite{liu2020w3t} [33]), and personalized PageRank-based propagation \\cite{liu2020w3t} [12]. \\cite{liu2020w3t} argues that for *moderate* depths, over-smoothing is not the primary culprit, and previous descriptions of over-smoothing often simplify assumptions.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{liu2020w3t} systematically analyzes the performance degradation in deep GNNs and argues that the *key factor* is the **entanglement of representation transformation and propagation** in current graph convolution operations (e.g., GCN).\n    *   **Novelty/Difference:**\n        *   **Decoupling Operations:** The paper proposes to decouple these two operations: first apply a transformation (e.g., an MLP) to the initial features, then perform *k*-steps of propagation. This allows for deep propagation without increasing the number of trainable parameters per propagation step. The proposed model for analysis is `Z = MLP(X)` followed by `X_out = softmax(bA^k Z)`.\n        *   **Re-evaluation of Over-smoothing:** \\cite{liu2020w3t} demonstrates that after decoupling, deeper GNNs can be built without performance degradation, and over-smoothing only becomes an issue at *extremely* large receptive fields (e.g., 75-hop).\n        *   **Theoretical Analysis:** Provides a more rigorous theoretical analysis of over-smoothing, showing that node representations become indistinguishable when depth approaches infinity, aligning with the over-smoothing concept but with fewer simplifying assumptions.\n        *   **Deep Adaptive Graph Neural Network (DAGNN):** Based on their analysis, they propose DAGNN to adaptively incorporate information from large receptive fields. (Specific architectural details of DAGNN are not fully elaborated in the provided abstract/introduction, beyond the core decoupling principle).\n        *   **Quantitative Smoothness Metric:** Introduces `SMV_G` (Smoothness Metric Value for Graph) to quantitatively measure the overall smoothness of node representations, defined as the average Euclidean distance between normalized node representations.\n\n*   **Key Technical Contributions**\n    *   A systematic analysis identifying the entanglement of representation transformation and propagation as the primary cause of performance degradation in moderately deep GNNs.\n    *   Empirical and theoretical evidence supporting the effectiveness of decoupling these operations for building deeper GNNs.\n    *   A more rigorous theoretical analysis of the over-smoothing phenomenon.\n    *   The proposal of Deep Adaptive Graph Neural Network (DAGNN) based on these insights.\n    *   A novel quantitative metric (`SMV_G`) for measuring the smoothness of graph node representations.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated standard GCNs with varying depths (0 to many layers) on node classification.\n        *   Evaluated their proposed decoupled model (Eq. 6) with varying propagation steps (`k`).\n        *   Used t-SNE visualization to qualitatively assess the discriminative power of learned node representations.\n        *   Utilized their `SMV_G` metric to quantitatively track representation smoothness.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Datasets:** Citation datasets (Cora, CiteSeer, PubMed), co-authorship, and co-purchase datasets.\n        *   **GCN Performance:** Standard GCNs showed initial accuracy increase but degraded dramatically after 3 layers, with t-SNE visualizations showing indistinguishable representations at 6 layers. `SMV_G` showed a slight downward trend, not near zero, suggesting over-smoothing wasn't severe at these depths.\n        *   **Decoupled Model Performance:** The decoupled model (Eq. 6) maintained high test accuracy and discriminative power (t-SNE) even with large receptive fields (e.g., 50-hop on Cora). Performance degradation and `SMV_G` values approaching zero (indicating severe over-smoothing) only occurred at *extremely* large receptive fields (e.g., 75-hop on Cora).\n        *   **Conclusion:** Experiments confirmed that decoupling allows for deeper GNNs without performance degradation, and over-smoothing is a concern only at very extreme depths.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The analysis primarily focuses on unweighted and undirected graphs. While the core insight of decoupling is general, its direct applicability to more complex graph types (e.g., directed, weighted, heterogeneous) might require further investigation. The specific architecture of DAGNN is not fully detailed in the provided text, making a deeper assessment of its unique limitations difficult.\n    *   **Scope of Applicability:** The findings are particularly relevant for tasks requiring long-range information propagation, such as node classification in large graphs or semi-supervised learning where training signals need to reach distant unlabeled nodes.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{liu2020w3t} significantly advances the understanding of deep GNNs by challenging the prevailing view that over-smoothing is the *sole* or *primary* cause of performance degradation at moderate depths. It provides a new, more nuanced explanation centered on the entanglement of operations.\n    *   **Potential Impact:** This work opens new avenues for designing truly deep GNNs capable of leveraging large receptive fields. By decoupling transformation and propagation, it offers a principled way to build more powerful GNN architectures, potentially leading to improved performance in various graph-based learning tasks, especially those requiring global context or effective signal propagation across sparse training data. The rigorous theoretical analysis also contributes to a deeper foundational understanding of GNN behavior.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph-based learning, yet their ability to capture long-range dependencies remains severely hampered by performance degradation in deep architectures. While \"over-smoothing\" is widely cited as the primary culprit, this paper unveils a more fundamental bottleneck: the entanglement of representation transformation and propagation within conventional graph convolution operations.\n\nWe systematically demonstrate that for moderately deep GNNs, this entanglement, rather than over-smoothing, is the key factor limiting performance. Pioneering a novel paradigm, we propose to decouple these operations, first transforming node features and then performing *k*-step propagation. Our rigorous theoretical and empirical analysis reveals that, with decoupling, GNNs can achieve significantly larger receptive fields (e.g., 50-hop) without degradation, with over-smoothing only becoming problematic at *extremely* deep levels (e.g., 75-hop).\n\nBased on these insights, we introduce the Deep Adaptive Graph Neural Network (DAGNN) and a quantitative Smoothness Metric Value for Graph (SMV_G). This work fundamentally redefines the path to truly deep GNNs, enabling robust capture of long-range dependencies crucial for complex graph tasks like node classification, and opening new avenues for powerful, scalable graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Deep GNNs degradation",
      "receptive fields",
      "over-smoothing re-evaluation",
      "entanglement of representation transformation and propagation",
      "decoupling operations",
      "Deep Adaptive Graph Neural Network (DAGNN)",
      "Smoothness Metric Value for Graph (SMV_G)",
      "long-range dependencies",
      "node classification",
      "theoretical analysis",
      "semi-supervised learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/639206a9a32d91386924f1c94e9760dfb43df72e.pdf",
    "citation_key": "liu2020w3t",
    "metadata": {
      "title": "Towards Deeper Graph Neural Networks",
      "authors": [
        "Meng Liu",
        "Hongyang Gao",
        "Shuiwang Ji"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.",
      "file_path": "paper_data/Graph_Neural_Networks/639206a9a32d91386924f1c94e9760dfb43df72e.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{liu2020w3t} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer from significant performance degradation when stacking multiple layers to increase their receptive fields. This limits their ability to capture long-range dependencies in graphs.\n    *   **Importance & Challenge:** Graphs are ubiquitous, and GNNs have shown great success. However, the inability to build deeper GNNs restricts their power, especially for tasks requiring information from distant nodes or in semi-supervised settings with limited labeled data where training signals need to propagate widely. The challenge lies in understanding *why* performance degrades and developing architectures that can effectively leverage larger receptive fields.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper acknowledges that current GNNs (e.g., GCN \\cite{liu2020w3t} [11], GraphSAGE \\cite{liu2020w3t} [9], GAT \\cite{liu2020w3t} [30], GIN \\cite{liu2020w3t} [32]) typically perform neighborhood aggregation. It builds upon the existing understanding of the \"over-smoothing\" issue, which posits that repeated propagation makes node representations indistinguishable \\cite{liu2020w3t} [3, 15, 33].\n    *   **Limitations of Previous Solutions:** Previous works attribute performance degradation primarily to over-smoothing. Solutions include smoothness regularizers \\cite{liu2020w3t} [3], layer-aggregation mechanisms (e.g., Jumping Knowledge Network \\cite{liu2020w3t} [33]), and personalized PageRank-based propagation \\cite{liu2020w3t} [12]. \\cite{liu2020w3t} argues that for *moderate* depths, over-smoothing is not the primary culprit, and previous descriptions of over-smoothing often simplify assumptions.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{liu2020w3t} systematically analyzes the performance degradation in deep GNNs and argues that the *key factor* is the **entanglement of representation transformation and propagation** in current graph convolution operations (e.g., GCN).\n    *   **Novelty/Difference:**\n        *   **Decoupling Operations:** The paper proposes to decouple these two operations: first apply a transformation (e.g., an MLP) to the initial features, then perform *k*-steps of propagation. This allows for deep propagation without increasing the number of trainable parameters per propagation step. The proposed model for analysis is `Z = MLP(X)` followed by `X_out = softmax(bA^k Z)`.\n        *   **Re-evaluation of Over-smoothing:** \\cite{liu2020w3t} demonstrates that after decoupling, deeper GNNs can be built without performance degradation, and over-smoothing only becomes an issue at *extremely* large receptive fields (e.g., 75-hop).\n        *   **Theoretical Analysis:** Provides a more rigorous theoretical analysis of over-smoothing, showing that node representations become indistinguishable when depth approaches infinity, aligning with the over-smoothing concept but with fewer simplifying assumptions.\n        *   **Deep Adaptive Graph Neural Network (DAGNN):** Based on their analysis, they propose DAGNN to adaptively incorporate information from large receptive fields. (Specific architectural details of DAGNN are not fully elaborated in the provided abstract/introduction, beyond the core decoupling principle).\n        *   **Quantitative Smoothness Metric:** Introduces `SMV_G` (Smoothness Metric Value for Graph) to quantitatively measure the overall smoothness of node representations, defined as the average Euclidean distance between normalized node representations.\n\n*   **Key Technical Contributions**\n    *   A systematic analysis identifying the entanglement of representation transformation and propagation as the primary cause of performance degradation in moderately deep GNNs.\n    *   Empirical and theoretical evidence supporting the effectiveness of decoupling these operations for building deeper GNNs.\n    *   A more rigorous theoretical analysis of the over-smoothing phenomenon.\n    *   The proposal of Deep Adaptive Graph Neural Network (DAGNN) based on these insights.\n    *   A novel quantitative metric (`SMV_G`) for measuring the smoothness of graph node representations.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated standard GCNs with varying depths (0 to many layers) on node classification.\n        *   Evaluated their proposed decoupled model (Eq. 6) with varying propagation steps (`k`).\n        *   Used t-SNE visualization to qualitatively assess the discriminative power of learned node representations.\n        *   Utilized their `SMV_G` metric to quantitatively track representation smoothness.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Datasets:** Citation datasets (Cora, CiteSeer, PubMed), co-authorship, and co-purchase datasets.\n        *   **GCN Performance:** Standard GCNs showed initial accuracy increase but degraded dramatically after 3 layers, with t-SNE visualizations showing indistinguishable representations at 6 layers. `SMV_G` showed a slight downward trend, not near zero, suggesting over-smoothing wasn't severe at these depths.\n        *   **Decoupled Model Performance:** The decoupled model (Eq. 6) maintained high test accuracy and discriminative power (t-SNE) even with large receptive fields (e.g., 50-hop on Cora). Performance degradation and `SMV_G` values approaching zero (indicating severe over-smoothing) only occurred at *extremely* large receptive fields (e.g., 75-hop on Cora).\n        *   **Conclusion:** Experiments confirmed that decoupling allows for deeper GNNs without performance degradation, and over-smoothing is a concern only at very extreme depths.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The analysis primarily focuses on unweighted and undirected graphs. While the core insight of decoupling is general, its direct applicability to more complex graph types (e.g., directed, weighted, heterogeneous) might require further investigation. The specific architecture of DAGNN is not fully detailed in the provided text, making a deeper assessment of its unique limitations difficult.\n    *   **Scope of Applicability:** The findings are particularly relevant for tasks requiring long-range information propagation, such as node classification in large graphs or semi-supervised learning where training signals need to reach distant unlabeled nodes.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{liu2020w3t} significantly advances the understanding of deep GNNs by challenging the prevailing view that over-smoothing is the *sole* or *primary* cause of performance degradation at moderate depths. It provides a new, more nuanced explanation centered on the entanglement of operations.\n    *   **Potential Impact:** This work opens new avenues for designing truly deep GNNs capable of leveraging large receptive fields. By decoupling transformation and propagation, it offers a principled way to build more powerful GNN architectures, potentially leading to improved performance in various graph-based learning tasks, especially those requiring global context or effective signal propagation across sparse training data. The rigorous theoretical analysis also contributes to a deeper foundational understanding of GNN behavior.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Deep GNNs degradation",
        "receptive fields",
        "over-smoothing re-evaluation",
        "entanglement of representation transformation and propagation",
        "decoupling operations",
        "Deep Adaptive Graph Neural Network (DAGNN)",
        "Smoothness Metric Value for Graph (SMV_G)",
        "long-range dependencies",
        "node classification",
        "theoretical analysis",
        "semi-supervised learning"
      ],
      "paper_type": "this paper should be classified as **technical**.\n\nhere's why:\n\n*   **core contribution:** the abstract explicitly states, \"based on our theoretical and empirical analysis, we **propose deep adaptive graph neural network (dagnn)** to adaptively incorporate information from large receptive fields.\" this is the hallmark of a technical paper: presenting a new method, algorithm, or system.\n*   **problem and solution:** the paper identifies a technical problem (\"performance decreases when going deeper\" due to \"over-smoothing\") and proposes a solution (\"decoupling these two operations\" leading to dagnn).\n*   **supporting elements:** while the paper includes \"theoretical analysis\" and \"experiments on citation, co-authorship, and co-purchase datasets\" (empirical elements), these serve to support and validate the *proposed new method* (dagnn). they are components of the technical contribution, not the sole focus of the paper."
    },
    "file_name": "639206a9a32d91386924f1c94e9760dfb43df72e.pdf"
  },
  {
    "success": true,
    "doc_id": "99f9190bd2ee70635e08f9265b4b028f",
    "summary": "Here's a focused summary of the paper \\cite{longa202399q} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** While Graph Neural Networks (GNNs) excel at learning on static graph-structured data, many real-world systems are dynamic, with graphs and their attributes changing over time. Traditional graph models and static GNNs are ill-equipped to capture this temporal evolution \\cite{longa202399q}.\n    *   **Importance and Challenge:** Extending GNN capabilities to temporal graphs (Temporal GNNs or TGNNs) is a promising research area. However, the field lacked a comprehensive, systematized overview, a rigorous formalization of learning settings and tasks, and a unified taxonomy for existing TGNN approaches \\cite{longa202399q}. Existing surveys were either too general, too specific, or did not provide in-depth coverage of TGNNs \\cite{longa202399q}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a survey that specifically focuses on GNN-based models for temporal graphs, acknowledging other approaches like matrix factorization, temporal motif-based methods, random walks (e.g., DyANE \\cite{longa202399q}), temporal point processes (e.g., DyRep \\cite{longa202399q}), NMF, and other deep learning techniques (e.g., DynGem \\cite{longa202399q}, TRRN \\cite{longa202399q}, STAR \\cite{longa202399q}, TSNet \\cite{longa202399q}) that also address temporal graphs \\cite{longa202399q}.\n    *   **Limitations of Previous Solutions (Surveys):** Previous surveys either discussed general temporal graph learning techniques with only brief mentions of GNNs, focused on narrow topics like temporal link prediction or graph generation, or provided GNN overviews without deep coverage of temporal aspects \\cite{longa202399q}. This work aims to fill the gap by providing the first comprehensive systematization of GNN-based methods for temporal graphs \\cite{longa202399q}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive survey that systematically organizes and formalizes the field of Temporal Graph Neural Networks (TGNNs). It does not propose a new TGNN model but rather provides a foundational framework for understanding existing ones \\cite{longa202399q}.\n    *   **Novelty/Difference:** The core innovation lies in its structured approach to the literature:\n        *   It introduces a rigorous formalization of learning settings and tasks specific to temporal graphs \\cite{longa202399q}.\n        *   It proposes a novel taxonomy that categorizes existing TGNN approaches based on how the temporal aspect is represented (Snapshot-based Temporal Graphs - STG, or Event-based Temporal Graphs - ETG) and processed \\cite{longa202399q}.\n        *   It defines distinct learning settings: transductive vs. inductive, and past vs. future prediction, for temporal graph tasks \\cite{longa202399q}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A coherent formalization of different learning settings (transductive, inductive, past prediction, future prediction) and tasks (e.g., temporal node classification, link prediction) for temporal graphs, unifying disparate definitions found in the literature \\cite{longa202399q}.\n        *   A comprehensive taxonomy that groups existing TGNN methods based on their temporal representation strategy (snapshot-based vs. event-based) and the mechanism used to incorporate time \\cite{longa202399q}.\n        *   Formal definitions for various types of temporal graphs, including Static Graphs (SG), Temporal Graphs (TG), Discrete Time Temporal Graphs (DTTG), Snapshot-based Temporal Graphs (STG), and Event-based Temporal Graphs (ETG) \\cite{longa202399q}.\n    *   **Theoretical Insights or Analysis:** The paper provides a structured theoretical framework for the field, clarifying fundamental concepts, learning objectives, and evaluation contexts for TGNNs \\cite{longa202399q}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{longa202399q} does not present new experimental results from its own proposed models. Instead, it reviews and categorizes the experimental validations performed by the individual TGNN papers it surveys.\n    *   **Key Performance Metrics and Comparison Results:** The paper notes that TGNNs have achieved state-of-the-art results on tasks such as temporal link prediction, node classification, and edge classification \\cite{longa202399q}. It sets the context for understanding how these models are typically evaluated, but does not provide specific metrics or comparative results within the survey itself.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):** The survey explicitly focuses on methods that leverage and adapt the GNN framework to temporal graphs, acknowledging but not deeply exploring other non-GNN approaches for temporal graph learning \\cite{longa202399q}.\n    *   **Limitations of Current TGNN Methods (as identified by the survey):** The paper concludes with a discussion of the most relevant open challenges and limitations of current TGNN methods, from both research and application perspectives, though these specific challenges are not detailed in the provided abstract/introduction \\cite{longa202399q}.\n    *   **Scope of Applicability:** TGNNs are applicable to a wide range of dynamic systems, including recommendation systems, social network analysis, transportation systems, face-to-face interactions, human mobility, and epidemic modeling \\cite{longa202399q}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematized overview of Temporal Graph Neural Networks, filling a critical gap in the existing literature \\cite{longa202399q}.\n    *   **Potential Impact on Future Research:**\n        *   It offers a unified formalization and common language, which is crucial for facilitating comparison, understanding, and the development of new TGNN models \\cite{longa202399q}.\n        *   By highlighting limitations and open challenges, it provides a clear roadmap for future research directions in the field \\cite{longa202399q}.\n        *   The proposed taxonomy and formal definitions will serve as a foundational reference for researchers and practitioners working with dynamic graph data \\cite{longa202399q}.",
    "intriguing_abstract": "The real world is dynamic, yet traditional Graph Neural Networks (GNNs) primarily excel on static graph structures, leaving the rich temporal evolution of data largely unaddressed. While Temporal Graph Neural Networks (TGNNs) offer a powerful paradigm for modeling dynamic systems, the field has lacked a comprehensive, systematized overview to guide research and development. This paper fills that critical void, presenting the first rigorous formalization and unified taxonomy for GNN-based methods on temporal graphs.\n\nWe introduce a novel framework that categorizes existing TGNN approaches based on their temporal representation, distinguishing between Snapshot-based Temporal Graphs (STG) and Event-based Temporal Graphs (ETG). Furthermore, we formalize diverse learning settings—including transductive, inductive, past, and future prediction—for tasks like temporal node classification and link prediction. This foundational survey provides a common language, clarifies fundamental concepts, and outlines open challenges, serving as an indispensable roadmap for researchers and practitioners navigating the complex landscape of dynamic graph data and accelerating the next generation of TGNN innovations.",
    "keywords": [
      "Temporal Graph Neural Networks (TGNNs)",
      "Temporal graphs",
      "Comprehensive systematization",
      "Formalization of learning settings",
      "Unified taxonomy",
      "Snapshot-based Temporal Graphs (STG)",
      "Event-based Temporal Graphs (ETG)",
      "Temporal link prediction",
      "Node classification",
      "GNN-based methods",
      "Open challenges",
      "Dynamic systems applications",
      "State-of-the-art performance"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/b88f456daaf29860d2b59c621be3bd878a581a59.pdf",
    "citation_key": "longa202399q",
    "metadata": {
      "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
      "authors": [
        "Antonio Longa",
        "Veronica Lachi",
        "G. Santin",
        "M. Bianchini",
        "B. Lepri",
        "P. Lió",
        "F. Scarselli",
        "Andrea Passerini"
      ],
      "published_date": "2023",
      "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
      "file_path": "paper_data/Graph_Neural_Networks/b88f456daaf29860d2b59c621be3bd878a581a59.pdf",
      "venue": "Trans. Mach. Learn. Res.",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{longa202399q} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** While Graph Neural Networks (GNNs) excel at learning on static graph-structured data, many real-world systems are dynamic, with graphs and their attributes changing over time. Traditional graph models and static GNNs are ill-equipped to capture this temporal evolution \\cite{longa202399q}.\n    *   **Importance and Challenge:** Extending GNN capabilities to temporal graphs (Temporal GNNs or TGNNs) is a promising research area. However, the field lacked a comprehensive, systematized overview, a rigorous formalization of learning settings and tasks, and a unified taxonomy for existing TGNN approaches \\cite{longa202399q}. Existing surveys were either too general, too specific, or did not provide in-depth coverage of TGNNs \\cite{longa202399q}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a survey that specifically focuses on GNN-based models for temporal graphs, acknowledging other approaches like matrix factorization, temporal motif-based methods, random walks (e.g., DyANE \\cite{longa202399q}), temporal point processes (e.g., DyRep \\cite{longa202399q}), NMF, and other deep learning techniques (e.g., DynGem \\cite{longa202399q}, TRRN \\cite{longa202399q}, STAR \\cite{longa202399q}, TSNet \\cite{longa202399q}) that also address temporal graphs \\cite{longa202399q}.\n    *   **Limitations of Previous Solutions (Surveys):** Previous surveys either discussed general temporal graph learning techniques with only brief mentions of GNNs, focused on narrow topics like temporal link prediction or graph generation, or provided GNN overviews without deep coverage of temporal aspects \\cite{longa202399q}. This work aims to fill the gap by providing the first comprehensive systematization of GNN-based methods for temporal graphs \\cite{longa202399q}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive survey that systematically organizes and formalizes the field of Temporal Graph Neural Networks (TGNNs). It does not propose a new TGNN model but rather provides a foundational framework for understanding existing ones \\cite{longa202399q}.\n    *   **Novelty/Difference:** The core innovation lies in its structured approach to the literature:\n        *   It introduces a rigorous formalization of learning settings and tasks specific to temporal graphs \\cite{longa202399q}.\n        *   It proposes a novel taxonomy that categorizes existing TGNN approaches based on how the temporal aspect is represented (Snapshot-based Temporal Graphs - STG, or Event-based Temporal Graphs - ETG) and processed \\cite{longa202399q}.\n        *   It defines distinct learning settings: transductive vs. inductive, and past vs. future prediction, for temporal graph tasks \\cite{longa202399q}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A coherent formalization of different learning settings (transductive, inductive, past prediction, future prediction) and tasks (e.g., temporal node classification, link prediction) for temporal graphs, unifying disparate definitions found in the literature \\cite{longa202399q}.\n        *   A comprehensive taxonomy that groups existing TGNN methods based on their temporal representation strategy (snapshot-based vs. event-based) and the mechanism used to incorporate time \\cite{longa202399q}.\n        *   Formal definitions for various types of temporal graphs, including Static Graphs (SG), Temporal Graphs (TG), Discrete Time Temporal Graphs (DTTG), Snapshot-based Temporal Graphs (STG), and Event-based Temporal Graphs (ETG) \\cite{longa202399q}.\n    *   **Theoretical Insights or Analysis:** The paper provides a structured theoretical framework for the field, clarifying fundamental concepts, learning objectives, and evaluation contexts for TGNNs \\cite{longa202399q}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{longa202399q} does not present new experimental results from its own proposed models. Instead, it reviews and categorizes the experimental validations performed by the individual TGNN papers it surveys.\n    *   **Key Performance Metrics and Comparison Results:** The paper notes that TGNNs have achieved state-of-the-art results on tasks such as temporal link prediction, node classification, and edge classification \\cite{longa202399q}. It sets the context for understanding how these models are typically evaluated, but does not provide specific metrics or comparative results within the survey itself.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):** The survey explicitly focuses on methods that leverage and adapt the GNN framework to temporal graphs, acknowledging but not deeply exploring other non-GNN approaches for temporal graph learning \\cite{longa202399q}.\n    *   **Limitations of Current TGNN Methods (as identified by the survey):** The paper concludes with a discussion of the most relevant open challenges and limitations of current TGNN methods, from both research and application perspectives, though these specific challenges are not detailed in the provided abstract/introduction \\cite{longa202399q}.\n    *   **Scope of Applicability:** TGNNs are applicable to a wide range of dynamic systems, including recommendation systems, social network analysis, transportation systems, face-to-face interactions, human mobility, and epidemic modeling \\cite{longa202399q}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematized overview of Temporal Graph Neural Networks, filling a critical gap in the existing literature \\cite{longa202399q}.\n    *   **Potential Impact on Future Research:**\n        *   It offers a unified formalization and common language, which is crucial for facilitating comparison, understanding, and the development of new TGNN models \\cite{longa202399q}.\n        *   By highlighting limitations and open challenges, it provides a clear roadmap for future research directions in the field \\cite{longa202399q}.\n        *   The proposed taxonomy and formal definitions will serve as a foundational reference for researchers and practitioners working with dynamic graph data \\cite{longa202399q}.",
      "keywords": [
        "Temporal Graph Neural Networks (TGNNs)",
        "Temporal graphs",
        "Comprehensive systematization",
        "Formalization of learning settings",
        "Unified taxonomy",
        "Snapshot-based Temporal Graphs (STG)",
        "Event-based Temporal Graphs (ETG)",
        "Temporal link prediction",
        "Node classification",
        "GNN-based methods",
        "Open challenges",
        "Dynamic systems applications",
        "State-of-the-art performance"
      ],
      "paper_type": "based on the abstract and introduction, this paper clearly fits the **survey** type.\n\nhere's why:\n\n*   **abstract:**\n    *   explicitly states: \"we provide the first comprehensive overview of the current state-of-the-art of temporal gnn\".\n    *   mentions: \"introducing a rigorous formalization... and a novel taxonomy categorizing existing approaches\". this describes the methodology of the survey.\n    *   concludes with: \"we conclude the survey with a discussion of the most relevant open challenges\". the word \"survey\" is used directly.\n*   **introduction:**\n    *   discusses the importance and emergence of gnns for temporal graphs, setting the stage for a comprehensive review of the field.\n    *   highlights the success of tgnns and the need to explore their potential, implying a need to consolidate and understand existing work.\n\nthese points directly align with the criteria for a **survey** paper."
    },
    "file_name": "b88f456daaf29860d2b59c621be3bd878a581a59.pdf"
  },
  {
    "success": true,
    "doc_id": "7c06cf30cdbb69008f9c5268d9c57eaa",
    "summary": "Here's a focused summary of the paper \"DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks\" \\cite{papp20211ac} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressiveness, often unable to distinguish even simple graph structures, a limitation tied to the Weisfeiler-Lehman (WL) test \\cite{papp20211ac}.\n    *   This limitation hinders GNNs from recognizing certain distinct graph neighborhoods, impacting their performance in applications where graph structure is crucial \\cite{papp20211ac}.\n    *   The problem is challenging because increasing expressiveness often leads to high computational costs (e.g., k-tuple GNNs) or poor generalization (e.g., adding unique IDs) \\cite{papp20211ac}.\n\n*   **Related Work & Positioning**\n    *   **Limitations of previous solutions**:\n        *   **Feature augmentation (e.g., port numbers, angles, unique IDs, random features)**: While some augmentations help, they often fail to distinguish simple graphs \\cite{papp20211ac}. Adding IDs or random features can lead to overfitting and poor generalization to unseen graphs \\cite{papp20211ac}.\n        *   **K-tuple GNNs**: Increase expressiveness by operating on tuples of nodes, but this leads to a quadratic blow-up in graph size and computational cost, destroying local semantics \\cite{papp20211ac}.\n        *   **Randomized smoothing**: Also uses multiple runs on perturbed data, but aims for robustness by smoothing out atypical variants. In contrast, DropGNN *identifies* these perturbed variants to distinguish graphs \\cite{papp20211ac}.\n        *   **Dropout regularization**: A common technique to prevent co-adaptation during training. DropGNN's dropout is fundamentally different, applied during both training and testing to observe diverse neighborhood patterns, not just for regularization \\cite{papp20211ac}.\n    *   **Positioning**: DropGNN offers a novel approach that increases expressiveness beyond the WL-test with only a small overhead (run repetition), while preserving local graph structure and generalizing well, unlike many prior methods \\cite{papp20211ac}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: DropGNNs execute multiple independent runs of a standard GNN on the input graph \\cite{papp20211ac}. In each run, nodes are randomly and independently \"dropped out\" (removed) with a small probability `p` \\cite{papp20211ac}.\n    *   **Mechanism**: Dropped nodes do not send or receive messages, effectively perturbing the `d`-hop neighborhood of other nodes \\cite{papp20211ac}. This allows the GNN to observe various slightly altered versions of a node's neighborhood \\cite{papp20211ac}.\n    *   **Run Aggregation**: After `r` runs, each node has `r` distinct final embeddings. These are merged into a single final embedding using a permutation-invariant run aggregation function (e.g., sum or max, often preceded by a non-linear transformation) before the final readout \\cite{papp20211ac}.\n    *   **Novelty**: The key innovation is using random node dropouts *during both training and testing* across multiple runs, not for regularization, but to systematically explore perturbed graph structures. This allows the GNN to learn to distinguish subtle differences in neighborhood patterns that are otherwise indistinguishable by standard message-passing GNNs \\cite{papp20211ac}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of Dropout Graph Neural Networks (DropGNNs) as a new paradigm for increasing GNN expressiveness \\cite{papp20211ac}.\n    *   **Theoretical Proofs of Expressiveness**: Proves that DropGNNs can distinguish various graph neighborhoods (e.g., cycles of different lengths, specific structural patterns) that are beyond the capabilities of message-passing GNNs and even some extended GNN variants \\cite{papp20211ac}.\n    *   **Theoretical Bounds for Runs**: Derives theoretical bounds for the number of runs (`r`) required to ensure a reliable distribution of dropouts, particularly focusing on observing \"1-complete dropouts\" (where each single node dropout is observed multiple times) \\cite{papp20211ac}.\n    *   **Enhanced Aggregation**: Demonstrates that dropouts can increase the expressive power of GNNs even with simpler aggregation methods like `mean` aggregation, which is typically less expressive than `sum` \\cite{papp20211ac}.\n\n*   **Experimental Validation**\n    *   **Expressiveness Validation**: Experiments were conducted on established problems known to be impossible for standard GNNs (e.g., distinguishing specific non-isomorphic graphs that fool the WL-test) \\cite{papp20211ac}.\n    *   **Key Results on Expressiveness**: DropGNNs clearly outperformed competing methods on these \"impossible\" datasets, validating the theoretical findings on increased expressiveness \\cite{papp20211ac}.\n    *   **Benchmark Performance**: DropGNNs achieved competitive performance on several established GNN benchmarks \\cite{papp20211ac}.\n    *   **Specific Strengths**: Showed particularly impressive results in applications where the graph structure is a crucial factor \\cite{papp20211ac}.\n\n*   **Limitations & Scope**\n    *   **Computational Cost**: While more efficient than k-tuple GNNs, the need for `r` runs introduces an overhead. Observing *all* possible dropout combinations is exponentially complex and not viable in practice \\cite{papp20211ac}.\n    *   **Focus on 1-complete dropouts**: The theoretical analysis and practical application often focus on ensuring sufficient observation of 1-dropouts (single node removals), as observing higher-order dropouts becomes increasingly rare and computationally intensive \\cite{papp20211ac}.\n    *   **Hyperparameter `\\Gamma`**: The choice of the neighborhood size `\\Gamma` (the region of interest for dropout distribution) is a trade-off hyperparameter, affecting the required number of runs and the observed variations \\cite{papp20211ac}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: DropGNN significantly advances the technical state-of-the-art by providing a principled and effective method to overcome the expressiveness limitations of standard GNNs, particularly those tied to the WL-test \\cite{papp20211ac}.\n    *   **Novel Paradigm**: Introduces a novel paradigm of using random perturbations during inference (and training) across multiple runs to enhance structural awareness, distinct from regularization or robustness techniques \\cite{papp20211ac}.\n    *   **Potential Impact**: Opens new avenues for designing more powerful GNN architectures capable of distinguishing complex graph structures with relatively low overhead, potentially impacting fields like quantum chemistry, physics, and social networks where structural understanding is paramount \\cite{papp20211ac}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their expressiveness is fundamentally limited by the Weisfeiler-Lehman (WL) test, preventing them from distinguishing many crucial graph structures. This bottleneck severely impacts their ability to learn rich graph representations. We introduce **DropGNN**, a novel paradigm that dramatically enhances GNN expressiveness with minimal overhead.\n\nUnlike conventional dropout used solely for regularization, DropGNN employs random node dropouts *during both training and inference* across multiple independent runs. By systematically perturbing `d`-hop neighborhoods, DropGNN allows the underlying GNN to observe diverse structural patterns that are otherwise indistinguishable by standard message-passing mechanisms. We theoretically prove that DropGNNs can distinguish a wide array of graph structures, including cycles of varying lengths, that are beyond the capabilities of the 1-WL test and many advanced GNN variants.\n\nOur framework includes theoretical bounds for the number of runs required and demonstrates enhanced expressiveness even with simpler aggregation functions. Experimentally, DropGNN significantly outperforms state-of-the-art methods on challenging graph isomorphism tasks designed to fool WL-test-limited GNNs, while maintaining competitive performance on standard benchmarks. DropGNN offers a principled, efficient solution to a long-standing challenge in graph representation learning, paving the way for more powerful GNNs in structure-critical applications from chemistry to social networks.",
    "keywords": [
      "DropGNN",
      "Graph Neural Networks (GNNs)",
      "expressiveness limitations",
      "Weisfeiler-Lehman (WL) test",
      "random node dropouts",
      "multiple independent runs",
      "run aggregation",
      "theoretical proofs of expressiveness",
      "distinguishing graph structures",
      "novel GNN paradigm",
      "computational overhead",
      "structural awareness",
      "perturbed graph structures"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/0d4184cff17f093e0487b27180be515c385feff6.pdf",
    "citation_key": "papp20211ac",
    "metadata": {
      "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks",
      "authors": [
        "P. Papp",
        "Karolis Martinkus",
        "Lukas Faber",
        "Roger Wattenhofer"
      ],
      "published_date": "2021",
      "abstract": "This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the final result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical findings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks.",
      "file_path": "paper_data/Graph_Neural_Networks/0d4184cff17f093e0487b27180be515c385feff6.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks\" \\cite{papp20211ac} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressiveness, often unable to distinguish even simple graph structures, a limitation tied to the Weisfeiler-Lehman (WL) test \\cite{papp20211ac}.\n    *   This limitation hinders GNNs from recognizing certain distinct graph neighborhoods, impacting their performance in applications where graph structure is crucial \\cite{papp20211ac}.\n    *   The problem is challenging because increasing expressiveness often leads to high computational costs (e.g., k-tuple GNNs) or poor generalization (e.g., adding unique IDs) \\cite{papp20211ac}.\n\n*   **Related Work & Positioning**\n    *   **Limitations of previous solutions**:\n        *   **Feature augmentation (e.g., port numbers, angles, unique IDs, random features)**: While some augmentations help, they often fail to distinguish simple graphs \\cite{papp20211ac}. Adding IDs or random features can lead to overfitting and poor generalization to unseen graphs \\cite{papp20211ac}.\n        *   **K-tuple GNNs**: Increase expressiveness by operating on tuples of nodes, but this leads to a quadratic blow-up in graph size and computational cost, destroying local semantics \\cite{papp20211ac}.\n        *   **Randomized smoothing**: Also uses multiple runs on perturbed data, but aims for robustness by smoothing out atypical variants. In contrast, DropGNN *identifies* these perturbed variants to distinguish graphs \\cite{papp20211ac}.\n        *   **Dropout regularization**: A common technique to prevent co-adaptation during training. DropGNN's dropout is fundamentally different, applied during both training and testing to observe diverse neighborhood patterns, not just for regularization \\cite{papp20211ac}.\n    *   **Positioning**: DropGNN offers a novel approach that increases expressiveness beyond the WL-test with only a small overhead (run repetition), while preserving local graph structure and generalizing well, unlike many prior methods \\cite{papp20211ac}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: DropGNNs execute multiple independent runs of a standard GNN on the input graph \\cite{papp20211ac}. In each run, nodes are randomly and independently \"dropped out\" (removed) with a small probability `p` \\cite{papp20211ac}.\n    *   **Mechanism**: Dropped nodes do not send or receive messages, effectively perturbing the `d`-hop neighborhood of other nodes \\cite{papp20211ac}. This allows the GNN to observe various slightly altered versions of a node's neighborhood \\cite{papp20211ac}.\n    *   **Run Aggregation**: After `r` runs, each node has `r` distinct final embeddings. These are merged into a single final embedding using a permutation-invariant run aggregation function (e.g., sum or max, often preceded by a non-linear transformation) before the final readout \\cite{papp20211ac}.\n    *   **Novelty**: The key innovation is using random node dropouts *during both training and testing* across multiple runs, not for regularization, but to systematically explore perturbed graph structures. This allows the GNN to learn to distinguish subtle differences in neighborhood patterns that are otherwise indistinguishable by standard message-passing GNNs \\cite{papp20211ac}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of Dropout Graph Neural Networks (DropGNNs) as a new paradigm for increasing GNN expressiveness \\cite{papp20211ac}.\n    *   **Theoretical Proofs of Expressiveness**: Proves that DropGNNs can distinguish various graph neighborhoods (e.g., cycles of different lengths, specific structural patterns) that are beyond the capabilities of message-passing GNNs and even some extended GNN variants \\cite{papp20211ac}.\n    *   **Theoretical Bounds for Runs**: Derives theoretical bounds for the number of runs (`r`) required to ensure a reliable distribution of dropouts, particularly focusing on observing \"1-complete dropouts\" (where each single node dropout is observed multiple times) \\cite{papp20211ac}.\n    *   **Enhanced Aggregation**: Demonstrates that dropouts can increase the expressive power of GNNs even with simpler aggregation methods like `mean` aggregation, which is typically less expressive than `sum` \\cite{papp20211ac}.\n\n*   **Experimental Validation**\n    *   **Expressiveness Validation**: Experiments were conducted on established problems known to be impossible for standard GNNs (e.g., distinguishing specific non-isomorphic graphs that fool the WL-test) \\cite{papp20211ac}.\n    *   **Key Results on Expressiveness**: DropGNNs clearly outperformed competing methods on these \"impossible\" datasets, validating the theoretical findings on increased expressiveness \\cite{papp20211ac}.\n    *   **Benchmark Performance**: DropGNNs achieved competitive performance on several established GNN benchmarks \\cite{papp20211ac}.\n    *   **Specific Strengths**: Showed particularly impressive results in applications where the graph structure is a crucial factor \\cite{papp20211ac}.\n\n*   **Limitations & Scope**\n    *   **Computational Cost**: While more efficient than k-tuple GNNs, the need for `r` runs introduces an overhead. Observing *all* possible dropout combinations is exponentially complex and not viable in practice \\cite{papp20211ac}.\n    *   **Focus on 1-complete dropouts**: The theoretical analysis and practical application often focus on ensuring sufficient observation of 1-dropouts (single node removals), as observing higher-order dropouts becomes increasingly rare and computationally intensive \\cite{papp20211ac}.\n    *   **Hyperparameter `\\Gamma`**: The choice of the neighborhood size `\\Gamma` (the region of interest for dropout distribution) is a trade-off hyperparameter, affecting the required number of runs and the observed variations \\cite{papp20211ac}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: DropGNN significantly advances the technical state-of-the-art by providing a principled and effective method to overcome the expressiveness limitations of standard GNNs, particularly those tied to the WL-test \\cite{papp20211ac}.\n    *   **Novel Paradigm**: Introduces a novel paradigm of using random perturbations during inference (and training) across multiple runs to enhance structural awareness, distinct from regularization or robustness techniques \\cite{papp20211ac}.\n    *   **Potential Impact**: Opens new avenues for designing more powerful GNN architectures capable of distinguishing complex graph structures with relatively low overhead, potentially impacting fields like quantum chemistry, physics, and social networks where structural understanding is paramount \\cite{papp20211ac}.",
      "keywords": [
        "DropGNN",
        "Graph Neural Networks (GNNs)",
        "expressiveness limitations",
        "Weisfeiler-Lehman (WL) test",
        "random node dropouts",
        "multiple independent runs",
        "run aggregation",
        "theoretical proofs of expressiveness",
        "distinguishing graph structures",
        "novel GNN paradigm",
        "computational overhead",
        "structural awareness",
        "perturbed graph structures"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **new method/system (technical aspect):** the paper introduces \"dropout graph neural networks (dropgnns), a new approach.\" it describes how dropgnns work (\"execute multiple runs,\" \"randomly and independently dropped nodes,\" \"combine results\"). this clearly aligns with the \"technical\" classification.\n2.  **mathematical analysis/proofs (theoretical aspect):** the abstract states: \"we prove that dropgnns can distinguish various graph neighborhoods,\" \"we derive theoretical bounds,\" and \"we prove several properties regarding the expressive capabilities and limits of dropgnns.\" this is a strong indicator of a \"theoretical\" paper.\n3.  **data-driven studies/experiments (empirical aspect):** the abstract mentions: \"we experimentally validate our theoretical ﬁndings on expressiveness. furthermore, we show that dropgnns perform competitively on established gnn benchmarks.\" this indicates an \"empirical\" component.\n\nthe paper clearly contains elements of technical, theoretical, and empirical work. however, the classification criteria ask for **one** type.\n\nlet's weigh the emphasis:\nthe abstract heavily emphasizes the \"prove,\" \"derive theoretical bounds,\" and \"prove several properties regarding the expressive capabilities and limits\" of dropgnns. the experimental validation is explicitly stated to validate \"our **theoretical ﬁndings** on expressiveness.\" this suggests that the core contribution and primary focus of the paper is the formal analysis and understanding of the new method's capabilities and limitations. while a new method is proposed (technical), and experiments are conducted (empirical), these seem to serve the overarching goal of providing a rigorous theoretical foundation for dropgnns' expressiveness.\n\ntherefore, the most fitting primary classification is **theoretical**.\n\nthe final answer is $\\boxed{theoretical}$"
    },
    "file_name": "0d4184cff17f093e0487b27180be515c385feff6.pdf"
  },
  {
    "success": true,
    "doc_id": "f6c589a90aa69e3b9e3fa81624eb6558",
    "summary": "Here's a focused summary of the technical paper \"Sequential Recommendation with Graph Neural Networks\" \\cite{chang2021yyt} for a literature review:\n\n### Focused Summary for Literature Review: Sequential Recommendation with Graph Neural Networks \\cite{chang2021yyt}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing sequential recommendation methods struggle to effectively leverage users' long historical behavior sequences due to two main challenges:\n        *   **Implicit and Noisy Preference Signals:** User behaviors (e.g., clicks, watches) are often implicit feedback, containing significant noise that obscures actual user preferences \\cite{chang2021yyt}.\n        *   **Dynamic and Drifting Preferences:** User preferences change rapidly over time, making it difficult to capture evolving patterns and identify currently activated interests from historical sequences \\cite{chang2021yyt}.\n    *   **Importance and Challenge:** Sequential recommendation is crucial for modern online systems. Accurately modeling dynamic preferences from noisy, long sequences is challenging because older behaviors might still contain relevant \"core interests\" that are hard to distinguish from irrelevant \"peripheral interests\" or noise, and the temporal evolution of these interests needs to be captured \\cite{chang2021yyt}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Early efforts:** Used human-designed rules or attention mechanisms with time-decaying weights \\cite{chang2021yyt}.\n        *   **RNN-based methods:** Leveraged recurrent neural networks to summarize sequences \\cite{chang2021yyt}.\n        *   **Joint long/short-term interest models:** Attempted to model both types of interests to avoid forgetting long-term preferences \\cite{chang2021yyt}.\n    *   **Limitations of Previous Solutions:**\n        *   **Focus on recent behaviors:** Most prior works concentrate on recent user behaviors, failing to fully mine older, potentially valuable, behavior sequences \\cite{chang2021yyt}.\n        *   **Short-term bottleneck:** RNNs suffer from difficulty in modeling long-range dependencies, limiting their ability to capture dynamic interests over long sequences \\cite{chang2021yyt}.\n        *   **Challenging division/integration:** Joint models struggle with the effective division and integration of long-term and short-term interests \\cite{chang2022yyt}.\n        *   **Inability to handle noise:** Existing methods do not sufficiently address the implicit and noisy nature of user preference signals in long sequences \\cite{chang2021yyt}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (SURGE):** SURGE (SeqUential Recommendation with Graph neural nEtworks) is a graph neural network model that transforms loose item sequences into tight item-item interest graphs to model user preferences \\cite{chang2021yyt}. It consists of four main parts:\n        *   **Interest Graph Construction:** Converts a user's interaction sequence into an undirected item-item graph. This is achieved using **metric learning** (specifically, multi-head weighted cosine similarity) to learn node similarities, and **$\\epsilon$-sparseness** (a relative ranking strategy) to create a sparse, non-negative adjacency matrix, ensuring graph connectivity and controlling sparsity without relying on absolute thresholds \\cite{chang2021yyt}.\n        *   **Interest-fusion Graph Convolutional Layer:** Employs a **cluster- and query-aware graph attentive convolutional layer** to aggregate information. It computes attention scores based on:\n            *   **Cluster-aware attention:** Identifies core interests by considering the target node as a cluster medoid and its k-hop neighborhood as the receptive field \\cite{chang2021yyt}.\n            *   **Query-aware attention:** Considers the correlation between source nodes and the current target item (query) to prioritize relevant information \\cite{chang2021yyt}.\n        *   **Interest-extraction Graph Pooling Layer:** Uses a dynamic graph pooling method (similar to downsampling in CNNs) to adaptively reserve \"activated core preferences.\" It generates a soft cluster assignment matrix to pool node information into cluster information, effectively coarsening the graph and extracting salient interests \\cite{chang2021yyt}.\n        *   **Prediction Layer:** Flattens the pooled graphs into reduced sequences to model the evolution of enhanced interest signals and predict the next item \\cite{chang2021yyt}.\n    *   **Novelty/Differentiation:**\n        *   **Graph-based representation of sequences:** Re-constructs loose sequences into item-item interest graphs, allowing for explicit modeling of relationships between items beyond simple sequential order \\cite{chang2021yyt}.\n        *   **Metric learning for dynamic graph construction:** Learns graph structures adaptively based on item similarities, which is robust to inductive learning (new items) and avoids issues of sparse co-occurrence \\cite{chang2021yyt}.\n        *   **Cluster- and query-aware attention:** Dynamically fuses interests by strengthening important signals (core interests, query-relevant items) and weakening noise, a more sophisticated attention mechanism than prior GCNs \\cite{chang2021yyt}.\n        *   **Dynamic graph pooling for preference extraction:** Adaptively extracts activated core preferences, addressing the challenge of rapidly changing user interests by filtering out deactivated or noisy signals \\cite{chang2021yyt}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel graph construction method based on metric learning and $\\epsilon$-sparseness for converting user interaction sequences into dynamic item-item interest graphs \\cite{chang2021yyt}.\n        *   A cluster- and query-aware graph attentive convolutional layer for robustly fusing implicit preference signals into explicit ones \\cite{chang2021yyt}.\n        *   A dynamic graph pooling technique for adaptively extracting and reserving currently activated core preferences from the fused interest graph \\cite{chang2021yyt}.\n    *   **System Design/Architectural Innovations:** The SURGE model integrates these components into an end-to-end GNN-based architecture specifically designed for sequential recommendation, effectively addressing both noisy signals and dynamic preferences \\cite{chang2021yyt}.\n    *   **Theoretical Insights/Analysis:** The paper provides a framework for understanding how graph structures can represent and process complex, noisy sequential user behaviors, and how attention and pooling mechanisms can dynamically adapt to evolving preferences \\cite{chang2021yyt}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on both public and proprietary industrial datasets \\cite{chang2021yyt}. Further studies were conducted on the impact of sequence length \\cite{chang2021yyt}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Demonstrated **significant performance gains** compared to state-of-the-art sequential recommendation methods \\cite{chang2021yyt}.\n        *   Confirmed that SURGE can **model long behavioral sequences effectively and efficiently**, outperforming baselines in handling longer histories \\cite{chang2021yyt}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The choice of aggregation function (e.g., Mean, Sum, Max, GRU) in the interest-fusion layer is simplified to \"sum\" with other functions left for future exploration, suggesting potential for further optimization \\cite{chang2021yyt}.\n        *   The number of clusters `m` in the graph pooling layer is a pre-defined model hyperparameter, which might require careful tuning \\cite{chang2021yyt}.\n        *   The complexity of GNNs, especially with dynamic graph construction and multi-head attention, could be a computational consideration for extremely large-scale graphs, though the $\\epsilon$-sparseness helps manage this \\cite{chang2021yyt}.\n    *   **Scope of Applicability:** Primarily focused on sequential recommendation tasks where user interactions are implicit and historical sequences can be long and noisy. Applicable to various online information systems like news, video, and advertisements \\cite{chang2021yyt}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** SURGE significantly advances the technical state-of-the-art in sequential recommendation by providing a robust GNN-based solution that simultaneously tackles the challenges of implicit/noisy preference signals and dynamic user preferences, which were not well-addressed by previous methods \\cite{chang2021yyt}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new avenues for applying dynamic graph learning and advanced GNN architectures to model complex user behaviors in recommendation systems.\n        *   Encourages further research into adaptive graph construction, sophisticated attention mechanisms (cluster- and query-aware), and dynamic graph pooling for extracting evolving user interests.\n        *   Provides a strong baseline and methodology for handling long, noisy, and implicit feedback sequences, which are prevalent in real-world applications \\cite{chang2021yyt}.",
    "intriguing_abstract": "Unlocking the true potential of long, noisy user behavior sequences for sequential recommendation remains a formidable challenge, as existing methods struggle with implicit feedback and rapidly drifting preferences. We introduce SURGE (SeqUential Recommendation with Graph neural nEtworks), a novel GNN-based framework that redefines how dynamic user preferences are modeled by transforming loose interaction sequences into adaptive **item-item interest graphs**.\n\nSURGE's innovation begins with a robust graph construction method leveraging **metric learning** and **$\\epsilon$-sparseness**, enabling adaptive representation of item relationships. At its core, a **cluster- and query-aware graph attentive convolutional layer** intelligently fuses implicit preference signals, strengthening core interests and weakening noise. This is complemented by a **dynamic graph pooling** mechanism that adaptively extracts currently activated core preferences, effectively filtering out deactivated or noisy signals to capture evolving tastes.\n\nExtensive experiments on diverse datasets demonstrate SURGE's superior performance, significantly outperforming state-of-the-art **sequential recommendation** methods, particularly in effectively leveraging long behavioral histories. This work not only advances the technical frontier of **Graph Neural Networks** for complex user behavior modeling but also offers a powerful paradigm for handling noisy, implicit feedback and dynamic preferences in real-world recommendation systems.",
    "keywords": [
      "Sequential recommendation",
      "Graph Neural Networks",
      "SURGE model",
      "Dynamic user preferences",
      "Implicit preference signals",
      "Item-item interest graphs",
      "Metric learning",
      "Dynamic graph construction",
      "Cluster- and query-aware attention",
      "Dynamic graph pooling",
      "Significant performance gains",
      "Long behavioral sequence modeling"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf",
    "citation_key": "chang2021yyt",
    "metadata": {
      "title": "Sequential Recommendation with Graph Neural Networks",
      "authors": [
        "Jianxin Chang",
        "Chen Gao",
        "Y. Zheng",
        "Yiqun Hui",
        "Yanan Niu",
        "Yang Song",
        "Depeng Jin",
        "Yong Li"
      ],
      "published_date": "2021",
      "abstract": "Sequential recommendation aims to leverage users' historical behaviors to predict their next interaction. Existing works have not yet addressed two main challenges in sequential recommendation. First, user behaviors in their rich historical sequences are often implicit and noisy preference signals, they cannot sufficiently reflect users' actual preferences. In addition, users' dynamic preferences often change rapidly over time, and hence it is difficult to capture user patterns in their historical sequences. In this work, we propose a graph neural network model called SURGE (short forSeqUential Recommendation with Graph neural nEtworks) to address these two issues. Specifically, SURGE integrates different types of preferences in long-term user behaviors into clusters in the graph by re-constructing loose item sequences into tight item-item interest graphs based on metric learning. This helps explicitly distinguish users' core interests, by forming dense clusters in the interest graph. Then, we perform cluster-aware and query-aware graph convolutional propagation and graph pooling on the constructed graph. It dynamically fuses and extracts users' current activated core interests from noisy user behavior sequences. We conduct extensive experiments on both public and proprietary industrial datasets. Experimental results demonstrate significant performance gains of our proposed method compared to state-of-the-art methods. Further studies on sequence length confirm that our method can model long behavioral sequences effectively and efficiently.",
      "file_path": "paper_data/Graph_Neural_Networks/fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \"Sequential Recommendation with Graph Neural Networks\" \\cite{chang2021yyt} for a literature review:\n\n### Focused Summary for Literature Review: Sequential Recommendation with Graph Neural Networks \\cite{chang2021yyt}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing sequential recommendation methods struggle to effectively leverage users' long historical behavior sequences due to two main challenges:\n        *   **Implicit and Noisy Preference Signals:** User behaviors (e.g., clicks, watches) are often implicit feedback, containing significant noise that obscures actual user preferences \\cite{chang2021yyt}.\n        *   **Dynamic and Drifting Preferences:** User preferences change rapidly over time, making it difficult to capture evolving patterns and identify currently activated interests from historical sequences \\cite{chang2021yyt}.\n    *   **Importance and Challenge:** Sequential recommendation is crucial for modern online systems. Accurately modeling dynamic preferences from noisy, long sequences is challenging because older behaviors might still contain relevant \"core interests\" that are hard to distinguish from irrelevant \"peripheral interests\" or noise, and the temporal evolution of these interests needs to be captured \\cite{chang2021yyt}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Early efforts:** Used human-designed rules or attention mechanisms with time-decaying weights \\cite{chang2021yyt}.\n        *   **RNN-based methods:** Leveraged recurrent neural networks to summarize sequences \\cite{chang2021yyt}.\n        *   **Joint long/short-term interest models:** Attempted to model both types of interests to avoid forgetting long-term preferences \\cite{chang2021yyt}.\n    *   **Limitations of Previous Solutions:**\n        *   **Focus on recent behaviors:** Most prior works concentrate on recent user behaviors, failing to fully mine older, potentially valuable, behavior sequences \\cite{chang2021yyt}.\n        *   **Short-term bottleneck:** RNNs suffer from difficulty in modeling long-range dependencies, limiting their ability to capture dynamic interests over long sequences \\cite{chang2021yyt}.\n        *   **Challenging division/integration:** Joint models struggle with the effective division and integration of long-term and short-term interests \\cite{chang2022yyt}.\n        *   **Inability to handle noise:** Existing methods do not sufficiently address the implicit and noisy nature of user preference signals in long sequences \\cite{chang2021yyt}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (SURGE):** SURGE (SeqUential Recommendation with Graph neural nEtworks) is a graph neural network model that transforms loose item sequences into tight item-item interest graphs to model user preferences \\cite{chang2021yyt}. It consists of four main parts:\n        *   **Interest Graph Construction:** Converts a user's interaction sequence into an undirected item-item graph. This is achieved using **metric learning** (specifically, multi-head weighted cosine similarity) to learn node similarities, and **$\\epsilon$-sparseness** (a relative ranking strategy) to create a sparse, non-negative adjacency matrix, ensuring graph connectivity and controlling sparsity without relying on absolute thresholds \\cite{chang2021yyt}.\n        *   **Interest-fusion Graph Convolutional Layer:** Employs a **cluster- and query-aware graph attentive convolutional layer** to aggregate information. It computes attention scores based on:\n            *   **Cluster-aware attention:** Identifies core interests by considering the target node as a cluster medoid and its k-hop neighborhood as the receptive field \\cite{chang2021yyt}.\n            *   **Query-aware attention:** Considers the correlation between source nodes and the current target item (query) to prioritize relevant information \\cite{chang2021yyt}.\n        *   **Interest-extraction Graph Pooling Layer:** Uses a dynamic graph pooling method (similar to downsampling in CNNs) to adaptively reserve \"activated core preferences.\" It generates a soft cluster assignment matrix to pool node information into cluster information, effectively coarsening the graph and extracting salient interests \\cite{chang2021yyt}.\n        *   **Prediction Layer:** Flattens the pooled graphs into reduced sequences to model the evolution of enhanced interest signals and predict the next item \\cite{chang2021yyt}.\n    *   **Novelty/Differentiation:**\n        *   **Graph-based representation of sequences:** Re-constructs loose sequences into item-item interest graphs, allowing for explicit modeling of relationships between items beyond simple sequential order \\cite{chang2021yyt}.\n        *   **Metric learning for dynamic graph construction:** Learns graph structures adaptively based on item similarities, which is robust to inductive learning (new items) and avoids issues of sparse co-occurrence \\cite{chang2021yyt}.\n        *   **Cluster- and query-aware attention:** Dynamically fuses interests by strengthening important signals (core interests, query-relevant items) and weakening noise, a more sophisticated attention mechanism than prior GCNs \\cite{chang2021yyt}.\n        *   **Dynamic graph pooling for preference extraction:** Adaptively extracts activated core preferences, addressing the challenge of rapidly changing user interests by filtering out deactivated or noisy signals \\cite{chang2021yyt}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel graph construction method based on metric learning and $\\epsilon$-sparseness for converting user interaction sequences into dynamic item-item interest graphs \\cite{chang2021yyt}.\n        *   A cluster- and query-aware graph attentive convolutional layer for robustly fusing implicit preference signals into explicit ones \\cite{chang2021yyt}.\n        *   A dynamic graph pooling technique for adaptively extracting and reserving currently activated core preferences from the fused interest graph \\cite{chang2021yyt}.\n    *   **System Design/Architectural Innovations:** The SURGE model integrates these components into an end-to-end GNN-based architecture specifically designed for sequential recommendation, effectively addressing both noisy signals and dynamic preferences \\cite{chang2021yyt}.\n    *   **Theoretical Insights/Analysis:** The paper provides a framework for understanding how graph structures can represent and process complex, noisy sequential user behaviors, and how attention and pooling mechanisms can dynamically adapt to evolving preferences \\cite{chang2021yyt}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on both public and proprietary industrial datasets \\cite{chang2021yyt}. Further studies were conducted on the impact of sequence length \\cite{chang2021yyt}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Demonstrated **significant performance gains** compared to state-of-the-art sequential recommendation methods \\cite{chang2021yyt}.\n        *   Confirmed that SURGE can **model long behavioral sequences effectively and efficiently**, outperforming baselines in handling longer histories \\cite{chang2021yyt}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The choice of aggregation function (e.g., Mean, Sum, Max, GRU) in the interest-fusion layer is simplified to \"sum\" with other functions left for future exploration, suggesting potential for further optimization \\cite{chang2021yyt}.\n        *   The number of clusters `m` in the graph pooling layer is a pre-defined model hyperparameter, which might require careful tuning \\cite{chang2021yyt}.\n        *   The complexity of GNNs, especially with dynamic graph construction and multi-head attention, could be a computational consideration for extremely large-scale graphs, though the $\\epsilon$-sparseness helps manage this \\cite{chang2021yyt}.\n    *   **Scope of Applicability:** Primarily focused on sequential recommendation tasks where user interactions are implicit and historical sequences can be long and noisy. Applicable to various online information systems like news, video, and advertisements \\cite{chang2021yyt}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** SURGE significantly advances the technical state-of-the-art in sequential recommendation by providing a robust GNN-based solution that simultaneously tackles the challenges of implicit/noisy preference signals and dynamic user preferences, which were not well-addressed by previous methods \\cite{chang2021yyt}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new avenues for applying dynamic graph learning and advanced GNN architectures to model complex user behaviors in recommendation systems.\n        *   Encourages further research into adaptive graph construction, sophisticated attention mechanisms (cluster- and query-aware), and dynamic graph pooling for extracting evolving user interests.\n        *   Provides a strong baseline and methodology for handling long, noisy, and implicit feedback sequences, which are prevalent in real-world applications \\cite{chang2021yyt}.",
      "keywords": [
        "Sequential recommendation",
        "Graph Neural Networks",
        "SURGE model",
        "Dynamic user preferences",
        "Implicit preference signals",
        "Item-item interest graphs",
        "Metric learning",
        "Dynamic graph construction",
        "Cluster- and query-aware attention",
        "Dynamic graph pooling",
        "Significant performance gains",
        "Long behavioral sequence modeling"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose a graph neural network model called surge\"**: this is a direct indicator of presenting a new method or system.\n2.  **description of surge's mechanism**: \"integrates different types of preferences... by re-constructing loose item sequences into tight item-item interest graphs based on metric learning\", \"perform cluster-aware and query-aware graph convolutional propagation and graph pooling\". these are detailed descriptions of a new technical approach.\n3.  **addressing challenges**: the paper identifies \"two main challenges in sequential recommendation\" and proposes surge to \"address these two issues\", which aligns with presenting a proposed solution to a technical problem.\n4.  **empirical validation**: while the paper mentions \"conduct extensive experiments\" and \"experimental results demonstrate significant performance gains\", this is typically done to validate a *new technical proposal*. the primary contribution is the creation of the new model, not just a data-driven study of existing phenomena.\n\nthe core of this paper is the development and presentation of a novel graph neural network model (surge) to solve specific problems in sequential recommendation. the experiments serve to validate this new method.\n\ntherefore, the most appropriate classification is **technical**."
    },
    "file_name": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf"
  },
  {
    "success": true,
    "doc_id": "7f54b8c5a92f948e773e82a999ea7973",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to the citation requirements:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   The paper addresses the critical problem of **overly optimistic robustness estimates for Graph Neural Network (GNN) defenses** \\cite{mujkanovic20238fi}.\n    *   This problem arises because virtually all existing GNN defenses are evaluated against **non-adaptive attacks**, which are often weak or static, failing to account for an adversary's knowledge of the defense mechanism \\cite{mujkanovic20238fi}.\n    *   This issue is important because it leads to a misleading understanding of GNN security and reliability in adversarial settings, a lesson the vision community learned years ago but has been largely ignored in the graph domain \\cite{mujkanovic20238fi}.\n    *   The challenge lies in designing **strong adaptive attacks** for GNNs, which is complex due to the discrete and sparse nature of graphs and the neighborhood-dependent representation of nodes \\cite{mujkanovic20238fi}.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself by drawing a parallel to the vision community, where adaptive attacks are considered the \"gold standard\" for evaluating adversarial robustness, following seminal works by Carlini & Wagner \\cite{mujkanovic20238fi} and Athalye et al. \\cite{mujkanovic20238fi} that exposed the fragility of many defenses against stronger, adaptive adversaries.\n    *   It critiques the existing GNN defense literature, stating that \"virtually no existing work that proposes an allegedly robust Graph Neural Network (GNN) evaluates against adaptive attacks\" \\cite{mujkanovic20238fi}.\n    *   The limitation of previous GNN defense evaluations is their reliance on non-adaptive attacks (e.g., transfer attacks from undefended proxies or attacks lacking defense-specific knowledge), which only test against a narrow subset of perturbations and lead to inflated robustness claims \\cite{mujkanovic20238fi}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical approach involves performing a **thorough robustness analysis of 7 popular GNN defenses** (spanning graph preprocessing, architecture modifications, and training improvements) using **custom-designed adaptive attacks** \\cite{mujkanovic20238fi}.\n    *   The paper introduces a **systematic, 6-step methodology for designing strong adaptive attacks** for GNNs, which is a key innovation:\n        1.  Understand and categorize the defense mechanism.\n        2.  Probe for obvious weaknesses using transfer or gradient-free attacks.\n        3.  Launch gradient-based adaptive attacks (e.g., FGA, PGD, and a novel **Meta-PGD** for poisoning).\n        4.  Address gradient issues (non-differentiability, vanishing/exploding gradients, obfuscated gradients) by adjusting hyperparameters, replacing components, or removing filtering operations.\n        5.  Adjust the attack loss function (e.g., using consistent losses like Carlini-Wagner variants, logit margin (LM), or tanh logit margin (TLM)) for optimal attack strength.\n        6.  Tune attack hyperparameters (e.g., PGD steps, learning rate, optimizer) \\cite{mujkanovic20238fi}.\n    *   This methodology is novel in its systematic approach to circumventing diverse GNN defense strategies, including handling non-differentiable components and optimizing attack objectives for maximum effectiveness \\cite{mujkanovic20238fi}.\n\n4.  **Key Technical Contributions**\n    *   **Systematic Methodology for Adaptive Attack Design**: A transparent, step-by-step guide and lessons learned for constructing strong adaptive attacks against GNN defenses, addressing common challenges like gradient stability and loss function selection \\cite{mujkanovic20238fi}.\n    *   **Novel Meta-PGD Attack**: Introduction of a stronger poisoning attack that combines Projected Gradient Descent (PGD) with meta-gradients, demonstrating superior performance over greedy meta-gradient attacks like Metattack \\cite{mujkanovic20238fi}.\n    *   **Comprehensive Robustness Analysis**: A rigorous empirical evaluation of 7 diverse and highly-cited GNN defenses, revealing that most offer \"no or only marginal improvement\" against adaptive adversaries compared to an undefended baseline \\cite{mujkanovic20238fi}.\n    *   **Collection of Perturbed Graphs**: The generated collection of perturbed adjacency matrices serves as a \"black-box unit test\" for GNN robustness, providing an initial assessment of a model's resilience \\cite{mujkanovic20238fi}.\n    *   **Extended Defense Taxonomy**: An extended categorization of GNN defenses, aiding in the systematic selection and analysis of different defense strategies \\cite{mujkanovic20238fi}.\n\n5.  **Experimental Validation**\n    *   **Defenses Evaluated**: The study rigorously evaluated 7 popular GNN defenses: Jaccard-GCN, SVD-GCN, ProGNN, GRAND, GNNGuard, RGCN, and Soft-Median-GDC \\cite{mujkanovic20238fi}.\n    *   **Experimental Settings**: Experiments were conducted across four critical adversarial settings: global poisoning, global evasion, local poisoning, and local evasion \\cite{mujkanovic20238fi}.\n    *   **Performance Metrics**: Adversarial accuracy (percentage of correct predictions) was the primary metric, comparing defense performance under adaptive attacks against non-adaptive attacks and an undefended GCN baseline \\cite{mujkanovic20238fi}.\n    *   **Key Results**:\n        *   Adaptive attacks consistently and significantly reduced adversarial accuracy compared to non-adaptive attacks across all settings \\cite{mujkanovic202388fi}.\n        *   \"Most defenses show no or only marginal improvement compared to an undefended baseline\" \\cite{mujkanovic20238fi}.\n        *   Many defenses, including some highly cited ones, performed *worse* than a vanilla GCN under adaptive attacks \\cite{mujkanovic20238fi}.\n        *   SVD-GCN was \"catastrophically broken\" by adaptive attacks, achieving adversarial accuracies as low as 9-24% \\cite{mujkanovic20238fi}.\n        *   The non-adaptive attacks used in the study were already stronger than those typically employed in prior defense evaluations \\cite{mujkanovic20238fi}.\n\n6.  **Limitations & Scope**\n    *   **Attack Scope**: The study primarily focuses on **structure perturbations** (edge flips) and does not extensively cover feature perturbations, although it briefly discusses adaptive feature attacks \\cite{mujkanovic20238fi}.\n    *   **White-Box Assumption**: The adaptive attacks are designed under a **white-box threat model**, assuming perfect knowledge of the model, parameters, and defensive measures to achieve the strongest possible attack (tightest upper bound on robustness) \\cite{mujkanovic20238fi}. This is a strong assumption for real-world scenarios but essential for worst-case robustness evaluation.\n    *   **Budget Constraint**: Attacks are constrained by an L0-ball budget, limiting the number of perturbed edges \\cite{mujkanovic20238fi}.\n    *   **Defense Exclusions**: Certain categories of defenses, such as \"robust training\" and \"miscellaneous\" (not explicitly designed for defense), were excluded from the study due to specific challenges or scope \\cite{mujkanovic20238fi}.\n    *   **Scalability**: The paper acknowledges that applying L0-PGD can have \"limited scalability\" \\cite{mujkanovic20238fi}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the technical state-of-the-art by rigorously demonstrating the inadequacy of current GNN defense evaluation practices and providing a concrete, systematic framework for designing stronger, adaptive attacks \\cite{mujkanovic20238fi}. It corrects the prevailing optimistic view of GNN robustness.\n    *   **Potential Impact on Future Research**:\n        *   **Paradigm Shift in Evaluation**: It advocates for and provides the tools to enforce adaptive attacks as the new \"gold standard\" for evaluating GNN defenses, pushing future research towards more realistic and robust solutions \\cite{mujkanovic20238fi}.\n        *   **Informed Defense Design**: The insights gained from breaking existing defenses will guide the development of truly robust GNN architectures and training strategies that can withstand sophisticated adversaries \\cite{mujkanovic20238fi}.\n        *   **Benchmarking and Unit Testing**: The provided collection of perturbed graphs offers a valuable resource for initial robustness testing, enabling researchers to quickly assess new models against known strong attacks \\cite{mujkanovic20238fi}.\n        *   **New Research Directions**: The findings highlight the urgent need for novel GNN defense mechanisms that are inherently robust to adaptive adversaries, potentially spurring research into certified robustness or fundamentally different graph learning paradigms \\cite{mujkanovic20238fi}.",
    "intriguing_abstract": "The perceived robustness of Graph Neural Networks (GNNs) against adversarial attacks is dangerously optimistic. Drawing a critical lesson from the vision community, we expose a fundamental flaw in current GNN defense evaluation: an over-reliance on weak, non-adaptive attacks that fail to account for sophisticated adversaries.\n\nThis paper introduces a systematic, 6-step methodology for designing potent adaptive attacks, specifically tailored to circumvent diverse GNN defense mechanisms. We also present **Meta-PGD**, a novel poisoning attack that significantly outperforms existing methods, alongside powerful **evasion attacks**. Applying these advanced **white-box adversarial attacks** to 7 popular GNN defenses (spanning preprocessing, architecture, and training), our comprehensive analysis reveals a sobering truth: most defenses offer no or only marginal improvement over an undefended baseline. Alarmingly, some highly-cited defenses are catastrophically broken, performing *worse* under adaptive adversaries. Our work necessitates a paradigm shift, establishing adaptive attacks as the new gold standard for **GNN adversarial robustness** evaluation. This research provides critical insights and tools to guide the development of truly resilient GNNs, ensuring their reliability in real-world, adversarial settings.",
    "keywords": [
      "Graph Neural Network (GNN) defenses",
      "adversarial robustness",
      "adaptive attacks",
      "non-adaptive attacks",
      "systematic methodology for adaptive attack design",
      "Meta-PGD attack",
      "poisoning and evasion attacks",
      "white-box threat model",
      "overly optimistic robustness estimates",
      "ineffectiveness of GNN defenses",
      "comprehensive robustness analysis",
      "structure perturbations",
      "paradigm shift in GNN evaluation"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf",
    "citation_key": "mujkanovic20238fi",
    "metadata": {
      "title": "Are Defenses for Graph Neural Networks Robust?",
      "authors": [
        "Felix Mujkanovic",
        "Simon Geisler",
        "Stephan Gunnemann",
        "Aleksandar Bojchevski"
      ],
      "published_date": "2023",
      "abstract": "A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw - virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering - most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.",
      "file_path": "paper_data/Graph_Neural_Networks/3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the citation requirements:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   The paper addresses the critical problem of **overly optimistic robustness estimates for Graph Neural Network (GNN) defenses** \\cite{mujkanovic20238fi}.\n    *   This problem arises because virtually all existing GNN defenses are evaluated against **non-adaptive attacks**, which are often weak or static, failing to account for an adversary's knowledge of the defense mechanism \\cite{mujkanovic20238fi}.\n    *   This issue is important because it leads to a misleading understanding of GNN security and reliability in adversarial settings, a lesson the vision community learned years ago but has been largely ignored in the graph domain \\cite{mujkanovic20238fi}.\n    *   The challenge lies in designing **strong adaptive attacks** for GNNs, which is complex due to the discrete and sparse nature of graphs and the neighborhood-dependent representation of nodes \\cite{mujkanovic20238fi}.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself by drawing a parallel to the vision community, where adaptive attacks are considered the \"gold standard\" for evaluating adversarial robustness, following seminal works by Carlini & Wagner \\cite{mujkanovic20238fi} and Athalye et al. \\cite{mujkanovic20238fi} that exposed the fragility of many defenses against stronger, adaptive adversaries.\n    *   It critiques the existing GNN defense literature, stating that \"virtually no existing work that proposes an allegedly robust Graph Neural Network (GNN) evaluates against adaptive attacks\" \\cite{mujkanovic20238fi}.\n    *   The limitation of previous GNN defense evaluations is their reliance on non-adaptive attacks (e.g., transfer attacks from undefended proxies or attacks lacking defense-specific knowledge), which only test against a narrow subset of perturbations and lead to inflated robustness claims \\cite{mujkanovic20238fi}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical approach involves performing a **thorough robustness analysis of 7 popular GNN defenses** (spanning graph preprocessing, architecture modifications, and training improvements) using **custom-designed adaptive attacks** \\cite{mujkanovic20238fi}.\n    *   The paper introduces a **systematic, 6-step methodology for designing strong adaptive attacks** for GNNs, which is a key innovation:\n        1.  Understand and categorize the defense mechanism.\n        2.  Probe for obvious weaknesses using transfer or gradient-free attacks.\n        3.  Launch gradient-based adaptive attacks (e.g., FGA, PGD, and a novel **Meta-PGD** for poisoning).\n        4.  Address gradient issues (non-differentiability, vanishing/exploding gradients, obfuscated gradients) by adjusting hyperparameters, replacing components, or removing filtering operations.\n        5.  Adjust the attack loss function (e.g., using consistent losses like Carlini-Wagner variants, logit margin (LM), or tanh logit margin (TLM)) for optimal attack strength.\n        6.  Tune attack hyperparameters (e.g., PGD steps, learning rate, optimizer) \\cite{mujkanovic20238fi}.\n    *   This methodology is novel in its systematic approach to circumventing diverse GNN defense strategies, including handling non-differentiable components and optimizing attack objectives for maximum effectiveness \\cite{mujkanovic20238fi}.\n\n4.  **Key Technical Contributions**\n    *   **Systematic Methodology for Adaptive Attack Design**: A transparent, step-by-step guide and lessons learned for constructing strong adaptive attacks against GNN defenses, addressing common challenges like gradient stability and loss function selection \\cite{mujkanovic20238fi}.\n    *   **Novel Meta-PGD Attack**: Introduction of a stronger poisoning attack that combines Projected Gradient Descent (PGD) with meta-gradients, demonstrating superior performance over greedy meta-gradient attacks like Metattack \\cite{mujkanovic20238fi}.\n    *   **Comprehensive Robustness Analysis**: A rigorous empirical evaluation of 7 diverse and highly-cited GNN defenses, revealing that most offer \"no or only marginal improvement\" against adaptive adversaries compared to an undefended baseline \\cite{mujkanovic20238fi}.\n    *   **Collection of Perturbed Graphs**: The generated collection of perturbed adjacency matrices serves as a \"black-box unit test\" for GNN robustness, providing an initial assessment of a model's resilience \\cite{mujkanovic20238fi}.\n    *   **Extended Defense Taxonomy**: An extended categorization of GNN defenses, aiding in the systematic selection and analysis of different defense strategies \\cite{mujkanovic20238fi}.\n\n5.  **Experimental Validation**\n    *   **Defenses Evaluated**: The study rigorously evaluated 7 popular GNN defenses: Jaccard-GCN, SVD-GCN, ProGNN, GRAND, GNNGuard, RGCN, and Soft-Median-GDC \\cite{mujkanovic20238fi}.\n    *   **Experimental Settings**: Experiments were conducted across four critical adversarial settings: global poisoning, global evasion, local poisoning, and local evasion \\cite{mujkanovic20238fi}.\n    *   **Performance Metrics**: Adversarial accuracy (percentage of correct predictions) was the primary metric, comparing defense performance under adaptive attacks against non-adaptive attacks and an undefended GCN baseline \\cite{mujkanovic20238fi}.\n    *   **Key Results**:\n        *   Adaptive attacks consistently and significantly reduced adversarial accuracy compared to non-adaptive attacks across all settings \\cite{mujkanovic202388fi}.\n        *   \"Most defenses show no or only marginal improvement compared to an undefended baseline\" \\cite{mujkanovic20238fi}.\n        *   Many defenses, including some highly cited ones, performed *worse* than a vanilla GCN under adaptive attacks \\cite{mujkanovic20238fi}.\n        *   SVD-GCN was \"catastrophically broken\" by adaptive attacks, achieving adversarial accuracies as low as 9-24% \\cite{mujkanovic20238fi}.\n        *   The non-adaptive attacks used in the study were already stronger than those typically employed in prior defense evaluations \\cite{mujkanovic20238fi}.\n\n6.  **Limitations & Scope**\n    *   **Attack Scope**: The study primarily focuses on **structure perturbations** (edge flips) and does not extensively cover feature perturbations, although it briefly discusses adaptive feature attacks \\cite{mujkanovic20238fi}.\n    *   **White-Box Assumption**: The adaptive attacks are designed under a **white-box threat model**, assuming perfect knowledge of the model, parameters, and defensive measures to achieve the strongest possible attack (tightest upper bound on robustness) \\cite{mujkanovic20238fi}. This is a strong assumption for real-world scenarios but essential for worst-case robustness evaluation.\n    *   **Budget Constraint**: Attacks are constrained by an L0-ball budget, limiting the number of perturbed edges \\cite{mujkanovic20238fi}.\n    *   **Defense Exclusions**: Certain categories of defenses, such as \"robust training\" and \"miscellaneous\" (not explicitly designed for defense), were excluded from the study due to specific challenges or scope \\cite{mujkanovic20238fi}.\n    *   **Scalability**: The paper acknowledges that applying L0-PGD can have \"limited scalability\" \\cite{mujkanovic20238fi}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the technical state-of-the-art by rigorously demonstrating the inadequacy of current GNN defense evaluation practices and providing a concrete, systematic framework for designing stronger, adaptive attacks \\cite{mujkanovic20238fi}. It corrects the prevailing optimistic view of GNN robustness.\n    *   **Potential Impact on Future Research**:\n        *   **Paradigm Shift in Evaluation**: It advocates for and provides the tools to enforce adaptive attacks as the new \"gold standard\" for evaluating GNN defenses, pushing future research towards more realistic and robust solutions \\cite{mujkanovic20238fi}.\n        *   **Informed Defense Design**: The insights gained from breaking existing defenses will guide the development of truly robust GNN architectures and training strategies that can withstand sophisticated adversaries \\cite{mujkanovic20238fi}.\n        *   **Benchmarking and Unit Testing**: The provided collection of perturbed graphs offers a valuable resource for initial robustness testing, enabling researchers to quickly assess new models against known strong attacks \\cite{mujkanovic20238fi}.\n        *   **New Research Directions**: The findings highlight the urgent need for novel GNN defense mechanisms that are inherently robust to adaptive adversaries, potentially spurring research into certified robustness or fundamentally different graph learning paradigms \\cite{mujkanovic20238fi}.",
      "keywords": [
        "Graph Neural Network (GNN) defenses",
        "adversarial robustness",
        "adaptive attacks",
        "non-adaptive attacks",
        "systematic methodology for adaptive attack design",
        "Meta-PGD attack",
        "poisoning and evasion attacks",
        "white-box threat model",
        "overly optimistic robustness estimates",
        "ineffectiveness of GNN defenses",
        "comprehensive robustness analysis",
        "structure perturbations",
        "paradigm shift in GNN evaluation"
      ],
      "paper_type": "**empirical**\n\n**reasoning:**\n\nthe abstract and introduction clearly indicate that the paper performs a data-driven study to evaluate the robustness of existing graph neural network (gnn) defenses.\n\n*   **abstract mentions:** \"we perform a thorough robustness analysis of 7 of the most popular defenses...\", \"the results are sobering – most defenses show no or only marginal improvement...\", \"we advocate using custom adaptive attacks as a gold standard...\"\n*   **introduction discusses:** the need for \"specific carefully crafted attacks to properly evaluate the adversarial robustness of a defense,\" drawing parallels to lessons learned in the vision community. it then presents figure 1, which shows \"adversarial accuracy (%)\" for various defenses under adaptive vs. non-adaptive attacks on a specific dataset (cora ml), explicitly stating \"adaptive attacks draw a different picture of robustness. all defenses are less robust than reported...\"\n\nthese elements strongly align with the \"empirical\" classification, which focuses on data-driven studies with statistical analysis and findings. the paper is not proposing a new defense (technical), reviewing literature (survey), proving theorems (theoretical), or detailing a single application (case study), but rather *experimentally evaluating* existing methods."
    },
    "file_name": "3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf"
  },
  {
    "success": true,
    "doc_id": "6ac0f9004e9977cc8bbc85cab8fc2a97",
    "summary": "Here's a focused summary of the paper \"Identity-aware Graph Neural Networks\" by You et al. \\cite{you2021uxi} for a literature review:\n\n### Technical Paper Analysis: Identity-aware Graph Neural Networks \\cite{you2021uxi}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing message passing Graph Neural Networks (GNNs) are fundamentally limited in their expressive power, being upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test. This means they cannot differentiate between certain graph structures or nodes with identical local neighborhood structures but different global properties.\n    *   **Importance & Challenge:** This limitation leads to failures in crucial tasks:\n        *   Inability to predict node clustering coefficients or shortest path distances.\n        *   Failure to differentiate between different d-regular graphs.\n        *   Nodes with distinct neighborhood structures can have identical computational graphs, making them indistinguishable.\n        *   While task-specific feature augmentation can mitigate some issues, it lacks generality and can hinder inductive power. The challenge is to extend the expressive power of *general message passing GNNs* beyond 1-WL without sacrificing their simplicity, efficiency, and broad applicability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Acknowledges recent methods that surpass 1-WL (e.g., P-GNNs, higher-order WL tests) but positions ID-GNN as a *general extension* to *any* message passing GNN, unlike these often task-specific (graph or link level) or computationally complex alternatives.\n        *   Differentiates from existing inductive coloring techniques (e.g., for link prediction, algorithm execution) by proposing a *general model* applicable to node, edge, and graph-level tasks, and compatible with rich node/edge features.\n        *   Emphasizes that ID-GNN is fundamentally different from GNNs with anisotropic message passing (e.g., GAT), as anisotropic methods apply the same function symmetrically across nodes and thus do not change the underlying computational graph limitations.\n    *   **Limitations of Previous Solutions:**\n        *   Many expressive GNNs are task/domain-specific (e.g., P-GNNs for link prediction, not node/graph tasks).\n        *   Increased complexity in computation or implementation for more expressive models.\n        *   Existing coloring techniques are problem-specific and not generally applicable.\n        *   Anisotropic message passing does not address the core limitation of identical computational graphs for structurally different nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (ID-GNN):** `\\cite{you2021uxi}` introduces Identity-aware Graph Neural Networks (ID-GNNs) which extend existing message passing GNNs by inductively considering nodes' identities during message passing.\n        *   **Inductive Identity Coloring:** To embed a target node `v`, its `K`-hop ego network is extracted. The central node `v` is assigned a unique \"coloring\" (identity) that differentiates it from other surrounding nodes within its ego network. This coloring is inductive, allowing generalization to unseen graphs, unlike transductive one-hot encodings.\n        *   **Heterogeneous Message Passing:** During message passing, two distinct sets of parameters (`MSG_1` and `MSG_0`) are used. `MSG_1` is applied to the \"colored\" center node, while `MSG_0` is applied to all other (uncolored) nodes in the ego network. This embeds the identity information directly into the GNN's computational graph.\n    *   **Simplified/Faster Version (ID-GNN-Fast):** This variant injects identity information by using cycle counts (originating from a given node) as augmented node features. These cycle counts can be efficiently computed using powers of the graph's adjacency matrix.\n    *   **Novelty:** The core innovation lies in providing a *minimal yet powerful* and *general* extension to *any* message passing GNN architecture. By introducing inductive identity coloring and heterogeneous message passing, `\\cite{you2021uxi}` allows GNNs to distinguish computational graphs that would otherwise be identical, thereby surpassing the 1-WL test's expressive power while maintaining the benefits of standard message passing GNNs (simplicity, efficiency, broad applicability).\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Proposed Identity-aware Graph Neural Networks (ID-GNNs) as a general framework to enhance the expressive power of message passing GNNs.\n        *   Introduced \"inductive identity coloring\" and \"heterogeneous message passing\" as core mechanisms for injecting and utilizing node identity information.\n        *   Developed ID-GNN-Fast, a simplified version leveraging cycle counts as augmented features for efficiency.\n    *   **Theoretical Insights/Analysis:**\n        *   Theoretically demonstrated that ID-GNNs are strictly more expressive than existing message passing GNNs (e.g., GIN), capable of differentiating graphs that GIN fails to distinguish (Proposition 1).\n        *   Proved that ID-GNNs can count cycles of specific lengths starting and ending at a node (Proposition 2), which is crucial for tasks like predicting clustering coefficients.\n        *   Showed how ID-GNNs can predict clustering coefficients and shortest path distances, and differentiate d-regular graphs, overcoming known GNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on challenging synthetic graph property prediction tasks where existing GNNs fail (predicting node clustering coefficient, shortest path distance, differentiating random d-regular graphs).\n        *   Applied to real-world datasets for node classification, graph classification, and link prediction benchmarks.\n        *   Compared against both existing GNNs and other task-specific expressive graph networks.\n    *   **Key Performance Metrics & Results:**\n        *   **Challenging Property Prediction:** Achieved an average of **40% accuracy improvement** on tasks like predicting node clustering coefficients and shortest path distances, and differentiating d-regular graphs. For instance, ID-GNNs with 6 layers could differentiate 100% of 100 non-isomorphic d-regular graphs, while 1-WL/GNNs differentiate none (Table 1).\n        *   **Node & Graph Classification:** Demonstrated an average of **3% accuracy improvement** on standard benchmarks.\n        *   **Link Prediction:** Showed an average of **15% ROC AUC improvement** on real-world tasks.\n        *   **Comparison with Other Expressive Models:** ID-GNNs achieved improved or comparable performance against other task-specific expressive graph networks, highlighting their versatility.\n        *   **Efficiency:** Matched the number of trainable parameters and computational FLOPS with mini-batch GNNs in experiments, indicating comparable efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** For edge-level tasks, if the target node `v` is outside the `K`-hop ego network of node `u`, ID-GNNs might still suffer from existing GNN failure cases. This necessitates using deeper ID-GNNs for such scenarios.\n    *   **Scope of Applicability:** While presented as a general solution, the ID-GNN-Fast variant relies on cycle counts, which might not capture all forms of identity information relevant to every possible graph task.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{you2021uxi}` significantly advances the technical state-of-the-art by providing a principled, general, and efficient method to overcome the fundamental expressive power limitations of message passing GNNs. It demonstrates that GNNs can indeed surpass the 1-WL test without resorting to complex, higher-order architectures or task-specific modifications.\n    *   **Potential Impact:** This work opens avenues for developing more robust and accurate GNN models across a wide range of applications (node, edge, and graph-level tasks) where structural differentiation is critical. It provides a blueprint for enhancing existing GNN architectures with minimal changes, potentially leading to widespread adoption and improved performance in various graph-based machine learning problems.",
    "intriguing_abstract": "Despite the remarkable success of Graph Neural Networks (GNNs), their expressive power is fundamentally limited by the 1-Weisfeiler-Lehman (1-WL) test, preventing them from distinguishing crucial graph structures or nodes with identical local neighborhoods but distinct global properties. This inherent limitation leads to failures in predicting essential graph properties like clustering coefficients, shortest path distances, and differentiating d-regular graphs.\n\nWe introduce **Identity-aware Graph Neural Networks (ID-GNNs)**, a novel framework that dramatically enhances the expressive power of *any* message passing GNN. Our core innovation lies in **inductive identity coloring** and **heterogeneous message passing**. By assigning a unique, inductive identity to a target node within its ego network and employing distinct message parameters for this 'colored' node, ID-GNNs overcome the 1-WL barrier, enabling GNNs to differentiate computational graphs previously deemed identical.\n\nTheoretically, we prove ID-GNNs are strictly more expressive, capable of counting cycles and predicting complex graph properties. Empirically, ID-GNNs achieve an average of **40% accuracy improvement** on challenging synthetic tasks, **15% ROC AUC improvement** on link prediction, and **3% accuracy gains** on node and graph classification benchmarks. Crucially, ID-GNNs maintain the simplicity and efficiency of standard GNNs, offering a general, powerful, and efficient paradigm shift for robust graph representation learning across diverse applications.",
    "keywords": [
      "Identity-aware Graph Neural Networks (ID-GNNs)",
      "message passing GNNs",
      "1-Weisfeiler-Lehman (1-WL) test",
      "expressive power limitations",
      "inductive identity coloring",
      "heterogeneous message passing",
      "ego network",
      "cycle counts",
      "surpassing 1-WL",
      "graph property prediction",
      "node classification",
      "link prediction",
      "clustering coefficients",
      "shortest path distances",
      "differentiating d-regular graphs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/44b9f16ba417b90e2e7c42f9074378dd06415809.pdf",
    "citation_key": "you2021uxi",
    "metadata": {
      "title": "Identity-aware Graph Neural Networks",
      "authors": [
        "Jiaxuan You",
        "Jonathan M. Gomes-Selman",
        "Rex Ying",
        "J. Leskovec"
      ],
      "published_date": "2021",
      "abstract": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes’ identities during message passing. To embed a given node, ID-GNN first extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified but faster version of ID-GNN that injects node identity information as augmented node features. Alto- gether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs demonstrate improved or comparable performance over other task-specific graph networks.",
      "file_path": "paper_data/Graph_Neural_Networks/44b9f16ba417b90e2e7c42f9074378dd06415809.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Identity-aware Graph Neural Networks\" by You et al. \\cite{you2021uxi} for a literature review:\n\n### Technical Paper Analysis: Identity-aware Graph Neural Networks \\cite{you2021uxi}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing message passing Graph Neural Networks (GNNs) are fundamentally limited in their expressive power, being upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test. This means they cannot differentiate between certain graph structures or nodes with identical local neighborhood structures but different global properties.\n    *   **Importance & Challenge:** This limitation leads to failures in crucial tasks:\n        *   Inability to predict node clustering coefficients or shortest path distances.\n        *   Failure to differentiate between different d-regular graphs.\n        *   Nodes with distinct neighborhood structures can have identical computational graphs, making them indistinguishable.\n        *   While task-specific feature augmentation can mitigate some issues, it lacks generality and can hinder inductive power. The challenge is to extend the expressive power of *general message passing GNNs* beyond 1-WL without sacrificing their simplicity, efficiency, and broad applicability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Acknowledges recent methods that surpass 1-WL (e.g., P-GNNs, higher-order WL tests) but positions ID-GNN as a *general extension* to *any* message passing GNN, unlike these often task-specific (graph or link level) or computationally complex alternatives.\n        *   Differentiates from existing inductive coloring techniques (e.g., for link prediction, algorithm execution) by proposing a *general model* applicable to node, edge, and graph-level tasks, and compatible with rich node/edge features.\n        *   Emphasizes that ID-GNN is fundamentally different from GNNs with anisotropic message passing (e.g., GAT), as anisotropic methods apply the same function symmetrically across nodes and thus do not change the underlying computational graph limitations.\n    *   **Limitations of Previous Solutions:**\n        *   Many expressive GNNs are task/domain-specific (e.g., P-GNNs for link prediction, not node/graph tasks).\n        *   Increased complexity in computation or implementation for more expressive models.\n        *   Existing coloring techniques are problem-specific and not generally applicable.\n        *   Anisotropic message passing does not address the core limitation of identical computational graphs for structurally different nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (ID-GNN):** `\\cite{you2021uxi}` introduces Identity-aware Graph Neural Networks (ID-GNNs) which extend existing message passing GNNs by inductively considering nodes' identities during message passing.\n        *   **Inductive Identity Coloring:** To embed a target node `v`, its `K`-hop ego network is extracted. The central node `v` is assigned a unique \"coloring\" (identity) that differentiates it from other surrounding nodes within its ego network. This coloring is inductive, allowing generalization to unseen graphs, unlike transductive one-hot encodings.\n        *   **Heterogeneous Message Passing:** During message passing, two distinct sets of parameters (`MSG_1` and `MSG_0`) are used. `MSG_1` is applied to the \"colored\" center node, while `MSG_0` is applied to all other (uncolored) nodes in the ego network. This embeds the identity information directly into the GNN's computational graph.\n    *   **Simplified/Faster Version (ID-GNN-Fast):** This variant injects identity information by using cycle counts (originating from a given node) as augmented node features. These cycle counts can be efficiently computed using powers of the graph's adjacency matrix.\n    *   **Novelty:** The core innovation lies in providing a *minimal yet powerful* and *general* extension to *any* message passing GNN architecture. By introducing inductive identity coloring and heterogeneous message passing, `\\cite{you2021uxi}` allows GNNs to distinguish computational graphs that would otherwise be identical, thereby surpassing the 1-WL test's expressive power while maintaining the benefits of standard message passing GNNs (simplicity, efficiency, broad applicability).\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Proposed Identity-aware Graph Neural Networks (ID-GNNs) as a general framework to enhance the expressive power of message passing GNNs.\n        *   Introduced \"inductive identity coloring\" and \"heterogeneous message passing\" as core mechanisms for injecting and utilizing node identity information.\n        *   Developed ID-GNN-Fast, a simplified version leveraging cycle counts as augmented features for efficiency.\n    *   **Theoretical Insights/Analysis:**\n        *   Theoretically demonstrated that ID-GNNs are strictly more expressive than existing message passing GNNs (e.g., GIN), capable of differentiating graphs that GIN fails to distinguish (Proposition 1).\n        *   Proved that ID-GNNs can count cycles of specific lengths starting and ending at a node (Proposition 2), which is crucial for tasks like predicting clustering coefficients.\n        *   Showed how ID-GNNs can predict clustering coefficients and shortest path distances, and differentiate d-regular graphs, overcoming known GNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on challenging synthetic graph property prediction tasks where existing GNNs fail (predicting node clustering coefficient, shortest path distance, differentiating random d-regular graphs).\n        *   Applied to real-world datasets for node classification, graph classification, and link prediction benchmarks.\n        *   Compared against both existing GNNs and other task-specific expressive graph networks.\n    *   **Key Performance Metrics & Results:**\n        *   **Challenging Property Prediction:** Achieved an average of **40% accuracy improvement** on tasks like predicting node clustering coefficients and shortest path distances, and differentiating d-regular graphs. For instance, ID-GNNs with 6 layers could differentiate 100% of 100 non-isomorphic d-regular graphs, while 1-WL/GNNs differentiate none (Table 1).\n        *   **Node & Graph Classification:** Demonstrated an average of **3% accuracy improvement** on standard benchmarks.\n        *   **Link Prediction:** Showed an average of **15% ROC AUC improvement** on real-world tasks.\n        *   **Comparison with Other Expressive Models:** ID-GNNs achieved improved or comparable performance against other task-specific expressive graph networks, highlighting their versatility.\n        *   **Efficiency:** Matched the number of trainable parameters and computational FLOPS with mini-batch GNNs in experiments, indicating comparable efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** For edge-level tasks, if the target node `v` is outside the `K`-hop ego network of node `u`, ID-GNNs might still suffer from existing GNN failure cases. This necessitates using deeper ID-GNNs for such scenarios.\n    *   **Scope of Applicability:** While presented as a general solution, the ID-GNN-Fast variant relies on cycle counts, which might not capture all forms of identity information relevant to every possible graph task.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{you2021uxi}` significantly advances the technical state-of-the-art by providing a principled, general, and efficient method to overcome the fundamental expressive power limitations of message passing GNNs. It demonstrates that GNNs can indeed surpass the 1-WL test without resorting to complex, higher-order architectures or task-specific modifications.\n    *   **Potential Impact:** This work opens avenues for developing more robust and accurate GNN models across a wide range of applications (node, edge, and graph-level tasks) where structural differentiation is critical. It provides a blueprint for enhancing existing GNN architectures with minimal changes, potentially leading to widespread adoption and improved performance in various graph-based machine learning problems.",
      "keywords": [
        "Identity-aware Graph Neural Networks (ID-GNNs)",
        "message passing GNNs",
        "1-Weisfeiler-Lehman (1-WL) test",
        "expressive power limitations",
        "inductive identity coloring",
        "heterogeneous message passing",
        "ego network",
        "cycle counts",
        "surpassing 1-WL",
        "graph property prediction",
        "node classification",
        "link prediction",
        "clustering coefficients",
        "shortest path distances",
        "differentiating d-regular graphs"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"here we **develop a class of message passing gnns, named identity-aware graph neural networks (id-gnns)**\", \"id-gnn offers a minimal but powerful **solution** to limitations of existing gnns.\", \"id-gnn **extends existing gnn architectures**\", \"we further **propose a simpliﬁed but faster version of id-gnn**\".\n*   the introduction discusses a \"fundamental limitation of existing gnns\" (a technical problem) which the proposed id-gnns aim to solve.\n*   the mention of experiments and accuracy improvements serves to validate the effectiveness of the *newly developed method*.\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems.\n\n**classification: technical**"
    },
    "file_name": "44b9f16ba417b90e2e7c42f9074378dd06415809.pdf"
  },
  {
    "success": true,
    "doc_id": "4a6c3b864684c21c169031fc5881af44",
    "summary": "Here's a focused summary of the paper for literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining predictions made by Graph Neural Networks (GNNs) is challenging. Existing methods primarily offer local, instance-level explanations (e.g., important subgraph structures or node features for a single prediction).\n    *   **Importance & Challenge**: This instance-level approach leads to explanations that are \"painstakingly customized,\" lack generalizability, and hinder understanding of the GNN model's global behavior. Training such explanation models for each instance is also time-consuming, especially for large-scale datasets, making them unsuitable for inductive settings.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work directly addresses the limitations of \"leading methods\" that focus on local, single-instance explanations.\n    *   **Limitations of Previous Solutions**: Previous solutions generate unique explanations for each instance independently, failing to provide a global understanding of the GNN model. They lack generalizability, cannot be used in inductive settings without retraining, and are inefficient for large datasets due to per-instance training.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes PGExplainer, a parameterized explainer for GNNs. It adopts a deep neural network to parameterize the generation process of explanations.\n    *   **Novelty**: This parameterization allows PGExplainer to generate \"multi-instance explanations,\" moving beyond the single-instance paradigm. It enables better generalization and applicability in an inductive setting without requiring retraining for new instances.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of PGExplainer, a parameterized deep neural network-based approach for GNN explanation generation.\n    *   **System Design/Architectural Innovations**: The design allows for multi-instance explanations, a significant departure from prior local explanation methods.\n    *   **Efficiency**: Achieves significant speed-up compared to leading methods due to its inductive capabilities and lack of per-instance retraining.\n    *   **Regularization**: The explanation networks can also be utilized as a regularizer to improve the generalization power of existing GNNs when jointly trained with downstream tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on both synthetic and real-life datasets.\n    *   **Key Performance Metrics**: Performance is measured using metrics such as AUC (Area Under the Curve) for explaining graph classification.\n    *   **Comparison Results**: PGExplainer demonstrates \"highly competitive performance,\" achieving up to a 24.7% relative improvement in AUC on explaining graph classification over the leading baseline.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily highlights the limitations of *previous* methods (local, instance-specific, inefficient) which PGExplainer aims to overcome. The text does not explicitly state limitations of PGExplainer itself, but rather its broad applicability.\n    *   **Scope of Applicability**: Applicable to multi-instance explanation generation, inductive settings, and large-scale real-life datasets. It can also serve as a regularizer for GNNs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PGExplainer advances GNN interpretability by shifting from local, instance-specific explanations to a more generalizable, multi-instance, and parameterized approach.\n    *   **Potential Impact**: Enables a global understanding of GNN models, significantly improves efficiency for explanation generation, and allows for the use of explanation models in inductive settings. Its regularization capability also offers a novel way to enhance GNN performance \\cite{luo2024euy}.",
    "intriguing_abstract": "Unlocking the black box of Graph Neural Networks (GNNs) remains a critical challenge, with existing explanation methods largely confined to local, instance-specific insights that are inefficient and lack generalizability. We introduce PGExplainer, a novel **parameterized explainer** that fundamentally shifts the paradigm of GNN interpretability. By employing a **deep neural network** to generate explanations, PGExplainer moves beyond painstaking single-instance customization to provide robust **multi-instance explanations**.\n\nThis innovative approach enables seamless application in **inductive settings**, drastically reducing computation time and making GNN explanations scalable for large datasets. Furthermore, PGExplainer can serve as a powerful **regularizer**, enhancing the generalization capabilities of GNNs when jointly trained. Extensive experiments on synthetic and real-world datasets demonstrate PGExplainer's superior performance, achieving up to a 24.7% relative improvement in explaining **graph classification** tasks. Our work paves the way for a more global, efficient, and generalizable understanding of GNN behavior, significantly advancing the state-of-the-art in GNN **explainability**.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN interpretability",
      "PGExplainer",
      "Parameterized explainer",
      "Multi-instance explanations",
      "Inductive explanation generation",
      "Global model understanding",
      "GNN regularization",
      "Explanation efficiency",
      "Graph classification",
      "Deep neural networks",
      "Instance-level explanations",
      "Generalizability"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/35792d528bd07aed95df46f0ecb87019cb123147.pdf",
    "citation_key": "luo2024euy",
    "metadata": {
      "title": "Towards Inductive and Efficient Explanations for Graph Neural Networks",
      "authors": [
        "Dongsheng Luo",
        "Tianxiang Zhao",
        "Wei Cheng",
        "Dongkuan Xu",
        "Feng Han",
        "Wenchao Yu",
        "Xiao Liu",
        "Haifeng Chen",
        "Xiang Zhang"
      ],
      "published_date": "2024",
      "abstract": "Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging and nascent problem. The leading method mainly considers the local explanations, i.e., important subgraph structure and node features, to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized at the instance level. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, training the explanation model explaining for each instance is time-consuming for large-scale real-life datasets. In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which renders PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has better generalization ability and can be utilized in an inductive setting without training the model for new instances. Thus, PGExplainer is much more efficient than the leading method with significant speed-up. In addition, the explanation networks can also be utilized as a regularizer to improve the generalization power of existing GNNs when jointly trained with downstream tasks. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7% relative improvement in AUC on explaining graph classification over the leading baseline.",
      "file_path": "paper_data/Graph_Neural_Networks/35792d528bd07aed95df46f0ecb87019cb123147.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining predictions made by Graph Neural Networks (GNNs) is challenging. Existing methods primarily offer local, instance-level explanations (e.g., important subgraph structures or node features for a single prediction).\n    *   **Importance & Challenge**: This instance-level approach leads to explanations that are \"painstakingly customized,\" lack generalizability, and hinder understanding of the GNN model's global behavior. Training such explanation models for each instance is also time-consuming, especially for large-scale datasets, making them unsuitable for inductive settings.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work directly addresses the limitations of \"leading methods\" that focus on local, single-instance explanations.\n    *   **Limitations of Previous Solutions**: Previous solutions generate unique explanations for each instance independently, failing to provide a global understanding of the GNN model. They lack generalizability, cannot be used in inductive settings without retraining, and are inefficient for large datasets due to per-instance training.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes PGExplainer, a parameterized explainer for GNNs. It adopts a deep neural network to parameterize the generation process of explanations.\n    *   **Novelty**: This parameterization allows PGExplainer to generate \"multi-instance explanations,\" moving beyond the single-instance paradigm. It enables better generalization and applicability in an inductive setting without requiring retraining for new instances.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of PGExplainer, a parameterized deep neural network-based approach for GNN explanation generation.\n    *   **System Design/Architectural Innovations**: The design allows for multi-instance explanations, a significant departure from prior local explanation methods.\n    *   **Efficiency**: Achieves significant speed-up compared to leading methods due to its inductive capabilities and lack of per-instance retraining.\n    *   **Regularization**: The explanation networks can also be utilized as a regularizer to improve the generalization power of existing GNNs when jointly trained with downstream tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on both synthetic and real-life datasets.\n    *   **Key Performance Metrics**: Performance is measured using metrics such as AUC (Area Under the Curve) for explaining graph classification.\n    *   **Comparison Results**: PGExplainer demonstrates \"highly competitive performance,\" achieving up to a 24.7% relative improvement in AUC on explaining graph classification over the leading baseline.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily highlights the limitations of *previous* methods (local, instance-specific, inefficient) which PGExplainer aims to overcome. The text does not explicitly state limitations of PGExplainer itself, but rather its broad applicability.\n    *   **Scope of Applicability**: Applicable to multi-instance explanation generation, inductive settings, and large-scale real-life datasets. It can also serve as a regularizer for GNNs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PGExplainer advances GNN interpretability by shifting from local, instance-specific explanations to a more generalizable, multi-instance, and parameterized approach.\n    *   **Potential Impact**: Enables a global understanding of GNN models, significantly improves efficiency for explanation generation, and allows for the use of explanation models in inductive settings. Its regularization capability also offers a novel way to enhance GNN performance \\cite{luo2024euy}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN interpretability",
        "PGExplainer",
        "Parameterized explainer",
        "Multi-instance explanations",
        "Inductive explanation generation",
        "Global model understanding",
        "GNN regularization",
        "Explanation efficiency",
        "Graph classification",
        "Deep neural networks",
        "Instance-level explanations",
        "Generalizability"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** identifies a significant problem with existing methods for explaining gnns: they are local, lack generalizability, are not inductive, and are time-consuming. this sets the stage for a new solution.\n*   the **introduction** immediately introduces \"pgexplainer\" as a \"natural approach\" that addresses these limitations, offering \"better generalization ability,\" \"inductive setting\" capabilities, and \"significant speed-up.\" it describes the features and benefits of this new approach.\n*   crucially, the introduction also mentions: \"experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7% relative improvement in auc...\" this indicates empirical validation of the proposed method.\n\nwhile there is a strong empirical component (experiments and results), the core contribution described is the **development and presentation of a new method/system (pgexplainer)** to solve a technical problem. the experiments serve to validate the effectiveness of this new method.\n\ntherefore, this paper best fits the **technical** classification.\n\n**classification:** technical"
    },
    "file_name": "35792d528bd07aed95df46f0ecb87019cb123147.pdf"
  },
  {
    "success": true,
    "doc_id": "9b7e2a7e77919d71019f856c04e61769",
    "summary": "Here's a focused summary of the paper by Cui et al. \\cite{cui2022pap} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Analyzing connectome-based brain networks for brain disorder diagnosis, specifically addressing the lack of interpretability in deep learning models (like GNNs) and their suboptimal handling of unique brain network properties.\n    *   **Importance & Challenge**: Understanding brain mechanisms is crucial for neuroscience and clinical therapy. Deep models, while powerful, are \"black boxes,\" preventing their use in decision-critical healthcare contexts. Existing GNN explanation methods often produce subject-specific explanations for graph-level tasks, whereas disorder analysis benefits from *group-level, disorder-specific* patterns. Brain networks also feature both positive and negative edge weights (correlations) that conventional GNNs struggle with.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on brain networks includes shallow models (graph kernels, tensor factorizations) and deep models (CNNs, GNNs). GNNs are recognized for their power in analyzing graph-structured data and capturing complex connectome structures.\n    *   **Limitations of Previous Solutions**:\n        *   Shallow models may lack the expressiveness to capture sophisticated connectome structures.\n        *   GNNs suffer from low interpretability and are prone to overfitting, hindering their application in critical medical diagnosis.\n        *   Most GNN explanation methods focus on node-level tasks or generate unique explanations for each subject in graph-level tasks, which is not ideal for identifying common disorder-specific patterns across a group.\n        *   Vanilla GNNs do not correctly handle brain network edge weights that can be both positive and negative.\n        *   Directly using attention weights from GNNs as explanations is known to be problematic.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: A two-module interpretable GNN framework:\n        1.  **Backbone Model (IBGNN)**: A brain-network-oriented message passing GNN for disease prediction.\n            *   **Edge-Weight-Aware Message Passing**: Addresses the challenge of positive and negative edge weights in brain networks by constructing message vectors that concatenate node embeddings and the edge weight.\n            *   **Propagation Rule**: Aggregates messages from neighbors using a non-linear activation function.\n            *   **Readout Function**: Summarizes node embeddings into a graph-level embedding using an MLP with residual connections.\n        2.  **Globally Shared Explanation Generator**: Learns a disorder-specific edge mask `M` (M ∈ R^M×M) that is applied to *all* brain network subjects in a dataset.\n            *   **Objective**: Maximizes agreement between predictions on the original graph and an explanation graph induced by the mask (W' = W ⊙ σ(M)), formulated as a cross-entropy loss.\n            *   **Regularization**: Incorporates sparsity (`L_sps`) and discreteness (`L_ent`) terms to encourage compact and clear explanations.\n    *   **Combined Model (IBGNN+)**: The learned explanation mask is used to enhance the original brain networks, and the backbone model is fine-tuned again. This allows for simultaneous prediction and interpretation, leveraging the identified biomarkers to boost prediction performance.\n    *   **Novelty**:\n        *   **Group-level, disorder-specific explanations**: A key innovation is learning a *globally shared* mask to identify common patterns for specific disorders across a group, addressing the limitation of subject-specific explanations.\n        *   **Brain-network-specific GNN design**: The IBGNN backbone is explicitly designed to handle the unique characteristics of brain networks, particularly the positive and negative edge weights.\n        *   **Closed-loop prediction and interpretation**: The framework demonstrates that the identified explanations can be integrated back into the model to *improve* prediction performance, creating a synergistic relationship.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   An interpretable GNN framework with a specialized backbone (IBGNN) and a globally shared explanation generator.\n        *   The IBGNN backbone features an edge-weight-aware message passing mechanism tailored for brain networks.\n        *   A novel globally shared explanation generator that learns a single, disorder-specific edge mask for group-level interpretation.\n    *   **System Design/Architectural Innovations**:\n        *   A modular framework that integrates prediction and interpretation, allowing for the enhancement of the backbone model with learned explanations (IBGNN+) to boost performance.\n    *   **Theoretical Insights/Analysis**:\n        *   The recognition that for graph-level connectome analysis, disorder-specific explanations *across instances* are preferable, guiding the design of the globally shared mask.\n        *   The insight that leveraging disorder-specific signals (via the explanation mask) can mitigate noise in raw brain network data and improve prediction.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated prediction performance (Accuracy, F1, AUC) of IBGNN and IBGNN+ against various shallow and deep learning baselines.\n        *   Analyzed the generated disorder-specific biomarkers (salient ROIs and important connections) for their neuroscientific meaningfulness.\n        *   Demonstrated the performance boost achieved by enhancing the backbone with learned explanations (IBGNN+).\n    *   **Datasets**: Three real-world neuroimaging datasets of different modalities:\n        *   **Human Immunodeﬁciency Virus Infection (HIV)**: fMRI, 70 subjects, 90 cerebral regions.\n        *   **Bipolar Disorder (BP)**: DTI, 97 subjects, 82 regions.\n        *   **Parkinson’s Progression Markers Initiative (PPMI)**: DTI, 754 subjects, 84 ROIs.\n    *   **Key Performance Metrics**: Accuracy, F1-score, and Area Under the Curve (AUC).\n    *   **Comparison Results**:\n        *   **IBGNN** significantly outperformed both shallow and state-of-the-art deep baselines, showing up to 11% absolute improvement on BP, validating its brain network-oriented design.\n        *   **IBGNN+** further boosted prediction performance, achieving up to 9.7% *relative* improvement over IBGNN, demonstrating that the learned explanation mask effectively highlights disorder-specific signals and reduces noise.\n        *   The explanation generator successfully revealed disorder-specific biomarkers that coincided with neuroscience findings.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes that common brain network patterns exist for subjects with the same disorder, justifying the globally shared explanation. The specific details of how the identified biomarkers align with neuroscience findings are mentioned but not elaborated in the provided text.\n    *   **Scope of Applicability**: The framework is primarily designed for connectome-based brain disorder analysis using various neuroimaging modalities. The explanation enhancement strategy is noted to be compatible with any backbone model, suggesting broader applicability for improving robustness in graph-level prediction tasks.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the state-of-the-art by providing a novel, interpretable GNN framework that not only achieves superior prediction performance for brain disorder analysis but also offers meaningful, group-level explanations. It bridges the critical gap between deep learning performance and interpretability in a high-stakes medical domain.\n    *   **Potential Impact on Future Research**: The framework's ability to provide disorder-specific biomarkers can facilitate early diagnosis, enhance understanding of neurological disorders, and guide future neuroscience research. The concept of globally shared explanation masks and explanation-enhanced backbones could inspire new interpretable GNN designs for other complex graph-structured data where group-level insights are crucial.",
    "intriguing_abstract": "Deep learning models, particularly Graph Neural Networks (GNNs), offer immense potential for analyzing complex brain connectome data in neurological disorder diagnosis. However, their inherent \"black-box\" nature and inability to provide *group-level, disorder-specific* insights critically hinder clinical adoption. We introduce a novel interpretable GNN framework designed to bridge this gap. Our approach features an **Interpretable Brain Graph Neural Network (IBGNN)** with an innovative edge-weight-aware message passing mechanism, specifically tailored for the unique positive and negative correlations in brain networks. Crucially, we develop a **globally shared explanation generator** that learns a single, disorder-specific edge mask, providing unprecedented group-level biomarkers for common patterns across patients. This mask not only offers neuroscientifically meaningful interpretations but, when integrated back into the model (IBGNN+), significantly *boosts prediction performance*, demonstrating a powerful closed-loop synergy between interpretability and accuracy. Evaluated on three real-world neuroimaging datasets (fMRI, DTI), IBGNN+ achieves state-of-the-art diagnostic accuracy while revealing robust, disorder-specific connectomic biomarkers. This work paves the way for more trustworthy and effective brain disorder diagnosis.",
    "keywords": [
      "Interpretable GNN framework",
      "Brain disorder diagnosis",
      "Connectome-based brain networks",
      "Globally shared explanation mask",
      "Group-level disorder-specific explanations",
      "Edge-weight-aware message passing",
      "Neuroscientifically meaningful biomarkers",
      "Closed-loop prediction and interpretation",
      "Deep learning interpretability",
      "Positive and negative edge weights",
      "Neuroimaging datasets",
      "Prediction performance enhancement"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf",
    "citation_key": "cui2022pap",
    "metadata": {
      "title": "Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis",
      "authors": [
        "Hejie Cui",
        "Wei Dai",
        "Yanqiao Zhu",
        "Xiaoxiao Li",
        "Lifang He",
        "Carl Yang"
      ],
      "published_date": "2022",
      "abstract": "Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.",
      "file_path": "paper_data/Graph_Neural_Networks/a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper by Cui et al. \\cite{cui2022pap} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Analyzing connectome-based brain networks for brain disorder diagnosis, specifically addressing the lack of interpretability in deep learning models (like GNNs) and their suboptimal handling of unique brain network properties.\n    *   **Importance & Challenge**: Understanding brain mechanisms is crucial for neuroscience and clinical therapy. Deep models, while powerful, are \"black boxes,\" preventing their use in decision-critical healthcare contexts. Existing GNN explanation methods often produce subject-specific explanations for graph-level tasks, whereas disorder analysis benefits from *group-level, disorder-specific* patterns. Brain networks also feature both positive and negative edge weights (correlations) that conventional GNNs struggle with.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on brain networks includes shallow models (graph kernels, tensor factorizations) and deep models (CNNs, GNNs). GNNs are recognized for their power in analyzing graph-structured data and capturing complex connectome structures.\n    *   **Limitations of Previous Solutions**:\n        *   Shallow models may lack the expressiveness to capture sophisticated connectome structures.\n        *   GNNs suffer from low interpretability and are prone to overfitting, hindering their application in critical medical diagnosis.\n        *   Most GNN explanation methods focus on node-level tasks or generate unique explanations for each subject in graph-level tasks, which is not ideal for identifying common disorder-specific patterns across a group.\n        *   Vanilla GNNs do not correctly handle brain network edge weights that can be both positive and negative.\n        *   Directly using attention weights from GNNs as explanations is known to be problematic.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: A two-module interpretable GNN framework:\n        1.  **Backbone Model (IBGNN)**: A brain-network-oriented message passing GNN for disease prediction.\n            *   **Edge-Weight-Aware Message Passing**: Addresses the challenge of positive and negative edge weights in brain networks by constructing message vectors that concatenate node embeddings and the edge weight.\n            *   **Propagation Rule**: Aggregates messages from neighbors using a non-linear activation function.\n            *   **Readout Function**: Summarizes node embeddings into a graph-level embedding using an MLP with residual connections.\n        2.  **Globally Shared Explanation Generator**: Learns a disorder-specific edge mask `M` (M ∈ R^M×M) that is applied to *all* brain network subjects in a dataset.\n            *   **Objective**: Maximizes agreement between predictions on the original graph and an explanation graph induced by the mask (W' = W ⊙ σ(M)), formulated as a cross-entropy loss.\n            *   **Regularization**: Incorporates sparsity (`L_sps`) and discreteness (`L_ent`) terms to encourage compact and clear explanations.\n    *   **Combined Model (IBGNN+)**: The learned explanation mask is used to enhance the original brain networks, and the backbone model is fine-tuned again. This allows for simultaneous prediction and interpretation, leveraging the identified biomarkers to boost prediction performance.\n    *   **Novelty**:\n        *   **Group-level, disorder-specific explanations**: A key innovation is learning a *globally shared* mask to identify common patterns for specific disorders across a group, addressing the limitation of subject-specific explanations.\n        *   **Brain-network-specific GNN design**: The IBGNN backbone is explicitly designed to handle the unique characteristics of brain networks, particularly the positive and negative edge weights.\n        *   **Closed-loop prediction and interpretation**: The framework demonstrates that the identified explanations can be integrated back into the model to *improve* prediction performance, creating a synergistic relationship.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   An interpretable GNN framework with a specialized backbone (IBGNN) and a globally shared explanation generator.\n        *   The IBGNN backbone features an edge-weight-aware message passing mechanism tailored for brain networks.\n        *   A novel globally shared explanation generator that learns a single, disorder-specific edge mask for group-level interpretation.\n    *   **System Design/Architectural Innovations**:\n        *   A modular framework that integrates prediction and interpretation, allowing for the enhancement of the backbone model with learned explanations (IBGNN+) to boost performance.\n    *   **Theoretical Insights/Analysis**:\n        *   The recognition that for graph-level connectome analysis, disorder-specific explanations *across instances* are preferable, guiding the design of the globally shared mask.\n        *   The insight that leveraging disorder-specific signals (via the explanation mask) can mitigate noise in raw brain network data and improve prediction.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated prediction performance (Accuracy, F1, AUC) of IBGNN and IBGNN+ against various shallow and deep learning baselines.\n        *   Analyzed the generated disorder-specific biomarkers (salient ROIs and important connections) for their neuroscientific meaningfulness.\n        *   Demonstrated the performance boost achieved by enhancing the backbone with learned explanations (IBGNN+).\n    *   **Datasets**: Three real-world neuroimaging datasets of different modalities:\n        *   **Human Immunodeﬁciency Virus Infection (HIV)**: fMRI, 70 subjects, 90 cerebral regions.\n        *   **Bipolar Disorder (BP)**: DTI, 97 subjects, 82 regions.\n        *   **Parkinson’s Progression Markers Initiative (PPMI)**: DTI, 754 subjects, 84 ROIs.\n    *   **Key Performance Metrics**: Accuracy, F1-score, and Area Under the Curve (AUC).\n    *   **Comparison Results**:\n        *   **IBGNN** significantly outperformed both shallow and state-of-the-art deep baselines, showing up to 11% absolute improvement on BP, validating its brain network-oriented design.\n        *   **IBGNN+** further boosted prediction performance, achieving up to 9.7% *relative* improvement over IBGNN, demonstrating that the learned explanation mask effectively highlights disorder-specific signals and reduces noise.\n        *   The explanation generator successfully revealed disorder-specific biomarkers that coincided with neuroscience findings.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes that common brain network patterns exist for subjects with the same disorder, justifying the globally shared explanation. The specific details of how the identified biomarkers align with neuroscience findings are mentioned but not elaborated in the provided text.\n    *   **Scope of Applicability**: The framework is primarily designed for connectome-based brain disorder analysis using various neuroimaging modalities. The explanation enhancement strategy is noted to be compatible with any backbone model, suggesting broader applicability for improving robustness in graph-level prediction tasks.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the state-of-the-art by providing a novel, interpretable GNN framework that not only achieves superior prediction performance for brain disorder analysis but also offers meaningful, group-level explanations. It bridges the critical gap between deep learning performance and interpretability in a high-stakes medical domain.\n    *   **Potential Impact on Future Research**: The framework's ability to provide disorder-specific biomarkers can facilitate early diagnosis, enhance understanding of neurological disorders, and guide future neuroscience research. The concept of globally shared explanation masks and explanation-enhanced backbones could inspire new interpretable GNN designs for other complex graph-structured data where group-level insights are crucial.",
      "keywords": [
        "Interpretable GNN framework",
        "Brain disorder diagnosis",
        "Connectome-based brain networks",
        "Globally shared explanation mask",
        "Group-level disorder-specific explanations",
        "Edge-weight-aware message passing",
        "Neuroscientifically meaningful biomarkers",
        "Closed-loop prediction and interpretation",
        "Deep learning interpretability",
        "Positive and negative edge weights",
        "Neuroimaging datasets",
        "Prediction performance enhancement"
      ],
      "paper_type": "this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we **propose** an interpretable framework\", \"the **proposed framework consists of two modules**\". these directly align with the \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" criteria for a technical paper.\n*   **introduction discusses:** it identifies a \"technical problem\" (gnns lack transparency in decision-critical contexts like healthcare, especially for graph-level tasks) and sets the stage for a \"proposed solution\" (the interpretable framework).\n*   while it also conducts \"experiments on three real-world datasets\" and discusses \"results,\" which are strong indicators of an **empirical** paper, the core contribution is the *development of a new framework*. the experiments serve to validate this novel technical contribution. many technical papers include empirical validation. if forced to choose one, the emphasis on proposing and describing a new system/method makes \"technical\" the primary classification."
    },
    "file_name": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf"
  },
  {
    "success": true,
    "doc_id": "9c877654fac36b2a1add1a8f5f44f61c",
    "summary": "Here's a focused summary of the paper \"Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels\" \\cite{dai2022xze} for a literature review:\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer significant performance degradation when trained on real-world graphs that contain structural noise (noisy/adversarial edges) and have a limited number of labeled nodes (label sparsity).\n*   **Importance and Challenge:**\n    *   **Noisy Edges:** GNNs' message-passing mechanism makes them vulnerable to noisy or adversarial edges, which can contaminate node representations and propagate errors. Real-world graphs often contain inherent noise (e.g., bots in social networks) or are subject to poisoning attacks.\n    *   **Limited Labeled Nodes:** Many applications have sparsely labeled graphs. Label sparsity reduces the involvement of unlabeled nodes in message passing, making GNNs less effective in leveraging neighborhood information for semi-supervised learning.\n    *   **Combined Challenge:** Developing robust GNNs that can *simultaneously* handle both noisy edges and label sparsity is challenging because the training graph itself is noisy (requiring supervision to identify/mitigate noise) and obtaining more labels is expensive. Existing work primarily addresses these issues in isolation.\n\n### 2. Related Work & Positioning\n\n*   **Existing Approaches & Limitations:**\n    *   **Robust GNNs (against adversarial edges):** Methods like pruning edges based on similarity \\cite{dai2022xze} or using low-rank approximation (e.g., Pro-GNN \\cite{dai2022xze}) exist. However, these often focus *only* on adversarial edges, not label sparsity. Pro-GNN, while learning a clean graph, incurs high computational costs due to direct graph learning and sparse low-rank constraints.\n    *   **GNNs for Sparse Labels:** Approaches using self-supervised learning tasks (e.g., pseudo label prediction, global context predictions) aim to obtain better representations. However, these generally do not address structural noise.\n    *   **Gap:** The paper highlights a significant gap: \"little efforts are taken for robust GNNs that can simultaneously handle noisy edges and label sparsity.\"\n*   **Positioning of this Work:** This work is novel because it studies the *combined problem* of developing robust GNNs for *both* noisy graphs and label sparsity. Unlike prior robust GNNs, it simultaneously tackles both issues by learning a link predictor to down-weight noisy edges and connect similar nodes, and it uses a link predictor instead of direct graph learning to improve computational efficiency.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method (RS-GNN Framework):**\n    1.  **Denoised and Dense Graph Learning:** A novel MLP-based link predictor is employed. It takes node attributes as input to learn a new, dense adjacency matrix (S).\n        *   **Denoising:** The link predictor assigns small weights to links connecting nodes with dissimilar features (likely noisy edges) and large weights to links between similar nodes, effectively down-weighting or eliminating noisy edges.\n        *   **Densifying:** It predicts missing links between similar nodes, thereby densifying the graph. This increases the number of unlabeled nodes involved in message passing, alleviating label sparsity.\n    2.  **Feature Similarity Weighted Edge-Reconstruction Loss:** A novel loss function is designed to train the link predictor, specifically to reduce the negative effects of noisy edges on the link prediction process itself.\n    3.  **GNN Classifier:** A standard GCN classifier (or similar GNN) is trained on the *learned* denoised and densified graph (S) and original node features (X) for node classification.\n    4.  **Label Smoothness Regularization:** The predicted edges from the link predictor are further used to explicitly regularize the predictions of unlabeled nodes. This encourages adjacent nodes in the learned graph to have similar labels, further mitigating the label sparsity issue.\n*   **Novelty/Differentiation:**\n    *   Simultaneously addresses the dual challenges of noisy edges and sparse labels within a single framework.\n    *   Introduces a link predictor that leverages node attributes and noisy edges as supervision to *dynamically learn* a denoised and densified graph, rather than relying on fixed preprocessing or computationally expensive direct graph learning.\n    *   The use of a feature similarity weighted edge-reconstruction loss specifically for training the link predictor in a noisy environment is innovative.\n    *   Integrates the learned graph for both GNN input and as a source for label smoothness regularization.\n\n### 4. Key Technical Contributions\n\n*   **Novel Problem Formulation:** First to study the problem of learning robust noise-resistant GNNs that can handle both noisy graphs and limited labeled nodes simultaneously \\cite{dai2022xze}.\n*   **Novel Framework (RS-GNN):** Proposes a unified framework that concurrently learns a denoised and densified graph and a robust GNN.\n*   **Link Predictor for Graph Reconstruction:** Introduces an MLP-based link predictor that uses node attributes to effectively down-weight/remove noisy edges and complete missing links, which is crucial for both robustness and alleviating label sparsity.\n*   **Feature Similarity Weighted Edge-Reconstruction Loss:** A specific loss function designed to train the link predictor robustly in the presence of noisy edges.\n*   **Label Smoothness Regularization:** Incorporates a regularization term based on the predicted edges to improve predictions for unlabeled nodes, leveraging the learned graph structure.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted:** Extensive experiments were performed on real-world datasets (Citeseer, Cora, Cora-ML) to evaluate the robustness of RS-GNN.\n*   **Key Performance Metrics & Comparison Results:**\n    *   The paper demonstrates the robustness of RS-GNN on *both* noisy and clean graphs, particularly under conditions of limited labeled nodes.\n    *   Preliminary analysis shows that decreasing label rate significantly increases the uninvolved node rate in GNN training, and conversely, increasing graph density significantly reduces this rate, validating the core idea of densification.\n    *   (Specific quantitative results are not provided in the abstract/intro but are implied by \"extensive experiments\" and \"demonstrate the robustness\").\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions:**\n    *   The approach assumes that nodes with similar features tend to be linked and have similar labels, while noisy edges connect dissimilar nodes. This assumption underpins the link predictor's ability to denoise and densify.\n    *   The effectiveness of the MLP-based link predictor relies on the quality and discriminative power of node attributes.\n    *   The paper focuses on semi-supervised node classification, and its direct applicability to other graph tasks (e.g., graph classification, link prediction itself as a primary task) might require adaptation.\n*   **Scope of Applicability:** Primarily applicable to semi-supervised node classification tasks on attributed graphs where structural noise and label sparsity are prevalent, such as social networks, citation networks, and other real-world graph datasets.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art:** RS-GNN significantly advances the state-of-the-art by providing a unified and computationally efficient solution to the critical dual problem of GNN robustness against structural noise and label sparsity. It moves beyond addressing these issues in isolation.\n*   **Potential Impact on Future Research:**\n    *   Opens new avenues for research into joint learning of robust graph structures and GNN models.\n    *   Encourages further exploration of attribute-based link prediction for graph denoising and densification in challenging real-world scenarios.\n    *   Could inspire more sophisticated regularization techniques that leverage learned graph structures to improve GNN performance under data scarcity.\n    *   Its computational efficiency (using a link predictor instead of direct graph learning) makes it a practical baseline for future robust GNN designs.",
    "intriguing_abstract": "Real-world graphs present a formidable challenge for Graph Neural Networks (GNNs): their performance plummets under the dual pressures of structural noise and severe label sparsity. Existing robust GNNs typically address these issues in isolation, leaving a critical void for solutions that can simultaneously navigate *both* noisy edges and limited labeled data. This paper introduces **RS-GNN**, a novel, unified framework for robust semi-supervised node classification.\n\nRS-GNN pioneers an MLP-based link predictor that dynamically learns a denoised and densified adjacency matrix directly from node attributes. This innovative approach effectively down-weights noisy connections while completing missing links between similar nodes, thereby enhancing message passing and mitigating the detrimental effects of structural noise and label scarcity. A novel feature similarity weighted edge-reconstruction loss ensures robust training of this predictor, further complemented by label smoothness regularization to leverage the learned graph for unlabeled nodes. Extensive experiments demonstrate RS-GNN's superior robustness and accuracy across diverse noise levels and label rates, significantly advancing the state-of-the-art. Our computationally efficient framework offers a practical and powerful solution, paving the way for more reliable GNN applications in complex, real-world graph data.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "noisy graphs",
      "label sparsity",
      "robust GNNs",
      "RS-GNN framework",
      "Denoised and Dense Graph Learning",
      "MLP-based link predictor",
      "Feature Similarity Weighted Edge-Reconstruction Loss",
      "Label Smoothness Regularization",
      "node classification",
      "combined problem",
      "computational efficiency",
      "semi-supervised learning",
      "graph reconstruction"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf",
    "citation_key": "dai2022xze",
    "metadata": {
      "title": "Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels",
      "authors": [
        "Enyan Dai",
        "Wei-dong Jin",
        "Hui Liu",
        "Suhang Wang"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) have shown their great ability in modeling graph structured data. However, real-world graphs usually contain structure noises and have limited labeled nodes. The performance of GNNs would drop significantly when trained on such graphs, which hinders the adoption of GNNs on many applications. Thus, it is important to develop noise-resistant GNNs with limited labeled nodes. However, the work on this is rather limited. Therefore, we study a novel problem of developing robust GNNs on noisy graphs with limited labeled nodes. Our analysis shows that both the noisy edges and limited labeled nodes could harm the message-passing mechanism of GNNs. To mitigate these issues, we propose a novel framework which adopts the noisy edges as supervision to learn a denoised and dense graph, which can down-weight or eliminate noisy edges and facilitate message passing of GNNs to alleviate the issue of limited labeled nodes. The generated edges are further used to regularize the predictions of unlabeled nodes with label smoothness to better train GNNs. Experimental results on real-world datasets demonstrate the robustness of the proposed framework on noisy graphs with limited labeled nodes.",
      "file_path": "paper_data/Graph_Neural_Networks/bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf",
      "venue": "Web Search and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels\" \\cite{dai2022xze} for a literature review:\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer significant performance degradation when trained on real-world graphs that contain structural noise (noisy/adversarial edges) and have a limited number of labeled nodes (label sparsity).\n*   **Importance and Challenge:**\n    *   **Noisy Edges:** GNNs' message-passing mechanism makes them vulnerable to noisy or adversarial edges, which can contaminate node representations and propagate errors. Real-world graphs often contain inherent noise (e.g., bots in social networks) or are subject to poisoning attacks.\n    *   **Limited Labeled Nodes:** Many applications have sparsely labeled graphs. Label sparsity reduces the involvement of unlabeled nodes in message passing, making GNNs less effective in leveraging neighborhood information for semi-supervised learning.\n    *   **Combined Challenge:** Developing robust GNNs that can *simultaneously* handle both noisy edges and label sparsity is challenging because the training graph itself is noisy (requiring supervision to identify/mitigate noise) and obtaining more labels is expensive. Existing work primarily addresses these issues in isolation.\n\n### 2. Related Work & Positioning\n\n*   **Existing Approaches & Limitations:**\n    *   **Robust GNNs (against adversarial edges):** Methods like pruning edges based on similarity \\cite{dai2022xze} or using low-rank approximation (e.g., Pro-GNN \\cite{dai2022xze}) exist. However, these often focus *only* on adversarial edges, not label sparsity. Pro-GNN, while learning a clean graph, incurs high computational costs due to direct graph learning and sparse low-rank constraints.\n    *   **GNNs for Sparse Labels:** Approaches using self-supervised learning tasks (e.g., pseudo label prediction, global context predictions) aim to obtain better representations. However, these generally do not address structural noise.\n    *   **Gap:** The paper highlights a significant gap: \"little efforts are taken for robust GNNs that can simultaneously handle noisy edges and label sparsity.\"\n*   **Positioning of this Work:** This work is novel because it studies the *combined problem* of developing robust GNNs for *both* noisy graphs and label sparsity. Unlike prior robust GNNs, it simultaneously tackles both issues by learning a link predictor to down-weight noisy edges and connect similar nodes, and it uses a link predictor instead of direct graph learning to improve computational efficiency.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method (RS-GNN Framework):**\n    1.  **Denoised and Dense Graph Learning:** A novel MLP-based link predictor is employed. It takes node attributes as input to learn a new, dense adjacency matrix (S).\n        *   **Denoising:** The link predictor assigns small weights to links connecting nodes with dissimilar features (likely noisy edges) and large weights to links between similar nodes, effectively down-weighting or eliminating noisy edges.\n        *   **Densifying:** It predicts missing links between similar nodes, thereby densifying the graph. This increases the number of unlabeled nodes involved in message passing, alleviating label sparsity.\n    2.  **Feature Similarity Weighted Edge-Reconstruction Loss:** A novel loss function is designed to train the link predictor, specifically to reduce the negative effects of noisy edges on the link prediction process itself.\n    3.  **GNN Classifier:** A standard GCN classifier (or similar GNN) is trained on the *learned* denoised and densified graph (S) and original node features (X) for node classification.\n    4.  **Label Smoothness Regularization:** The predicted edges from the link predictor are further used to explicitly regularize the predictions of unlabeled nodes. This encourages adjacent nodes in the learned graph to have similar labels, further mitigating the label sparsity issue.\n*   **Novelty/Differentiation:**\n    *   Simultaneously addresses the dual challenges of noisy edges and sparse labels within a single framework.\n    *   Introduces a link predictor that leverages node attributes and noisy edges as supervision to *dynamically learn* a denoised and densified graph, rather than relying on fixed preprocessing or computationally expensive direct graph learning.\n    *   The use of a feature similarity weighted edge-reconstruction loss specifically for training the link predictor in a noisy environment is innovative.\n    *   Integrates the learned graph for both GNN input and as a source for label smoothness regularization.\n\n### 4. Key Technical Contributions\n\n*   **Novel Problem Formulation:** First to study the problem of learning robust noise-resistant GNNs that can handle both noisy graphs and limited labeled nodes simultaneously \\cite{dai2022xze}.\n*   **Novel Framework (RS-GNN):** Proposes a unified framework that concurrently learns a denoised and densified graph and a robust GNN.\n*   **Link Predictor for Graph Reconstruction:** Introduces an MLP-based link predictor that uses node attributes to effectively down-weight/remove noisy edges and complete missing links, which is crucial for both robustness and alleviating label sparsity.\n*   **Feature Similarity Weighted Edge-Reconstruction Loss:** A specific loss function designed to train the link predictor robustly in the presence of noisy edges.\n*   **Label Smoothness Regularization:** Incorporates a regularization term based on the predicted edges to improve predictions for unlabeled nodes, leveraging the learned graph structure.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted:** Extensive experiments were performed on real-world datasets (Citeseer, Cora, Cora-ML) to evaluate the robustness of RS-GNN.\n*   **Key Performance Metrics & Comparison Results:**\n    *   The paper demonstrates the robustness of RS-GNN on *both* noisy and clean graphs, particularly under conditions of limited labeled nodes.\n    *   Preliminary analysis shows that decreasing label rate significantly increases the uninvolved node rate in GNN training, and conversely, increasing graph density significantly reduces this rate, validating the core idea of densification.\n    *   (Specific quantitative results are not provided in the abstract/intro but are implied by \"extensive experiments\" and \"demonstrate the robustness\").\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions:**\n    *   The approach assumes that nodes with similar features tend to be linked and have similar labels, while noisy edges connect dissimilar nodes. This assumption underpins the link predictor's ability to denoise and densify.\n    *   The effectiveness of the MLP-based link predictor relies on the quality and discriminative power of node attributes.\n    *   The paper focuses on semi-supervised node classification, and its direct applicability to other graph tasks (e.g., graph classification, link prediction itself as a primary task) might require adaptation.\n*   **Scope of Applicability:** Primarily applicable to semi-supervised node classification tasks on attributed graphs where structural noise and label sparsity are prevalent, such as social networks, citation networks, and other real-world graph datasets.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art:** RS-GNN significantly advances the state-of-the-art by providing a unified and computationally efficient solution to the critical dual problem of GNN robustness against structural noise and label sparsity. It moves beyond addressing these issues in isolation.\n*   **Potential Impact on Future Research:**\n    *   Opens new avenues for research into joint learning of robust graph structures and GNN models.\n    *   Encourages further exploration of attribute-based link prediction for graph denoising and densification in challenging real-world scenarios.\n    *   Could inspire more sophisticated regularization techniques that leverage learned graph structures to improve GNN performance under data scarcity.\n    *   Its computational efficiency (using a link predictor instead of direct graph learning) makes it a practical baseline for future robust GNN designs.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "noisy graphs",
        "label sparsity",
        "robust GNNs",
        "RS-GNN framework",
        "Denoised and Dense Graph Learning",
        "MLP-based link predictor",
        "Feature Similarity Weighted Edge-Reconstruction Loss",
        "Label Smoothness Regularization",
        "node classification",
        "combined problem",
        "computational efficiency",
        "semi-supervised learning",
        "graph reconstruction"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we propose a novel framework\", \"develop noise-resistant gnns\". it describes a new method/system to address a specific problem (robust gnns on noisy graphs with sparse labels).\n*   **introduction discusses:** the problem (gnns performance drop on noisy graphs with limited labels) and the proposed solution (a framework to learn a denoised and dense graph, regularize predictions). figure 1 also illustrates the proposed approach.\n*   **empirical aspect:** while the abstract mentions \"experimental results on real-world datasets demonstrate the robustness of the proposed framework,\" this is the validation of the *new method*, which is a standard component of a technical paper. the primary contribution is the proposed framework itself, not just a data-driven study of existing phenomena."
    },
    "file_name": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf"
  },
  {
    "success": true,
    "doc_id": "c99b1a246fc0013bef43a636ae692999",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Text Classification is a fundamental Natural Language Processing (NLP) task. While numerous recent models apply sequential deep learning techniques, they often struggle to handle complex relationships between words and documents and cannot efficiently explore contextual-aware word relations \\cite{wang2023wrg}.\n    *   **Importance & Challenge:** Many real-world text classification applications can be naturally cast into a graph structure, which can capture global features across words, documents, and the entire corpus. Graph Neural Networks (GNNs) offer a powerful paradigm to directly deal with such complex structured text data and exploit global information, addressing the limitations of traditional sequential models \\cite{wang2023wrg}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Previous text classification methods include traditional machine learning (e.g., SVM with N-gram/TF-IDF, later dense representations), sequential deep learning (e.g., CNN, RNN, LSTM, GRU), and attention-based models/Transformers (e.g., hierarchical attention, self-attention, large language models) \\cite{wang2023wrg}.\n    *   **Limitations of Previous Solutions:**\n        *   Traditional ML and sequential models often fail to capture complex, non-Euclidean relationships between words and documents, or only capture local dependencies \\cite{wang2023wrg}.\n        *   Attention-based and Transformer models primarily focus on relations *within* input text bodies, often overlooking global and corpus-level information \\cite{wang2023wrg}.\n        *   Existing surveys on GNNs are either general or provide only brief introductions to GNNs in text classification, lacking a comprehensive and critical overview of the field \\cite{wang2023wrg}.\n    *   **Positioning:** This paper is presented as the *first comprehensive survey* specifically focused on GNNs for text classification, covering methods up to 2023. It aims to fill the gap by providing a detailed categorization, comparative analysis, and discussion of challenges and future directions in this rapidly evolving area \\cite{wang2023wrg}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (of the surveyed papers):** The paper surveys GNN-based models that transform text into graph structures to leverage GNNs for classification. The general procedures involve:\n        *   **Graph Construction:** Categorized into Corpus-level graphs (representing the entire corpus, e.g., word-document co-occurrence) and Document-level graphs (representing individual documents, e.g., word-word relations within a text). Further distinctions include Homogeneous/Heterogeneous and Static/Dynamic graphs \\cite{wang2023wrg}.\n        *   **Initial Node Representation:** Utilizes either non-contextual word embeddings (e.g., GloVe, Word2vec, FastText) or contextualized word embeddings (e.g., BERT, ELMo) \\cite{wang2023wrg}.\n        *   **Graph-based Learning Process:** Applies various GNN architectures (e.g., GCN) to propagate information across the constructed graph, updating node representations for classification \\cite{wang2023wrg}.\n    *   **Novelty (of the survey itself):**\n        *   Introduces a novel categorization framework for GNN text classification models, dividing them into corpus-level and document-level graphs, each with multiple sub-categories (e.g., Word and Document nodes, Multi-Graph/Multi-Dimensional Edge, Inductive Learning, Extra Topic Nodes for corpus-level; Local word consecutive, Global Word Co-occurrence, Other Word Graphs for document-level) \\cite{wang2023wrg}.\n        *   Provides a detailed discussion of graph construction mechanisms, initial node representation strategies, and the graph-based learning process across various models \\cite{wang2023wrg}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Categorization and Analysis:** Presents a unique and comprehensive categorization of over twenty GNN text classification models, facilitating structured understanding and comparison \\cite{wang2023wrg}.\n    *   **Comparative Insights:** Offers critical discussions and comparisons of these models based on their graph construction techniques, node embedding initialization methods, and graph learning approaches \\cite{wang2023wrg}.\n    *   **Identification of Challenges and Future Directions:** Systematically discusses existing technical challenges in GNN-based text classification and proposes potential future research avenues, such as dynamic graph learning, inductive learning, and integration with advanced language models \\cite{wang2023wrg}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted (by surveyed papers):** The survey itself does not conduct new experiments. Instead, it synthesizes and reports on the experimental validation performed by the surveyed research papers \\cite{wang2023wrg}.\n    *   **Key Performance Metrics & Comparison Results (reported by the survey):**\n        *   Summarizes commonly used benchmark datasets and evaluation metrics for GNN-based text classification models \\cite{wang2023wrg}.\n        *   Presents a comprehensive summary of published performance results of various GNN models on these publicly available benchmarks, enabling direct comparison of different techniques \\cite{wang2023wrg}.\n        *   Discusses the key findings from these performance comparisons and identifies the pros and cons of various evaluation metrics \\cite{wang2023wrg}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of the survey):** The survey explicitly focuses *only* on GNN-based text classification models and does *not* cover GNN applications for token-level Natural Language Understanding (NLU) tasks such as Named Entity Recognition (NER) or slot filling \\cite{wang2023wrg}.\n    *   **Scope of Applicability:** The survey covers methods published up to 2023, encompassing both corpus-level and document-level GNN architectures. It details graph construction, node representation, graph learning, relevant datasets, evaluation metrics, and experimental design \\cite{wang2023wrg}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the first dedicated, comprehensive, and critical overview of GNNs for text classification. It consolidates fragmented knowledge, offering a structured understanding of the field's progress and methodologies \\cite{wang2023wrg}.\n    *   **Potential Impact on Future Research:** It serves as a crucial reference for researchers, guiding them through the landscape of GNN-based text classification. By identifying current challenges and outlining promising future directions (e.g., dynamic graph learning, inductive learning, multi-graph integration, scalability, interpretability), it is expected to stimulate further innovation and lead to the development of more robust and effective GNN models for complex text understanding tasks \\cite{wang2023wrg}.",
    "intriguing_abstract": "Text classification, a cornerstone of Natural Language Processing (NLP), often struggles to capture the intricate, non-Euclidean relationships between words and documents, and the global contextual information crucial for robust understanding. While sequential and attention-based deep learning models have advanced, they frequently overlook these complex, corpus-level dependencies. Graph Neural Networks (GNNs) offer a powerful paradigm to inherently model text as rich graph structures, directly addressing these limitations. However, a dedicated, comprehensive overview of GNNs for text classification has been conspicuously absent.\n\nThis paper presents the *first comprehensive survey* of GNN-based text classification models up to 2023. We introduce a novel categorization framework, distinguishing between corpus-level and document-level graph constructions, and meticulously analyze diverse graph construction strategies, initial node representations, and GNN architectures. By critically comparing over twenty models, we illuminate their strengths, weaknesses, and performance on benchmark datasets. Furthermore, we identify pressing technical challenges, including dynamic graph learning, inductive capabilities, scalability, and interpretability, charting promising future research directions. This survey serves as an indispensable resource, guiding researchers toward developing more robust and effective GNN solutions for complex text understanding.",
    "keywords": [
      "Text Classification",
      "Graph Neural Networks (GNNs)",
      "Natural Language Processing (NLP)",
      "Comprehensive Survey",
      "Graph Construction",
      "Node Representation",
      "Graph-based Learning",
      "Novel Categorization Framework",
      "Challenges and Future Directions",
      "Corpus-level graphs",
      "Document-level graphs",
      "Word Embeddings",
      "Dynamic Graph Learning",
      "Inductive Learning",
      "Global Features"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf",
    "citation_key": "wang2023wrg",
    "metadata": {
      "title": "Graph Neural Networks for Text Classification: A Survey",
      "authors": [
        "Kunze Wang",
        "Yihao Ding",
        "S. Han"
      ],
      "published_date": "2023",
      "abstract": "Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchmarks. Note that we present a comprehensive comparison between different techniques and identify the pros and cons of various evaluation metrics in this survey.",
      "file_path": "paper_data/Graph_Neural_Networks/03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf",
      "venue": "Artificial Intelligence Review",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Text Classification is a fundamental Natural Language Processing (NLP) task. While numerous recent models apply sequential deep learning techniques, they often struggle to handle complex relationships between words and documents and cannot efficiently explore contextual-aware word relations \\cite{wang2023wrg}.\n    *   **Importance & Challenge:** Many real-world text classification applications can be naturally cast into a graph structure, which can capture global features across words, documents, and the entire corpus. Graph Neural Networks (GNNs) offer a powerful paradigm to directly deal with such complex structured text data and exploit global information, addressing the limitations of traditional sequential models \\cite{wang2023wrg}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Previous text classification methods include traditional machine learning (e.g., SVM with N-gram/TF-IDF, later dense representations), sequential deep learning (e.g., CNN, RNN, LSTM, GRU), and attention-based models/Transformers (e.g., hierarchical attention, self-attention, large language models) \\cite{wang2023wrg}.\n    *   **Limitations of Previous Solutions:**\n        *   Traditional ML and sequential models often fail to capture complex, non-Euclidean relationships between words and documents, or only capture local dependencies \\cite{wang2023wrg}.\n        *   Attention-based and Transformer models primarily focus on relations *within* input text bodies, often overlooking global and corpus-level information \\cite{wang2023wrg}.\n        *   Existing surveys on GNNs are either general or provide only brief introductions to GNNs in text classification, lacking a comprehensive and critical overview of the field \\cite{wang2023wrg}.\n    *   **Positioning:** This paper is presented as the *first comprehensive survey* specifically focused on GNNs for text classification, covering methods up to 2023. It aims to fill the gap by providing a detailed categorization, comparative analysis, and discussion of challenges and future directions in this rapidly evolving area \\cite{wang2023wrg}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (of the surveyed papers):** The paper surveys GNN-based models that transform text into graph structures to leverage GNNs for classification. The general procedures involve:\n        *   **Graph Construction:** Categorized into Corpus-level graphs (representing the entire corpus, e.g., word-document co-occurrence) and Document-level graphs (representing individual documents, e.g., word-word relations within a text). Further distinctions include Homogeneous/Heterogeneous and Static/Dynamic graphs \\cite{wang2023wrg}.\n        *   **Initial Node Representation:** Utilizes either non-contextual word embeddings (e.g., GloVe, Word2vec, FastText) or contextualized word embeddings (e.g., BERT, ELMo) \\cite{wang2023wrg}.\n        *   **Graph-based Learning Process:** Applies various GNN architectures (e.g., GCN) to propagate information across the constructed graph, updating node representations for classification \\cite{wang2023wrg}.\n    *   **Novelty (of the survey itself):**\n        *   Introduces a novel categorization framework for GNN text classification models, dividing them into corpus-level and document-level graphs, each with multiple sub-categories (e.g., Word and Document nodes, Multi-Graph/Multi-Dimensional Edge, Inductive Learning, Extra Topic Nodes for corpus-level; Local word consecutive, Global Word Co-occurrence, Other Word Graphs for document-level) \\cite{wang2023wrg}.\n        *   Provides a detailed discussion of graph construction mechanisms, initial node representation strategies, and the graph-based learning process across various models \\cite{wang2023wrg}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Categorization and Analysis:** Presents a unique and comprehensive categorization of over twenty GNN text classification models, facilitating structured understanding and comparison \\cite{wang2023wrg}.\n    *   **Comparative Insights:** Offers critical discussions and comparisons of these models based on their graph construction techniques, node embedding initialization methods, and graph learning approaches \\cite{wang2023wrg}.\n    *   **Identification of Challenges and Future Directions:** Systematically discusses existing technical challenges in GNN-based text classification and proposes potential future research avenues, such as dynamic graph learning, inductive learning, and integration with advanced language models \\cite{wang2023wrg}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted (by surveyed papers):** The survey itself does not conduct new experiments. Instead, it synthesizes and reports on the experimental validation performed by the surveyed research papers \\cite{wang2023wrg}.\n    *   **Key Performance Metrics & Comparison Results (reported by the survey):**\n        *   Summarizes commonly used benchmark datasets and evaluation metrics for GNN-based text classification models \\cite{wang2023wrg}.\n        *   Presents a comprehensive summary of published performance results of various GNN models on these publicly available benchmarks, enabling direct comparison of different techniques \\cite{wang2023wrg}.\n        *   Discusses the key findings from these performance comparisons and identifies the pros and cons of various evaluation metrics \\cite{wang2023wrg}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of the survey):** The survey explicitly focuses *only* on GNN-based text classification models and does *not* cover GNN applications for token-level Natural Language Understanding (NLU) tasks such as Named Entity Recognition (NER) or slot filling \\cite{wang2023wrg}.\n    *   **Scope of Applicability:** The survey covers methods published up to 2023, encompassing both corpus-level and document-level GNN architectures. It details graph construction, node representation, graph learning, relevant datasets, evaluation metrics, and experimental design \\cite{wang2023wrg}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the first dedicated, comprehensive, and critical overview of GNNs for text classification. It consolidates fragmented knowledge, offering a structured understanding of the field's progress and methodologies \\cite{wang2023wrg}.\n    *   **Potential Impact on Future Research:** It serves as a crucial reference for researchers, guiding them through the landscape of GNN-based text classification. By identifying current challenges and outlining promising future directions (e.g., dynamic graph learning, inductive learning, multi-graph integration, scalability, interpretability), it is expected to stimulate further innovation and lead to the development of more robust and effective GNN models for complex text understanding tasks \\cite{wang2023wrg}.",
      "keywords": [
        "Text Classification",
        "Graph Neural Networks (GNNs)",
        "Natural Language Processing (NLP)",
        "Comprehensive Survey",
        "Graph Construction",
        "Node Representation",
        "Graph-based Learning",
        "Novel Categorization Framework",
        "Challenges and Future Directions",
        "Corpus-level graphs",
        "Document-level graphs",
        "Word Embeddings",
        "Dynamic Graph Learning",
        "Inductive Learning",
        "Global Features"
      ],
      "paper_type": "the paper type is: **survey**\n\n**reasoning:**\n\n1.  **title:** the title explicitly states \"graph neural networks for text classification: a **survey**\". this is the strongest indicator.\n2.  **venue:** \"artificial intelligence **review**\" also strongly suggests a review or survey paper.\n3.  **abstract content:** although incomplete, the abstract snippet discusses \"commonly used text classification datasets by gnn-based models\" and lists various datasets (ohsumed, r8/r52, 20ng) along with references to papers that used them. this is characteristic of a survey paper that summarizes the landscape of research, including common benchmarks.\n4.  **introduction content:** the introduction provides background on text classification, discusses \"traditional text classification methods,\" \"deep learning models,\" and then introduces \"graph neural networks (gnn)\" as a solution to certain obstacles. it then mentions \"two main approaches to constructing graphs, corpus-level graphs and document-level graphs.\" this structure of reviewing existing methods, identifying challenges, and then discussing different approaches within a specific solution (gnns) is typical for a comprehensive literature review or survey."
    },
    "file_name": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf"
  },
  {
    "success": true,
    "doc_id": "7d5f51afcbab5e44fe3b9ca8246e3654",
    "summary": "This paper, \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\" by Khemani et al. \\cite{khemani2024i8r}, provides a comprehensive overview of the rapidly evolving field of Graph Neural Networks (GNNs).\n\nHere's a focused summary for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively processing and learning from graph-structured data, which is prevalent in various domains (e.g., social networks, biology, recommendation systems, cybersecurity). Traditional deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are ill-suited for non-Euclidean graph data due to its irregular structure and complex relational dependencies \\cite{khemani2024i8r}.\n    *   **Importance and Challenge**: The problem is important because interconnected data is ubiquitous, and understanding the intricate structural information (dependencies, connections, contextual relationships) within graphs is crucial for informed predictions and decisions. The challenge lies in developing models that can capture these complex relationships without relying on fixed grid-like or sequential data structures \\cite{khemani2024i8r}. The paper's *own* problem is to synthesize and organize the vast and growing literature on GNNs to provide a holistic understanding for researchers, practitioners, and students.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: As a review paper, this work consolidates and categorizes existing GNN approaches. It traces the evolution of GNNs from their inception in 2005, highlighting key milestones and influential models like Graph Convolutional Networks (GCNs), GraphSAGE, and Graph Attention Networks (GATs) \\cite{khemani2024i8r}.\n    *   **Limitations of Previous Solutions (addressed by GNNs)**: The paper explicitly discusses the limitations of CNNs and RNNs for graph data. CNNs are designed for grid-like data (e.g., images) and cannot handle the irregular structure of graphs, while RNNs are tailored for sequential data (e.g., text) and fail to capture complex non-sequential relationships in graphs. GNNs overcome these limitations by adapting convolutional operations to aggregate information from neighboring nodes \\cite{khemani2024i8r}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (of GNNs reviewed)**: The paper details the fundamental concepts of GNNs, emphasizing the \"message-passing mechanism\" as the core technique. This mechanism allows nodes to iteratively aggregate information from their neighbors, enabling them to learn rich, context-aware representations that encapsulate neighborhood data \\cite{khemani2024i8r}. It reviews specific GNN architectures such as GCNs (which adapt spectral filters to graphs), GraphSAGE (which learns inductive embeddings), and GATs (which use attention mechanisms to weigh neighbor contributions).\n    *   **Novelty of This Paper's Approach**: The paper's innovation lies in its *comprehensive and structured review methodology*. It systematically covers GNN concepts, architectural designs, training techniques, prevalent challenges, diverse datasets, practical applications, and future research directions, aiming to serve as a foundational resource for the field \\cite{khemani2024i8r}.\n\n4.  **Key Technical Contributions (of GNNs, as reviewed by the paper)**\n    *   **Novel Algorithms/Methods**: The paper highlights the message-passing paradigm as a foundational technique for GNNs, enabling them to learn from graph topology and node features. It reviews various GNN models that implement this paradigm with different aggregation and update functions, such as GCNs, GraphSAGE, and GATs \\cite{khemani2024i8r}.\n    *   **System Design/Architectural Innovations**: It discusses how GNN architectures extend deep learning principles to non-Euclidean data, allowing for the capture of complex relational information. This includes adaptations of convolutional and attention mechanisms for graph structures \\cite{khemani2024i8r}.\n    *   **Theoretical Insights**: The paper implicitly touches upon the theoretical underpinnings of GNNs by explaining how they learn state embeddings for nodes that encapsulate neighborhood data, enabling tasks like node classification, link prediction, and clustering \\cite{khemani2024i8r}.\n\n5.  **Experimental Validation (as summarized from reviewed literature)**\n    *   **Experiments Conducted**: The paper does not conduct its own experiments. Instead, it provides an extensive literature review (e.g., Table 1 and Table 2) summarizing experimental validations from numerous GNN research papers published between 2018 and 2023 \\cite{khemani2024i8r}. These reviewed experiments cover a wide range of applications.\n    *   **Key Performance Metrics and Comparison Results**: The summarized results include metrics such as Area Under Curve (AUC) for link prediction, Root Mean Square Error (RMSE) for recommendation systems and time-series forecasting, and accuracy for node/text classification and named entity recognition \\cite{khemani2024i8r}. For example, GNNs achieved high AUCs (e.g., 97.09% for Usair dataset) in link prediction, and GCNs showed strong accuracy (e.g., 82.98% on CORA) in node classification \\cite{khemani2024i8r}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of GNNs, as discussed)**: The paper explores the \"prevalent challenges and limitations\" of GNNs, which can include issues like over-smoothing (nodes becoming indistinguishable after many layers), scalability to very large graphs, and handling dynamic graphs \\cite{khemani2024i8r}.\n    *   **Scope of Applicability**: The review's scope is extensive, covering GNNs' applicability across diverse domains including Natural Language Processing (NLP), computer vision, healthcare, social networks, recommendation systems, and traffic prediction \\cite{khemani2024i8r}. It also covers various GNN learning styles (node-focused, edge-focused, graph-focused tasks) and the Python libraries supporting GNN models.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This review significantly advances the technical state-of-the-art by consolidating and structuring the vast and rapidly expanding knowledge base of GNNs. It provides a critical analysis of existing models, techniques, and applications, making complex information accessible \\cite{khemani2024i8r}.\n    *   **Potential Impact on Future Research**: The paper identifies specific research gaps and outlines \"intriguing future directions\" for GNN research. By offering a holistic understanding and highlighting challenges, it serves as an invaluable resource to guide future investigations into more robust, scalable, and versatile GNN models, fostering innovation in graph-based machine learning \\cite{khemani2024i8r}.",
    "intriguing_abstract": "The ubiquitous nature of graph-structured data presents a formidable challenge for traditional deep learning models, which struggle with its irregular, non-Euclidean topology. Graph Neural Networks (GNNs) have emerged as a transformative paradigm, uniquely capable of learning intricate relational dependencies through their powerful message-passing mechanism. This comprehensive review, \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions,\" synthesizes the rapidly evolving GNN landscape.\n\nWe delve into fundamental concepts, explore influential architectures like Graph Convolutional Networks (GCNs), GraphSAGE, and Graph Attention Networks (GATs), and dissect prevalent challenges such as over-smoothing and scalability. By systematically mapping diverse applications—from social network analysis and recommendation systems to drug discovery and cybersecurity—and outlining critical future directions, this paper serves as an indispensable resource. It aims to equip researchers and practitioners with a holistic understanding, fostering innovation and guiding the development of more robust and impactful graph-based machine learning solutions.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "graph-structured data",
      "message-passing mechanism",
      "GNN architectures",
      "non-Euclidean data processing",
      "node classification",
      "link prediction",
      "recommendation systems",
      "social networks",
      "over-smoothing",
      "scalability",
      "comprehensive literature review",
      "future research directions"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/1fad14bcfc2b75797c686a5a05779076437a683e.pdf",
    "citation_key": "khemani2024i8r",
    "metadata": {
      "title": "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions",
      "authors": [
        "Bharti Khemani",
        "S. Patil",
        "K. Kotecha",
        "Sudeep Tanwar"
      ],
      "published_date": "2024",
      "abstract": "Deep learning has seen significant growth recently and is now applied to a wide range of conventional use cases, including graphs. Graph data provides relational information between elements and is a standard data format for various machine learning and deep learning tasks. Models that can learn from such inputs are essential for working with graph data effectively. This paper identifies nodes and edges within specific applications, such as text, entities, and relations, to create graph structures. Different applications may require various graph neural network (GNN) models. GNNs facilitate the exchange of information between nodes in a graph, enabling them to understand dependencies within the nodes and edges. The paper delves into specific GNN models like graph convolution networks (GCNs), GraphSAGE, and graph attention networks (GATs), which are widely used in various applications today. It also discusses the message-passing mechanism employed by GNN models and examines the strengths and limitations of these models in different domains. Furthermore, the paper explores the diverse applications of GNNs, the datasets commonly used with them, and the Python libraries that support GNN models. It offers an extensive overview of the landscape of GNN research and its practical implementations.",
      "file_path": "paper_data/Graph_Neural_Networks/1fad14bcfc2b75797c686a5a05779076437a683e.pdf",
      "venue": "Journal of Big Data",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\" by Khemani et al. \\cite{khemani2024i8r}, provides a comprehensive overview of the rapidly evolving field of Graph Neural Networks (GNNs).\n\nHere's a focused summary for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively processing and learning from graph-structured data, which is prevalent in various domains (e.g., social networks, biology, recommendation systems, cybersecurity). Traditional deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are ill-suited for non-Euclidean graph data due to its irregular structure and complex relational dependencies \\cite{khemani2024i8r}.\n    *   **Importance and Challenge**: The problem is important because interconnected data is ubiquitous, and understanding the intricate structural information (dependencies, connections, contextual relationships) within graphs is crucial for informed predictions and decisions. The challenge lies in developing models that can capture these complex relationships without relying on fixed grid-like or sequential data structures \\cite{khemani2024i8r}. The paper's *own* problem is to synthesize and organize the vast and growing literature on GNNs to provide a holistic understanding for researchers, practitioners, and students.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: As a review paper, this work consolidates and categorizes existing GNN approaches. It traces the evolution of GNNs from their inception in 2005, highlighting key milestones and influential models like Graph Convolutional Networks (GCNs), GraphSAGE, and Graph Attention Networks (GATs) \\cite{khemani2024i8r}.\n    *   **Limitations of Previous Solutions (addressed by GNNs)**: The paper explicitly discusses the limitations of CNNs and RNNs for graph data. CNNs are designed for grid-like data (e.g., images) and cannot handle the irregular structure of graphs, while RNNs are tailored for sequential data (e.g., text) and fail to capture complex non-sequential relationships in graphs. GNNs overcome these limitations by adapting convolutional operations to aggregate information from neighboring nodes \\cite{khemani2024i8r}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (of GNNs reviewed)**: The paper details the fundamental concepts of GNNs, emphasizing the \"message-passing mechanism\" as the core technique. This mechanism allows nodes to iteratively aggregate information from their neighbors, enabling them to learn rich, context-aware representations that encapsulate neighborhood data \\cite{khemani2024i8r}. It reviews specific GNN architectures such as GCNs (which adapt spectral filters to graphs), GraphSAGE (which learns inductive embeddings), and GATs (which use attention mechanisms to weigh neighbor contributions).\n    *   **Novelty of This Paper's Approach**: The paper's innovation lies in its *comprehensive and structured review methodology*. It systematically covers GNN concepts, architectural designs, training techniques, prevalent challenges, diverse datasets, practical applications, and future research directions, aiming to serve as a foundational resource for the field \\cite{khemani2024i8r}.\n\n4.  **Key Technical Contributions (of GNNs, as reviewed by the paper)**\n    *   **Novel Algorithms/Methods**: The paper highlights the message-passing paradigm as a foundational technique for GNNs, enabling them to learn from graph topology and node features. It reviews various GNN models that implement this paradigm with different aggregation and update functions, such as GCNs, GraphSAGE, and GATs \\cite{khemani2024i8r}.\n    *   **System Design/Architectural Innovations**: It discusses how GNN architectures extend deep learning principles to non-Euclidean data, allowing for the capture of complex relational information. This includes adaptations of convolutional and attention mechanisms for graph structures \\cite{khemani2024i8r}.\n    *   **Theoretical Insights**: The paper implicitly touches upon the theoretical underpinnings of GNNs by explaining how they learn state embeddings for nodes that encapsulate neighborhood data, enabling tasks like node classification, link prediction, and clustering \\cite{khemani2024i8r}.\n\n5.  **Experimental Validation (as summarized from reviewed literature)**\n    *   **Experiments Conducted**: The paper does not conduct its own experiments. Instead, it provides an extensive literature review (e.g., Table 1 and Table 2) summarizing experimental validations from numerous GNN research papers published between 2018 and 2023 \\cite{khemani2024i8r}. These reviewed experiments cover a wide range of applications.\n    *   **Key Performance Metrics and Comparison Results**: The summarized results include metrics such as Area Under Curve (AUC) for link prediction, Root Mean Square Error (RMSE) for recommendation systems and time-series forecasting, and accuracy for node/text classification and named entity recognition \\cite{khemani2024i8r}. For example, GNNs achieved high AUCs (e.g., 97.09% for Usair dataset) in link prediction, and GCNs showed strong accuracy (e.g., 82.98% on CORA) in node classification \\cite{khemani2024i8r}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of GNNs, as discussed)**: The paper explores the \"prevalent challenges and limitations\" of GNNs, which can include issues like over-smoothing (nodes becoming indistinguishable after many layers), scalability to very large graphs, and handling dynamic graphs \\cite{khemani2024i8r}.\n    *   **Scope of Applicability**: The review's scope is extensive, covering GNNs' applicability across diverse domains including Natural Language Processing (NLP), computer vision, healthcare, social networks, recommendation systems, and traffic prediction \\cite{khemani2024i8r}. It also covers various GNN learning styles (node-focused, edge-focused, graph-focused tasks) and the Python libraries supporting GNN models.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This review significantly advances the technical state-of-the-art by consolidating and structuring the vast and rapidly expanding knowledge base of GNNs. It provides a critical analysis of existing models, techniques, and applications, making complex information accessible \\cite{khemani2024i8r}.\n    *   **Potential Impact on Future Research**: The paper identifies specific research gaps and outlines \"intriguing future directions\" for GNN research. By offering a holistic understanding and highlighting challenges, it serves as an invaluable resource to guide future investigations into more robust, scalable, and versatile GNN models, fostering innovation in graph-based machine learning \\cite{khemani2024i8r}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "graph-structured data",
        "message-passing mechanism",
        "GNN architectures",
        "non-Euclidean data processing",
        "node classification",
        "link prediction",
        "recommendation systems",
        "social networks",
        "over-smoothing",
        "scalability",
        "comprehensive literature review",
        "future research directions"
      ],
      "paper_type": "based on the provided abstract and introduction, the paper clearly fits the **survey** type.\n\nhere's why:\n\n*   **title:** \"a **review** of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\" - the word \"review\" is a direct indicator.\n*   **abstract:**\n    *   \"the paper delves into specific gnn models like graph convolution networks (gcns), graphsage, and graph attention networks (gats)...\" - discussing existing models.\n    *   \"it also discusses the message-passing mechanism employed by gnn models and examines the strengths and limitations of these models...\" - analyzing existing work.\n    *   \"furthermore, the paper explores the diverse applications of gnns, the datasets commonly used with them, and the python libraries that support gnn models.\" - comprehensive coverage of the field.\n    *   \"it offers an **extensive overview of the landscape of gnn research** and its practical implementations.\" - this phrase perfectly matches the definition of a survey.\n*   **introduction:** sets the stage for understanding the importance and evolution of gnns, which is typical for a comprehensive review paper.\n\nthe paper aims to review and synthesize existing knowledge, rather than proposing new methods (technical), proving theorems (theoretical), conducting new experiments (empirical), focusing on a single application in detail (case_study), arguing a specific viewpoint (position), or being a brief communication (short, especially given it's 43 pages)."
    },
    "file_name": "1fad14bcfc2b75797c686a5a05779076437a683e.pdf"
  },
  {
    "success": true,
    "doc_id": "f7ffd0b0c758b0f714424780e66da30a",
    "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **CITATION**: \\cite{agarwal2022xfp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical challenge of reliably evaluating the quality and trustworthiness of explanations generated by Graph Neural Networks (GNNs).\n    *   **Importance & Challenge**: GNNs are increasingly used in high-stakes applications (e.g., criminal justice, molecular chemistry), making understanding and trusting their predictions crucial. However, evaluating GNN explanations is difficult because existing graph datasets either lack ground-truth explanations or provide unreliable ones. Previous approaches are prone to pitfalls like redundant explanations, weak GNN predictors, or trivially correct explanations, and lack diversity to represent real-world scenarios. Existing graph deep learning libraries primarily benchmark GNN *predictors* and are not designed for evaluating GNN *explainers*.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself against existing benchmark datasets (e.g., Open Graph Benchmark (OGB), GNNMark) and programming libraries (e.g., PyTorch Geometric (PyG), Deep Graph Library (DGL)).\n    *   **Limitations of Previous Solutions**: These existing resources are mainly used to benchmark the performance of GNN *predictors* and are not suited to evaluate the correctness of GNN *explainers* because they do not capture ground-truth explanations. They also suffer from pitfalls such as redundant or non-unique explanations, weak GNN predictors, and ground-truth explanations recoverable by trivial baselines, making them unreliable for assessing post-hoc explanations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces two main components:\n        *   **SHAPEGG EN (Synthetic Graph Generator)**: A novel, flexible, synthetic XAI-ready dataset generator that automatically creates diverse benchmark datasets (varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by reliable ground-truth explanations. It is designed to avoid the pitfalls of existing ground-truth explanations.\n        *   **GRAPHXAI (Graph Explainability Library)**: A general-purpose framework that incorporates SHAPEGG EN, several real-world graph datasets, data loaders, data processing functions, visualizers, GNN model implementations, and a comprehensive suite of evaluation metrics.\n    *   **Novelty/Difference**:\n        *   **Reliable Ground-Truth Generation**: SHAPEGG EN's ability to generate diverse synthetic datasets with *guaranteed reliable ground-truth explanations* that are not prone to common pitfalls (redundancy, triviality, weak GNN predictors) is a key innovation.\n        *   **Comprehensive Evaluation Ecosystem**: GRAPHXAI provides a holistic environment for benchmarking GNN explainers, integrating data generation, diverse datasets (synthetic and real-world), GNN models, and a novel set of evaluation metrics specifically tailored for GNN explanations.\n        *   **Mimicking Real-World Data**: SHAPEGG EN's flexibility allows it to mimic properties of real-world data across various domains, enhancing the generalizability of evaluations.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **SHAPEGG EN**: A synthetic graph data generator capable of producing diverse graph datasets with robust, reliable ground-truth explanations for GNN explainability evaluation.\n        *   **Comprehensive Evaluation Metrics**: Introduction of four broad categories of GNN explanation evaluation metrics: Graph Explanation Accuracy (GEA), Graph Explanation Faithfulness (GEF), Graph Explanation Stability (GES), and Graph Explanation Fairness (GECF, GEGF), specifically adapted for graph explanations and addressing issues like multiple ground-truth explanations.\n    *   **System Design/Architectural Innovations**:\n        *   **GRAPHXAI Library**: A unified, open-source framework that integrates data generation, diverse datasets, GNN models, and evaluation tools, providing a standardized platform for GNN explainability research.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors systematically benchmarked eight state-of-the-art GNN explainers (including gradient-based, perturbation-based, and surrogate-based methods) against random baselines. Experiments were conducted on both SHAPEGG EN-generated synthetic datasets (with varying properties like homophily/heterophily, explanation size, fairness properties, and informative node features) and real-world molecular datasets (MUTAG, Benzene, Fluoride Carbonyl).\n    *   **Key Performance Metrics**:\n        *   **Graph Explanation Accuracy (GEA)**: Jaccard index between predicted and ground-truth explanations, accounting for multiple ground truths.\n        *   **Graph Explanation Faithfulness (GEF)**: KL divergence between GNN predictions on original and explanation-masked subgraphs (higher values indicate unfaithfulness).\n        *   **Graph Explanation Stability (GES)**: Cosine distance between explanations for original and infinitesimally perturbed graphs (higher values indicate instability).\n        *   **Graph Explanation Fairness (GECF, GEGF)**: Counterfactual fairness mismatch (cosine distance between explanations for original and counterfactual graphs) and group fairness mismatch (statistical parity difference).\n    *   **Comparison Results**:\n        *   No single explainer performed well across all properties.\n        *   **SubgraphX** generally outperformed other methods on SHAPEGG EN node classification datasets in terms of accuracy and faithfulness.\n        *   **PGExplainer** generated the least unstable explanations.\n        *   **Node explanation masks** were found to be more reliable than edge- and node feature masks.\n        *   State-of-the-art explainers showed better faithfulness on synthetic graphs compared to real-world graphs.\n        *   **Significant Gaps Identified**: Existing GNN explainers performed poorly on:\n            *   **Heterophilic graphs**: 55.98% less faithful than on homophilic graphs.\n            *   **Graphs with large ground-truth explanations**: 59.98% less faithful for 'house' (larger) explanations compared to 'triangle' (smaller) ones.\n            *   **Fairness properties**: Explainers generally failed to preserve counterfactual fairness, producing unfair explanations even for weakly-unfair ground truths, and failing to capture unfairness in strongly-unfair scenarios.\n            *   Varying degrees of informative node features showed minimal differences between explainers.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of existing explainers, identified by the framework)**: The paper highlights that current state-of-the-art GNN explainers have significant limitations, particularly in handling diverse graph types (heterophilic), large ground-truth explanations, and preserving fairness properties.\n    *   **Scope of Applicability**: GRAPHXAI and SHAPEGG EN are designed for general-purpose evaluation of post-hoc GNN explanations across diverse real-world applications, supporting various explanation types (node feature-based, node-based, edge-based). The framework itself is broad and aims to be a comprehensive resource.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing the first comprehensive, standardized, and reliable framework (GRAPHXAI) and data generator (SHAPEGG EN) for evaluating GNN explainability. It moves beyond ad-hoc evaluations to systematic benchmarking.\n    *   **Potential Impact on Future Research**:\n        *   **Enabling Robust Evaluation**: Provides researchers with essential tools and datasets to rigorously test and compare new GNN explainability methods, fostering more reliable and trustworthy GNN explanations.\n        *   **Identifying Research Gaps**: The empirical findings clearly expose critical weaknesses in existing GNN explainers (e.g., performance on heterophilic graphs, large explanations, fairness), thereby guiding future algorithmic innovation and research directions in GNN explainability.\n        *   **Promoting Fairness in XAI**: Emphasizes the importance of studying fairness in XAI, which can enhance user confidence and aid in detecting/correcting biases in GNN models.",
    "intriguing_abstract": "As Graph Neural Networks (GNNs) increasingly power high-stakes decisions, from drug discovery to criminal justice, the imperative for trustworthy and interpretable predictions has never been greater. Yet, reliably evaluating the quality of GNN explanations remains a critical bottleneck, plagued by unreliable ground-truth and a lack of diverse benchmarks. We introduce **GRAPHXAI**, a pioneering open-source library, and **SHAPEGG EN**, a novel synthetic graph generator, to revolutionize GNN explainability benchmarking.\n\nSHAPEGG EN uniquely creates diverse, XAI-ready datasets with *guaranteed reliable ground-truth explanations*, meticulously designed to avoid common pitfalls. GRAPHXAI integrates this generator with real-world datasets, GNN models, and a comprehensive suite of novel evaluation metrics, including crucial dimensions of **fairness**, **faithfulness**, and **stability**. Our systematic benchmarking of eight state-of-the-art GNN explainers reveals startling limitations: existing methods struggle profoundly with **heterophilic graphs**, large explanations, and consistently fail to preserve or detect **fairness** properties. This work not only provides the first standardized, robust framework for GNN explanation evaluation but also uncovers critical research gaps, paving the way for the development of truly reliable, fair, and transparent GNNs. GRAPHXAI is an indispensable tool for advancing the next generation of explainable AI.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN explainability",
      "Ground-truth explanations",
      "SHAPEGG EN (Synthetic Graph Generator)",
      "GRAPHXAI (Graph Explainability Library)",
      "Synthetic graph generation",
      "Graph Explanation Accuracy",
      "Graph Explanation Faithfulness",
      "Graph Explanation Fairness",
      "Benchmarking GNN explainers",
      "Heterophilic graphs",
      "Limitations of GNN explainers",
      "SubgraphX"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf",
    "citation_key": "agarwal2022xfp",
    "metadata": {
      "title": "Evaluating explainability for graph neural networks",
      "authors": [
        "Chirag Agarwal",
        "Owen Queen",
        "Himabindu Lakkaraju",
        "M. Zitnik"
      ],
      "published_date": "2022",
      "abstract": "As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, Shape GG en , which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows Shape GG en to mimic the data in various real-world areas. We include Shape GG en and several real-world graph datasets in a graph explainability library, G raph XAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, G raph XAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.",
      "file_path": "paper_data/Graph_Neural_Networks/f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf",
      "venue": "Scientific Data",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **CITATION**: \\cite{agarwal2022xfp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical challenge of reliably evaluating the quality and trustworthiness of explanations generated by Graph Neural Networks (GNNs).\n    *   **Importance & Challenge**: GNNs are increasingly used in high-stakes applications (e.g., criminal justice, molecular chemistry), making understanding and trusting their predictions crucial. However, evaluating GNN explanations is difficult because existing graph datasets either lack ground-truth explanations or provide unreliable ones. Previous approaches are prone to pitfalls like redundant explanations, weak GNN predictors, or trivially correct explanations, and lack diversity to represent real-world scenarios. Existing graph deep learning libraries primarily benchmark GNN *predictors* and are not designed for evaluating GNN *explainers*.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself against existing benchmark datasets (e.g., Open Graph Benchmark (OGB), GNNMark) and programming libraries (e.g., PyTorch Geometric (PyG), Deep Graph Library (DGL)).\n    *   **Limitations of Previous Solutions**: These existing resources are mainly used to benchmark the performance of GNN *predictors* and are not suited to evaluate the correctness of GNN *explainers* because they do not capture ground-truth explanations. They also suffer from pitfalls such as redundant or non-unique explanations, weak GNN predictors, and ground-truth explanations recoverable by trivial baselines, making them unreliable for assessing post-hoc explanations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces two main components:\n        *   **SHAPEGG EN (Synthetic Graph Generator)**: A novel, flexible, synthetic XAI-ready dataset generator that automatically creates diverse benchmark datasets (varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by reliable ground-truth explanations. It is designed to avoid the pitfalls of existing ground-truth explanations.\n        *   **GRAPHXAI (Graph Explainability Library)**: A general-purpose framework that incorporates SHAPEGG EN, several real-world graph datasets, data loaders, data processing functions, visualizers, GNN model implementations, and a comprehensive suite of evaluation metrics.\n    *   **Novelty/Difference**:\n        *   **Reliable Ground-Truth Generation**: SHAPEGG EN's ability to generate diverse synthetic datasets with *guaranteed reliable ground-truth explanations* that are not prone to common pitfalls (redundancy, triviality, weak GNN predictors) is a key innovation.\n        *   **Comprehensive Evaluation Ecosystem**: GRAPHXAI provides a holistic environment for benchmarking GNN explainers, integrating data generation, diverse datasets (synthetic and real-world), GNN models, and a novel set of evaluation metrics specifically tailored for GNN explanations.\n        *   **Mimicking Real-World Data**: SHAPEGG EN's flexibility allows it to mimic properties of real-world data across various domains, enhancing the generalizability of evaluations.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **SHAPEGG EN**: A synthetic graph data generator capable of producing diverse graph datasets with robust, reliable ground-truth explanations for GNN explainability evaluation.\n        *   **Comprehensive Evaluation Metrics**: Introduction of four broad categories of GNN explanation evaluation metrics: Graph Explanation Accuracy (GEA), Graph Explanation Faithfulness (GEF), Graph Explanation Stability (GES), and Graph Explanation Fairness (GECF, GEGF), specifically adapted for graph explanations and addressing issues like multiple ground-truth explanations.\n    *   **System Design/Architectural Innovations**:\n        *   **GRAPHXAI Library**: A unified, open-source framework that integrates data generation, diverse datasets, GNN models, and evaluation tools, providing a standardized platform for GNN explainability research.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors systematically benchmarked eight state-of-the-art GNN explainers (including gradient-based, perturbation-based, and surrogate-based methods) against random baselines. Experiments were conducted on both SHAPEGG EN-generated synthetic datasets (with varying properties like homophily/heterophily, explanation size, fairness properties, and informative node features) and real-world molecular datasets (MUTAG, Benzene, Fluoride Carbonyl).\n    *   **Key Performance Metrics**:\n        *   **Graph Explanation Accuracy (GEA)**: Jaccard index between predicted and ground-truth explanations, accounting for multiple ground truths.\n        *   **Graph Explanation Faithfulness (GEF)**: KL divergence between GNN predictions on original and explanation-masked subgraphs (higher values indicate unfaithfulness).\n        *   **Graph Explanation Stability (GES)**: Cosine distance between explanations for original and infinitesimally perturbed graphs (higher values indicate instability).\n        *   **Graph Explanation Fairness (GECF, GEGF)**: Counterfactual fairness mismatch (cosine distance between explanations for original and counterfactual graphs) and group fairness mismatch (statistical parity difference).\n    *   **Comparison Results**:\n        *   No single explainer performed well across all properties.\n        *   **SubgraphX** generally outperformed other methods on SHAPEGG EN node classification datasets in terms of accuracy and faithfulness.\n        *   **PGExplainer** generated the least unstable explanations.\n        *   **Node explanation masks** were found to be more reliable than edge- and node feature masks.\n        *   State-of-the-art explainers showed better faithfulness on synthetic graphs compared to real-world graphs.\n        *   **Significant Gaps Identified**: Existing GNN explainers performed poorly on:\n            *   **Heterophilic graphs**: 55.98% less faithful than on homophilic graphs.\n            *   **Graphs with large ground-truth explanations**: 59.98% less faithful for 'house' (larger) explanations compared to 'triangle' (smaller) ones.\n            *   **Fairness properties**: Explainers generally failed to preserve counterfactual fairness, producing unfair explanations even for weakly-unfair ground truths, and failing to capture unfairness in strongly-unfair scenarios.\n            *   Varying degrees of informative node features showed minimal differences between explainers.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of existing explainers, identified by the framework)**: The paper highlights that current state-of-the-art GNN explainers have significant limitations, particularly in handling diverse graph types (heterophilic), large ground-truth explanations, and preserving fairness properties.\n    *   **Scope of Applicability**: GRAPHXAI and SHAPEGG EN are designed for general-purpose evaluation of post-hoc GNN explanations across diverse real-world applications, supporting various explanation types (node feature-based, node-based, edge-based). The framework itself is broad and aims to be a comprehensive resource.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing the first comprehensive, standardized, and reliable framework (GRAPHXAI) and data generator (SHAPEGG EN) for evaluating GNN explainability. It moves beyond ad-hoc evaluations to systematic benchmarking.\n    *   **Potential Impact on Future Research**:\n        *   **Enabling Robust Evaluation**: Provides researchers with essential tools and datasets to rigorously test and compare new GNN explainability methods, fostering more reliable and trustworthy GNN explanations.\n        *   **Identifying Research Gaps**: The empirical findings clearly expose critical weaknesses in existing GNN explainers (e.g., performance on heterophilic graphs, large explanations, fairness), thereby guiding future algorithmic innovation and research directions in GNN explainability.\n        *   **Promoting Fairness in XAI**: Emphasizes the importance of studying fairness in XAI, which can enhance user confidence and aid in detecting/correcting biases in GNN models.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN explainability",
        "Ground-truth explanations",
        "SHAPEGG EN (Synthetic Graph Generator)",
        "GRAPHXAI (Graph Explainability Library)",
        "Synthetic graph generation",
        "Graph Explanation Accuracy",
        "Graph Explanation Faithfulness",
        "Graph Explanation Fairness",
        "Benchmarking GNN explainers",
        "Heterophilic graphs",
        "Limitations of GNN explainers",
        "SubgraphX"
      ],
      "paper_type": "the paper's abstract and introduction strongly indicate a **technical** classification.\n\nhere's why:\n\n1.  **problem identification and proposed solution:** the introduction clearly identifies a technical problem: \"explainability in graph machine learning is an emerging area lacking standardized evaluation strategies and reliable data resources to evaluate, test, and compare gnn explanations.\" it then proposes a concrete solution: \"developing a broader ecosystem of data resources for benchmarking state-of-the-art gnn explainers.\"\n2.  **\"develop\" and \"present\" implied:** the phrase \"developing a broader ecosystem of data resources\" suggests the paper is either describing the development process or presenting the developed resources themselves. this aligns with the \"technical\" criterion of \"propose,\" \"develop,\" or \"present\" new systems or methods.\n3.  **venue \"scientific data\":** this journal specifically publishes descriptions of scientifically valuable datasets and data resources. this strongly supports the interpretation that the paper is presenting a new \"system\" in the form of data resources for benchmarking.\n4.  **focus on resources for evaluation:** while the paper is about \"evaluating explainability,\" its core contribution, as described in the introduction, is to provide the *means* for evaluation (standardized strategies and data resources), which is a technical contribution.\n\nwhile it does argue for a need (which could lean towards \"position\"), the emphasis on \"developing a broader ecosystem of data resources for benchmarking\" and the venue \"scientific data\" point to the actual presentation and description of these resources, making it a technical paper."
    },
    "file_name": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf"
  },
  {
    "success": true,
    "doc_id": "98e42cfea8c909d228751986596e7130",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{dwivedi20239ab}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: The rapid growth of Graph Neural Networks (GNNs) has led to numerous promising techniques, but a lack of community-standard benchmarks makes it challenging to track progress, fairly compare models, and identify truly impactful architectural advancements.\n*   **Importance & Challenge**:\n    *   Existing evaluations often use small, non-discriminative datasets (e.g., Cora, Citeseer, TU) that fail to differentiate complex from simple or graph-agnostic architectures.\n    *   Inconsistent experimental settings and varying parameter budgets across studies make fair comparisons difficult, hindering the identification of universal, generalizable, and scalable GNN principles.\n    *   Designing effective benchmarks is challenging, requiring a rigorous experimental setting, reproducibility, and appropriate datasets that can statistically separate model performance.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**: While GNNs have been extensively developed across various domains (chemistry, physics, social sciences, etc.), the paper positions itself as addressing a critical gap in the *evaluation* and *benchmarking* of these models. Previous GNN research focused on developing new architectures (message-passing, attention-based, theoretically expressive GNNs) but lacked a standardized framework for their rigorous comparison.\n*   **Limitations of Previous Solutions**:\n    *   Evaluation on traditionally-used small datasets (e.g., Cora, Citeseer, TU) is insufficient to differentiate the expressive power of various GNN architectures \\cite{dwivedi20239ab}.\n    *   Inconsistent experimental comparisons and a lack of consensus on unifying experimental settings make it difficult to draw reliable conclusions about model performance gains \\cite{dwivedi20239ab}.\n    *   Absence of fixed parameter budgets means performance improvements could be due to increased model capacity rather than architectural innovation.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method/Algorithm**: The paper introduces an open-source, modular benchmarking framework for GNNs built upon PyTorch and DGL. It provides a standardized environment for evaluating GNNs across diverse tasks and datasets.\n*   **Novelty/Difference**:\n    *   **Diverse Dataset Collection**: Comprises 12 medium-scale datasets, including both real-world (e.g., ZINC, AQSOL, OGB-COLLAB, WikiCS, MNIST, CIFAR10) and essential mathematical graphs (e.g., PATTERN, CLUSTER, TSP, CSL, CYCLES, GraphTheoryProp) designed to test specific theoretical graph properties and discriminate GNN performance. AQSOL is a new addition with real-world measured chemical targets.\n    *   **Fair Comparison Protocol**: Enforces fixed parameter budgets (100k and 500k) for all GNN models, ensuring that performance differences are attributable to architectural design rather than varying model capacity.\n    *   **Modular & Reproducible Infrastructure**: Provides an easy-to-use, open-source code infrastructure with independent components for data pipelines, GNN layers/models, training/evaluation functions, and configurations, enabling researchers to experiment with new ideas at any stage.\n    *   **Focus on Medium-Scale Datasets**: Prioritizes medium-scale datasets to enable swift yet reliable prototyping and achieve statistical differences in GNN performance within reasonable computational times (e.g., 12 hours for a single experiment run).\n\n### 4. Key Technical Contributions\n\n*   **System Design/Architectural Innovations**:\n    *   Development of a comprehensive, open-source GNN benchmarking framework with a modular architecture (data, configs, layers, nets, train modules) that promotes ease-of-use, extensibility, and reproducibility \\cite{dwivedi20239ab}.\n    *   Introduction of a standardized experimental protocol, including fixed parameter budgets, to ensure fair and rigorous comparison of GNN architectures.\n*   **Novel Methods/Techniques (demonstrated via the benchmark)**:\n    *   The benchmark itself facilitated the introduction and validation of **Graph Positional Encoding (PE) using Laplacian eigenvectors** as a crucial component for GNNs, especially for graphs lacking canonical positional information or node features \\cite{dwivedi20239ab}. This technique significantly improved MP-GCN performance on synthetic and real-world datasets.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted (using the benchmark)**: The paper demonstrates the utility of its framework by studying the case of **Graph Positional Encoding (PE)**. It used the benchmark to validate and quantify the improvement provided by Laplacian eigenvectors as node positional encodings.\n*   **Key Performance Metrics & Comparison Results**:\n    *   Laplacian PE was shown to effectively improve Message-Passing GCNs (MP-GCNs) on synthetic datasets (CSL, CYCLES, GraphTheoryProp) where nodes are anonymous and traditional GCNs perform poorly.\n    *   Improvements were also observed on other real-world datasets, including the newly added AQSOL dataset.\n    *   While specific numerical results are detailed in the appendix (not provided in the excerpt), the paper emphasizes that the benchmark allowed for robust experimental settings to quantify these gains.\n*   **Community Adoption as Validation**: The framework's wide usage (2,000+ GitHub stars, 380+ forks, 470+ citations) since its initial release serves as strong empirical validation of its utility and impact on the GNN research community \\cite{dwivedi20239ab}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus on **medium-scale datasets** is motivated by enabling swift prototyping and academic-scale research, which might not fully capture the computational challenges or specific properties of very large-scale graphs.\n    *   The use of **fixed parameter budgets** for fair comparison, while crucial, means the benchmark does not aim to find the *optimal* hyperparameters for every specific model, which is computationally expensive.\n*   **Scope of Applicability**: The benchmark is designed for evaluating and prototyping GNNs across graph-level, node-level, and edge-level tasks, and is particularly useful for exploring fundamental GNN design choices (e.g., aggregation functions, expressive power, pooling, normalization, robustness, positional encodings). It supports both Message Passing GCNs and Weisfeiler Lehman GNNs.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: The framework significantly advances the technical state-of-the-art by providing a much-needed standardized, fair, and reproducible environment for GNN research. It moves beyond inconsistent evaluations on small datasets, enabling rigorous comparison and identification of fundamental GNN principles.\n*   **Potential Impact on Future Research**:\n    *   It serves as a critical tool for accelerating GNN development by allowing researchers to quickly and robustly test new ideas and explore insights.\n    *   The benchmark has already demonstrated its ability to steer research directions, notably by introducing Graph Positional Encoding (PE) with Laplacian eigenvectors, which subsequently spurred extensive follow-up research on improving PE for GNNs and Transformers \\cite{dwivedi20239ab}.\n    *   It fosters a more scientific approach to GNN development by enabling the community to identify universal, generalizable, and scalable architectures.",
    "intriguing_abstract": "The explosive growth of Graph Neural Networks (GNNs) has outpaced the development of robust, community-standard benchmarks, leading to inconsistent evaluations and hindering true architectural progress. We introduce a pioneering, open-source benchmarking framework built on PyTorch and DGL, designed to revolutionize GNN research by enabling fair, rigorous, and reproducible comparisons. Our framework features a curated collection of 12 diverse, medium-scale datasets—encompassing both real-world applications (e.g., ZINC, AQSOL, OGB-COLLAB) and theoretically challenging synthetic graphs (e.g., CSL, PATTERN)—specifically chosen to discriminate GNN performance and test fundamental graph properties.\n\nCrucially, we enforce standardized experimental protocols, including fixed parameter budgets, ensuring that performance gains reflect genuine architectural innovation rather than mere capacity increases. This rigorous environment has already facilitated significant discoveries, notably validating the critical role of **Graph Positional Encoding (PE) using Laplacian eigenvectors** in enhancing Message-Passing GCNs, particularly on graphs lacking canonical node features. Widely adopted by the GNN community, this framework accelerates the identification of universal, generalizable, and scalable GNN principles, steering future research directions and fostering a new era of scientific rigor in graph representation learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN benchmarking framework",
      "standardized experimental protocol",
      "fair model comparison",
      "reproducible modular infrastructure",
      "diverse medium-scale datasets",
      "fixed parameter budgets",
      "Graph Positional Encoding (PE)",
      "Laplacian eigenvectors",
      "Message-Passing GCNs (MP-GCNs)",
      "PyTorch and DGL",
      "architectural advancements",
      "scientific GNN development"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf",
    "citation_key": "dwivedi20239ab",
    "metadata": {
      "title": "Benchmarking Graph Neural Networks",
      "authors": [
        "Vijay Prakash Dwivedi",
        "Chaitanya K. Joshi",
        "T. Laurent",
        "Yoshua Bengio",
        "X. Bresson"
      ],
      "published_date": "2023",
      "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf",
      "venue": "Journal of machine learning research",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{dwivedi20239ab}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: The rapid growth of Graph Neural Networks (GNNs) has led to numerous promising techniques, but a lack of community-standard benchmarks makes it challenging to track progress, fairly compare models, and identify truly impactful architectural advancements.\n*   **Importance & Challenge**:\n    *   Existing evaluations often use small, non-discriminative datasets (e.g., Cora, Citeseer, TU) that fail to differentiate complex from simple or graph-agnostic architectures.\n    *   Inconsistent experimental settings and varying parameter budgets across studies make fair comparisons difficult, hindering the identification of universal, generalizable, and scalable GNN principles.\n    *   Designing effective benchmarks is challenging, requiring a rigorous experimental setting, reproducibility, and appropriate datasets that can statistically separate model performance.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**: While GNNs have been extensively developed across various domains (chemistry, physics, social sciences, etc.), the paper positions itself as addressing a critical gap in the *evaluation* and *benchmarking* of these models. Previous GNN research focused on developing new architectures (message-passing, attention-based, theoretically expressive GNNs) but lacked a standardized framework for their rigorous comparison.\n*   **Limitations of Previous Solutions**:\n    *   Evaluation on traditionally-used small datasets (e.g., Cora, Citeseer, TU) is insufficient to differentiate the expressive power of various GNN architectures \\cite{dwivedi20239ab}.\n    *   Inconsistent experimental comparisons and a lack of consensus on unifying experimental settings make it difficult to draw reliable conclusions about model performance gains \\cite{dwivedi20239ab}.\n    *   Absence of fixed parameter budgets means performance improvements could be due to increased model capacity rather than architectural innovation.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method/Algorithm**: The paper introduces an open-source, modular benchmarking framework for GNNs built upon PyTorch and DGL. It provides a standardized environment for evaluating GNNs across diverse tasks and datasets.\n*   **Novelty/Difference**:\n    *   **Diverse Dataset Collection**: Comprises 12 medium-scale datasets, including both real-world (e.g., ZINC, AQSOL, OGB-COLLAB, WikiCS, MNIST, CIFAR10) and essential mathematical graphs (e.g., PATTERN, CLUSTER, TSP, CSL, CYCLES, GraphTheoryProp) designed to test specific theoretical graph properties and discriminate GNN performance. AQSOL is a new addition with real-world measured chemical targets.\n    *   **Fair Comparison Protocol**: Enforces fixed parameter budgets (100k and 500k) for all GNN models, ensuring that performance differences are attributable to architectural design rather than varying model capacity.\n    *   **Modular & Reproducible Infrastructure**: Provides an easy-to-use, open-source code infrastructure with independent components for data pipelines, GNN layers/models, training/evaluation functions, and configurations, enabling researchers to experiment with new ideas at any stage.\n    *   **Focus on Medium-Scale Datasets**: Prioritizes medium-scale datasets to enable swift yet reliable prototyping and achieve statistical differences in GNN performance within reasonable computational times (e.g., 12 hours for a single experiment run).\n\n### 4. Key Technical Contributions\n\n*   **System Design/Architectural Innovations**:\n    *   Development of a comprehensive, open-source GNN benchmarking framework with a modular architecture (data, configs, layers, nets, train modules) that promotes ease-of-use, extensibility, and reproducibility \\cite{dwivedi20239ab}.\n    *   Introduction of a standardized experimental protocol, including fixed parameter budgets, to ensure fair and rigorous comparison of GNN architectures.\n*   **Novel Methods/Techniques (demonstrated via the benchmark)**:\n    *   The benchmark itself facilitated the introduction and validation of **Graph Positional Encoding (PE) using Laplacian eigenvectors** as a crucial component for GNNs, especially for graphs lacking canonical positional information or node features \\cite{dwivedi20239ab}. This technique significantly improved MP-GCN performance on synthetic and real-world datasets.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted (using the benchmark)**: The paper demonstrates the utility of its framework by studying the case of **Graph Positional Encoding (PE)**. It used the benchmark to validate and quantify the improvement provided by Laplacian eigenvectors as node positional encodings.\n*   **Key Performance Metrics & Comparison Results**:\n    *   Laplacian PE was shown to effectively improve Message-Passing GCNs (MP-GCNs) on synthetic datasets (CSL, CYCLES, GraphTheoryProp) where nodes are anonymous and traditional GCNs perform poorly.\n    *   Improvements were also observed on other real-world datasets, including the newly added AQSOL dataset.\n    *   While specific numerical results are detailed in the appendix (not provided in the excerpt), the paper emphasizes that the benchmark allowed for robust experimental settings to quantify these gains.\n*   **Community Adoption as Validation**: The framework's wide usage (2,000+ GitHub stars, 380+ forks, 470+ citations) since its initial release serves as strong empirical validation of its utility and impact on the GNN research community \\cite{dwivedi20239ab}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus on **medium-scale datasets** is motivated by enabling swift prototyping and academic-scale research, which might not fully capture the computational challenges or specific properties of very large-scale graphs.\n    *   The use of **fixed parameter budgets** for fair comparison, while crucial, means the benchmark does not aim to find the *optimal* hyperparameters for every specific model, which is computationally expensive.\n*   **Scope of Applicability**: The benchmark is designed for evaluating and prototyping GNNs across graph-level, node-level, and edge-level tasks, and is particularly useful for exploring fundamental GNN design choices (e.g., aggregation functions, expressive power, pooling, normalization, robustness, positional encodings). It supports both Message Passing GCNs and Weisfeiler Lehman GNNs.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: The framework significantly advances the technical state-of-the-art by providing a much-needed standardized, fair, and reproducible environment for GNN research. It moves beyond inconsistent evaluations on small datasets, enabling rigorous comparison and identification of fundamental GNN principles.\n*   **Potential Impact on Future Research**:\n    *   It serves as a critical tool for accelerating GNN development by allowing researchers to quickly and robustly test new ideas and explore insights.\n    *   The benchmark has already demonstrated its ability to steer research directions, notably by introducing Graph Positional Encoding (PE) with Laplacian eigenvectors, which subsequently spurred extensive follow-up research on improving PE for GNNs and Transformers \\cite{dwivedi20239ab}.\n    *   It fosters a more scientific approach to GNN development by enabling the community to identify universal, generalizable, and scalable architectures.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN benchmarking framework",
        "standardized experimental protocol",
        "fair model comparison",
        "reproducible modular infrastructure",
        "diverse medium-scale datasets",
        "fixed parameter budgets",
        "Graph Positional Encoding (PE)",
        "Laplacian eigenvectors",
        "Message-Passing GCNs (MP-GCNs)",
        "PyTorch and DGL",
        "architectural advancements",
        "scientific GNN development"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"benchmarks must be developed to quantify progress. this led us in march 2020 to release a benchmark framework...\"** - this indicates the creation and release of a system/tool.\n2.  **\"i) comprises of a diverse collection..., ii) enables fair model comparison..., iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is ﬂexible for researchers to experiment with new theoretical ideas.\"** - these points describe the features and design of the proposed framework/system.\n3.  **\"demonstrates the utility of the proposed open-source framework through the wide usage by the gnn community.\"** - highlights the impact and adoption of their developed system.\n4.  **\"in this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics...\"** - the paper's core contribution is presenting this framework.\n5.  **\"however, tracking progress is often challenging in the absence of a community-standard benchmark... make it diﬃcult to diﬀerentiate complex, simple and graph-agnostic architectures\"** - this sets up the problem that their benchmark *system* aims to solve.\n\nthe paper clearly presents a new system (a benchmark framework) and discusses its design, features, and utility. this aligns perfectly with the \"technical\" classification.\n\n**classification: technical**"
    },
    "file_name": "d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf"
  },
  {
    "success": true,
    "doc_id": "c66688b6ab03b241971b7f01ac1a05e3",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs), particularly Message Passing Neural Networks (MPNNs), are limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism heuristic. This means they cannot distinguish between many non-isomorphic graphs.\n    *   Existing higher-order GNNs (e.g., k-GNNs, invariant/equivariant graph networks) overcome this limitation but are computationally very demanding, requiring excessive memory (e.g., O(|V|^k) or O(|V|^2) for universality), making them impractical for many applications.\n    *   The problem is to enhance GNNs' expressive power beyond 1-WL without incurring the prohibitive computational costs of higher-order models.\n\n*   **Related Work & Positioning**\n    *   This work builds upon the empirical observation that Random Node Initialization (RNI) can improve MPNN performance and detect fixed substructures \\cite{abboud2020x5e}.\n    *   It positions itself against higher-order GNNs (k-GNNs, invariant/equivariant graph networks, PPGNs) which achieve higher expressiveness but suffer from severe computational and memory requirements (e.g., O(|V|^k) or O(|V|^2) memory for universality), or require exponentially many samples to learn necessary functions.\n    *   The paper provides the first theoretical justification for the effectiveness of RNI, proving its universality for memory-efficient GNNs, which was previously lacking.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to enhance standard MPNNs with Random Node Initialization (RNI), where initial node embeddings are randomized.\n    *   The primary innovation is the *proof* that MPNNs with RNI are **universal**, meaning they can approximate any function defined on graphs of any fixed order. This is a first such universality result for GNNs that does not rely on computationally demanding higher-order properties.\n    *   The approach also demonstrates that RNI preserves the permutation-invariance of GNNs *in expectation*, addressing a potential concern about randomization.\n    *   The proof leverages a logical characterization of MPNN expressiveness (C2 logic) and shows that RNI individualizes input graphs with high probability, allowing MPNNs to capture arbitrary Boolean functions and, by extension, real-valued functions.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Insight**: Proof of universality for MPNNs with RNI, establishing that they can approximate any invariant function on graphs, while maintaining permutation-invariance in expectation. This breaks the 1-WL barrier for memory-efficient GNNs.\n    *   **Logical Characterization**: Provides a logical characterization for MPNNs with RNI, substantiating how randomization improves expressiveness by individualizing nodes.\n    *   **Partial RNI**: Demonstrates that universality holds even with partially randomized initial node features (e.g., only one randomized dimension), offering flexibility.\n    *   **Novel Datasets**: Introduction of two carefully designed synthetic datasets, EXP and CEXP, specifically constructed to evaluate GNN expressiveness beyond 1-WL (requiring 2-WL or higher), which are crucial for rigorous empirical validation.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on two synthetic datasets:\n        *   **EXP**: Designed to require 2-WL expressive power, containing 1-WL indistinguishable but 2-WL distinguishable non-isomorphic graph pairs with different SAT outcomes.\n        *   **CEXP**: A modification of EXP with 50% 1-WL distinguishable data and 50% data requiring >1-WL expressiveness, making the learning task more challenging.\n    *   **Models Compared**:\n        *   **1-WL GCN (1-GCN)**: A standard MPNN, expected to perform at chance (50%) on EXP.\n        *   **GCN-RNI**: 1-GCN enhanced with RNI, tested with various initialization distributions (Normal, Uniform, Xavier Normal, Xavier Uniform).\n        *   **GCN-x%RNI**: GCN-RNI with partial randomization (e.g., 12.5%, 50%, 87.5% of dimensions randomized).\n        *   **Higher-order GNNs**: PPGN (2-WL power), 1-2-3-GCN-L (emulating 2-WL on 3-node tuples), and 3-GCN (full 2-WL procedure over 3-node tuples).\n    *   **Key Performance Metrics & Results**:\n        *   **Expressiveness on EXP**: GCN-RNI models achieved near-perfect test accuracy (e.g., 97.3% to 98.0%), substantially surpassing the 50% baseline of 1-GCN and PPGN, and closely matching the performance of the computationally intensive 3-GCN (99.7%).\n        *   **Convergence**: GCN-RNI models showed slower convergence compared to deterministic GNNs.\n        *   **Partial RNI**: Partially randomizing initial node features (e.g., GCN-12.5%RNI) significantly improved model convergence and often led to higher accuracy compared to full RNI, demonstrating a practical trade-off.\n        *   **CEXP**: GCN-RNI models also performed well on CEXP, demonstrating robustness in mixed-expressiveness settings.\n        *   **Sparser Datasets**: Similar behavior was observed with sparser datasets and longer training, though with more volatility.\n\n*   **Limitations & Scope**\n    *   **Theoretical vs. Practical Construction Size**: While universal, the theoretical constructions for approximating arbitrary functions can be very large, a common limitation of universality results.\n    *   **Slower Convergence**: Empirically, GNNs with RNI exhibit slower convergence compared to standard GNNs, which might require longer training times.\n    *   **Volatility**: Experiments on sparser datasets showed more volatile behavior, suggesting potential sensitivity to graph density.\n    *   **Scope**: The universality result is for functions defined on graphs of *any fixed order* (i.e., fixed number of nodes `n`).\n\n*   **Technical Significance**\n    *   **Breaks Expressiveness Barrier**: Provides a theoretically sound and empirically validated method to overcome the 1-WL expressiveness limitation of standard MPNNs without resorting to computationally prohibitive higher-order GNNs.\n    *   **Justifies RNI**: Offers a strong theoretical justification for the widespread empirical success of random initialization in GNNs.\n    *   **Practical Viability**: Demonstrates that MPNNs with RNI combine high expressiveness with practical efficiency, making them a viable alternative to complex higher-order models.\n    *   **Future Research**: Opens new avenues for logically grounded theoretical studies of randomized MPNN models and for developing more expressive and memory-efficient GNN architectures.",
    "intriguing_abstract": "Graph Neural Networks (GNNs), particularly Message Passing Neural Networks (MPNNs), are powerful but fundamentally constrained by the 1-Weisfeiler-Leman (1-WL) test, limiting their **expressiveness** and ability to distinguish many non-isomorphic graphs. While higher-order GNNs overcome this barrier, their prohibitive computational and memory costs (e.g., O(|V|^k)) render them impractical for real-world applications.\n\nThis paper presents a groundbreaking solution: we provide the first theoretical proof that MPNNs enhanced with **Random Node Initialization (RNI)** are **universal**. This establishes that RNI-augmented MPNNs can approximate any invariant function defined on graphs of fixed order, effectively breaking the 1-WL barrier without incurring the computational burden of higher-order models. Our innovation demonstrates that RNI, by individualizing nodes, allows MPNNs to capture arbitrary graph structures while preserving **permutation-invariance** in expectation. Empirical validation on novel synthetic datasets (EXP, CEXP), specifically designed to require beyond 1-WL expressiveness, shows GCN-RNI achieving near-perfect accuracy, often surpassing or matching computationally intensive higher-order counterparts. This work offers a theoretically sound and practically viable path to highly expressive and **computationally efficient GNNs**, providing crucial justification for RNI's empirical success and opening new avenues for graph representation learning.",
    "keywords": [
      "Message Passing Neural Networks (MPNNs)",
      "1-Weisfeiler-Leman (1-WL) expressiveness limitation",
      "Random Node Initialization (RNI)",
      "Universality proof for GNNs",
      "Higher-order GNNs",
      "Computational efficiency",
      "Permutation-invariance",
      "C2 logic",
      "Expressive power enhancement",
      "Synthetic datasets (EXP",
      "CEXP)",
      "Partial RNI",
      "Memory-efficient GNNs",
      "Breaking 1-WL barrier"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf",
    "citation_key": "abboud2020x5e",
    "metadata": {
      "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization",
      "authors": [
        "Ralph Abboud",
        ".Ismail .Ilkan Ceylan",
        "Martin Grohe",
        "Thomas Lukasiewicz"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs), particularly Message Passing Neural Networks (MPNNs), are limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism heuristic. This means they cannot distinguish between many non-isomorphic graphs.\n    *   Existing higher-order GNNs (e.g., k-GNNs, invariant/equivariant graph networks) overcome this limitation but are computationally very demanding, requiring excessive memory (e.g., O(|V|^k) or O(|V|^2) for universality), making them impractical for many applications.\n    *   The problem is to enhance GNNs' expressive power beyond 1-WL without incurring the prohibitive computational costs of higher-order models.\n\n*   **Related Work & Positioning**\n    *   This work builds upon the empirical observation that Random Node Initialization (RNI) can improve MPNN performance and detect fixed substructures \\cite{abboud2020x5e}.\n    *   It positions itself against higher-order GNNs (k-GNNs, invariant/equivariant graph networks, PPGNs) which achieve higher expressiveness but suffer from severe computational and memory requirements (e.g., O(|V|^k) or O(|V|^2) memory for universality), or require exponentially many samples to learn necessary functions.\n    *   The paper provides the first theoretical justification for the effectiveness of RNI, proving its universality for memory-efficient GNNs, which was previously lacking.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to enhance standard MPNNs with Random Node Initialization (RNI), where initial node embeddings are randomized.\n    *   The primary innovation is the *proof* that MPNNs with RNI are **universal**, meaning they can approximate any function defined on graphs of any fixed order. This is a first such universality result for GNNs that does not rely on computationally demanding higher-order properties.\n    *   The approach also demonstrates that RNI preserves the permutation-invariance of GNNs *in expectation*, addressing a potential concern about randomization.\n    *   The proof leverages a logical characterization of MPNN expressiveness (C2 logic) and shows that RNI individualizes input graphs with high probability, allowing MPNNs to capture arbitrary Boolean functions and, by extension, real-valued functions.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Insight**: Proof of universality for MPNNs with RNI, establishing that they can approximate any invariant function on graphs, while maintaining permutation-invariance in expectation. This breaks the 1-WL barrier for memory-efficient GNNs.\n    *   **Logical Characterization**: Provides a logical characterization for MPNNs with RNI, substantiating how randomization improves expressiveness by individualizing nodes.\n    *   **Partial RNI**: Demonstrates that universality holds even with partially randomized initial node features (e.g., only one randomized dimension), offering flexibility.\n    *   **Novel Datasets**: Introduction of two carefully designed synthetic datasets, EXP and CEXP, specifically constructed to evaluate GNN expressiveness beyond 1-WL (requiring 2-WL or higher), which are crucial for rigorous empirical validation.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on two synthetic datasets:\n        *   **EXP**: Designed to require 2-WL expressive power, containing 1-WL indistinguishable but 2-WL distinguishable non-isomorphic graph pairs with different SAT outcomes.\n        *   **CEXP**: A modification of EXP with 50% 1-WL distinguishable data and 50% data requiring >1-WL expressiveness, making the learning task more challenging.\n    *   **Models Compared**:\n        *   **1-WL GCN (1-GCN)**: A standard MPNN, expected to perform at chance (50%) on EXP.\n        *   **GCN-RNI**: 1-GCN enhanced with RNI, tested with various initialization distributions (Normal, Uniform, Xavier Normal, Xavier Uniform).\n        *   **GCN-x%RNI**: GCN-RNI with partial randomization (e.g., 12.5%, 50%, 87.5% of dimensions randomized).\n        *   **Higher-order GNNs**: PPGN (2-WL power), 1-2-3-GCN-L (emulating 2-WL on 3-node tuples), and 3-GCN (full 2-WL procedure over 3-node tuples).\n    *   **Key Performance Metrics & Results**:\n        *   **Expressiveness on EXP**: GCN-RNI models achieved near-perfect test accuracy (e.g., 97.3% to 98.0%), substantially surpassing the 50% baseline of 1-GCN and PPGN, and closely matching the performance of the computationally intensive 3-GCN (99.7%).\n        *   **Convergence**: GCN-RNI models showed slower convergence compared to deterministic GNNs.\n        *   **Partial RNI**: Partially randomizing initial node features (e.g., GCN-12.5%RNI) significantly improved model convergence and often led to higher accuracy compared to full RNI, demonstrating a practical trade-off.\n        *   **CEXP**: GCN-RNI models also performed well on CEXP, demonstrating robustness in mixed-expressiveness settings.\n        *   **Sparser Datasets**: Similar behavior was observed with sparser datasets and longer training, though with more volatility.\n\n*   **Limitations & Scope**\n    *   **Theoretical vs. Practical Construction Size**: While universal, the theoretical constructions for approximating arbitrary functions can be very large, a common limitation of universality results.\n    *   **Slower Convergence**: Empirically, GNNs with RNI exhibit slower convergence compared to standard GNNs, which might require longer training times.\n    *   **Volatility**: Experiments on sparser datasets showed more volatile behavior, suggesting potential sensitivity to graph density.\n    *   **Scope**: The universality result is for functions defined on graphs of *any fixed order* (i.e., fixed number of nodes `n`).\n\n*   **Technical Significance**\n    *   **Breaks Expressiveness Barrier**: Provides a theoretically sound and empirically validated method to overcome the 1-WL expressiveness limitation of standard MPNNs without resorting to computationally prohibitive higher-order GNNs.\n    *   **Justifies RNI**: Offers a strong theoretical justification for the widespread empirical success of random initialization in GNNs.\n    *   **Practical Viability**: Demonstrates that MPNNs with RNI combine high expressiveness with practical efficiency, making them a viable alternative to complex higher-order models.\n    *   **Future Research**: Opens new avenues for logically grounded theoretical studies of randomized MPNN models and for developing more expressive and memory-efficient GNN architectures.",
      "keywords": [
        "Message Passing Neural Networks (MPNNs)",
        "1-Weisfeiler-Leman (1-WL) expressiveness limitation",
        "Random Node Initialization (RNI)",
        "Universality proof for GNNs",
        "Higher-order GNNs",
        "Computational efficiency",
        "Permutation-invariance",
        "C2 logic",
        "Expressive power enhancement",
        "Synthetic datasets (EXP",
        "CEXP)",
        "Partial RNI",
        "Memory-efficient GNNs",
        "Breaking 1-WL barrier"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   **theoretical indicators:**\n    *   abstract: \"prove that these models are universal\", \"universality result\", \"first such result\", \"expressive power\", \"invariance properties\".\n    *   introduction: discusses \"expressive power\" and limitations of gnns, setting the stage for a theoretical analysis of a solution.\n\n*   **empirical indicators:**\n    *   abstract: \"empirically analyze the effect of rni on gnns, based on carefully constructed datasets\", \"our empirical ﬁndings support the superior performance\".\n\nthe paper clearly contains both strong theoretical and empirical components. however, the abstract leads with the theoretical contribution (\"prove that these models are universal, a ﬁrst such result\") and then follows with the empirical analysis that \"supports\" these findings. the core novel contribution appears to be the mathematical proof of universality.\n\ntherefore, the primary classification is **theoretical**."
    },
    "file_name": "aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf"
  },
  {
    "success": true,
    "doc_id": "c77b1a44fc0f824d2c129f269e4a3342",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{liu2023v3e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) suffer significant performance degradation when input graph data contains \"weak information,\" which encompasses incomplete structure (missing edges), incomplete features (missing node attributes), and insufficient labels.\n    *   **Importance and Challenge**:\n        *   Most GNNs assume ideal, complete data, which is often invalid in real-world scenarios due to privacy concerns, data collection errors, or high annotation costs.\n        *   Existing solutions primarily address only *one type* of weak information (e.g., structure learning, feature imputation, label-efficient learning).\n        *   The critical challenge is to develop a universal and effective GNN framework that can handle *simultaneously occurring and mutually affecting* diverse data deficiencies, particularly in \"extreme GLWI\" scenarios where structure, features, and labels are all weak.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges prior work in graph structure learning, attribute completion, and label-efficient graph learning, which are specialized GNN designs to handle specific data deficiencies.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-aspect focus**: A major limitation is that most existing methods only consider data deficiency from a single perspective (e.g., only weak structure or only weak labels). They fail to address scenarios where multiple types of weak information coexist and interact.\n        *   **Computational Cost**: Many specialized methods require complex, carefully-crafted learning procedures, leading to high computational costs and reduced efficiency on large-scale graphs.\n        *   The paper positions itself as the *first attempt* to investigate graph learning with *extremely weak information* where all three aspects (structure, features, labels) are simultaneously incomplete.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **D2PT (Dual-channel Diffused Propagation then Transformation)**, a dual-channel GNN framework designed to enable effective information propagation on graphs with weak information.\n    *   **Novelty/Differentiation**:\n        *   **Empirically-driven Design Principles**: Based on empirical analysis, the authors identify two key design focal points: 1) enabling **long-range propagation** in GNNs, and 2) allowing information propagation to **stray nodes** (isolated from the largest connected component).\n        *   **Dual-Channel Architecture**:\n            *   **Channel 1 (DPT Backbone)**: Performs efficient long-range information propagation on the *input graph* (even with incomplete structure) using a graph diffusion-based backbone.\n            *   **Channel 2 (Global Graph)**: Learns a *global graph* by connecting nodes sharing similar semantics, derived from the propagated features. This channel specifically addresses the \"stray node problem\" by providing connections for isolated nodes.\n        *   **Prototype Contrastive Alignment**: A novel algorithm that aligns class-level prototypes learned from both channels. This mechanism allows the two different information propagation processes to mutually benefit, enhancing the model's ability to handle GLWI.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **D2PT Framework**: A principled dual-channel GNN architecture for comprehensive GLWI, integrating long-range propagation and global semantic connections.\n        *   **Graph Diffusion-based DPT Backbone**: An efficient mechanism for long-range message passing, crucial for recovering missing information and spreading supervision signals.\n        *   **Global Graph Construction**: A method to dynamically learn a global graph based on node semantic similarities, specifically designed to connect \"stray nodes\" and facilitate information flow to them.\n        *   **Prototype Contrastive Alignment**: A novel contrastive learning objective that aligns class-level prototypes between the two channels, ensuring consistency and mutual enhancement of learned representations.\n    *   **Problem Formulation**: The paper formally defines and investigates the \"extreme GLWI scenario,\" where structure, features, and labels are simultaneously deficient, advancing the research scope beyond single-aspect deficiencies.\n    *   **Empirical Analysis**: Provides a comprehensive analysis of the impact of data deficiency on GNNs, identifying long-range propagation and the stray node problem as critical factors, which directly guides the D2PT design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Performance evaluation of D2PT against various baseline GNNs (GCN, GAT, SGC, PPNP, APPNP) and GLWI-specific methods across different GLWI scenarios.\n        *   Ablation studies to validate the effectiveness of D2PT's components (e.g., dual-channel design, prototype alignment, global graph).\n        *   Efficiency analysis (training time, inference time).\n        *   Generalization capability across different datasets and GLWI settings.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   D2PT consistently demonstrates **superior performance** over baseline GNNs and state-of-the-art GLWI methods in various weak information scenarios, including weak structure, weak features, weak labels, and especially the challenging \"extreme GLWI\" scenario.\n        *   The empirical analysis (Figure 2) shows that graph diffusion-based models with long-range propagation (like DPT) generally outperform shallow GNNs in basic GLWI scenarios, motivating D2PT's design.\n        *   Ablation studies confirm the individual contributions of the dual-channel design, global graph, and prototype contrastive alignment to D2PT's overall effectiveness.\n        *   D2PT also exhibits **high efficiency**, demonstrating competitive or superior performance with reasonable computational costs.\n    *   **Datasets**: Experiments were conducted on eight real-world benchmark datasets (e.g., Cora, CiteSeer, PubMed, Coauthor-CS, Amazon-Photo, etc.).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While D2PT addresses a broad range of weak information, the specific mechanisms for constructing the global graph and performing prototype alignment might have dependencies on the quality of initial feature embeddings or the number of classes. The paper does not explicitly detail theoretical guarantees for the prototype alignment's convergence or optimality.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks. Its direct applicability to other graph learning tasks (e.g., link prediction, graph classification) or different types of graph data (e.g., heterogeneous graphs, dynamic graphs) is not explicitly explored in this paper.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023v3e} significantly advances the state-of-the-art by providing a unified, effective, and efficient solution for the complex problem of graph learning with *simultaneously occurring* weak information (structure, features, and labels). This moves beyond the single-aspect focus of prior research.\n    *   **Potential Impact on Future Research**:\n        *   **Robust GNN Design**: The D2PT framework offers a blueprint for designing more robust GNNs that can operate reliably in real-world, imperfect data environments.\n        *   **Multi-modal/Multi-source Weakness**: The dual-channel approach and prototype alignment could inspire future research into handling other forms of multi-modal or multi-source data deficiencies in graph learning.\n        *   **Diffusion-based GNNs**: Reinforces the importance of long-range propagation and graph diffusion mechanisms for handling data sparsity and incompleteness.\n        *   **Addressing Stray Nodes**: The explicit strategy for connecting stray nodes via semantic similarity provides a valuable direction for improving connectivity in sparse or fragmented graphs.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) often falter when confronted with the pervasive reality of imperfect data: \"weak information\" encompassing incomplete structure, missing features, and insufficient labels. This paper addresses the critical, yet underexplored, challenge of *extreme Graph Learning with Weak Information (GLWI)*, where all three deficiencies simultaneously degrade GNN performance. We introduce **D2PT (Dual-channel Diffused Propagation then Transformation)**, a novel and robust GNN framework designed to overcome these pervasive data imperfections.\n\nD2PT employs a unique dual-channel architecture. One channel leverages a graph diffusion-based backbone for efficient long-range information propagation, crucial for recovering missing links and spreading supervision signals. The second channel dynamically constructs a *global graph* based on semantic similarity, specifically addressing the critical 'stray node problem' by providing connections for isolated nodes. A novel *prototype contrastive alignment* mechanism then harmonizes representations from both channels, ensuring mutual enhancement. Extensive experiments on eight benchmark datasets demonstrate D2PT's superior performance and efficiency across diverse GLWI scenarios, particularly in extreme settings, significantly outperforming state-of-the-art methods for semi-supervised node classification. D2PT offers a unified, principled solution, paving the way for more resilient GNNs in real-world applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Weak Information (incomplete structure",
      "features",
      "labels)",
      "Extreme GLWI",
      "D2PT Framework",
      "Dual-channel GNN architecture",
      "Graph Diffusion-based Propagation",
      "Global Graph Construction",
      "Stray Node Problem",
      "Prototype Contrastive Alignment",
      "Long-range Information Propagation",
      "Semi-supervised Node Classification",
      "Robust GNN Design",
      "Simultaneously occurring data deficiencies"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf",
    "citation_key": "liu2023v3e",
    "metadata": {
      "title": "Learning Strong Graph Neural Networks with Weak Information",
      "authors": [
        "Yixin Liu",
        "Kaize Ding",
        "Jianling Wang",
        "Vincent Lee",
        "Huan Liu",
        "Shirui Pan"
      ],
      "published_date": "2023",
      "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",
      "file_path": "paper_data/Graph_Neural_Networks/c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{liu2023v3e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) suffer significant performance degradation when input graph data contains \"weak information,\" which encompasses incomplete structure (missing edges), incomplete features (missing node attributes), and insufficient labels.\n    *   **Importance and Challenge**:\n        *   Most GNNs assume ideal, complete data, which is often invalid in real-world scenarios due to privacy concerns, data collection errors, or high annotation costs.\n        *   Existing solutions primarily address only *one type* of weak information (e.g., structure learning, feature imputation, label-efficient learning).\n        *   The critical challenge is to develop a universal and effective GNN framework that can handle *simultaneously occurring and mutually affecting* diverse data deficiencies, particularly in \"extreme GLWI\" scenarios where structure, features, and labels are all weak.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges prior work in graph structure learning, attribute completion, and label-efficient graph learning, which are specialized GNN designs to handle specific data deficiencies.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-aspect focus**: A major limitation is that most existing methods only consider data deficiency from a single perspective (e.g., only weak structure or only weak labels). They fail to address scenarios where multiple types of weak information coexist and interact.\n        *   **Computational Cost**: Many specialized methods require complex, carefully-crafted learning procedures, leading to high computational costs and reduced efficiency on large-scale graphs.\n        *   The paper positions itself as the *first attempt* to investigate graph learning with *extremely weak information* where all three aspects (structure, features, labels) are simultaneously incomplete.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **D2PT (Dual-channel Diffused Propagation then Transformation)**, a dual-channel GNN framework designed to enable effective information propagation on graphs with weak information.\n    *   **Novelty/Differentiation**:\n        *   **Empirically-driven Design Principles**: Based on empirical analysis, the authors identify two key design focal points: 1) enabling **long-range propagation** in GNNs, and 2) allowing information propagation to **stray nodes** (isolated from the largest connected component).\n        *   **Dual-Channel Architecture**:\n            *   **Channel 1 (DPT Backbone)**: Performs efficient long-range information propagation on the *input graph* (even with incomplete structure) using a graph diffusion-based backbone.\n            *   **Channel 2 (Global Graph)**: Learns a *global graph* by connecting nodes sharing similar semantics, derived from the propagated features. This channel specifically addresses the \"stray node problem\" by providing connections for isolated nodes.\n        *   **Prototype Contrastive Alignment**: A novel algorithm that aligns class-level prototypes learned from both channels. This mechanism allows the two different information propagation processes to mutually benefit, enhancing the model's ability to handle GLWI.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **D2PT Framework**: A principled dual-channel GNN architecture for comprehensive GLWI, integrating long-range propagation and global semantic connections.\n        *   **Graph Diffusion-based DPT Backbone**: An efficient mechanism for long-range message passing, crucial for recovering missing information and spreading supervision signals.\n        *   **Global Graph Construction**: A method to dynamically learn a global graph based on node semantic similarities, specifically designed to connect \"stray nodes\" and facilitate information flow to them.\n        *   **Prototype Contrastive Alignment**: A novel contrastive learning objective that aligns class-level prototypes between the two channels, ensuring consistency and mutual enhancement of learned representations.\n    *   **Problem Formulation**: The paper formally defines and investigates the \"extreme GLWI scenario,\" where structure, features, and labels are simultaneously deficient, advancing the research scope beyond single-aspect deficiencies.\n    *   **Empirical Analysis**: Provides a comprehensive analysis of the impact of data deficiency on GNNs, identifying long-range propagation and the stray node problem as critical factors, which directly guides the D2PT design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Performance evaluation of D2PT against various baseline GNNs (GCN, GAT, SGC, PPNP, APPNP) and GLWI-specific methods across different GLWI scenarios.\n        *   Ablation studies to validate the effectiveness of D2PT's components (e.g., dual-channel design, prototype alignment, global graph).\n        *   Efficiency analysis (training time, inference time).\n        *   Generalization capability across different datasets and GLWI settings.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   D2PT consistently demonstrates **superior performance** over baseline GNNs and state-of-the-art GLWI methods in various weak information scenarios, including weak structure, weak features, weak labels, and especially the challenging \"extreme GLWI\" scenario.\n        *   The empirical analysis (Figure 2) shows that graph diffusion-based models with long-range propagation (like DPT) generally outperform shallow GNNs in basic GLWI scenarios, motivating D2PT's design.\n        *   Ablation studies confirm the individual contributions of the dual-channel design, global graph, and prototype contrastive alignment to D2PT's overall effectiveness.\n        *   D2PT also exhibits **high efficiency**, demonstrating competitive or superior performance with reasonable computational costs.\n    *   **Datasets**: Experiments were conducted on eight real-world benchmark datasets (e.g., Cora, CiteSeer, PubMed, Coauthor-CS, Amazon-Photo, etc.).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While D2PT addresses a broad range of weak information, the specific mechanisms for constructing the global graph and performing prototype alignment might have dependencies on the quality of initial feature embeddings or the number of classes. The paper does not explicitly detail theoretical guarantees for the prototype alignment's convergence or optimality.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks. Its direct applicability to other graph learning tasks (e.g., link prediction, graph classification) or different types of graph data (e.g., heterogeneous graphs, dynamic graphs) is not explicitly explored in this paper.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023v3e} significantly advances the state-of-the-art by providing a unified, effective, and efficient solution for the complex problem of graph learning with *simultaneously occurring* weak information (structure, features, and labels). This moves beyond the single-aspect focus of prior research.\n    *   **Potential Impact on Future Research**:\n        *   **Robust GNN Design**: The D2PT framework offers a blueprint for designing more robust GNNs that can operate reliably in real-world, imperfect data environments.\n        *   **Multi-modal/Multi-source Weakness**: The dual-channel approach and prototype alignment could inspire future research into handling other forms of multi-modal or multi-source data deficiencies in graph learning.\n        *   **Diffusion-based GNNs**: Reinforces the importance of long-range propagation and graph diffusion mechanisms for handling data sparsity and incompleteness.\n        *   **Addressing Stray Nodes**: The explicit strategy for connecting stray nodes via semantic similarity provides a valuable direction for improving connectivity in sparse or fragmented graphs.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Weak Information (incomplete structure",
        "features",
        "labels)",
        "Extreme GLWI",
        "D2PT Framework",
        "Dual-channel GNN architecture",
        "Graph Diffusion-based Propagation",
        "Global Graph Construction",
        "Stray Node Problem",
        "Prototype Contrastive Alignment",
        "Long-range Information Propagation",
        "Semi-supervised Node Classification",
        "Robust GNN Design",
        "Simultaneously occurring data deficiencies"
      ],
      "paper_type": "**technical**\n\n**reasoning:**\n\nthe abstract and introduction strongly indicate a **technical** paper based on the following points:\n\n*   **problem identification:** the abstract clearly identifies a technical problem: gnn performance deterioration with \"weak information\" (incomplete structure, features, labels) and the inadequacy of prior studies.\n*   **proposed solution:** the abstract explicitly states the paper's aim \"to develop an effective and principled approach\" and then details the proposed solution: \"we propose d2pt, a dual-channel gnn framework\" and \"we further develop a prototype contrastive alignment algorithm.\" these phrases directly align with the \"propose,\" \"develop,\" \"algorithm,\" and \"method\" keywords for technical papers.\n*   **evaluation of proposed solution:** the abstract mentions \"extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods.\" this empirical evaluation is conducted to validate the *new methods* presented, which is typical for technical papers.\n*   **introduction focus:** the introduction sets the stage by discussing the capabilities of gnns and then immediately highlights a \"fundamental assumption\" that is \"often invalid in practical scenarios,\" leading to the problem that the proposed technical solution aims to address.\n\nwhile the paper includes \"empirical analysis\" and \"experiments,\" these are in service of developing and validating the *new technical methods* (d2pt framework and alignment algorithm), making it primarily a technical contribution rather than a purely empirical study of existing phenomena or data."
    },
    "file_name": "c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf"
  },
  {
    "success": true,
    "doc_id": "7d2cbec475b4e27a425b60711d21ea1b",
    "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n### Technical Paper Analysis: Elastic Graph Neural Networks \\cite{liu2021ee2}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Graph Neural Networks (GNNs) predominantly rely on `L2`-based graph smoothing, which enforces a uniform, global level of smoothness across the entire graph. This approach struggles to adapt to scenarios where different regions of a graph require varying degrees of smoothness (e.g., sharp changes between clusters, but smooth within them).\n    *   **Importance & Challenge:** Enhancing the *local smoothness adaptivity* of GNNs is crucial for learning more accurate and robust graph representations. However, incorporating `L1`-based smoothing (known for its ability to preserve discontinuities and promote sparsity) into GNNs presents significant challenges:\n        *   Developing an efficient, scalable optimization solver for the non-smooth objective function.\n        *   Ensuring the derived message passing scheme is compatible with back-propagation training.\n        *   Implementing appropriate normalization for `L1`-based graph difference operators to handle diverse node degrees, a detail often overlooked in prior `L1`-based graph signal processing methods.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Many popular GNNs (e.g., GCN, PPNP, APPNP) are shown to implicitly perform `L2`-based graph signal denoising, which penalizes the squared differences between neighboring node features.\n    *   **Limitations of Previous Solutions:** These `L2`-based methods enforce global smoothness, leading to potential over-smoothing or an inability to capture important discontinuities in graph signals. While `L1`-based methods like Graph Trend Filtering (GTF) exist and demonstrate superior adaptivity and piecewise properties, their integration into the deep learning framework of GNNs has been largely unexplored. `\\cite{liu2021ee2}` bridges this gap by introducing `L1`-based smoothing into GNN design.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** `\\cite{liu2021ee2}` proposes a novel **Elastic Graph Signal Estimator** that combines `L1`-based and `L2`-based graph smoothing terms with a fidelity term. The `L1` term (`\\lambda_1 ||\\tilde{\\Gamma} F||_1` or `\\lambda_1 ||\\tilde{\\Gamma} F||_{2,1}`) promotes sparsity in node differences, enabling adaptive local smoothness and discontinuity preservation, while the `L2` term (`\\lambda_2/2 tr(F^T \\tilde{L} F)`) provides global smoothing. To solve this challenging non-smooth, non-separable optimization problem, the paper derives a novel **Elastic Message Passing (EMP)** scheme. EMP is based on a primal-dual algorithm (inspired by Proximal Alternating Predictor-Corrector, PAPC) that efficiently solves an equivalent saddle point problem.\n    *   **Novelty/Difference:**\n        *   **First Integration of `L1`-based Smoothing in GNNs:** This is the first work to systematically incorporate `L1`-based graph smoothing into the design of GNNs to enhance local adaptivity.\n        *   **Novel Normalization for `L1`:** Introduces a degree-based normalization for the graph incident matrix (`\\tilde{\\Gamma} = \\Gamma \\hat{D}^{-1/2}`) for the `L1` term, crucial for numerical stability and handling diverse node degrees in real-world graphs.\n        *   **`L21` Norm for Multi-dimensional Features:** Offers an `L21` norm option for the `L1` penalty to couple multi-dimensional features, promoting shared sparsity patterns across feature dimensions.\n        *   **Efficient Primal-Dual Message Passing:** The EMP scheme is a novel iterative algorithm designed for efficiency, compatibility with back-propagation, and theoretical convergence guarantees, which can also recover existing `L2`-based GNN propagation rules (like APPNP) as special cases.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Elastic Graph Signal Estimator:** A new objective function that synergistically combines `L1` and `L2` graph smoothing for adaptive local and global smoothness.\n        *   **Elastic Message Passing (EMP):** A novel, general, and efficient message passing algorithm derived from a primal-dual optimization framework, specifically designed to be friendly to back-propagation training.\n    *   **System Design/Architectural Innovations:**\n        *   **Elastic GNNs:** A new family of GNN architectures that integrate the proposed EMP scheme into deep neural networks, following a decoupled propagation-transformation approach.\n    *   **Theoretical Insights/Analysis:**\n        *   **Convergence Guarantee:** Provides a theoretical proof (Theorem 1) ensuring the convergence of the EMP scheme to the optimal solution of the elastic graph signal estimator under specified stepsize conditions.\n        *   **Normalization Justification:** Introduces and theoretically justifies a degree-based normalization for the `L1`-based graph difference operator, maintaining its relation with the normalized Laplacian matrix.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated the proposed Elastic GNNs on semi-supervised node classification tasks using benchmark datasets (e.g., Cora, Citeseer, Pubmed).\n    *   **Key Performance Metrics:** Standard classification accuracy.\n    *   **Comparison Results:**\n        *   Elastic GNNs demonstrate **superior adaptivity** and achieve **better performance** on various real-world datasets compared to existing GNNs.\n        *   The models exhibit **significant robustness to graph adversarial attacks**, outperforming other GNNs in adversarial settings.\n        *   Ablation studies on different variants confirm that `L1`-based and `L2`-based graph smoothing are **complementary**, and their combination in Elastic GNNs leads to more versatile and robust models.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The current work primarily focuses on the 0-th order graph difference operator (`\\Gamma`), which corresponds to a piecewise constant prior for graph representation learning. While the framework is general enough to consider higher-order differences, these are not explored in this paper.\n    *   **Scope of Applicability:** The empirical validation is focused on semi-supervised node classification. While the general EMP scheme is flexible, its applicability and performance on other graph learning tasks (e.g., graph classification, link prediction) are not explicitly demonstrated.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{liu2021ee2}` makes a significant technical advancement by overcoming the limitations of global `L2`-based smoothing in GNNs. By introducing `L1`-based adaptive local smoothing, it enables GNNs to learn more nuanced, piecewise representations that better reflect the inherent heterogeneity of real-world graph signals.\n    *   **Potential Impact:**\n        *   Provides a novel and theoretically grounded framework for designing GNNs that are more robust to noise and adversarial attacks, and better equipped to handle complex graph structures with varying local properties.\n        *   The general EMP scheme offers a flexible building block for future GNN architectures, potentially inspiring new research into adaptive smoothing techniques and their applications across various graph learning domains.\n        *   The demonstrated robustness has implications for developing more secure and reliable graph-based machine learning systems.",
    "intriguing_abstract": "Unlocking the full potential of Graph Neural Networks (GNNs) demands models that can adapt to the inherent heterogeneity of real-world graph signals, moving beyond the uniform smoothness enforced by traditional `L2`-based methods. We introduce a groundbreaking framework that integrates `L1`-based graph smoothing into GNNs for the first time, enabling unprecedented local adaptivity and robust discontinuity preservation.\n\nOur novel **Elastic Graph Signal Estimator** synergistically combines `L1` and `L2` regularization, offering a powerful balance between piecewise constant representations and global smoothness. To efficiently optimize this challenging non-smooth objective, we derive **Elastic Message Passing (EMP)**, a general and back-propagation compatible scheme rooted in primal-dual optimization. EMP features a novel degree-based normalization for the `L1` term and can leverage an `L21` norm for multi-dimensional features, promoting shared sparsity.\n\nEmpirical evaluations on semi-supervised node classification demonstrate that our **Elastic GNNs** achieve superior performance and remarkable robustness against adversarial attacks, outperforming state-of-the-art models. This work provides a theoretically grounded, flexible architecture for learning nuanced graph representations, paving the way for more resilient and adaptable graph-based machine learning systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "L1-based graph smoothing",
      "local smoothness adaptivity",
      "discontinuity preservation",
      "Elastic Graph Signal Estimator",
      "Elastic Message Passing (EMP)",
      "primal-dual optimization",
      "Elastic GNNs",
      "novel L1 normalization",
      "L21 norm",
      "semi-supervised node classification",
      "graph adversarial attacks",
      "robustness to adversarial attacks",
      "convergence guarantee",
      "decoupled propagation-transformation"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf",
    "citation_key": "liu2021ee2",
    "metadata": {
      "title": "Elastic Graph Neural Networks",
      "authors": [
        "Xiaorui Liu",
        "W. Jin",
        "Yao Ma",
        "Yaxin Li",
        "Hua Liu",
        "Yiqi Wang",
        "Ming Yan",
        "Jiliang Tang"
      ],
      "published_date": "2021",
      "abstract": "While many existing graph neural networks (GNNs) have been proven to perform $\\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \\url{https://github.com/lxiaorui/ElasticGNN}.",
      "file_path": "paper_data/Graph_Neural_Networks/140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n### Technical Paper Analysis: Elastic Graph Neural Networks \\cite{liu2021ee2}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Graph Neural Networks (GNNs) predominantly rely on `L2`-based graph smoothing, which enforces a uniform, global level of smoothness across the entire graph. This approach struggles to adapt to scenarios where different regions of a graph require varying degrees of smoothness (e.g., sharp changes between clusters, but smooth within them).\n    *   **Importance & Challenge:** Enhancing the *local smoothness adaptivity* of GNNs is crucial for learning more accurate and robust graph representations. However, incorporating `L1`-based smoothing (known for its ability to preserve discontinuities and promote sparsity) into GNNs presents significant challenges:\n        *   Developing an efficient, scalable optimization solver for the non-smooth objective function.\n        *   Ensuring the derived message passing scheme is compatible with back-propagation training.\n        *   Implementing appropriate normalization for `L1`-based graph difference operators to handle diverse node degrees, a detail often overlooked in prior `L1`-based graph signal processing methods.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Many popular GNNs (e.g., GCN, PPNP, APPNP) are shown to implicitly perform `L2`-based graph signal denoising, which penalizes the squared differences between neighboring node features.\n    *   **Limitations of Previous Solutions:** These `L2`-based methods enforce global smoothness, leading to potential over-smoothing or an inability to capture important discontinuities in graph signals. While `L1`-based methods like Graph Trend Filtering (GTF) exist and demonstrate superior adaptivity and piecewise properties, their integration into the deep learning framework of GNNs has been largely unexplored. `\\cite{liu2021ee2}` bridges this gap by introducing `L1`-based smoothing into GNN design.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** `\\cite{liu2021ee2}` proposes a novel **Elastic Graph Signal Estimator** that combines `L1`-based and `L2`-based graph smoothing terms with a fidelity term. The `L1` term (`\\lambda_1 ||\\tilde{\\Gamma} F||_1` or `\\lambda_1 ||\\tilde{\\Gamma} F||_{2,1}`) promotes sparsity in node differences, enabling adaptive local smoothness and discontinuity preservation, while the `L2` term (`\\lambda_2/2 tr(F^T \\tilde{L} F)`) provides global smoothing. To solve this challenging non-smooth, non-separable optimization problem, the paper derives a novel **Elastic Message Passing (EMP)** scheme. EMP is based on a primal-dual algorithm (inspired by Proximal Alternating Predictor-Corrector, PAPC) that efficiently solves an equivalent saddle point problem.\n    *   **Novelty/Difference:**\n        *   **First Integration of `L1`-based Smoothing in GNNs:** This is the first work to systematically incorporate `L1`-based graph smoothing into the design of GNNs to enhance local adaptivity.\n        *   **Novel Normalization for `L1`:** Introduces a degree-based normalization for the graph incident matrix (`\\tilde{\\Gamma} = \\Gamma \\hat{D}^{-1/2}`) for the `L1` term, crucial for numerical stability and handling diverse node degrees in real-world graphs.\n        *   **`L21` Norm for Multi-dimensional Features:** Offers an `L21` norm option for the `L1` penalty to couple multi-dimensional features, promoting shared sparsity patterns across feature dimensions.\n        *   **Efficient Primal-Dual Message Passing:** The EMP scheme is a novel iterative algorithm designed for efficiency, compatibility with back-propagation, and theoretical convergence guarantees, which can also recover existing `L2`-based GNN propagation rules (like APPNP) as special cases.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Elastic Graph Signal Estimator:** A new objective function that synergistically combines `L1` and `L2` graph smoothing for adaptive local and global smoothness.\n        *   **Elastic Message Passing (EMP):** A novel, general, and efficient message passing algorithm derived from a primal-dual optimization framework, specifically designed to be friendly to back-propagation training.\n    *   **System Design/Architectural Innovations:**\n        *   **Elastic GNNs:** A new family of GNN architectures that integrate the proposed EMP scheme into deep neural networks, following a decoupled propagation-transformation approach.\n    *   **Theoretical Insights/Analysis:**\n        *   **Convergence Guarantee:** Provides a theoretical proof (Theorem 1) ensuring the convergence of the EMP scheme to the optimal solution of the elastic graph signal estimator under specified stepsize conditions.\n        *   **Normalization Justification:** Introduces and theoretically justifies a degree-based normalization for the `L1`-based graph difference operator, maintaining its relation with the normalized Laplacian matrix.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated the proposed Elastic GNNs on semi-supervised node classification tasks using benchmark datasets (e.g., Cora, Citeseer, Pubmed).\n    *   **Key Performance Metrics:** Standard classification accuracy.\n    *   **Comparison Results:**\n        *   Elastic GNNs demonstrate **superior adaptivity** and achieve **better performance** on various real-world datasets compared to existing GNNs.\n        *   The models exhibit **significant robustness to graph adversarial attacks**, outperforming other GNNs in adversarial settings.\n        *   Ablation studies on different variants confirm that `L1`-based and `L2`-based graph smoothing are **complementary**, and their combination in Elastic GNNs leads to more versatile and robust models.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The current work primarily focuses on the 0-th order graph difference operator (`\\Gamma`), which corresponds to a piecewise constant prior for graph representation learning. While the framework is general enough to consider higher-order differences, these are not explored in this paper.\n    *   **Scope of Applicability:** The empirical validation is focused on semi-supervised node classification. While the general EMP scheme is flexible, its applicability and performance on other graph learning tasks (e.g., graph classification, link prediction) are not explicitly demonstrated.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{liu2021ee2}` makes a significant technical advancement by overcoming the limitations of global `L2`-based smoothing in GNNs. By introducing `L1`-based adaptive local smoothing, it enables GNNs to learn more nuanced, piecewise representations that better reflect the inherent heterogeneity of real-world graph signals.\n    *   **Potential Impact:**\n        *   Provides a novel and theoretically grounded framework for designing GNNs that are more robust to noise and adversarial attacks, and better equipped to handle complex graph structures with varying local properties.\n        *   The general EMP scheme offers a flexible building block for future GNN architectures, potentially inspiring new research into adaptive smoothing techniques and their applications across various graph learning domains.\n        *   The demonstrated robustness has implications for developing more secure and reliable graph-based machine learning systems.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "L1-based graph smoothing",
        "local smoothness adaptivity",
        "discontinuity preservation",
        "Elastic Graph Signal Estimator",
        "Elastic Message Passing (EMP)",
        "primal-dual optimization",
        "Elastic GNNs",
        "novel L1 normalization",
        "L21 norm",
        "semi-supervised node classification",
        "graph adversarial attacks",
        "robustness to adversarial attacks",
        "convergence guarantee",
        "decoupled propagation-transformation"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce a family of gnns (elastic gnns)\", \"we propose a novel and general message passing scheme into gnns.\" it describes a new algorithm and system.\n*   the introduction sets up the context for gnns and the message passing framework, leading to the proposed solution.\n*   while it mentions \"theoretical convergence guarantee\" (suggesting a theoretical component) and \"experiments... demonstrate\" (suggesting an empirical component), these are presented as validations and properties of the *newly proposed* methods/algorithms. the core contribution is the development and presentation of these new methods.\n\ntherefore, the paper primarily fits the **technical** classification."
    },
    "file_name": "140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf"
  },
  {
    "success": true,
    "doc_id": "7bc0862b654413ec5bbc2ea73e4721da",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements and bullet format:\n\n*   **CITATION**: \\cite{balcilar20215ga}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Message Passing Neural Networks (MPNNs) are computationally efficient (linear complexity for sparse graphs) but suffer from limited expressive power, theoretically equivalent to at most the 1-Weisfeiler-Lehman (1-WL) test. This limitation prevents them from distinguishing many non-isomorphic graphs and counting simple graph substructures (e.g., triangles, 4-cycles).\n*   **Importance and Challenge**:\n    *   The inability to distinguish non-isomorphic graphs or count basic graphlets hinders MPNNs' performance on various graph-related tasks.\n    *   Existing provably more powerful Graph Neural Networks (GNNs), such as those equivalent to the 3-WL test (e.g., PPGN), achieve higher expressive power but at a prohibitive computational cost (O(n^3) complexity, O(n^2) memory) and often rely on non-local update mechanisms, breaking the locality principle crucial for Euclidean learning.\n    *   Other attempts to enhance MPNNs (e.g., randomization, unique labels, handcrafted features) have their own drawbacks, such as slow convergence, massive training data requirements, or reliance on domain expertise.\n    *   Many MPNNs also act as low-pass filters, further reducing their spectral expressive power.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   The work directly addresses the limitations of standard MPNNs (GCN, GAT, GraphSage, GIN) which are shown to be no more powerful than the 1-WL test \\cite{balcilar20215ga}.\n    *   It positions itself against higher-order GNNs (e.g., PPGN \\cite{balcilar20215ga}, Morris et al., 2019) that achieve 3-WL equivalence but with significantly higher computational and memory costs.\n*   **Limitations of Previous Solutions**:\n    *   **1-WL MPNNs**: Cannot distinguish 1-WL equivalent non-isomorphic graphs (e.g., Decalin and Bicyclopentyl) and fail to count graphlets like triangles or 4-cycles \\cite{balcilar20215ga}.\n    *   **Higher-Order GNNs (e.g., 3-WL equivalent)**: Suffer from O(n^3) computational complexity and O(n^2) memory usage, making them impractical for large graphs. They also typically employ non-local update mechanisms and do not inherently provide spectral richness in their output profiles \\cite{balcilar20215ga}.\n    *   **Other MPNN enhancements**: Approaches like node feature randomization or unique labels require extensive training and slow convergence. Handcrafted features demand domain expertise and feature selection.\n    *   **Spectral limitations**: Most MPNNs function as low-pass filters, limiting their ability to capture diverse frequency components in graph signals \\cite{balcilar20215ga}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes designing graph convolution supports in the spectral domain using **non-linear custom functions of eigenvalues** and masking these supports with an **arbitrarily large receptive field** while maintaining spatial locality \\cite{balcilar20215ga}.\n    *   The general MPNN formulation `H(l+1)= \\sigma(\\sum_s C(s)H(l)W(l;s))` is used, where `C(s)` are convolution supports \\cite{balcilar20215ga}.\n    *   For spectral GNNs, `C(s) = U diag(\\phi_s(\\lambda)) U^T`, where `U` and `\\lambda` are eigenvectors and eigenvalues of the graph Laplacian, and `\\phi_s(\\lambda)` is the custom filter function \\cite{balcilar20215ga}.\n    *   The paper introduces two models:\n        *   **GNNML1**: An MPNN with a richer node update schema including element-wise feature multiplication, shown to be exactly as powerful as the 1-WL test \\cite{balcilar20215ga}.\n        *   **GNNML3**: The primary innovation, which leverages the spectral design. By precomputing the eigendecomposition of the graph Laplacian and designing `\\phi_s(\\lambda)` as custom non-linear functions of eigenvalues, it implicitly incorporates operations like `trace` and `element-wise multiplication` (identified by MATLANG as crucial for exceeding 1-WL) into the convolution process. The convolution supports `C(s)` are then masked to ensure sparsity and spatial locality.\n*   **Novelty/Differentiation**:\n    *   **Spectral Design for Expressivity**: Unlike previous MPNNs that implicitly act as low-pass filters, this approach explicitly designs spectral filters (`\\phi_s(\\lambda)`) to capture various frequency components and implicitly perform higher-order operations.\n    *   **Breaking 1-WL with Efficiency**: It theoretically surpasses the 1-WL expressive power and experimentally matches 3-WL equivalent models, *while retaining linear computational complexity* (after an initial eigendecomposition preprocessing step) and spatially localized updates \\cite{balcilar20215ga}. This is a significant departure from O(n^3) 3-WL models.\n    *   **Rich Output Profile**: The custom filter functions enable the output to have diverse frequency components, offering richer representations than most existing GNNs \\cite{balcilar20215ga}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms/Methods**:\n    *   A novel MPNN architecture (GNNML3) that designs graph convolution supports in the spectral domain using non-linear custom functions of eigenvalues, masked to ensure spatial locality and linear complexity \\cite{balcilar20215ga}.\n    *   Introduction of GNNML1, an MPNN with feature-wise multiplication, provably 1-WL equivalent but with richer node representations \\cite{balcilar20215ga}.\n*   **Theoretical Insights/Analysis**:\n    *   Demonstrates that by leveraging spectral domain design with custom non-linear filters and masked receptive fields, MPNNs can theoretically exceed the 1-WL expressive power \\cite{balcilar20215ga}.\n    *   Utilizes the MATLANG framework to formally characterize the expressive power of various MPNNs and identify the operations (trace, element-wise multiplication) required to surpass 1-WL and reach 3-WL equivalence \\cite{balcilar20215ga}.\n    *   Proves that Chebnet can be more powerful than 1-WL under specific conditions (different maximum eigenvalues) \\cite{balcilar20215ga}.\n    *   Characterizes the MATLANG operations needed to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle) \\cite{balcilar20215ga}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**:\n    *   **Graph Isomorphism Tests**: Evaluated the ability to distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL tests.\n    *   **Substructure Counting**: Tested the models' capacity to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle).\n    *   **Downstream Tasks**: Performance on various benchmark graph learning tasks (though specific datasets/tasks are not detailed in the provided excerpt, the abstract claims state-of-the-art results).\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Distinguishing 1-WL Equivalent Graphs**: GNNML3 successfully distinguishes Decalin and Bicyclopentyl graphs (Figure 1), which are 1-WL equivalent but have different maximum eigenvalues or can be distinguished by `tr(A^5)` \\cite{balcilar20215ga}.\n    *   **Distinguishing Cospectral Graphs**: GNNML3 can distinguish cospectral graphs (Figure 3), which are `L1` and `L2` equivalent, by implicitly leveraging element-wise multiplication operations \\cite{balcilar20215ga}.\n    *   **Substructure Counting**: Unlike 1-WL equivalent MPNNs, GNNML3 is shown to be capable of counting triangles, 4-cycles, and tailed triangles, demonstrating its enhanced expressive power for structural features \\cite{balcilar20215ga}.\n    *   **Overall Performance**: The proposed method achieves state-of-the-art results on many downstream tasks, outperforming existing MPNNs and matching the performance of computationally expensive 3-WL equivalent models \\cite{balcilar20215ga}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The eigendecomposition of the graph Laplacian is a preprocessing step, which itself has a complexity of O(n^3) in the general case. While the subsequent MPNN operations are linear, this initial cost can be significant for very large graphs where even O(n^3) preprocessing is prohibitive \\cite{balcilar20215ga}.\n    *   The \"3-WL equivalent\" claim for GNNML3 is stated as \"experimentally as powerful as a 3-WL existing models,\" implying empirical validation rather than a formal proof of equivalence to the 3-WL test in the same theoretical sense as some O(n^3) models \\cite{balcilar20215ga}.\n*   **Scope of Applicability**:\n    *   The theoretical analysis using MATLANG primarily focuses on undirected graphs with monochromatic edges and nodes for simplicity in proofs \\cite{balcilar20215ga}.\n    *   The method is particularly beneficial for sparse graphs, where the linear complexity of the MPNN operations (after preprocessing) is maintained.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: This paper significantly advances the state-of-the-art by demonstrating a practical method to break the 1-WL expressive power limit of MPNNs without incurring the prohibitive computational costs of existing higher-order GNNs \\cite{balcilar20215ga}. It offers a compelling alternative to O(n^3) 3-WL models.\n*   **Impact on Future Research**:\n    *   **Efficient Powerful GNNs**: It opens new avenues for designing highly expressive yet computationally efficient GNNs by leveraging spectral domain insights and custom filter functions.\n    *   **Spectral Design Importance**: Highlights the critical role of spectral filter design in enhancing GNN expressive power and generating rich output representations.\n    *   **Bridging Theory and Practice**: Provides a practical framework that bridges theoretical insights from WL tests and matrix languages with efficient, localized MPNN architectures.\n    *   **Substructure Learning**: Improves the ability of MPNNs to learn and count complex graph substructures, which is crucial for many real-world applications in chemistry, biology, and social networks.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) face a critical expressivity bottleneck: standard Message Passing Neural Networks (MPNNs) are limited by the 1-Weisfeiler-Lehman (1-WL) test, failing to distinguish many non-isomorphic graphs and count crucial graph substructures. While provably more powerful GNNs exist, they often incur prohibitive O(n^3) computational costs. We introduce GNNML3, a novel MPNN architecture that shatters the 1-WL barrier by ingeniously designing graph convolution supports in the spectral domain. By employing custom non-linear functions of eigenvalues and masking these supports for spatial locality, GNNML3 implicitly performs higher-order operations crucial for enhanced expressivity. Our approach experimentally matches the power of 3-WL equivalent models, yet maintains linear computational complexity for message passing after an initial preprocessing step. We demonstrate GNNML3's superior ability to distinguish 1-WL equivalent and even cospectral graphs, accurately count graphlets like triangles and 4-cycles, and achieve state-of-the-art performance on various downstream tasks. This work bridges the gap between theoretical expressive power and practical efficiency, paving the way for scalable and highly discriminative GNNs.",
    "keywords": [
      "Message Passing Neural Networks (MPNNs)",
      "1-Weisfeiler-Lehman (1-WL) test",
      "graph expressive power",
      "spectral domain design",
      "non-linear eigenvalue functions",
      "linear computational complexity",
      "spatial locality",
      "GNNML3",
      "breaking 1-WL barrier",
      "efficient 3-WL equivalence",
      "graph substructure counting",
      "graph isomorphism",
      "MATLANG framework",
      "eigendecomposition"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/aafe1338caef4682069e92378f1190785ec24c2c.pdf",
    "citation_key": "balcilar20215ga",
    "metadata": {
      "title": "Breaking the Limits of Message Passing Graph Neural Networks",
      "authors": [
        "M. Balcilar",
        "P. Héroux",
        "Benoit Gaüzère",
        "P. Vasseur",
        "Sébastien Adam",
        "P. Honeine"
      ],
      "published_date": "2021",
      "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/aafe1338caef4682069e92378f1190785ec24c2c.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements and bullet format:\n\n*   **CITATION**: \\cite{balcilar20215ga}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Message Passing Neural Networks (MPNNs) are computationally efficient (linear complexity for sparse graphs) but suffer from limited expressive power, theoretically equivalent to at most the 1-Weisfeiler-Lehman (1-WL) test. This limitation prevents them from distinguishing many non-isomorphic graphs and counting simple graph substructures (e.g., triangles, 4-cycles).\n*   **Importance and Challenge**:\n    *   The inability to distinguish non-isomorphic graphs or count basic graphlets hinders MPNNs' performance on various graph-related tasks.\n    *   Existing provably more powerful Graph Neural Networks (GNNs), such as those equivalent to the 3-WL test (e.g., PPGN), achieve higher expressive power but at a prohibitive computational cost (O(n^3) complexity, O(n^2) memory) and often rely on non-local update mechanisms, breaking the locality principle crucial for Euclidean learning.\n    *   Other attempts to enhance MPNNs (e.g., randomization, unique labels, handcrafted features) have their own drawbacks, such as slow convergence, massive training data requirements, or reliance on domain expertise.\n    *   Many MPNNs also act as low-pass filters, further reducing their spectral expressive power.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   The work directly addresses the limitations of standard MPNNs (GCN, GAT, GraphSage, GIN) which are shown to be no more powerful than the 1-WL test \\cite{balcilar20215ga}.\n    *   It positions itself against higher-order GNNs (e.g., PPGN \\cite{balcilar20215ga}, Morris et al., 2019) that achieve 3-WL equivalence but with significantly higher computational and memory costs.\n*   **Limitations of Previous Solutions**:\n    *   **1-WL MPNNs**: Cannot distinguish 1-WL equivalent non-isomorphic graphs (e.g., Decalin and Bicyclopentyl) and fail to count graphlets like triangles or 4-cycles \\cite{balcilar20215ga}.\n    *   **Higher-Order GNNs (e.g., 3-WL equivalent)**: Suffer from O(n^3) computational complexity and O(n^2) memory usage, making them impractical for large graphs. They also typically employ non-local update mechanisms and do not inherently provide spectral richness in their output profiles \\cite{balcilar20215ga}.\n    *   **Other MPNN enhancements**: Approaches like node feature randomization or unique labels require extensive training and slow convergence. Handcrafted features demand domain expertise and feature selection.\n    *   **Spectral limitations**: Most MPNNs function as low-pass filters, limiting their ability to capture diverse frequency components in graph signals \\cite{balcilar20215ga}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes designing graph convolution supports in the spectral domain using **non-linear custom functions of eigenvalues** and masking these supports with an **arbitrarily large receptive field** while maintaining spatial locality \\cite{balcilar20215ga}.\n    *   The general MPNN formulation `H(l+1)= \\sigma(\\sum_s C(s)H(l)W(l;s))` is used, where `C(s)` are convolution supports \\cite{balcilar20215ga}.\n    *   For spectral GNNs, `C(s) = U diag(\\phi_s(\\lambda)) U^T`, where `U` and `\\lambda` are eigenvectors and eigenvalues of the graph Laplacian, and `\\phi_s(\\lambda)` is the custom filter function \\cite{balcilar20215ga}.\n    *   The paper introduces two models:\n        *   **GNNML1**: An MPNN with a richer node update schema including element-wise feature multiplication, shown to be exactly as powerful as the 1-WL test \\cite{balcilar20215ga}.\n        *   **GNNML3**: The primary innovation, which leverages the spectral design. By precomputing the eigendecomposition of the graph Laplacian and designing `\\phi_s(\\lambda)` as custom non-linear functions of eigenvalues, it implicitly incorporates operations like `trace` and `element-wise multiplication` (identified by MATLANG as crucial for exceeding 1-WL) into the convolution process. The convolution supports `C(s)` are then masked to ensure sparsity and spatial locality.\n*   **Novelty/Differentiation**:\n    *   **Spectral Design for Expressivity**: Unlike previous MPNNs that implicitly act as low-pass filters, this approach explicitly designs spectral filters (`\\phi_s(\\lambda)`) to capture various frequency components and implicitly perform higher-order operations.\n    *   **Breaking 1-WL with Efficiency**: It theoretically surpasses the 1-WL expressive power and experimentally matches 3-WL equivalent models, *while retaining linear computational complexity* (after an initial eigendecomposition preprocessing step) and spatially localized updates \\cite{balcilar20215ga}. This is a significant departure from O(n^3) 3-WL models.\n    *   **Rich Output Profile**: The custom filter functions enable the output to have diverse frequency components, offering richer representations than most existing GNNs \\cite{balcilar20215ga}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms/Methods**:\n    *   A novel MPNN architecture (GNNML3) that designs graph convolution supports in the spectral domain using non-linear custom functions of eigenvalues, masked to ensure spatial locality and linear complexity \\cite{balcilar20215ga}.\n    *   Introduction of GNNML1, an MPNN with feature-wise multiplication, provably 1-WL equivalent but with richer node representations \\cite{balcilar20215ga}.\n*   **Theoretical Insights/Analysis**:\n    *   Demonstrates that by leveraging spectral domain design with custom non-linear filters and masked receptive fields, MPNNs can theoretically exceed the 1-WL expressive power \\cite{balcilar20215ga}.\n    *   Utilizes the MATLANG framework to formally characterize the expressive power of various MPNNs and identify the operations (trace, element-wise multiplication) required to surpass 1-WL and reach 3-WL equivalence \\cite{balcilar20215ga}.\n    *   Proves that Chebnet can be more powerful than 1-WL under specific conditions (different maximum eigenvalues) \\cite{balcilar20215ga}.\n    *   Characterizes the MATLANG operations needed to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle) \\cite{balcilar20215ga}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**:\n    *   **Graph Isomorphism Tests**: Evaluated the ability to distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL tests.\n    *   **Substructure Counting**: Tested the models' capacity to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle).\n    *   **Downstream Tasks**: Performance on various benchmark graph learning tasks (though specific datasets/tasks are not detailed in the provided excerpt, the abstract claims state-of-the-art results).\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Distinguishing 1-WL Equivalent Graphs**: GNNML3 successfully distinguishes Decalin and Bicyclopentyl graphs (Figure 1), which are 1-WL equivalent but have different maximum eigenvalues or can be distinguished by `tr(A^5)` \\cite{balcilar20215ga}.\n    *   **Distinguishing Cospectral Graphs**: GNNML3 can distinguish cospectral graphs (Figure 3), which are `L1` and `L2` equivalent, by implicitly leveraging element-wise multiplication operations \\cite{balcilar20215ga}.\n    *   **Substructure Counting**: Unlike 1-WL equivalent MPNNs, GNNML3 is shown to be capable of counting triangles, 4-cycles, and tailed triangles, demonstrating its enhanced expressive power for structural features \\cite{balcilar20215ga}.\n    *   **Overall Performance**: The proposed method achieves state-of-the-art results on many downstream tasks, outperforming existing MPNNs and matching the performance of computationally expensive 3-WL equivalent models \\cite{balcilar20215ga}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The eigendecomposition of the graph Laplacian is a preprocessing step, which itself has a complexity of O(n^3) in the general case. While the subsequent MPNN operations are linear, this initial cost can be significant for very large graphs where even O(n^3) preprocessing is prohibitive \\cite{balcilar20215ga}.\n    *   The \"3-WL equivalent\" claim for GNNML3 is stated as \"experimentally as powerful as a 3-WL existing models,\" implying empirical validation rather than a formal proof of equivalence to the 3-WL test in the same theoretical sense as some O(n^3) models \\cite{balcilar20215ga}.\n*   **Scope of Applicability**:\n    *   The theoretical analysis using MATLANG primarily focuses on undirected graphs with monochromatic edges and nodes for simplicity in proofs \\cite{balcilar20215ga}.\n    *   The method is particularly beneficial for sparse graphs, where the linear complexity of the MPNN operations (after preprocessing) is maintained.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: This paper significantly advances the state-of-the-art by demonstrating a practical method to break the 1-WL expressive power limit of MPNNs without incurring the prohibitive computational costs of existing higher-order GNNs \\cite{balcilar20215ga}. It offers a compelling alternative to O(n^3) 3-WL models.\n*   **Impact on Future Research**:\n    *   **Efficient Powerful GNNs**: It opens new avenues for designing highly expressive yet computationally efficient GNNs by leveraging spectral domain insights and custom filter functions.\n    *   **Spectral Design Importance**: Highlights the critical role of spectral filter design in enhancing GNN expressive power and generating rich output representations.\n    *   **Bridging Theory and Practice**: Provides a practical framework that bridges theoretical insights from WL tests and matrix languages with efficient, localized MPNN architectures.\n    *   **Substructure Learning**: Improves the ability of MPNNs to learn and count complex graph substructures, which is crucial for many real-world applications in chemistry, biology, and social networks.",
      "keywords": [
        "Message Passing Neural Networks (MPNNs)",
        "1-Weisfeiler-Lehman (1-WL) test",
        "graph expressive power",
        "spectral domain design",
        "non-linear eigenvalue functions",
        "linear computational complexity",
        "spatial locality",
        "GNNML3",
        "breaking 1-WL barrier",
        "efficient 3-WL equivalence",
        "graph substructure counting",
        "graph isomorphism",
        "MATLANG framework",
        "eigendecomposition"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function... the mpnn is theoretically more powerful...\" and \"the proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.\"\n*   the introduction sets up the problem with existing message passing neural networks (mpnns) and their limitations, implying the paper will present a solution.\n*   key phrases like \"designed in spectral-domain by a non-linear custom function\", \"proposed method\", and \"overcomes all these aforementioned problems\" strongly indicate the development and presentation of a new approach or system.\n*   while it mentions \"theoretically more powerful\" (theoretical aspect) and \"experimentally as powerful\" and \"state-of-the-art results\" (empirical aspect), these are justifications and evaluations of the *new method* being presented.\n\ntherefore, the primary classification is **technical**."
    },
    "file_name": "aafe1338caef4682069e92378f1190785ec24c2c.pdf"
  },
  {
    "success": true,
    "doc_id": "63b35cf56150319873cb6fe8ef5ddb36",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Strategies for Pre-training Graph Neural Networks \\cite{hu2019r47}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: How to effectively pre-train Graph Neural Networks (GNNs) for transfer learning, especially when downstream tasks have scarce labeled data and involve out-of-distribution test examples.\n    *   **Importance & Challenge**:\n        *   Labeled graph data, particularly in scientific domains (chemistry, biology), is often extremely scarce and expensive to obtain.\n        *   Real-world graph applications frequently involve out-of-distribution samples, where test graphs are structurally different from training graphs.\n        *   Naive pre-training strategies can lead to \"negative transfer,\" harming generalization performance on downstream tasks, which limits the reliability and applicability of pre-trained GNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While deep transfer learning and pre-training have been highly successful in computer vision and natural language processing, few studies have effectively generalized these techniques to graph data.\n    *   **Limitations of Previous Solutions**:\n        *   Existing GNN pre-training efforts are limited, and a systematic large-scale investigation of strategies was lacking.\n        *   Naive pre-training strategies, such as solely focusing on graph-level or node-level properties, often yield only marginal improvements and can even cause negative transfer on many downstream tasks. For instance, extensive graph-level multi-task supervised pre-training, despite using state-of-the-art GNN architectures, showed limited gains and negative transfer on a significant number of tasks \\cite{hu2019r47}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel pre-training strategy that simultaneously pre-trains an expressive GNN at both the individual node level and the entire graph level. This approach aims to learn useful local (node-level semantics) and global (graph-level properties) representations concurrently.\n    *   **Novelty/Difference**:\n        *   **Combined Node- and Graph-level Pre-training**: Unlike naive strategies that focus on one level, this work emphasizes the synergy between learning meaningful node embeddings and composable graph embeddings (Figure 1 (a.iii) in \\cite{hu2019r47}).\n        *   **Self-supervised Node-level Methods**: Introduces two novel self-supervised methods for node-level pre-training:\n            *   **Context Prediction**: Trains the GNN to predict the surrounding graph structure (context graph) of a node's K-hop neighborhood. It uses an auxiliary GNN to encode context graphs into fixed-length vectors and employs negative sampling for learning.\n            *   **Attribute Masking**: Randomly masks node and/or edge attributes (e.g., atom types) and trains the GNN to predict these masked attributes based on the neighboring structure. This helps capture domain-specific regularities like chemical valency or interaction types.\n        *   **Regularized Graph-level Pre-training**: Integrates multi-task supervised graph-level property prediction, but crucially, it regularizes the GNN first with the proposed node-level pre-training methods. This prevents negative transfer often seen with standalone graph-level pre-training by ensuring the underlying node embeddings are meaningful.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Context Prediction**: A self-supervised method for node-level pre-training that learns to map nodes in similar structural contexts to nearby embeddings by predicting surrounding graph structures.\n        *   **Attribute Masking**: A self-supervised method for node/edge attribute prediction on masked inputs, enabling GNNs to learn domain-specific regularities from attribute distributions.\n    *   **System Design/Architectural Innovations**: A holistic pre-training strategy that combines these self-supervised node-level tasks with supervised graph-level tasks, ensuring robust and transferable representations at both local and global scales.\n    *   **Empirical Insights**: First systematic large-scale investigation of GNN pre-training strategies, demonstrating that naive approaches can lead to negative transfer and that a combined strategy is essential.\n    *   **Dataset Contribution**: Creation and release of two large new pre-training datasets (2 million chemistry graphs, 395K biology graphs) to facilitate further research.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Systematic study on multiple graph classification datasets, focusing on molecular property prediction and protein function prediction.\n    *   **Key Performance Metrics**: ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n    *   **Comparison Results**:\n        *   **Significant Generalization Improvement**: Achieved up to 9.4% absolute improvements in ROC-AUC over non-pre-trained GNN models.\n        *   **Superior to Naive Strategies**: Outperformed GNNs with extensive graph-level multi-task supervised pre-training by up to 5.2% higher average ROC-AUC.\n        *   **State-of-the-Art (SOTA)**: Achieved SOTA performance for molecular property prediction and protein function prediction.\n        *   **Negative Transfer Avoidance**: The proposed strategy successfully avoided negative transfer across all tested downstream tasks, unlike naive strategies which showed negative transfer on many tasks (e.g., 2 out of 8 molecular datasets and 13 out of 40 protein prediction tasks).\n        *   **Architecture Benefits**: More expressive GNN architectures (e.g., GIN) benefited more from pre-training.\n        *   **Efficiency**: Pre-training led to orders-of-magnitude faster training and convergence during the fine-tuning stage.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The paper notes that structural similarity prediction as a graph-level pre-training task (e.g., modeling graph edit distance) was considered but left for future work due to the difficulty in obtaining ground truth distances and the computational complexity for large datasets \\cite{hu2019r47}.\n    *   **Scope of Applicability**: The methods are primarily validated on molecular and protein graphs, which are richly annotated and benefit from the attribute masking approach. While the general strategy is applicable to other graph types, its effectiveness might vary depending on the availability and richness of node/edge attributes and structural contexts.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work provides the first systematic and large-scale investigation into GNN pre-training strategies, establishing a robust and effective approach that significantly advances the state-of-the-art in graph-level property prediction.\n    *   **Addressing Negative Transfer**: It offers a crucial solution to the pervasive problem of negative transfer in GNN transfer learning, making pre-trained GNNs more reliable and broadly applicable.\n    *   **Impact on Future Research**: The proposed combined node- and graph-level pre-training strategy, along with the self-supervised methods (Context Prediction, Attribute Masking), provides a strong foundation and benchmark for future research in GNN pre-training, especially for domains with scarce labeled data and out-of-distribution challenges. The released datasets also enable further exploration.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) hold immense promise across scientific domains, yet their full potential is often hampered by scarce labeled data, out-of-distribution challenges, and the pervasive problem of \"negative transfer\" during pre-training. We introduce a novel, systematic pre-training strategy designed to learn robust and transferable representations for GNNs, overcoming these critical limitations.\n\nOur approach uniquely combines self-supervised node-level and supervised graph-level pre-training, fostering a synergy that captures both local semantics and global properties. We propose two innovative self-supervised methods: **Context Prediction**, which learns node embeddings by predicting surrounding graph structures, and **Attribute Masking**, which captures domain-specific regularities by reconstructing masked node/edge attributes. These methods crucially regularize the GNN, preventing the negative transfer often observed with standalone graph-level pre-training.\n\nExtensive experiments on molecular and protein datasets demonstrate remarkable improvements, achieving up to 9.4% absolute ROC-AUC gains over non-pre-trained models and outperforming naive strategies by 5.2%. Our method consistently avoids negative transfer, establishes new state-of-the-art performance, and enables orders-of-magnitude faster fine-tuning. This work provides a critical advancement for GNN transfer learning, making pre-trained models reliable and broadly applicable, especially in data-scarce scientific domains.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN Pre-training Strategies",
      "Transfer Learning",
      "Negative Transfer Avoidance",
      "Combined Node- and Graph-level Pre-training",
      "Self-supervised Context Prediction",
      "Self-supervised Attribute Masking",
      "Molecular Property Prediction",
      "Protein Function Prediction",
      "Scarce Labeled Data",
      "Out-of-distribution Generalization",
      "State-of-the-Art Performance",
      "Large-scale Empirical Investigation",
      "Node and Graph Embeddings"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf",
    "citation_key": "hu2019r47",
    "metadata": {
      "title": "Strategies for Pre-training Graph Neural Networks",
      "authors": [
        "Weihua Hu",
        "Bowen Liu",
        "Joseph Gomes",
        "M. Zitnik",
        "Percy Liang",
        "V. Pande",
        "J. Leskovec"
      ],
      "published_date": "2019",
      "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.",
      "file_path": "paper_data/Graph_Neural_Networks/789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Strategies for Pre-training Graph Neural Networks \\cite{hu2019r47}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: How to effectively pre-train Graph Neural Networks (GNNs) for transfer learning, especially when downstream tasks have scarce labeled data and involve out-of-distribution test examples.\n    *   **Importance & Challenge**:\n        *   Labeled graph data, particularly in scientific domains (chemistry, biology), is often extremely scarce and expensive to obtain.\n        *   Real-world graph applications frequently involve out-of-distribution samples, where test graphs are structurally different from training graphs.\n        *   Naive pre-training strategies can lead to \"negative transfer,\" harming generalization performance on downstream tasks, which limits the reliability and applicability of pre-trained GNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While deep transfer learning and pre-training have been highly successful in computer vision and natural language processing, few studies have effectively generalized these techniques to graph data.\n    *   **Limitations of Previous Solutions**:\n        *   Existing GNN pre-training efforts are limited, and a systematic large-scale investigation of strategies was lacking.\n        *   Naive pre-training strategies, such as solely focusing on graph-level or node-level properties, often yield only marginal improvements and can even cause negative transfer on many downstream tasks. For instance, extensive graph-level multi-task supervised pre-training, despite using state-of-the-art GNN architectures, showed limited gains and negative transfer on a significant number of tasks \\cite{hu2019r47}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel pre-training strategy that simultaneously pre-trains an expressive GNN at both the individual node level and the entire graph level. This approach aims to learn useful local (node-level semantics) and global (graph-level properties) representations concurrently.\n    *   **Novelty/Difference**:\n        *   **Combined Node- and Graph-level Pre-training**: Unlike naive strategies that focus on one level, this work emphasizes the synergy between learning meaningful node embeddings and composable graph embeddings (Figure 1 (a.iii) in \\cite{hu2019r47}).\n        *   **Self-supervised Node-level Methods**: Introduces two novel self-supervised methods for node-level pre-training:\n            *   **Context Prediction**: Trains the GNN to predict the surrounding graph structure (context graph) of a node's K-hop neighborhood. It uses an auxiliary GNN to encode context graphs into fixed-length vectors and employs negative sampling for learning.\n            *   **Attribute Masking**: Randomly masks node and/or edge attributes (e.g., atom types) and trains the GNN to predict these masked attributes based on the neighboring structure. This helps capture domain-specific regularities like chemical valency or interaction types.\n        *   **Regularized Graph-level Pre-training**: Integrates multi-task supervised graph-level property prediction, but crucially, it regularizes the GNN first with the proposed node-level pre-training methods. This prevents negative transfer often seen with standalone graph-level pre-training by ensuring the underlying node embeddings are meaningful.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Context Prediction**: A self-supervised method for node-level pre-training that learns to map nodes in similar structural contexts to nearby embeddings by predicting surrounding graph structures.\n        *   **Attribute Masking**: A self-supervised method for node/edge attribute prediction on masked inputs, enabling GNNs to learn domain-specific regularities from attribute distributions.\n    *   **System Design/Architectural Innovations**: A holistic pre-training strategy that combines these self-supervised node-level tasks with supervised graph-level tasks, ensuring robust and transferable representations at both local and global scales.\n    *   **Empirical Insights**: First systematic large-scale investigation of GNN pre-training strategies, demonstrating that naive approaches can lead to negative transfer and that a combined strategy is essential.\n    *   **Dataset Contribution**: Creation and release of two large new pre-training datasets (2 million chemistry graphs, 395K biology graphs) to facilitate further research.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Systematic study on multiple graph classification datasets, focusing on molecular property prediction and protein function prediction.\n    *   **Key Performance Metrics**: ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n    *   **Comparison Results**:\n        *   **Significant Generalization Improvement**: Achieved up to 9.4% absolute improvements in ROC-AUC over non-pre-trained GNN models.\n        *   **Superior to Naive Strategies**: Outperformed GNNs with extensive graph-level multi-task supervised pre-training by up to 5.2% higher average ROC-AUC.\n        *   **State-of-the-Art (SOTA)**: Achieved SOTA performance for molecular property prediction and protein function prediction.\n        *   **Negative Transfer Avoidance**: The proposed strategy successfully avoided negative transfer across all tested downstream tasks, unlike naive strategies which showed negative transfer on many tasks (e.g., 2 out of 8 molecular datasets and 13 out of 40 protein prediction tasks).\n        *   **Architecture Benefits**: More expressive GNN architectures (e.g., GIN) benefited more from pre-training.\n        *   **Efficiency**: Pre-training led to orders-of-magnitude faster training and convergence during the fine-tuning stage.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The paper notes that structural similarity prediction as a graph-level pre-training task (e.g., modeling graph edit distance) was considered but left for future work due to the difficulty in obtaining ground truth distances and the computational complexity for large datasets \\cite{hu2019r47}.\n    *   **Scope of Applicability**: The methods are primarily validated on molecular and protein graphs, which are richly annotated and benefit from the attribute masking approach. While the general strategy is applicable to other graph types, its effectiveness might vary depending on the availability and richness of node/edge attributes and structural contexts.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work provides the first systematic and large-scale investigation into GNN pre-training strategies, establishing a robust and effective approach that significantly advances the state-of-the-art in graph-level property prediction.\n    *   **Addressing Negative Transfer**: It offers a crucial solution to the pervasive problem of negative transfer in GNN transfer learning, making pre-trained GNNs more reliable and broadly applicable.\n    *   **Impact on Future Research**: The proposed combined node- and graph-level pre-training strategy, along with the self-supervised methods (Context Prediction, Attribute Masking), provides a strong foundation and benchmark for future research in GNN pre-training, especially for domains with scarce labeled data and out-of-distribution challenges. The released datasets also enable further exploration.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN Pre-training Strategies",
        "Transfer Learning",
        "Negative Transfer Avoidance",
        "Combined Node- and Graph-level Pre-training",
        "Self-supervised Context Prediction",
        "Self-supervised Attribute Masking",
        "Molecular Property Prediction",
        "Protein Function Prediction",
        "Scarce Labeled Data",
        "Out-of-distribution Generalization",
        "State-of-the-Art Performance",
        "Large-scale Empirical Investigation",
        "Node and Graph Embeddings"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **technical** paper.\n\nhere's why:\n\n*   **abstract mentions:** \"we **develop a new strategy and self-supervised methods** for pre-training graph neural networks (gnns).\" this directly aligns with the \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" keywords for technical papers.\n*   **introduction discusses:** the introduction clearly outlines a \"technical problem\" (how to effectively use pre-training on graph datasets) and then immediately presents their \"proposed solution\" (their new strategy and self-supervised methods).\n*   while it also mentions \"systematically study pre-training on multiple graph classiﬁcation datasets\" and \"we ﬁnd that...\", which are indicative of empirical work, the *primary contribution* highlighted is the *development* of the new strategy and methods. the empirical study serves to validate these new technical contributions."
    },
    "file_name": "789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf"
  },
  {
    "success": true,
    "doc_id": "2522823149989f651c9f7a97e4296e12",
    "summary": "Here's a focused summary of the paper \"Graph Neural Networks for Link Prediction with Subgraph Sketching\" \\cite{chamberlain2022fym} for a literature review:\n\n---\n\n*   **CITATION**: \\cite{chamberlain2022fym}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs), particularly Message Passing GNNs (MPNNs), perform poorly on Link Prediction (LP) tasks compared to simple heuristics. State-of-the-art subgraph-based GNNs (SGNNs) achieve high performance but suffer from severe inefficiency due to expensive subgraph construction, irregular subgraph batching, and high inference costs.\n    *   **Importance and Challenge**: LP is crucial for applications like recommender systems, drug discovery, and knowledge graph construction. The challenge lies in developing GNNs that are both expressive enough to capture complex link patterns (like triangle counts and distinguishing automorphic nodes) and computationally efficient and scalable for large graphs, overcoming the limitations of both MPNNs and SGNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and critically analyzes Subgraph GNNs (SGNNs) \\cite{chamberlain2022fym}, which convert LP into binary subgraph classification and augment node features with structural information (e.g., Zero-One, Double Radius Node Labeling (DRNL), Distance Encoding (DE)). It also contrasts with traditional MPNNs, which are limited by their equivalence to the Weisfeiler-Leman (WL) test.\n    *   **Limitations of Previous Solutions**:\n        *   **MPNNs**: Provably incapable of counting triangles or distinguishing automorphic nodes (nodes with identical structural roles), leading to poor LP performance \\cite{chamberlain2022fym}.\n        *   **SGNNs**: While achieving state-of-the-art accuracy, they are highly inefficient. Subgraph construction is expensive (O(deg^k) or O(|E|)), batching irregular subgraphs is inefficient on GPUs, and inference costs are nearly as high as training, precluding scalability \\cite{chamberlain2022fym}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes two novel full-graph GNN models: ELPH (Efficient Link Prediction with Hashing) and BUDDY.\n        *   **ELPH**: A full-graph MPNN that approximates the key components of SGNNs (specifically, structure feature counts like DRNL/DE labels) by passing \"subgraph sketches\" as messages between nodes. These sketches are compact representations of node neighborhoods, estimated using HyperLogLog and MinHashing techniques to approximate set intersections and cardinalities \\cite{chamberlain2022fym}. This avoids explicit subgraph construction.\n        *   **BUDDY**: An extension of ELPH designed for scalability when data exceeds GPU memory. It precomputes the subgraph sketches and node features, allowing for highly scalable training and inference without sacrificing predictive performance \\cite{chamberlain2022fym}.\n    *   **Novelty/Difference**:\n        *   **Subgraph Sketching**: Instead of constructing explicit subgraphs for each link, \\cite{chamberlain2022fym} introduces the novel idea of summarizing crucial subgraph properties (like counts of specific distance-based node labels) into compact, node-wise \"sketches.\" These sketches are then propagated via message passing, effectively embedding subgraph-level information into node representations.\n        *   **Full-Graph GNN for SGNN Expressivity**: ELPH achieves the expressive power of SGNNs (solving the automorphic node problem and enabling triangle counting) within a full-graph GNN framework, making it significantly faster and more efficient than SGNNs \\cite{chamberlain2022fym}.\n        *   **Scalability for Large Graphs**: BUDDY addresses the common GNN limitation of requiring data to fit in GPU memory by precomputing features, enabling application to very large datasets.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **ELPH (Efficient Link Prediction with Hashing)**: A full-graph MPNN that uses node-wise subgraph sketches (based on HyperLogLog and MinHashing) to approximate structure feature counts, enabling SGNN-level expressivity without explicit subgraph construction \\cite{chamberlain2022fym}.\n        *   **BUDDY**: A highly scalable model that precomputes these subgraph sketches and node features, addressing memory limitations for large datasets \\cite{chamberlain2022fym}.\n    *   **System Design/Architectural Innovations**: The concept of \"subgraph sketching\" as a message-passing mechanism to encode complex structural information (like DE/DRNL counts) into node features, effectively bridging the gap between the expressivity of SGNNs and the efficiency of full-graph MPNNs \\cite{chamberlain2022fym}.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of SGNN components revealing that structure features are crucial, their propagation via GNNs is often sub-optimal, and edge-level readout functions are superior \\cite{chamberlain2022fym}.\n        *   Proof that ELPH is strictly more expressive than standard MPNNs for LP and effectively solves the automorphic node problem \\cite{chamberlain2022fym}.\n    *   **Open-Source Library**: Provision of an open-source PyTorch library for (sub)graph sketching, facilitating further research and application \\cite{chamberlain2022fym}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The models (ELPH and BUDDY) were evaluated on standard LP benchmarks, including Cora, Citeseer, and Pubmed datasets. Ablation studies were performed on SGNN components to understand the importance of structure features, propagation mechanisms, and readout functions \\cite{chamberlain2022fym}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: ELPH and BUDDY consistently outperform existing SGNN models on many standard LP benchmarks \\cite{chamberlain2022fym}.\n        *   **Speed/Efficiency**: ELPH is orders of magnitude faster than SGNNs. BUDDY is even faster than ELPH and highly scalable, demonstrating superior efficiency while maintaining or improving predictive performance \\cite{chamberlain2022fym}.\n        *   **Ablation Findings**: The analysis showed that structure features significantly improve performance, that an MLP on structure feature counts can outperform SGNN propagation, and that edge-level readout functions are more effective than graph pooling \\cite{chamberlain2022fym}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **ELPH**: Shares the common GNN limitation that it is only efficient when the entire dataset fits into GPU memory \\cite{chamberlain2022fym}.\n        *   **Sketching Accuracy**: The accuracy of structure feature approximation depends on the parameters of HyperLogLog and MinHashing, introducing a trade-off between speed and precision \\cite{chamberlain2022fym}.\n    *   **Scope of Applicability**: The methods are primarily designed for link prediction tasks on static graphs. While BUDDY addresses scalability for large graphs, the core sketching mechanism is tailored to approximating specific structural features relevant to LP.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chamberlain2022fym} significantly advances the state-of-the-art in GNN-based link prediction by resolving the long-standing trade-off between expressivity (achieved by SGNNs) and efficiency/scalability (lacking in SGNNs). It demonstrates that complex structural information can be efficiently encoded and propagated without explicit subgraph construction.\n    *   **Potential Impact on Future Research**:\n        *   **Efficient Expressive GNNs**: The subgraph sketching paradigm could inspire new designs for expressive GNNs that overcome WL limitations without incurring high computational costs, potentially extending to other graph tasks beyond LP.\n        *   **Scalable GNN Architectures**: BUDDY's precomputation strategy offers a blueprint for developing highly scalable GNNs for very large graphs, which is critical for real-world applications.\n        *   **Theoretical Understanding**: The analysis of SGNN components provides valuable insights into what makes GNNs effective for LP, guiding future research in designing more targeted and efficient models.",
    "intriguing_abstract": "The promise of Graph Neural Networks (GNNs) for Link Prediction (LP) has long been hampered by a critical trade-off: standard Message Passing GNNs (MPNNs) lack the expressivity to capture complex structural patterns, while state-of-the-art Subgraph GNNs (SGNNs) achieve high accuracy but are prohibitively inefficient. We introduce a paradigm shift with **ELPH (Efficient Link Prediction with Hashing)** and **BUDDY**, novel full-graph GNNs that elegantly resolve this dilemma.\n\nOur core innovation is **subgraph sketching**, a technique that approximates crucial structural features—like those from Distance Encoding (DE) or Double Radius Node Labeling (DRNL)—using compact, node-wise sketches derived from **HyperLogLog** and **MinHashing**. This allows ELPH to achieve the expressive power of SGNNs, provably solving the automorphic node problem and enabling triangle counting, without the computational burden of explicit subgraph construction. For massive graphs, BUDDY extends this by precomputing sketches, offering unparalleled **scalability** by decoupling feature generation from training. ELPH and BUDDY consistently outperform existing SGNNs in accuracy while being orders of magnitude faster, making them practical for real-world applications in recommender systems and drug discovery. This work redefines efficient and expressive GNNs for LP, providing an open-source library for broader impact.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Link Prediction",
      "Subgraph Sketching",
      "ELPH",
      "BUDDY",
      "Computational Efficiency",
      "Scalability",
      "Expressivity",
      "Message Passing GNNs (MPNNs)",
      "Subgraph GNNs (SGNNs)",
      "HyperLogLog",
      "MinHashing",
      "Automorphic Node Problem",
      "Full-Graph GNNs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf",
    "citation_key": "chamberlain2022fym",
    "metadata": {
      "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
      "authors": [
        "B. Chamberlain",
        "S. Shirobokov",
        "Emanuele Rossi",
        "Fabrizio Frasca",
        "Thomas Markovich",
        "Nils Y. Hammerla",
        "Michael M. Bronstein",
        "Max Hansmire"
      ],
      "published_date": "2022",
      "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.",
      "file_path": "paper_data/Graph_Neural_Networks/68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Graph Neural Networks for Link Prediction with Subgraph Sketching\" \\cite{chamberlain2022fym} for a literature review:\n\n---\n\n*   **CITATION**: \\cite{chamberlain2022fym}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs), particularly Message Passing GNNs (MPNNs), perform poorly on Link Prediction (LP) tasks compared to simple heuristics. State-of-the-art subgraph-based GNNs (SGNNs) achieve high performance but suffer from severe inefficiency due to expensive subgraph construction, irregular subgraph batching, and high inference costs.\n    *   **Importance and Challenge**: LP is crucial for applications like recommender systems, drug discovery, and knowledge graph construction. The challenge lies in developing GNNs that are both expressive enough to capture complex link patterns (like triangle counts and distinguishing automorphic nodes) and computationally efficient and scalable for large graphs, overcoming the limitations of both MPNNs and SGNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and critically analyzes Subgraph GNNs (SGNNs) \\cite{chamberlain2022fym}, which convert LP into binary subgraph classification and augment node features with structural information (e.g., Zero-One, Double Radius Node Labeling (DRNL), Distance Encoding (DE)). It also contrasts with traditional MPNNs, which are limited by their equivalence to the Weisfeiler-Leman (WL) test.\n    *   **Limitations of Previous Solutions**:\n        *   **MPNNs**: Provably incapable of counting triangles or distinguishing automorphic nodes (nodes with identical structural roles), leading to poor LP performance \\cite{chamberlain2022fym}.\n        *   **SGNNs**: While achieving state-of-the-art accuracy, they are highly inefficient. Subgraph construction is expensive (O(deg^k) or O(|E|)), batching irregular subgraphs is inefficient on GPUs, and inference costs are nearly as high as training, precluding scalability \\cite{chamberlain2022fym}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes two novel full-graph GNN models: ELPH (Efficient Link Prediction with Hashing) and BUDDY.\n        *   **ELPH**: A full-graph MPNN that approximates the key components of SGNNs (specifically, structure feature counts like DRNL/DE labels) by passing \"subgraph sketches\" as messages between nodes. These sketches are compact representations of node neighborhoods, estimated using HyperLogLog and MinHashing techniques to approximate set intersections and cardinalities \\cite{chamberlain2022fym}. This avoids explicit subgraph construction.\n        *   **BUDDY**: An extension of ELPH designed for scalability when data exceeds GPU memory. It precomputes the subgraph sketches and node features, allowing for highly scalable training and inference without sacrificing predictive performance \\cite{chamberlain2022fym}.\n    *   **Novelty/Difference**:\n        *   **Subgraph Sketching**: Instead of constructing explicit subgraphs for each link, \\cite{chamberlain2022fym} introduces the novel idea of summarizing crucial subgraph properties (like counts of specific distance-based node labels) into compact, node-wise \"sketches.\" These sketches are then propagated via message passing, effectively embedding subgraph-level information into node representations.\n        *   **Full-Graph GNN for SGNN Expressivity**: ELPH achieves the expressive power of SGNNs (solving the automorphic node problem and enabling triangle counting) within a full-graph GNN framework, making it significantly faster and more efficient than SGNNs \\cite{chamberlain2022fym}.\n        *   **Scalability for Large Graphs**: BUDDY addresses the common GNN limitation of requiring data to fit in GPU memory by precomputing features, enabling application to very large datasets.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **ELPH (Efficient Link Prediction with Hashing)**: A full-graph MPNN that uses node-wise subgraph sketches (based on HyperLogLog and MinHashing) to approximate structure feature counts, enabling SGNN-level expressivity without explicit subgraph construction \\cite{chamberlain2022fym}.\n        *   **BUDDY**: A highly scalable model that precomputes these subgraph sketches and node features, addressing memory limitations for large datasets \\cite{chamberlain2022fym}.\n    *   **System Design/Architectural Innovations**: The concept of \"subgraph sketching\" as a message-passing mechanism to encode complex structural information (like DE/DRNL counts) into node features, effectively bridging the gap between the expressivity of SGNNs and the efficiency of full-graph MPNNs \\cite{chamberlain2022fym}.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of SGNN components revealing that structure features are crucial, their propagation via GNNs is often sub-optimal, and edge-level readout functions are superior \\cite{chamberlain2022fym}.\n        *   Proof that ELPH is strictly more expressive than standard MPNNs for LP and effectively solves the automorphic node problem \\cite{chamberlain2022fym}.\n    *   **Open-Source Library**: Provision of an open-source PyTorch library for (sub)graph sketching, facilitating further research and application \\cite{chamberlain2022fym}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The models (ELPH and BUDDY) were evaluated on standard LP benchmarks, including Cora, Citeseer, and Pubmed datasets. Ablation studies were performed on SGNN components to understand the importance of structure features, propagation mechanisms, and readout functions \\cite{chamberlain2022fym}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: ELPH and BUDDY consistently outperform existing SGNN models on many standard LP benchmarks \\cite{chamberlain2022fym}.\n        *   **Speed/Efficiency**: ELPH is orders of magnitude faster than SGNNs. BUDDY is even faster than ELPH and highly scalable, demonstrating superior efficiency while maintaining or improving predictive performance \\cite{chamberlain2022fym}.\n        *   **Ablation Findings**: The analysis showed that structure features significantly improve performance, that an MLP on structure feature counts can outperform SGNN propagation, and that edge-level readout functions are more effective than graph pooling \\cite{chamberlain2022fym}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **ELPH**: Shares the common GNN limitation that it is only efficient when the entire dataset fits into GPU memory \\cite{chamberlain2022fym}.\n        *   **Sketching Accuracy**: The accuracy of structure feature approximation depends on the parameters of HyperLogLog and MinHashing, introducing a trade-off between speed and precision \\cite{chamberlain2022fym}.\n    *   **Scope of Applicability**: The methods are primarily designed for link prediction tasks on static graphs. While BUDDY addresses scalability for large graphs, the core sketching mechanism is tailored to approximating specific structural features relevant to LP.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chamberlain2022fym} significantly advances the state-of-the-art in GNN-based link prediction by resolving the long-standing trade-off between expressivity (achieved by SGNNs) and efficiency/scalability (lacking in SGNNs). It demonstrates that complex structural information can be efficiently encoded and propagated without explicit subgraph construction.\n    *   **Potential Impact on Future Research**:\n        *   **Efficient Expressive GNNs**: The subgraph sketching paradigm could inspire new designs for expressive GNNs that overcome WL limitations without incurring high computational costs, potentially extending to other graph tasks beyond LP.\n        *   **Scalable GNN Architectures**: BUDDY's precomputation strategy offers a blueprint for developing highly scalable GNNs for very large graphs, which is critical for real-world applications.\n        *   **Theoretical Understanding**: The analysis of SGNN components provides valuable insights into what makes GNNs effective for LP, guiding future research in designing more targeted and efficient models.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Link Prediction",
        "Subgraph Sketching",
        "ELPH",
        "BUDDY",
        "Computational Efficiency",
        "Scalability",
        "Expressivity",
        "Message Passing GNNs (MPNNs)",
        "Subgraph GNNs (SGNNs)",
        "HyperLogLog",
        "MinHashing",
        "Automorphic Node Problem",
        "Full-Graph GNNs"
      ],
      "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract keywords:** \"propose a novel full-graph gnn called elph,\" \"develop a highly scalable model, called buddy,\" \"outperforms existing sgnn models,\" \"orders of magnitude faster.\" these all indicate the presentation of new methods, algorithms, and systems, along with their performance evaluation.\n*   **introduction focus:** the introduction identifies a specific technical problem (\"mpnns tend to be poor link predictors\" due to \"limitations in expressive power\") and discusses the technical reasons behind it (inability to count triangles, equivalence to wl test). this sets the stage for the proposed technical solutions.\n*   **supporting elements:** while the paper also includes theoretical aspects (\"provably more expressive\") and empirical validation (\"outperforms existing sgnn models on many standard lp benchmarks\"), these serve to support and demonstrate the effectiveness of the *new methods* being proposed. the primary contribution is the development of elph and buddy."
    },
    "file_name": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf"
  },
  {
    "success": true,
    "doc_id": "0fe185ba38cfb047a6be92e421410306",
    "summary": "This paper, a review article by \\cite{reiser2022b08}, provides a comprehensive overview of Graph Neural Networks (GNNs) in chemistry and materials science.\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively leveraging machine learning for tasks in chemistry and materials science, such as predicting material properties, accelerating simulations, designing new materials, and predicting synthesis routes. Traditional machine learning methods often rely on hand-crafted feature representations, which can be labor-intensive and may not fully capture the complex structural and geometric information inherent in molecules and materials.\n*   **Importance and Challenge:** This problem is crucial because it can significantly accelerate the materials development cycle, from discovery to synthesis. It is challenging due to the irregular, graph-structured nature of molecular and material data (atoms and bonds), which traditional deep learning models (like CNNs for grid-like data) are not inherently designed to handle. Capturing complete atomic-level representations, incorporating physical laws, and dealing with various scales (from atomic to larger phenomena like doping) are key difficulties.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** This work positions GNNs as a powerful alternative and advancement over conventional machine learning models (e.g., decision tree ensembles, densely connected neural networks, random forests, Gaussian process regression) that typically require predefined feature representations (e.g., compositional or fixed-sized vectors, or descriptors like ACSF, SOAP, MBTR). It also relates GNNs to Convolutional Neural Networks (CNNs) by interpreting them as a generalization of CNNs to irregular graph structures.\n*   **Limitations of Previous Solutions:**\n    *   **Hand-crafted Features:** Previous methods often rely on hand-crafted feature representations, which can be incomplete, less flexible, and may not capture all relevant information, especially geometric dependencies.\n    *   **Lack of End-to-End Learning:** Many traditional approaches separate feature engineering from model training, limiting end-to-end optimization.\n    *   **Inability to Directly Process Graph Data:** Conventional deep learning models are not naturally suited for graph-structured data, requiring conversion or flattening that can lose topological information.\n    *   **Data Requirements:** While GNNs can require significant data, the paper notes that traditional methods with hand-crafted features often struggle to learn complex relationships without extensive feature engineering.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper focuses on **Message Passing Graph Neural Networks (MPNNs)** as the unifying framework for most GNNs in chemistry and materials science \\cite{reiser2022b08}.\n    *   MPNNs operate by iteratively propagating information (messages) between neighboring nodes (atoms) and updating node embeddings based on these messages.\n    *   This process is repeated for a fixed number of steps (K), allowing information to travel within the K-hop neighborhood.\n    *   Finally, a graph-level embedding is obtained by pooling all node embeddings, which is then used for prediction tasks (regression or classification).\n    *   The core operations involve learnable functions for message generation ($M_t$) and node updates ($U_t$), and a permutation-invariant readout function ($R$).\n*   **Novelty/Difference:**\n    *   **Direct Graph Processing:** GNNs directly operate on the natural graph representation of molecules and materials (atoms as nodes, bonds as edges), eliminating the need for extensive manual feature engineering.\n    *   **Representation Learning:** They learn informative molecular/material representations end-to-end, allowing the model to discover relevant features from the raw structural data.\n    *   **Incorporation of Geometric Information:** GNNs can flexibly incorporate geometric information (distances, angles) as additional edge or node features, or through specialized architectures, which is crucial for quantum-mechanical properties.\n    *   **Symmetry Awareness:** The approach emphasizes the importance of incorporating physical symmetries (rotational, translational, permutation, periodicity for crystals) into the model's representation or architecture, leading to more data-efficient and robust models.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   **Formalization of MPNNs:** The paper highlights the MPNN framework \\cite{reiser2022b08} as a foundational and widely applicable scheme for GNNs in this domain, detailing its message passing, node update, and readout phases.\n    *   **Integration of Geometric Data:** Discussion of how GNNs integrate geometric information (distances, bond/dihedral angles) as features, often expanded into Gaussian-like, radial, or spherical Fourier-Bessel functions, moving beyond purely topological graphs.\n    *   **Symmetry-Equivariant Representations:** Emphasis on the development of equivariant representations \\cite{reiser2022b08} that are invariant or equivariant under translation, rotation, and permutation operations, crucial for predicting tensorial properties and reducing data requirements.\n    *   **Periodic Extensions:** Introduction of periodic extensions of crystal graphs \\cite{reiser2022b08} to handle solid crystals and periodic structures, incorporating periodicity and space group symmetries.\n*   **System Design or Architectural Innovations (as discussed in the review):**\n    *   **Edge Updates and Skip Connections:** Mention of extensions like D-MPNN \\cite{reiser2022b08} (directed edge embeddings, message passing between edges) and the use of skip connections \\cite{reiser2022b08} to alleviate issues like over-smoothing.\n    *   **Attention Mechanisms:** Integration of masked self-attention layers, as seen in Graph Attention Networks (GATs) \\cite{reiser2022b08} and AttentiveFingerprint models \\cite{reiser2022b08}, to weigh the importance of neighboring nodes.\n    *   **Graph Pooling/Coarsening:** Discussion of algorithms \\cite{reiser2022b08} to reduce input representation and condense structural information for larger molecules.\n*   **Theoretical Insights or Analysis:**\n    *   **Generalization of CNNs:** GNNs are presented as a generalization of CNNs to irregular graph structures, providing a theoretical link to established deep learning paradigms.\n    *   **Addressing GNN Limitations:** Acknowledgment of open research questions regarding GNNs' limited expressive performance \\cite{reiser2022b08} (e.g., comparison to Weisfeiler-Lehman hierarchy \\cite{reiser2022b08}), over-squashing \\cite{reiser2022b08}, and over-smoothing \\cite{reiser2022b08}, and discussion of proposed solutions like hypergraph representations \\cite{reiser2022b08} and higher-order graph networks \\cite{reiser2022b08}.\n\n### 5. Experimental Validation\n*   **Experiments Conducted (as reported in the review):** The paper itself is a review, so it doesn't present new experimental results. However, it summarizes the empirical validation of GNNs by referencing:\n    *   **Benchmark Datasets:** A wide range of benchmark datasets are listed for both molecules (e.g., QM7, QM9, PDBBind, Tox21) and crystals (e.g., Materials Project (MP), Open Quantum Materials Database (OQMD), Open Catalyst Project (OC20)) \\cite{reiser2022b08}. These datasets are used for supervised tasks like regression (e.g., quantum calculations, protein binding affinity, formation energy) and classification (e.g., toxicity, blood-brain barrier penetration).\n    *   **Performance Comparison:** The review highlights that GNNs have \"outperformed conventional machine learning models in predicting molecular properties throughout the last years\" \\cite{reiser2022b08}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **QM9 Benchmark:** Figure 1c in \\cite{reiser2022b08} specifically shows the Mean Absolute Error (MAE) for the prediction of internal, HOMO, and LUMO energies on the QM9 dataset for various GNN models since 2017. This figure visually demonstrates the continuous improvement in prediction accuracy achieved by different GNN architectures over time for quantum properties.\n    *   **General Superiority:** The text generally states that GNNs show \"a systematic advantage over traditional feature-based methods\" \\cite{reiser2022b08} and have \"outperformed conventional machine learning models in predicting molecular properties\" \\cite{reiser2022b08}.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions:**\n    *   **Data Requirements:** GNNs often come at the cost of higher data requirements, potentially limiting their applicability to scenarios where large amounts of data are available \\cite{reiser2022b08}.\n    *   **Expressive Power:** A main open research question revolves around the limited expressive performance of GNNs for specific tasks and how they compare with the Weisfeiler-Lehman hierarchy for graph isomorphism testing \\cite{reiser2022b08}.\n    *   **Over-smoothing and Over-squashing:** Challenges like over-smoothing (indistinguishable representations of neighboring nodes) and over-squashing (distortion of information from long-range dependencies) limit the depth and effectiveness of message passing \\cite{reiser2022b08}.\n    *   **Training Instability:** Training instability \\cite{reiser2022b08} is also an ongoing research subject.\n*   **Scope of Applicability:** The review focuses specifically on applications in chemistry and materials science, primarily for predicting molecular and material properties, accelerating simulations, and aiding in material design. While GNNs have broader applications, this paper's scope is confined to these domains.\n\n### 7. Technical Significance\n*   **Advancement of State-of-the-Art:** This review highlights how GNNs significantly advance the technical state-of-the-art by providing an end-to-end, representation-learning approach that directly processes graph-structured data. This capability allows for more accurate and flexible modeling of complex chemical and material systems compared to traditional methods relying on hand-crafted features. The continuous improvement shown on benchmarks like QM9 \\cite{reiser2022b08} underscores their superior predictive power.\n*   **Potential Impact on Future Research:**\n    *   **Accelerated Discovery:** GNNs have the potential to revolutionize virtual materials design and accelerate the discovery of new materials and drugs by enabling more efficient property prediction and inverse design.\n    *   **Integration of Physics:** The flexibility to incorporate physical laws and symmetries into GNN architectures opens avenues for developing more physically informed and robust models.\n    *   **Addressing Current Limitations:** Ongoing research into improving expressive power, mitigating over-smoothing/over-squashing, and enhancing data efficiency will further broaden GNN applicability.\n    *   **New Architectures:** The field is ripe for developing novel GNN architectures that can better handle complex geometries, periodic boundary conditions, and multi-scale phenomena in materials science.",
    "intriguing_abstract": "Accelerating the discovery of novel materials and drugs is paramount, yet traditional machine learning struggles with the complex, graph-structured nature of chemical data. This review illuminates the transformative power of **Graph Neural Networks (GNNs)** in chemistry and materials science, presenting **Message Passing Neural Networks (MPNNs)** as a unifying framework. GNNs directly learn rich, **end-to-end representations** from molecular and crystal structures, eliminating the need for laborious hand-crafted features. A key innovation lies in their flexible incorporation of **geometric information** and the development of **symmetry-equivariant architectures**, crucial for accurately modeling quantum-mechanical properties and **periodic systems**.\n\nGNNs have consistently outperformed conventional methods in predicting diverse material properties, from quantum energies on **QM9** to formation energies in crystals. This comprehensive overview underscores GNNs' potential to revolutionize **virtual materials design**, accelerate simulations, and drive scientific discovery by enabling data-efficient and physically informed predictions. We also discuss current limitations and exciting future research directions, positioning GNNs at the forefront of **computational chemistry** and **materials informatics**.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "chemistry and materials science",
      "molecular and material property prediction",
      "Message Passing Neural Networks (MPNNs)",
      "end-to-end representation learning",
      "symmetry-equivariant representations",
      "geometric information incorporation",
      "periodic crystal graphs",
      "GNN limitations (over-smoothing",
      "over-squashing)",
      "accelerated materials development",
      "Convolutional Neural Networks generalization",
      "virtual materials design"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf",
    "citation_key": "reiser2022b08",
    "metadata": {
      "title": "Graph neural networks for materials science and chemistry",
      "authors": [
        "Patrick Reiser",
        "Marlen Neubert",
        "Andr'e Eberhard",
        "Luca Torresi",
        "Chen Zhou",
        "Chen Shao",
        "Houssam Metni",
        "Clint van Hoesel",
        "Henrik Schopmans",
        "T. Sommer",
        "Pascal Friederich"
      ],
      "published_date": "2022",
      "abstract": "Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs. Graph neural networks are machine learning models that directly access the structural representation of molecules and materials. This Review discusses state-of-the-art architectures and applications of graph neural networks in materials science and chemistry, indicating a possible road-map for their further development.",
      "file_path": "paper_data/Graph_Neural_Networks/81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf",
      "venue": "Communications Materials",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, a review article by \\cite{reiser2022b08}, provides a comprehensive overview of Graph Neural Networks (GNNs) in chemistry and materials science.\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively leveraging machine learning for tasks in chemistry and materials science, such as predicting material properties, accelerating simulations, designing new materials, and predicting synthesis routes. Traditional machine learning methods often rely on hand-crafted feature representations, which can be labor-intensive and may not fully capture the complex structural and geometric information inherent in molecules and materials.\n*   **Importance and Challenge:** This problem is crucial because it can significantly accelerate the materials development cycle, from discovery to synthesis. It is challenging due to the irregular, graph-structured nature of molecular and material data (atoms and bonds), which traditional deep learning models (like CNNs for grid-like data) are not inherently designed to handle. Capturing complete atomic-level representations, incorporating physical laws, and dealing with various scales (from atomic to larger phenomena like doping) are key difficulties.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** This work positions GNNs as a powerful alternative and advancement over conventional machine learning models (e.g., decision tree ensembles, densely connected neural networks, random forests, Gaussian process regression) that typically require predefined feature representations (e.g., compositional or fixed-sized vectors, or descriptors like ACSF, SOAP, MBTR). It also relates GNNs to Convolutional Neural Networks (CNNs) by interpreting them as a generalization of CNNs to irregular graph structures.\n*   **Limitations of Previous Solutions:**\n    *   **Hand-crafted Features:** Previous methods often rely on hand-crafted feature representations, which can be incomplete, less flexible, and may not capture all relevant information, especially geometric dependencies.\n    *   **Lack of End-to-End Learning:** Many traditional approaches separate feature engineering from model training, limiting end-to-end optimization.\n    *   **Inability to Directly Process Graph Data:** Conventional deep learning models are not naturally suited for graph-structured data, requiring conversion or flattening that can lose topological information.\n    *   **Data Requirements:** While GNNs can require significant data, the paper notes that traditional methods with hand-crafted features often struggle to learn complex relationships without extensive feature engineering.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper focuses on **Message Passing Graph Neural Networks (MPNNs)** as the unifying framework for most GNNs in chemistry and materials science \\cite{reiser2022b08}.\n    *   MPNNs operate by iteratively propagating information (messages) between neighboring nodes (atoms) and updating node embeddings based on these messages.\n    *   This process is repeated for a fixed number of steps (K), allowing information to travel within the K-hop neighborhood.\n    *   Finally, a graph-level embedding is obtained by pooling all node embeddings, which is then used for prediction tasks (regression or classification).\n    *   The core operations involve learnable functions for message generation ($M_t$) and node updates ($U_t$), and a permutation-invariant readout function ($R$).\n*   **Novelty/Difference:**\n    *   **Direct Graph Processing:** GNNs directly operate on the natural graph representation of molecules and materials (atoms as nodes, bonds as edges), eliminating the need for extensive manual feature engineering.\n    *   **Representation Learning:** They learn informative molecular/material representations end-to-end, allowing the model to discover relevant features from the raw structural data.\n    *   **Incorporation of Geometric Information:** GNNs can flexibly incorporate geometric information (distances, angles) as additional edge or node features, or through specialized architectures, which is crucial for quantum-mechanical properties.\n    *   **Symmetry Awareness:** The approach emphasizes the importance of incorporating physical symmetries (rotational, translational, permutation, periodicity for crystals) into the model's representation or architecture, leading to more data-efficient and robust models.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   **Formalization of MPNNs:** The paper highlights the MPNN framework \\cite{reiser2022b08} as a foundational and widely applicable scheme for GNNs in this domain, detailing its message passing, node update, and readout phases.\n    *   **Integration of Geometric Data:** Discussion of how GNNs integrate geometric information (distances, bond/dihedral angles) as features, often expanded into Gaussian-like, radial, or spherical Fourier-Bessel functions, moving beyond purely topological graphs.\n    *   **Symmetry-Equivariant Representations:** Emphasis on the development of equivariant representations \\cite{reiser2022b08} that are invariant or equivariant under translation, rotation, and permutation operations, crucial for predicting tensorial properties and reducing data requirements.\n    *   **Periodic Extensions:** Introduction of periodic extensions of crystal graphs \\cite{reiser2022b08} to handle solid crystals and periodic structures, incorporating periodicity and space group symmetries.\n*   **System Design or Architectural Innovations (as discussed in the review):**\n    *   **Edge Updates and Skip Connections:** Mention of extensions like D-MPNN \\cite{reiser2022b08} (directed edge embeddings, message passing between edges) and the use of skip connections \\cite{reiser2022b08} to alleviate issues like over-smoothing.\n    *   **Attention Mechanisms:** Integration of masked self-attention layers, as seen in Graph Attention Networks (GATs) \\cite{reiser2022b08} and AttentiveFingerprint models \\cite{reiser2022b08}, to weigh the importance of neighboring nodes.\n    *   **Graph Pooling/Coarsening:** Discussion of algorithms \\cite{reiser2022b08} to reduce input representation and condense structural information for larger molecules.\n*   **Theoretical Insights or Analysis:**\n    *   **Generalization of CNNs:** GNNs are presented as a generalization of CNNs to irregular graph structures, providing a theoretical link to established deep learning paradigms.\n    *   **Addressing GNN Limitations:** Acknowledgment of open research questions regarding GNNs' limited expressive performance \\cite{reiser2022b08} (e.g., comparison to Weisfeiler-Lehman hierarchy \\cite{reiser2022b08}), over-squashing \\cite{reiser2022b08}, and over-smoothing \\cite{reiser2022b08}, and discussion of proposed solutions like hypergraph representations \\cite{reiser2022b08} and higher-order graph networks \\cite{reiser2022b08}.\n\n### 5. Experimental Validation\n*   **Experiments Conducted (as reported in the review):** The paper itself is a review, so it doesn't present new experimental results. However, it summarizes the empirical validation of GNNs by referencing:\n    *   **Benchmark Datasets:** A wide range of benchmark datasets are listed for both molecules (e.g., QM7, QM9, PDBBind, Tox21) and crystals (e.g., Materials Project (MP), Open Quantum Materials Database (OQMD), Open Catalyst Project (OC20)) \\cite{reiser2022b08}. These datasets are used for supervised tasks like regression (e.g., quantum calculations, protein binding affinity, formation energy) and classification (e.g., toxicity, blood-brain barrier penetration).\n    *   **Performance Comparison:** The review highlights that GNNs have \"outperformed conventional machine learning models in predicting molecular properties throughout the last years\" \\cite{reiser2022b08}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **QM9 Benchmark:** Figure 1c in \\cite{reiser2022b08} specifically shows the Mean Absolute Error (MAE) for the prediction of internal, HOMO, and LUMO energies on the QM9 dataset for various GNN models since 2017. This figure visually demonstrates the continuous improvement in prediction accuracy achieved by different GNN architectures over time for quantum properties.\n    *   **General Superiority:** The text generally states that GNNs show \"a systematic advantage over traditional feature-based methods\" \\cite{reiser2022b08} and have \"outperformed conventional machine learning models in predicting molecular properties\" \\cite{reiser2022b08}.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions:**\n    *   **Data Requirements:** GNNs often come at the cost of higher data requirements, potentially limiting their applicability to scenarios where large amounts of data are available \\cite{reiser2022b08}.\n    *   **Expressive Power:** A main open research question revolves around the limited expressive performance of GNNs for specific tasks and how they compare with the Weisfeiler-Lehman hierarchy for graph isomorphism testing \\cite{reiser2022b08}.\n    *   **Over-smoothing and Over-squashing:** Challenges like over-smoothing (indistinguishable representations of neighboring nodes) and over-squashing (distortion of information from long-range dependencies) limit the depth and effectiveness of message passing \\cite{reiser2022b08}.\n    *   **Training Instability:** Training instability \\cite{reiser2022b08} is also an ongoing research subject.\n*   **Scope of Applicability:** The review focuses specifically on applications in chemistry and materials science, primarily for predicting molecular and material properties, accelerating simulations, and aiding in material design. While GNNs have broader applications, this paper's scope is confined to these domains.\n\n### 7. Technical Significance\n*   **Advancement of State-of-the-Art:** This review highlights how GNNs significantly advance the technical state-of-the-art by providing an end-to-end, representation-learning approach that directly processes graph-structured data. This capability allows for more accurate and flexible modeling of complex chemical and material systems compared to traditional methods relying on hand-crafted features. The continuous improvement shown on benchmarks like QM9 \\cite{reiser2022b08} underscores their superior predictive power.\n*   **Potential Impact on Future Research:**\n    *   **Accelerated Discovery:** GNNs have the potential to revolutionize virtual materials design and accelerate the discovery of new materials and drugs by enabling more efficient property prediction and inverse design.\n    *   **Integration of Physics:** The flexibility to incorporate physical laws and symmetries into GNN architectures opens avenues for developing more physically informed and robust models.\n    *   **Addressing Current Limitations:** Ongoing research into improving expressive power, mitigating over-smoothing/over-squashing, and enhancing data efficiency will further broaden GNN applicability.\n    *   **New Architectures:** The field is ripe for developing novel GNN architectures that can better handle complex geometries, periodic boundary conditions, and multi-scale phenomena in materials science.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "chemistry and materials science",
        "molecular and material property prediction",
        "Message Passing Neural Networks (MPNNs)",
        "end-to-end representation learning",
        "symmetry-equivariant representations",
        "geometric information incorporation",
        "periodic crystal graphs",
        "GNN limitations (over-smoothing",
        "over-squashing)",
        "accelerated materials development",
        "Convolutional Neural Networks generalization",
        "virtual materials design"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly states: \"in this **review article**, we provide an **overview** of the basic principles of gnns, widely used datasets and **state-of-the-art** architectures, followed by a discussion of a wide range of recent applications... and concluding with a **road-map** for the further development and application of gnns.\"\n*   the **introduction** sets the stage by discussing the increasing role of machine learning and gnns in natural sciences, indicating a broad discussion of an existing field.\n\nthese phrases directly align with the criteria for a **survey** paper:\n*   \"review article\"\n*   \"overview\"\n*   \"state-of-the-art\"\n*   \"comprehensive analysis\" (implied by \"wide range of recent applications\" and \"overview of basic principles, datasets, architectures\")\n*   \"road-map\" (often included in surveys to discuss future directions)\n\nthe paper is clearly reviewing and synthesizing existing knowledge and applications of graph neural networks in chemistry and materials science, rather than presenting new methods, empirical data, or a detailed case study.\n\n**classification: survey**"
    },
    "file_name": "81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf"
  },
  {
    "success": true,
    "doc_id": "94d99974a30caffc0f39c94190609f40",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Training Graph Neural Networks with 1000 Layers \\cite{li2021orq}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The primary challenge is the immense memory complexity (specifically, for intermediate activations) when training deep Graph Neural Networks (GNNs) on increasingly large graph datasets with millions of nodes and edges. This memory bottleneck prevents the development and deployment of deeper and wider GNNs.\n    *   **Importance & Challenge:** Deep and over-parameterized models have shown superior generalization capabilities in other domains (e.g., NLP), suggesting similar benefits for GNNs. However, existing GNN architectures' memory consumption scales linearly with the number of layers (O(LND)), making deep GNNs impractical on current hardware. Prior solutions like graph sampling or partitioning only partially address the issue, as memory still scales with depth, and they can introduce structural compromises or non-trainable propagation schemes.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Deep GNNs with Skip Connections:** Previous works successfully trained GNNs up to 56 or 112 layers using residual and dense connections to mitigate over-smoothing and vanishing gradients.\n        *   **Normalization and Regularization:** Various techniques like DropEdge, PairNorm, GraphNorm.\n        *   **Efficient Propagation Schemes:** Methods using K-power adjacency matrices or graph diffusion matrices to incorporate multi-hop information in a single layer.\n        *   **Mini-batch Training/Sampling:** GraphSAGE, VR-GCN, FastGCN, Cluster-GCN, GraphSAINT reduce the number of nodes or subgraphs processed per batch.\n    *   **Limitations of Previous Solutions:**\n        *   **Memory Scaling with Depth:** Even with skip connections, the memory complexity for activations remains O(LND), limiting depth.\n        *   **Non-trainable Propagation:** Efficient propagation schemes are often non-trainable, which can lead to sub-optimality and limited capacity for learning hierarchical features.\n        *   **Sampling Limitations:** Sampling methods, while reducing the N or M dimension, still incur memory costs that scale with the number of layers (e.g., O(RLBD) or O(LBD)), and can introduce hyperparameters or break important graph structures if not carefully tuned.\n    *   **Positioning:** This work directly addresses the *L* (depth) dimension of memory complexity, proposing orthogonal approaches that eliminate the dependency of memory consumption on network depth, allowing for unprecedentedly deep GNNs. These techniques can also be combined with existing sampling methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper investigates and combines several techniques:\n        *   **Grouped Reversible GNNs (RevGNNs):** This is the primary innovation. Inspired by reversible networks and grouped convolutions, the input vertex features are partitioned into groups. The forward pass is designed such that intermediate activations can be reconstructed during the backward pass, eliminating the need to store them.\n        *   **Parameter-Efficient GNNs:**\n            *   **Weight-tied GNNs:** Shares weights across all layers, significantly reducing parameter count.\n            *   **Deep Equilibrium GNNs (DEQ-GNNs):** Formulates the GNN as finding an equilibrium point of an infinite-depth, weight-tied network. The forward pass uses a root-finding algorithm, and gradients are computed via implicit differentiation, avoiding storage of intermediate states.\n    *   **Novelty/Difference:**\n        *   **O(1) Memory for Activations:** The Grouped Reversible GNNs reduce the memory complexity for activations from O(LND) to O(ND), making it independent of network depth. This is a significant departure from prior deep GNNs.\n        *   **Modified Dropout for Reversibility:** A novel approach to handle dropout in reversible GNNs by sharing dropout patterns across layers, preventing the memory cost from reverting to O(LND).\n        *   **First to 1000+ Layers:** This approach enables the training of GNNs with over 1000 layers, an order of magnitude deeper than previously reported GNNs.\n        *   **Adaptation of Advanced Architectures:** Successfully adapts reversible connections, weight-tying, and deep equilibrium models (previously successful in CV/NLP) to the GNN domain.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Grouped Reversible GNNs:** A new GNN architecture that achieves O(ND) memory complexity for activations, independent of network depth, by reconstructing intermediate states during backpropagation \\cite{li2021orq}.\n        *   **Layer-Shared Dropout:** A technique to apply dropout within reversible GNNs without incurring O(LND) memory overhead for dropout masks, by sharing a single dropout pattern across all layers \\cite{li2021orq}.\n        *   **Adaptation of Weight-Tied and Deep Equilibrium Networks for GNNs:** Demonstrates how these parameter-efficient paradigms can be applied to GNNs, offering constant parameter cost and O(ND) memory for DEQ-GNNs \\cite{li2021orq}.\n    *   **System Design/Architectural Innovations:** The specific design of the reversible GNN block, including how groups are processed and how information is exchanged (e.g., X'0 = sum(Xi)), is a key architectural innovation.\n    *   **Theoretical Insights/Analysis:** The paper provides an analysis of the memory and parameter complexities of the proposed methods, highlighting the O(ND) memory for reversible and equilibrium models and the constant parameter cost for weight-tied and equilibrium models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Extensive evaluation of reversible GNNs (RevGNN-x), weight-tied GNNs (WT-x), and deep equilibrium GNNs (DEQ-x) against residual GNNs (ResGNN-x).\n        *   Comparison across different depths (up to 112 layers for initial comparisons) and widths (channels).\n        *   Training of extremely deep models (RevGNN-Deep with 1001 layers, RevGNN-Wide with 448 layers).\n        *   Demonstration of generality by applying techniques to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN).\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Dataset:** Primarily evaluated on the large-scale `ogbn-proteins` dataset, also mentioned \"several datasets.\"\n        *   **Metrics:** ROC-AUC score for performance, and GPU memory consumption (GB).\n        *   **RevGNN-Deep (1001 layers, 80 channels):** Achieved 87.74 ±0.13 ROC-AUC on `ogbn-proteins` with only 2.86 GB GPU memory, outperforming the previous state-of-the-art while consuming an order of magnitude less memory \\cite{li2021orq}.\n        *   **RevGNN-Wide (448 layers, 224 channels):** Achieved 88.24 ±0.15 ROC-AUC on `ogbn-proteins`, ranking first on the leaderboard by a significant margin \\cite{li2021orq}.\n        *   **Memory Efficiency:** Reversible models consistently achieved comparable or better performance than baselines using a fraction of the memory (e.g., Figure 1 shows RevGNN-224 using ~11GB vs. ResGNN-224 running out of memory at 32GB) \\cite{li2021orq}.\n        *   **Parameter Efficiency:** Weight-tied and equilibrium models offered a good performance-to-parameter efficiency trade-off.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   Weight-tied models, while parameter-efficient, may suffer performance on large datasets due to fewer parameters, requiring increased width to compensate \\cite{li2021orq}.\n        *   Graph equilibrium models' training time vs. performance trade-off can be adjusted by tuning the number of iterations in each optimization step \\cite{li2021orq}.\n        *   The analysis primarily focuses on memory consumption induced by activations, assuming network parameters' memory footprint is negligible.\n    *   **Scope of Applicability:** The proposed techniques are general and can be applied to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN) and can be combined with existing sampling-based methods for further memory reduction or performance boosts \\cite{li2021orq}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** Pushes the technical state-of-the-art in GNN depth by successfully training models with over 1000 layers, a feat previously considered impractical due to memory constraints \\cite{li2021orq}.\n    *   **Memory Bottleneck Resolution:** Provides a fundamental solution to the memory bottleneck for deep GNNs by achieving O(1) memory complexity with respect to depth, making deep and over-parameterized GNNs feasible on commodity hardware \\cite{li2021orq}.\n    *   **Enables Over-parameterization for GNNs:** By freeing up memory, the work demonstrates that deeper and wider GNNs can significantly outperform existing methods, suggesting that over-parameterization is beneficial for GNNs as it is for other deep learning models \\cite{li2021orq}.\n    *   **New Architectural Paradigms for GNNs:** Introduces and validates reversible connections, weight-tying, and deep equilibrium models as powerful architectural choices for designing efficient GNNs, opening new avenues for future research in GNN architecture design \\cite{li2021orq}.\n    *   **Practical Impact:** The ability to train highly performant, extremely deep GNNs on single commodity GPUs democratizes access to advanced GNN models for large-scale graph problems.",
    "intriguing_abstract": "Training truly deep Graph Neural Networks (GNNs) has long been hampered by an intractable memory bottleneck, where intermediate activations scale linearly with network depth (O(LND)). This fundamental limitation prevents GNNs from leveraging the benefits of over-parameterization seen in other deep learning domains. We shatter this barrier by introducing novel architectural paradigms that decouple memory consumption from network depth.\n\nOur work presents Grouped Reversible GNNs (RevGNNs) and Deep Equilibrium GNNs (DEQ-GNNs), which achieve an unprecedented O(ND) memory complexity for activations, making it independent of depth. RevGNNs reconstruct activations during backpropagation, while DEQ-GNNs formulate GNNs as finding an equilibrium point. This breakthrough enables us to train GNNs with over 1000 layers, an order of magnitude deeper than previously possible. We demonstrate state-of-the-art performance on large-scale graph datasets like `ogbn-proteins`, achieving superior ROC-AUC scores with significantly reduced GPU memory footprint. These innovations unlock the potential of massively deep and over-parameterized GNNs, paving the way for new advancements in graph representation learning and making powerful GNNs accessible on commodity hardware.",
    "keywords": [
      "Deep Graph Neural Networks (GNNs)",
      "Memory bottleneck",
      "Intermediate activations",
      "Grouped Reversible GNNs",
      "O(1) memory complexity",
      "1000+ layer GNNs",
      "Weight-tied GNNs",
      "Deep Equilibrium GNNs",
      "Layer-Shared Dropout",
      "Over-parameterization",
      "Large graph datasets",
      "Reconstructing intermediate states",
      "Novel GNN architectural paradigms",
      "GPU memory efficiency"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/cf30fb61a5943781144c8442563e3ef9c38df871.pdf",
    "citation_key": "li2021orq",
    "metadata": {
      "title": "Training Graph Neural Networks with 1000 Layers",
      "authors": [
        "Guohao Li",
        "Matthias Müller",
        "Bernard Ghanem",
        "V. Koltun"
      ],
      "published_date": "2021",
      "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.24 \\pm 0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude. Please visit our project website https://www.deepgcns.org/arch/gnn1000 for more information.",
      "file_path": "paper_data/Graph_Neural_Networks/cf30fb61a5943781144c8442563e3ef9c38df871.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Training Graph Neural Networks with 1000 Layers \\cite{li2021orq}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The primary challenge is the immense memory complexity (specifically, for intermediate activations) when training deep Graph Neural Networks (GNNs) on increasingly large graph datasets with millions of nodes and edges. This memory bottleneck prevents the development and deployment of deeper and wider GNNs.\n    *   **Importance & Challenge:** Deep and over-parameterized models have shown superior generalization capabilities in other domains (e.g., NLP), suggesting similar benefits for GNNs. However, existing GNN architectures' memory consumption scales linearly with the number of layers (O(LND)), making deep GNNs impractical on current hardware. Prior solutions like graph sampling or partitioning only partially address the issue, as memory still scales with depth, and they can introduce structural compromises or non-trainable propagation schemes.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Deep GNNs with Skip Connections:** Previous works successfully trained GNNs up to 56 or 112 layers using residual and dense connections to mitigate over-smoothing and vanishing gradients.\n        *   **Normalization and Regularization:** Various techniques like DropEdge, PairNorm, GraphNorm.\n        *   **Efficient Propagation Schemes:** Methods using K-power adjacency matrices or graph diffusion matrices to incorporate multi-hop information in a single layer.\n        *   **Mini-batch Training/Sampling:** GraphSAGE, VR-GCN, FastGCN, Cluster-GCN, GraphSAINT reduce the number of nodes or subgraphs processed per batch.\n    *   **Limitations of Previous Solutions:**\n        *   **Memory Scaling with Depth:** Even with skip connections, the memory complexity for activations remains O(LND), limiting depth.\n        *   **Non-trainable Propagation:** Efficient propagation schemes are often non-trainable, which can lead to sub-optimality and limited capacity for learning hierarchical features.\n        *   **Sampling Limitations:** Sampling methods, while reducing the N or M dimension, still incur memory costs that scale with the number of layers (e.g., O(RLBD) or O(LBD)), and can introduce hyperparameters or break important graph structures if not carefully tuned.\n    *   **Positioning:** This work directly addresses the *L* (depth) dimension of memory complexity, proposing orthogonal approaches that eliminate the dependency of memory consumption on network depth, allowing for unprecedentedly deep GNNs. These techniques can also be combined with existing sampling methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper investigates and combines several techniques:\n        *   **Grouped Reversible GNNs (RevGNNs):** This is the primary innovation. Inspired by reversible networks and grouped convolutions, the input vertex features are partitioned into groups. The forward pass is designed such that intermediate activations can be reconstructed during the backward pass, eliminating the need to store them.\n        *   **Parameter-Efficient GNNs:**\n            *   **Weight-tied GNNs:** Shares weights across all layers, significantly reducing parameter count.\n            *   **Deep Equilibrium GNNs (DEQ-GNNs):** Formulates the GNN as finding an equilibrium point of an infinite-depth, weight-tied network. The forward pass uses a root-finding algorithm, and gradients are computed via implicit differentiation, avoiding storage of intermediate states.\n    *   **Novelty/Difference:**\n        *   **O(1) Memory for Activations:** The Grouped Reversible GNNs reduce the memory complexity for activations from O(LND) to O(ND), making it independent of network depth. This is a significant departure from prior deep GNNs.\n        *   **Modified Dropout for Reversibility:** A novel approach to handle dropout in reversible GNNs by sharing dropout patterns across layers, preventing the memory cost from reverting to O(LND).\n        *   **First to 1000+ Layers:** This approach enables the training of GNNs with over 1000 layers, an order of magnitude deeper than previously reported GNNs.\n        *   **Adaptation of Advanced Architectures:** Successfully adapts reversible connections, weight-tying, and deep equilibrium models (previously successful in CV/NLP) to the GNN domain.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Grouped Reversible GNNs:** A new GNN architecture that achieves O(ND) memory complexity for activations, independent of network depth, by reconstructing intermediate states during backpropagation \\cite{li2021orq}.\n        *   **Layer-Shared Dropout:** A technique to apply dropout within reversible GNNs without incurring O(LND) memory overhead for dropout masks, by sharing a single dropout pattern across all layers \\cite{li2021orq}.\n        *   **Adaptation of Weight-Tied and Deep Equilibrium Networks for GNNs:** Demonstrates how these parameter-efficient paradigms can be applied to GNNs, offering constant parameter cost and O(ND) memory for DEQ-GNNs \\cite{li2021orq}.\n    *   **System Design/Architectural Innovations:** The specific design of the reversible GNN block, including how groups are processed and how information is exchanged (e.g., X'0 = sum(Xi)), is a key architectural innovation.\n    *   **Theoretical Insights/Analysis:** The paper provides an analysis of the memory and parameter complexities of the proposed methods, highlighting the O(ND) memory for reversible and equilibrium models and the constant parameter cost for weight-tied and equilibrium models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Extensive evaluation of reversible GNNs (RevGNN-x), weight-tied GNNs (WT-x), and deep equilibrium GNNs (DEQ-x) against residual GNNs (ResGNN-x).\n        *   Comparison across different depths (up to 112 layers for initial comparisons) and widths (channels).\n        *   Training of extremely deep models (RevGNN-Deep with 1001 layers, RevGNN-Wide with 448 layers).\n        *   Demonstration of generality by applying techniques to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN).\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Dataset:** Primarily evaluated on the large-scale `ogbn-proteins` dataset, also mentioned \"several datasets.\"\n        *   **Metrics:** ROC-AUC score for performance, and GPU memory consumption (GB).\n        *   **RevGNN-Deep (1001 layers, 80 channels):** Achieved 87.74 ±0.13 ROC-AUC on `ogbn-proteins` with only 2.86 GB GPU memory, outperforming the previous state-of-the-art while consuming an order of magnitude less memory \\cite{li2021orq}.\n        *   **RevGNN-Wide (448 layers, 224 channels):** Achieved 88.24 ±0.15 ROC-AUC on `ogbn-proteins`, ranking first on the leaderboard by a significant margin \\cite{li2021orq}.\n        *   **Memory Efficiency:** Reversible models consistently achieved comparable or better performance than baselines using a fraction of the memory (e.g., Figure 1 shows RevGNN-224 using ~11GB vs. ResGNN-224 running out of memory at 32GB) \\cite{li2021orq}.\n        *   **Parameter Efficiency:** Weight-tied and equilibrium models offered a good performance-to-parameter efficiency trade-off.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   Weight-tied models, while parameter-efficient, may suffer performance on large datasets due to fewer parameters, requiring increased width to compensate \\cite{li2021orq}.\n        *   Graph equilibrium models' training time vs. performance trade-off can be adjusted by tuning the number of iterations in each optimization step \\cite{li2021orq}.\n        *   The analysis primarily focuses on memory consumption induced by activations, assuming network parameters' memory footprint is negligible.\n    *   **Scope of Applicability:** The proposed techniques are general and can be applied to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN) and can be combined with existing sampling-based methods for further memory reduction or performance boosts \\cite{li2021orq}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** Pushes the technical state-of-the-art in GNN depth by successfully training models with over 1000 layers, a feat previously considered impractical due to memory constraints \\cite{li2021orq}.\n    *   **Memory Bottleneck Resolution:** Provides a fundamental solution to the memory bottleneck for deep GNNs by achieving O(1) memory complexity with respect to depth, making deep and over-parameterized GNNs feasible on commodity hardware \\cite{li2021orq}.\n    *   **Enables Over-parameterization for GNNs:** By freeing up memory, the work demonstrates that deeper and wider GNNs can significantly outperform existing methods, suggesting that over-parameterization is beneficial for GNNs as it is for other deep learning models \\cite{li2021orq}.\n    *   **New Architectural Paradigms for GNNs:** Introduces and validates reversible connections, weight-tying, and deep equilibrium models as powerful architectural choices for designing efficient GNNs, opening new avenues for future research in GNN architecture design \\cite{li2021orq}.\n    *   **Practical Impact:** The ability to train highly performant, extremely deep GNNs on single commodity GPUs democratizes access to advanced GNN models for large-scale graph problems.",
      "keywords": [
        "Deep Graph Neural Networks (GNNs)",
        "Memory bottleneck",
        "Intermediate activations",
        "Grouped Reversible GNNs",
        "O(1) memory complexity",
        "1000+ layer GNNs",
        "Weight-tied GNNs",
        "Deep Equilibrium GNNs",
        "Layer-Shared Dropout",
        "Over-parameterization",
        "Large graph datasets",
        "Reconstructing intermediate states",
        "Novel GNN architectural paradigms",
        "GPU memory efficiency"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract identifies a technical problem (\"memory complexity has become a major obstacle when training deep gnns\").\n*   it then proposes a solution: \"we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efﬁciency of gnns.\" this indicates the development of new methods or algorithms.\n*   it presents specific models (\"our models revgnn-deep... and revgnn-wide...\") and their performance, demonstrating the effectiveness of the proposed technical solution.\n*   the introduction further elaborates on the problem and shows empirical results (figure 1) comparing their proposed \"revgnn\" models against others, reinforcing the presentation of a new system/method and its validation.\n\nwhile the paper includes significant empirical validation (experiments, data, statistical findings), its core contribution is the **development and presentation of new methods and architectures** (reversible connections, revgnn-deep, revgnn-wide) to solve a specific technical challenge (memory complexity in deep gnns). the empirical results serve to demonstrate the efficacy of these proposed technical solutions.\n\ntherefore, this paper is best classified as **technical**."
    },
    "file_name": "cf30fb61a5943781144c8442563e3ef9c38df871.pdf"
  },
  {
    "success": true,
    "doc_id": "7bbdde5451d0925de3ad1071e9dbfdfc",
    "summary": "Here's a focused summary of the paper \\cite{wang2022u2l} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The expressive power of Spectral Graph Neural Networks (GNNs) is largely under-researched, despite their widespread use. Specifically, it's unclear whether nonlinearities or Multi-Layer Perceptrons (MLPs) are truly necessary for spectral GNNs to achieve high expressiveness. Additionally, existing spectral GNNs differ mainly in their choice of polynomial bases for spectral filters, but a systematic analysis of these differences and their impact on performance is lacking.\n    *   **Importance & Challenge**: GNNs achieve state-of-the-art performance in graph representation learning. Understanding the theoretical limits and conditions for universality of spectral GNNs is crucial for designing more powerful and efficient models. The challenge lies in formally proving universality conditions and connecting them to established concepts like Graph Isomorphism (GI) testing, which is typically used for spatial GNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions itself against existing spectral GNNs (e.g., ChebyNet, GPRGNN, BernNet) by focusing on their underlying polynomial filter mechanisms and questioning the necessity of nonlinearity. It also contrasts with prior work on removing nonlinearity from GNNs (e.g., APPNP, GBP), which primarily focused on scalability with restricted filters, whereas \\cite{wang2022u2l} analyzes expressive power with arbitrary polynomial filters.\n    *   **Limitations of Previous Solutions**: Previous works on GNN expressivity often focused on spatial GNNs using the Weisfeiler-Lehman (WL) test. There was a gap in connecting spectral GNN expressivity (in terms of universal approximation) to GI testing. Furthermore, while many spectral GNNs use polynomial filters, the theoretical implications of different polynomial bases on optimization and empirical performance, despite having the same expressive power, were not systematically explained.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper theoretically analyzes the expressive power of *linear* spectral GNNs (i.e., without nonlinearity or MLPs, formulated as `Z = g(^L)XW`). It proves conditions under which these linear models can achieve universal approximation. It then connects these universality conditions to Graph Isomorphism (GI) testing, bridging the analysis of spectral and spatial GNNs. Finally, it investigates the optimization properties of different polynomial bases for spectral filters.\n    *   **Novelty**:\n        1.  **Universality of Linear Spectral GNNs**: Proves that linear spectral GNNs can produce arbitrary graph signals under specific, mild conditions, demonstrating that nonlinearity is not strictly necessary for high expressiveness.\n        2.  **Connection to Graph Isomorphism**: Establishes a novel link between the universality conditions of spectral GNNs and the discriminative power of the 1-WL test, showing how these conditions enable 1-WL to differentiate non-isomorphic nodes and constrain graph symmetry.\n        3.  **Optimization Analysis of Filter Bases**: Provides an optimization perspective on why different polynomial bases, despite having the same expressive power, lead to varying empirical performance, suggesting that orthogonal bases weighted by graph signal density can maximize convergence speed.\n        4.  **JacobiConv**: Introduces a novel spectral GNN, JacobiConv, which leverages these insights by using Jacobi polynomials as an orthogonal and flexible basis, and explicitly deserts nonlinearity.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insights**:\n        *   Proof that linear spectral GNNs are universal for one-dimensional predictions under two conditions: 1) no multiple eigenvalues of the graph Laplacian, and 2) no missing frequency components in node features (Theorem 4.1).\n        *   Formal connection between these universality conditions and the 1-WL test, showing that these conditions enable 1-WL to differentiate all non-isomorphic nodes and imply a lack of graph automorphisms (Corollary 4.4, Theorem 4.5, Theorem 4.6).\n        *   Analysis demonstrating that using an orthogonal basis with a weight function corresponding to the graph signal density can maximize convergence speed during optimization.\n    *   **Novel Algorithm/System Design**:\n        *   **JacobiConv**: A new spectral GNN model that foregoes nonlinearity, utilizes Jacobi basis for its spectral filters, and is designed to adapt to a wide range of graph signal densities.\n        *   **Polynomial Coefficient Decomposition (PCD)**: A novel technique proposed to improve the optimization of filter coefficients within JacobiConv.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of JacobiConv's expressive power to approximate filter functions on synthetic datasets.\n        *   Performance comparison of JacobiConv against state-of-the-art spectral GNN baselines on ten real-world datasets for node classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   On synthetic datasets, JacobiConv achieved the lowest loss in learning target filter functions, demonstrating its superior approximation capability.\n        *   On ten real-world datasets, JacobiConv consistently outperformed all baselines, achieving performance improvements of up to 12%.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The universality proofs for linear GNNs rely on specific conditions: one-dimensional prediction, no multiple eigenvalues of the graph Laplacian, and no missing frequency components in node features. While the paper empirically shows these conditions are largely met in real-world datasets (e.g., less than 1% multiple eigenvalues, no missing components), they are theoretical constraints. The analysis of linear GNNs with limited polynomial degrees is discussed in the appendix.\n    *   **Scope of Applicability**: The analysis primarily focuses on node property prediction tasks on fixed graphs with fixed node features. While the insights on filter design and optimization are general, the direct universality proofs are specific to these settings.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wang2022u2l} significantly advances the theoretical understanding of spectral GNNs by formally proving their expressive power without nonlinearity and establishing a crucial link to graph isomorphism testing. This challenges the common assumption that nonlinearity is indispensable for GNN expressiveness.\n    *   **Potential Impact on Future Research**: The findings motivate the design of more efficient and powerful spectral GNNs by focusing on optimal filter basis choices and potentially simplifying architectures by removing unnecessary nonlinearities. The connection between spectral properties and GI testing opens new avenues for unified expressivity analysis across different GNN paradigms. The proposed JacobiConv and PCD technique offer practical improvements for spectral GNN design.",
    "intriguing_abstract": "Challenging a core assumption in Graph Neural Network (GNN) design, this paper reveals that linear spectral GNNs, devoid of nonlinearities or Multi-Layer Perceptrons (MLPs), can achieve universal approximation under specific, mild conditions. We provide the first theoretical proofs for this surprising expressivity, establishing their capacity for universal approximation of graph signals. Crucially, we establish a novel connection between spectral GNN universality and the discriminative power of the 1-Weisfeiler-Lehman (1-WL) test, bridging the gap between spectral and spatial GNN expressivity analyses and showing how these conditions constrain graph symmetry. Furthermore, we systematically analyze the optimization properties of different polynomial bases for spectral filters, proving that orthogonal bases weighted by graph signal density can significantly accelerate convergence. Leveraging these insights, we introduce **JacobiConv**, a novel spectral GNN that explicitly foregoes nonlinearity and employs flexible Jacobi polynomials as an optimal filter basis. Extensive experiments on ten real-world datasets confirm JacobiConv's superior approximation capabilities and achieve up to 12% performance improvements in node classification, paving the way for simpler, more efficient, and theoretically grounded GNN architectures.",
    "keywords": [
      "Spectral Graph Neural Networks",
      "expressive power",
      "linear spectral GNNs",
      "universal approximation",
      "Graph Isomorphism testing",
      "polynomial bases",
      "optimization analysis",
      "orthogonal bases",
      "JacobiConv",
      "Polynomial Coefficient Decomposition (PCD)",
      "nonlinearity necessity",
      "graph Laplacian eigenvalues",
      "node classification",
      "convergence speed"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf",
    "citation_key": "wang2022u2l",
    "metadata": {
      "title": "How Powerful are Spectral Graph Neural Networks",
      "authors": [
        "Xiyuan Wang",
        "Muhan Zhang"
      ],
      "published_date": "2022",
      "abstract": "Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{wang2022u2l} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The expressive power of Spectral Graph Neural Networks (GNNs) is largely under-researched, despite their widespread use. Specifically, it's unclear whether nonlinearities or Multi-Layer Perceptrons (MLPs) are truly necessary for spectral GNNs to achieve high expressiveness. Additionally, existing spectral GNNs differ mainly in their choice of polynomial bases for spectral filters, but a systematic analysis of these differences and their impact on performance is lacking.\n    *   **Importance & Challenge**: GNNs achieve state-of-the-art performance in graph representation learning. Understanding the theoretical limits and conditions for universality of spectral GNNs is crucial for designing more powerful and efficient models. The challenge lies in formally proving universality conditions and connecting them to established concepts like Graph Isomorphism (GI) testing, which is typically used for spatial GNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions itself against existing spectral GNNs (e.g., ChebyNet, GPRGNN, BernNet) by focusing on their underlying polynomial filter mechanisms and questioning the necessity of nonlinearity. It also contrasts with prior work on removing nonlinearity from GNNs (e.g., APPNP, GBP), which primarily focused on scalability with restricted filters, whereas \\cite{wang2022u2l} analyzes expressive power with arbitrary polynomial filters.\n    *   **Limitations of Previous Solutions**: Previous works on GNN expressivity often focused on spatial GNNs using the Weisfeiler-Lehman (WL) test. There was a gap in connecting spectral GNN expressivity (in terms of universal approximation) to GI testing. Furthermore, while many spectral GNNs use polynomial filters, the theoretical implications of different polynomial bases on optimization and empirical performance, despite having the same expressive power, were not systematically explained.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper theoretically analyzes the expressive power of *linear* spectral GNNs (i.e., without nonlinearity or MLPs, formulated as `Z = g(^L)XW`). It proves conditions under which these linear models can achieve universal approximation. It then connects these universality conditions to Graph Isomorphism (GI) testing, bridging the analysis of spectral and spatial GNNs. Finally, it investigates the optimization properties of different polynomial bases for spectral filters.\n    *   **Novelty**:\n        1.  **Universality of Linear Spectral GNNs**: Proves that linear spectral GNNs can produce arbitrary graph signals under specific, mild conditions, demonstrating that nonlinearity is not strictly necessary for high expressiveness.\n        2.  **Connection to Graph Isomorphism**: Establishes a novel link between the universality conditions of spectral GNNs and the discriminative power of the 1-WL test, showing how these conditions enable 1-WL to differentiate non-isomorphic nodes and constrain graph symmetry.\n        3.  **Optimization Analysis of Filter Bases**: Provides an optimization perspective on why different polynomial bases, despite having the same expressive power, lead to varying empirical performance, suggesting that orthogonal bases weighted by graph signal density can maximize convergence speed.\n        4.  **JacobiConv**: Introduces a novel spectral GNN, JacobiConv, which leverages these insights by using Jacobi polynomials as an orthogonal and flexible basis, and explicitly deserts nonlinearity.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insights**:\n        *   Proof that linear spectral GNNs are universal for one-dimensional predictions under two conditions: 1) no multiple eigenvalues of the graph Laplacian, and 2) no missing frequency components in node features (Theorem 4.1).\n        *   Formal connection between these universality conditions and the 1-WL test, showing that these conditions enable 1-WL to differentiate all non-isomorphic nodes and imply a lack of graph automorphisms (Corollary 4.4, Theorem 4.5, Theorem 4.6).\n        *   Analysis demonstrating that using an orthogonal basis with a weight function corresponding to the graph signal density can maximize convergence speed during optimization.\n    *   **Novel Algorithm/System Design**:\n        *   **JacobiConv**: A new spectral GNN model that foregoes nonlinearity, utilizes Jacobi basis for its spectral filters, and is designed to adapt to a wide range of graph signal densities.\n        *   **Polynomial Coefficient Decomposition (PCD)**: A novel technique proposed to improve the optimization of filter coefficients within JacobiConv.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of JacobiConv's expressive power to approximate filter functions on synthetic datasets.\n        *   Performance comparison of JacobiConv against state-of-the-art spectral GNN baselines on ten real-world datasets for node classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   On synthetic datasets, JacobiConv achieved the lowest loss in learning target filter functions, demonstrating its superior approximation capability.\n        *   On ten real-world datasets, JacobiConv consistently outperformed all baselines, achieving performance improvements of up to 12%.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The universality proofs for linear GNNs rely on specific conditions: one-dimensional prediction, no multiple eigenvalues of the graph Laplacian, and no missing frequency components in node features. While the paper empirically shows these conditions are largely met in real-world datasets (e.g., less than 1% multiple eigenvalues, no missing components), they are theoretical constraints. The analysis of linear GNNs with limited polynomial degrees is discussed in the appendix.\n    *   **Scope of Applicability**: The analysis primarily focuses on node property prediction tasks on fixed graphs with fixed node features. While the insights on filter design and optimization are general, the direct universality proofs are specific to these settings.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wang2022u2l} significantly advances the theoretical understanding of spectral GNNs by formally proving their expressive power without nonlinearity and establishing a crucial link to graph isomorphism testing. This challenges the common assumption that nonlinearity is indispensable for GNN expressiveness.\n    *   **Potential Impact on Future Research**: The findings motivate the design of more efficient and powerful spectral GNNs by focusing on optimal filter basis choices and potentially simplifying architectures by removing unnecessary nonlinearities. The connection between spectral properties and GI testing opens new avenues for unified expressivity analysis across different GNN paradigms. The proposed JacobiConv and PCD technique offer practical improvements for spectral GNN design.",
      "keywords": [
        "Spectral Graph Neural Networks",
        "expressive power",
        "linear spectral GNNs",
        "universal approximation",
        "Graph Isomorphism testing",
        "polynomial bases",
        "optimization analysis",
        "orthogonal bases",
        "JacobiConv",
        "Polynomial Coefficient Decomposition (PCD)",
        "nonlinearity necessity",
        "graph Laplacian eigenvalues",
        "node classification",
        "convergence speed"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **survey**: not a survey. it identifies a gap in research (\"spectral gnns’ expressive power is still under-researched\", \"no study has systematically explained these differences\") but doesn't aim to comprehensively review existing literature or classify schemes.\n2.  **technical**: it does propose a new method, \"jacobiconv\", which is a technical contribution. however, this proposal is explicitly stated as being \"inspired by the analysis,\" suggesting it's a consequence of a deeper investigation.\n3.  **theoretical**: this is a strong fit. the abstract explicitly states: \"this paper **studies spectral gnns’ expressive power theoretically**. we ﬁrst **prove** that even spectral gnns without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality... we also establish a connection between the expressive power of spectral gnns and graph isomorphism (gi) testing... moreover, we study the difference in empirical performance among different spectral gnns with the same expressive power from an **optimization perspective**\". the introduction also highlights that \"spectral gnns’ expressive power is still under-researched.\" the title \"how powerful are spectral graph neural networks\" directly asks a theoretical question.\n4.  **empirical**: while it mentions \"outperforming all baselines on both synthetic and real-world datasets,\" this is presented as a validation of the proposed jacobiconv, which itself is inspired by theoretical analysis. the primary focus is not a data-driven study to answer a research question, but rather a theoretical investigation.\n5.  **case_study**: no mention of specific applications or case studies as the main focus.\n6.  **position**: it identifies problems and proposes a direction, but the core of the paper is rigorous theoretical analysis and a concrete technical proposal, not just arguing a viewpoint.\n7.  **short**: the venue (icml) is a major conference, typically for full papers, and there are no indicators of it being a short paper or work-in-progress.\n\nthe paper's primary contribution, as highlighted in the abstract, is the theoretical analysis of spectral gnns' expressive power, including proofs and establishing connections. the proposed method (jacobiconv) is a direct outcome or application of this theoretical understanding. therefore, \"theoretical\" best describes the core nature of this paper.\n\n**classification: theoretical**"
    },
    "file_name": "641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf"
  },
  {
    "success": true,
    "doc_id": "fe99175b66c105ca52935fbd487740d6",
    "summary": "Here's a focused summary of the paper \"ProtGNN: Towards Self-Explaining Graph Neural Networks\" \\cite{zhang2021wgf} for a literature review:\n\n### ProtGNN: Towards Self-Explaining Graph Neural Networks \\cite{zhang2021wgf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the predictions made by Graph Neural Networks (GNNs) remains a significant challenge. GNNs are often black-box models, hindering trust and widespread adoption in critical applications.\n    *   **Importance and Challenge**: Understanding GNN rationales is crucial for model debugging, error analysis, and deployment in sensitive areas like medical diagnosis. Existing explanation methods are predominantly *post-hoc*, meaning they employ a separate explanatory model *after* a GNN is trained. This approach often fails to reveal the *original reasoning process* of the GNN, leading to potentially inaccurate or incomplete explanations. The challenge is to build GNNs with *built-in* or *inherent interpretability*.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Current GNN explanation methods are categorized into gradients/features-based, perturbation-based (e.g., GNNExplainer, PGExplainer), decomposition methods, and surrogate methods (e.g., PGM-Explainer, XGNN).\n    *   **Limitations of Previous Solutions**: The primary limitation is that nearly all existing methods are *post-hoc*. This means they do not intrinsically reflect the GNN's decision-making process, potentially leading to explanations that do not precisely fit the original model's reasoning.\n    *   **Positioning**: \\cite{zhang2021wgf} positions ProtGNN as a novel approach to address the limitations of post-hoc explanations by introducing *built-in interpretability* to GNNs. It leverages prototype learning, a form of case-based reasoning, which has not been previously explored for self-explaining GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Prototype Graph Neural Network (ProtGNN)**, which integrates prototype learning with GNNs to achieve self-explaining capabilities.\n        *   It consists of a GNN encoder (e.g., GCN, GAT, GIN) to map input graphs into a latent embedding.\n        *   A prototype layer then computes similarity scores between the input graph's embedding and a set of learned prototype embeddings.\n        *   Predictions are derived from these similarities, providing case-based explanations (i.e., \"this graph is similar to prototype X, which belongs to class Y\").\n    *   **Novelty/Differentiation**:\n        *   **Built-in Interpretability**: Unlike post-hoc methods, ProtGNN's explanations are an integral part of its classification process, directly reflecting its reasoning.\n        *   **Novel Learning Objective**: The objective function includes a cross-entropy loss for accuracy, a *cluster cost* (encouraging embeddings to be close to prototypes of their own class), a *separation cost* (encouraging distance from prototypes of other classes), and a *diversity loss* (penalizing prototypes that are too similar to each other).\n        *   **Monte Carlo Tree Search (MCTS) for Prototype Projection**: To make abstract prototype embeddings interpretable, MCTS is employed during training to project each prototype onto the nearest *concrete subgraph* from the training data, allowing visualization of what each prototype represents.\n        *   **ProtGNN+ with Conditional Subgraph Sampling Module**: For enhanced interpretability and efficiency, ProtGNN+ introduces a novel *conditional subgraph sampling module*. This module identifies which *part* (subgraph) of an *input graph* is most similar to each learned prototype, providing fine-grained explanations. This parameterized approach is more efficient (O(|E|)) and parallelizable than MCTS for input-specific subgraph identification.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **ProtGNN architecture** itself, integrating prototype learning for inherent GNN interpretability.\n        *   A **multi-component objective function** (cross-entropy, cluster, separation, diversity losses) tailored for learning interpretable prototypes.\n        *   **MCTS-guided prototype projection** for visualizing abstract prototypes as concrete subgraphs.\n        *   The **conditional subgraph sampling module** in ProtGNN+ for efficient and parameterized identification of input subgraphs most similar to prototypes.\n    *   **Theoretical Insights**: Provides **Theorem 1**, which offers theoretical understanding of how input graph sampling (as performed in ProtGNN+) affects classification accuracy, stating that if subgraph sampling does not significantly alter the graph embedding, ProtGNN+ can maintain correct predictions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The method was evaluated on a \"wide range of datasets\" (specific datasets not detailed in the provided abstract/intro but implied to be standard graph classification benchmarks). Concrete case studies were also performed to demonstrate interpretability.\n    *   **Key Performance Metrics and Comparison Results**: Extensive results show that ProtGNN and ProtGNN+ achieve classification accuracy \"on par with the non-interpretable counterparts\" while simultaneously providing \"built-in interpretability.\"\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The discreteness of graph edges and the combinatorial nature of graph structures pose challenges for prototype projection and visualization.\n        *   MCTS, while effective for prototype projection, can be computationally intensive.\n        *   The conditional subgraph sampling module requires additional training.\n        *   Theorem 1 relies on specific assumptions about prototype weights and embedding distances.\n    *   **Scope of Applicability**: Primarily demonstrated for graph classification tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: ProtGNN significantly advances the technical state-of-the-art by moving beyond post-hoc GNN explanation methods to a *self-explaining* paradigm. It introduces a novel way to build interpretability directly into the GNN model's reasoning process.\n    *   **Potential Impact on Future Research**: This work provides a new perspective on GNN interpretability, opening avenues for future research in case-based reasoning for graph-structured data. It could lead to more trustworthy and widely applicable GNN models in critical domains by making their decisions transparent.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph-structured data analysis, yet their inherent black-box nature remains a critical barrier to trust and widespread adoption in sensitive applications. Existing *post-hoc* explanation methods often provide incomplete or inaccurate rationales, failing to reflect the GNN's true decision-making process. We introduce **ProtGNN**, a pioneering framework that fundamentally shifts GNN interpretability from external analysis to *built-in* and *self-explaining* capabilities.\n\nProtGNN integrates *prototype learning* directly into the GNN architecture, enabling predictions based on similarity to a set of learned, inherently interpretable prototypes. This provides intuitive, case-based explanations: \"this input graph is similar to prototype X, which belongs to class Y.\" Our novel contributions include a multi-component objective function and a *Monte Carlo Tree Search (MCTS)*-guided method to project abstract prototype embeddings onto concrete, visualizable subgraphs from the training data. Furthermore, **ProtGNN+** introduces an efficient *conditional subgraph sampling module* that identifies the most relevant input subgraphs driving each prediction, offering fine-grained, input-specific explanations. ProtGNN achieves classification accuracy on par with non-interpretable counterparts while providing unparalleled transparency, paving the way for truly trustworthy and debuggable GNNs in critical domains. This work redefines GNN explainability, making their complex decisions inherently transparent.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Self-explaining GNNs",
      "Built-in interpretability",
      "Post-hoc explanation methods",
      "ProtGNN architecture",
      "Prototype learning",
      "Case-based reasoning",
      "Multi-component objective function",
      "Monte Carlo Tree Search (MCTS)",
      "Prototype projection",
      "Conditional subgraph sampling module",
      "Graph classification",
      "On-par classification accuracy",
      "Trustworthy AI"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/7de413da6e0a00e14270cfaed2a31666e7c28747.pdf",
    "citation_key": "zhang2021wgf",
    "metadata": {
      "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
      "authors": [
        "Zaixin Zhang",
        "Qi Liu",
        "Hao Wang",
        "Chengqiang Lu",
        "Chee-Kong Lee"
      ],
      "published_date": "2021",
      "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions\n made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.\n Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
      "file_path": "paper_data/Graph_Neural_Networks/7de413da6e0a00e14270cfaed2a31666e7c28747.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"ProtGNN: Towards Self-Explaining Graph Neural Networks\" \\cite{zhang2021wgf} for a literature review:\n\n### ProtGNN: Towards Self-Explaining Graph Neural Networks \\cite{zhang2021wgf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the predictions made by Graph Neural Networks (GNNs) remains a significant challenge. GNNs are often black-box models, hindering trust and widespread adoption in critical applications.\n    *   **Importance and Challenge**: Understanding GNN rationales is crucial for model debugging, error analysis, and deployment in sensitive areas like medical diagnosis. Existing explanation methods are predominantly *post-hoc*, meaning they employ a separate explanatory model *after* a GNN is trained. This approach often fails to reveal the *original reasoning process* of the GNN, leading to potentially inaccurate or incomplete explanations. The challenge is to build GNNs with *built-in* or *inherent interpretability*.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Current GNN explanation methods are categorized into gradients/features-based, perturbation-based (e.g., GNNExplainer, PGExplainer), decomposition methods, and surrogate methods (e.g., PGM-Explainer, XGNN).\n    *   **Limitations of Previous Solutions**: The primary limitation is that nearly all existing methods are *post-hoc*. This means they do not intrinsically reflect the GNN's decision-making process, potentially leading to explanations that do not precisely fit the original model's reasoning.\n    *   **Positioning**: \\cite{zhang2021wgf} positions ProtGNN as a novel approach to address the limitations of post-hoc explanations by introducing *built-in interpretability* to GNNs. It leverages prototype learning, a form of case-based reasoning, which has not been previously explored for self-explaining GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Prototype Graph Neural Network (ProtGNN)**, which integrates prototype learning with GNNs to achieve self-explaining capabilities.\n        *   It consists of a GNN encoder (e.g., GCN, GAT, GIN) to map input graphs into a latent embedding.\n        *   A prototype layer then computes similarity scores between the input graph's embedding and a set of learned prototype embeddings.\n        *   Predictions are derived from these similarities, providing case-based explanations (i.e., \"this graph is similar to prototype X, which belongs to class Y\").\n    *   **Novelty/Differentiation**:\n        *   **Built-in Interpretability**: Unlike post-hoc methods, ProtGNN's explanations are an integral part of its classification process, directly reflecting its reasoning.\n        *   **Novel Learning Objective**: The objective function includes a cross-entropy loss for accuracy, a *cluster cost* (encouraging embeddings to be close to prototypes of their own class), a *separation cost* (encouraging distance from prototypes of other classes), and a *diversity loss* (penalizing prototypes that are too similar to each other).\n        *   **Monte Carlo Tree Search (MCTS) for Prototype Projection**: To make abstract prototype embeddings interpretable, MCTS is employed during training to project each prototype onto the nearest *concrete subgraph* from the training data, allowing visualization of what each prototype represents.\n        *   **ProtGNN+ with Conditional Subgraph Sampling Module**: For enhanced interpretability and efficiency, ProtGNN+ introduces a novel *conditional subgraph sampling module*. This module identifies which *part* (subgraph) of an *input graph* is most similar to each learned prototype, providing fine-grained explanations. This parameterized approach is more efficient (O(|E|)) and parallelizable than MCTS for input-specific subgraph identification.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **ProtGNN architecture** itself, integrating prototype learning for inherent GNN interpretability.\n        *   A **multi-component objective function** (cross-entropy, cluster, separation, diversity losses) tailored for learning interpretable prototypes.\n        *   **MCTS-guided prototype projection** for visualizing abstract prototypes as concrete subgraphs.\n        *   The **conditional subgraph sampling module** in ProtGNN+ for efficient and parameterized identification of input subgraphs most similar to prototypes.\n    *   **Theoretical Insights**: Provides **Theorem 1**, which offers theoretical understanding of how input graph sampling (as performed in ProtGNN+) affects classification accuracy, stating that if subgraph sampling does not significantly alter the graph embedding, ProtGNN+ can maintain correct predictions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The method was evaluated on a \"wide range of datasets\" (specific datasets not detailed in the provided abstract/intro but implied to be standard graph classification benchmarks). Concrete case studies were also performed to demonstrate interpretability.\n    *   **Key Performance Metrics and Comparison Results**: Extensive results show that ProtGNN and ProtGNN+ achieve classification accuracy \"on par with the non-interpretable counterparts\" while simultaneously providing \"built-in interpretability.\"\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The discreteness of graph edges and the combinatorial nature of graph structures pose challenges for prototype projection and visualization.\n        *   MCTS, while effective for prototype projection, can be computationally intensive.\n        *   The conditional subgraph sampling module requires additional training.\n        *   Theorem 1 relies on specific assumptions about prototype weights and embedding distances.\n    *   **Scope of Applicability**: Primarily demonstrated for graph classification tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: ProtGNN significantly advances the technical state-of-the-art by moving beyond post-hoc GNN explanation methods to a *self-explaining* paradigm. It introduces a novel way to build interpretability directly into the GNN model's reasoning process.\n    *   **Potential Impact on Future Research**: This work provides a new perspective on GNN interpretability, opening avenues for future research in case-based reasoning for graph-structured data. It could lead to more trustworthy and widely applicable GNN models in critical domains by making their decisions transparent.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Self-explaining GNNs",
        "Built-in interpretability",
        "Post-hoc explanation methods",
        "ProtGNN architecture",
        "Prototype learning",
        "Case-based reasoning",
        "Multi-component objective function",
        "Monte Carlo Tree Search (MCTS)",
        "Prototype projection",
        "Conditional subgraph sampling module",
        "Graph classification",
        "On-par classification accuracy",
        "Trustworthy AI"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **survey**: the paper discusses existing methods to set the context but does not aim to comprehensively review or classify them. it proposes a new solution.\n2.  **technical**: the abstract explicitly states, \"in this work, we **propose** prototype graph neural network (protgnn)...\" and describes a \"novel conditional subgraph sampling module is incorporated\". the introduction discusses a technical problem (explaining gnn predictions) and the abstract details the proposed solution and its mechanism. this strongly aligns with presenting new methods, algorithms, or systems.\n3.  **theoretical**: there is no mention of mathematical proofs, theorems, or formal models as the primary contribution.\n4.  **empirical**: the paper does mention \"evaluate our method on a wide range of datasets and perform concrete case studies. extensive results show...\", indicating an empirical component. however, the primary contribution is the *proposal of a new method*, which is then empirically validated. the empirical work supports the technical contribution.\n5.  **case_study**: while \"concrete case studies\" are performed, they are part of the evaluation of a general method, not the sole or primary focus of the paper.\n6.  **position**: the paper identifies a problem and proposes a concrete solution, rather than arguing for a general viewpoint or future direction without a specific technical proposal.\n7.  **short**: the content describes a complete method and evaluation, and aaai is a major conference, not typically for \"short\" papers in the sense of brief communications.\n\nthe core contribution is the development and proposal of a new system/method (protgnn and protgnn+). the empirical evaluation serves to demonstrate the effectiveness of this new technical contribution.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "7de413da6e0a00e14270cfaed2a31666e7c28747.pdf"
  },
  {
    "success": true,
    "doc_id": "305aa21c0cf35e9cb901b2d184315736",
    "summary": "Here's a focused summary of the technical paper \\cite{garg2020z6o} for a literature review:\n\n### Technical Paper Analysis: Generalization and Representational Limits of Graph Neural Networks \\cite{garg2020z6o}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses two fundamental questions about Graph Neural Networks (GNNs):\n        1.  What are the inherent representational limits (expressivity) of GNNs, particularly their ability to distinguish graphs based on important structural properties?\n        2.  What are the generalization properties of GNNs, specifically how well they can predict labels for unseen graphs?\n    *   **Importance and Challenge**: GNNs are widely adopted for graph-structured data, but a comprehensive theoretical understanding of their discriminative power and generalization capabilities is lacking. Understanding these limits is crucial for designing more powerful, reliable, and theoretically grounded GNN architectures. Existing generalization bounds for GNNs are often loose or apply to restricted settings, making it challenging to assess their real-world performance guarantees.\n\n2.  **Related Work & Positioning**\n    *   **Expressivity**: The work builds upon existing research on GNN expressivity, particularly connections to the Weisfeiler-Lehman (WL) test. While some GNNs (like GIN) match WL power and others (like CPNGNN) are proven to be strictly more powerful, this paper *positions itself by demonstrating fundamental limitations* even for these advanced variants. It shows that despite advancements, a broad class of GNNs cannot capture certain important graph properties.\n    *   **Generalization**: It differentiates GNNs from traditional FFNs and RNNs due to their operation on irregular graph structures and the use of local permutation-invariant aggregations. It improves upon prior work, such as VC-dimension bounds established by Scarselli et al. (2018) for a restricted class of GNNs, by providing the *first data-dependent generalization bounds* for message passing GNNs that are significantly tighter and more broadly applicable.\n\n3.  **Technical Approach & Innovation**\n    *   **Representational Limits**:\n        *   The paper employs a constructive proof approach, designing novel pairs of graphs that differ in specific, important graph properties (e.g., girth, diameter, k-clique) but are proven to be indistinguishable by various GNN architectures.\n        *   It introduces a novel graph-theoretic formalism, defining \"port-covers\" and \"port-locally isomorphic\" graphs, to rigorously analyze and prove the indistinguishability for CPNGNNs, which then extends to Locally Unordered GNNs (LU-GNNs) and is adapted for DimeNets.\n    *   **Generalization Bounds**:\n        *   It provides the first data-dependent generalization bounds for message passing GNNs.\n        *   The analysis explicitly accounts for the unique characteristic of local permutation invariance in GNN aggregation functions.\n        *   The bounds are derived for a specific sum-form aggregation, which is shown to extend to models aggregating port-numbered messages, thereby enabling the analysis of CPNGNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   **Impossibility Proofs**: Demonstrates that a wide range of GNNs, including popular LU-GNNs (GraphSAGE, GCN, GIN, GAT), CPNGNNs, and DimeNets, cannot compute fundamental graph properties such as girth, circumference, diameter, radius, conjoint cycle, total number of cycles, and k-clique.\n        *   **Novel Graph-Theoretic Formalism**: Introduces the concept of \"port-local isomorphism\" to formally characterize and prove the indistinguishability of graphs by CPNGNNs.\n    *   **Theoretical Insights or Analysis**:\n        *   **First Data-Dependent Generalization Bounds**: Provides the first data-dependent generalization bounds for message passing GNNs, which are shown to be significantly tighter than existing VC-dimension based guarantees and comparable to Rademacher bounds for recurrent neural networks.\n        *   **Local Permutation Invariance Analysis**: The generalization analysis explicitly incorporates the local permutation invariance of GNN aggregation, a crucial aspect of GNNs that was not adequately addressed in prior generalization studies.\n\n5.  **Experimental Validation**\n    *   The paper primarily relies on **theoretical validation** through rigorous mathematical proofs and the construction of specific counter-example graphs.\n    *   It presents illustrative graph constructions (e.g., Figures 1-4) to visually demonstrate how different GNN variants fail to distinguish graphs that possess distinct structural properties.\n    *   The generalization bounds are derived analytically, and their tightness is discussed in comparison to existing theoretical bounds, rather than through empirical experiments on datasets.\n\n6.  **Limitations & Scope**\n    *   **Representational Limits**: The impossibility results apply to GNNs that rely *entirely on local information* and message passing, even when augmented with spatial cues like port numbering or directional information.\n    *   **CPNGNNs**: The discriminative power of CPNGNNs is shown to be dependent on the specific consistent port numbering chosen, implying that a suboptimal ordering can limit their expressivity.\n    *   **Generalization Bounds**: The bounds are established for message passing GNNs and folding networks, and specifically for aggregation functions that can be expressed in a sum form.\n    *   **Scope**: The analysis focuses on graph classification tasks where a readout function combines node embeddings into a single graph vector for prediction.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the theoretical understanding of GNNs by providing fundamental limits on their expressivity, demonstrating that even powerful spatial variants cannot capture all important graph properties. It also sets a new standard for generalization analysis in GNNs by introducing the first data-dependent bounds that are tighter and account for unique GNN characteristics.\n    *   **Potential Impact on Future Research**:\n        *   **Guides GNN Design**: The identified representational limitations provide critical insights for designing future GNN architectures that can overcome these shortcomings, potentially by incorporating more global information, higher-order structures, or more sophisticated mechanisms for distinguishing local neighborhoods.\n        *   **Improved Theoretical Guarantees**: The novel generalization bounds offer a more robust framework for understanding and predicting GNN performance, enabling better model selection, hyperparameter tuning, and confidence in GNN applications.\n        *   **New Analytical Tools**: The introduced graph-theoretic formalism provides a valuable tool for further theoretical investigations into the discriminative power of GNNs.",
    "intriguing_abstract": "Despite the transformative success of Graph Neural Networks (GNNs) in modeling complex relational data, a comprehensive theoretical understanding of their inherent capabilities and limitations remains elusive. This paper presents groundbreaking insights into both the **expressivity** and **generalization** properties of GNNs, revealing surprising fundamental constraints and offering novel theoretical guarantees.\n\nWe rigorously demonstrate that a broad class of GNNs, including popular architectures like LU-GNNs, CPNGNNs, and DimeNets, are fundamentally incapable of distinguishing graphs based on crucial structural properties such as **girth, diameter, total number of cycles, or the presence of k-cliques**. These impossibility proofs, underpinned by a novel graph-theoretic formalism of **port-local isomorphism**, expose deep limitations on their discriminative power, even for models leveraging spatial or directional information.\n\nFurthermore, we provide the **first data-dependent generalization bounds** for message passing GNNs. These bounds significantly tighten existing theoretical guarantees by explicitly accounting for the unique **local permutation invariance** of GNN aggregation functions, offering a more accurate understanding of their predictive reliability. Our findings are critical for guiding the development of next-generation GNN architectures, providing essential theoretical foundations to overcome current limitations and build more powerful, robust, and theoretically grounded models for graph-structured data.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Representational limits (Expressivity)",
      "Generalization properties",
      "Message passing GNNs",
      "Impossibility proofs",
      "Fundamental graph properties",
      "Port-local isomorphism",
      "Data-dependent generalization bounds",
      "Local permutation invariance",
      "Theoretical validation",
      "Counter-example graphs",
      "GNN architecture design",
      "Weisfeiler-Lehman test"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf",
    "citation_key": "garg2020z6o",
    "metadata": {
      "title": "Generalization and Representational Limits of Graph Neural Networks",
      "authors": [
        "Vikas K. Garg",
        "S. Jegelka",
        "T. Jaakkola"
      ],
      "published_date": "2020",
      "abstract": "We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.",
      "file_path": "paper_data/Graph_Neural_Networks/3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{garg2020z6o} for a literature review:\n\n### Technical Paper Analysis: Generalization and Representational Limits of Graph Neural Networks \\cite{garg2020z6o}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses two fundamental questions about Graph Neural Networks (GNNs):\n        1.  What are the inherent representational limits (expressivity) of GNNs, particularly their ability to distinguish graphs based on important structural properties?\n        2.  What are the generalization properties of GNNs, specifically how well they can predict labels for unseen graphs?\n    *   **Importance and Challenge**: GNNs are widely adopted for graph-structured data, but a comprehensive theoretical understanding of their discriminative power and generalization capabilities is lacking. Understanding these limits is crucial for designing more powerful, reliable, and theoretically grounded GNN architectures. Existing generalization bounds for GNNs are often loose or apply to restricted settings, making it challenging to assess their real-world performance guarantees.\n\n2.  **Related Work & Positioning**\n    *   **Expressivity**: The work builds upon existing research on GNN expressivity, particularly connections to the Weisfeiler-Lehman (WL) test. While some GNNs (like GIN) match WL power and others (like CPNGNN) are proven to be strictly more powerful, this paper *positions itself by demonstrating fundamental limitations* even for these advanced variants. It shows that despite advancements, a broad class of GNNs cannot capture certain important graph properties.\n    *   **Generalization**: It differentiates GNNs from traditional FFNs and RNNs due to their operation on irregular graph structures and the use of local permutation-invariant aggregations. It improves upon prior work, such as VC-dimension bounds established by Scarselli et al. (2018) for a restricted class of GNNs, by providing the *first data-dependent generalization bounds* for message passing GNNs that are significantly tighter and more broadly applicable.\n\n3.  **Technical Approach & Innovation**\n    *   **Representational Limits**:\n        *   The paper employs a constructive proof approach, designing novel pairs of graphs that differ in specific, important graph properties (e.g., girth, diameter, k-clique) but are proven to be indistinguishable by various GNN architectures.\n        *   It introduces a novel graph-theoretic formalism, defining \"port-covers\" and \"port-locally isomorphic\" graphs, to rigorously analyze and prove the indistinguishability for CPNGNNs, which then extends to Locally Unordered GNNs (LU-GNNs) and is adapted for DimeNets.\n    *   **Generalization Bounds**:\n        *   It provides the first data-dependent generalization bounds for message passing GNNs.\n        *   The analysis explicitly accounts for the unique characteristic of local permutation invariance in GNN aggregation functions.\n        *   The bounds are derived for a specific sum-form aggregation, which is shown to extend to models aggregating port-numbered messages, thereby enabling the analysis of CPNGNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   **Impossibility Proofs**: Demonstrates that a wide range of GNNs, including popular LU-GNNs (GraphSAGE, GCN, GIN, GAT), CPNGNNs, and DimeNets, cannot compute fundamental graph properties such as girth, circumference, diameter, radius, conjoint cycle, total number of cycles, and k-clique.\n        *   **Novel Graph-Theoretic Formalism**: Introduces the concept of \"port-local isomorphism\" to formally characterize and prove the indistinguishability of graphs by CPNGNNs.\n    *   **Theoretical Insights or Analysis**:\n        *   **First Data-Dependent Generalization Bounds**: Provides the first data-dependent generalization bounds for message passing GNNs, which are shown to be significantly tighter than existing VC-dimension based guarantees and comparable to Rademacher bounds for recurrent neural networks.\n        *   **Local Permutation Invariance Analysis**: The generalization analysis explicitly incorporates the local permutation invariance of GNN aggregation, a crucial aspect of GNNs that was not adequately addressed in prior generalization studies.\n\n5.  **Experimental Validation**\n    *   The paper primarily relies on **theoretical validation** through rigorous mathematical proofs and the construction of specific counter-example graphs.\n    *   It presents illustrative graph constructions (e.g., Figures 1-4) to visually demonstrate how different GNN variants fail to distinguish graphs that possess distinct structural properties.\n    *   The generalization bounds are derived analytically, and their tightness is discussed in comparison to existing theoretical bounds, rather than through empirical experiments on datasets.\n\n6.  **Limitations & Scope**\n    *   **Representational Limits**: The impossibility results apply to GNNs that rely *entirely on local information* and message passing, even when augmented with spatial cues like port numbering or directional information.\n    *   **CPNGNNs**: The discriminative power of CPNGNNs is shown to be dependent on the specific consistent port numbering chosen, implying that a suboptimal ordering can limit their expressivity.\n    *   **Generalization Bounds**: The bounds are established for message passing GNNs and folding networks, and specifically for aggregation functions that can be expressed in a sum form.\n    *   **Scope**: The analysis focuses on graph classification tasks where a readout function combines node embeddings into a single graph vector for prediction.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper significantly advances the theoretical understanding of GNNs by providing fundamental limits on their expressivity, demonstrating that even powerful spatial variants cannot capture all important graph properties. It also sets a new standard for generalization analysis in GNNs by introducing the first data-dependent bounds that are tighter and account for unique GNN characteristics.\n    *   **Potential Impact on Future Research**:\n        *   **Guides GNN Design**: The identified representational limitations provide critical insights for designing future GNN architectures that can overcome these shortcomings, potentially by incorporating more global information, higher-order structures, or more sophisticated mechanisms for distinguishing local neighborhoods.\n        *   **Improved Theoretical Guarantees**: The novel generalization bounds offer a more robust framework for understanding and predicting GNN performance, enabling better model selection, hyperparameter tuning, and confidence in GNN applications.\n        *   **New Analytical Tools**: The introduced graph-theoretic formalism provides a valuable tool for further theoretical investigations into the discriminative power of GNNs.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Representational limits (Expressivity)",
        "Generalization properties",
        "Message passing GNNs",
        "Impossibility proofs",
        "Fundamental graph properties",
        "Port-local isomorphism",
        "Data-dependent generalization bounds",
        "Local permutation invariance",
        "Theoretical validation",
        "Counter-example graphs",
        "GNN architecture design",
        "Weisfeiler-Lehman test"
      ],
      "paper_type": "based on the abstract and introduction:\n\nthe abstract explicitly states:\n*   \"we **prove** that several important graph properties cannot be computed by gnns\"\n*   \"our treatment includes a novel graph-theoretic **formalism**.\"\n*   \"we provide the ﬁrst data dependent **generalization bounds** for message passing gnns.\"\n*   \"this **analysis** explicitly accounts for the local permutation invariance of gnns.\"\n*   \"our bounds are much tighter than existing **vc-dimension based guarantees** for gnns, and are comparable to **rademacher bounds** for recurrent neural networks.\"\n\nthese phrases strongly align with the criteria for a **theoretical** paper, which focuses on mathematical analysis, proofs, and formal models. the introduction further sets the stage by discussing the fundamental connections of gnns to concepts like message passing and the weisfeiler-lehman graph isomorphism test, reinforcing the theoretical nature of the investigation into their limits and generalization properties.\n\n**classification: theoretical**"
    },
    "file_name": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf"
  },
  {
    "success": true,
    "doc_id": "b8be1daba391d5c1eaf64cf2393598e0",
    "summary": "Here's a focused summary of the technical paper \\cite{fatemi2021dmb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) perform well when a graph structure is provided, but this structure is often unavailable in real-world applications. Inferring a task-specific latent graph structure is challenging.\n    *   **Importance & Challenge:** The space of possible graph structures grows super-exponentially with the number of nodes, making it difficult for task-specific supervision alone to effectively learn both the graph structure and GNN parameters. This leads to a \"supervision starvation\" problem, where edges far from labeled nodes receive insufficient supervision, resulting in poor structure learning and generalization, especially for semi-supervised node classification.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work falls under the category of \"latent graph learning,\" where a graph generator with learnable parameters is used to infer a graph structure. It builds upon approaches that learn node projections to a latent space where similarities define edge weights.\n    *   **Limitations of Previous Solutions:**\n        *   **Similarity graphs:** Heavily dependent on the choice of similarity metric(s).\n        *   **Fully connected graphs:** High computational complexity, limiting applicability to small graphs.\n        *   **Latent graph learning (general):** Suffers from the identified \"supervision starvation\" problem, where many edges (e.g., 48.8% for Cora, 91.6% for Pubmed in their original structures) receive no supervision from the classification loss, leading to suboptimal structures.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{fatemi2021dmb} proposes **SLAPS (Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision)**, a multi-task learning framework that supplements the primary classification task with a self-supervised task to provide additional supervision for learning the graph structure.\n    *   **Novelty:**\n        *   **Self-supervision for Structure Learning:** Unlike prior work that uses self-supervision for learning better GNN parameters, SLAPS leverages it specifically to learn a more robust and task-appropriate *graph structure*.\n        *   **Hypothesis-driven Self-supervision:** It operates on the hypothesis that \"a graph structure suitable for predicting node features is also suitable for predicting node labels.\"\n        *   **Denoising Autoencoder for Adjacency Learning:** The self-supervised task is based on a denoising autoencoder (GNN_DAE). A GNN is trained to reconstruct original node features from noisy versions, using the *learned adjacency matrix*. This forces the adjacency matrix to capture feature-level relationships, which are hypothesized to correlate with label-level relationships.\n    *   **Architecture:** SLAPS comprises four components:\n        1.  **Generator:** Creates an initial adjacency matrix (~A) from node features (X). Two variants: Full Parameterization (FP) for direct adjacency learning (n^2 parameters) or MLP-kNN (MLP projects features, then kNN creates sparse graph).\n        2.  **Adjacency Processor:** Transforms ~A into a symmetric, non-negative, and normalized adjacency matrix (A) suitable for GNNs.\n        3.  **Classifier (GNN_C):** A standard GNN (e.g., 2-layer GCN) that takes X and A to predict node labels.\n        4.  **Self-supervision (GNN_DAE):** A separate GNN that takes noisy features (~X) and the *same learned adjacency A* to reconstruct the original features (X).\n\n*   **Key Technical Contributions**\n    *   **Novel Problem Identification:** Formal identification and quantification of the \"supervision starvation\" problem in latent graph learning, particularly for GNNs with limited receptive fields (e.g., 2-layer GCNs).\n    *   **Novel Self-supervised Objective for Structure Learning:** Introduction of a self-supervised denoising autoencoder task specifically designed to provide additional supervision for the *adjacency matrix learning process*, rather than just GNN parameters.\n    *   **SLAPS Framework:** A comprehensive model integrating a learnable graph generator, an adjacency processor, a GNN classifier, and the proposed self-supervision module into a unified training objective (L = L_C + λ * L_DAE).\n    *   **Scalability:** An implementation designed to scale to graphs with hundreds of thousands of nodes.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experimental study on nine datasets (with thirteen variations) of various sizes and domains.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   SLAPS substantially outperforms existing latent graph learning baselines from various categories.\n        *   Demonstrated scalability to large graphs with hundreds of thousands of nodes.\n        *   The self-supervision component is shown to be crucial for performance, addressing the supervision starvation problem.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on two generator types (FP and MLP-kNN) and suggests exploring more sophisticated graph generators as future work. The self-supervision hypothesis relies on a correlation between feature predictability and label predictability, which might not hold universally across all datasets.\n    *   **Scope of Applicability:** Primarily demonstrated for semi-supervised node classification tasks where an initial graph structure is not readily available. The method is general-purpose and does not rely on domain-specific knowledge.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{fatemi2021dmb} significantly advances latent graph learning by addressing a fundamental limitation (supervision starvation) through a novel self-supervised approach. It enables GNNs to be effectively applied to scenarios lacking explicit graph structures with improved performance and scalability.\n    *   **Potential Impact on Future Research:** The concept of using self-supervision specifically for learning graph structures opens new avenues for research in graph representation learning, especially in data-scarce or structure-agnostic settings. It encourages further exploration of different self-supervised tasks and generator architectures for robust graph inference.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) revolutionize data analysis, yet their reliance on explicit graph structures often limits real-world applicability. Inferring a task-specific latent graph is a critical challenge, frequently hampered by \"supervision starvation\"—a fundamental problem where the vast majority of potential edges receive inadequate supervision, leading to suboptimal structure learning and poor GNN generalization.\n\nWe introduce **SLAPS (Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision)**, a novel multi-task learning framework designed to overcome this bottleneck. SLAPS innovatively employs a self-supervised denoising autoencoder (GNN_DAE) to provide crucial, feature-level supervision directly for the *adjacency matrix learning process*. This unique approach, grounded in the hypothesis that a graph structure suitable for predicting node features also benefits label prediction, forces the graph generator to infer a robust, task-appropriate structure.\n\nOur comprehensive experiments demonstrate SLAPS's superior performance in semi-supervised node classification, substantially outperforming existing latent graph learning baselines. Furthermore, SLAPS scales efficiently to graphs with hundreds of thousands of nodes. By addressing supervision starvation, SLAPS significantly advances graph structure inference, enabling GNNs to thrive in structure-agnostic environments and opening new frontiers in graph representation learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "latent graph structure inference",
      "supervision starvation problem",
      "SLAPS framework",
      "self-supervision for structure learning",
      "multi-task learning",
      "denoising autoencoder",
      "learned adjacency matrix",
      "semi-supervised node classification",
      "scalability to large graphs",
      "latent graph learning",
      "graph generator"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf",
    "citation_key": "fatemi2021dmb",
    "metadata": {
      "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks",
      "authors": [
        "Bahare Fatemi",
        "Layla El Asri",
        "Seyed Mehran Kazemi"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.",
      "file_path": "paper_data/Graph_Neural_Networks/4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{fatemi2021dmb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) perform well when a graph structure is provided, but this structure is often unavailable in real-world applications. Inferring a task-specific latent graph structure is challenging.\n    *   **Importance & Challenge:** The space of possible graph structures grows super-exponentially with the number of nodes, making it difficult for task-specific supervision alone to effectively learn both the graph structure and GNN parameters. This leads to a \"supervision starvation\" problem, where edges far from labeled nodes receive insufficient supervision, resulting in poor structure learning and generalization, especially for semi-supervised node classification.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work falls under the category of \"latent graph learning,\" where a graph generator with learnable parameters is used to infer a graph structure. It builds upon approaches that learn node projections to a latent space where similarities define edge weights.\n    *   **Limitations of Previous Solutions:**\n        *   **Similarity graphs:** Heavily dependent on the choice of similarity metric(s).\n        *   **Fully connected graphs:** High computational complexity, limiting applicability to small graphs.\n        *   **Latent graph learning (general):** Suffers from the identified \"supervision starvation\" problem, where many edges (e.g., 48.8% for Cora, 91.6% for Pubmed in their original structures) receive no supervision from the classification loss, leading to suboptimal structures.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{fatemi2021dmb} proposes **SLAPS (Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision)**, a multi-task learning framework that supplements the primary classification task with a self-supervised task to provide additional supervision for learning the graph structure.\n    *   **Novelty:**\n        *   **Self-supervision for Structure Learning:** Unlike prior work that uses self-supervision for learning better GNN parameters, SLAPS leverages it specifically to learn a more robust and task-appropriate *graph structure*.\n        *   **Hypothesis-driven Self-supervision:** It operates on the hypothesis that \"a graph structure suitable for predicting node features is also suitable for predicting node labels.\"\n        *   **Denoising Autoencoder for Adjacency Learning:** The self-supervised task is based on a denoising autoencoder (GNN_DAE). A GNN is trained to reconstruct original node features from noisy versions, using the *learned adjacency matrix*. This forces the adjacency matrix to capture feature-level relationships, which are hypothesized to correlate with label-level relationships.\n    *   **Architecture:** SLAPS comprises four components:\n        1.  **Generator:** Creates an initial adjacency matrix (~A) from node features (X). Two variants: Full Parameterization (FP) for direct adjacency learning (n^2 parameters) or MLP-kNN (MLP projects features, then kNN creates sparse graph).\n        2.  **Adjacency Processor:** Transforms ~A into a symmetric, non-negative, and normalized adjacency matrix (A) suitable for GNNs.\n        3.  **Classifier (GNN_C):** A standard GNN (e.g., 2-layer GCN) that takes X and A to predict node labels.\n        4.  **Self-supervision (GNN_DAE):** A separate GNN that takes noisy features (~X) and the *same learned adjacency A* to reconstruct the original features (X).\n\n*   **Key Technical Contributions**\n    *   **Novel Problem Identification:** Formal identification and quantification of the \"supervision starvation\" problem in latent graph learning, particularly for GNNs with limited receptive fields (e.g., 2-layer GCNs).\n    *   **Novel Self-supervised Objective for Structure Learning:** Introduction of a self-supervised denoising autoencoder task specifically designed to provide additional supervision for the *adjacency matrix learning process*, rather than just GNN parameters.\n    *   **SLAPS Framework:** A comprehensive model integrating a learnable graph generator, an adjacency processor, a GNN classifier, and the proposed self-supervision module into a unified training objective (L = L_C + λ * L_DAE).\n    *   **Scalability:** An implementation designed to scale to graphs with hundreds of thousands of nodes.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experimental study on nine datasets (with thirteen variations) of various sizes and domains.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   SLAPS substantially outperforms existing latent graph learning baselines from various categories.\n        *   Demonstrated scalability to large graphs with hundreds of thousands of nodes.\n        *   The self-supervision component is shown to be crucial for performance, addressing the supervision starvation problem.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on two generator types (FP and MLP-kNN) and suggests exploring more sophisticated graph generators as future work. The self-supervision hypothesis relies on a correlation between feature predictability and label predictability, which might not hold universally across all datasets.\n    *   **Scope of Applicability:** Primarily demonstrated for semi-supervised node classification tasks where an initial graph structure is not readily available. The method is general-purpose and does not rely on domain-specific knowledge.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{fatemi2021dmb} significantly advances latent graph learning by addressing a fundamental limitation (supervision starvation) through a novel self-supervised approach. It enables GNNs to be effectively applied to scenarios lacking explicit graph structures with improved performance and scalability.\n    *   **Potential Impact on Future Research:** The concept of using self-supervision specifically for learning graph structures opens new avenues for research in graph representation learning, especially in data-scarce or structure-agnostic settings. It encourages further exploration of different self-supervised tasks and generator architectures for robust graph inference.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "latent graph structure inference",
        "supervision starvation problem",
        "SLAPS framework",
        "self-supervision for structure learning",
        "multi-task learning",
        "denoising autoencoder",
        "learned adjacency matrix",
        "semi-supervised node classification",
        "scalability to large graphs",
        "latent graph learning",
        "graph generator"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this work, we **propose** the simultaneous learning of adjacency and gnn parameters with self-supervision, or **slaps**, a **method** that provides more supervision...\"\n*   the introduction further reinforces this: \"we **identify a supervision starvation problem**... we **propose a solution for this problem by adopting a multi-task learning framework**...\"\n*   the paper describes a new approach/system (slaps) to solve a specific technical problem (learning graph structure for gnns when it's not available).\n*   it also mentions an \"experimental study\" demonstrating its performance, which is a common component of technical papers to validate the proposed method.\n\nthis aligns directly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and uses keywords like \"propose\", \"develop\", \"present\", \"algorithm\", \"method\".\n\n**classification: technical**"
    },
    "file_name": "4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf"
  },
  {
    "success": true,
    "doc_id": "f123bf024416d70d8f0dd8bd6814587a",
    "summary": "Here is a focused summary of the provided survey/review paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey chapter focuses on Deep Learning on Graphs, specifically Graph Neural Networks (GNNs), which are identified as the most successful approach in this domain. Its main objective is to systematically organize existing GNN research by introducing fundamental aspects, discussing frontier research, and summarizing application procedures.\n\n2.  **Literature Coverage**\n    The paper aims to systematically organize \"existing research\" on GNNs, implying a broad coverage of the field. However, the specific time period covered, selection criteria, and methodology for literature inclusion are not detailed in this introductory excerpt.\n\n3.  **Classification Framework**\n    The survey organizes the literature along three main axes:\n    *   **Foundations:** Covering popular GNN models, their expressive powers, scalability, interpretability, and robustness.\n    *   **Frontiers:** Discussing advanced research topics such as graph classification, link prediction, graph generation and transformation, graph matching, and graph structure learning.\n    *   **Applications:** Summarizing procedures for exploiting GNNs across a large number of application domains.\n\n4.  **Key Findings & Insights**\n    *   Deep Learning on graphs is a significant research area, with GNNs being the most successful approach for various learning tasks \\cite{zhang2021jqr}.\n    *   The field encompasses fundamental aspects like model design and robustness, alongside advanced frontier topics.\n    *   GNNs are applicable across a wide array of domains, requiring specific procedures for effective utilization.\n    *   The systematic organization highlights the breadth and depth of GNN research from foundational theory to practical applications.\n\n5.  **Research Gaps & Future Directions**\n    While the excerpt mentions summarizing a \"roadmap of the various research topics of GNNs,\" it does not explicitly detail specific research gaps or recommended future directions within this introductory text.\n\n6.  **Survey Contribution**\n    This survey chapter provides a unique and systematic organization of the vast and rapidly growing field of Graph Neural Networks, covering their foundations, frontier research, and diverse applications. It aims to offer a comprehensive and structured overview, serving as a foundational resource for understanding GNNs \\cite{zhang2021jqr}.",
    "intriguing_abstract": "The burgeoning field of Deep Learning on Graphs has revolutionized data analysis, with Graph Neural Networks (GNNs) emerging as the preeminent paradigm for tackling complex relational data. This comprehensive survey systematically disentangles the vast and rapidly growing GNN landscape, offering an unparalleled, structured overview designed to captivate both novice and seasoned researchers.\n\nWe meticulously organize existing GNN research along three critical axes. First, we establish the **foundations**, exploring popular GNN models, their inherent **expressive power**, critical considerations for **scalability**, challenges in **interpretability**, and essential aspects of **robustness**. Second, we navigate the cutting-edge **frontiers**, delving into advanced topics like **graph classification**, **link prediction**, **graph generation and transformation**, **graph matching**, and **graph structure learning**. Finally, we summarize effective procedures for deploying GNNs across a myriad of real-world **application domains**.\n\nThis work serves as an indispensable roadmap, illuminating the breadth and depth of GNN research from theoretical underpinnings to practical implementations. It is a foundational resource for understanding, navigating, and advancing this transformative domain, poised to inspire future innovations in graph-centric AI.",
    "keywords": [
      "Deep Learning on Graphs",
      "Graph Neural Networks (GNNs)",
      "systematic organization",
      "GNN foundations",
      "GNN frontier research",
      "graph classification",
      "link prediction",
      "graph generation and transformation",
      "graph structure learning",
      "application domains",
      "comprehensive overview",
      "foundational resource"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf",
    "citation_key": "zhang2021jqr",
    "metadata": {
      "title": "Graph Neural Networks",
      "authors": [
        "Yuyu Zhang",
        "Xinshi Chen",
        "Yuan Yang",
        "Arun Ramamurthy",
        "Bo Li",
        "Yuan Qi",
        "Le Song"
      ],
      "published_date": "2021",
      "abstract": "Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Although conventional deep learning techniques have achieved huge successes on Euclidean data such as images, or sequence data such as text, there are many applications that are naturally or best represented with a graph structure. This gap has driven a tide in research for deep learning on graphs, among them Graph Neural Networks (GNNs) are the most successful in coping with various learning tasks across a large number of application domains. In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations, frontiers, and applications. We will introduce the fundamental aspects of GNNs ranging from the popular models and their expressive powers, to the scalability, interpretability and robustness of GNNs. Then, we will discuss various frontier research, ranging from graph classification and link prediction, to graph generation and transformation, graph matching and graph structure learning. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our book and summarize the roadmap of the various research topics of GNNs. Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Le Song Mohamed bin Zayed University of Artificial Intelligence, e-mail: dasongle@gmail.com",
      "file_path": "paper_data/Graph_Neural_Networks/510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf",
      "venue": "Deep Learning on Graphs",
      "citationCount": 0,
      "score": 0,
      "summary": "Here is a focused summary of the provided survey/review paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey chapter focuses on Deep Learning on Graphs, specifically Graph Neural Networks (GNNs), which are identified as the most successful approach in this domain. Its main objective is to systematically organize existing GNN research by introducing fundamental aspects, discussing frontier research, and summarizing application procedures.\n\n2.  **Literature Coverage**\n    The paper aims to systematically organize \"existing research\" on GNNs, implying a broad coverage of the field. However, the specific time period covered, selection criteria, and methodology for literature inclusion are not detailed in this introductory excerpt.\n\n3.  **Classification Framework**\n    The survey organizes the literature along three main axes:\n    *   **Foundations:** Covering popular GNN models, their expressive powers, scalability, interpretability, and robustness.\n    *   **Frontiers:** Discussing advanced research topics such as graph classification, link prediction, graph generation and transformation, graph matching, and graph structure learning.\n    *   **Applications:** Summarizing procedures for exploiting GNNs across a large number of application domains.\n\n4.  **Key Findings & Insights**\n    *   Deep Learning on graphs is a significant research area, with GNNs being the most successful approach for various learning tasks \\cite{zhang2021jqr}.\n    *   The field encompasses fundamental aspects like model design and robustness, alongside advanced frontier topics.\n    *   GNNs are applicable across a wide array of domains, requiring specific procedures for effective utilization.\n    *   The systematic organization highlights the breadth and depth of GNN research from foundational theory to practical applications.\n\n5.  **Research Gaps & Future Directions**\n    While the excerpt mentions summarizing a \"roadmap of the various research topics of GNNs,\" it does not explicitly detail specific research gaps or recommended future directions within this introductory text.\n\n6.  **Survey Contribution**\n    This survey chapter provides a unique and systematic organization of the vast and rapidly growing field of Graph Neural Networks, covering their foundations, frontier research, and diverse applications. It aims to offer a comprehensive and structured overview, serving as a foundational resource for understanding GNNs \\cite{zhang2021jqr}.",
      "keywords": [
        "Deep Learning on Graphs",
        "Graph Neural Networks (GNNs)",
        "systematic organization",
        "GNN foundations",
        "GNN frontier research",
        "graph classification",
        "link prediction",
        "graph generation and transformation",
        "graph structure learning",
        "application domains",
        "comprehensive overview",
        "foundational resource"
      ],
      "paper_type": "survey"
    },
    "file_name": "510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf"
  },
  {
    "success": true,
    "doc_id": "8c5e165b329bab804fc0aa69a435ac58",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of publicly available, GNN-tailored graph datasets for training and benchmarking machine learning models in electrical power grid applications. This gap hinders the development of real-time, data-driven solutions for critical power system analyses like power flow, optimal power flow, and cascading failure prediction.\n    *   **Importance and Challenge**: Power grids are critical infrastructure, but their increasing complexity (due to electrification and intermittent energy sources) challenges traditional, computationally intensive analysis methods. Real-time monitoring and prediction of events like cascading failures (which can lead to blackouts) are crucial but difficult due to the rarity of events, scarcity of historical data, and the computational burden of physics-based simulations. Machine learning, particularly Graph Neural Networks (GNNs), offers potential for real-time solutions, but its application is limited by the absence of suitable benchmark datasets and a lack of explainability for critical decision-making.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Machine learning models, especially GNNs, are increasingly explored for power flow (PF) and optimal power flow (OPF) problems \\cite{varbella20242iz}. Similarly, ML is being applied to predict cascading failures \\cite{varbella20242iz}.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional grid analysis methods are computationally slow, preventing real-time use by Transmission System Operators (TSOs) \\cite{varbella20242iz}.\n        *   Existing ML approaches for PF/OPF often rely on synthetic power systems or limited benchmark IEEE grids, lacking comprehensive real-world data \\cite{varbella20242iz}.\n        *   Prior ML methodologies for cascading failures often lack generalizability across diverse failure sets or use less accurate linear approximations of AC power flow \\cite{varbella20242iz}.\n        *   Publicly available power grid datasets (e.g., EGS, PSML, Simbench) are *not specifically tailored for machine learning on graphs* \\cite{varbella20242iz}.\n        *   A critical gap exists in the OGB taxonomy, with no GNN dataset available in the \"society\" domain for graph-level tasks \\cite{varbella20242iz}.\n        *   There is a significant lack of real-world graph datasets with empirical ground-truth explanations for benchmarking GNN explainability methods in power systems \\cite{varbella20242iz}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **PowerGraph** \\cite{varbella20242iz}, a novel, comprehensive, and GNN-tailored benchmark dataset for power grid analysis. It comprises datasets for:\n        *   Node-level tasks: Power Flow (PF) and Optimal Power Flow (OPF) analysis, generated using MATPOWER simulations \\cite{varbella20242iz}.\n        *   Graph-level tasks: Cascading Failure Analysis, generated using the AC physics-based Cascades model \\cite{varbella20242iz}.\n    *   **Novelty/Differentiation**:\n        *   **GNN-Tailored Design**: PowerGraph is explicitly designed for GNNs, providing attributed graphs with node and edge features relevant to power systems \\cite{varbella20242iz}.\n        *   **Comprehensive Scope**: It covers diverse tasks (PF, OPF, cascading failures) and includes four real-world-based power grids (IEEE24, IEEE39, IEEE118, UK transmission system) with varying scales and topologies \\cite{varbella20242iz}.\n        *   **Ground-Truth Explanations**: For cascading failure analysis, it provides empirical ground-truth explanations by identifying \"cascading edges\" (branches that fail during the event), enabling the benchmarking of GNN explainability methods \\cite{varbella20242iz}.\n        *   **Multi-faceted Tasks**: Offers data for regression, binary classification, and multi-class classification at both node and graph levels \\cite{varbella20242iz}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Dataset**: Introduction of PowerGraph \\cite{varbella20242iz}, the first public, GNN-tailored dataset for power flow, optimal power flow, and cascading failure analysis, encompassing diverse power grid scenarios and topologies.\n    *   **Ground-Truth Explainability Data**: Provides the first real-world GNN dataset with empirical ground-truth explanations (cascading edges) for graph-level tasks, specifically for cascading failure events, enabling the development and benchmarking of GNN explainability methods \\cite{varbella20242iz}.\n    *   **Comprehensive Benchmarking**: Offers a complete benchmarking of various GNN architectures (GCNConv, GATConv, GINEConv, TransformerConv) for node-level and graph-level tasks, including performance metrics and architectural insights \\cite{varbella20242iz}.\n    *   **Facilitating Real-Time Analysis**: Enables a data-driven approach for analyzing power flow and cascading failure events in power grids, paving the way for real-time operational tools \\cite{varbella20242iz}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The authors benchmarked four baseline GNN architectures (GCNConv, GATConv, GINEConv, TransformerConv) on the PowerGraph dataset for:\n        *   Node-level tasks: Power Flow (PF) and Optimal Power Flow (OPF) regression \\cite{varbella20242iz}.\n        *   Graph-level tasks: Cascading Failure Analysis, framed as binary classification, multi-class classification, and regression problems \\cite{varbella20242iz}.\n        *   A grid search was performed over the number of message passing layers (1, 2, 3) and hidden dimensionality (8, 16, 32) \\cite{varbella20242iz}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Regression**: Mean Squared Error (MSE) for PF/OPF and cascading failure regression; R2 score for cascading failure regression \\cite{varbella20242iz}.\n        *   **Classification**: Balanced Accuracy for cascading failure classification (due to class imbalance) \\cite{varbella20242iz}.\n        *   **Results**:\n            *   **TransformerConv** consistently achieved the best performance across both node-level (PF/OPF) and graph-level tasks, particularly with three message passing layers \\cite{varbella20242iz}.\n            *   GINEConv and GATConv also showed strong performance, while GCNConv generally performed lower, highlighting the importance of edge features in power grid analysis \\cite{varbella20242iz}.\n            *   GNNs effectively predicted PF and OPF solutions with low MSE across different grid sizes and topologies \\cite{varbella20242iz}.\n            *   Binary and multi-class classification models for cascading failures showed good results \\cite{varbella20242iz}.\n            *   **Regression models for predicting the exact Demand Not Served (DNS) showed limitations**, with R2 scores peaking at 0.43 for IEEE24 but falling below 0.26 for other systems, indicating a need for further research in this area \\cite{varbella20242iz}.\n            *   GNNs sometimes underperformed simpler models like Gradient Boosted Trees (GBT) for node-level regression but were consistently superior for graph-level tasks \\cite{varbella20242iz}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations**:\n        *   The performance of GNNs on *regression tasks*, particularly for predicting the exact Demand Not Served (DNS) in cascading failures and for node-level regression, is a significant limitation. The low R2 scores (below 0.26 for most grids) indicate that current GNN architectures may not be optimally suited for these specific regression problems \\cite{varbella20242iz}.\n        *   For node-level regression, simpler models like Gradient Boosted Trees sometimes outperform GNNs, suggesting a need for GNN architectures specifically optimized for regression in power systems \\cite{varbella20242iz}.\n    *   **Scope of Applicability**: The dataset focuses on static power flow, optimal power flow, and cascading failure analysis using specific IEEE and UK transmission system models. It is tailored for GNNs and their explainability, but dynamic stability or other complex power system phenomena are not explicitly covered.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: PowerGraph \\cite{varbella20242iz} significantly advances the technical state-of-the-art by providing the *first GNN-tailored, publicly available, real-world-based dataset* for power grid analysis, critically including empirical ground-truth explanations. This addresses a major bottleneck in applying GNNs to critical infrastructure.\n    *   **Potential Impact on Future Research**:\n        *   It will democratize the use of GNNs in power systems, enabling researchers to develop more accurate, robust, and explainable ML models for power flow, optimal power flow, and cascading failure prediction \\cite{varbella20242iz}.\n        *   The dataset will spur research into novel GNN architectures specifically optimized for regression tasks in power systems, given the current limitations observed \\cite{varbella20242iz}.\n        *   The inclusion of ground-truth explanations will drive advancements in GNN explainability methods, which are crucial for building trust and providing actionable insights to TSOs regarding vulnerable grid components \\cite{varbella20242iz}.\n        *   Even minor improvements in classification tasks, facilitated by this dataset, can have a significant impact on real-time decision-making and operational reliability in critical power infrastructures \\cite{varbella20242iz}.",
    "intriguing_abstract": "The increasing complexity of modern power grids demands real-time, data-driven solutions for critical analyses like power flow (PF), optimal power flow (OPF), and cascading failure prediction. While Graph Neural Networks (GNNs) hold immense promise for these challenges, their application has been severely hampered by a lack of publicly available, GNN-tailored benchmark datasets.\n\nWe introduce **PowerGraph**, a novel and comprehensive dataset designed to bridge this crucial gap. PowerGraph provides attributed graphs for diverse node-level (PF, OPF) and graph-level (cascading failure) tasks across four real-world-based power grids. Uniquely, it offers empirical ground-truth explanations for cascading failures, identifying \"cascading edges\" to enable rigorous benchmarking of GNN explainability methods—a vital step for building trust in critical infrastructure applications. Our extensive benchmarking of various GNN architectures reveals TransformerConv as a top performer, while also highlighting current limitations in exact regression tasks. PowerGraph democratizes GNN research in power systems, fostering the development of robust, accurate, and explainable AI models essential for real-time operational decision-making by Transmission System Operators (TSOs) and enhancing grid resilience.",
    "keywords": [
      "PowerGraph dataset",
      "Graph Neural Networks (GNNs)",
      "electrical power grids",
      "power flow analysis",
      "optimal power flow",
      "cascading failure prediction",
      "GNN-tailored datasets",
      "ground-truth explanations",
      "real-time power system analysis",
      "TransformerConv architecture",
      "GNN explainability methods",
      "Demand Not Served (DNS) prediction",
      "benchmarking GNNs",
      "node and edge features"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf",
    "citation_key": "varbella20242iz",
    "metadata": {
      "title": "PowerGraph: A power grid benchmark dataset for graph neural networks",
      "authors": [
        "Anna Varbella",
        "Kenza Amara",
        "B. Gjorgiev",
        "G. Sansavini"
      ],
      "published_date": "2024",
      "abstract": "Power grids are critical infrastructures of paramount importance to modern society and, therefore, engineered to operate under diverse conditions and failures. The ongoing energy transition poses new challenges for the decision-makers and system operators. Therefore, developing grid analysis algorithms is important for supporting reliable operations. These key tools include power flow analysis and system security analysis, both needed for effective operational and strategic planning. The literature review shows a growing trend of machine learning (ML) models that perform these analyses effectively. In particular, Graph Neural Networks (GNNs) stand out in such applications because of the graph-based structure of power grids. However, there is a lack of publicly available graph datasets for training and benchmarking ML models in electrical power grid applications. First, we present PowerGraph, which comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. Second, we provide ground-truth explanations for the cascading failure analysis. Finally, we perform a complete benchmarking of GNN methods for node-level and graph-level tasks and explainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse tasks that includes power flow and fault scenarios with real-world explanations, providing a valuable resource for developing improved GNN models for node-level, graph-level tasks and explainability methods in power system modeling. The dataset is available at https://figshare.com/articles/dataset/PowerGraph/22820534 and the code at https://github.com/PowerGraph-Datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of publicly available, GNN-tailored graph datasets for training and benchmarking machine learning models in electrical power grid applications. This gap hinders the development of real-time, data-driven solutions for critical power system analyses like power flow, optimal power flow, and cascading failure prediction.\n    *   **Importance and Challenge**: Power grids are critical infrastructure, but their increasing complexity (due to electrification and intermittent energy sources) challenges traditional, computationally intensive analysis methods. Real-time monitoring and prediction of events like cascading failures (which can lead to blackouts) are crucial but difficult due to the rarity of events, scarcity of historical data, and the computational burden of physics-based simulations. Machine learning, particularly Graph Neural Networks (GNNs), offers potential for real-time solutions, but its application is limited by the absence of suitable benchmark datasets and a lack of explainability for critical decision-making.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Machine learning models, especially GNNs, are increasingly explored for power flow (PF) and optimal power flow (OPF) problems \\cite{varbella20242iz}. Similarly, ML is being applied to predict cascading failures \\cite{varbella20242iz}.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional grid analysis methods are computationally slow, preventing real-time use by Transmission System Operators (TSOs) \\cite{varbella20242iz}.\n        *   Existing ML approaches for PF/OPF often rely on synthetic power systems or limited benchmark IEEE grids, lacking comprehensive real-world data \\cite{varbella20242iz}.\n        *   Prior ML methodologies for cascading failures often lack generalizability across diverse failure sets or use less accurate linear approximations of AC power flow \\cite{varbella20242iz}.\n        *   Publicly available power grid datasets (e.g., EGS, PSML, Simbench) are *not specifically tailored for machine learning on graphs* \\cite{varbella20242iz}.\n        *   A critical gap exists in the OGB taxonomy, with no GNN dataset available in the \"society\" domain for graph-level tasks \\cite{varbella20242iz}.\n        *   There is a significant lack of real-world graph datasets with empirical ground-truth explanations for benchmarking GNN explainability methods in power systems \\cite{varbella20242iz}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **PowerGraph** \\cite{varbella20242iz}, a novel, comprehensive, and GNN-tailored benchmark dataset for power grid analysis. It comprises datasets for:\n        *   Node-level tasks: Power Flow (PF) and Optimal Power Flow (OPF) analysis, generated using MATPOWER simulations \\cite{varbella20242iz}.\n        *   Graph-level tasks: Cascading Failure Analysis, generated using the AC physics-based Cascades model \\cite{varbella20242iz}.\n    *   **Novelty/Differentiation**:\n        *   **GNN-Tailored Design**: PowerGraph is explicitly designed for GNNs, providing attributed graphs with node and edge features relevant to power systems \\cite{varbella20242iz}.\n        *   **Comprehensive Scope**: It covers diverse tasks (PF, OPF, cascading failures) and includes four real-world-based power grids (IEEE24, IEEE39, IEEE118, UK transmission system) with varying scales and topologies \\cite{varbella20242iz}.\n        *   **Ground-Truth Explanations**: For cascading failure analysis, it provides empirical ground-truth explanations by identifying \"cascading edges\" (branches that fail during the event), enabling the benchmarking of GNN explainability methods \\cite{varbella20242iz}.\n        *   **Multi-faceted Tasks**: Offers data for regression, binary classification, and multi-class classification at both node and graph levels \\cite{varbella20242iz}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Dataset**: Introduction of PowerGraph \\cite{varbella20242iz}, the first public, GNN-tailored dataset for power flow, optimal power flow, and cascading failure analysis, encompassing diverse power grid scenarios and topologies.\n    *   **Ground-Truth Explainability Data**: Provides the first real-world GNN dataset with empirical ground-truth explanations (cascading edges) for graph-level tasks, specifically for cascading failure events, enabling the development and benchmarking of GNN explainability methods \\cite{varbella20242iz}.\n    *   **Comprehensive Benchmarking**: Offers a complete benchmarking of various GNN architectures (GCNConv, GATConv, GINEConv, TransformerConv) for node-level and graph-level tasks, including performance metrics and architectural insights \\cite{varbella20242iz}.\n    *   **Facilitating Real-Time Analysis**: Enables a data-driven approach for analyzing power flow and cascading failure events in power grids, paving the way for real-time operational tools \\cite{varbella20242iz}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The authors benchmarked four baseline GNN architectures (GCNConv, GATConv, GINEConv, TransformerConv) on the PowerGraph dataset for:\n        *   Node-level tasks: Power Flow (PF) and Optimal Power Flow (OPF) regression \\cite{varbella20242iz}.\n        *   Graph-level tasks: Cascading Failure Analysis, framed as binary classification, multi-class classification, and regression problems \\cite{varbella20242iz}.\n        *   A grid search was performed over the number of message passing layers (1, 2, 3) and hidden dimensionality (8, 16, 32) \\cite{varbella20242iz}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Regression**: Mean Squared Error (MSE) for PF/OPF and cascading failure regression; R2 score for cascading failure regression \\cite{varbella20242iz}.\n        *   **Classification**: Balanced Accuracy for cascading failure classification (due to class imbalance) \\cite{varbella20242iz}.\n        *   **Results**:\n            *   **TransformerConv** consistently achieved the best performance across both node-level (PF/OPF) and graph-level tasks, particularly with three message passing layers \\cite{varbella20242iz}.\n            *   GINEConv and GATConv also showed strong performance, while GCNConv generally performed lower, highlighting the importance of edge features in power grid analysis \\cite{varbella20242iz}.\n            *   GNNs effectively predicted PF and OPF solutions with low MSE across different grid sizes and topologies \\cite{varbella20242iz}.\n            *   Binary and multi-class classification models for cascading failures showed good results \\cite{varbella20242iz}.\n            *   **Regression models for predicting the exact Demand Not Served (DNS) showed limitations**, with R2 scores peaking at 0.43 for IEEE24 but falling below 0.26 for other systems, indicating a need for further research in this area \\cite{varbella20242iz}.\n            *   GNNs sometimes underperformed simpler models like Gradient Boosted Trees (GBT) for node-level regression but were consistently superior for graph-level tasks \\cite{varbella20242iz}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations**:\n        *   The performance of GNNs on *regression tasks*, particularly for predicting the exact Demand Not Served (DNS) in cascading failures and for node-level regression, is a significant limitation. The low R2 scores (below 0.26 for most grids) indicate that current GNN architectures may not be optimally suited for these specific regression problems \\cite{varbella20242iz}.\n        *   For node-level regression, simpler models like Gradient Boosted Trees sometimes outperform GNNs, suggesting a need for GNN architectures specifically optimized for regression in power systems \\cite{varbella20242iz}.\n    *   **Scope of Applicability**: The dataset focuses on static power flow, optimal power flow, and cascading failure analysis using specific IEEE and UK transmission system models. It is tailored for GNNs and their explainability, but dynamic stability or other complex power system phenomena are not explicitly covered.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: PowerGraph \\cite{varbella20242iz} significantly advances the technical state-of-the-art by providing the *first GNN-tailored, publicly available, real-world-based dataset* for power grid analysis, critically including empirical ground-truth explanations. This addresses a major bottleneck in applying GNNs to critical infrastructure.\n    *   **Potential Impact on Future Research**:\n        *   It will democratize the use of GNNs in power systems, enabling researchers to develop more accurate, robust, and explainable ML models for power flow, optimal power flow, and cascading failure prediction \\cite{varbella20242iz}.\n        *   The dataset will spur research into novel GNN architectures specifically optimized for regression tasks in power systems, given the current limitations observed \\cite{varbella20242iz}.\n        *   The inclusion of ground-truth explanations will drive advancements in GNN explainability methods, which are crucial for building trust and providing actionable insights to TSOs regarding vulnerable grid components \\cite{varbella20242iz}.\n        *   Even minor improvements in classification tasks, facilitated by this dataset, can have a significant impact on real-time decision-making and operational reliability in critical power infrastructures \\cite{varbella20242iz}.",
      "keywords": [
        "PowerGraph dataset",
        "Graph Neural Networks (GNNs)",
        "electrical power grids",
        "power flow analysis",
        "optimal power flow",
        "cascading failure prediction",
        "GNN-tailored datasets",
        "ground-truth explanations",
        "real-time power system analysis",
        "TransformerConv architecture",
        "GNN explainability methods",
        "Demand Not Served (DNS) prediction",
        "benchmarking GNNs",
        "node and edge features"
      ],
      "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n*   **abstract keywords/themes:**\n    *   \"lack of publicly available graph datasets for training and benchmarking ml models\" - identifies a data-related problem.\n    *   \"we present powergraph, which comprises gnn-tailored datasets\" - primary contribution is a new **dataset**.\n    *   \"we perform a complete benchmarking of gnn methods\" - explicitly states an **experimental/data-driven study**.\n    *   \"powergraph is a multifaceted gnn dataset... providing a valuable resource for developing improved gnn models\" - emphasizes the dataset's role in future empirical work.\n    *   \"the dataset is available at... and the code at...\" - indicates a concrete, data-driven resource.\n\n*   **introduction keywords/themes:**\n    *   discusses the need for \"online tools for effective power systems monitoring\" and the limitations of \"current methods.\"\n    *   highlights \"machine learning techniques, particularly gnns, offer significant potential for providing real-time solutions.\"\n    *   sets the stage for an investigation into how gnns perform on power grid tasks, which the dataset and benchmarking facilitate.\n\n*   **matching to criteria:**\n    *   **empirical** - \"data-driven studies with statistical analysis.\" this paper introduces a new dataset and uses it to perform a \"complete benchmarking of gnn methods,\" which is a data-driven experimental study. the creation of the dataset itself is a foundational step for empirical research.\n\nwhile the paper *presents* a resource (the dataset), its core contribution, as described, includes the *use* of that resource for benchmarking, which is an empirical activity. the neurips \"datasets and benchmarks\" track further reinforces this classification, as papers in this track are inherently empirical in nature, focusing on data and its experimental evaluation."
    },
    "file_name": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf"
  },
  {
    "success": true,
    "doc_id": "3e01d6d7188f74b589fe94b9ec034cd0",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and formatting requirements:\n\n### Technical Paper Analysis: A Survey on Oversmoothing in Graph Neural Networks \\cite{rusch2023xev}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer from \"over-smoothing\" as their depth increases, where node features become increasingly similar, converging to a non-informative constant value. This limits the ability to build deep, expressive GNNs.\n    *   **Importance and Challenge:** Deep GNNs are crucial for learning complex, long-range interactions in relational data, similar to the success of deep convolutional neural networks (CNNs). Over-smoothing is a key impediment to developing deep GNNs, impacting their performance on various tasks, especially on heterophilic graphs where node labels differ from neighbors. Existing definitions and measures of over-smoothing lack formal rigor and a unified framework.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper builds upon previous attempts to quantify over-smoothing using measures like graph Dirichlet energy, cosine similarity, and Mean Average Distance (MAD). It also reviews various mitigation strategies proposed in the literature, categorizing them into normalization/regularization, changes in GNN dynamics, and residual connections.\n    *   **Limitations of Previous Solutions:**\n        *   Previous work lacked a formal, unified definition of over-smoothing, leading to a conceptual gap.\n        *   Many proposed measures (e.g., MAD) are not sufficiently rigorous or robust, failing to satisfy basic conditions for a node-similarity measure (e.g., MAD can be zero even if features are not constant in scalar cases).\n        *   Previous approaches often did not explicitly consider the *rate* of convergence of over-smoothing measures with increasing GNN layers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces an axiomatic definition of over-smoothing based on a \"node-similarity measure\" $\\phi$. This measure must satisfy three conditions:\n        1.  $\\phi(X) = 0$ if and only if all node features $X_i$ are identical to a constant vector $c$.\n        2.  $\\phi$ must satisfy subadditivity (a form of triangle inequality).\n        3.  Over-smoothing is defined as the *layer-wise exponential convergence* of $\\phi(X^n)$ to zero, i.e., $\\phi(X^n) \\le C_1 e^{-C_2 n}$.\n    *   **Novelty/Difference:**\n        *   **Formal Unification:** This axiomatic definition provides a rigorous and unified framework for understanding and quantifying over-smoothing, addressing the conceptual gap in prior work.\n        *   **Quantitative Rigor:** It explicitly introduces the requirement of *exponential convergence* as a defining characteristic of over-smoothing, distinguishing it from mere algebraic convergence.\n        *   **Measure Validation:** It allows for the validation of existing and new over-smoothing measures, ruling out problematic ones (like MAD in certain contexts) that do not meet the axiomatic criteria.\n        *   **Extension to Continuous-Time GNNs:** The definition is extended to the emerging field of continuous-time GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods/Techniques:**\n        *   An axiomatic, unified, and tractable definition of over-smoothing for GNNs \\cite{rusch2023xev}.\n        *   Identification of the graph Dirichlet energy as a bona fide node-similarity measure that satisfies all axiomatic conditions, and critical analysis of other measures like Mean Average Distance (MAD).\n    *   **Theoretical Insights/Analysis:**\n        *   The emphasis on *exponential convergence* as a necessary condition for over-smoothing, providing a more stringent quantitative measure.\n        *   The insight that mitigating over-smoothing (preventing feature convergence) is a *necessary but not sufficient* condition for building deep, expressive GNNs that perform well on learning tasks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Evaluation of Over-smoothing Measures:** The paper empirically demonstrated the exponential convergence of Dirichlet energy and MAD on various GNN architectures (GCN, GAT, GraphSAGE) across different graph scales.\n        *   **Evaluation of Mitigation Methods:** Six representative over-smoothing mitigation methods (DropEdge, PairNorm, GraphCON, G2, Res-GCN, GCNII) were empirically tested for their effectiveness in preventing the exponential decay of Dirichlet energy.\n        *   **Expressivity vs. Over-smoothing Mitigation:** An experiment was conducted to show that a GCN with a bias term could prevent Dirichlet energy from converging but still perform poorly on a learning task, highlighting the \"necessary but not sufficient\" insight.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Metrics:** Layer-wise Dirichlet energy and Mean Average Distance (MAD) were used to quantify over-smoothing, plotted logarithmically against layer number.\n        *   **Results for Over-smoothing:** GCN, GAT, and GraphSAGE all showed exponential convergence of both Dirichlet energy and MAD to zero on all tested datasets.\n        *   **Results for Mitigation Methods:**\n            *   DropEdge-GCN and Res-GCN still suffered from exponential convergence of Dirichlet energy.\n            *   G2-GCN, GraphCON-GCN, PairNorm, and GCNII successfully mitigated over-smoothing by keeping the layer-wise Dirichlet energy approximately constant.\n        *   **Datasets:** Experiments were conducted on small-scale (Texas), medium-scale (Cora), and large-scale (Cornell5) real-world graph datasets, using 128-dimensional random normal features to isolate GNN dynamics.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The axiomatic definition initially assumes connected graphs, though it is generalized to disconnected graphs by summing measures over connected components. The paper focuses primarily on message-passing GNNs.\n    *   **Scope of Applicability:** While the definition is general, the empirical evaluation focuses on specific GNN architectures and mitigation strategies. The \"necessary but not sufficient\" insight suggests that simply preventing over-smoothing is not a silver bullet for deep GNNs, implying further research is needed on maintaining expressivity.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The paper significantly advances the understanding of over-smoothing by providing a rigorous, unified, and quantitative definition. This formalization allows for clearer analysis and comparison of over-smoothing measures and mitigation techniques.\n    *   **Potential Impact on Future Research:**\n        *   It provides a robust framework for evaluating new GNN architectures and mitigation strategies against over-smoothing.\n        *   The \"necessary but not sufficient\" insight redirects future research towards not just preventing over-smoothing, but also ensuring that deep GNNs retain or enhance their expressive power.\n        *   The extension of the definition to continuous-time GNNs opens avenues for research in this rapidly growing sub-field.\n        *   It encourages the development of new node-similarity measures that strictly adhere to the proposed axioms.",
    "intriguing_abstract": "The quest for deep Graph Neural Networks (GNNs) capable of capturing complex relational patterns is fundamentally hampered by over-smoothing, a critical phenomenon where node features become indistinguishable with increasing network depth. This paper presents a groundbreaking axiomatic definition of over-smoothing, rigorously formalizing it as the *layer-wise exponential convergence* of a node-similarity measure to zero. This unified framework allows for the precise validation of existing metrics, confirming graph Dirichlet energy as a robust measure while critically analyzing others like Mean Average Distance.\n\nOur empirical investigations across diverse GNN architectures and datasets not only validate this exponential decay but also reveal a crucial insight: mitigating feature convergence is a *necessary but not sufficient* condition for developing truly expressive deep GNNs. By extending this definition to continuous-time GNNs, we provide a robust, quantitative foundation for evaluating future architectures and mitigation strategies. This work fundamentally advances our understanding of GNN limitations, redirecting research towards building models that achieve both depth and sustained expressivity, unlocking the full potential of graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Over-smoothing",
      "Axiomatic definition",
      "Node-similarity measure",
      "Exponential convergence",
      "Graph Dirichlet energy",
      "Deep GNNs",
      "Over-smoothing mitigation",
      "Heterophilic graphs",
      "Continuous-time GNNs",
      "Expressivity",
      "Necessary but not sufficient condition"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/90dead8a056b848be164c2e5cdadfa2e191c3265.pdf",
    "citation_key": "rusch2023xev",
    "metadata": {
      "title": "A Survey on Oversmoothing in Graph Neural Networks",
      "authors": [
        "T. Konstantin Rusch",
        "Michael M. Bronstein",
        "Siddhartha Mishra"
      ],
      "published_date": "2023",
      "abstract": "Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/90dead8a056b848be164c2e5cdadfa2e191c3265.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and formatting requirements:\n\n### Technical Paper Analysis: A Survey on Oversmoothing in Graph Neural Networks \\cite{rusch2023xev}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) suffer from \"over-smoothing\" as their depth increases, where node features become increasingly similar, converging to a non-informative constant value. This limits the ability to build deep, expressive GNNs.\n    *   **Importance and Challenge:** Deep GNNs are crucial for learning complex, long-range interactions in relational data, similar to the success of deep convolutional neural networks (CNNs). Over-smoothing is a key impediment to developing deep GNNs, impacting their performance on various tasks, especially on heterophilic graphs where node labels differ from neighbors. Existing definitions and measures of over-smoothing lack formal rigor and a unified framework.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper builds upon previous attempts to quantify over-smoothing using measures like graph Dirichlet energy, cosine similarity, and Mean Average Distance (MAD). It also reviews various mitigation strategies proposed in the literature, categorizing them into normalization/regularization, changes in GNN dynamics, and residual connections.\n    *   **Limitations of Previous Solutions:**\n        *   Previous work lacked a formal, unified definition of over-smoothing, leading to a conceptual gap.\n        *   Many proposed measures (e.g., MAD) are not sufficiently rigorous or robust, failing to satisfy basic conditions for a node-similarity measure (e.g., MAD can be zero even if features are not constant in scalar cases).\n        *   Previous approaches often did not explicitly consider the *rate* of convergence of over-smoothing measures with increasing GNN layers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces an axiomatic definition of over-smoothing based on a \"node-similarity measure\" $\\phi$. This measure must satisfy three conditions:\n        1.  $\\phi(X) = 0$ if and only if all node features $X_i$ are identical to a constant vector $c$.\n        2.  $\\phi$ must satisfy subadditivity (a form of triangle inequality).\n        3.  Over-smoothing is defined as the *layer-wise exponential convergence* of $\\phi(X^n)$ to zero, i.e., $\\phi(X^n) \\le C_1 e^{-C_2 n}$.\n    *   **Novelty/Difference:**\n        *   **Formal Unification:** This axiomatic definition provides a rigorous and unified framework for understanding and quantifying over-smoothing, addressing the conceptual gap in prior work.\n        *   **Quantitative Rigor:** It explicitly introduces the requirement of *exponential convergence* as a defining characteristic of over-smoothing, distinguishing it from mere algebraic convergence.\n        *   **Measure Validation:** It allows for the validation of existing and new over-smoothing measures, ruling out problematic ones (like MAD in certain contexts) that do not meet the axiomatic criteria.\n        *   **Extension to Continuous-Time GNNs:** The definition is extended to the emerging field of continuous-time GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods/Techniques:**\n        *   An axiomatic, unified, and tractable definition of over-smoothing for GNNs \\cite{rusch2023xev}.\n        *   Identification of the graph Dirichlet energy as a bona fide node-similarity measure that satisfies all axiomatic conditions, and critical analysis of other measures like Mean Average Distance (MAD).\n    *   **Theoretical Insights/Analysis:**\n        *   The emphasis on *exponential convergence* as a necessary condition for over-smoothing, providing a more stringent quantitative measure.\n        *   The insight that mitigating over-smoothing (preventing feature convergence) is a *necessary but not sufficient* condition for building deep, expressive GNNs that perform well on learning tasks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Evaluation of Over-smoothing Measures:** The paper empirically demonstrated the exponential convergence of Dirichlet energy and MAD on various GNN architectures (GCN, GAT, GraphSAGE) across different graph scales.\n        *   **Evaluation of Mitigation Methods:** Six representative over-smoothing mitigation methods (DropEdge, PairNorm, GraphCON, G2, Res-GCN, GCNII) were empirically tested for their effectiveness in preventing the exponential decay of Dirichlet energy.\n        *   **Expressivity vs. Over-smoothing Mitigation:** An experiment was conducted to show that a GCN with a bias term could prevent Dirichlet energy from converging but still perform poorly on a learning task, highlighting the \"necessary but not sufficient\" insight.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Metrics:** Layer-wise Dirichlet energy and Mean Average Distance (MAD) were used to quantify over-smoothing, plotted logarithmically against layer number.\n        *   **Results for Over-smoothing:** GCN, GAT, and GraphSAGE all showed exponential convergence of both Dirichlet energy and MAD to zero on all tested datasets.\n        *   **Results for Mitigation Methods:**\n            *   DropEdge-GCN and Res-GCN still suffered from exponential convergence of Dirichlet energy.\n            *   G2-GCN, GraphCON-GCN, PairNorm, and GCNII successfully mitigated over-smoothing by keeping the layer-wise Dirichlet energy approximately constant.\n        *   **Datasets:** Experiments were conducted on small-scale (Texas), medium-scale (Cora), and large-scale (Cornell5) real-world graph datasets, using 128-dimensional random normal features to isolate GNN dynamics.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The axiomatic definition initially assumes connected graphs, though it is generalized to disconnected graphs by summing measures over connected components. The paper focuses primarily on message-passing GNNs.\n    *   **Scope of Applicability:** While the definition is general, the empirical evaluation focuses on specific GNN architectures and mitigation strategies. The \"necessary but not sufficient\" insight suggests that simply preventing over-smoothing is not a silver bullet for deep GNNs, implying further research is needed on maintaining expressivity.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The paper significantly advances the understanding of over-smoothing by providing a rigorous, unified, and quantitative definition. This formalization allows for clearer analysis and comparison of over-smoothing measures and mitigation techniques.\n    *   **Potential Impact on Future Research:**\n        *   It provides a robust framework for evaluating new GNN architectures and mitigation strategies against over-smoothing.\n        *   The \"necessary but not sufficient\" insight redirects future research towards not just preventing over-smoothing, but also ensuring that deep GNNs retain or enhance their expressive power.\n        *   The extension of the definition to continuous-time GNNs opens avenues for research in this rapidly growing sub-field.\n        *   It encourages the development of new node-similarity measures that strictly adhere to the proposed axioms.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Over-smoothing",
        "Axiomatic definition",
        "Node-similarity measure",
        "Exponential convergence",
        "Graph Dirichlet energy",
        "Deep GNNs",
        "Over-smoothing mitigation",
        "Heterophilic graphs",
        "Continuous-time GNNs",
        "Expressivity",
        "Necessary but not sufficient condition"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "90dead8a056b848be164c2e5cdadfa2e191c3265.pdf"
  },
  {
    "success": true,
    "doc_id": "8fe2e0856b28a17ee5aaddc3ffe29f68",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the specific technical problem of understanding the expressive power of Graph Neural Networks (GNNs) by examining their ability to detect and count substructures within graphs \\cite{chen2020e6g}.\n    *   **Importance & Challenge:** Substructure counting is crucial for many real-world tasks in computational chemistry, biology, and social network analysis. Existing GNN expressive power studies (e.g., graph isomorphism testing, universal function approximation) have limitations: isomorphism testing often finds GNNs already near maximally powerful, and universal approximators are impractical. A new, intuitive, and practically relevant perspective is needed to guide the development of more powerful GNN architectures \\cite{chen2020e6g}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior work on GNN expressive power primarily focused on distinguishing non-isomorphic graphs (e.g., showing MPNNs are at most as powerful as 1-WL/2-WL \\cite{chen2020e6g}) or approximating permutation-invariant functions (often leading to impractical models \\cite{chen2020e6g}). Other works provided impossibility results for GNNs on specific graph properties like k-cliques \\cite{chen2020e6g}.\n    *   **Limitations of Previous Solutions:** Graph isomorphism testing, while theoretically interesting, is less relevant to many practical tasks, and GNNs are already quite powerful in this regard. Universal function approximators are computationally infeasible.\n    *   **Positioning:** This work introduces substructure counting as a novel and practically relevant lens to characterize GNN expressive power, distinguishing between \"induced-subgraph-count\" and \"subgraph-count\" \\cite{chen2020e6g}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper formalizes the problem of substructure counting for attributed graphs, defining two types: `subgraph-count` (CS) and `induced-subgraph-count` (CI) \\cite{chen2020e6g}. It then theoretically analyzes the capabilities of popular GNN architectures (Message Passing Neural Networks (MPNNs), k-Weisfeiler-Lehman (k-WL) tests, and k-Invariant Graph Networks (k-IGNs)) to perform these counts.\n    *   **Innovation:**\n        *   It systematically investigates substructure counting as a measure of GNN expressive power, a novel perspective.\n        *   It establishes a theoretical equivalence between 2-WL and 2-IGNs in distinguishing non-isomorphic graphs, partly answering an open problem \\cite{chen2020e6g}.\n        *   Motivated by the limitations of existing GNNs, it proposes a novel GNN architecture called **Local Relational Pooling (LRP)**, designed to effectively count substructures by leveraging local neighborhood information and learning relevant patterns \\cite{chen2020e6g}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Formal definitions and framework for evaluating GNNs' ability to perform `subgraph-count` and `induced-subgraph-count` for attributed graphs \\cite{chen2020e6g}.\n        *   Introduction of the **Local Relational Pooling (LRP)** model, a new GNN architecture capable of learning and counting substructures by processing local relational information \\cite{chen2020e6g}.\n    *   **Theoretical Insights/Analysis:**\n        *   **Negative Results:** Proves that MPNNs and 2-IGNs *cannot* perform induced-subgraph-count for *any connected pattern with 3 or more nodes* \\cite{chen2020e6g}.\n        *   **Positive Results:** Shows that MPNNs and 2-IGNs *can* perform subgraph-count for *star-shaped patterns* \\cite{chen2020e6g}.\n        *   **Equivalence:** Establishes that 2-WL and 2-IGNs have equivalent discriminative power for non-isomorphic graphs \\cite{chen2020e6g}.\n        *   **k-WL/k-IGN Hierarchy:** Demonstrates that k-WL and k-IGNs can count (induced) subgraphs for patterns of size at most k \\cite{chen2020e6g}.\n        *   **Finite Depth Limitations:** Proves that T iterations of k-WL cannot induced-subgraph-count path patterns of `(k+1)2T` or more nodes, highlighting depth limitations \\cite{chen2020e6g}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Experiments were performed on synthetic graphs to empirically support the theoretical negative results for MPNNs and 2-IGNs \\cite{chen2020e6g}.\n        *   The proposed LRP model was evaluated on substructure counting tasks using random synthetic graphs \\cite{chen2020e6g}.\n        *   LRP's performance was also assessed on real-world molecular prediction tasks \\cite{chen2020e6g}.\n    *   **Key Performance Metrics & Results:**\n        *   LRP demonstrated effectiveness in counting both subgraphs and induced subgraphs on synthetic datasets \\cite{chen2020e6g}.\n        *   LRP achieved competitive performance on molecular prediction tasks, indicating its practical utility beyond just counting \\cite{chen2020e6g}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The negative results for MPNNs and 2-IGNs are specific to *induced-subgraph-count* and *connected patterns of 3 or more nodes* \\cite{chen2020e6g}. The k-WL limitation applies to *path patterns* and a *finite number of iterations* \\cite{chen2020e6g}.\n    *   **Scope of Applicability:** The theoretical analysis primarily covers common GNN architectures (MPNNs, k-WL, k-IGNs). The LRP model is presented as a solution for substructure counting and is shown to be effective for molecular property prediction, suggesting broader applicability for tasks requiring fine-grained local information \\cite{chen2020e6g}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the understanding of GNN expressive power by introducing a novel, practically relevant framework based on substructure counting \\cite{chen2020e6g}. It provides fundamental theoretical limitations for widely used GNNs (MPNNs, 2-IGNs) and settles an open problem regarding the equivalence of 2-WL and 2-IGNs. The proposed LRP model offers a new direction for GNN design, demonstrating improved capabilities for substructure counting and competitive performance on real-world tasks \\cite{chen2020e6g}.\n    *   **Potential Impact on Future Research:** The identified limitations can guide the development of next-generation GNN architectures that are explicitly designed to capture and count complex substructures. The LRP model, by focusing on local relational pooling and learning relevant substructures, opens avenues for research into more powerful and interpretable GNNs \\cite{chen2020e6g}.",
    "intriguing_abstract": "Beyond graph isomorphism, understanding the true expressive power of Graph Neural Networks (GNNs) for complex real-world tasks remains elusive. This paper pioneers a novel and practically relevant framework by examining GNNs' ability to perform **substructure counting**. We formally define `subgraph-count` and `induced-subgraph-count` for attributed graphs, unveiling fundamental limitations of popular architectures. Our theoretical analysis rigorously proves that Message Passing Neural Networks (MPNNs) and 2-Invariant Graph Networks (2-IGNs) *cannot* detect induced subgraphs for any connected pattern of three or more nodes, a critical insight. We also establish the discriminative equivalence between 2-Weisfeiler-Lehman (2-WL) and 2-IGNs. To address these limitations, we introduce **Local Relational Pooling (LRP)**, a novel GNN architecture engineered to effectively learn and count substructures by leveraging fine-grained local relational information. Empirical validation on synthetic and real-world molecular datasets demonstrates LRP's superior performance in substructure counting and competitive results in downstream prediction tasks. This work provides pivotal theoretical foundations and a new architectural direction, paving the way for next-generation GNNs capable of capturing the intricate relational patterns vital for advancements in computational chemistry, biology, and social network analysis.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "expressive power",
      "substructure counting",
      "induced-subgraph-count",
      "subgraph-count",
      "Local Relational Pooling (LRP) model",
      "Message Passing Neural Networks (MPNNs)",
      "k-Weisfeiler-Lehman (k-WL) tests",
      "k-Invariant Graph Networks (k-IGNs)",
      "theoretical analysis",
      "experimental validation",
      "MPNNs/2-IGNs limitations",
      "2-WL/2-IGNs equivalence",
      "molecular prediction tasks"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/536da0e76290aea9cbe75c29bac096aeb45ef875.pdf",
    "citation_key": "chen2020e6g",
    "metadata": {
      "title": "Can graph neural networks count substructures?",
      "authors": [
        "Zhengdao Chen",
        "Lei Chen",
        "Soledad Villar",
        "Joan Bruna"
      ],
      "published_date": "2020",
      "abstract": "The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting, we propose a local relational pooling approach with inspirations from Murphy et al. (2019) and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on real-world tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/536da0e76290aea9cbe75c29bac096aeb45ef875.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the specific technical problem of understanding the expressive power of Graph Neural Networks (GNNs) by examining their ability to detect and count substructures within graphs \\cite{chen2020e6g}.\n    *   **Importance & Challenge:** Substructure counting is crucial for many real-world tasks in computational chemistry, biology, and social network analysis. Existing GNN expressive power studies (e.g., graph isomorphism testing, universal function approximation) have limitations: isomorphism testing often finds GNNs already near maximally powerful, and universal approximators are impractical. A new, intuitive, and practically relevant perspective is needed to guide the development of more powerful GNN architectures \\cite{chen2020e6g}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior work on GNN expressive power primarily focused on distinguishing non-isomorphic graphs (e.g., showing MPNNs are at most as powerful as 1-WL/2-WL \\cite{chen2020e6g}) or approximating permutation-invariant functions (often leading to impractical models \\cite{chen2020e6g}). Other works provided impossibility results for GNNs on specific graph properties like k-cliques \\cite{chen2020e6g}.\n    *   **Limitations of Previous Solutions:** Graph isomorphism testing, while theoretically interesting, is less relevant to many practical tasks, and GNNs are already quite powerful in this regard. Universal function approximators are computationally infeasible.\n    *   **Positioning:** This work introduces substructure counting as a novel and practically relevant lens to characterize GNN expressive power, distinguishing between \"induced-subgraph-count\" and \"subgraph-count\" \\cite{chen2020e6g}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper formalizes the problem of substructure counting for attributed graphs, defining two types: `subgraph-count` (CS) and `induced-subgraph-count` (CI) \\cite{chen2020e6g}. It then theoretically analyzes the capabilities of popular GNN architectures (Message Passing Neural Networks (MPNNs), k-Weisfeiler-Lehman (k-WL) tests, and k-Invariant Graph Networks (k-IGNs)) to perform these counts.\n    *   **Innovation:**\n        *   It systematically investigates substructure counting as a measure of GNN expressive power, a novel perspective.\n        *   It establishes a theoretical equivalence between 2-WL and 2-IGNs in distinguishing non-isomorphic graphs, partly answering an open problem \\cite{chen2020e6g}.\n        *   Motivated by the limitations of existing GNNs, it proposes a novel GNN architecture called **Local Relational Pooling (LRP)**, designed to effectively count substructures by leveraging local neighborhood information and learning relevant patterns \\cite{chen2020e6g}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Formal definitions and framework for evaluating GNNs' ability to perform `subgraph-count` and `induced-subgraph-count` for attributed graphs \\cite{chen2020e6g}.\n        *   Introduction of the **Local Relational Pooling (LRP)** model, a new GNN architecture capable of learning and counting substructures by processing local relational information \\cite{chen2020e6g}.\n    *   **Theoretical Insights/Analysis:**\n        *   **Negative Results:** Proves that MPNNs and 2-IGNs *cannot* perform induced-subgraph-count for *any connected pattern with 3 or more nodes* \\cite{chen2020e6g}.\n        *   **Positive Results:** Shows that MPNNs and 2-IGNs *can* perform subgraph-count for *star-shaped patterns* \\cite{chen2020e6g}.\n        *   **Equivalence:** Establishes that 2-WL and 2-IGNs have equivalent discriminative power for non-isomorphic graphs \\cite{chen2020e6g}.\n        *   **k-WL/k-IGN Hierarchy:** Demonstrates that k-WL and k-IGNs can count (induced) subgraphs for patterns of size at most k \\cite{chen2020e6g}.\n        *   **Finite Depth Limitations:** Proves that T iterations of k-WL cannot induced-subgraph-count path patterns of `(k+1)2T` or more nodes, highlighting depth limitations \\cite{chen2020e6g}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Experiments were performed on synthetic graphs to empirically support the theoretical negative results for MPNNs and 2-IGNs \\cite{chen2020e6g}.\n        *   The proposed LRP model was evaluated on substructure counting tasks using random synthetic graphs \\cite{chen2020e6g}.\n        *   LRP's performance was also assessed on real-world molecular prediction tasks \\cite{chen2020e6g}.\n    *   **Key Performance Metrics & Results:**\n        *   LRP demonstrated effectiveness in counting both subgraphs and induced subgraphs on synthetic datasets \\cite{chen2020e6g}.\n        *   LRP achieved competitive performance on molecular prediction tasks, indicating its practical utility beyond just counting \\cite{chen2020e6g}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The negative results for MPNNs and 2-IGNs are specific to *induced-subgraph-count* and *connected patterns of 3 or more nodes* \\cite{chen2020e6g}. The k-WL limitation applies to *path patterns* and a *finite number of iterations* \\cite{chen2020e6g}.\n    *   **Scope of Applicability:** The theoretical analysis primarily covers common GNN architectures (MPNNs, k-WL, k-IGNs). The LRP model is presented as a solution for substructure counting and is shown to be effective for molecular property prediction, suggesting broader applicability for tasks requiring fine-grained local information \\cite{chen2020e6g}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the understanding of GNN expressive power by introducing a novel, practically relevant framework based on substructure counting \\cite{chen2020e6g}. It provides fundamental theoretical limitations for widely used GNNs (MPNNs, 2-IGNs) and settles an open problem regarding the equivalence of 2-WL and 2-IGNs. The proposed LRP model offers a new direction for GNN design, demonstrating improved capabilities for substructure counting and competitive performance on real-world tasks \\cite{chen2020e6g}.\n    *   **Potential Impact on Future Research:** The identified limitations can guide the development of next-generation GNN architectures that are explicitly designed to capture and count complex substructures. The LRP model, by focusing on local relational pooling and learning relevant substructures, opens avenues for research into more powerful and interpretable GNNs \\cite{chen2020e6g}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "expressive power",
        "substructure counting",
        "induced-subgraph-count",
        "subgraph-count",
        "Local Relational Pooling (LRP) model",
        "Message Passing Neural Networks (MPNNs)",
        "k-Weisfeiler-Lehman (k-WL) tests",
        "k-Invariant Graph Networks (k-IGNs)",
        "theoretical analysis",
        "experimental validation",
        "MPNNs/2-IGNs limitations",
        "2-WL/2-IGNs equivalence",
        "molecular prediction tasks"
      ],
      "paper_type": "this paper is best classified as **theoretical**.\n\nhere's why:\n\n*   **strong emphasis on proofs and formal analysis:** the abstract repeatedly uses phrases like \"we prove that...\", \"establish both positive and negative answers\", and discusses the \"expressive power\" and \"equivalence\" of different gnn architectures (mpnns, wl, igns). this directly aligns with the \"mathematical analysis, proofs, formal models\" criterion for theoretical papers.\n*   **addressing an open problem:** the abstract mentions \"partly answering an open problem raised in [38]\", which is characteristic of theoretical research.\n*   **experiments support theoretical results:** while experiments are conducted, they are explicitly stated to \"support the theoretical results,\" indicating that the theoretical findings are the primary contribution, with empirical validation as a secondary, supporting role.\n*   **new model motivated by theoretical understanding:** the proposal of the \"local relational pooling model\" comes after the extensive theoretical analysis and is presented as \"motivated by substructure counting,\" suggesting it's a practical outcome or application of the theoretical insights gained.\n\nwhile there are elements of \"technical\" (proposing a new model) and \"empirical\" (conducting experiments), the core focus and the most detailed part of the abstract are dedicated to the formal analysis and proofs regarding the expressive power of gnns."
    },
    "file_name": "536da0e76290aea9cbe75c29bac096aeb45ef875.pdf"
  },
  {
    "success": true,
    "doc_id": "bbc01b56d805036a1a9321ef1ad9ede0",
    "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{zhang20222g3}\" when referencing this paper.\n\n---\n\n### Focused Summary for Literature Review: Trustworthy Graph Neural Networks\n\n**1. Research Problem & Motivation**\n\n*   **Specific technical problem**: Performance-oriented Graph Neural Networks (GNNs) exhibit potential adverse effects, including vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, and excessive resource consumption in edge computing environments \\cite{zhang20222g3}.\n*   **Why is this problem important and challenging?**: In critical and sensitive domains (e.g., anomaly detection, credit scoring, drug discovery), high accuracy alone is insufficient. There is a growing need for GNNs to be reliable, responsible, ethical, and socially beneficial to be trusted, thereby avoiding unintentional harms \\cite{zhang20222g3}. The unique characteristics of graph data (non-Euclidean, discrete, irregular, non-IID nodes) make building trustworthy GNNs fundamentally different and more challenging than general trustworthy AI \\cite{zhang20222g3}.\n\n**2. Related Work & Positioning**\n\n*   **How does this work relate to existing approaches?**: This paper positions itself within the broader context of \"trustworthy AI,\" acknowledging global consensus and existing efforts (guidelines, mechanisms, technology views) \\cite{zhang20222g3}. It systematically transfers the key aspects of trustworthiness (robustness, explainability, privacy, fairness, accountability, well-being) from general AI to the specific domain of GNNs \\cite{zhang20222g3}.\n*   **What are the limitations of previous solutions?**: While general trustworthy AI research exists, it often overlooks the unique characteristics of graph data. This paper highlights how graph data's properties necessitate specialized approaches for GNNs, differentiating them from general AI in terms of:\n    *   **Robustness**: Discrete perturbations on graphs (e.g., adding/deleting edges) versus continuous perturbations in Euclidean data \\cite{zhang20222g3}.\n    *   **Explainability**: GNN explanations (nodes, edges, subgraphs) inherit graph irregularity, requiring specialized generation methods \\cite{zhang20222g3}.\n    *   **Privacy**: The need to protect relationships (edges) in addition to entities (nodes) \\cite{zhang20222g3}.\n    *   **Fairness**: The existence of edges breaks the IID assumption, requiring specific fairness definitions for graph-based tasks \\cite{zhang20222g3}.\n    *   **Accountability**: Unique graph data characteristics demand different evaluation standards and graph-based metrics \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Efficiency bottlenecks in GNNs are primarily driven by graph scale and data irregularity, distinct from model scale or energy-intensive architectures in general AI \\cite{zhang20222g3}.\n\n**3. Technical Approach & Innovation**\n\n*   **What is the core technical method or algorithm?**: The paper proposes a comprehensive roadmap and an \"open framework\" for building trustworthy GNNs \\cite{zhang20222g3}. This framework systematically categorizes and summarizes existing efforts across six core aspects: robustness, explainability, privacy, fairness, accountability, and environmental well-being \\cite{zhang20222g3}. For each aspect, it details typical methods and provides a methodology categorization (e.g., for robustness: attack methods, threat models, defenses before/during/after training) \\cite{zhang20222g3}.\n*   **What makes this approach novel or different?**: The novelty lies in providing the first comprehensive and systematic survey specifically focused on *trustworthy GNNs*, distinguishing it from general trustworthy AI surveys \\cite{zhang20222g3}. It explicitly addresses how the unique characteristics of graph data necessitate specialized approaches for each trustworthiness aspect, offering a structured overview of the field, its challenges, and future directions \\cite{zhang20222g3}. The framework is designed to be \"open,\" allowing for the incorporation of additional trust-oriented characteristics \\cite{zhang20222g3}.\n\n**4. Key Technical Contributions**\n\n*   **Novel algorithms, methods, or techniques**: The paper *identifies and categorizes* a wide array of existing and emerging techniques for trustworthy GNNs across six dimensions, rather than proposing new ones. This includes:\n    *   **Robustness**: Categorization of adversarial attacks and defense strategies (pre-, in-, post-training) \\cite{zhang20222g3}.\n    *   **Explainability**: Classification of methods into intrinsically interpretable GNNs and post-hoc explainers (e.g., gradient/feature-based, perturbation-based, surrogate, decomposition, generation methods) \\cite{zhang20222g3}.\n    *   **Privacy**: Overview of privacy attacks (model extraction, membership inference, model inversion) and privacy-preserving techniques (federated learning, differential privacy, insusceptible training, security computation) \\cite{zhang20222g3}.\n    *   **Fairness**: Methods for fair representation learning and fair prediction enhancement \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Techniques like scalable GNN architectures, efficient data communication, efficient frameworks/accelerators, and model compression \\cite{zhang20222g3}.\n*   **System design or architectural innovations**: Introduction of an \"open framework\" for trustworthy GNNs that integrates and structures the diverse aspects of trustworthiness, providing a holistic view for researchers and practitioners \\cite{zhang20222g3}.\n*   **Theoretical insights or analysis**:\n    *   Detailed analysis of the fundamental differences between trustworthy GNNs and general trustworthy AI for each aspect, highlighting how graph data characteristics impact research directions \\cite{zhang20222g3}.\n    *   Identification and discussion of intricate cross-aspect relations between the six trustworthiness aspects \\cite{zhang20222g3}.\n    *   Comprehensive summary of typical metrics used for evaluating each trustworthiness aspect in GNNs \\cite{zhang20222g3}.\n\n**5. Experimental Validation**\n\n*   **What experiments were conducted?**: As a survey paper, this work does not conduct new experiments. Instead, it synthesizes and reviews the experimental validation approaches and metrics commonly employed in the existing literature on trustworthy GNNs \\cite{zhang20222g3}.\n*   **Key performance metrics and comparison results**: The paper systematically lists and explains the typical metrics used to evaluate each aspect of trustworthiness in GNNs, which are derived from various studies:\n    *   **Robustness**: Attack Success Rate (ASR), accuracy, mis-classification rate, structural similarity score, attack budget \\cite{zhang20222g3}.\n    *   **Explainability**: Explanation accuracy, faithfulness, stability, sparsity, and domain-specific metrics \\cite{zhang20222g3}.\n    *   **Privacy**: ASR of privacy attacks, data leakage, and the ability to defend against attacks \\cite{zhang20222g3}.\n    *   **Fairness**: Group fairness (e.g., demographic parity), individual fairness (e.g., similarity-based fairness), and counterfactual fairness \\cite{zhang20222g3}.\n    *   **Accountability**: Existence of standard evaluation processes and comprehensiveness of specifications for the GNN life cycle \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Inference time, memory footprint, nodes-per-joule \\cite{zhang20222g3}.\n\n**6. Limitations & Scope**\n\n*   **Technical limitations or assumptions**:\n    *   As a survey, it primarily synthesizes existing knowledge rather than introducing new empirical results or theoretical proofs \\cite{zhang20222g3}.\n    *   The comparison between general AI and GNNs focuses on *typical* research differences, acknowledging that these differences may be limited in some specific cases (e.g., AI fairness methods not requiring IID assumptions) \\cite{zhang20222g3}.\n    *   While it proposes an \"open framework,\" the primary focus is on the six identified aspects, with \"others\" being a broader category \\cite{zhang20222g3}.\n*   **Scope of applicability**: The survey's scope is specifically on Graph Neural Networks and their trustworthiness aspects \\cite{zhang20222g3}. It covers a broad range of GNN applications, from recommendation systems and question answering to drug discovery and n-body simulation, where trustworthiness is critical \\cite{zhang20222g3}. It aims to provide a roadmap for building competent GNNs across various computing technologies \\cite{zhang20222g3}.\n\n**7. Technical Significance**\n\n*   **How does this advance the technical state-of-the-art?**: This paper significantly advances the state-of-the-art by providing the first comprehensive and structured overview of trustworthy GNNs, filling a critical gap in the literature \\cite{zhang20222g3}. It systematically defines, categorizes, and differentiates the various aspects of trustworthiness in the unique context of graph data, offering a foundational reference for the field \\cite{zhang20222g3}.\n*   **Potential impact on future research**:\n    *   **Roadmap for Research**: It provides a clear roadmap for future research by identifying open challenges, intricate cross-aspect relations, and trending directions for facilitating the research and industrialization of trustworthy GNNs \\cite{zhang20222g3}.\n    *   **Standardization and Benchmarking**: By summarizing metrics and research differences, it can aid in developing standardized evaluation processes and benchmarks for trustworthy GNNs \\cite{zhang20222g3}.\n    *   **Interdisciplinary Collaboration**: The comprehensive nature of the survey can foster interdisciplinary collaboration by highlighting the ethical, societal, and technical dimensions of GNN deployment \\cite{zhang20222g3}.\n    *   **Practical Deployment**: By emphasizing trustworthiness, it promotes the development of GNNs that are not only performant but also reliable, responsible, and ethical, accelerating their adoption in critical real-world applications \\cite{zhang20222g3}.",
    "intriguing_abstract": "While Graph Neural Networks (GNNs) revolutionize fields from drug discovery to anomaly detection, their deployment in critical applications is hampered by inherent vulnerabilities to adversarial attacks, potential for unfair discrimination, and excessive resource consumption in edge computing. Achieving high accuracy alone is insufficient; GNNs must be reliable, responsible, and ethical. This paper presents the *first comprehensive and systematic survey* specifically on **trustworthy GNNs**, addressing the unique challenges posed by non-Euclidean graph data that fundamentally differentiate it from general trustworthy AI.\n\nWe introduce an **open framework** that meticulously categorizes and synthesizes existing efforts across six crucial dimensions: **robustness**, **explainability**, **privacy**, **fairness**, **accountability**, and **environmental well-being**. For each, we detail specialized methods, from defending against discrete perturbations and protecting graph relationships (e.g., via federated learning or differential privacy) to ensuring equitable outcomes and efficient resource use. By analyzing fundamental differences, intricate cross-aspect relations, and typical evaluation metrics, this work provides a vital roadmap for researchers. It aims to accelerate the development and responsible deployment of GNNs that are not only powerful but also inherently trustworthy, fostering safer and more ethical AI systems.",
    "keywords": [
      "Trustworthy Graph Neural Networks",
      "Robustness",
      "Explainability",
      "Privacy",
      "Fairness",
      "Accountability",
      "Environmental Well-being",
      "Graph data characteristics",
      "Adversarial attacks",
      "Comprehensive survey",
      "Open framework",
      "Cross-aspect relations",
      "Specialized GNN trustworthiness approaches",
      "Critical application domains"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf",
    "citation_key": "zhang20222g3",
    "metadata": {
      "title": "Trustworthy Graph Neural Networks: Aspects, Methods, and Trends",
      "authors": [
        "He Zhang",
        "Bang Wu",
        "Xingliang Yuan",
        "Shirui Pan",
        "Hanghang Tong",
        "Jian Pei"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications such as recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects, such as vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterized by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarize existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. In addition, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialization of trustworthy GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf",
      "venue": "Proceedings of the IEEE",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{zhang20222g3}\" when referencing this paper.\n\n---\n\n### Focused Summary for Literature Review: Trustworthy Graph Neural Networks\n\n**1. Research Problem & Motivation**\n\n*   **Specific technical problem**: Performance-oriented Graph Neural Networks (GNNs) exhibit potential adverse effects, including vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, and excessive resource consumption in edge computing environments \\cite{zhang20222g3}.\n*   **Why is this problem important and challenging?**: In critical and sensitive domains (e.g., anomaly detection, credit scoring, drug discovery), high accuracy alone is insufficient. There is a growing need for GNNs to be reliable, responsible, ethical, and socially beneficial to be trusted, thereby avoiding unintentional harms \\cite{zhang20222g3}. The unique characteristics of graph data (non-Euclidean, discrete, irregular, non-IID nodes) make building trustworthy GNNs fundamentally different and more challenging than general trustworthy AI \\cite{zhang20222g3}.\n\n**2. Related Work & Positioning**\n\n*   **How does this work relate to existing approaches?**: This paper positions itself within the broader context of \"trustworthy AI,\" acknowledging global consensus and existing efforts (guidelines, mechanisms, technology views) \\cite{zhang20222g3}. It systematically transfers the key aspects of trustworthiness (robustness, explainability, privacy, fairness, accountability, well-being) from general AI to the specific domain of GNNs \\cite{zhang20222g3}.\n*   **What are the limitations of previous solutions?**: While general trustworthy AI research exists, it often overlooks the unique characteristics of graph data. This paper highlights how graph data's properties necessitate specialized approaches for GNNs, differentiating them from general AI in terms of:\n    *   **Robustness**: Discrete perturbations on graphs (e.g., adding/deleting edges) versus continuous perturbations in Euclidean data \\cite{zhang20222g3}.\n    *   **Explainability**: GNN explanations (nodes, edges, subgraphs) inherit graph irregularity, requiring specialized generation methods \\cite{zhang20222g3}.\n    *   **Privacy**: The need to protect relationships (edges) in addition to entities (nodes) \\cite{zhang20222g3}.\n    *   **Fairness**: The existence of edges breaks the IID assumption, requiring specific fairness definitions for graph-based tasks \\cite{zhang20222g3}.\n    *   **Accountability**: Unique graph data characteristics demand different evaluation standards and graph-based metrics \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Efficiency bottlenecks in GNNs are primarily driven by graph scale and data irregularity, distinct from model scale or energy-intensive architectures in general AI \\cite{zhang20222g3}.\n\n**3. Technical Approach & Innovation**\n\n*   **What is the core technical method or algorithm?**: The paper proposes a comprehensive roadmap and an \"open framework\" for building trustworthy GNNs \\cite{zhang20222g3}. This framework systematically categorizes and summarizes existing efforts across six core aspects: robustness, explainability, privacy, fairness, accountability, and environmental well-being \\cite{zhang20222g3}. For each aspect, it details typical methods and provides a methodology categorization (e.g., for robustness: attack methods, threat models, defenses before/during/after training) \\cite{zhang20222g3}.\n*   **What makes this approach novel or different?**: The novelty lies in providing the first comprehensive and systematic survey specifically focused on *trustworthy GNNs*, distinguishing it from general trustworthy AI surveys \\cite{zhang20222g3}. It explicitly addresses how the unique characteristics of graph data necessitate specialized approaches for each trustworthiness aspect, offering a structured overview of the field, its challenges, and future directions \\cite{zhang20222g3}. The framework is designed to be \"open,\" allowing for the incorporation of additional trust-oriented characteristics \\cite{zhang20222g3}.\n\n**4. Key Technical Contributions**\n\n*   **Novel algorithms, methods, or techniques**: The paper *identifies and categorizes* a wide array of existing and emerging techniques for trustworthy GNNs across six dimensions, rather than proposing new ones. This includes:\n    *   **Robustness**: Categorization of adversarial attacks and defense strategies (pre-, in-, post-training) \\cite{zhang20222g3}.\n    *   **Explainability**: Classification of methods into intrinsically interpretable GNNs and post-hoc explainers (e.g., gradient/feature-based, perturbation-based, surrogate, decomposition, generation methods) \\cite{zhang20222g3}.\n    *   **Privacy**: Overview of privacy attacks (model extraction, membership inference, model inversion) and privacy-preserving techniques (federated learning, differential privacy, insusceptible training, security computation) \\cite{zhang20222g3}.\n    *   **Fairness**: Methods for fair representation learning and fair prediction enhancement \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Techniques like scalable GNN architectures, efficient data communication, efficient frameworks/accelerators, and model compression \\cite{zhang20222g3}.\n*   **System design or architectural innovations**: Introduction of an \"open framework\" for trustworthy GNNs that integrates and structures the diverse aspects of trustworthiness, providing a holistic view for researchers and practitioners \\cite{zhang20222g3}.\n*   **Theoretical insights or analysis**:\n    *   Detailed analysis of the fundamental differences between trustworthy GNNs and general trustworthy AI for each aspect, highlighting how graph data characteristics impact research directions \\cite{zhang20222g3}.\n    *   Identification and discussion of intricate cross-aspect relations between the six trustworthiness aspects \\cite{zhang20222g3}.\n    *   Comprehensive summary of typical metrics used for evaluating each trustworthiness aspect in GNNs \\cite{zhang20222g3}.\n\n**5. Experimental Validation**\n\n*   **What experiments were conducted?**: As a survey paper, this work does not conduct new experiments. Instead, it synthesizes and reviews the experimental validation approaches and metrics commonly employed in the existing literature on trustworthy GNNs \\cite{zhang20222g3}.\n*   **Key performance metrics and comparison results**: The paper systematically lists and explains the typical metrics used to evaluate each aspect of trustworthiness in GNNs, which are derived from various studies:\n    *   **Robustness**: Attack Success Rate (ASR), accuracy, mis-classification rate, structural similarity score, attack budget \\cite{zhang20222g3}.\n    *   **Explainability**: Explanation accuracy, faithfulness, stability, sparsity, and domain-specific metrics \\cite{zhang20222g3}.\n    *   **Privacy**: ASR of privacy attacks, data leakage, and the ability to defend against attacks \\cite{zhang20222g3}.\n    *   **Fairness**: Group fairness (e.g., demographic parity), individual fairness (e.g., similarity-based fairness), and counterfactual fairness \\cite{zhang20222g3}.\n    *   **Accountability**: Existence of standard evaluation processes and comprehensiveness of specifications for the GNN life cycle \\cite{zhang20222g3}.\n    *   **Environmental Well-being**: Inference time, memory footprint, nodes-per-joule \\cite{zhang20222g3}.\n\n**6. Limitations & Scope**\n\n*   **Technical limitations or assumptions**:\n    *   As a survey, it primarily synthesizes existing knowledge rather than introducing new empirical results or theoretical proofs \\cite{zhang20222g3}.\n    *   The comparison between general AI and GNNs focuses on *typical* research differences, acknowledging that these differences may be limited in some specific cases (e.g., AI fairness methods not requiring IID assumptions) \\cite{zhang20222g3}.\n    *   While it proposes an \"open framework,\" the primary focus is on the six identified aspects, with \"others\" being a broader category \\cite{zhang20222g3}.\n*   **Scope of applicability**: The survey's scope is specifically on Graph Neural Networks and their trustworthiness aspects \\cite{zhang20222g3}. It covers a broad range of GNN applications, from recommendation systems and question answering to drug discovery and n-body simulation, where trustworthiness is critical \\cite{zhang20222g3}. It aims to provide a roadmap for building competent GNNs across various computing technologies \\cite{zhang20222g3}.\n\n**7. Technical Significance**\n\n*   **How does this advance the technical state-of-the-art?**: This paper significantly advances the state-of-the-art by providing the first comprehensive and structured overview of trustworthy GNNs, filling a critical gap in the literature \\cite{zhang20222g3}. It systematically defines, categorizes, and differentiates the various aspects of trustworthiness in the unique context of graph data, offering a foundational reference for the field \\cite{zhang20222g3}.\n*   **Potential impact on future research**:\n    *   **Roadmap for Research**: It provides a clear roadmap for future research by identifying open challenges, intricate cross-aspect relations, and trending directions for facilitating the research and industrialization of trustworthy GNNs \\cite{zhang20222g3}.\n    *   **Standardization and Benchmarking**: By summarizing metrics and research differences, it can aid in developing standardized evaluation processes and benchmarks for trustworthy GNNs \\cite{zhang20222g3}.\n    *   **Interdisciplinary Collaboration**: The comprehensive nature of the survey can foster interdisciplinary collaboration by highlighting the ethical, societal, and technical dimensions of GNN deployment \\cite{zhang20222g3}.\n    *   **Practical Deployment**: By emphasizing trustworthiness, it promotes the development of GNNs that are not only performant but also reliable, responsible, and ethical, accelerating their adoption in critical real-world applications \\cite{zhang20222g3}.",
      "keywords": [
        "Trustworthy Graph Neural Networks",
        "Robustness",
        "Explainability",
        "Privacy",
        "Fairness",
        "Accountability",
        "Environmental Well-being",
        "Graph data characteristics",
        "Adversarial attacks",
        "Comprehensive survey",
        "Open framework",
        "Cross-aspect relations",
        "Specialized GNN trustworthiness approaches",
        "Critical application domains"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf"
  },
  {
    "success": true,
    "doc_id": "255488205b2f0ac8b195f79ff2c0fb74",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n### Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Traditional Graph Neural Networks (GNNs) employ a uniform global filter (e.g., low-pass for homophilic graphs, high-pass for heterophilic graphs) across all nodes \\cite{han2024rkj}. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, and these patterns can vary significantly even among different communities within the same graph \\cite{han2024rkj}.\n    *   **Why important and challenging:** A single global filter is suboptimal and can adversely affect performance on nodes with differing patterns, leading to misclassification \\cite{han2024rkj}. The challenge lies in adaptively applying appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns \\cite{han2024rkj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Most GNNs, including those with learnable graph convolutions, apply a uniform global filter across all nodes \\cite{han2024rkj}.\n    *   **Limitations of previous solutions:** The \"one-size-fits-all\" filtering strategy is ineffective for graphs with mixed homophilic and heterophilic patterns. A global filter optimized for one pattern (e.g., low-pass for homophily) can incur significant losses for nodes exhibiting other patterns (e.g., heterophily) \\cite{han2024rkj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that leverages a Mixture of Experts (MoE) approach to adaptively select and apply appropriate filters for different nodes \\cite{han2024rkj}.\n    *   **What makes this approach novel:** NODE-MOE moves beyond uniform global filtering by dynamically applying distinct filters to individual nodes based on their specific structural patterns. It integrates a gating model to assign weights to different \"expert\" GNNs, each potentially equipped with a different filter type, allowing for node-specific processing \\cite{han2024rkj}.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical insights:** The paper theoretically demonstrates, using a Contextual Stochastic Block Model (CSBM), that a global filter optimized for one pattern can incur significant losses for nodes with other patterns, while node-wise filtering can achieve linear separability for all nodes under mild conditions (Theorem 1) \\cite{han2024rkj}.\n    *   **Novel framework:** Introduction of NODE-MOE, a flexible and efficient Mixture of Experts framework for node-wise filtering in GNNs \\cite{han2024rkj}.\n    *   **Gating Model design:** A novel gating model that estimates node patterns by incorporating contextual features `[X, |AX-X|, |A^2X-X|]` and uses a GNN (e.g., GIN) with low-pass filters to ensure neighboring nodes receive similar expert selections, leveraging community detection capabilities \\cite{han2024rkj}.\n    *   **Expert Model strategy:** Utilizes GNNs with learnable graph convolutions as experts, initialized with diverse filter types (e.g., low-pass, constant, high-pass) to encourage specialization and handle different structural patterns \\cite{han2024rkj}.\n    *   **Filter Smoothing Loss:** Introduction of a `L_s = sum(|f_o(x_i) - f_o(x_{i-1})|^2)` loss to ensure learned filters exhibit smooth behavior in the spectral domain, mitigating training challenges and improving interpretability \\cite{han2024rkj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on both homophilic (Cora, CiteSeer) and heterophilic (Chameleon, Squirrel) graph datasets \\cite{han2024rkj}.\n    *   **Key performance metrics and comparison results:** The experiments demonstrate the effectiveness of NODE-MOE, illustrating significant performance improvement on both types of graphs \\cite{han2024rkj}. The gating model is shown to efficiently assign different nodes to their suitable filters (Section 4.3) \\cite{han2024rkj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The framework addresses the challenge of incorporating various filters and selecting appropriate ones without ground truth on node patterns. The filter smoothing loss is introduced to mitigate issues like filter oscillations and improve interpretability when training multiple filters simultaneously \\cite{han2024rkj}.\n    *   **Scope of applicability:** NODE-MOE is primarily designed for node classification tasks on graphs that exhibit a complex mixture of homophilic and heterophilic structural patterns \\cite{han2024rkj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** NODE-MOE represents a significant advancement by moving beyond the limitations of uniform global filtering in GNNs. It provides a principled and adaptive approach to handle the diverse structural patterns prevalent in real-world graphs, which is a common challenge for existing GNN models \\cite{han2024rkj}.\n    *   **Potential impact on future research:** This work opens avenues for future research into more sophisticated gating mechanisms, dynamic expert selection, and the application of node-wise filtering to other graph learning tasks beyond node classification. The theoretical foundation also encourages further analysis of GNN behavior on mixed-pattern graphs \\cite{han2024rkj}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph learning, yet their pervasive reliance on a uniform global filter presents a critical bottleneck. Real-world graphs rarely conform to a single pattern, often exhibiting a complex interplay of homophilic and heterophilic structures. This \"one-size-fits-all\" filtering strategy leads to suboptimal performance and misclassification on diverse node patterns.\n\nWe introduce NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that fundamentally rethinks graph convolution by adaptively applying distinct spectral filters to individual nodes. NODE-MOE employs a sophisticated gating model, informed by contextual features and neighborhood consistency, to dynamically route nodes to specialized \"expert\" GNNs. These experts, initialized with diverse filter types (e.g., low-pass, high-pass), are designed to optimally process specific structural patterns. We theoretically demonstrate that global filters incur significant losses on mixed patterns, while node-wise filtering can achieve linear separability. A novel filter smoothing loss further ensures robust training and interpretability. Extensive experiments confirm NODE-MOE's superior performance in node classification across various datasets, marking a significant advancement in handling heterogeneous graph structures and unlocking the full potential of GNNs on complex real-world data.",
    "keywords": [
      "Node-wise Filtering",
      "Graph Neural Networks (GNNs)",
      "Mixture of Experts (MoE)",
      "Homophilic and Heterophilic Graphs",
      "Adaptive Filtering",
      "NODE-MOE",
      "Gating Model",
      "Learnable Graph Convolutions",
      "Filter Smoothing Loss",
      "Contextual Stochastic Block Model (CSBM)",
      "Node Classification",
      "Complex Structural Patterns",
      "Spectral Domain"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf",
    "citation_key": "han2024rkj",
    "metadata": {
      "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
      "authors": [
        "Haoyu Han",
        "Juanhui Li",
        "Wei Huang",
        "Xianfeng Tang",
        "Hanqing Lu",
        "Chen Luo",
        "Hui Liu",
        "Jiliang Tang"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
      "file_path": "paper_data/Graph_Neural_Networks/f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Traditional Graph Neural Networks (GNNs) employ a uniform global filter (e.g., low-pass for homophilic graphs, high-pass for heterophilic graphs) across all nodes \\cite{han2024rkj}. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, and these patterns can vary significantly even among different communities within the same graph \\cite{han2024rkj}.\n    *   **Why important and challenging:** A single global filter is suboptimal and can adversely affect performance on nodes with differing patterns, leading to misclassification \\cite{han2024rkj}. The challenge lies in adaptively applying appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns \\cite{han2024rkj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Most GNNs, including those with learnable graph convolutions, apply a uniform global filter across all nodes \\cite{han2024rkj}.\n    *   **Limitations of previous solutions:** The \"one-size-fits-all\" filtering strategy is ineffective for graphs with mixed homophilic and heterophilic patterns. A global filter optimized for one pattern (e.g., low-pass for homophily) can incur significant losses for nodes exhibiting other patterns (e.g., heterophily) \\cite{han2024rkj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that leverages a Mixture of Experts (MoE) approach to adaptively select and apply appropriate filters for different nodes \\cite{han2024rkj}.\n    *   **What makes this approach novel:** NODE-MOE moves beyond uniform global filtering by dynamically applying distinct filters to individual nodes based on their specific structural patterns. It integrates a gating model to assign weights to different \"expert\" GNNs, each potentially equipped with a different filter type, allowing for node-specific processing \\cite{han2024rkj}.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical insights:** The paper theoretically demonstrates, using a Contextual Stochastic Block Model (CSBM), that a global filter optimized for one pattern can incur significant losses for nodes with other patterns, while node-wise filtering can achieve linear separability for all nodes under mild conditions (Theorem 1) \\cite{han2024rkj}.\n    *   **Novel framework:** Introduction of NODE-MOE, a flexible and efficient Mixture of Experts framework for node-wise filtering in GNNs \\cite{han2024rkj}.\n    *   **Gating Model design:** A novel gating model that estimates node patterns by incorporating contextual features `[X, |AX-X|, |A^2X-X|]` and uses a GNN (e.g., GIN) with low-pass filters to ensure neighboring nodes receive similar expert selections, leveraging community detection capabilities \\cite{han2024rkj}.\n    *   **Expert Model strategy:** Utilizes GNNs with learnable graph convolutions as experts, initialized with diverse filter types (e.g., low-pass, constant, high-pass) to encourage specialization and handle different structural patterns \\cite{han2024rkj}.\n    *   **Filter Smoothing Loss:** Introduction of a `L_s = sum(|f_o(x_i) - f_o(x_{i-1})|^2)` loss to ensure learned filters exhibit smooth behavior in the spectral domain, mitigating training challenges and improving interpretability \\cite{han2024rkj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on both homophilic (Cora, CiteSeer) and heterophilic (Chameleon, Squirrel) graph datasets \\cite{han2024rkj}.\n    *   **Key performance metrics and comparison results:** The experiments demonstrate the effectiveness of NODE-MOE, illustrating significant performance improvement on both types of graphs \\cite{han2024rkj}. The gating model is shown to efficiently assign different nodes to their suitable filters (Section 4.3) \\cite{han2024rkj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The framework addresses the challenge of incorporating various filters and selecting appropriate ones without ground truth on node patterns. The filter smoothing loss is introduced to mitigate issues like filter oscillations and improve interpretability when training multiple filters simultaneously \\cite{han2024rkj}.\n    *   **Scope of applicability:** NODE-MOE is primarily designed for node classification tasks on graphs that exhibit a complex mixture of homophilic and heterophilic structural patterns \\cite{han2024rkj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** NODE-MOE represents a significant advancement by moving beyond the limitations of uniform global filtering in GNNs. It provides a principled and adaptive approach to handle the diverse structural patterns prevalent in real-world graphs, which is a common challenge for existing GNN models \\cite{han2024rkj}.\n    *   **Potential impact on future research:** This work opens avenues for future research into more sophisticated gating mechanisms, dynamic expert selection, and the application of node-wise filtering to other graph learning tasks beyond node classification. The theoretical foundation also encourages further analysis of GNN behavior on mixed-pattern graphs \\cite{han2024rkj}.",
      "keywords": [
        "Node-wise Filtering",
        "Graph Neural Networks (GNNs)",
        "Mixture of Experts (MoE)",
        "Homophilic and Heterophilic Graphs",
        "Adaptive Filtering",
        "NODE-MOE",
        "Gating Model",
        "Learnable Graph Convolutions",
        "Filter Smoothing Loss",
        "Contextual Stochastic Block Model (CSBM)",
        "Node Classification",
        "Complex Structural Patterns",
        "Spectral Domain"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce a novel gnn framework node-moe that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes.\" this directly aligns with presenting new methods, algorithms, or systems.\n*   the introduction sets up a technical problem (suboptimality of uniform global filters) and the abstract then proposes a solution (node-moe).\n*   while it mentions \"theoretically demonstrate\" (theoretical aspect) and \"extensive experiments demonstrate\" (empirical aspect), these serve to justify and validate the *new framework* being proposed. the core contribution is the development and presentation of this novel framework.\n\ntherefore, the paper is best classified as **technical**."
    },
    "file_name": "f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf"
  },
  {
    "success": true,
    "doc_id": "fcbf7d9ac3a21c0eca4994f7bd5b8d3d",
    "summary": "Here's a focused summary of the paper \"SIGN: Scalable Inception Graph Neural Networks\" by Frasca et al. \\cite{rossi2020otv} for a literature review:\n\n### SIGN: Scalable Inception Graph Neural Networks \\cite{rossi2020otv}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The primary challenge addressed is the scalability of Graph Neural Networks (GNNs) to very large, \"web-scale\" graphs (e.g., social networks with millions to billions of nodes and edges).\n    *   **Importance & Challenge:** GNNs have shown great promise in various applications, but their computational and memory complexity, especially due to information diffusion across nodes and exponential growth of receptive fields, makes direct application to large graphs infeasible. Existing solutions often rely on graph sampling, which can introduce bias and still incur significant computational costs during training.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{rossi2020otv} positions itself as an alternative to prevalent graph sampling strategies (e.g., Node-wise sampling like GraphSAGE \\cite{rossi2020otv}, Layer-wise sampling, Graph-wise sampling like ClusterGCN and GraphSAINT \\cite{rossi2020otv}).\n    *   **Limitations of Previous Solutions:**\n        *   Sampling strategies (e.g., GraphSAGE, ClusterGCN, GraphSAINT) alleviate computational cost but can introduce bias into the optimization procedure.\n        *   The forward and backward pass complexity of sampling-based methods still depends on the graph structure, leading to slower training and inference compared to the proposed method.\n        *   Most research focused on small-scale datasets, with limited effort on scaling to web-scale graphs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{rossi2020otv} proposes SIGN (Scalable Inception Graph Neural Networks), an architecture inspired by the Inception module in CNNs. It uses a set of linear diffusion operators ($A_1, \\dots, A_r$) applied to node features, which are then concatenated and passed through an MLP.\n        *   The core equation is $Z = \\text{concat}([X \\Theta_0; A_1X \\Theta_1; \\dots; A_rX \\Theta_r])$ followed by $Y = \\sigma(Z \\Theta)$.\n    *   **Novelty/Difference:**\n        *   **Precomputation:** The key innovation is that the matrix products $A_kX$ (where $A_k$ are diffusion operators and $X$ are node features) do not depend on learnable parameters and can be precomputed once. This effectively reduces the training and inference complexity to that of a Multi-Layer Perceptron (MLP), making it independent of the graph structure during these phases.\n        *   **Sidesteps Sampling:** Unlike prior scalable GNNs, SIGN does not rely on any graph sampling techniques, avoiding potential biases.\n        *   **Parallel Multi-scale Operators:** It allows for the parallel application of different local graph operators (e.g., powers of GCN-normalized adjacency, Personalized PageRank-based, or triangle-based adjacency matrices), enabling the model to capture diverse connectivity patterns and higher-order structures.\n        *   **Shallow Architecture:** SIGN is inherently \"shallow,\" collapsing graph convolutions into a single linear filtering operation or parallel applications, challenging the common trend of designing deep GNNs for irregular graphs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Architecture:** Introduction of the SIGN architecture, which leverages an Inception-like module for graph data.\n    *   **Efficient Precomputation Strategy:** A method to precompute graph diffusion operations, decoupling the graph structure from the online training and inference complexity. This leads to an $O(rL_M Nd^2)$ complexity for forward/backward pass, where $N$ is nodes, $d$ is features, $L_M$ is MLP layers, and $r$ is filter size, significantly faster than sampling methods.\n    *   **Flexibility in Operators:** The framework allows for the incorporation of various domain-specific diffusion operators, enhancing expressivity.\n    *   **Theoretical Insight:** The paper implicitly argues that for general irregular graphs (like social networks), shallow architectures with expressive local operators might be more effective and scalable than deep GNNs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive evaluation on node-wise classification tasks in both transductive and inductive settings.\n    *   **Datasets:**\n        *   Inductive: Reddit, Flickr, Yelp, PPI.\n        *   Transductive: ogbn-products, ogbn-papers100M (the largest public graph dataset with 111M nodes, 1.5B edges).\n        *   Scalability test: Wikipedia links.\n    *   **Key Performance Metrics & Results:**\n        *   **Competitive Performance:** SIGN achieves results on par with state-of-the-art sampling-based models on several large-scale graph learning datasets.\n        *   **State-of-the-Art on OGBN-Papers100M:** Achieved state-of-the-art results on ogbn-papers100M, demonstrating superior scalability and effectiveness on extremely large graphs.\n        *   **Significant Speedup:** Experimentally shown to be significantly faster (even an order of magnitude speedup) in training and especially inference compared to GraphSAGE, ClusterGCN, and GraphSAINT, due to its graph-structure-independent forward/backward pass.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The precomputation step itself can be costly ($O(r|E|d)$), though it's a one-time cost.\n        *   The paper conjectures that deep GNN architectures are not useful for *general irregular graphs*, implying that SIGN's shallow nature might not be optimal for all graph types (e.g., geometric graphs where depth has shown benefits).\n    *   **Scope of Applicability:** Primarily focused on node-wise classification tasks on large, irregular graphs, particularly those where scalability is a critical concern.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{rossi2020otv} provides a highly efficient and scalable alternative to sampling-based GNNs, pushing the boundaries of what's possible for GNNs on truly massive graphs. It achieves state-of-the-art results on the largest publicly available graph benchmark.\n    *   **Potential Impact on Future Research:**\n        *   Challenges the prevailing paradigm of designing deep GNNs for irregular graphs, suggesting that focusing on more expressive local operators in shallow architectures might be a more fruitful direction for scalability.\n        *   Opens avenues for exploring diverse precomputable graph operators to capture different structural properties.\n        *   Its efficiency makes GNNs more practical for industrial applications dealing with web-scale data.",
    "intriguing_abstract": "Unlocking the full potential of Graph Neural Networks (GNNs) on web-scale data has remained an elusive challenge, with existing methods struggling against prohibitive computational and memory costs, often resorting to biased graph sampling. We introduce SIGN (Scalable Inception Graph Neural Networks), a novel architecture that fundamentally redefines GNN scalability. Inspired by Inception modules, SIGN leverages parallel linear diffusion operators whose computations are entirely *precomputed*. This critical decoupling transforms online training and inference into a simple Multi-Layer Perceptron (MLP) operation, making its complexity independent of the graph structure and sidestepping the biases inherent in traditional graph sampling. By integrating diverse multi-scale operators, SIGN effectively captures rich, higher-order structural information within a shallow, yet powerful, framework. Evaluated on massive datasets, including the 111-million-node ogbn-papers100M, SIGN achieves state-of-the-art performance with an order-of-magnitude speedup over leading sampling-based methods. Our work not only unlocks GNNs for truly web-scale applications but also challenges the prevailing deep GNN paradigm, suggesting a new direction for designing highly efficient and expressive graph learning models.",
    "keywords": [
      "Scalability of Graph Neural Networks",
      "web-scale graphs",
      "SIGN (Scalable Inception Graph Neural Networks)",
      "Inception-like module",
      "linear diffusion operators",
      "precomputation strategy",
      "graph-structure-independent complexity",
      "sidesteps graph sampling",
      "parallel multi-scale operators",
      "shallow GNN architecture",
      "node-wise classification",
      "OGBN-Papers100M",
      "state-of-the-art performance",
      "significant speedup"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/993377a3fc8334558463b82053904e3d684f29c0.pdf",
    "citation_key": "rossi2020otv",
    "metadata": {
      "title": "SIGN: Scalable Inception Graph Neural Networks",
      "authors": [
        "Emanuele Rossi",
        "Fabrizio Frasca",
        "B. Chamberlain",
        "D. Eynard",
        "M. Bronstein",
        "Federico Monti"
      ],
      "published_date": "2020",
      "abstract": "Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time.",
      "file_path": "paper_data/Graph_Neural_Networks/993377a3fc8334558463b82053904e3d684f29c0.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"SIGN: Scalable Inception Graph Neural Networks\" by Frasca et al. \\cite{rossi2020otv} for a literature review:\n\n### SIGN: Scalable Inception Graph Neural Networks \\cite{rossi2020otv}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The primary challenge addressed is the scalability of Graph Neural Networks (GNNs) to very large, \"web-scale\" graphs (e.g., social networks with millions to billions of nodes and edges).\n    *   **Importance & Challenge:** GNNs have shown great promise in various applications, but their computational and memory complexity, especially due to information diffusion across nodes and exponential growth of receptive fields, makes direct application to large graphs infeasible. Existing solutions often rely on graph sampling, which can introduce bias and still incur significant computational costs during training.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{rossi2020otv} positions itself as an alternative to prevalent graph sampling strategies (e.g., Node-wise sampling like GraphSAGE \\cite{rossi2020otv}, Layer-wise sampling, Graph-wise sampling like ClusterGCN and GraphSAINT \\cite{rossi2020otv}).\n    *   **Limitations of Previous Solutions:**\n        *   Sampling strategies (e.g., GraphSAGE, ClusterGCN, GraphSAINT) alleviate computational cost but can introduce bias into the optimization procedure.\n        *   The forward and backward pass complexity of sampling-based methods still depends on the graph structure, leading to slower training and inference compared to the proposed method.\n        *   Most research focused on small-scale datasets, with limited effort on scaling to web-scale graphs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{rossi2020otv} proposes SIGN (Scalable Inception Graph Neural Networks), an architecture inspired by the Inception module in CNNs. It uses a set of linear diffusion operators ($A_1, \\dots, A_r$) applied to node features, which are then concatenated and passed through an MLP.\n        *   The core equation is $Z = \\text{concat}([X \\Theta_0; A_1X \\Theta_1; \\dots; A_rX \\Theta_r])$ followed by $Y = \\sigma(Z \\Theta)$.\n    *   **Novelty/Difference:**\n        *   **Precomputation:** The key innovation is that the matrix products $A_kX$ (where $A_k$ are diffusion operators and $X$ are node features) do not depend on learnable parameters and can be precomputed once. This effectively reduces the training and inference complexity to that of a Multi-Layer Perceptron (MLP), making it independent of the graph structure during these phases.\n        *   **Sidesteps Sampling:** Unlike prior scalable GNNs, SIGN does not rely on any graph sampling techniques, avoiding potential biases.\n        *   **Parallel Multi-scale Operators:** It allows for the parallel application of different local graph operators (e.g., powers of GCN-normalized adjacency, Personalized PageRank-based, or triangle-based adjacency matrices), enabling the model to capture diverse connectivity patterns and higher-order structures.\n        *   **Shallow Architecture:** SIGN is inherently \"shallow,\" collapsing graph convolutions into a single linear filtering operation or parallel applications, challenging the common trend of designing deep GNNs for irregular graphs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Architecture:** Introduction of the SIGN architecture, which leverages an Inception-like module for graph data.\n    *   **Efficient Precomputation Strategy:** A method to precompute graph diffusion operations, decoupling the graph structure from the online training and inference complexity. This leads to an $O(rL_M Nd^2)$ complexity for forward/backward pass, where $N$ is nodes, $d$ is features, $L_M$ is MLP layers, and $r$ is filter size, significantly faster than sampling methods.\n    *   **Flexibility in Operators:** The framework allows for the incorporation of various domain-specific diffusion operators, enhancing expressivity.\n    *   **Theoretical Insight:** The paper implicitly argues that for general irregular graphs (like social networks), shallow architectures with expressive local operators might be more effective and scalable than deep GNNs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive evaluation on node-wise classification tasks in both transductive and inductive settings.\n    *   **Datasets:**\n        *   Inductive: Reddit, Flickr, Yelp, PPI.\n        *   Transductive: ogbn-products, ogbn-papers100M (the largest public graph dataset with 111M nodes, 1.5B edges).\n        *   Scalability test: Wikipedia links.\n    *   **Key Performance Metrics & Results:**\n        *   **Competitive Performance:** SIGN achieves results on par with state-of-the-art sampling-based models on several large-scale graph learning datasets.\n        *   **State-of-the-Art on OGBN-Papers100M:** Achieved state-of-the-art results on ogbn-papers100M, demonstrating superior scalability and effectiveness on extremely large graphs.\n        *   **Significant Speedup:** Experimentally shown to be significantly faster (even an order of magnitude speedup) in training and especially inference compared to GraphSAGE, ClusterGCN, and GraphSAINT, due to its graph-structure-independent forward/backward pass.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The precomputation step itself can be costly ($O(r|E|d)$), though it's a one-time cost.\n        *   The paper conjectures that deep GNN architectures are not useful for *general irregular graphs*, implying that SIGN's shallow nature might not be optimal for all graph types (e.g., geometric graphs where depth has shown benefits).\n    *   **Scope of Applicability:** Primarily focused on node-wise classification tasks on large, irregular graphs, particularly those where scalability is a critical concern.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{rossi2020otv} provides a highly efficient and scalable alternative to sampling-based GNNs, pushing the boundaries of what's possible for GNNs on truly massive graphs. It achieves state-of-the-art results on the largest publicly available graph benchmark.\n    *   **Potential Impact on Future Research:**\n        *   Challenges the prevailing paradigm of designing deep GNNs for irregular graphs, suggesting that focusing on more expressive local operators in shallow architectures might be a more fruitful direction for scalability.\n        *   Opens avenues for exploring diverse precomputable graph operators to capture different structural properties.\n        *   Its efficiency makes GNNs more practical for industrial applications dealing with web-scale data.",
      "keywords": [
        "Scalability of Graph Neural Networks",
        "web-scale graphs",
        "SIGN (Scalable Inception Graph Neural Networks)",
        "Inception-like module",
        "linear diffusion operators",
        "precomputation strategy",
        "graph-structure-independent complexity",
        "sidesteps graph sampling",
        "parallel multi-scale operators",
        "shallow GNN architecture",
        "node-wise classification",
        "OGBN-Papers100M",
        "state-of-the-art performance",
        "significant speedup"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly states: \"in this paper we **propose** a new, efﬁcient and scalable graph deep learning **architecture**...\" and describes its features and advantages. it also mentions \"we conduct extensive **experimental evaluation**\" and \"obtain **state-of-the-art results**,\" which are empirical findings supporting the proposed technical solution.\n*   the **introduction** discusses a \"major challenge\" (scaling gnns) which is a technical problem, and the need to generalize convolutional architectures to graphs.\n\nthis paper clearly presents a **new method/architecture** (sign: scalable inception graph neural networks) and validates it through experiments. this aligns perfectly with the **technical** classification criteria.\n\n**classification: technical**"
    },
    "file_name": "993377a3fc8334558463b82053904e3d684f29c0.pdf"
  },
  {
    "success": true,
    "doc_id": "6b02f916953749475a24c420f1372cbd",
    "summary": "Here's a focused summary of the paper \\cite{wu2022vcx} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Existing intrinsically interpretable Graph Neural Networks (GNNs) for rationalization often rely on \"shortcut features\" or data biases to compose rationales and make predictions. These shortcuts are unstable and change in out-of-distribution (OOD) data, leading to a significant drop in both interpretability (rationales don't reflect true causal patterns) and predictive performance on OOD data.\n    *   **Importance & Challenge**: Understanding \"what knowledge drives the model\" is crucial for auditing GNNs, justifying predictions, and enabling real-world applications (e.g., drug discovery). The challenge lies in identifying the *causal* patterns that are stable across different data distributions, rather than spurious correlations, especially when environments are not explicitly observable in the training data.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: GNN rationalization falls into post-hoc explainability (separate explanation method) and intrinsic interpretability (rationalization module within the model). This work focuses on intrinsic interpretability, where methods like graph attention and pooling operators generate masks on input graphs to identify rationales.\n    *   **Limitations of Previous Solutions**: Current intrinsic rationalization methods, despite their appeal, are shown to be prone to exploiting data biases and shortcuts. They minimize empirical risk, which can lead to learning non-causal statistical associations rather than the true underlying causal mechanisms, resulting in poor generalization to OOD data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{wu2022vcx} proposes Discovering Invariant Rationale (DIR), a novel invariant learning strategy for intrinsically interpretable GNNs. DIR aims to identify causal patterns that are stable (invariant) across different data distributions while filtering out unstable, spurious patterns.\n    *   **Novelty**:\n        *   **Causal Intervention for Environment Generation**: Unlike prior invariant learning (IL) methods that assume observable environments, DIR generates \"interventional distributions\" by performing causal interventions (`do(S=s)`) on the non-causal part (`S`) of the input graph. This allows the model to simulate multiple environments from a standard training set, enabling the distinction between causal and non-causal patterns.\n        *   **DIR Principle**: The learning strategy minimizes both the average s-interventional risk and the variance of these risks across different interventional distributions. This encourages the model to find rationales whose relationship with the label is stable.\n        *   **Modular Architecture**: DIR consists of four key modules:\n            1.  **Rationale Generator**: Splits the input graph into a potential causal part (`~c`) and a non-causal part (`~s`) by masking edges based on importance scores.\n            2.  **Distribution Intervener**: Creates interventional distributions by sampling non-causal parts from a memory bank and replacing the original non-causal part of an instance, forming an intervened pair `(~c_j, ~s_i)`.\n            3.  **Graph Encoder**: A shared GNN encoder processes both `~c` and `~s` into graph representations.\n            4.  **Two Classifiers**: One classifier (`phi_c`) predicts from the causal part (`^y_~c`), and another (`phi_s`) predicts from the non-causal part (`^y_~s`). The joint prediction `^y` under intervention is formulated as `^y_~c` masked by `sigmoid(^y_~s)`, where the sigmoid function helps compensate for spurious biases.\n        *   **Optimization Strategy**: The overall objective combines the DIR principle's invariant risk minimization with an additional loss for the non-causal classifier (`phi_s`) that is *not* backpropagated to other components. This encourages `phi_s` to learn spurious biases from non-causal features without interfering with the causal representation learning. During inference, only the causal rationale `~c` and its prediction `^y_~c` are used.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the Discovering Invariant Rationale (DIR) algorithm, a new invariant learning approach specifically tailored for intrinsically interpretable GNNs.\n    *   **Causal Intervention Mechanism**: A method to generate interventional distributions via `do(S=s)` operations on the non-causal graph components, addressing the challenge of unobservable environments in standard IL settings.\n    *   **Theoretical Justification**: Provides causality-theoretic analysis to guarantee that the DIR principle can discover invariant rationales and that the oracle model respects this principle.\n    *   **System Design**: A modular framework integrating a rationale generator, distribution intervener, shared encoder, and dual classifiers with a specialized optimization strategy for invariant learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate DIR's effectiveness in discovering causal features and improving generalization, as well as to understand its learning patterns.\n    *   **Datasets**:\n        *   **Spurious-Motif (synthetic)**: A controlled dataset where graph labels are *solely* determined by a \"motif\" (causal part), while \"bases\" (non-causal part) are introduced with varying degrees of spurious correlation to the label in the training set. Testing data has random motif-base attachments to assess OOD generalization.\n        *   Three real-world graph classification datasets (details in Appendix D).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   DIR consistently demonstrated superior generalization ability compared to state-of-the-art invariant learning methods (e.g., IRM \\cite{arjovsky2019invariant}, V-REx \\cite{krueger2021out}, GroupDRO \\cite{sagawa2019distributionally}).\n        *   It also outperformed attention- and pooling-based rationalization methods (e.g., GAT \\cite{velickovic2018graph}, GNNExplainer \\cite{ying2019gnnexplainer}) in terms of interpretability (discovering true causal features) and predictive performance on OOD data.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper assumes a causal view of the data-generating process with a clear distinction between causal and non-causal parts, and that the causal part is the sole determinant of the ground-truth label. The effectiveness of the rationale generator in accurately splitting the graph into `~c` and `~s` is crucial. The `do(S=s)` intervention relies on the ability to isolate and manipulate the non-causal part.\n    *   **Scope of Applicability**: The current implementation is demonstrated for graph classification tasks. While the paper states the invariant learning algorithm is \"suitable for any deep models,\" its specific implementation details (e.g., rationale generator for graphs) are tailored to GNNs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wu2022vcx} significantly advances the field of interpretable GNNs by moving beyond statistical correlations to identify truly causal and invariant rationales. It addresses a critical weakness of existing methods: their vulnerability to data biases and poor OOD generalization.\n    *   **Potential Impact**: This work provides a robust framework for building more trustworthy and generalizable GNNs. By ensuring that models learn from causal patterns, it can lead to more reliable insights in sensitive applications (e.g., drug discovery, material science) where understanding the *why* behind predictions is paramount. The novel use of causal interventions to generate environments opens new avenues for invariant learning research in settings where explicit environment labels are unavailable.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) promise powerful insights, yet their intrinsic interpretability often falters in real-world applications. Existing rationalization methods frequently exploit unstable \"shortcut features\" and data biases, leading to unreliable explanations and catastrophic performance drops on out-of-distribution (OOD) data. This critical limitation hinders GNN trustworthiness and adoption in sensitive domains like drug discovery, where understanding *causal* patterns is paramount.\n\nWe introduce Discovering Invariant Rationale (DIR), a novel invariant learning strategy for intrinsically interpretable GNNs. Unlike prior methods that assume observable environments, DIR tackles this challenge by performing *causal interventions* on non-causal graph components. This innovative approach generates diverse interventional distributions, enabling the model to distinguish stable, causal patterns from spurious correlations. Our modular framework minimizes both the average and variance of interventional risks, ensuring rationales are robust and generalize across varying data landscapes. Extensive experiments demonstrate DIR's superior OOD generalization and ability to identify true causal features, paving the way for more reliable, auditable, and impactful GNN applications.",
    "keywords": [
      "Discovering Invariant Rationale (DIR)",
      "Intrinsically Interpretable GNNs",
      "Causal Intervention",
      "Invariant Learning",
      "Out-of-Distribution (OOD) Generalization",
      "Invariant Rationales",
      "Spurious Correlations",
      "Environment Generation",
      "Graph Classification",
      "Modular Architecture",
      "Theoretical Justification",
      "Drug Discovery",
      "Material Science",
      "Invariant Risk Minimization"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf",
    "citation_key": "wu2022vcx",
    "metadata": {
      "title": "Discovering Invariant Rationales for Graph Neural Networks",
      "authors": [
        "Yingmin Wu",
        "Xiang Wang",
        "An Zhang",
        "Xiangnan He",
        "Tat-seng Chua"
      ],
      "published_date": "2022",
      "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
      "file_path": "paper_data/Graph_Neural_Networks/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{wu2022vcx} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Existing intrinsically interpretable Graph Neural Networks (GNNs) for rationalization often rely on \"shortcut features\" or data biases to compose rationales and make predictions. These shortcuts are unstable and change in out-of-distribution (OOD) data, leading to a significant drop in both interpretability (rationales don't reflect true causal patterns) and predictive performance on OOD data.\n    *   **Importance & Challenge**: Understanding \"what knowledge drives the model\" is crucial for auditing GNNs, justifying predictions, and enabling real-world applications (e.g., drug discovery). The challenge lies in identifying the *causal* patterns that are stable across different data distributions, rather than spurious correlations, especially when environments are not explicitly observable in the training data.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: GNN rationalization falls into post-hoc explainability (separate explanation method) and intrinsic interpretability (rationalization module within the model). This work focuses on intrinsic interpretability, where methods like graph attention and pooling operators generate masks on input graphs to identify rationales.\n    *   **Limitations of Previous Solutions**: Current intrinsic rationalization methods, despite their appeal, are shown to be prone to exploiting data biases and shortcuts. They minimize empirical risk, which can lead to learning non-causal statistical associations rather than the true underlying causal mechanisms, resulting in poor generalization to OOD data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{wu2022vcx} proposes Discovering Invariant Rationale (DIR), a novel invariant learning strategy for intrinsically interpretable GNNs. DIR aims to identify causal patterns that are stable (invariant) across different data distributions while filtering out unstable, spurious patterns.\n    *   **Novelty**:\n        *   **Causal Intervention for Environment Generation**: Unlike prior invariant learning (IL) methods that assume observable environments, DIR generates \"interventional distributions\" by performing causal interventions (`do(S=s)`) on the non-causal part (`S`) of the input graph. This allows the model to simulate multiple environments from a standard training set, enabling the distinction between causal and non-causal patterns.\n        *   **DIR Principle**: The learning strategy minimizes both the average s-interventional risk and the variance of these risks across different interventional distributions. This encourages the model to find rationales whose relationship with the label is stable.\n        *   **Modular Architecture**: DIR consists of four key modules:\n            1.  **Rationale Generator**: Splits the input graph into a potential causal part (`~c`) and a non-causal part (`~s`) by masking edges based on importance scores.\n            2.  **Distribution Intervener**: Creates interventional distributions by sampling non-causal parts from a memory bank and replacing the original non-causal part of an instance, forming an intervened pair `(~c_j, ~s_i)`.\n            3.  **Graph Encoder**: A shared GNN encoder processes both `~c` and `~s` into graph representations.\n            4.  **Two Classifiers**: One classifier (`phi_c`) predicts from the causal part (`^y_~c`), and another (`phi_s`) predicts from the non-causal part (`^y_~s`). The joint prediction `^y` under intervention is formulated as `^y_~c` masked by `sigmoid(^y_~s)`, where the sigmoid function helps compensate for spurious biases.\n        *   **Optimization Strategy**: The overall objective combines the DIR principle's invariant risk minimization with an additional loss for the non-causal classifier (`phi_s`) that is *not* backpropagated to other components. This encourages `phi_s` to learn spurious biases from non-causal features without interfering with the causal representation learning. During inference, only the causal rationale `~c` and its prediction `^y_~c` are used.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the Discovering Invariant Rationale (DIR) algorithm, a new invariant learning approach specifically tailored for intrinsically interpretable GNNs.\n    *   **Causal Intervention Mechanism**: A method to generate interventional distributions via `do(S=s)` operations on the non-causal graph components, addressing the challenge of unobservable environments in standard IL settings.\n    *   **Theoretical Justification**: Provides causality-theoretic analysis to guarantee that the DIR principle can discover invariant rationales and that the oracle model respects this principle.\n    *   **System Design**: A modular framework integrating a rationale generator, distribution intervener, shared encoder, and dual classifiers with a specialized optimization strategy for invariant learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate DIR's effectiveness in discovering causal features and improving generalization, as well as to understand its learning patterns.\n    *   **Datasets**:\n        *   **Spurious-Motif (synthetic)**: A controlled dataset where graph labels are *solely* determined by a \"motif\" (causal part), while \"bases\" (non-causal part) are introduced with varying degrees of spurious correlation to the label in the training set. Testing data has random motif-base attachments to assess OOD generalization.\n        *   Three real-world graph classification datasets (details in Appendix D).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   DIR consistently demonstrated superior generalization ability compared to state-of-the-art invariant learning methods (e.g., IRM \\cite{arjovsky2019invariant}, V-REx \\cite{krueger2021out}, GroupDRO \\cite{sagawa2019distributionally}).\n        *   It also outperformed attention- and pooling-based rationalization methods (e.g., GAT \\cite{velickovic2018graph}, GNNExplainer \\cite{ying2019gnnexplainer}) in terms of interpretability (discovering true causal features) and predictive performance on OOD data.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper assumes a causal view of the data-generating process with a clear distinction between causal and non-causal parts, and that the causal part is the sole determinant of the ground-truth label. The effectiveness of the rationale generator in accurately splitting the graph into `~c` and `~s` is crucial. The `do(S=s)` intervention relies on the ability to isolate and manipulate the non-causal part.\n    *   **Scope of Applicability**: The current implementation is demonstrated for graph classification tasks. While the paper states the invariant learning algorithm is \"suitable for any deep models,\" its specific implementation details (e.g., rationale generator for graphs) are tailored to GNNs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wu2022vcx} significantly advances the field of interpretable GNNs by moving beyond statistical correlations to identify truly causal and invariant rationales. It addresses a critical weakness of existing methods: their vulnerability to data biases and poor OOD generalization.\n    *   **Potential Impact**: This work provides a robust framework for building more trustworthy and generalizable GNNs. By ensuring that models learn from causal patterns, it can lead to more reliable insights in sensitive applications (e.g., drug discovery, material science) where understanding the *why* behind predictions is paramount. The novel use of causal interventions to generate environments opens new avenues for invariant learning research in settings where explicit environment labels are unavailable.",
      "keywords": [
        "Discovering Invariant Rationale (DIR)",
        "Intrinsically Interpretable GNNs",
        "Causal Intervention",
        "Invariant Learning",
        "Out-of-Distribution (OOD) Generalization",
        "Invariant Rationales",
        "Spurious Correlations",
        "Environment Generation",
        "Graph Classification",
        "Modular Architecture",
        "Theoretical Justification",
        "Drug Discovery",
        "Material Science",
        "Invariant Risk Minimization"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose a new strategy\" / \"propose a novel invariant learning algorithm\"**: the abstract and introduction explicitly state the development of a new method called \"discovering invariant rationale (dir)\". this is a core indicator of a technical paper.\n2.  **description of a system/algorithm**: the paper details the \"dir strategy\" consisting of \"four modules: a rationale generator, a distribution intervener, a feature encoder, two classiﬁers.\" this describes the architecture and components of a new system/algorithm.\n3.  **implementation details**: section 2.4 is titled \"dir-guided implementation of intrinsically-interpretable gnns\" and describes how the proposed method is put into practice.\n4.  **empirical validation of a new method**: while the paper includes extensive experiments (\"experiments on both synthetic and real-world datasets validate the superiority of our dir\"), these experiments are conducted to demonstrate the effectiveness of the *newly proposed* dir algorithm, rather than being a standalone empirical study of existing phenomena or methods.\n5.  **theoretical justification for a new method**: the paper offers \"causality theoretic analysis to guarantee the preeminence of dir\" and mentions \"theorem 1\". this theoretical work supports the proposed technical solution, but the primary contribution is the algorithm itself.\n\nthe paper's main goal is to introduce, describe, and validate a novel algorithm/strategy for discovering invariant rationales in gnns, which aligns perfectly with the definition of a **technical** paper."
    },
    "file_name": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf"
  },
  {
    "success": true,
    "doc_id": "37530b570592d896dc568587e2c2c3e8",
    "summary": "Here's a focused summary of the paper \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" \\cite{morris20185sd} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Standard Graph Neural Networks (GNNs) have shown empirical success but lack a strong theoretical understanding of their expressive power in distinguishing graph structures. It is unclear how they encode graph structure information and whether they have theoretical advantages over traditional graph kernels.\n    *   **Importance & Challenge**: Understanding the theoretical limits of GNNs is crucial for principled design and improvement. Many real-world graphs (e.g., social networks, molecules) possess complex higher-order structures (e.g., triangles, cliques, communities) that simple node-level message passing might not effectively capture, leading to limitations in tasks like graph classification and regression.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work theoretically connects GNNs to the 1-dimensional Weisfeiler-Leman (1-WL) graph isomorphism heuristic, which is a foundational algorithm for many successful graph kernels (e.g., WL subtree kernel).\n    *   **Limitations of Previous Solutions**:\n        *   **Graph Kernels**: Typically rely on fixed feature construction schemes (e.g., indicator features over subgraphs), limiting their adaptability to specific data distributions. Most focus only on graph structure and cannot interpret continuous node/edge labels.\n        *   **Standard GNNs (1-GNNs)**: While addressing some kernel limitations (adaptability, continuous features), this paper theoretically demonstrates that 1-GNNs have the same expressive power as the 1-WL algorithm. Consequently, they inherit the same shortcomings, such as inability to distinguish certain non-isomorphic graphs or capture simple higher-order properties like triangle counts.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **k-dimensional GNNs (k-GNNs)**, a generalization of 1-GNNs based on the k-dimensional Weisfeiler-Leman (k-WL) algorithm. Instead of performing message passing between individual nodes, k-GNNs perform message passing directly between *k-element subsets* (or k-tuples) of nodes.\n    *   **Novelty/Difference**:\n        *   **Higher-Order Message Passing**: This is the core innovation, allowing the model to capture structural information that is not visible at the node-level. The update rule for a k-set `s` aggregates features from its \"neighborhood\" of other k-sets `t` that share `k-1` elements with `s`.\n        *   **Hierarchical k-GNNs (1-k-GNNs)**: A further innovation is the design of hierarchical variants. These models combine graph representations learned at different granularities. Specifically, the initial messages (features) for k-sets in a k-GNN are derived from the output of a lower-dimensional k'-GNN (e.g., a 1-GNN). This allows the model to effectively capture both fine-grained (node-level) and coarse-grained (subgraph-level) structures in an end-to-end trainable framework.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight**: Proved that standard GNNs (1-GNNs) are not more powerful than the 1-WL algorithm in distinguishing non-isomorphic (sub-)graphs. Furthermore, with suitable parameter initialization, 1-GNNs achieve the same expressive power as 1-WL. This establishes a fundamental theoretical upper bound on the expressive power of current GNN architectures.\n    *   **Novel Algorithms/Methods**: Introduction of **k-GNNs**, which are strictly more powerful than 1-GNNs by operating on k-tuples/subsets.\n    *   **System Design/Architectural Innovation**: Proposal of **hierarchical 1-k-GNNs**, which leverage the outputs of lower-dimensional GNNs to initialize higher-dimensional GNNs, enabling multi-scale structural learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments on both graph classification and graph regression tasks to validate their theoretical findings and the utility of higher-order information.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Graph Regression (QM9 benchmark)**: On 12 molecular property prediction tasks, hierarchical models (1-2-GNN, 1-3-GNN) consistently and significantly outperformed traditional 1-GNNs. The 1-2-GNN reduced the Mean Absolute Error (MAE) by **54.45% on average** compared to the 1-GNN.\n        *   **Graph Classification (MUTAG, PTC_MR, NCI1, PROTEINS datasets)**: Hierarchical models (e.g., 1-2-GNN) also led to performance gains, though these were described as \"slight\" compared to the dramatic improvements seen in regression.\n    *   **Conclusion**: The experimental results confirm that higher-order graph properties are important for successful graph classification and regression, especially for tasks requiring a deeper understanding of molecular structures.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper highlights that 1-GNNs, like 1-WL, cannot capture simple graph-theoretic properties such as triangle counts or distinguish certain non-isomorphic graphs (e.g., a triangle vs. a 4-cycle). The proposed k-GNNs address these limitations by increasing `k`, but higher `k` values inherently lead to increased computational complexity and memory requirements due to the exponential growth in the number of k-sets. The specific set-based k-WL variant used for k-GNNs is noted to be different from the folklore k-WL but is shown to be as powerful as the folklore (k-1)-WL.\n    *   **Scope of Applicability**: k-GNNs are particularly relevant for applications where higher-order structural motifs (e.g., cycles, cliques, specific substructures) are crucial for understanding graph properties, such as in chemoinformatics (molecular graphs) and social network analysis.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper provides the first rigorous theoretical characterization of the expressive power of standard GNNs, linking them directly to the well-understood 1-WL algorithm. It then proposes a principled and theoretically grounded method (k-GNNs) to overcome these identified limitations, significantly advancing the expressive capabilities of GNNs.\n    *   **Potential Impact on Future Research**: The work opens new avenues for designing more powerful GNN architectures by explicitly incorporating higher-order structural information. It encourages further theoretical analysis of GNNs and the development of scalable k-GNN variants, potentially leading to breakthroughs in tasks requiring fine-grained structural understanding. The concept of hierarchical GNNs also suggests future research into multi-scale graph representation learning.",
    "intriguing_abstract": "While Graph Neural Networks (GNNs) have revolutionized graph learning, their theoretical expressive power has remained a critical bottleneck, often limiting them to the capabilities of the 1-dimensional Weisfeiler-Leman (1-WL) algorithm. This fundamental limitation prevents standard GNNs from effectively distinguishing complex graph structures and capturing vital higher-order properties like cycles and cliques.\n\nThis paper unveils **k-dimensional GNNs (k-GNNs)**, a novel paradigm extending message passing from individual nodes to *k-element subsets*, directly leveraging the power of the k-dimensional Weisfeiler-Leman algorithm. We rigorously prove that 1-GNNs are no more powerful than 1-WL, then demonstrate that k-GNNs are strictly more expressive, capable of capturing intricate structural information previously inaccessible. Further, we introduce **hierarchical 1-k-GNNs**, which combine representations learned at different granularities, enabling multi-scale structural understanding. Our empirical validation shows unprecedented gains, with hierarchical models reducing Mean Absolute Error by **54.45% on average** in molecular property prediction (QM9 benchmark) compared to 1-GNNs, and achieving notable improvements in graph classification. This work provides a principled path to designing significantly more powerful GNNs, opening new frontiers for understanding and predicting complex graph properties.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Weisfeiler-Leman (WL) algorithm",
      "Expressive power of GNNs",
      "k-dimensional GNNs (k-GNNs)",
      "Higher-order message passing",
      "Hierarchical GNNs",
      "Graph isomorphism",
      "Theoretical upper bound",
      "Graph kernels",
      "Graph classification",
      "Graph regression",
      "Molecular property prediction",
      "Computational complexity",
      "Multi-scale graph representation learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf",
    "citation_key": "morris20185sd",
    "metadata": {
      "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
      "authors": [
        "Christopher Morris",
        "Martin Ritzert",
        "Matthias Fey",
        "William L. Hamilton",
        "J. E. Lenssen",
        "Gaurav Rattan",
        "Martin Grohe"
      ],
      "published_date": "2018",
      "abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.",
      "file_path": "paper_data/Graph_Neural_Networks/6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" \\cite{morris20185sd} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Standard Graph Neural Networks (GNNs) have shown empirical success but lack a strong theoretical understanding of their expressive power in distinguishing graph structures. It is unclear how they encode graph structure information and whether they have theoretical advantages over traditional graph kernels.\n    *   **Importance & Challenge**: Understanding the theoretical limits of GNNs is crucial for principled design and improvement. Many real-world graphs (e.g., social networks, molecules) possess complex higher-order structures (e.g., triangles, cliques, communities) that simple node-level message passing might not effectively capture, leading to limitations in tasks like graph classification and regression.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work theoretically connects GNNs to the 1-dimensional Weisfeiler-Leman (1-WL) graph isomorphism heuristic, which is a foundational algorithm for many successful graph kernels (e.g., WL subtree kernel).\n    *   **Limitations of Previous Solutions**:\n        *   **Graph Kernels**: Typically rely on fixed feature construction schemes (e.g., indicator features over subgraphs), limiting their adaptability to specific data distributions. Most focus only on graph structure and cannot interpret continuous node/edge labels.\n        *   **Standard GNNs (1-GNNs)**: While addressing some kernel limitations (adaptability, continuous features), this paper theoretically demonstrates that 1-GNNs have the same expressive power as the 1-WL algorithm. Consequently, they inherit the same shortcomings, such as inability to distinguish certain non-isomorphic graphs or capture simple higher-order properties like triangle counts.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **k-dimensional GNNs (k-GNNs)**, a generalization of 1-GNNs based on the k-dimensional Weisfeiler-Leman (k-WL) algorithm. Instead of performing message passing between individual nodes, k-GNNs perform message passing directly between *k-element subsets* (or k-tuples) of nodes.\n    *   **Novelty/Difference**:\n        *   **Higher-Order Message Passing**: This is the core innovation, allowing the model to capture structural information that is not visible at the node-level. The update rule for a k-set `s` aggregates features from its \"neighborhood\" of other k-sets `t` that share `k-1` elements with `s`.\n        *   **Hierarchical k-GNNs (1-k-GNNs)**: A further innovation is the design of hierarchical variants. These models combine graph representations learned at different granularities. Specifically, the initial messages (features) for k-sets in a k-GNN are derived from the output of a lower-dimensional k'-GNN (e.g., a 1-GNN). This allows the model to effectively capture both fine-grained (node-level) and coarse-grained (subgraph-level) structures in an end-to-end trainable framework.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight**: Proved that standard GNNs (1-GNNs) are not more powerful than the 1-WL algorithm in distinguishing non-isomorphic (sub-)graphs. Furthermore, with suitable parameter initialization, 1-GNNs achieve the same expressive power as 1-WL. This establishes a fundamental theoretical upper bound on the expressive power of current GNN architectures.\n    *   **Novel Algorithms/Methods**: Introduction of **k-GNNs**, which are strictly more powerful than 1-GNNs by operating on k-tuples/subsets.\n    *   **System Design/Architectural Innovation**: Proposal of **hierarchical 1-k-GNNs**, which leverage the outputs of lower-dimensional GNNs to initialize higher-dimensional GNNs, enabling multi-scale structural learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments on both graph classification and graph regression tasks to validate their theoretical findings and the utility of higher-order information.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Graph Regression (QM9 benchmark)**: On 12 molecular property prediction tasks, hierarchical models (1-2-GNN, 1-3-GNN) consistently and significantly outperformed traditional 1-GNNs. The 1-2-GNN reduced the Mean Absolute Error (MAE) by **54.45% on average** compared to the 1-GNN.\n        *   **Graph Classification (MUTAG, PTC_MR, NCI1, PROTEINS datasets)**: Hierarchical models (e.g., 1-2-GNN) also led to performance gains, though these were described as \"slight\" compared to the dramatic improvements seen in regression.\n    *   **Conclusion**: The experimental results confirm that higher-order graph properties are important for successful graph classification and regression, especially for tasks requiring a deeper understanding of molecular structures.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper highlights that 1-GNNs, like 1-WL, cannot capture simple graph-theoretic properties such as triangle counts or distinguish certain non-isomorphic graphs (e.g., a triangle vs. a 4-cycle). The proposed k-GNNs address these limitations by increasing `k`, but higher `k` values inherently lead to increased computational complexity and memory requirements due to the exponential growth in the number of k-sets. The specific set-based k-WL variant used for k-GNNs is noted to be different from the folklore k-WL but is shown to be as powerful as the folklore (k-1)-WL.\n    *   **Scope of Applicability**: k-GNNs are particularly relevant for applications where higher-order structural motifs (e.g., cycles, cliques, specific substructures) are crucial for understanding graph properties, such as in chemoinformatics (molecular graphs) and social network analysis.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper provides the first rigorous theoretical characterization of the expressive power of standard GNNs, linking them directly to the well-understood 1-WL algorithm. It then proposes a principled and theoretically grounded method (k-GNNs) to overcome these identified limitations, significantly advancing the expressive capabilities of GNNs.\n    *   **Potential Impact on Future Research**: The work opens new avenues for designing more powerful GNN architectures by explicitly incorporating higher-order structural information. It encourages further theoretical analysis of GNNs and the development of scalable k-GNN variants, potentially leading to breakthroughs in tasks requiring fine-grained structural understanding. The concept of hierarchical GNNs also suggests future research into multi-scale graph representation learning.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Weisfeiler-Leman (WL) algorithm",
        "Expressive power of GNNs",
        "k-dimensional GNNs (k-GNNs)",
        "Higher-order message passing",
        "Hierarchical GNNs",
        "Graph isomorphism",
        "Theoretical upper bound",
        "Graph kernels",
        "Graph classification",
        "Graph regression",
        "Molecular property prediction",
        "Computational complexity",
        "Multi-scale graph representation learning"
      ],
      "paper_type": "this paper is best classified as **theoretical**.\n\nhere's why:\n\n1.  **explicit theoretical focus:** the abstract explicitly states: \"the following work investigates gnns from a theoretical point of view and relates them to the 1-dimensional weisfeiler-leman graph isomorphism heuristic (1-wl).\"\n2.  **formal analysis of expressiveness:** a core contribution is \"we show that gnns have the same expressiveness as the 1-wl in terms of distinguishing non-isomorphic (sub-)graphs.\" this is a formal analysis of the capabilities and limitations of gnns, which falls under mathematical analysis and formal models.\n3.  **theoretical basis for proposal:** the proposal of \"k-dimensional gnns (k-gnns)\" is directly \"based on this\" theoretical finding, indicating that the theoretical analysis is foundational to the new method.\n4.  **empirical confirmation:** while there is an \"experimental evaluation,\" its purpose is to \"conﬁrm our theoretical ﬁndings\" and the usefulness of the proposed model, making it supportive of the primary theoretical and technical contributions rather than the sole focus.\n\nalthough it also proposes a new method (technical) and includes experiments (empirical), the driving force and initial significant contribution are the theoretical investigation and formal analysis of gnns' expressiveness."
    },
    "file_name": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf"
  },
  {
    "success": true,
    "doc_id": "761315fd304c0e5eb4b0036d1fd333c8",
    "summary": "This paper, \"A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability\" by Dai et al. \\cite{dai2022hsi}, provides a detailed review of the critical aspects of trustworthiness in Graph Neural Networks (GNNs).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Despite the rapid advancements and widespread application of GNNs in high-stakes scenarios (e.g., financial analysis, drug discovery), they suffer from significant trustworthiness issues. These include vulnerability to privacy leakage, adversarial attacks, inherent biases leading to unfair outcomes, and a lack of interpretability \\cite{dai2022hsi}.\n    *   **Importance and Challenge:** These issues pose substantial risks of unintentional harm to users and society, severely limiting the adoption of GNNs in critical real-world applications. The unique complexities of graph topology and the message-passing mechanism in GNNs mean that trustworthy AI solutions designed for independent and identically distributed (i.i.d) data are often not directly applicable, necessitating dedicated research into trustworthy GNNs \\cite{dai2022hsi}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a comprehensive survey, building upon existing, more narrowly focused surveys on GNN robustness, explainability, and fairness \\cite{dai2022hsi}. It also acknowledges general trustworthy AI surveys, but highlights their limitations regarding graph-structured data \\cite{dai2022hsi}.\n    *   **Limitations of Previous Solutions:**\n        *   Prior GNN-specific surveys often lack comprehensive coverage, omitting crucial dimensions like privacy and fairness, or failing to include emerging techniques (e.g., scalable attacks, backdoor attacks, self-explainable GNNs) \\cite{dai2022hsi}.\n        *   Surveys on trustworthy AI systems primarily focus on i.i.d data, making their findings less relevant or directly transferable to GNNs due to the distinct challenges posed by graph topology and message-passing mechanisms \\cite{dai2022hsi}.\n        *   Compared to a concurrent survey on trustworthy GNNs, this work covers more recent advanced topics such as machine unlearning, model ownership verification, scalable adversarial attacks, fair contrastive learning, explanation-enhanced fairness, and self-explainable GNNs \\cite{dai2022hsi}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** As a survey paper, the core method involves systematically reviewing, categorizing, and synthesizing the vast literature on GNN trustworthiness. For each of the four aspects (privacy, robustness, fairness, explainability), the paper provides:\n        *   A detailed taxonomy of existing methods \\cite{dai2022hsi}.\n        *   Formulation of general frameworks for different categories of trustworthy GNNs \\cite{dai2022hsi}.\n        *   Discussions on future research directions and the interconnections between these aspects \\cite{dai2022hsi}.\n    *   **Novelty/Difference:** The innovation lies in its *holistic and up-to-date coverage* of all four critical trustworthiness dimensions specifically for GNNs, addressing a significant gap in the literature. It provides a structured and comprehensive overview, including recent advancements, and explicitly discusses the interdependencies between privacy, robustness, fairness, and explainability, which is crucial for achieving truly trustworthy GNNs \\cite{dai2022hsi}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The paper does not propose new algorithms but contributes by:\n        *   Presenting a comprehensive taxonomy of privacy attacks (membership inference, property inference, reconstruction, model extraction) and defense methods on GNNs \\cite{dai2022hsi}.\n        *   Categorizing various adversarial attack and defense methods for GNN robustness, including recent advances like scalable attacks, graph backdoor attacks, and self-supervised learning defenses \\cite{dai2022hsi}.\n        *   Thoroughly discussing fairness in GNNs, covering biases, fairness definitions on graph data, and various fair GNN models \\cite{dai2022hsi}.\n        *   Providing a taxonomic summary of methodologies for GNN explainability, detailing motivations, challenges, and experimental settings \\cite{dai2022hsi}.\n    *   **Theoretical Insights or Analysis:** The paper offers insights into the unique challenges of achieving trustworthiness in GNNs due to their message-passing mechanism and graph topology. It highlights the interconnectedness of privacy, robustness, fairness, and explainability, suggesting that advancements in one area can inform or impact others (e.g., explanations aiding in debugging adversarial attacks or evaluating bias) \\cite{dai2022hsi}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, no new experiments were conducted.\n    *   **Key Performance Metrics and Comparison Results:** The paper summarizes the experimental validation from the reviewed literature. It lists relevant graph datasets used in privacy research and discusses the applications and datasets employed by various fair GNN models, providing context for the empirical findings of the surveyed works \\cite{dai2022hsi}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The survey explicitly focuses on the computational aspects of trustworthiness (privacy, robustness, fairness, explainability) and does not delve into the \"Respect for human autonomy\" principle, which is more aligned with human-computer interaction \\cite{dai2022hsi}. While it discusses connections, a unified framework for simultaneously addressing all four aspects remains an open challenge, which the paper identifies as a future direction \\cite{dai2022hsi}.\n    *   **Scope of Applicability:** The survey's scope is strictly limited to Graph Neural Networks and their trustworthiness, covering various GNN architectures and graph analysis tasks (node classification, link prediction, graph classification, community detection) \\cite{dai2022hsi}.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** This survey significantly advances the state-of-the-art by providing a consolidated, comprehensive, and up-to-date resource on trustworthy GNNs. It systematically organizes fragmented knowledge, clarifies the current landscape, and highlights emerging research directions, thereby serving as a foundational reference for the field \\cite{dai2022hsi}.\n    *   **Potential Impact on Future Research:** It is expected to guide future research by identifying critical gaps and challenges in privacy, robustness, fairness, and explainability for GNNs. By emphasizing the interconnections between these aspects, it encourages the development of more holistic and integrated solutions for building truly trustworthy GNN systems, moving beyond isolated problem-solving \\cite{dai2022hsi}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are transforming high-stakes domains, yet their widespread adoption is severely hampered by critical trustworthiness concerns. Pervasive issues like privacy leakage, vulnerability to adversarial attacks, inherent biases leading to unfair outcomes, and a profound lack of interpretability pose substantial risks, demanding dedicated solutions beyond those for i.i.d. data. This comprehensive survey addresses this formidable challenge by providing a holistic and up-to-date review of trustworthiness in GNNs.\n\nWe systematically categorize and synthesize the vast literature across four crucial dimensions: **privacy, robustness, fairness, and explainability**. Unlike prior fragmented efforts, our work uniquely covers recent advancements such as scalable adversarial attacks, graph backdoor attacks, fair contrastive learning, and self-explainable GNNs, while explicitly highlighting the intricate interdependencies among these aspects. By offering detailed taxonomies, general frameworks, and future research directions, this survey serves as a foundational resource. It is imperative for guiding the development of truly trustworthy GNN systems, fostering integrated solutions that unlock their full, responsible potential in critical real-world applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Trustworthiness",
      "Privacy",
      "Robustness",
      "Fairness",
      "Explainability",
      "Graph topology",
      "Message-passing mechanism",
      "Adversarial attacks",
      "Comprehensive survey",
      "Taxonomy",
      "Interconnectedness",
      "High-stakes applications",
      "Emerging techniques"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf",
    "citation_key": "dai2022hsi",
    "metadata": {
      "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
      "authors": [
        "Enyan Dai",
        "Tianxiang Zhao",
        "Huaisheng Zhu",
        "Jun Xu",
        "Zhimeng Guo",
        "Hui Liu",
        "Jiliang Tang",
        "Suhang Wang"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users’ trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
      "file_path": "paper_data/Graph_Neural_Networks/cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf",
      "venue": "Machine Intelligence Research",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, \"A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability\" by Dai et al. \\cite{dai2022hsi}, provides a detailed review of the critical aspects of trustworthiness in Graph Neural Networks (GNNs).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Despite the rapid advancements and widespread application of GNNs in high-stakes scenarios (e.g., financial analysis, drug discovery), they suffer from significant trustworthiness issues. These include vulnerability to privacy leakage, adversarial attacks, inherent biases leading to unfair outcomes, and a lack of interpretability \\cite{dai2022hsi}.\n    *   **Importance and Challenge:** These issues pose substantial risks of unintentional harm to users and society, severely limiting the adoption of GNNs in critical real-world applications. The unique complexities of graph topology and the message-passing mechanism in GNNs mean that trustworthy AI solutions designed for independent and identically distributed (i.i.d) data are often not directly applicable, necessitating dedicated research into trustworthy GNNs \\cite{dai2022hsi}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a comprehensive survey, building upon existing, more narrowly focused surveys on GNN robustness, explainability, and fairness \\cite{dai2022hsi}. It also acknowledges general trustworthy AI surveys, but highlights their limitations regarding graph-structured data \\cite{dai2022hsi}.\n    *   **Limitations of Previous Solutions:**\n        *   Prior GNN-specific surveys often lack comprehensive coverage, omitting crucial dimensions like privacy and fairness, or failing to include emerging techniques (e.g., scalable attacks, backdoor attacks, self-explainable GNNs) \\cite{dai2022hsi}.\n        *   Surveys on trustworthy AI systems primarily focus on i.i.d data, making their findings less relevant or directly transferable to GNNs due to the distinct challenges posed by graph topology and message-passing mechanisms \\cite{dai2022hsi}.\n        *   Compared to a concurrent survey on trustworthy GNNs, this work covers more recent advanced topics such as machine unlearning, model ownership verification, scalable adversarial attacks, fair contrastive learning, explanation-enhanced fairness, and self-explainable GNNs \\cite{dai2022hsi}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** As a survey paper, the core method involves systematically reviewing, categorizing, and synthesizing the vast literature on GNN trustworthiness. For each of the four aspects (privacy, robustness, fairness, explainability), the paper provides:\n        *   A detailed taxonomy of existing methods \\cite{dai2022hsi}.\n        *   Formulation of general frameworks for different categories of trustworthy GNNs \\cite{dai2022hsi}.\n        *   Discussions on future research directions and the interconnections between these aspects \\cite{dai2022hsi}.\n    *   **Novelty/Difference:** The innovation lies in its *holistic and up-to-date coverage* of all four critical trustworthiness dimensions specifically for GNNs, addressing a significant gap in the literature. It provides a structured and comprehensive overview, including recent advancements, and explicitly discusses the interdependencies between privacy, robustness, fairness, and explainability, which is crucial for achieving truly trustworthy GNNs \\cite{dai2022hsi}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The paper does not propose new algorithms but contributes by:\n        *   Presenting a comprehensive taxonomy of privacy attacks (membership inference, property inference, reconstruction, model extraction) and defense methods on GNNs \\cite{dai2022hsi}.\n        *   Categorizing various adversarial attack and defense methods for GNN robustness, including recent advances like scalable attacks, graph backdoor attacks, and self-supervised learning defenses \\cite{dai2022hsi}.\n        *   Thoroughly discussing fairness in GNNs, covering biases, fairness definitions on graph data, and various fair GNN models \\cite{dai2022hsi}.\n        *   Providing a taxonomic summary of methodologies for GNN explainability, detailing motivations, challenges, and experimental settings \\cite{dai2022hsi}.\n    *   **Theoretical Insights or Analysis:** The paper offers insights into the unique challenges of achieving trustworthiness in GNNs due to their message-passing mechanism and graph topology. It highlights the interconnectedness of privacy, robustness, fairness, and explainability, suggesting that advancements in one area can inform or impact others (e.g., explanations aiding in debugging adversarial attacks or evaluating bias) \\cite{dai2022hsi}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, no new experiments were conducted.\n    *   **Key Performance Metrics and Comparison Results:** The paper summarizes the experimental validation from the reviewed literature. It lists relevant graph datasets used in privacy research and discusses the applications and datasets employed by various fair GNN models, providing context for the empirical findings of the surveyed works \\cite{dai2022hsi}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The survey explicitly focuses on the computational aspects of trustworthiness (privacy, robustness, fairness, explainability) and does not delve into the \"Respect for human autonomy\" principle, which is more aligned with human-computer interaction \\cite{dai2022hsi}. While it discusses connections, a unified framework for simultaneously addressing all four aspects remains an open challenge, which the paper identifies as a future direction \\cite{dai2022hsi}.\n    *   **Scope of Applicability:** The survey's scope is strictly limited to Graph Neural Networks and their trustworthiness, covering various GNN architectures and graph analysis tasks (node classification, link prediction, graph classification, community detection) \\cite{dai2022hsi}.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** This survey significantly advances the state-of-the-art by providing a consolidated, comprehensive, and up-to-date resource on trustworthy GNNs. It systematically organizes fragmented knowledge, clarifies the current landscape, and highlights emerging research directions, thereby serving as a foundational reference for the field \\cite{dai2022hsi}.\n    *   **Potential Impact on Future Research:** It is expected to guide future research by identifying critical gaps and challenges in privacy, robustness, fairness, and explainability for GNNs. By emphasizing the interconnections between these aspects, it encourages the development of more holistic and integrated solutions for building truly trustworthy GNN systems, moving beyond isolated problem-solving \\cite{dai2022hsi}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Trustworthiness",
        "Privacy",
        "Robustness",
        "Fairness",
        "Explainability",
        "Graph topology",
        "Message-passing mechanism",
        "Adversarial attacks",
        "Comprehensive survey",
        "Taxonomy",
        "Interconnectedness",
        "High-stakes applications",
        "Emerging techniques"
      ],
      "paper_type": "the paper type is **survey**.\n\n**reasoning:**\n\n1.  **title:** the title \"a comprehensive survey on trustworthy graph neural networks: privacy, robustness, fairness, and explainability\" explicitly uses the term \"survey\" and \"comprehensive,\" which are direct indicators of a survey paper.\n2.  **abstract:**\n    *   it states, \"in this survey, we also have some discussions about the interactions of the trustworthiness aspects in the future directions.\"\n    *   it compares itself to other existing surveys: \"there are several surveys of gnns in robustness... explainability... and fairness. however, none of them thoroughly discuss about the trustworthiness of gnns...\"\n    *   its \"major contributions\" are listed as: \"give a comprehensive survey of the existing works in privacy attacks and defense on gnns,\" \"various categories of adversarial attack and defense methods on gnn models are discussed,\" \"the fairness of trustworthy gnns is thoroughly discussed,\" and \"a comprehensive survey of gnn explainability is presented.\" these all describe reviewing and organizing existing literature.\n3.  **introduction:** the introduction sets the stage for the importance of trustworthy gnns and then immediately delves into the various aspects (privacy, robustness, fairness, explainability) that the paper will cover, consistent with a comprehensive review.\n4.  **content structure (as indicated by abstract):** the abstract outlines sections (3, 4, 5, 6) dedicated to reviewing existing works in privacy, robustness, fairness, and explainability, respectively. section 2 is \"preliminaries of graph neural networks,\" which is typical for a survey to provide background.\n5.  **keywords in criteria:** the abstract directly mentions \"survey\" and \"comprehensive analysis\" and discusses \"literature organization\" and \"classification schemes\" (e.g., \"taxonomy of privacy attacks,\" \"categorization of representative graph adversarial attacks,\" \"categorization of the privacy-preserving gnns\")."
    },
    "file_name": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf"
  },
  {
    "success": true,
    "doc_id": "e3582007d6acd6a359a222fcf93368e1",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Network (GNN) explanation methods primarily focus on \"subgraph-specific\" (node-wise, local) interpretations, which are insufficient for graph-level tasks that necessitate understanding \"long-range dependencies and global interactions\" across the entire graph \\cite{wang2024j6z}.\n    *   **Importance and Challenge**: This gap between local explanations and the need for global-level insights hinders human trust and limits GNN application in critical domains. Addressing this requires overcoming high computational complexity when modeling global interactions involving numerous nodes, and identifying diverse yet representative global patterns within acceptable overhead \\cite{wang2024j6z}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against both post-hoc (e.g., GNNExplainer, PGExplainer) and intrinsically interpretable (e.g., GIB, GSAT, ProtGNN) GNN explanation methods \\cite{wang2024j6z}.\n    *   **Limitations of Previous Solutions**:\n        *   Post-hoc methods may be \"disloyal to the original model,\" leading to \"distorted attribution analysis\" \\cite{wang2024j6z}.\n        *   Intrinsic methods like ProtGNN can introduce \"explanatory biases\" through extra projection processes \\cite{wang2024j6z}.\n        *   Crucially, all prior methods \"only provide one-side attribution analysis from a localized viewpoint,\" resulting in \"under-representative explanations when higher-order node interactions or global graph structure play a pivotal role\" \\cite{wang2024j6z}. They struggle with the computational demands of extracting global patterns.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Global Interactive Pattern (GIP) learning, an intrinsically interpretable scheme for graph classification that explicitly interprets decisions using learnable global interactive patterns \\cite{wang2024j6z}.\n    *   **Novelty/Difference**: GIP tackles interpretation from a global perspective by first compressing the graph and then identifying inter-cluster interactions in the coarsened graph instances. It consists of two key modules:\n        1.  **Clustering Assignment Module**: Employs a constrained graph clustering module to iteratively aggregate similar components into cluster-level representations. It learns a trainable cluster assignment matrix `S` by minimizing an unsupervised loss term based on a relaxation of the K-way normalized cut, ensuring the clustering reflects real-world graph characteristics \\cite{wang2024j6z}.\n        2.  **Interactive Pattern Matching Module**: Matches the coarsened graph with a batch of \"self-interpretable graph prototypes\" (learnable interactive patterns). Unlike prior work that learns embeddings in hidden space, GIP defines these patterns directly \"in the form of graph structure\" to explicitly reveal vital graph-level patterns. It uses Random Walk Graph Kernels to measure similarity between the coarsened graph and these patterns, which then drives the prediction via a softmax layer \\cite{wang2024j6z}.\n\n*   **Key Technical Contributions**\n    *   **Novel Task Formulation**: Introduction of Global Interactive Pattern (GIP) learning, shifting GNN interpretability from local subgraph explanations to global interactive patterns \\cite{wang2024j6z}.\n    *   **Holistic Framework**: A two-stage framework that achieves both \"high computational efficiency and accurate pattern discovery\" by integrating learnable cluster constraints and graph prototypes for reliable graph-level explanations \\cite{wang2024j6z}.\n    *   **Directly Learnable Global Patterns**: The innovation of defining and learning interactive patterns directly as graph structures, rather than hidden embeddings, enabling transparent graph-level reasoning \\cite{wang2024j6z}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \"Extensive experiments conducted on both synthetic and real-world benchmarks\" \\cite{wang2024j6z}.\n    *   **Key Performance Metrics and Comparison Results**: GIP \"yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts.\" Visualization of explanations further demonstrates its \"superior capability in identifying global interactive patterns\" \\cite{wang2024j6z}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The proposed GIP framework is specifically designed for \"graph classification\" tasks \\cite{wang2024j6z}.\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes that global interactive patterns can be effectively represented and learned through a coarsening process and matching with graph prototypes. While it aims for computational efficiency, the inherent complexity of global pattern discovery remains a challenge that GIP seeks to mitigate rather than eliminate entirely \\cite{wang2024j6z}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GIP significantly advances the technical state-of-the-art by bridging the \"significant gap between local subgraph-specific explanations and global-level explanations\" for GNNs, particularly for graph-level tasks \\cite{wang2024j6z}.\n    *   **Potential Impact**: It enables a \"transparent graph-level reasoning process,\" providing more comprehensive and accurate explanations. This can enhance human trust in GNNs, facilitating their adoption in \"safety-critical domains\" by offering interpretable insights into complex global interactions \\cite{wang2024j6z}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel in complex graph-level tasks, yet their black-box nature, particularly the inability to explain decisions based on global interactions, severely limits trust and adoption in critical domains. Current explanation methods are predominantly local, failing to capture the crucial long-range dependencies that drive whole-graph predictions. We introduce Global Interactive Pattern (GIP) learning, a novel intrinsically interpretable framework designed to bridge this significant gap. GIP explicitly interprets GNN graph classification decisions by identifying learnable global interactive patterns.\n\nOur two-stage approach first employs a constrained graph clustering module to efficiently coarsen the input graph, then matches these cluster-level representations against self-interpretable graph prototypes. Crucially, these prototypes are learned directly as graph structures, not hidden embeddings, enabling transparent graph-level reasoning via Random Walk Graph Kernels. Extensive experiments demonstrate GIP's significantly superior interpretability and competitive performance, offering unprecedented insights into global GNN decision-making. This advancement fosters greater human trust and expands GNN applicability in safety-critical domains by providing comprehensive, global explanations.",
    "keywords": [
      "GNN interpretability",
      "Global Interactive Pattern (GIP) learning",
      "graph-level tasks",
      "long-range dependencies and global interactions",
      "intrinsically interpretable GNNs",
      "constrained graph clustering",
      "self-interpretable graph prototypes",
      "directly learnable graph structures",
      "transparent graph-level reasoning",
      "graph classification",
      "superior interpretability",
      "computational efficiency",
      "safety-critical domains"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/6dc0932670a0b5140a426ca310bbb03783ff2240.pdf",
    "citation_key": "wang2024j6z",
    "metadata": {
      "title": "Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks",
      "authors": [
        "Yuwen Wang",
        "Shunyu Liu",
        "Tongya Zheng",
        "Kaixuan Chen",
        "Mingli Song"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available¹.",
      "file_path": "paper_data/Graph_Neural_Networks/6dc0932670a0b5140a426ca310bbb03783ff2240.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Network (GNN) explanation methods primarily focus on \"subgraph-specific\" (node-wise, local) interpretations, which are insufficient for graph-level tasks that necessitate understanding \"long-range dependencies and global interactions\" across the entire graph \\cite{wang2024j6z}.\n    *   **Importance and Challenge**: This gap between local explanations and the need for global-level insights hinders human trust and limits GNN application in critical domains. Addressing this requires overcoming high computational complexity when modeling global interactions involving numerous nodes, and identifying diverse yet representative global patterns within acceptable overhead \\cite{wang2024j6z}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against both post-hoc (e.g., GNNExplainer, PGExplainer) and intrinsically interpretable (e.g., GIB, GSAT, ProtGNN) GNN explanation methods \\cite{wang2024j6z}.\n    *   **Limitations of Previous Solutions**:\n        *   Post-hoc methods may be \"disloyal to the original model,\" leading to \"distorted attribution analysis\" \\cite{wang2024j6z}.\n        *   Intrinsic methods like ProtGNN can introduce \"explanatory biases\" through extra projection processes \\cite{wang2024j6z}.\n        *   Crucially, all prior methods \"only provide one-side attribution analysis from a localized viewpoint,\" resulting in \"under-representative explanations when higher-order node interactions or global graph structure play a pivotal role\" \\cite{wang2024j6z}. They struggle with the computational demands of extracting global patterns.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Global Interactive Pattern (GIP) learning, an intrinsically interpretable scheme for graph classification that explicitly interprets decisions using learnable global interactive patterns \\cite{wang2024j6z}.\n    *   **Novelty/Difference**: GIP tackles interpretation from a global perspective by first compressing the graph and then identifying inter-cluster interactions in the coarsened graph instances. It consists of two key modules:\n        1.  **Clustering Assignment Module**: Employs a constrained graph clustering module to iteratively aggregate similar components into cluster-level representations. It learns a trainable cluster assignment matrix `S` by minimizing an unsupervised loss term based on a relaxation of the K-way normalized cut, ensuring the clustering reflects real-world graph characteristics \\cite{wang2024j6z}.\n        2.  **Interactive Pattern Matching Module**: Matches the coarsened graph with a batch of \"self-interpretable graph prototypes\" (learnable interactive patterns). Unlike prior work that learns embeddings in hidden space, GIP defines these patterns directly \"in the form of graph structure\" to explicitly reveal vital graph-level patterns. It uses Random Walk Graph Kernels to measure similarity between the coarsened graph and these patterns, which then drives the prediction via a softmax layer \\cite{wang2024j6z}.\n\n*   **Key Technical Contributions**\n    *   **Novel Task Formulation**: Introduction of Global Interactive Pattern (GIP) learning, shifting GNN interpretability from local subgraph explanations to global interactive patterns \\cite{wang2024j6z}.\n    *   **Holistic Framework**: A two-stage framework that achieves both \"high computational efficiency and accurate pattern discovery\" by integrating learnable cluster constraints and graph prototypes for reliable graph-level explanations \\cite{wang2024j6z}.\n    *   **Directly Learnable Global Patterns**: The innovation of defining and learning interactive patterns directly as graph structures, rather than hidden embeddings, enabling transparent graph-level reasoning \\cite{wang2024j6z}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \"Extensive experiments conducted on both synthetic and real-world benchmarks\" \\cite{wang2024j6z}.\n    *   **Key Performance Metrics and Comparison Results**: GIP \"yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts.\" Visualization of explanations further demonstrates its \"superior capability in identifying global interactive patterns\" \\cite{wang2024j6z}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The proposed GIP framework is specifically designed for \"graph classification\" tasks \\cite{wang2024j6z}.\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes that global interactive patterns can be effectively represented and learned through a coarsening process and matching with graph prototypes. While it aims for computational efficiency, the inherent complexity of global pattern discovery remains a challenge that GIP seeks to mitigate rather than eliminate entirely \\cite{wang2024j6z}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GIP significantly advances the technical state-of-the-art by bridging the \"significant gap between local subgraph-specific explanations and global-level explanations\" for GNNs, particularly for graph-level tasks \\cite{wang2024j6z}.\n    *   **Potential Impact**: It enables a \"transparent graph-level reasoning process,\" providing more comprehensive and accurate explanations. This can enhance human trust in GNNs, facilitating their adoption in \"safety-critical domains\" by offering interpretable insights into complex global interactions \\cite{wang2024j6z}.",
      "keywords": [
        "GNN interpretability",
        "Global Interactive Pattern (GIP) learning",
        "graph-level tasks",
        "long-range dependencies and global interactions",
        "intrinsically interpretable GNNs",
        "constrained graph clustering",
        "self-interpretable graph prototypes",
        "directly learnable graph structures",
        "transparent graph-level reasoning",
        "graph classification",
        "superior interpretability",
        "computational efficiency",
        "safety-critical domains"
      ],
      "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   the abstract explicitly states: \"this paper **proposes a novel intrinsically interpretable scheme** for graph classification, termed as global interactive pattern (gip) learning, which **introduces learnable global interactive patterns** to explicitly interpret decisions.\"\n*   it then details the components and steps of this proposed scheme: \"gip first tackles the complexity of interpretation by clustering numerous nodes... then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes...\"\n*   the introduction sets up a technical problem (lack of explainability in gnns for graph-level tasks) and immediately points to the proposed solution.\n*   the mention of \"extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed gip yields significantly superior interpretability\" indicates validation of the *new method*, rather than the primary goal being the empirical study itself.\n*   keywords like \"proposes,\" \"introduces,\" \"scheme,\" \"module,\" and \"process\" strongly align with the criteria for a technical paper presenting new methods, algorithms, or systems."
    },
    "file_name": "6dc0932670a0b5140a426ca310bbb03783ff2240.pdf"
  },
  {
    "success": true,
    "doc_id": "3dc776dbb99407031d0c4949f50952d3",
    "summary": "Here is a focused summary of the technical paper \\cite{ju2023prm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Formally understanding and quantifying the generalization performance (the gap between training and test performance) of Graph Neural Networks (GNNs), especially for deep and overparameterized models and in fine-tuning scenarios.\n    *   **Importance & Challenge**: Generalization is crucial for machine learning model success. For deep models like GNNs, classical complexity measures (e.g., VC dimension) often lead to vacuous bounds that do not explain empirical performance. Prior GNN generalization bounds scale with the graph's maximum degree, `d^(l-1)`, which can be numerically very large for real-world graphs, rendering them uninformative. Fine-tuning large GNNs often leads to overfitting without proper intervention, necessitating a better understanding of generalization.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on PAC-Bayesian analysis and stability properties, similar to prior generalization bounds for fully-connected networks. It extends previous GNN generalization analyses.\n    *   **Limitations of Previous Solutions**:\n        *   Naive application of bounds from feedforward networks to GNNs results in vacuous terms scaling with `n^(l-1)` (number of nodes).\n        *   VC dimension for GNNs scales with `n`, ignoring graph structure.\n        *   Verma and Zhang (2019) provided bounds scaling with the largest singular value of the graph diffusion matrix but only for single-layer GNNs and node prediction tasks.\n        *   Garg et al. (2020) and Liao et al. (2021) analyzed multi-layer message-passing GNNs for graph prediction, but their bounds scaled with `d^(l-1)` (maximum degree), which is often numerically much larger than observed generalization gaps and the spectral norm of diffusion matrices.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs PAC-Bayesian analysis to derive generalization bounds by measuring the stability of GNNs against noise perturbations. The key insight is to quantify this stability using Hessians of the loss function.\n    *   **Novelty/Difference**:\n        *   **Spectral Norm Dependence**: Instead of the maximum degree, the bounds scale with the largest singular value (spectral norm) of the graph diffusion matrix `P_G^(l-1)`. This provides significantly tighter and non-vacuous bounds.\n        *   **Unified Model Analysis**: The analysis applies to a unified GNN model that subsumes various architectures, including Message-Passing Neural Networks (MPNNs), Graph Convolutional Networks (GCNs), and Graph Isomorphism Networks (GINs), without requiring weight tying across layers.\n        *   **Hessian-based Stability Measure**: Introduces a novel approach to measure noise stability via the trace of the loss Hessian matrix, which is shown to correlate accurately with observed generalization gaps. This technique involves a uniform convergence analysis of the Hessian matrix.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Sharp Generalization Bounds (Theorem 3.1)**: Proves PAC-Bayesian generalization bounds for `l`-layer GNNs (MPNN, GCN, GIN) that scale with the spectral norm of `P_G^(l-1)`, where `P_G` is the graph diffusion matrix. These bounds are numerically orders of magnitude smaller than prior `d^(l-1)` bounds.\n        *   **Matching Lower Bound (Theorem 3.2)**: Constructs a lower bound instance where the generalization gap asymptotically matches their upper bound, demonstrating the tightness of their results.\n        *   **Hessian-based Generalization Measure (Lemma 4.3)**: Shows that the trace of the loss Hessian matrix can accurately measure the noise stability and, consequently, the generalization of GNNs. This applies to twice-differentiable and Lipschitz-continuous activations.\n        *   **Hessian Regularization Algorithm**: Proposes an algorithm that performs gradient updates on perturbed weight matrices, effectively minimizing the average loss of multiple perturbed models, which is equivalent to regularizing the GNN's Hessian in expectation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        1.  **Numerical Comparison of Bounds**: Evaluated the numerical values of their spectral norm-based bounds against prior maximum degree-based bounds for GCNs and MPNNs with varying layers.\n        2.  **Correlation of Hessian Measure**: Measured the Hessian-based generalization measure and compared it with empirically observed generalization gaps of GNNs.\n        3.  **Fine-tuning Algorithm Performance**: Applied their Hessian regularization algorithm for fine-tuning pretrained GNNs on graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Bounds Comparison**: On real-world graphs (IMDB-B, IMDB-M, COLLAB, REDDIT-B, REDDIT-M), the spectral norm bounds were found to be orders of magnitude smaller (e.g., `10^3` to `10^27` times smaller) than prior `d^(l-1)` bounds, even when accounting for weight norms (Figure 1a, Figure 2). For GCNs using normalized adjacency matrices, the graph dependence reduces to 1, providing exponential improvement.\n        *   **Hessian Measure Correlation**: The Hessian-based generalization measure accurately correlated with the empirically observed generalization gaps of GNNs (Figure 1b), demonstrating its practical utility as a diagnostic tool.\n        *   **Fine-tuning Performance**: The proposed Hessian regularization algorithm improved test performance on several graph-level classification tasks using Molecular graphs, demonstrating its practical benefit in robust fine-tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis for the Hessian-based measure specifically applies to GNNs with twice-differentiable and Lipschitz-continuous activation functions (e.g., tanh, sigmoid).\n    *   **Scope of Applicability**: Primarily focuses on graph-level prediction tasks, though the authors note that the results permit extension to node prediction tasks. The unified model covers a broad range of common GNN architectures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ju2023prm} significantly advances the theoretical understanding of generalization in GNNs by providing the first sharp, non-vacuous generalization bounds that scale with the spectral norm of the graph diffusion matrix, rather than the often-large maximum degree. This offers a more realistic and tighter quantification of GNN complexity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a practical, Hessian-based tool for measuring GNN generalization, which can guide model design and hyperparameter tuning.\n        *   Inspires the development of robust fine-tuning methods for GNNs by leveraging stability properties, as demonstrated by their proposed algorithm.\n        *   The developed tools and analysis techniques (e.g., uniform convergence of Hessians) may be useful for studying other aspects of GNNs, such as extrapolation to different graph sizes.",
    "intriguing_abstract": "Unraveling the generalization mystery of deep Graph Neural Networks (GNNs) remains a critical challenge, with existing theoretical bounds often proving vacuous due to their dependence on the graph's maximum degree. We present a groundbreaking PAC-Bayesian framework that delivers the first sharp, non-vacuous generalization bounds for multi-layer GNNs, encompassing MPNNs, GCNs, and GINs. Our novel bounds scale with the spectral norm of the graph diffusion matrix, rather than the often-prohibitive maximum degree, yielding improvements by orders of magnitude and offering a truly realistic quantification of GNN complexity.\n\nBeyond theoretical tightness, we introduce a novel Hessian-based measure of GNN stability, demonstrating its accurate correlation with empirical generalization gaps. This provides an invaluable diagnostic tool for understanding and predicting GNN performance. Leveraging this insight, we propose a Hessian regularization algorithm that significantly enhances the robustness and generalization of fine-tuned GNNs on graph classification tasks. This work fundamentally advances our understanding of GNN generalization, offering both profound theoretical insights and practical tools for designing more reliable and performant graph learning models.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "generalization performance",
      "PAC-Bayesian analysis",
      "spectral norm dependence",
      "graph diffusion matrix",
      "Hessian-based stability measure",
      "sharp generalization bounds",
      "Hessian regularization algorithm",
      "GNN fine-tuning",
      "overparameterized GNNs",
      "Message-Passing Neural Networks (MPNNs)",
      "graph-level prediction"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/fcdd4300f937cef11af297329ed4bd2b611871e7.pdf",
    "citation_key": "ju2023prm",
    "metadata": {
      "title": "Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion",
      "authors": [
        "Haotian Ju",
        "Dongyue Li",
        "Aneesh Sharma",
        "Hongyang Zhang"
      ],
      "published_date": "2023",
      "abstract": "Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately. Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/fcdd4300f937cef11af297329ed4bd2b611871e7.pdf",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "citationCount": 0,
      "score": 0,
      "summary": "Here is a focused summary of the technical paper \\cite{ju2023prm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Formally understanding and quantifying the generalization performance (the gap between training and test performance) of Graph Neural Networks (GNNs), especially for deep and overparameterized models and in fine-tuning scenarios.\n    *   **Importance & Challenge**: Generalization is crucial for machine learning model success. For deep models like GNNs, classical complexity measures (e.g., VC dimension) often lead to vacuous bounds that do not explain empirical performance. Prior GNN generalization bounds scale with the graph's maximum degree, `d^(l-1)`, which can be numerically very large for real-world graphs, rendering them uninformative. Fine-tuning large GNNs often leads to overfitting without proper intervention, necessitating a better understanding of generalization.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on PAC-Bayesian analysis and stability properties, similar to prior generalization bounds for fully-connected networks. It extends previous GNN generalization analyses.\n    *   **Limitations of Previous Solutions**:\n        *   Naive application of bounds from feedforward networks to GNNs results in vacuous terms scaling with `n^(l-1)` (number of nodes).\n        *   VC dimension for GNNs scales with `n`, ignoring graph structure.\n        *   Verma and Zhang (2019) provided bounds scaling with the largest singular value of the graph diffusion matrix but only for single-layer GNNs and node prediction tasks.\n        *   Garg et al. (2020) and Liao et al. (2021) analyzed multi-layer message-passing GNNs for graph prediction, but their bounds scaled with `d^(l-1)` (maximum degree), which is often numerically much larger than observed generalization gaps and the spectral norm of diffusion matrices.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs PAC-Bayesian analysis to derive generalization bounds by measuring the stability of GNNs against noise perturbations. The key insight is to quantify this stability using Hessians of the loss function.\n    *   **Novelty/Difference**:\n        *   **Spectral Norm Dependence**: Instead of the maximum degree, the bounds scale with the largest singular value (spectral norm) of the graph diffusion matrix `P_G^(l-1)`. This provides significantly tighter and non-vacuous bounds.\n        *   **Unified Model Analysis**: The analysis applies to a unified GNN model that subsumes various architectures, including Message-Passing Neural Networks (MPNNs), Graph Convolutional Networks (GCNs), and Graph Isomorphism Networks (GINs), without requiring weight tying across layers.\n        *   **Hessian-based Stability Measure**: Introduces a novel approach to measure noise stability via the trace of the loss Hessian matrix, which is shown to correlate accurately with observed generalization gaps. This technique involves a uniform convergence analysis of the Hessian matrix.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Sharp Generalization Bounds (Theorem 3.1)**: Proves PAC-Bayesian generalization bounds for `l`-layer GNNs (MPNN, GCN, GIN) that scale with the spectral norm of `P_G^(l-1)`, where `P_G` is the graph diffusion matrix. These bounds are numerically orders of magnitude smaller than prior `d^(l-1)` bounds.\n        *   **Matching Lower Bound (Theorem 3.2)**: Constructs a lower bound instance where the generalization gap asymptotically matches their upper bound, demonstrating the tightness of their results.\n        *   **Hessian-based Generalization Measure (Lemma 4.3)**: Shows that the trace of the loss Hessian matrix can accurately measure the noise stability and, consequently, the generalization of GNNs. This applies to twice-differentiable and Lipschitz-continuous activations.\n        *   **Hessian Regularization Algorithm**: Proposes an algorithm that performs gradient updates on perturbed weight matrices, effectively minimizing the average loss of multiple perturbed models, which is equivalent to regularizing the GNN's Hessian in expectation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        1.  **Numerical Comparison of Bounds**: Evaluated the numerical values of their spectral norm-based bounds against prior maximum degree-based bounds for GCNs and MPNNs with varying layers.\n        2.  **Correlation of Hessian Measure**: Measured the Hessian-based generalization measure and compared it with empirically observed generalization gaps of GNNs.\n        3.  **Fine-tuning Algorithm Performance**: Applied their Hessian regularization algorithm for fine-tuning pretrained GNNs on graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Bounds Comparison**: On real-world graphs (IMDB-B, IMDB-M, COLLAB, REDDIT-B, REDDIT-M), the spectral norm bounds were found to be orders of magnitude smaller (e.g., `10^3` to `10^27` times smaller) than prior `d^(l-1)` bounds, even when accounting for weight norms (Figure 1a, Figure 2). For GCNs using normalized adjacency matrices, the graph dependence reduces to 1, providing exponential improvement.\n        *   **Hessian Measure Correlation**: The Hessian-based generalization measure accurately correlated with the empirically observed generalization gaps of GNNs (Figure 1b), demonstrating its practical utility as a diagnostic tool.\n        *   **Fine-tuning Performance**: The proposed Hessian regularization algorithm improved test performance on several graph-level classification tasks using Molecular graphs, demonstrating its practical benefit in robust fine-tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis for the Hessian-based measure specifically applies to GNNs with twice-differentiable and Lipschitz-continuous activation functions (e.g., tanh, sigmoid).\n    *   **Scope of Applicability**: Primarily focuses on graph-level prediction tasks, though the authors note that the results permit extension to node prediction tasks. The unified model covers a broad range of common GNN architectures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ju2023prm} significantly advances the theoretical understanding of generalization in GNNs by providing the first sharp, non-vacuous generalization bounds that scale with the spectral norm of the graph diffusion matrix, rather than the often-large maximum degree. This offers a more realistic and tighter quantification of GNN complexity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a practical, Hessian-based tool for measuring GNN generalization, which can guide model design and hyperparameter tuning.\n        *   Inspires the development of robust fine-tuning methods for GNNs by leveraging stability properties, as demonstrated by their proposed algorithm.\n        *   The developed tools and analysis techniques (e.g., uniform convergence of Hessians) may be useful for studying other aspects of GNNs, such as extrapolation to different graph sizes.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "generalization performance",
        "PAC-Bayesian analysis",
        "spectral norm dependence",
        "graph diffusion matrix",
        "Hessian-based stability measure",
        "sharp generalization bounds",
        "Hessian regularization algorithm",
        "GNN fine-tuning",
        "overparameterized GNNs",
        "Message-Passing Neural Networks (MPNNs)",
        "graph-level prediction"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **theoretical** paper.\n\nhere's why:\n\n*   **abstract keywords:** \"present generalization bounds\", \"construct a lower bound\", \"analyze a unified model\", \"measure the stability... using hessians\". these phrases strongly indicate mathematical analysis, formal models, and the development of theoretical frameworks (generalization bounds). the title itself, \"improved pac-bayesian bounds,\" directly points to mathematical bounds.\n*   **introduction keywords:** \"formally understand generalization\", \"quantified via complexity notions\", \"formally explaining the empirical generalization performance\". this emphasizes the paper's goal of providing a mathematical and formal understanding of a phenomenon.\n*   **core contribution:** the paper's main contribution is the development of new generalization bounds, which are mathematical statements about the performance of gnns.\n*   **supporting evidence:** while the abstract mentions empirical findings (\"empirically, we find that hessian-based measurements correlate...\", \"optimizing noise stability... improves test performance\"), these appear to be validations or applications of their theoretical framework, rather than the primary focus of the paper being an empirical study itself. the core is the derivation and analysis of the bounds."
    },
    "file_name": "fcdd4300f937cef11af297329ed4bd2b611871e7.pdf"
  },
  {
    "success": true,
    "doc_id": "a4839acae09f628424f9596c32bdfb0d",
    "summary": "This paper, `A Review of Graph Neural Networks in Epidemic Modeling` by Liu et al. \\cite{liu20242g6}, provides a comprehensive survey of Graph Neural Networks (GNNs) in the context of epidemic modeling.\n\nHere's a focused summary for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional mechanistic epidemic models suffer from oversimplified or fixed assumptions, leading to sub-optimal predictive power and an inability to efficiently capture complex relational information. Existing data-driven approaches like CNNs and RNNs also fall short in incorporating crucial relational data (e.g., human mobility, geographic connections, contact tracing) essential for accurate epidemic forecasting.\n    *   **Importance & Challenge**: Accurate and timely epidemic modeling is critical for public health decision-making, resource allocation, and effective intervention strategies. The challenge lies in developing models that can effectively integrate and leverage the complex, dynamic relational data inherent in disease transmission networks to provide more precise and generalizable predictions.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Mechanistic Models**: Such as SIR, SEIR, and SIRD, mathematically describe disease transmission but are limited by their reliance on fixed and often oversimplified assumptions.\n        *   **Data-driven Models (CNNs, RNNs)**: While successful in some epidemiological predictive tasks (e.g., forecasting case counts), they often lack the capability to incorporate relational information effectively.\n    *   **Limitations of Previous Solutions**: Both mechanistic and general deep learning models struggle to capture the intricate relational dynamics crucial for understanding and predicting disease spread, leading to biases and compromised accuracy.\n    *   **Positioning of this Work**: This paper \\cite{liu20242g6} distinguishes itself as a *comprehensive and pioneering review* specifically focused on the application of GNNs in epidemic modeling. Unlike prior surveys that are often narrow in scope (e.g., specific viruses, single tasks, or general deep learning without deep GNN integration), this work offers a broader and more detailed overview of GNN-based approaches across a spectrum of epidemic tasks, aiming to bridge existing literature gaps.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic and exhaustive review of GNNs in epidemic modeling. This involves:\n        *   Developing hierarchical taxonomies for both epidemic tasks (categorized into Detection, Surveillance, Prediction, and Projection) and GNN methodologies (categorized into Neural Models and Hybrid Models).\n        *   Providing detailed explanations and definitions for each task category.\n        *   Systematically examining existing GNN-based methodologies, including their technical details, data resources, and graph construction techniques.\n    *   **Novelty**: The innovation lies in providing the *first comprehensive and structured review* dedicated solely to GNNs in epidemiology. It offers novel taxonomies for organizing the field, a meticulous examination of existing methods, and a systematic identification of limitations and future research directions, thereby fostering interdisciplinary synergy.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Taxonomies**: Introduction of hierarchical taxonomies for epidemic tasks (Detection, Surveillance, Prediction, Projection) and GNN methodologies (Neural Models, Hybrid Models), providing a structured framework for understanding the field.\n    *   **Systematic Review Framework**: Establishment of a comprehensive framework for analyzing and categorizing GNN applications, encompassing task objectives, data types, graph construction techniques, and technical details of various GNN models.\n    *   **Identification of Limitations and Future Directions**: Systematically discusses the limitations of current GNN methods in epidemiology and proposes concrete, prospective research directions to guide future advancements.\n    *   **Resource Compilation**: Provides a curated list of relevant papers (via a GitHub repository) to serve as a valuable resource for researchers in this interdisciplinary domain.\n\n*   **5. Experimental Validation**\n    *   As a review paper, \\cite{liu20242g6} does not present new experimental results. Instead, its \"validation\" comes from the comprehensive synthesis of empirical evidence reported in the *reviewed literature*. The paper highlights that GNNs have demonstrated significant success in various epidemiological tasks, such as infection prediction, outbreak source detection, and intervention modeling, by effectively capturing relational dynamics and yielding more precise predictions compared to traditional methods. The thoroughness and systematic organization of the reviewed works implicitly validate the claims made about the utility and potential of GNNs in this field.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations (of reviewed methods, as identified by the paper)**: The paper points out that traditional mechanistic models suffer from oversimplified assumptions, while general deep learning models (CNNs, RNNs) often fail to incorporate crucial relational information. While GNNs offer advantages, the paper's unprovided Section 5 is dedicated to discussing the limitations of existing GNN methods from diverse perspectives.\n    *   **Scope of Applicability**: The review focuses exclusively on the application of Graph Neural Networks within the domain of *epidemic modeling*. It covers a broad range of epidemic tasks and GNN methodologies, aiming for a comprehensive overview within this specific interdisciplinary intersection.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper \\cite{liu20242g6} significantly advances the technical state-of-the-art by providing the first comprehensive and structured overview of GNN applications in epidemiology. It consolidates fragmented knowledge, clarifies the landscape of existing methods, and underscores the unique advantages of GNNs in handling complex relational data for epidemic tasks.\n    *   **Potential Impact**:\n        *   **Guidance for Researchers**: Offers a clear roadmap and foundational understanding for researchers entering or working in this interdisciplinary field, facilitating model selection and task comprehension.\n        *   **Promotion of Interdisciplinary Synergy**: Aims to bridge the GNN and epidemiology communities, fostering collaborative research and accelerating advancements in both fields.\n        *   **Identification of Future Research**: Systematically outlines promising future research directions, stimulating innovation and guiding efforts to address current limitations in GNN-based epidemic modeling.",
    "intriguing_abstract": "The relentless challenge of epidemic forecasting demands models capable of deciphering intricate relational dynamics—from human mobility to contact networks—a critical gap traditional mechanistic and general deep learning approaches often fail to bridge. This paper presents the *first comprehensive and pioneering review* of **Graph Neural Networks (GNNs)** in **epidemic modeling**, illuminating their transformative potential.\n\nWe systematically survey the burgeoning landscape of GNN applications, introducing novel hierarchical **taxonomies** for **epidemic tasks** (categorized into **Detection, Surveillance, Prediction, and Projection**) and **GNN methodologies** (**Neural Models** and **Hybrid Models**). Our framework meticulously examines existing GNN-based approaches, their technical nuances, **data resources**, and **graph construction techniques**. By consolidating fragmented knowledge and identifying critical limitations, this review provides an indispensable roadmap for researchers. It underscores how GNNs, by effectively leveraging complex **relational data**, can significantly enhance **predictive power** and generalizability, thereby informing more precise public health interventions and resource allocation. This work bridges interdisciplinary gaps, fostering innovation and charting crucial future research directions in this vital domain.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "epidemic modeling",
      "relational data integration",
      "disease transmission networks",
      "mechanistic model limitations",
      "data-driven model limitations (CNNs",
      "RNNs)",
      "systematic review framework",
      "hierarchical taxonomies",
      "epidemic forecasting",
      "outbreak detection",
      "graph construction techniques",
      "future research directions"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf",
    "citation_key": "liu20242g6",
    "metadata": {
      "title": "A Review of Graph Neural Networks in Epidemic Modeling",
      "authors": [
        "Zewen Liu",
        "Guancheng Wan",
        "B. A. Prakash",
        "M. S. Lau",
        "Wei Jin"
      ],
      "published_date": "2024",
      "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
      "file_path": "paper_data/Graph_Neural_Networks/9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, `A Review of Graph Neural Networks in Epidemic Modeling` by Liu et al. \\cite{liu20242g6}, provides a comprehensive survey of Graph Neural Networks (GNNs) in the context of epidemic modeling.\n\nHere's a focused summary for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional mechanistic epidemic models suffer from oversimplified or fixed assumptions, leading to sub-optimal predictive power and an inability to efficiently capture complex relational information. Existing data-driven approaches like CNNs and RNNs also fall short in incorporating crucial relational data (e.g., human mobility, geographic connections, contact tracing) essential for accurate epidemic forecasting.\n    *   **Importance & Challenge**: Accurate and timely epidemic modeling is critical for public health decision-making, resource allocation, and effective intervention strategies. The challenge lies in developing models that can effectively integrate and leverage the complex, dynamic relational data inherent in disease transmission networks to provide more precise and generalizable predictions.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Mechanistic Models**: Such as SIR, SEIR, and SIRD, mathematically describe disease transmission but are limited by their reliance on fixed and often oversimplified assumptions.\n        *   **Data-driven Models (CNNs, RNNs)**: While successful in some epidemiological predictive tasks (e.g., forecasting case counts), they often lack the capability to incorporate relational information effectively.\n    *   **Limitations of Previous Solutions**: Both mechanistic and general deep learning models struggle to capture the intricate relational dynamics crucial for understanding and predicting disease spread, leading to biases and compromised accuracy.\n    *   **Positioning of this Work**: This paper \\cite{liu20242g6} distinguishes itself as a *comprehensive and pioneering review* specifically focused on the application of GNNs in epidemic modeling. Unlike prior surveys that are often narrow in scope (e.g., specific viruses, single tasks, or general deep learning without deep GNN integration), this work offers a broader and more detailed overview of GNN-based approaches across a spectrum of epidemic tasks, aiming to bridge existing literature gaps.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic and exhaustive review of GNNs in epidemic modeling. This involves:\n        *   Developing hierarchical taxonomies for both epidemic tasks (categorized into Detection, Surveillance, Prediction, and Projection) and GNN methodologies (categorized into Neural Models and Hybrid Models).\n        *   Providing detailed explanations and definitions for each task category.\n        *   Systematically examining existing GNN-based methodologies, including their technical details, data resources, and graph construction techniques.\n    *   **Novelty**: The innovation lies in providing the *first comprehensive and structured review* dedicated solely to GNNs in epidemiology. It offers novel taxonomies for organizing the field, a meticulous examination of existing methods, and a systematic identification of limitations and future research directions, thereby fostering interdisciplinary synergy.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Taxonomies**: Introduction of hierarchical taxonomies for epidemic tasks (Detection, Surveillance, Prediction, Projection) and GNN methodologies (Neural Models, Hybrid Models), providing a structured framework for understanding the field.\n    *   **Systematic Review Framework**: Establishment of a comprehensive framework for analyzing and categorizing GNN applications, encompassing task objectives, data types, graph construction techniques, and technical details of various GNN models.\n    *   **Identification of Limitations and Future Directions**: Systematically discusses the limitations of current GNN methods in epidemiology and proposes concrete, prospective research directions to guide future advancements.\n    *   **Resource Compilation**: Provides a curated list of relevant papers (via a GitHub repository) to serve as a valuable resource for researchers in this interdisciplinary domain.\n\n*   **5. Experimental Validation**\n    *   As a review paper, \\cite{liu20242g6} does not present new experimental results. Instead, its \"validation\" comes from the comprehensive synthesis of empirical evidence reported in the *reviewed literature*. The paper highlights that GNNs have demonstrated significant success in various epidemiological tasks, such as infection prediction, outbreak source detection, and intervention modeling, by effectively capturing relational dynamics and yielding more precise predictions compared to traditional methods. The thoroughness and systematic organization of the reviewed works implicitly validate the claims made about the utility and potential of GNNs in this field.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations (of reviewed methods, as identified by the paper)**: The paper points out that traditional mechanistic models suffer from oversimplified assumptions, while general deep learning models (CNNs, RNNs) often fail to incorporate crucial relational information. While GNNs offer advantages, the paper's unprovided Section 5 is dedicated to discussing the limitations of existing GNN methods from diverse perspectives.\n    *   **Scope of Applicability**: The review focuses exclusively on the application of Graph Neural Networks within the domain of *epidemic modeling*. It covers a broad range of epidemic tasks and GNN methodologies, aiming for a comprehensive overview within this specific interdisciplinary intersection.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper \\cite{liu20242g6} significantly advances the technical state-of-the-art by providing the first comprehensive and structured overview of GNN applications in epidemiology. It consolidates fragmented knowledge, clarifies the landscape of existing methods, and underscores the unique advantages of GNNs in handling complex relational data for epidemic tasks.\n    *   **Potential Impact**:\n        *   **Guidance for Researchers**: Offers a clear roadmap and foundational understanding for researchers entering or working in this interdisciplinary field, facilitating model selection and task comprehension.\n        *   **Promotion of Interdisciplinary Synergy**: Aims to bridge the GNN and epidemiology communities, fostering collaborative research and accelerating advancements in both fields.\n        *   **Identification of Future Research**: Systematically outlines promising future research directions, stimulating innovation and guiding efforts to address current limitations in GNN-based epidemic modeling.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "epidemic modeling",
        "relational data integration",
        "disease transmission networks",
        "mechanistic model limitations",
        "data-driven model limitations (CNNs",
        "RNNs)",
        "systematic review framework",
        "hierarchical taxonomies",
        "epidemic forecasting",
        "outbreak detection",
        "graph construction techniques",
        "future research directions"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"a review of graph neural networks in epidemic modeling\" explicitly uses the word \"review.\"\n*   **abstract:**\n    *   \"we endeavor to furnish a **comprehensive review** of gnns in epidemic tasks\"\n    *   \"we introduce **hierarchical taxonomies** for both epidemic tasks and methodologies\"\n    *   \"we **categorize existing work** into neural models and hybrid models\"\n    *   \"we perform an **exhaustive and systematic examination** of the methodologies\"\n    *   \"this **survey aims to bridge literature gaps** and promote the progression of this promising field\"\n    *   it discusses limitations of existing methods and proposes future research directions, which are common components of survey papers.\n*   **introduction:** sets the context for the field and the need for modeling, leading into the review of gnns.\n\nthese points directly align with the criteria for a **survey** paper: \"reviews existing literature comprehensively,\" \"abstract mentions: 'survey', 'review', 'comprehensive analysis', 'state-of-the-art',\" and \"introduction discusses: literature organization, classification schemes.\""
    },
    "file_name": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf"
  },
  {
    "success": true,
    "doc_id": "b523827c2bda4158ccf5c450459d16f2",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{zhang2018kdl}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Addressing the limitations of predefined heuristic methods for link prediction in network-structured data. Existing heuristics (e.g., common neighbors, Katz index) make strong assumptions about link formation, which often fail in diverse real-world networks (e.g., protein-protein interaction networks).\n*   **Importance & Challenge**: Link prediction is crucial for applications like friend recommendation, knowledge graph completion, and metabolic network reconstruction. The challenge lies in automatically learning a suitable \"heuristic\" from a given network that adapts to its specific link formation patterns, rather than relying on fixed, potentially inaccurate assumptions. Previous attempts (like WLNM \\cite{zhang2018kdl}) struggled with effectively learning high-order features from local subgraphs without requiring computationally expensive large `h`-hop subgraphs (approaching the entire network).\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   **Heuristic Methods**: The paper positions itself as an improvement over traditional heuristic methods (e.g., Common Neighbors, Adamic-Adar, Katz, PageRank, SimRank) by learning general graph structure features instead of using predefined ones.\n    *   **Latent Feature Methods**: Acknowledges and incorporates latent features (e.g., from DeepWalk, node2vec, LINE) and explicit node attributes, but aims to learn structural features directly from the graph.\n    *   **Supervised Heuristic Learning**: Builds upon the paradigm of learning heuristics.\n*   **Limitations of Previous Solutions**:\n    *   **Predefined Heuristics**: Strong, often incorrect assumptions about link existence.\n    *   **WLNM \\cite{zhang2018kdl} (closest prior work)**:\n        *   Uses fully-connected neural networks on fixed-size adjacency matrices, requiring truncation of subgraphs and potentially losing structural information.\n        *   Cannot effectively incorporate latent or explicit node features.\n        *   Lacked theoretical justification for learning high-order heuristics from local subgraphs.\n    *   **Other Supervised Methods**: Still rely on combinations of *predefined* heuristics, rather than learning *general* graph structure features from scratch.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The SEAL (Subgraph Embedding for Link prediction) framework learns general graph structure features for link prediction from local enclosing subgraphs using a Graph Neural Network (GNN).\n*   **Novelty/Difference**:\n    *   **`$\\beta$-decaying Heuristic Theory`**: A novel theoretical contribution proving that a wide range of high-order heuristics (including Katz, rooted PageRank, and SimRank) can be unified under a `$\\beta$-decaying` framework. Crucially, it demonstrates that these heuristics can be accurately approximated from small `h`-hop enclosing subgraphs, with approximation error decreasing exponentially with `h`. This resolves the perceived need for large `h` to capture high-order information.\n    *   **SEAL Framework**:\n        *   **GNN-based Learning**: Replaces the fully-connected neural network of WLNM \\cite{zhang2018kdl} with a GNN, which is inherently better suited for learning from variable-sized graph structures and extracting local substructure features.\n        *   **Comprehensive Node Information Matrix**: Constructs a rich node feature matrix for the GNN input, comprising:\n            *   **Structural Node Labels**: A novel labeling scheme that assigns integer labels to nodes within the enclosing subgraph based on their relative positions and roles concerning the target nodes `x` and `y`. This helps the GNN understand the context of each node.\n            *   **Node Embeddings**: Incorporates latent features from network embedding methods.\n            *   **Node Attributes**: Integrates explicit side information.\n        *   **Enclosing Subgraph Extraction**: Extracts `h`-hop enclosing subgraphs around target link candidates, providing localized structural context.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms, Methods, or Techniques**:\n    *   The `$\\beta$-decaying heuristic theory`, which unifies a broad class of link prediction heuristics and provides theoretical justification for learning from local subgraphs.\n    *   Proof that high-order heuristics like Katz, PageRank, and SimRank are `$\\beta$-decaying and can be effectively approximated from `h`-hop enclosing subgraphs.\n    *   The SEAL framework, a GNN-based approach for link prediction that learns general graph structure features.\n    *   A novel structural node labeling scheme for nodes within enclosing subgraphs, enabling GNNs to understand node roles relative to the target link.\n*   **System Design or Architectural Innovations**:\n    *   Integration of GNNs with local enclosing subgraphs for link prediction, allowing for variable-sized inputs and effective feature learning.\n    *   A flexible node information matrix construction that combines structural labels, latent embeddings, and explicit attributes.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**: SEAL was extensively evaluated against a wide range of baseline methods on multiple datasets.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The abstract states that SEAL achieved \"unprecedented performance, working consistently well on a wide range of problems.\"\n    *   It \"outperforms all heuristic methods, latent feature methods, and recent network embedding methods by large margins.\"\n    *   Crucially, SEAL \"also outperforms the previous state-of-the-art method, WLNM \\cite{zhang2018kdl}.\"\n    *   Empirically verified \"much improved performance\" over WLNM.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The `$\\beta$-decaying theory` relies on specific properties of the function `f(x,y,l)` and the decaying factor `$\\beta$`. While shown to hold for many common heuristics, it might not encompass all possible link formation mechanisms.\n    *   The effectiveness of the `h`-hop enclosing subgraph approach depends on the assumption that remote parts of the network contribute exponentially less to link existence.\n*   **Scope of Applicability**: Primarily focused on undirected graphs. The computational cost of extracting subgraphs, while mitigated by the theory suggesting small `h`, could still be a factor for extremely large or dense networks if `h` needs to be larger for certain problems.\n\n### 7. Technical Significance\n\n*   **Advancement of Technical State-of-the-Art**:\n    *   Provides a robust theoretical foundation for learning link prediction heuristics from local subgraphs, challenging the conventional wisdom that high-order heuristics require global network information.\n    *   Demonstrates the superior capability of GNNs for learning complex graph structure features in the context of link prediction, surpassing traditional heuristics and prior neural network approaches.\n    *   Introduces a principled way to combine structural, latent, and explicit features for enhanced link prediction performance.\n*   **Potential Impact on Future Research**:\n    *   Encourages further development of GNN architectures specifically designed for local subgraph analysis in various graph learning tasks.\n    *   Opens new avenues for analyzing and designing link prediction heuristics based on the `$\\beta$-decaying` framework.\n    *   Promotes the use of structural node labeling as a powerful technique to inject domain-specific structural information into GNNs.\n    *   Could inspire similar local-to-global learning paradigms in other graph-based prediction problems.",
    "intriguing_abstract": "Challenging the conventional wisdom that high-order link prediction heuristics necessitate global network information, we introduce a groundbreaking `$\\beta$-decaying heuristic theory`. This theory rigorously proves that a broad class of high-order heuristics, including Katz and PageRank, can be accurately approximated from surprisingly small local enclosing subgraphs, with approximation error decreasing exponentially. This theoretical leap underpins SEAL (Subgraph Embedding for Link prediction), a novel Graph Neural Network (GNN) framework. SEAL leverages GNNs to learn general graph structure features directly from these localized contexts, integrating a novel structural node labeling scheme with node embeddings and explicit attributes. This allows GNNs to discern intricate relational patterns crucial for link formation. Our extensive experiments demonstrate SEAL's unprecedented performance, consistently outperforming all traditional heuristic methods, latent feature approaches, and the prior state-of-the-art. This work not only provides a robust theoretical foundation for learning adaptive link prediction heuristics but also sets a new benchmark, paving the way for more accurate and interpretable predictions in diverse network-structured data.",
    "keywords": [
      "Link prediction",
      "Graph Neural Networks (GNNs)",
      "$\\beta$-decaying heuristic theory",
      "SEAL framework",
      "Local enclosing subgraphs",
      "High-order heuristics",
      "Structural node labeling",
      "Network-structured data",
      "Supervised heuristic learning",
      "Node embeddings",
      "Knowledge graph completion",
      "Theoretical justification",
      "Predefined heuristic limitations"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf",
    "citation_key": "zhang2018kdl",
    "metadata": {
      "title": "Link Prediction Based on Graph Neural Networks",
      "authors": [
        "Muhan Zhang",
        "Yixin Chen"
      ],
      "published_date": "2018",
      "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",
      "file_path": "paper_data/Graph_Neural_Networks/e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{zhang2018kdl}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Addressing the limitations of predefined heuristic methods for link prediction in network-structured data. Existing heuristics (e.g., common neighbors, Katz index) make strong assumptions about link formation, which often fail in diverse real-world networks (e.g., protein-protein interaction networks).\n*   **Importance & Challenge**: Link prediction is crucial for applications like friend recommendation, knowledge graph completion, and metabolic network reconstruction. The challenge lies in automatically learning a suitable \"heuristic\" from a given network that adapts to its specific link formation patterns, rather than relying on fixed, potentially inaccurate assumptions. Previous attempts (like WLNM \\cite{zhang2018kdl}) struggled with effectively learning high-order features from local subgraphs without requiring computationally expensive large `h`-hop subgraphs (approaching the entire network).\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   **Heuristic Methods**: The paper positions itself as an improvement over traditional heuristic methods (e.g., Common Neighbors, Adamic-Adar, Katz, PageRank, SimRank) by learning general graph structure features instead of using predefined ones.\n    *   **Latent Feature Methods**: Acknowledges and incorporates latent features (e.g., from DeepWalk, node2vec, LINE) and explicit node attributes, but aims to learn structural features directly from the graph.\n    *   **Supervised Heuristic Learning**: Builds upon the paradigm of learning heuristics.\n*   **Limitations of Previous Solutions**:\n    *   **Predefined Heuristics**: Strong, often incorrect assumptions about link existence.\n    *   **WLNM \\cite{zhang2018kdl} (closest prior work)**:\n        *   Uses fully-connected neural networks on fixed-size adjacency matrices, requiring truncation of subgraphs and potentially losing structural information.\n        *   Cannot effectively incorporate latent or explicit node features.\n        *   Lacked theoretical justification for learning high-order heuristics from local subgraphs.\n    *   **Other Supervised Methods**: Still rely on combinations of *predefined* heuristics, rather than learning *general* graph structure features from scratch.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The SEAL (Subgraph Embedding for Link prediction) framework learns general graph structure features for link prediction from local enclosing subgraphs using a Graph Neural Network (GNN).\n*   **Novelty/Difference**:\n    *   **`$\\beta$-decaying Heuristic Theory`**: A novel theoretical contribution proving that a wide range of high-order heuristics (including Katz, rooted PageRank, and SimRank) can be unified under a `$\\beta$-decaying` framework. Crucially, it demonstrates that these heuristics can be accurately approximated from small `h`-hop enclosing subgraphs, with approximation error decreasing exponentially with `h`. This resolves the perceived need for large `h` to capture high-order information.\n    *   **SEAL Framework**:\n        *   **GNN-based Learning**: Replaces the fully-connected neural network of WLNM \\cite{zhang2018kdl} with a GNN, which is inherently better suited for learning from variable-sized graph structures and extracting local substructure features.\n        *   **Comprehensive Node Information Matrix**: Constructs a rich node feature matrix for the GNN input, comprising:\n            *   **Structural Node Labels**: A novel labeling scheme that assigns integer labels to nodes within the enclosing subgraph based on their relative positions and roles concerning the target nodes `x` and `y`. This helps the GNN understand the context of each node.\n            *   **Node Embeddings**: Incorporates latent features from network embedding methods.\n            *   **Node Attributes**: Integrates explicit side information.\n        *   **Enclosing Subgraph Extraction**: Extracts `h`-hop enclosing subgraphs around target link candidates, providing localized structural context.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms, Methods, or Techniques**:\n    *   The `$\\beta$-decaying heuristic theory`, which unifies a broad class of link prediction heuristics and provides theoretical justification for learning from local subgraphs.\n    *   Proof that high-order heuristics like Katz, PageRank, and SimRank are `$\\beta$-decaying and can be effectively approximated from `h`-hop enclosing subgraphs.\n    *   The SEAL framework, a GNN-based approach for link prediction that learns general graph structure features.\n    *   A novel structural node labeling scheme for nodes within enclosing subgraphs, enabling GNNs to understand node roles relative to the target link.\n*   **System Design or Architectural Innovations**:\n    *   Integration of GNNs with local enclosing subgraphs for link prediction, allowing for variable-sized inputs and effective feature learning.\n    *   A flexible node information matrix construction that combines structural labels, latent embeddings, and explicit attributes.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**: SEAL was extensively evaluated against a wide range of baseline methods on multiple datasets.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The abstract states that SEAL achieved \"unprecedented performance, working consistently well on a wide range of problems.\"\n    *   It \"outperforms all heuristic methods, latent feature methods, and recent network embedding methods by large margins.\"\n    *   Crucially, SEAL \"also outperforms the previous state-of-the-art method, WLNM \\cite{zhang2018kdl}.\"\n    *   Empirically verified \"much improved performance\" over WLNM.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The `$\\beta$-decaying theory` relies on specific properties of the function `f(x,y,l)` and the decaying factor `$\\beta$`. While shown to hold for many common heuristics, it might not encompass all possible link formation mechanisms.\n    *   The effectiveness of the `h`-hop enclosing subgraph approach depends on the assumption that remote parts of the network contribute exponentially less to link existence.\n*   **Scope of Applicability**: Primarily focused on undirected graphs. The computational cost of extracting subgraphs, while mitigated by the theory suggesting small `h`, could still be a factor for extremely large or dense networks if `h` needs to be larger for certain problems.\n\n### 7. Technical Significance\n\n*   **Advancement of Technical State-of-the-Art**:\n    *   Provides a robust theoretical foundation for learning link prediction heuristics from local subgraphs, challenging the conventional wisdom that high-order heuristics require global network information.\n    *   Demonstrates the superior capability of GNNs for learning complex graph structure features in the context of link prediction, surpassing traditional heuristics and prior neural network approaches.\n    *   Introduces a principled way to combine structural, latent, and explicit features for enhanced link prediction performance.\n*   **Potential Impact on Future Research**:\n    *   Encourages further development of GNN architectures specifically designed for local subgraph analysis in various graph learning tasks.\n    *   Opens new avenues for analyzing and designing link prediction heuristics based on the `$\\beta$-decaying` framework.\n    *   Promotes the use of structural node labeling as a powerful technique to inject domain-specific structural information into GNNs.\n    *   Could inspire similar local-to-global learning paradigms in other graph-based prediction problems.",
      "keywords": [
        "Link prediction",
        "Graph Neural Networks (GNNs)",
        "$\\beta$-decaying heuristic theory",
        "SEAL framework",
        "Local enclosing subgraphs",
        "High-order heuristics",
        "Structural node labeling",
        "Network-structured data",
        "Supervised heuristic learning",
        "Node embeddings",
        "Knowledge graph completion",
        "Theoretical justification",
        "Predefined heuristic limitations"
      ],
      "paper_type": "based on the abstract and introduction provided, this paper can be classified as **theoretical**.\n\nhere's why:\n\n1.  **explicit mention of theory and proofs:** the abstract explicitly states: \"first, we develop a novel -decaying heuristic theory. the theory uniﬁes a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs.\" the keywords \"develop theory,\" \"uniﬁes,\" and \"proves\" are direct indicators of a theoretical contribution, aligning perfectly with the \"theoretical\" classification criteria: \"mathematical analysis, proofs, formal models.\"\n\n2.  **foundational contribution:** the theory is presented as the *first* and foundational contribution, upon which the subsequent technical method is built (\"second, based on the -decaying theory, we propose a new method...\"). this suggests that the theoretical framework is a primary and significant intellectual advancement of the paper.\n\nwhile the paper also proposes a \"new method to learn heuristics from local subgraphs using a graph neural network (gnn)\" (technical contribution) and presents \"experimental results\" (empirical contribution), the explicit and prominent development of a \"novel theory\" and \"proofs\" as a distinct and primary contribution makes \"theoretical\" the most fitting overarching classification. the technical method and empirical evaluation serve to demonstrate and validate this theoretical understanding."
    },
    "file_name": "e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf"
  },
  {
    "success": true,
    "doc_id": "a62574538e2e1524e67de75e930dfcd2",
    "summary": "Here's a focused summary of the paper \"Graph Neural Networks with Convolutional ARMA Filters\" \\cite{bianchi20194ea} for a literature review:\n\n---\n\n### Analysis of \"Graph Neural Networks with Convolutional ARMA Filters\" \\cite{bianchi20194ea}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) primarily rely on convolutional layers based on polynomial spectral filters. These filters have limitations in their frequency response, robustness to noise, and ability to capture global graph structures.\n    *   **Importance and Challenge**:\n        *   Polynomial filters offer limited modeling capabilities and cannot model sharp changes in frequency response.\n        *   To capture higher-order neighborhoods and global structures, high-degree polynomials are required, which are computationally expensive, prone to overfitting, and sensitive to noise or small changes in graph topology.\n        *   Spectral GNNs (e.g., Bruna et al., 2013) are computationally intensive due to eigendecomposition and are not transferable to graphs with different topologies (inductive inference).\n        *   Simplified polynomial filters like GCNs (Kipf & Welling, 2016a) suffer from over-smoothing, leading to a loss of initial node features after multiple layers.\n        *   The challenge is to design a graph convolutional layer that offers a more flexible frequency response, is robust, captures global dependencies efficiently, and is transferable across different graph structures, without incurring high computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Spectral GNNs (Bruna et al., 2013; Henaff et al., 2015)**: Implement convolutions in the spectral domain using non-linear trainable filters.\n        *   **Polynomial Filters (Defferrard et al., 2016; Kipf & Welling, 2016a)**: Approximate spectral filters with low-order polynomials (e.g., Chebyshev polynomials, GCN) to avoid expensive eigendecomposition and achieve localization in the node space.\n    *   **Limitations of Previous Solutions**:\n        *   **Spectral GNNs**: Computationally expensive (eigendecomposition, dense matrix operations), not localized in node space (account for whole graph), and not transferable to graphs with unseen topologies.\n        *   **Polynomial Filters**:\n            *   Limited modeling capabilities and inability to model sharp changes in frequency response due to their smoothness.\n            *   High-degree polynomials, necessary for capturing larger graph structures, lead to high computational cost, overfitting, and sensitivity to graph changes.\n            *   GCNs, while efficient, perform Laplacian smoothing that can lead to over-smoothing and loss of initial node features after stacking multiple layers.\n    *   **Positioning**: This work proposes a novel graph convolutional layer based on Auto-Regressive Moving Average (ARMA) filters, which are known in signal processing to offer a more versatile class of filters than polynomials, capable of modeling a larger variety of frequency responses and capturing longer dynamics with fewer parameters. It aims to overcome the limitations of polynomial filters while maintaining computational efficiency and transferability.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel graph convolutional layer inspired by the Auto-Regressive Moving Average (ARMA) filter. An ARMA filter's rational design (Eq. 6) allows for a more flexible frequency response compared to polynomial filters.\n    *   **Addressing ARMA Challenges**:\n        *   Direct implementation of ARMA filters involves a matrix inversion (Eq. 7), which is computationally intractable for GNNs and yields dense matrices.\n        *   The paper addresses this by approximating the effect of an ARMA(1) filter through a recursive update rule: $\\mathbf{X}^{(t+1)} = \\sigma(\\tilde{\\mathbf{L}}\\mathbf{X}^{(t)}\\mathbf{W} + \\mathbf{X}\\mathbf{V})$ \\cite{bianchi20194ea}. This is termed a **Graph Convolutional Skip (GCS) layer**.\n        *   The GCS layer is iterated for a fixed number of steps, $T$, instead of until full convergence, to ensure computational efficiency and stability.\n    *   **ARMA Layer Construction**: The full ARMA convolutional layer is constructed by combining the outputs of $K$ parallel stacks of these GCS layers, where each stack learns a potentially different filtering operation. The final output is an average of these $K$ outputs.\n    *   **Novelty/Difference**:\n        *   **Rational Filter Design**: First to effectively integrate ARMA filters into GNNs, providing superior frequency response flexibility compared to polynomial filters.\n        *   **Recursive and Distributed Formulation**: Overcomes the computational intractability of matrix inversion in ARMA filters through an iterative GCS layer, making it efficient and scalable.\n        *   **Localized and Transferable**: The recursive formulation ensures the filters are localized in the node space and independent of the underlying graph structure, allowing for inductive inference on graphs with unseen topologies.\n        *   **Skip Connections and Parameter Sharing**: The GCS layer includes a skip connection ($\\mathbf{X}\\mathbf{V}$) to preserve initial node features, mitigating the over-smoothing problem of GCNs. Parameter sharing within each GCS stack acts as a strong regularization.\n        *   **Parallel Stacks with Dropout**: Using multiple parallel GCS stacks with stochastic dropout on skip connections encourages learning a diverse set of features, enhancing representation power.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **ARMA Graph Convolutional Layer**, which leverages the superior filtering capabilities of ARMA filters over traditional polynomial filters in GNNs.\n    *   **System Design/Architectural Innovation**:\n        *   **Graph Convolutional Skip (GCS) Layer**: A novel recursive layer formulation ($\\mathbf{X}^{(t+1)} = \\sigma(\\tilde{\\mathbf{L}}\\mathbf{X}^{(t)}\\mathbf{W} + \\mathbf{X}\\mathbf{V})$) that approximates ARMA filtering efficiently.\n        *   **Parallel GCS Stacks**: An architecture combining multiple independent GCS stacks to form a powerful ARMA layer, allowing for diverse feature learning.\n        *   **Fixed Iteration Count**: A practical implementation strategy that fixes the number of iterations ($T$) for GCS layers, ensuring computational stability and efficiency during training and inference.\n    *   **Theoretical Insights/Analysis**:\n        *   **Spectral Analysis**: The paper performs a spectral analysis to demonstrate the filtering effect of the proposed ARMA layer (though details are not in the provided extract, it's mentioned in the abstract).\n        *   **Convergence Guarantee (Theorem 1)**: Provides a theoretical proof for the convergence of the GCS layer's recursive update, ensuring stability under certain conditions (e.g., non-expansive non-linearity, bounded weights).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed ARMA layer was evaluated on four distinct downstream tasks:\n        *   Semi-supervised node classification.\n        *   Graph signal classification.\n        *   Graph classification.\n        *   Graph regression.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The paper reports that a GNN equipped with ARMA layers **outperforms GNNs based on polynomial filters in every downstream task**.\n        *   This indicates that the ARMA layer's enhanced modeling capability, flexible frequency response, and ability to capture global graph structures translate into significant empirical improvements across various graph learning problems.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The recursive formulation of the ARMA filter is an approximation, not a direct computation of the matrix inverse.\n        *   The convergence proof (Theorem 1) relies on conditions like the non-linearity being non-expansive and weight matrix norms being less than 1, though the paper notes that L2 weight decay can relax the latter.\n        *   The choice of a fixed number of iterations ($T$) for the GCS stacks, while practical, means the layer might not always reach the true fixed point of the recursion.\n    *   **Scope of Applicability**: The proposed ARMA layer is applicable to various graph learning tasks, including node-level, graph-level, and graph signal processing tasks. Its inductive nature allows it to be applied to graphs with unseen topologies, making it suitable for tasks like graph classification and regression where each data point is a different graph.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in graph neural networks by introducing a more powerful and flexible graph convolutional operator than the widely used polynomial filters. It addresses fundamental limitations of existing GNNs related to filter expressiveness, robustness, and ability to capture global graph context.\n    *   **Potential Impact on Future Research**:\n        *   **Richer Graph Representations**: The ARMA layer's ability to capture global graph structures and offer a more flexible frequency response can lead to the learning of richer and more discriminative graph representations.\n        *   **Improved Inductive Capabilities**: Its design, which is independent of specific graph spectra, enhances the inductive capabilities of GNNs, making them more robust and transferable to new, unseen graphs.\n        *   **Foundation for Advanced Filtering**: It opens avenues for exploring other rational or more complex filter designs in GNNs, moving beyond simple polynomial approximations.\n        *   **Addressing Over-smoothing**: The skip connections and recursive nature provide a principled way to mitigate the over-smoothing problem prevalent in deep GCNs.\n        *   **Efficient Deep GNNs**: The fixed-iteration, parallel stack design offers a blueprint for building deeper and more expressive GNNs efficiently.",
    "intriguing_abstract": "Existing Graph Neural Networks (GNNs) often rely on polynomial spectral filters, which inherently limit their frequency response, struggle to capture global graph structures, and lead to over-smoothing in deeper architectures. We introduce a novel paradigm: **Graph Neural Networks with Convolutional ARMA Filters**. Inspired by the superior modeling capabilities of Auto-Regressive Moving Average (ARMA) filters in classical signal processing, our method overcomes these fundamental limitations.\n\nWe devise a computationally efficient, recursive **Graph Convolutional Skip (GCS) layer** that approximates the complex ARMA filtering operation, circumventing intractable matrix inversions. This GCS layer, combined in parallel stacks with skip connections and dropout, enables highly flexible frequency responses, effectively captures global dependencies, and robustly mitigates the notorious over-smoothing problem. Theoretically grounded with convergence guarantees, our ARMA-GNN significantly outperforms state-of-the-art polynomial-based GNNs across diverse tasks, including semi-supervised node classification, graph classification, and graph regression. This work paves the way for more expressive, robust, and inductively capable GNNs, offering richer graph representations for complex real-world applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Convolutional ARMA Filters",
      "Polynomial Spectral Filters",
      "Rational Filter Design",
      "Graph Convolutional Skip (GCS) Layer",
      "Recursive Formulation",
      "Over-smoothing Mitigation",
      "Inductive Inference",
      "Parallel GCS Stacks",
      "Node Classification",
      "Graph Classification",
      "Graph Regression",
      "Flexible Frequency Response",
      "Convergence Guarantee"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf",
    "citation_key": "bianchi20194ea",
    "metadata": {
      "title": "Graph Neural Networks With Convolutional ARMA Filters",
      "authors": [
        "F. Bianchi",
        "Daniele Grattarola",
        "L. Livi",
        "C. Alippi"
      ],
      "published_date": "2019",
      "abstract": "Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.",
      "file_path": "paper_data/Graph_Neural_Networks/f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Graph Neural Networks with Convolutional ARMA Filters\" \\cite{bianchi20194ea} for a literature review:\n\n---\n\n### Analysis of \"Graph Neural Networks with Convolutional ARMA Filters\" \\cite{bianchi20194ea}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) primarily rely on convolutional layers based on polynomial spectral filters. These filters have limitations in their frequency response, robustness to noise, and ability to capture global graph structures.\n    *   **Importance and Challenge**:\n        *   Polynomial filters offer limited modeling capabilities and cannot model sharp changes in frequency response.\n        *   To capture higher-order neighborhoods and global structures, high-degree polynomials are required, which are computationally expensive, prone to overfitting, and sensitive to noise or small changes in graph topology.\n        *   Spectral GNNs (e.g., Bruna et al., 2013) are computationally intensive due to eigendecomposition and are not transferable to graphs with different topologies (inductive inference).\n        *   Simplified polynomial filters like GCNs (Kipf & Welling, 2016a) suffer from over-smoothing, leading to a loss of initial node features after multiple layers.\n        *   The challenge is to design a graph convolutional layer that offers a more flexible frequency response, is robust, captures global dependencies efficiently, and is transferable across different graph structures, without incurring high computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Spectral GNNs (Bruna et al., 2013; Henaff et al., 2015)**: Implement convolutions in the spectral domain using non-linear trainable filters.\n        *   **Polynomial Filters (Defferrard et al., 2016; Kipf & Welling, 2016a)**: Approximate spectral filters with low-order polynomials (e.g., Chebyshev polynomials, GCN) to avoid expensive eigendecomposition and achieve localization in the node space.\n    *   **Limitations of Previous Solutions**:\n        *   **Spectral GNNs**: Computationally expensive (eigendecomposition, dense matrix operations), not localized in node space (account for whole graph), and not transferable to graphs with unseen topologies.\n        *   **Polynomial Filters**:\n            *   Limited modeling capabilities and inability to model sharp changes in frequency response due to their smoothness.\n            *   High-degree polynomials, necessary for capturing larger graph structures, lead to high computational cost, overfitting, and sensitivity to graph changes.\n            *   GCNs, while efficient, perform Laplacian smoothing that can lead to over-smoothing and loss of initial node features after stacking multiple layers.\n    *   **Positioning**: This work proposes a novel graph convolutional layer based on Auto-Regressive Moving Average (ARMA) filters, which are known in signal processing to offer a more versatile class of filters than polynomials, capable of modeling a larger variety of frequency responses and capturing longer dynamics with fewer parameters. It aims to overcome the limitations of polynomial filters while maintaining computational efficiency and transferability.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel graph convolutional layer inspired by the Auto-Regressive Moving Average (ARMA) filter. An ARMA filter's rational design (Eq. 6) allows for a more flexible frequency response compared to polynomial filters.\n    *   **Addressing ARMA Challenges**:\n        *   Direct implementation of ARMA filters involves a matrix inversion (Eq. 7), which is computationally intractable for GNNs and yields dense matrices.\n        *   The paper addresses this by approximating the effect of an ARMA(1) filter through a recursive update rule: $\\mathbf{X}^{(t+1)} = \\sigma(\\tilde{\\mathbf{L}}\\mathbf{X}^{(t)}\\mathbf{W} + \\mathbf{X}\\mathbf{V})$ \\cite{bianchi20194ea}. This is termed a **Graph Convolutional Skip (GCS) layer**.\n        *   The GCS layer is iterated for a fixed number of steps, $T$, instead of until full convergence, to ensure computational efficiency and stability.\n    *   **ARMA Layer Construction**: The full ARMA convolutional layer is constructed by combining the outputs of $K$ parallel stacks of these GCS layers, where each stack learns a potentially different filtering operation. The final output is an average of these $K$ outputs.\n    *   **Novelty/Difference**:\n        *   **Rational Filter Design**: First to effectively integrate ARMA filters into GNNs, providing superior frequency response flexibility compared to polynomial filters.\n        *   **Recursive and Distributed Formulation**: Overcomes the computational intractability of matrix inversion in ARMA filters through an iterative GCS layer, making it efficient and scalable.\n        *   **Localized and Transferable**: The recursive formulation ensures the filters are localized in the node space and independent of the underlying graph structure, allowing for inductive inference on graphs with unseen topologies.\n        *   **Skip Connections and Parameter Sharing**: The GCS layer includes a skip connection ($\\mathbf{X}\\mathbf{V}$) to preserve initial node features, mitigating the over-smoothing problem of GCNs. Parameter sharing within each GCS stack acts as a strong regularization.\n        *   **Parallel Stacks with Dropout**: Using multiple parallel GCS stacks with stochastic dropout on skip connections encourages learning a diverse set of features, enhancing representation power.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **ARMA Graph Convolutional Layer**, which leverages the superior filtering capabilities of ARMA filters over traditional polynomial filters in GNNs.\n    *   **System Design/Architectural Innovation**:\n        *   **Graph Convolutional Skip (GCS) Layer**: A novel recursive layer formulation ($\\mathbf{X}^{(t+1)} = \\sigma(\\tilde{\\mathbf{L}}\\mathbf{X}^{(t)}\\mathbf{W} + \\mathbf{X}\\mathbf{V})$) that approximates ARMA filtering efficiently.\n        *   **Parallel GCS Stacks**: An architecture combining multiple independent GCS stacks to form a powerful ARMA layer, allowing for diverse feature learning.\n        *   **Fixed Iteration Count**: A practical implementation strategy that fixes the number of iterations ($T$) for GCS layers, ensuring computational stability and efficiency during training and inference.\n    *   **Theoretical Insights/Analysis**:\n        *   **Spectral Analysis**: The paper performs a spectral analysis to demonstrate the filtering effect of the proposed ARMA layer (though details are not in the provided extract, it's mentioned in the abstract).\n        *   **Convergence Guarantee (Theorem 1)**: Provides a theoretical proof for the convergence of the GCS layer's recursive update, ensuring stability under certain conditions (e.g., non-expansive non-linearity, bounded weights).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed ARMA layer was evaluated on four distinct downstream tasks:\n        *   Semi-supervised node classification.\n        *   Graph signal classification.\n        *   Graph classification.\n        *   Graph regression.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The paper reports that a GNN equipped with ARMA layers **outperforms GNNs based on polynomial filters in every downstream task**.\n        *   This indicates that the ARMA layer's enhanced modeling capability, flexible frequency response, and ability to capture global graph structures translate into significant empirical improvements across various graph learning problems.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The recursive formulation of the ARMA filter is an approximation, not a direct computation of the matrix inverse.\n        *   The convergence proof (Theorem 1) relies on conditions like the non-linearity being non-expansive and weight matrix norms being less than 1, though the paper notes that L2 weight decay can relax the latter.\n        *   The choice of a fixed number of iterations ($T$) for the GCS stacks, while practical, means the layer might not always reach the true fixed point of the recursion.\n    *   **Scope of Applicability**: The proposed ARMA layer is applicable to various graph learning tasks, including node-level, graph-level, and graph signal processing tasks. Its inductive nature allows it to be applied to graphs with unseen topologies, making it suitable for tasks like graph classification and regression where each data point is a different graph.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in graph neural networks by introducing a more powerful and flexible graph convolutional operator than the widely used polynomial filters. It addresses fundamental limitations of existing GNNs related to filter expressiveness, robustness, and ability to capture global graph context.\n    *   **Potential Impact on Future Research**:\n        *   **Richer Graph Representations**: The ARMA layer's ability to capture global graph structures and offer a more flexible frequency response can lead to the learning of richer and more discriminative graph representations.\n        *   **Improved Inductive Capabilities**: Its design, which is independent of specific graph spectra, enhances the inductive capabilities of GNNs, making them more robust and transferable to new, unseen graphs.\n        *   **Foundation for Advanced Filtering**: It opens avenues for exploring other rational or more complex filter designs in GNNs, moving beyond simple polynomial approximations.\n        *   **Addressing Over-smoothing**: The skip connections and recursive nature provide a principled way to mitigate the over-smoothing problem prevalent in deep GCNs.\n        *   **Efficient Deep GNNs**: The fixed-iteration, parallel stack design offers a blueprint for building deeper and more expressive GNNs efficiently.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Convolutional ARMA Filters",
        "Polynomial Spectral Filters",
        "Rational Filter Design",
        "Graph Convolutional Skip (GCS) Layer",
        "Recursive Formulation",
        "Over-smoothing Mitigation",
        "Inductive Inference",
        "Parallel GCS Stacks",
        "Node Classification",
        "Graph Classification",
        "Graph Regression",
        "Flexible Frequency Response",
        "Convergence Guarantee"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose a novel graph convolutional layer inspired by the auto-regressive moving average (arma) ﬁlter\" and \"we propose a graph neural network implementation of the arma ﬁlter\".\n*   it discusses the advantages of this proposed method (\"more ﬂexible frequency response, is more robust to noise, and better captures the global graph structure\").\n*   it details the implementation (\"recursive and distributed formulation, obtaining a convolutional layer that is efﬁcient to train, localized in the node space, and can be transferred to new graphs at test time\").\n*   it mentions performing \"spectral analysis to study the ﬁltering effect of the proposed arma layer\" and reporting \"experiments on four downstream tasks\".\n\nthis content strongly aligns with the **technical** classification criteria: \"presents new methods, algorithms, or systems\" and the abstract mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\". while there are empirical elements (experiments), they serve to validate the *new method* being proposed.\n\ntherefore, the paper type is: **technical**"
    },
    "file_name": "f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf"
  },
  {
    "success": true,
    "doc_id": "123f290717d9cbe9edd6e1fb394f1bc8",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The prevailing belief is that Graph Neural Networks (GNNs), particularly standard Graph Convolutional Networks (GCNs), inherently rely on the homophily assumption (\"like attracts like\") and perform poorly on heterophilous graphs where dissimilar nodes connect \\cite{ma2021sim}.\n    *   This belief has led to the development of specialized GNN architectures designed to overcome heterophily-related limitations \\cite{ma2021sim}.\n    *   The paper challenges this widely held notion by empirically observing that standard GCNs can achieve strong performance on *some* commonly used heterophilous graphs, motivating a re-evaluation of whether strong homophily is truly a necessity for good GNN performance \\cite{ma2021sim}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches, such as H2GNN \\cite{ma2021sim}, CPGNN \\cite{ma2021sim}, and GPRGNN \\cite{ma2021sim}, are explicitly designed to handle heterophilous graphs through architectural modifications (e.g., skip-connections, specific aggregators) \\cite{ma2021sim}.\n    *   The paper positions itself by demonstrating that these specialized models, despite their design, can be *outperformed* by a carefully tuned standard GCN on certain heterophilous benchmarks, suggesting that their underlying premise (GCN's inherent failure on heterophily) might be incomplete \\cite{ma2021sim}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to theoretically and empirically characterize the conditions under which GCNs can perform well, even on heterophilous graphs \\cite{ma2021sim}.\n    *   The innovation lies in shifting the focus from the *presence* of heterophily to the *nature* of heterophily, distinguishing between \"good\" and \"bad\" heterophily \\cite{ma2021sim}.\n    *   The paper proposes that GCNs can achieve good performance if nodes with the same label share *similar neighborhood patterns* (i.e., their neighbors' label/feature distributions are similar), even if those neighbors are predominantly of different labels \\cite{ma2021sim}.\n    *   This is supported by analyzing the learned node embeddings from a GCN, showing that in expectation, nodes with the same label can have the same embedding, and their actual embeddings are close to this expectation with high probability, especially for high-degree nodes \\cite{ma2021sim}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Understanding**: The paper provides a theoretical framework (Theorem 1) demonstrating that GCNs map same-label nodes to similar embedding spaces if their feature distributions and neighbor label distributions (`Dyi`) are shared and distinguishable across classes \\cite{ma2021sim}.\n    *   **Characterization of Heterophily**: It introduces the concept of \"good\" heterophily (where same-label nodes have distinguishable, yet consistent, neighborhood patterns) versus \"bad\" heterophily, explaining GCN's varied performance \\cite{ma2021sim}.\n    *   **Empirical Re-evaluation**: It empirically shows that GCN can outperform heterophily-specific models on certain heterophilous datasets (e.g., Chameleon, Squirrel) after hyperparameter tuning \\cite{ma2021sim}.\n    *   **Analysis with CSBM**: The use of the Contextual Stochastic Block Model (CSBM) with two classes provides a controlled environment to explicitly characterize the distinguishability of neighborhood distributions and demonstrate linear separability under GCN operations \\cite{ma2021sim}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on semi-supervised node classification (SSNC) tasks \\cite{ma2021sim}.\n    *   **Datasets**: Two commonly used, highly heterophilous graph datasets, Chameleon and Squirrel (both with homophily ratio `h ≈ 0.2`), were used \\cite{ma2021sim}.\n    *   **Key Performance Metrics**: SSNC accuracy was the primary metric \\cite{ma2021sim}.\n    *   **Comparison Results**: GCN achieved significantly higher accuracy (e.g., 67.96% on Chameleon, 54.47% on Squirrel) compared to specialized heterophily-aware models like H2GCN-1 (57.11%, 36.42%), H2GCN-2 (59.39%, 37.90%), CPGNN-MLP (54.53%, 29.13%), CPGNN-Cheby (65.17%, 29.25%), and GPRGNN (66.31%, 50.56%) \\cite{ma2021sim}. This demonstrated GCN's unexpected strong performance on these heterophilous graphs.\n\n*   **Limitations & Scope**\n    *   The theoretical analysis relies on specific assumptions about feature and neighbor label distributions (e.g., features sampled from `Fyi`, neighbor labels independently sampled from `Dyi`, bounded features) \\cite{ma2021sim}.\n    *   The detailed theoretical derivations are primarily for a single-layer GCN without non-linearities, though the authors suggest the analysis can extend to more general message-passing neural networks \\cite{ma2021sim}.\n    *   The empirical validation, while compelling, is limited to a few specific heterophilous datasets where GCN happened to perform well, implying that \"bad\" heterophily cases still exist where GCN might fail \\cite{ma2021sim}.\n\n*   **Technical Significance**\n    *   This work fundamentally challenges the long-standing assumption that strong homophily is a prerequisite for effective GNN performance, particularly for GCNs \\cite{ma2021sim}.\n    *   It advances the technical state-of-the-art by providing a deeper, more nuanced understanding of GNN behavior on heterophilous graphs, moving beyond a simple homophily ratio to consider the *structure* of neighborhood patterns \\cite{ma2021sim}.\n    *   The findings have the potential to impact future research by guiding the design of more robust GNNs and evaluation benchmarks, encouraging researchers to consider the \"type\" of heterophily rather than just its presence, and potentially simplifying model choices for certain heterophilous scenarios \\cite{ma2021sim}.",
    "intriguing_abstract": "The long-standing dogma in Graph Neural Networks (GNNs) posits that standard Graph Convolutional Networks (GCNs) fundamentally struggle with heterophilous graphs, where dissimilar nodes connect. This belief has spurred a proliferation of complex, specialized architectures. Challenging this pervasive assumption, our paper reveals a surprising truth: GCNs can achieve state-of-the-art performance on *highly heterophilous datasets*, often outperforming models specifically designed for heterophily.\n\nWe introduce a novel theoretical framework demonstrating that GCNs effectively learn distinguishable node embeddings even in heterophilous settings, provided nodes of the same class share similar *neighborhood patterns* – a concept we term \"good\" heterophily. Through rigorous empirical re-evaluation on benchmarks like Chameleon and Squirrel, we show GCNs, when properly tuned, significantly surpass specialized models like H2GCN and GPRGNN in semi-supervised node classification. This work fundamentally redefines our understanding of GCN capabilities, shifting focus from the mere presence of heterophily to its underlying structure. It paves the way for simpler, yet more robust, GNN designs and a more nuanced evaluation of graph learning models.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Graph Convolutional Networks (GCNs)",
      "Homophily assumption",
      "Heterophilous graphs",
      "Neighborhood patterns",
      "Node embeddings",
      "\"Good\" heterophily",
      "\"Bad\" heterophily",
      "Theoretical framework",
      "Empirical re-evaluation",
      "Contextual Stochastic Block Model (CSBM)",
      "GCN performance on heterophilous graphs",
      "Challenging GNN assumptions",
      "Semi-supervised node classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf",
    "citation_key": "ma2021sim",
    "metadata": {
      "title": "Is Homophily a Necessity for Graph Neural Networks?",
      "authors": [
        "Yao Ma",
        "Xiaorui Liu",
        "Neil Shah",
        "Jiliang Tang"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\"like attracts like\"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions, and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.",
      "file_path": "paper_data/Graph_Neural_Networks/8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The prevailing belief is that Graph Neural Networks (GNNs), particularly standard Graph Convolutional Networks (GCNs), inherently rely on the homophily assumption (\"like attracts like\") and perform poorly on heterophilous graphs where dissimilar nodes connect \\cite{ma2021sim}.\n    *   This belief has led to the development of specialized GNN architectures designed to overcome heterophily-related limitations \\cite{ma2021sim}.\n    *   The paper challenges this widely held notion by empirically observing that standard GCNs can achieve strong performance on *some* commonly used heterophilous graphs, motivating a re-evaluation of whether strong homophily is truly a necessity for good GNN performance \\cite{ma2021sim}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches, such as H2GNN \\cite{ma2021sim}, CPGNN \\cite{ma2021sim}, and GPRGNN \\cite{ma2021sim}, are explicitly designed to handle heterophilous graphs through architectural modifications (e.g., skip-connections, specific aggregators) \\cite{ma2021sim}.\n    *   The paper positions itself by demonstrating that these specialized models, despite their design, can be *outperformed* by a carefully tuned standard GCN on certain heterophilous benchmarks, suggesting that their underlying premise (GCN's inherent failure on heterophily) might be incomplete \\cite{ma2021sim}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to theoretically and empirically characterize the conditions under which GCNs can perform well, even on heterophilous graphs \\cite{ma2021sim}.\n    *   The innovation lies in shifting the focus from the *presence* of heterophily to the *nature* of heterophily, distinguishing between \"good\" and \"bad\" heterophily \\cite{ma2021sim}.\n    *   The paper proposes that GCNs can achieve good performance if nodes with the same label share *similar neighborhood patterns* (i.e., their neighbors' label/feature distributions are similar), even if those neighbors are predominantly of different labels \\cite{ma2021sim}.\n    *   This is supported by analyzing the learned node embeddings from a GCN, showing that in expectation, nodes with the same label can have the same embedding, and their actual embeddings are close to this expectation with high probability, especially for high-degree nodes \\cite{ma2021sim}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Understanding**: The paper provides a theoretical framework (Theorem 1) demonstrating that GCNs map same-label nodes to similar embedding spaces if their feature distributions and neighbor label distributions (`Dyi`) are shared and distinguishable across classes \\cite{ma2021sim}.\n    *   **Characterization of Heterophily**: It introduces the concept of \"good\" heterophily (where same-label nodes have distinguishable, yet consistent, neighborhood patterns) versus \"bad\" heterophily, explaining GCN's varied performance \\cite{ma2021sim}.\n    *   **Empirical Re-evaluation**: It empirically shows that GCN can outperform heterophily-specific models on certain heterophilous datasets (e.g., Chameleon, Squirrel) after hyperparameter tuning \\cite{ma2021sim}.\n    *   **Analysis with CSBM**: The use of the Contextual Stochastic Block Model (CSBM) with two classes provides a controlled environment to explicitly characterize the distinguishability of neighborhood distributions and demonstrate linear separability under GCN operations \\cite{ma2021sim}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on semi-supervised node classification (SSNC) tasks \\cite{ma2021sim}.\n    *   **Datasets**: Two commonly used, highly heterophilous graph datasets, Chameleon and Squirrel (both with homophily ratio `h ≈ 0.2`), were used \\cite{ma2021sim}.\n    *   **Key Performance Metrics**: SSNC accuracy was the primary metric \\cite{ma2021sim}.\n    *   **Comparison Results**: GCN achieved significantly higher accuracy (e.g., 67.96% on Chameleon, 54.47% on Squirrel) compared to specialized heterophily-aware models like H2GCN-1 (57.11%, 36.42%), H2GCN-2 (59.39%, 37.90%), CPGNN-MLP (54.53%, 29.13%), CPGNN-Cheby (65.17%, 29.25%), and GPRGNN (66.31%, 50.56%) \\cite{ma2021sim}. This demonstrated GCN's unexpected strong performance on these heterophilous graphs.\n\n*   **Limitations & Scope**\n    *   The theoretical analysis relies on specific assumptions about feature and neighbor label distributions (e.g., features sampled from `Fyi`, neighbor labels independently sampled from `Dyi`, bounded features) \\cite{ma2021sim}.\n    *   The detailed theoretical derivations are primarily for a single-layer GCN without non-linearities, though the authors suggest the analysis can extend to more general message-passing neural networks \\cite{ma2021sim}.\n    *   The empirical validation, while compelling, is limited to a few specific heterophilous datasets where GCN happened to perform well, implying that \"bad\" heterophily cases still exist where GCN might fail \\cite{ma2021sim}.\n\n*   **Technical Significance**\n    *   This work fundamentally challenges the long-standing assumption that strong homophily is a prerequisite for effective GNN performance, particularly for GCNs \\cite{ma2021sim}.\n    *   It advances the technical state-of-the-art by providing a deeper, more nuanced understanding of GNN behavior on heterophilous graphs, moving beyond a simple homophily ratio to consider the *structure* of neighborhood patterns \\cite{ma2021sim}.\n    *   The findings have the potential to impact future research by guiding the design of more robust GNNs and evaluation benchmarks, encouraging researchers to consider the \"type\" of heterophily rather than just its presence, and potentially simplifying model choices for certain heterophilous scenarios \\cite{ma2021sim}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Graph Convolutional Networks (GCNs)",
        "Homophily assumption",
        "Heterophilous graphs",
        "Neighborhood patterns",
        "Node embeddings",
        "\"Good\" heterophily",
        "\"Bad\" heterophily",
        "Theoretical framework",
        "Empirical re-evaluation",
        "Contextual Stochastic Block Model (CSBM)",
        "GCN performance on heterophilous graphs",
        "Challenging GNN assumptions",
        "Semi-supervised node classification"
      ],
      "paper_type": "this paper should be classified as **theoretical**.\n\nhere's why:\n\n1.  **core question and answer:** the paper investigates a fundamental question about gnns: \"is homophily a necessity for graph neural networks?\". it provides a nuanced answer by characterizing conditions under which gcns perform well, regardless of homophily.\n2.  **explicit theoretical contributions:**\n    *   the abstract states: \"our work carefully characterizes the implications of different heterophily conditions, and provides supporting **theoretical understanding** and empirical observations.\"\n    *   the introduction explicitly mentions: \"we **theoretically support** this argument by investigating the learned node embeddings from the gcn model.\" and \"we carefully characterize these conditions and provide **theoretical understandings** on how gcns can achieve good ssnc performance under these conditions by investigating their embedding learning process.\"\n    *   the paper includes formal **theorems (theorem 1 and theorem 2)** and their proofs (in appendix a and b), which are hallmarks of theoretical work.\n    *   it uses a **formal model (contextual stochastic block model - csbm)** for its analysis in section 3.2.\n3.  **empirical support for theory:** while the paper also features extensive empirical investigations (sections 3.3 and 4, with figures and tables of results), these are presented as \"supporting empirical observations\" and \"empirical investigations\" that \"substantiate our claims\" and \"demonstrate these observations.\" the empirical work is used to validate and illustrate the theoretical framework developed.\n4.  **not a new method (technical):** the paper does not propose a new gnn architecture or algorithm. instead, it re-evaluates and provides a deeper understanding of an existing, fundamental model (gcn).\n5.  **not a survey:** it's not reviewing literature comprehensively to organize it.\n6.  **not a position paper (purely):** while it argues against a popular notion, it does so with rigorous scientific analysis (both theoretical and empirical), rather than just presenting an argument or vision.\n\nthe primary contribution is a deeper, theoretically grounded understanding of gcn behavior under varying homophily conditions, supported by empirical evidence."
    },
    "file_name": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf"
  },
  {
    "success": true,
    "doc_id": "451f34914791e8914b4379e43827464f",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel at structural pattern recognition but often lack the rich semantic understanding that Large Language Models (LLMs) provide, a gap successfully bridged in other domains by paradigms like CLIP. Extending this to general graph data faces formidable challenges: scarce datasets, extremely weak text supervision, and the inherent complexity of graph structures. We introduce **Morpher**, a novel multi-modal prompt learning framework that enables GNNs to leverage the rich semantic knowledge of LLMs, revolutionizing their adaptability.\n\nMorpher aligns independently pre-trained GNNs with LLM embedding spaces by simultaneously learning stable graph prompts and text prompts, crucially keeping backbone models frozen, and employing a contrastive loss for semantic alignment. Our innovative graph prompt design overcomes prior instability issues, ensuring balanced feature integration. Morpher achieves state-of-the-art performance in few-shot graph classification and, for the first time, enables CLIP-style zero-shot generalization for GNNs to unseen classes. This breakthrough paves the way for truly semantically aware and adaptable GNNs, even in low-resource settings, marking a significant advance in multi-modal graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Large Language Models (LLMs)",
      "Multi-modal Prompt Learning",
      "Morpher",
      "Improved Graph Prompt Design",
      "Extremely Weak Text Supervision",
      "Semantic Embedding Space",
      "Contrastive Learning",
      "Cross-modal Projector",
      "Few-shot Learning",
      "Zero-shot Generalization for GNNs",
      "CLIP-style pipeline",
      "Graph-Text Alignment"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf",
    "citation_key": "li202444f",
    "metadata": {
      "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
      "authors": [
        "Li",
        "Lecheng Zheng",
        "Bowen Jin",
        "Dongqi Fu",
        "Baoyu Jing",
        "Yikun Ban",
        "Jingrui He",
        "Jiawei Han"
      ],
      "published_date": "2024",
      "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
      "file_path": "paper_data/Graph_Neural_Networks/14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Large Language Models (LLMs)",
        "Multi-modal Prompt Learning",
        "Morpher",
        "Improved Graph Prompt Design",
        "Extremely Weak Text Supervision",
        "Semantic Embedding Space",
        "Contrastive Learning",
        "Cross-modal Projector",
        "Few-shot Learning",
        "Zero-shot Generalization for GNNs",
        "CLIP-style pipeline",
        "Graph-Text Alignment"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose a multi-modal prompt learning paradigm** to effectively adapt pre-trained gnn to downstream tasks and data...\" and \"our new paradigm embeds the graphs directly...\". it also mentions building \"the first clip-style zero-shot classification prototype\".\n*   the introduction discusses a technical problem (\"gnns... still do not understand what a label represents in the real world\") and the challenges in extending existing paradigms to graphs, setting the stage for the proposed solution.\n*   the paper presents a new method/paradigm and describes its mechanism, followed by a demonstration of its performance.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
    },
    "file_name": "14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf"
  },
  {
    "success": true,
    "doc_id": "a7ae3fc25365bab0448a9b1bb6eb15b8",
    "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the problem of inferring the underlying graph structure (specifically, the existence of links between any pair of nodes) from the black-box outputs of a Graph Neural Network (GNN) model that was trained on that graph \\cite{he2020kz4}.\n    *   **Motivation:** Graph data, such as chemical networks or social networks, is often considered confidential or private. This is either because data owners invest significant resources in collecting it (making it intellectual property) or because it contains sensitive information (e.g., private social relationships). GNNs are increasingly used in various applications (healthcare, recommender systems, fraud detection) due to their superior performance, but the security and privacy implications of training GNNs on such sensitive graphs are largely unexplored \\cite{he2020kz4}.\n\n*   **Related Work & Positioning**\n    *   This work is positioned as the *first* to propose and systematically study \"link stealing attacks\" against GNNs, specifically focusing on extracting the training graph's structure from a black-box GNN model's outputs \\cite{he2020kz4}.\n    *   It differentiates itself from conventional link prediction methods, which typically aim to predict missing links within a *known partial graph*, whereas this work aims to infer the existence of links in the *entire original graph* based solely on the GNN's predictions \\cite{he2020kz4}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** The fundamental intuition behind the attacks is that if two nodes are linked in a graph, their attributes and/or their prediction posteriors (outputs) from a GNN model trained on that graph should be more similar due to the GNN's message-passing and aggregation mechanisms \\cite{he2020kz4}.\n    *   **Threat Model Taxonomy:** A novel and comprehensive threat model is introduced, characterizing an adversary's background knowledge along three dimensions: access to the target dataset's nodes' attributes (F), access to a partial graph of the target dataset (A'), and access to an auxiliary \"shadow dataset\" (D'). This taxonomy systematically defines 8 different types of adversaries and their corresponding attack strategies \\cite{he2020kz4}.\n    *   **Attack Methodologies:**\n        *   **Unsupervised Attacks (e.g., Attack-0, Attack-2):** For adversaries with minimal knowledge, these attacks calculate distances (using 8 common metrics like Cosine, Euclidean) between node posteriors (from the target GNN) or node attributes to infer link existence \\cite{he2020kz4}.\n        *   **Supervised Attacks (e.g., Attack-4, Attack-6):** When a partial graph (A') is available, a binary classifier (Multi-Layer Perceptron - MLP) is trained. Features for this classifier are derived from node posteriors (distances, pairwise operations on entropies) and/or node attributes (distances, pairwise vector operations) \\cite{he2020kz4}.\n        *   **Transferring Attacks (e.g., Attack-1, Attack-3, Attack-5, Attack-7):** For adversaries with a shadow dataset (D'), a shadow GNN model is trained. Features are then extracted from the shadow model's posteriors (using distances and entropy-based operations) and transferred to train an attack model for the target GNN. This approach addresses the \"dimension mismatch\" problem between different datasets' posteriors by using dimension-agnostic feature representations \\cite{he2020kz4}.\n    *   **Novelty:** The paper's primary innovation lies in proposing the first systematic framework and concrete methodologies for link stealing against GNNs, demonstrating that GNN outputs inherently encode significant structural information about their training graphs, even under black-box access \\cite{he2020kz4}.\n\n*   **Key Technical Contributions**\n    *   **First Link Stealing Attacks:** Introduction of the concept and practical realization of link stealing attacks against GNNs, revealing a critical privacy vulnerability \\cite{he2020kz4}.\n    *   **Comprehensive Threat Model:** A novel, 3-dimensional threat model leading to an 8-attack taxonomy, providing a structured way to analyze adversary capabilities and attack effectiveness \\cite{he2020kz4}.\n    *   **Advanced Feature Engineering:** Development of effective feature sets for attack models, including various distance metrics on posteriors/attributes and pairwise operations on posterior entropies, designed to handle challenges like node pair order and dimension mismatch \\cite{he2020kz4}.\n    *   **Transfer Learning for Graph Inference:** Demonstration of successful transferring attacks that leverage knowledge from auxiliary datasets to infer links in a target graph, even across different domains \\cite{he2020kz4}.\n\n*   **Experimental Validation**\n    *   **Experiments:** Extensive experiments were conducted to evaluate the effectiveness of all 8 proposed link stealing attacks \\cite{he2020kz4}.\n    *   **Datasets:** The attacks were validated on 8 diverse real-world datasets, showcasing their broad applicability \\cite{he2020kz4}.\n    *   **Key Performance Metrics:** Attack performance was primarily measured using AUC (Area Under the ROC Curve) \\cite{he2020kz4}.\n    *   **Comparison Results:**\n        *   **High Effectiveness:** The attacks proved highly effective, achieving AUCs above 0.95 in multiple scenarios, indicating a strong ability to infer links \\cite{he2020kz4}.\n        *   **Impact of Background Knowledge:** Performance generally improved with more background knowledge (e.g., on Citeseer, AUC increased from 0.878 with only attributes to 0.977 with all available knowledge) \\cite{he2020kz4}.\n        *   **Knowledge Dimension Hierarchy:** The target dataset's partial graph (A') had the strongest impact on attack success, followed by nodes' attributes (F), with the shadow dataset (D') having the weakest but still beneficial impact \\cite{he2020kz4}.\n        *   **Transferring Attack Efficacy:** Transferring attacks achieved high AUCs, performing better when the shadow dataset originated from the same domain as the target dataset, suggesting that structural similarities aid knowledge transfer \\cite{he2020kz4}.\n        *   **Superiority over Baselines:** The proposed attacks consistently outperformed conventional link prediction methods \\cite{he2020kz4}.\n        *   **Generalizability:** The attacks were shown to be effective not only against GCNs but also against other popular GNN architectures like GraphSAGE and GAT \\cite{he2020kz4}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The paper primarily focuses on undirected graphs. While the attacks generalize to different GNN architectures, the specific GNN model's complexity or regularization might influence the degree of information leakage, which is not deeply explored as a limitation \\cite{he2020kz4}.\n    *   **Assumptions:** The attacks assume black-box access to the GNN model, allowing queries for node posteriors. It also assumes the adversary knows the set of nodes in the target graph \\cite{he2020kz4}.\n    *   **Scope of Applicability:** The findings are directly applicable to GNNs used for node classification tasks. The demonstrated vulnerability highlights a general concern for any application where the confidentiality or intellectual property of the underlying graph structure is paramount \\cite{he2020kz4}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the state-of-the-art by being the first to systematically identify, formalize, and demonstrate practical link stealing attacks against GNNs, thereby establishing a new frontier in GNN security and privacy research \\cite{he2020kz4}.\n    *   **Potential Impact on Future Research:**\n        *   **Privacy-Preserving GNNs:** The findings create an urgent need for research into privacy-preserving GNN training mechanisms, architectures, and differential privacy techniques to protect graph structure \\cite{he2020kz4}.\n        *   **Adversarial Machine Learning:** It contributes a novel attack vector to the broader field of adversarial machine learning, specifically tailored to the unique properties of graph data and GNNs \\cite{he2020kz4}.\n        *   **GNN Interpretability and Information Flow:** The results offer crucial insights into how GNNs encode and implicitly reveal graph structural information through their outputs, which can inform future GNN design and theoretical understanding \\cite{he2020kz4}.\n        *   **Ethical and Practical Implications:** It raises critical awareness for practitioners and organizations deploying GNNs on sensitive graph data, highlighting the potential for intellectual property theft and privacy breaches, and urging the consideration of these risks in deployment strategies \\cite{he2020kz4}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph-based learning, yet their widespread adoption on sensitive data raises critical privacy concerns. This paper unveils a profound vulnerability: **link stealing attacks**, demonstrating that a GNN's black-box outputs can inadvertently reveal the very **graph structure** it was trained on. We present the first systematic framework for inferring the existence of links between nodes solely from a GNN's predictions, even under minimal adversary knowledge.\n\nOur pioneering work introduces a comprehensive, 3-dimensional **threat model** that characterizes diverse adversary capabilities, leading to eight distinct attack strategies. Crucially, we develop innovative **transferring attacks** that leverage auxiliary \"shadow datasets\" to infer links in a target graph, even across domains, by exploiting the inherent structural information encoded in GNN **node posteriors** through their **message-passing** mechanisms. Extensive experiments across eight real-world datasets confirm the attacks' high effectiveness, often achieving AUCs above 0.95. This research establishes a new frontier in **adversarial machine learning** for graphs, underscoring the urgent need for **privacy-preserving GNNs** to safeguard sensitive graph data and intellectual property.",
    "keywords": [
      "Link stealing attacks",
      "Graph Neural Networks (GNNs)",
      "black-box model access",
      "graph structure inference",
      "privacy vulnerability",
      "threat model taxonomy",
      "node posteriors",
      "feature engineering",
      "transfer learning",
      "adversarial machine learning",
      "confidential graph data",
      "AUC",
      "privacy-preserving GNNs",
      "message-passing mechanisms"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/e4b1d7553020258d7e537e2cfa53865359389eac.pdf",
    "citation_key": "he2020kz4",
    "metadata": {
      "title": "Stealing Links from Graph Neural Networks",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "published_date": "2020",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "file_path": "paper_data/Graph_Neural_Networks/e4b1d7553020258d7e537e2cfa53865359389eac.pdf",
      "venue": "USENIX Security Symposium",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the problem of inferring the underlying graph structure (specifically, the existence of links between any pair of nodes) from the black-box outputs of a Graph Neural Network (GNN) model that was trained on that graph \\cite{he2020kz4}.\n    *   **Motivation:** Graph data, such as chemical networks or social networks, is often considered confidential or private. This is either because data owners invest significant resources in collecting it (making it intellectual property) or because it contains sensitive information (e.g., private social relationships). GNNs are increasingly used in various applications (healthcare, recommender systems, fraud detection) due to their superior performance, but the security and privacy implications of training GNNs on such sensitive graphs are largely unexplored \\cite{he2020kz4}.\n\n*   **Related Work & Positioning**\n    *   This work is positioned as the *first* to propose and systematically study \"link stealing attacks\" against GNNs, specifically focusing on extracting the training graph's structure from a black-box GNN model's outputs \\cite{he2020kz4}.\n    *   It differentiates itself from conventional link prediction methods, which typically aim to predict missing links within a *known partial graph*, whereas this work aims to infer the existence of links in the *entire original graph* based solely on the GNN's predictions \\cite{he2020kz4}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** The fundamental intuition behind the attacks is that if two nodes are linked in a graph, their attributes and/or their prediction posteriors (outputs) from a GNN model trained on that graph should be more similar due to the GNN's message-passing and aggregation mechanisms \\cite{he2020kz4}.\n    *   **Threat Model Taxonomy:** A novel and comprehensive threat model is introduced, characterizing an adversary's background knowledge along three dimensions: access to the target dataset's nodes' attributes (F), access to a partial graph of the target dataset (A'), and access to an auxiliary \"shadow dataset\" (D'). This taxonomy systematically defines 8 different types of adversaries and their corresponding attack strategies \\cite{he2020kz4}.\n    *   **Attack Methodologies:**\n        *   **Unsupervised Attacks (e.g., Attack-0, Attack-2):** For adversaries with minimal knowledge, these attacks calculate distances (using 8 common metrics like Cosine, Euclidean) between node posteriors (from the target GNN) or node attributes to infer link existence \\cite{he2020kz4}.\n        *   **Supervised Attacks (e.g., Attack-4, Attack-6):** When a partial graph (A') is available, a binary classifier (Multi-Layer Perceptron - MLP) is trained. Features for this classifier are derived from node posteriors (distances, pairwise operations on entropies) and/or node attributes (distances, pairwise vector operations) \\cite{he2020kz4}.\n        *   **Transferring Attacks (e.g., Attack-1, Attack-3, Attack-5, Attack-7):** For adversaries with a shadow dataset (D'), a shadow GNN model is trained. Features are then extracted from the shadow model's posteriors (using distances and entropy-based operations) and transferred to train an attack model for the target GNN. This approach addresses the \"dimension mismatch\" problem between different datasets' posteriors by using dimension-agnostic feature representations \\cite{he2020kz4}.\n    *   **Novelty:** The paper's primary innovation lies in proposing the first systematic framework and concrete methodologies for link stealing against GNNs, demonstrating that GNN outputs inherently encode significant structural information about their training graphs, even under black-box access \\cite{he2020kz4}.\n\n*   **Key Technical Contributions**\n    *   **First Link Stealing Attacks:** Introduction of the concept and practical realization of link stealing attacks against GNNs, revealing a critical privacy vulnerability \\cite{he2020kz4}.\n    *   **Comprehensive Threat Model:** A novel, 3-dimensional threat model leading to an 8-attack taxonomy, providing a structured way to analyze adversary capabilities and attack effectiveness \\cite{he2020kz4}.\n    *   **Advanced Feature Engineering:** Development of effective feature sets for attack models, including various distance metrics on posteriors/attributes and pairwise operations on posterior entropies, designed to handle challenges like node pair order and dimension mismatch \\cite{he2020kz4}.\n    *   **Transfer Learning for Graph Inference:** Demonstration of successful transferring attacks that leverage knowledge from auxiliary datasets to infer links in a target graph, even across different domains \\cite{he2020kz4}.\n\n*   **Experimental Validation**\n    *   **Experiments:** Extensive experiments were conducted to evaluate the effectiveness of all 8 proposed link stealing attacks \\cite{he2020kz4}.\n    *   **Datasets:** The attacks were validated on 8 diverse real-world datasets, showcasing their broad applicability \\cite{he2020kz4}.\n    *   **Key Performance Metrics:** Attack performance was primarily measured using AUC (Area Under the ROC Curve) \\cite{he2020kz4}.\n    *   **Comparison Results:**\n        *   **High Effectiveness:** The attacks proved highly effective, achieving AUCs above 0.95 in multiple scenarios, indicating a strong ability to infer links \\cite{he2020kz4}.\n        *   **Impact of Background Knowledge:** Performance generally improved with more background knowledge (e.g., on Citeseer, AUC increased from 0.878 with only attributes to 0.977 with all available knowledge) \\cite{he2020kz4}.\n        *   **Knowledge Dimension Hierarchy:** The target dataset's partial graph (A') had the strongest impact on attack success, followed by nodes' attributes (F), with the shadow dataset (D') having the weakest but still beneficial impact \\cite{he2020kz4}.\n        *   **Transferring Attack Efficacy:** Transferring attacks achieved high AUCs, performing better when the shadow dataset originated from the same domain as the target dataset, suggesting that structural similarities aid knowledge transfer \\cite{he2020kz4}.\n        *   **Superiority over Baselines:** The proposed attacks consistently outperformed conventional link prediction methods \\cite{he2020kz4}.\n        *   **Generalizability:** The attacks were shown to be effective not only against GCNs but also against other popular GNN architectures like GraphSAGE and GAT \\cite{he2020kz4}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The paper primarily focuses on undirected graphs. While the attacks generalize to different GNN architectures, the specific GNN model's complexity or regularization might influence the degree of information leakage, which is not deeply explored as a limitation \\cite{he2020kz4}.\n    *   **Assumptions:** The attacks assume black-box access to the GNN model, allowing queries for node posteriors. It also assumes the adversary knows the set of nodes in the target graph \\cite{he2020kz4}.\n    *   **Scope of Applicability:** The findings are directly applicable to GNNs used for node classification tasks. The demonstrated vulnerability highlights a general concern for any application where the confidentiality or intellectual property of the underlying graph structure is paramount \\cite{he2020kz4}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the state-of-the-art by being the first to systematically identify, formalize, and demonstrate practical link stealing attacks against GNNs, thereby establishing a new frontier in GNN security and privacy research \\cite{he2020kz4}.\n    *   **Potential Impact on Future Research:**\n        *   **Privacy-Preserving GNNs:** The findings create an urgent need for research into privacy-preserving GNN training mechanisms, architectures, and differential privacy techniques to protect graph structure \\cite{he2020kz4}.\n        *   **Adversarial Machine Learning:** It contributes a novel attack vector to the broader field of adversarial machine learning, specifically tailored to the unique properties of graph data and GNNs \\cite{he2020kz4}.\n        *   **GNN Interpretability and Information Flow:** The results offer crucial insights into how GNNs encode and implicitly reveal graph structural information through their outputs, which can inform future GNN design and theoretical understanding \\cite{he2020kz4}.\n        *   **Ethical and Practical Implications:** It raises critical awareness for practitioners and organizations deploying GNNs on sensitive graph data, highlighting the potential for intellectual property theft and privacy breaches, and urging the consideration of these risks in deployment strategies \\cite{he2020kz4}.",
      "keywords": [
        "Link stealing attacks",
        "Graph Neural Networks (GNNs)",
        "black-box model access",
        "graph structure inference",
        "privacy vulnerability",
        "threat model taxonomy",
        "node posteriors",
        "feature engineering",
        "transfer learning",
        "adversarial machine learning",
        "confidential graph data",
        "AUC",
        "privacy-preserving GNNs",
        "message-passing mechanisms"
      ],
      "paper_type": "based on the abstract and introduction:\n\nthe paper clearly states:\n*   \"we **propose** the first attacks to steal a graph from the outputs of a gnn model\"\n*   \"we **propose** a threat model to systematically characterize an adversary’s background knowledge\"\n*   \"we **propose** multiple novel **methods** to realize these 8 attacks.\"\n\nthe introduction further emphasizes:\n*   \"we take the first step to study the security and privacy implications of training gnns on graphs. in particular, we **propose** th...\"\n\nwhile the paper also includes \"extensive **experiments** on 8 real-world **datasets** show that our attacks are effective,\" this empirical evaluation serves to validate the **new methods and attacks** being proposed. the core contribution is the development and presentation of these novel attacks and methods.\n\ntherefore, this paper best fits the **technical** classification."
    },
    "file_name": "e4b1d7553020258d7e537e2cfa53865359389eac.pdf"
  },
  {
    "success": true,
    "doc_id": "5a81215e29c360fbf13a621b65c89e55",
    "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n---\n\n### Focused Summary for Literature Review: Universal Prompt Tuning for Graph Neural Networks\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: Adapting pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks is challenging due to the wide variety of GNN pre-training strategies. Existing prompt-based tuning methods for GNNs are specialized for particular pre-training tasks (e.g., edge prediction) and lack universality.\n*   **Importance & Challenge**:\n    *   GNNs face challenges like scarcity of labeled data and low out-of-distribution generalization, which pre-trained GNNs aim to mitigate.\n    *   The standard \"pre-train, fine-tune\" paradigm for GNNs suffers from objective misalignment between pre-training and downstream tasks, and catastrophic forgetting, especially in few-shot scenarios.\n    *   Prompt tuning, successful in NLP and CV, offers an alternative by modifying input data instead of model parameters. However, applying it to GNNs is difficult because there's no unified pre-training task (unlike masked language modeling in NLP), making it hard to design a universal prompting function.\n    *   Prior GNN prompt tuning efforts are intuition-based, specialized, and lack theoretical guarantees for effectiveness.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches**:\n    *   `\\cite{fang2022tjj}` builds upon the concept of pre-trained GNN models and the prompt tuning paradigm from NLP and CV.\n    *   It contrasts with traditional fine-tuning, which updates the entire pre-trained GNN model.\n    *   It relates to existing specialized prompt-based tuning methods for GNNs (e.g., \\cite{sun2022graphprompt, liu2023graphprompt}) that introduce virtual nodes or links for models pre-trained with edge prediction.\n*   **Limitations of previous solutions**:\n    *   **Fine-tuning**: Prone to catastrophic forgetting and sub-optimal performance due to objective misalignment, particularly with limited downstream data.\n    *   **Specialized GNN prompt tuning**: These methods are highly specific to certain pre-training strategies (e.g., edge prediction) and cannot be applied to GNNs pre-trained with other common strategies like attribute masking or contrastive learning. They are also often intuitively designed without theoretical guarantees.\n*   **Positioning**: `\\cite{fang2022tjj}` introduces the *first* universal prompt-based tuning method for pre-trained GNN models, designed to be applicable irrespective of the underlying pre-training strategy, thereby addressing the limitations of specialized approaches.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: `\\cite{fang2022tjj}` proposes **Graph Prompt Feature (GPF)**, a universal prompt-based tuning method that operates by modifying the input graph's *feature space*.\n    *   **GPF**: A learnable vector `p` (of dimension `F`, matching node feature dimensionality) is *added to all node features* `xi` in the input graph `X` to generate prompted features `X* = {x1+p, ..., xN+p}`. The pre-trained GNN model parameters are frozen, and only `p` and a learnable projection head `θ` are optimized for the downstream task.\n    *   **GPF-plus**: A theoretically stronger variant of GPF that incorporates *different* learnable prompted features for *different nodes* in the graph, allowing for more nuanced and adaptive input modifications.\n*   **Novelty/Difference**:\n    *   **Universality**: GPF is designed to be compatible with *any* pre-trained GNN model and *any* pre-training strategy, eliminating the need for strategy-specific manual prompting function design.\n    *   **Feature Space Manipulation**: Unlike methods that modify graph structure (e.g., adding virtual nodes/edges), GPF directly adapts the input node features, drawing inspiration from pixel-level visual prompts in computer vision.\n    *   **Theoretical Foundation**: The paper provides rigorous derivations to demonstrate that GPF can theoretically achieve an equivalent effect to *any* form of prompting function, offering a strong theoretical basis for its universality and effectiveness, which was lacking in prior intuitive designs.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The introduction of **Graph Prompt Feature (GPF)** and its variant **GPF-plus**, representing the first universal prompt-based tuning methods for pre-trained GNNs.\n    *   A novel paradigm for GNN prompt tuning by operating on the *input graph's feature space* rather than altering graph structure or fine-tuning model parameters.\n*   **Theoretical Insights/Analysis**:\n    *   Rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to *any* prompting function, thereby guaranteeing its broad applicability.\n    *   Theoretical proofs that GPF and GPF-plus are not weaker than full fine-tuning and can, in some cases, yield *superior* theoretical tuning results.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**:\n    *   Extensive experiments were conducted across various pre-training strategies (e.g., those involving edge prediction and attribute masking) and GNN architectures.\n    *   Evaluations were performed in both **full-shot** (sufficient labeled data) and **few-shot** (limited labeled data) scenarios.\n    *   Performance was compared against traditional **fine-tuning** and existing **specialized prompt-based tuning methods** (when applicable to their specialized pre-training strategies).\n*   **Key Performance Metrics & Comparison Results**:\n    *   GPF and GPF-plus consistently outperformed fine-tuning, despite utilizing significantly fewer tunable parameters.\n    *   **Average performance improvement over fine-tuning**: Approximately **1.4%** in full-shot scenarios and a more substantial **3.2%** in few-shot scenarios, highlighting its effectiveness in data-scarce environments.\n    *   **Superiority over specialized prompts**: GPF and GPF-plus significantly outperformed existing specialized prompt-based tuning methods even when applied to models pre-trained with the specific strategies those specialized methods were designed for (e.g., edge prediction).\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus and detailed validation in the main paper are on graph classification tasks, with extensions to node-wise tasks (node classification, link prediction) mentioned as being in the appendix (not provided in the excerpt).\n    *   While GPF-plus is described as theoretically stronger, the specific implementation details of how \"different prompted features for different nodes\" are learned and applied are not fully elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   Universally applicable to *any* pre-trained GNN model, regardless of its specific pre-training strategy.\n    *   Applicable across various GNN architectures.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   `\\cite{fang2022tjj}` makes a significant advancement by introducing the first *universal* prompt-based tuning method for GNNs, effectively addressing the challenge posed by diverse pre-training strategies.\n    *   It provides a more efficient, effective, and robust alternative to fine-tuning, particularly beneficial in data-limited settings, by adapting the input space while keeping the pre-trained model frozen.\n    *   The work establishes a strong theoretical foundation for GNN prompt tuning, moving beyond heuristic designs.\n*   **Potential Impact on Future Research**:\n    *   This work opens new research directions for developing universal adaptation strategies for GNNs and potentially other complex data structures with varied pre-training paradigms.\n    *   It could lead to the development of more generalizable and easily adaptable pre-trained GNN models, accelerating their deployment in real-world applications.\n    *   It encourages further theoretical and empirical investigation into the mechanisms and optimal designs of prompt tuning for graph-structured data.",
    "intriguing_abstract": "Adapting powerful pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks remains a significant hurdle, plagued by the heterogeneity of pre-training strategies and the limitations of traditional fine-tuning, such as catastrophic forgetting and data inefficiency. Existing GNN prompt tuning methods are specialized and lack universality. We introduce **Graph Prompt Feature (GPF)**, the first universal prompt-based tuning paradigm designed to seamlessly adapt *any* pre-trained GNN, irrespective of its pre-training objective. Unlike prior approaches that modify graph structure or fine-tune model parameters, GPF operates by introducing a learnable prompt directly into the input graph's *feature space*. We provide rigorous theoretical derivations demonstrating GPF's capacity to achieve the effect of *any* prompting function, offering a robust foundation previously absent. Extensive experiments show GPF consistently outperforms fine-tuning, achieving up to a 3.2% improvement in challenging few-shot scenarios while using orders of magnitude fewer tunable parameters. This novel, theoretically grounded approach offers a powerful, efficient, and universal solution for GNN adaptation, paving the way for more generalizable and robust graph learning systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "prompt tuning",
      "pre-trained GNNs",
      "universal prompt tuning",
      "Graph Prompt Feature (GPF)",
      "input feature space manipulation",
      "theoretical guarantees",
      "few-shot learning",
      "catastrophic forgetting",
      "objective misalignment",
      "fine-tuning",
      "downstream tasks"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf",
    "citation_key": "fang2022tjj",
    "metadata": {
      "title": "Universal Prompt Tuning for Graph Neural Networks",
      "authors": [
        "Taoran Fang",
        "Yunchao Zhang",
        "Yang Yang",
        "Chunping Wang",
        "Lei Chen"
      ],
      "published_date": "2022",
      "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
      "file_path": "paper_data/Graph_Neural_Networks/85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n---\n\n### Focused Summary for Literature Review: Universal Prompt Tuning for Graph Neural Networks\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: Adapting pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks is challenging due to the wide variety of GNN pre-training strategies. Existing prompt-based tuning methods for GNNs are specialized for particular pre-training tasks (e.g., edge prediction) and lack universality.\n*   **Importance & Challenge**:\n    *   GNNs face challenges like scarcity of labeled data and low out-of-distribution generalization, which pre-trained GNNs aim to mitigate.\n    *   The standard \"pre-train, fine-tune\" paradigm for GNNs suffers from objective misalignment between pre-training and downstream tasks, and catastrophic forgetting, especially in few-shot scenarios.\n    *   Prompt tuning, successful in NLP and CV, offers an alternative by modifying input data instead of model parameters. However, applying it to GNNs is difficult because there's no unified pre-training task (unlike masked language modeling in NLP), making it hard to design a universal prompting function.\n    *   Prior GNN prompt tuning efforts are intuition-based, specialized, and lack theoretical guarantees for effectiveness.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches**:\n    *   `\\cite{fang2022tjj}` builds upon the concept of pre-trained GNN models and the prompt tuning paradigm from NLP and CV.\n    *   It contrasts with traditional fine-tuning, which updates the entire pre-trained GNN model.\n    *   It relates to existing specialized prompt-based tuning methods for GNNs (e.g., \\cite{sun2022graphprompt, liu2023graphprompt}) that introduce virtual nodes or links for models pre-trained with edge prediction.\n*   **Limitations of previous solutions**:\n    *   **Fine-tuning**: Prone to catastrophic forgetting and sub-optimal performance due to objective misalignment, particularly with limited downstream data.\n    *   **Specialized GNN prompt tuning**: These methods are highly specific to certain pre-training strategies (e.g., edge prediction) and cannot be applied to GNNs pre-trained with other common strategies like attribute masking or contrastive learning. They are also often intuitively designed without theoretical guarantees.\n*   **Positioning**: `\\cite{fang2022tjj}` introduces the *first* universal prompt-based tuning method for pre-trained GNN models, designed to be applicable irrespective of the underlying pre-training strategy, thereby addressing the limitations of specialized approaches.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: `\\cite{fang2022tjj}` proposes **Graph Prompt Feature (GPF)**, a universal prompt-based tuning method that operates by modifying the input graph's *feature space*.\n    *   **GPF**: A learnable vector `p` (of dimension `F`, matching node feature dimensionality) is *added to all node features* `xi` in the input graph `X` to generate prompted features `X* = {x1+p, ..., xN+p}`. The pre-trained GNN model parameters are frozen, and only `p` and a learnable projection head `θ` are optimized for the downstream task.\n    *   **GPF-plus**: A theoretically stronger variant of GPF that incorporates *different* learnable prompted features for *different nodes* in the graph, allowing for more nuanced and adaptive input modifications.\n*   **Novelty/Difference**:\n    *   **Universality**: GPF is designed to be compatible with *any* pre-trained GNN model and *any* pre-training strategy, eliminating the need for strategy-specific manual prompting function design.\n    *   **Feature Space Manipulation**: Unlike methods that modify graph structure (e.g., adding virtual nodes/edges), GPF directly adapts the input node features, drawing inspiration from pixel-level visual prompts in computer vision.\n    *   **Theoretical Foundation**: The paper provides rigorous derivations to demonstrate that GPF can theoretically achieve an equivalent effect to *any* form of prompting function, offering a strong theoretical basis for its universality and effectiveness, which was lacking in prior intuitive designs.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The introduction of **Graph Prompt Feature (GPF)** and its variant **GPF-plus**, representing the first universal prompt-based tuning methods for pre-trained GNNs.\n    *   A novel paradigm for GNN prompt tuning by operating on the *input graph's feature space* rather than altering graph structure or fine-tuning model parameters.\n*   **Theoretical Insights/Analysis**:\n    *   Rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to *any* prompting function, thereby guaranteeing its broad applicability.\n    *   Theoretical proofs that GPF and GPF-plus are not weaker than full fine-tuning and can, in some cases, yield *superior* theoretical tuning results.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**:\n    *   Extensive experiments were conducted across various pre-training strategies (e.g., those involving edge prediction and attribute masking) and GNN architectures.\n    *   Evaluations were performed in both **full-shot** (sufficient labeled data) and **few-shot** (limited labeled data) scenarios.\n    *   Performance was compared against traditional **fine-tuning** and existing **specialized prompt-based tuning methods** (when applicable to their specialized pre-training strategies).\n*   **Key Performance Metrics & Comparison Results**:\n    *   GPF and GPF-plus consistently outperformed fine-tuning, despite utilizing significantly fewer tunable parameters.\n    *   **Average performance improvement over fine-tuning**: Approximately **1.4%** in full-shot scenarios and a more substantial **3.2%** in few-shot scenarios, highlighting its effectiveness in data-scarce environments.\n    *   **Superiority over specialized prompts**: GPF and GPF-plus significantly outperformed existing specialized prompt-based tuning methods even when applied to models pre-trained with the specific strategies those specialized methods were designed for (e.g., edge prediction).\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus and detailed validation in the main paper are on graph classification tasks, with extensions to node-wise tasks (node classification, link prediction) mentioned as being in the appendix (not provided in the excerpt).\n    *   While GPF-plus is described as theoretically stronger, the specific implementation details of how \"different prompted features for different nodes\" are learned and applied are not fully elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   Universally applicable to *any* pre-trained GNN model, regardless of its specific pre-training strategy.\n    *   Applicable across various GNN architectures.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   `\\cite{fang2022tjj}` makes a significant advancement by introducing the first *universal* prompt-based tuning method for GNNs, effectively addressing the challenge posed by diverse pre-training strategies.\n    *   It provides a more efficient, effective, and robust alternative to fine-tuning, particularly beneficial in data-limited settings, by adapting the input space while keeping the pre-trained model frozen.\n    *   The work establishes a strong theoretical foundation for GNN prompt tuning, moving beyond heuristic designs.\n*   **Potential Impact on Future Research**:\n    *   This work opens new research directions for developing universal adaptation strategies for GNNs and potentially other complex data structures with varied pre-training paradigms.\n    *   It could lead to the development of more generalizable and easily adaptable pre-trained GNN models, accelerating their deployment in real-world applications.\n    *   It encourages further theoretical and empirical investigation into the mechanisms and optimal designs of prompt tuning for graph-structured data.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "prompt tuning",
        "pre-trained GNNs",
        "universal prompt tuning",
        "Graph Prompt Feature (GPF)",
        "input feature space manipulation",
        "theoretical guarantees",
        "few-shot learning",
        "catastrophic forgetting",
        "objective misalignment",
        "fine-tuning",
        "downstream tasks"
      ],
      "paper_type": "this paper introduces a new method called graph prompt feature (gpf) for tuning pre-trained graph neural networks. it describes the problem, proposes a novel solution (gpf), provides \"rigorous derivations to demonstrate the universality of gpf,\" and presents \"experimental results\" to show its effectiveness.\n\nbased on the criteria:\n*   it **proposes** a new **method** (\"we introduce a universal prompt-based tuning method called graph prompt feature (gpf)\"). this aligns directly with the **technical** definition.\n*   it includes \"rigorous derivations,\" which points to a theoretical component.\n*   it presents \"experimental results,\" which points to an empirical component.\n\nhowever, the core contribution is the *development and presentation of a new method*. the theoretical analysis and empirical evaluation serve to support and validate this new technical contribution. therefore, the primary classification is **technical**.\n\n**technical**"
    },
    "file_name": "85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf"
  },
  {
    "success": true,
    "doc_id": "39e3eb369079c60b5f00228054766a6e",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the inadequate exploration of representational properties and limitations of existing Interpretable Graph Neural Networks (XGNNs), particularly their ability to provide faithful interpretations and generalize reliably. It identifies a \"huge gap\" in how current attention-based XGNNs approximate the underlying subgraph distribution, leading to degenerated interpretability.\n    *   **Importance and Challenge**: XGNNs are widely adopted in scientific applications (e.g., Physics, Chemistry, Materials), where faithful interpretation of predictions and generalization to unseen or Out-of-Distribution (OOD) graphs are crucial for scientific discovery and practice. The challenge lies in developing XGNNs that can reliably extract causal subgraphs and make robust predictions, while also providing theoretically grounded and empirically verifiable interpretations.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the paradigm of intrinsic XGNNs, which aim to provide reliable explanations and OOD generalizable predictions, contrasting with post-hoc explanation methods that are often suboptimal and sensitive to pre-trained GNN performance. Existing XGNNs typically use an information bottleneck or causal invariance to extract subgraphs, often employing attention mechanisms to learn edge/node sampling probabilities.\n    *   **Limitations of Previous Solutions**: Previous XGNNs, despite their success, lack a strong theoretical understanding of their representational properties and faithfulness. Specifically, the paper argues that the prevalent attention-based paradigm, which approximates the expected prediction over subgraphs (`E[f_c(G_c)]`) by applying the classifier to an expected \"soft\" subgraph (`f_c(E[G_c])`), is fundamentally flawed. This approximation only holds for linear classifiers, which is not the case for multi-layer GNNs, leading to a significant \"Subgraph Multilinear Extension (SubMT) approximation failure.\"\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a theoretical framework that formulates interpretable subgraph learning as approximating the **Subgraph Multilinear Extension (SubMT)**. SubMT is defined as the expected value of the subgraph classifier's output over a subgraph distribution, where edge sampling probabilities are derived from attention scores.\n    *   **Novelty**:\n        *   **SubMT Framework**: It's the first to propose SubMT as a theoretical lens to analyze the expressivity and faithfulness of XGNNs.\n        *   **Identification of Approximation Failure**: It rigorously proves (Proposition 3.3) that existing attention-based XGNNs, particularly those with non-linear GNNs (k > 1 layers), cannot accurately approximate SubMT due to the non-linearity of the classifier, leading to a gap between `f_c(E[G_c])` and `E[f_c(G_c)]`.\n        *   **Graph Multilinear ne T (GMT) Architecture**: To mitigate this, \\cite{chen2024woq} proposes GMT, a new XGNN architecture. GMT's core innovation is a new paradigm to effectively approximate SubMT by first performing **random subgraph sampling** onto the subgraph distribution. This directly estimates `E[f_c(G_c)]` more accurately than `f_c(E[G_c])`. A new classifier is then trained on these sampled subgraphs.\n        *   **Counterfactual Fidelity**: Introduces a novel faithfulness measure, \"(δ, ϵ)-counterfactual fidelity,\" which quantifies how sensitive predictions are to perturbations in the extracted interpretable subgraphs, directly linking interpretability to prediction robustness.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Framework**: The introduction of SubMT as the first theoretical framework for characterizing the expressivity of XGNNs.\n    *   **Formal Proof of Limitations**: A formal proof demonstrating the inherent approximation failure of existing attention-based XGNNs in fitting SubMT.\n    *   **Novel Architecture (GMT)**: The design of Graph Multilinear ne T (GMT), an XGNN architecture that is provably more powerful in approximating SubMT through random subgraph sampling.\n    *   **New Interpretability Metric**: The proposal of (δ, ϵ)-counterfactual fidelity, a novel measure for the causal faithfulness of XGNNs, addressing limitations of existing metrics for post-hoc explanations.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on a number of graph classification benchmarks, including 12 regular and geometric graph datasets. The validation aimed to empirically confirm the theoretical findings regarding SubMT approximation failure and demonstrate the superior performance of GMT.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Interpretability and Generalizability**: GMT significantly outperforms state-of-the-art XGNNs, achieving up to a 10% improvement in both interpretability and generalizability.\n        *   **Counterfactual Fidelity**: Experiments (e.g., on BA-2Motifs and Mutag, Fig. 2b, 2c) show that existing XGNNs (like GSAT) exhibit low counterfactual fidelity, while GMT (with different sampling configurations, e.g., GMT-sam-10, GMT-sam-100) achieves higher fidelity, validating the proposed interpretability measure and GMT's effectiveness.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on edge-centric subgraph sampling, though it states the method can be generalized to node-centric approaches. The SubMT formulation assumes a factorized Bernoulli distribution for subgraph sampling, which implicitly aligns with random graph data models. The computational cost of random subgraph sampling, while more accurate, might be higher than direct \"soft\" subgraph processing, though the paper doesn't explicitly detail this trade-off in the provided abstract/introduction.\n    *   **Scope of Applicability**: The framework and proposed GMT architecture are primarily applicable to intrinsic interpretable GNNs for graph classification tasks, with potential generalization to node-level tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chen2024woq} significantly advances the theoretical understanding of XGNNs by introducing the SubMT framework, formally identifying a critical limitation in existing attention-based methods, and proposing a provably more powerful architecture (GMT) to overcome it. This leads to substantial empirical improvements in both interpretability and OOD generalization.\n    *   **Potential Impact on Future Research**: The SubMT framework provides a new theoretical foundation for analyzing and designing XGNNs, potentially guiding the development of more expressive and faithful interpretable models. The concept of counterfactual fidelity offers a robust metric for evaluating XGNN interpretability. GMT's approach of using random subgraph sampling to approximate expectations could inspire new architectural designs for GNNs dealing with discrete graph structures and probabilistic interpretations.",
    "intriguing_abstract": "The promise of Interpretable Graph Neural Networks (XGNNs) for scientific discovery hinges on their ability to provide faithful explanations and generalize reliably. However, current attention-based XGNNs suffer from a critical, unaddressed theoretical limitation: a fundamental \"Subgraph Multilinear Extension (SubMT) approximation failure.\" We rigorously prove that existing methods, by approximating the expected prediction over subgraphs with the prediction over an expected \"soft\" subgraph, are inherently flawed for non-linear GNNs, leading to degenerated interpretability and poor out-of-distribution (OOD) generalization.\n\nTo overcome this, we introduce the SubMT framework, the first theoretical lens to characterize XGNN expressivity and faithfulness. Building on this, we propose Graph Multilinear ne T (GMT), a novel XGNN architecture. GMT directly approximates SubMT through a paradigm of **random subgraph sampling**, provably yielding more accurate estimations than prior approaches. Furthermore, we introduce **(δ, ϵ)-counterfactual fidelity**, a robust metric for evaluating causal faithfulness. Extensive experiments demonstrate GMT's superior performance, achieving up to 10% improvements in both interpretability and OOD generalization over state-of-the-art methods. This work provides a new theoretical foundation for XGNN design, paving the way for truly faithful and robust graph interpretations.",
    "keywords": [
      "Interpretable Graph Neural Networks (XGNNs)",
      "Subgraph Multilinear Extension (SubMT)",
      "Graph Multilinear ne T (GMT)",
      "attention-based XGNNs limitations",
      "SubMT approximation failure",
      "random subgraph sampling",
      "(δ",
      "ϵ)-counterfactual fidelity",
      "Out-of-Distribution (OOD) generalization",
      "faithful interpretations",
      "theoretical framework",
      "graph classification",
      "causal subgraphs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/854342cf063eef4428a5441c8d317dfbabb8117f.pdf",
    "citation_key": "chen2024woq",
    "metadata": {
      "title": "How Interpretable Are Interpretable Graph Neural Networks?",
      "authors": [
        "Yongqiang Chen",
        "Yatao Bian",
        "Bo Han",
        "James Cheng"
      ],
      "published_date": "2024",
      "abstract": "Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.",
      "file_path": "paper_data/Graph_Neural_Networks/854342cf063eef4428a5441c8d317dfbabb8117f.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the inadequate exploration of representational properties and limitations of existing Interpretable Graph Neural Networks (XGNNs), particularly their ability to provide faithful interpretations and generalize reliably. It identifies a \"huge gap\" in how current attention-based XGNNs approximate the underlying subgraph distribution, leading to degenerated interpretability.\n    *   **Importance and Challenge**: XGNNs are widely adopted in scientific applications (e.g., Physics, Chemistry, Materials), where faithful interpretation of predictions and generalization to unseen or Out-of-Distribution (OOD) graphs are crucial for scientific discovery and practice. The challenge lies in developing XGNNs that can reliably extract causal subgraphs and make robust predictions, while also providing theoretically grounded and empirically verifiable interpretations.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the paradigm of intrinsic XGNNs, which aim to provide reliable explanations and OOD generalizable predictions, contrasting with post-hoc explanation methods that are often suboptimal and sensitive to pre-trained GNN performance. Existing XGNNs typically use an information bottleneck or causal invariance to extract subgraphs, often employing attention mechanisms to learn edge/node sampling probabilities.\n    *   **Limitations of Previous Solutions**: Previous XGNNs, despite their success, lack a strong theoretical understanding of their representational properties and faithfulness. Specifically, the paper argues that the prevalent attention-based paradigm, which approximates the expected prediction over subgraphs (`E[f_c(G_c)]`) by applying the classifier to an expected \"soft\" subgraph (`f_c(E[G_c])`), is fundamentally flawed. This approximation only holds for linear classifiers, which is not the case for multi-layer GNNs, leading to a significant \"Subgraph Multilinear Extension (SubMT) approximation failure.\"\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a theoretical framework that formulates interpretable subgraph learning as approximating the **Subgraph Multilinear Extension (SubMT)**. SubMT is defined as the expected value of the subgraph classifier's output over a subgraph distribution, where edge sampling probabilities are derived from attention scores.\n    *   **Novelty**:\n        *   **SubMT Framework**: It's the first to propose SubMT as a theoretical lens to analyze the expressivity and faithfulness of XGNNs.\n        *   **Identification of Approximation Failure**: It rigorously proves (Proposition 3.3) that existing attention-based XGNNs, particularly those with non-linear GNNs (k > 1 layers), cannot accurately approximate SubMT due to the non-linearity of the classifier, leading to a gap between `f_c(E[G_c])` and `E[f_c(G_c)]`.\n        *   **Graph Multilinear ne T (GMT) Architecture**: To mitigate this, \\cite{chen2024woq} proposes GMT, a new XGNN architecture. GMT's core innovation is a new paradigm to effectively approximate SubMT by first performing **random subgraph sampling** onto the subgraph distribution. This directly estimates `E[f_c(G_c)]` more accurately than `f_c(E[G_c])`. A new classifier is then trained on these sampled subgraphs.\n        *   **Counterfactual Fidelity**: Introduces a novel faithfulness measure, \"(δ, ϵ)-counterfactual fidelity,\" which quantifies how sensitive predictions are to perturbations in the extracted interpretable subgraphs, directly linking interpretability to prediction robustness.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Framework**: The introduction of SubMT as the first theoretical framework for characterizing the expressivity of XGNNs.\n    *   **Formal Proof of Limitations**: A formal proof demonstrating the inherent approximation failure of existing attention-based XGNNs in fitting SubMT.\n    *   **Novel Architecture (GMT)**: The design of Graph Multilinear ne T (GMT), an XGNN architecture that is provably more powerful in approximating SubMT through random subgraph sampling.\n    *   **New Interpretability Metric**: The proposal of (δ, ϵ)-counterfactual fidelity, a novel measure for the causal faithfulness of XGNNs, addressing limitations of existing metrics for post-hoc explanations.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on a number of graph classification benchmarks, including 12 regular and geometric graph datasets. The validation aimed to empirically confirm the theoretical findings regarding SubMT approximation failure and demonstrate the superior performance of GMT.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Interpretability and Generalizability**: GMT significantly outperforms state-of-the-art XGNNs, achieving up to a 10% improvement in both interpretability and generalizability.\n        *   **Counterfactual Fidelity**: Experiments (e.g., on BA-2Motifs and Mutag, Fig. 2b, 2c) show that existing XGNNs (like GSAT) exhibit low counterfactual fidelity, while GMT (with different sampling configurations, e.g., GMT-sam-10, GMT-sam-100) achieves higher fidelity, validating the proposed interpretability measure and GMT's effectiveness.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on edge-centric subgraph sampling, though it states the method can be generalized to node-centric approaches. The SubMT formulation assumes a factorized Bernoulli distribution for subgraph sampling, which implicitly aligns with random graph data models. The computational cost of random subgraph sampling, while more accurate, might be higher than direct \"soft\" subgraph processing, though the paper doesn't explicitly detail this trade-off in the provided abstract/introduction.\n    *   **Scope of Applicability**: The framework and proposed GMT architecture are primarily applicable to intrinsic interpretable GNNs for graph classification tasks, with potential generalization to node-level tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chen2024woq} significantly advances the theoretical understanding of XGNNs by introducing the SubMT framework, formally identifying a critical limitation in existing attention-based methods, and proposing a provably more powerful architecture (GMT) to overcome it. This leads to substantial empirical improvements in both interpretability and OOD generalization.\n    *   **Potential Impact on Future Research**: The SubMT framework provides a new theoretical foundation for analyzing and designing XGNNs, potentially guiding the development of more expressive and faithful interpretable models. The concept of counterfactual fidelity offers a robust metric for evaluating XGNN interpretability. GMT's approach of using random subgraph sampling to approximate expectations could inspire new architectural designs for GNNs dealing with discrete graph structures and probabilistic interpretations.",
      "keywords": [
        "Interpretable Graph Neural Networks (XGNNs)",
        "Subgraph Multilinear Extension (SubMT)",
        "Graph Multilinear ne T (GMT)",
        "attention-based XGNNs limitations",
        "SubMT approximation failure",
        "random subgraph sampling",
        "(δ",
        "ϵ)-counterfactual fidelity",
        "Out-of-Distribution (OOD) generalization",
        "faithful interpretations",
        "theoretical framework",
        "graph classification",
        "causal subgraphs"
      ],
      "paper_type": "the paper should be classified as **theoretical**.\n\nhere's why:\n\n1.  **theoretical framework:** the abstract explicitly states, \"we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (submt).\" this directly aligns with the \"theoretical\" criterion's mention of \"formal models\" and \"mathematical analysis.\"\n2.  **mathematical analysis/proofs:** the abstract identifies a \"huge gap in fitting submt\" for existing methods and then states that the proposed architecture, gmt, \"is provably more powerful in approximating submt.\" the term \"provably\" indicates mathematical analysis and proofs, which are key characteristics of theoretical papers.\n3.  **foundation for technical solution:** while the paper also \"design[s] a new xgnn architecture called graph multilinear ne t(gmt)\" (a technical contribution) and \"empirically validate[s] our theoretical findings\" (an empirical contribution), the new architecture is presented as a solution *derived from* and *provably superior* according to the theoretical framework. the theoretical understanding of submt and its approximation failure is the core insight that drives the development of gmt.\n\nthe theoretical contribution (the submt framework and the analysis of existing xgnns' limitations within this framework) appears to be the foundational and primary contribution, with the technical solution and empirical validation serving to support and demonstrate this theoretical work."
    },
    "file_name": "854342cf063eef4428a5441c8d317dfbabb8117f.pdf"
  },
  {
    "success": true,
    "doc_id": "dd9b49ad2cc596a35a84c56366030fe2",
    "summary": "Here's a focused summary of the paper \"GraphPrompt : Unifying Pre-Training and Downstream Tasks for Graph Neural Networks\" \\cite{liu2023ent} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) typically require large amounts of task-specific labeled data for supervised learning. The \"pre-train, fine-tune\" paradigm, while reducing labeling, suffers from inconsistent objectives between pre-training and downstream tasks, leading to suboptimal performance. Existing graph prompting methods are limited, often only supporting specific tasks like node classification, lacking a universal approach for diverse downstream tasks (e.g., both node and graph classification).\n    *   **Importance & Challenge**: Reducing the reliance on extensive labeled data is crucial for GNN applicability. The challenge lies in: 1) unifying pre-training and various downstream tasks into a common framework to enable effective knowledge transfer, and 2) designing effective, task-specific prompts for graphs that can guide the pre-trained model without fine-tuning, similar to advancements in NLP.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon graph representation learning (graph embedding, GNNs) and graph pre-training techniques. It draws inspiration from prompting in natural language processing (NLP) to bridge the pre-training/downstream task gap.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional GNNs: Heavily dependent on large, task-specific labeled data.\n        *   \"Pre-train, fine-tune\" GNNs: Suffer from objective inconsistency between pre-training (e.g., link prediction) and downstream tasks (e.g., node classification), limiting generalization.\n        *   Meta-learning (e.g., L2P-GNN): Simulates fine-tuning but doesn't fundamentally address the objective discrepancy if downstream tasks differ from simulation.\n        *   Prior graph prompting (e.g., GPPT): Limited to specific tasks (e.g., node classification), lacking a universal design for different downstream tasks like graph classification.\n        *   Other \"GraphPrompt\" models: Some exist but focus on NLP tasks with auxiliary graphs, not general GNN pre-training and prompting.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liu2023ent} proposes **GraphPrompt**, a novel framework that unifies pre-training and downstream tasks (node and graph classification) into a common \"subgraph similarity\" template, and employs task-specific learnable prompts to guide the `ReadOut` operation.\n    *   **Novelty/Difference**:\n        *   **Unified Task Template**: Both pre-training (link prediction) and downstream tasks (node/graph classification) are reformulated as calculating the similarity between (sub)graph representations. This is achieved by representing all instances (nodes, graphs) as subgraphs: a node's contextual subgraph for node-level tasks, and the entire graph as its maximum subgraph for graph-level tasks.\n        *   **Learnable Prompts for `ReadOut`**: Instead of fine-tuning the entire GNN, \\cite{liu2023ent} introduces a novel task-specific learnable prompt that modifies the `ReadOut` operation. This prompt acts as parameters for the aggregation function, allowing different downstream tasks to adaptively fuse node representations into a subgraph representation, thereby \"pulling\" relevant knowledge from the frozen pre-trained model in a task-specific manner.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unification framework, **GraphPrompt**, that casts both pre-training (link prediction) and diverse downstream tasks (node and graph classification) as instances of learning subgraph similarity.\n        *   A novel prompting strategy that utilizes a learnable prompt to guide the `ReadOut` operation for task-specific aggregation, enabling effective exploitation of the pre-trained model without fine-tuning.\n    *   **System Design/Architectural Innovations**: The framework integrates a pre-trained GNN with a flexible prompting mechanism that adapts the `ReadOut` function based on the downstream task, allowing a single pre-trained model to serve multiple tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on five public datasets. The evaluation focuses on few-shot learning settings for both node and graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**: The paper demonstrates that **GraphPrompt** achieves superior performance compared to state-of-the-art approaches across the evaluated datasets, particularly in few-shot scenarios. This indicates its effectiveness in transferring knowledge and adapting to new tasks with limited labels.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations of **GraphPrompt** itself. The framework assumes the availability of label-free graphs for pre-training and focuses on the few-shot learning setting for downstream tasks.\n    *   **Scope of Applicability**: Primarily applicable to graph representation learning using GNNs, specifically for pre-training and subsequent adaptation to node classification and graph classification tasks, especially under limited supervision (few-shot learning). The unification is based on graph topology and subgraph similarity.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023ent} significantly advances the state-of-the-art by providing a universal and unified framework for GNN pre-training and prompting that can flexibly support diverse downstream tasks (node and graph classification). It effectively bridges the objective gap between pre-training and downstream tasks, a long-standing challenge in GNNs.\n    *   **Potential Impact**: This work has the potential to make GNNs more efficient and broadly applicable by reducing the need for extensive task-specific labeled data. It opens new avenues for research into more sophisticated graph prompting mechanisms and unified pre-training objectives, potentially leading to more robust and generalizable GNN models.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their widespread application is often challenged by the intensive need for labeled data and the objective inconsistency inherent in the \"pre-train, fine-tune\" paradigm. Existing graph prompting methods are typically task-specific, failing to offer a universal solution for diverse downstream tasks. We introduce **GraphPrompt**, a novel framework that revolutionizes GNN pre-training and downstream task adaptation. GraphPrompt unifies diverse tasks, including link prediction, node classification, and graph classification, into a common \"subgraph similarity\" template. Its core innovation lies in task-specific learnable prompts that adaptively guide the GNN's `ReadOut` operation, effectively transferring knowledge from a frozen pre-trained model without fine-tuning. This elegant approach significantly reduces reliance on labeled data, achieving superior performance in few-shot learning scenarios across multiple datasets. GraphPrompt bridges a critical gap in GNN research, paving the way for more efficient, adaptable, and generalizable graph learning models.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Pre-training",
      "Graph prompting",
      "Downstream tasks unification",
      "Node classification",
      "Graph classification",
      "GraphPrompt framework",
      "Subgraph similarity",
      "Learnable prompts for ReadOut",
      "Few-shot learning",
      "Bridging objective gap",
      "Knowledge transfer",
      "Limited labeled data"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf",
    "citation_key": "liu2023ent",
    "metadata": {
      "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
      "authors": [
        "Zemin Liu",
        "Xingtong Yu",
        "Yuan Fang",
        "Xinming Zhang"
      ],
      "published_date": "2023",
      "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
      "file_path": "paper_data/Graph_Neural_Networks/e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf",
      "venue": "The Web Conference",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"GraphPrompt : Unifying Pre-Training and Downstream Tasks for Graph Neural Networks\" \\cite{liu2023ent} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) typically require large amounts of task-specific labeled data for supervised learning. The \"pre-train, fine-tune\" paradigm, while reducing labeling, suffers from inconsistent objectives between pre-training and downstream tasks, leading to suboptimal performance. Existing graph prompting methods are limited, often only supporting specific tasks like node classification, lacking a universal approach for diverse downstream tasks (e.g., both node and graph classification).\n    *   **Importance & Challenge**: Reducing the reliance on extensive labeled data is crucial for GNN applicability. The challenge lies in: 1) unifying pre-training and various downstream tasks into a common framework to enable effective knowledge transfer, and 2) designing effective, task-specific prompts for graphs that can guide the pre-trained model without fine-tuning, similar to advancements in NLP.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon graph representation learning (graph embedding, GNNs) and graph pre-training techniques. It draws inspiration from prompting in natural language processing (NLP) to bridge the pre-training/downstream task gap.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional GNNs: Heavily dependent on large, task-specific labeled data.\n        *   \"Pre-train, fine-tune\" GNNs: Suffer from objective inconsistency between pre-training (e.g., link prediction) and downstream tasks (e.g., node classification), limiting generalization.\n        *   Meta-learning (e.g., L2P-GNN): Simulates fine-tuning but doesn't fundamentally address the objective discrepancy if downstream tasks differ from simulation.\n        *   Prior graph prompting (e.g., GPPT): Limited to specific tasks (e.g., node classification), lacking a universal design for different downstream tasks like graph classification.\n        *   Other \"GraphPrompt\" models: Some exist but focus on NLP tasks with auxiliary graphs, not general GNN pre-training and prompting.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liu2023ent} proposes **GraphPrompt**, a novel framework that unifies pre-training and downstream tasks (node and graph classification) into a common \"subgraph similarity\" template, and employs task-specific learnable prompts to guide the `ReadOut` operation.\n    *   **Novelty/Difference**:\n        *   **Unified Task Template**: Both pre-training (link prediction) and downstream tasks (node/graph classification) are reformulated as calculating the similarity between (sub)graph representations. This is achieved by representing all instances (nodes, graphs) as subgraphs: a node's contextual subgraph for node-level tasks, and the entire graph as its maximum subgraph for graph-level tasks.\n        *   **Learnable Prompts for `ReadOut`**: Instead of fine-tuning the entire GNN, \\cite{liu2023ent} introduces a novel task-specific learnable prompt that modifies the `ReadOut` operation. This prompt acts as parameters for the aggregation function, allowing different downstream tasks to adaptively fuse node representations into a subgraph representation, thereby \"pulling\" relevant knowledge from the frozen pre-trained model in a task-specific manner.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unification framework, **GraphPrompt**, that casts both pre-training (link prediction) and diverse downstream tasks (node and graph classification) as instances of learning subgraph similarity.\n        *   A novel prompting strategy that utilizes a learnable prompt to guide the `ReadOut` operation for task-specific aggregation, enabling effective exploitation of the pre-trained model without fine-tuning.\n    *   **System Design/Architectural Innovations**: The framework integrates a pre-trained GNN with a flexible prompting mechanism that adapts the `ReadOut` function based on the downstream task, allowing a single pre-trained model to serve multiple tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on five public datasets. The evaluation focuses on few-shot learning settings for both node and graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**: The paper demonstrates that **GraphPrompt** achieves superior performance compared to state-of-the-art approaches across the evaluated datasets, particularly in few-shot scenarios. This indicates its effectiveness in transferring knowledge and adapting to new tasks with limited labels.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations of **GraphPrompt** itself. The framework assumes the availability of label-free graphs for pre-training and focuses on the few-shot learning setting for downstream tasks.\n    *   **Scope of Applicability**: Primarily applicable to graph representation learning using GNNs, specifically for pre-training and subsequent adaptation to node classification and graph classification tasks, especially under limited supervision (few-shot learning). The unification is based on graph topology and subgraph similarity.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023ent} significantly advances the state-of-the-art by providing a universal and unified framework for GNN pre-training and prompting that can flexibly support diverse downstream tasks (node and graph classification). It effectively bridges the objective gap between pre-training and downstream tasks, a long-standing challenge in GNNs.\n    *   **Potential Impact**: This work has the potential to make GNNs more efficient and broadly applicable by reducing the need for extensive task-specific labeled data. It opens new avenues for research into more sophisticated graph prompting mechanisms and unified pre-training objectives, potentially leading to more robust and generalizable GNN models.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Pre-training",
        "Graph prompting",
        "Downstream tasks unification",
        "Node classification",
        "Graph classification",
        "GraphPrompt framework",
        "Subgraph similarity",
        "Learnable prompts for ReadOut",
        "Few-shot learning",
        "Bridging objective gap",
        "Knowledge transfer",
        "Limited labeled data"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose graphprompt, a novel pre-training and prompting framework on graphs.\"** - this is a strong indicator of a **technical** paper, as it explicitly states the proposal of a new method/framework.\n2.  **\"graphprompt not only unifies pre-training and downstream tasks... but also employs a learnable prompt...\"** - this describes the technical details and features of the proposed solution.\n3.  **\"finally, we conduct extensive experiments on five public datasets to evaluate and analyze graphprompt.\"** - while this indicates an empirical component, the experiments are conducted to *evaluate* the *proposed* framework. this is standard practice for technical papers presenting new methods. the primary contribution is the framework itself.\n4.  the introduction sets up a technical problem (gnn reliance on supervision, retraining for new tasks) that the proposed framework aims to solve.\n\nthe paper's core contribution is the development and presentation of a new system/framework (graphprompt). the empirical evaluation serves to validate this technical contribution.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf"
  },
  {
    "success": true,
    "doc_id": "6d44fb410063234ae34299bece5e4740",
    "summary": "Here's a focused summary of the technical paper \\cite{dong20225aw} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of effectively processing and learning from the massive, complex, and often non-Euclidean data generated by heterogeneous Internet of Things (IoT) systems. Traditional deep learning models (e.g., CNNs, RNNs) struggle to fully capture the intricate interactions and interdependencies within sensor topologies and among data instances over time in IoT environments. \\cite{dong20225aw}\n    *   **Importance and Challenge:** The IoT boom generates vast amounts of multi-modal sensory data. Handcrafted feature engineering is inefficient, requires significant domain knowledge, and lacks generalizability. While CNNs and RNNs are used, they are limited to fixed-size grid or line graph structures, failing to leverage the inherent complex, non-Euclidean graph-like relationships (e.g., sensor networks, device interactions, spatio-temporal dependencies) prevalent in many IoT scenarios. Graph Neural Networks (GNNs) offer a promising solution to model these complex relationships. \\cite{dong20225aw}\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous survey papers on GNNs have focused on their theoretical foundations, methodologies, and general applications across various domains like traffic forecasting, social recommendation, natural language processing, and action recognition. \\cite{dong20225aw}\n    *   **Limitations of Previous Solutions:** A significant gap existed where \"few works systematically summarize the applications of GNN models in IoT sensing solutions,\" and specifically, \"no survey work has yet focused on the applications of GNN in IoT.\" \\cite{dong20225aw}\n    *   **Positioning of this Work:** This paper aims to fill this gap by providing the first comprehensive and systematic review specifically connecting GNNs and IoT sensing technology, detailing how GNNs are leveraged to model problems using networked IoT solutions. \\cite{dong20225aw}\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** As a survey paper, the core method is a systematic literature review and synthesis. The authors establish a structured framework for analyzing GNN applications in IoT. \\cite{dong20225aw}\n    *   **Novelty/Difference:** The innovation lies in its focused scope and structured approach to this specific intersection. The authors:\n        *   Propose a unified framework for IoT sensing paradigms, categorizing them into human sensing, autonomous things, and environmental sensing. \\cite{dong20225aw}\n        *   Define a novel taxonomy for GNN modeling in IoT sensing, classifying methods based on how data is represented by graphs: multi-agent interaction, human state dynamics, and IoT sensor interconnection. \\cite{dong20225aw}\n        *   Conduct a deep dive analysis of GNN design within various IoT sensing environments. \\cite{dong20225aw}\n        *   Compile an overarching list of public datasets and source code from collected publications, maintaining a GitHub repository (GNN4IoT). \\cite{dong20225aw}\n\n*   **Key Technical Contributions**\n    *   **System Design/Architectural Innovations (for a survey):**\n        *   Introduction of a unified framework for categorizing IoT sensing paradigms (human, autonomous things, environment). \\cite{dong20225aw}\n        *   Development of a novel taxonomy for GNN applications in IoT sensing, based on graph representation (multi-agent interaction, human state dynamics, IoT sensor interconnection). \\cite{dong20225aw}\n    *   **Theoretical Insights/Analysis:**\n        *   Comprehensive review and synthesis of recent advances in GNN applications within the IoT field. \\cite{dong20225aw}\n        *   Identification and in-depth discussion of important challenges and future research directions for GNNs in IoT sensing. \\cite{dong20225aw}\n    *   **Practical Resources:**\n        *   Compilation of public data resources and open-source implementations, including the creation of a dedicated GitHub repository (GNN4IoT). \\cite{dong20225aw}\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** As a survey, this paper does not conduct new experiments. Instead, it synthesizes the experimental findings from the reviewed literature. \\cite{dong20225aw}\n    *   **Key Performance Metrics and Comparison Results:** The paper reports that GNNs have achieved \"remarkable success with exceeding or comparable performance to existing benchmarks\" across a wide range of IoT sensing applications, including traffic monitoring, remote scene classification, robotic grasping, movement analysis, energy management, and personal health. \\cite{dong20225aw} The specific metrics and comparisons are detailed within the individual papers cited by the survey.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of the survey):** The paper does not explicitly state limitations of its own survey methodology. However, inherent limitations of any survey include the potential for selection bias in reviewed papers and the challenge of keeping pace with the rapidly evolving field of GNNs and IoT.\n    *   **Scope of Applicability:** The survey's scope is strictly focused on the intersection of Graph Neural Networks and IoT sensing technologies, covering applications across human-centric, autonomous things, and environmental sensing paradigms. \\cite{dong20225aw}\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematic review dedicated to GNN applications in IoT sensing. It consolidates fragmented knowledge, offering a structured understanding of the field. \\cite{dong20225aw}\n    *   **Potential Impact on Future Research:**\n        *   It serves as a foundational resource for researchers and practitioners, guiding the application of GNNs to complex IoT problems. \\cite{dong20225aw}\n        *   By highlighting the benefits of GNNs for non-Euclidean IoT data, it encourages further exploration beyond traditional deep learning methods. \\cite{dong20225aw}\n        *   The identified challenges and future directions provide a roadmap for new research, fostering innovation in areas like GNN design for specific IoT environments, data efficiency, and interpretability. \\cite{dong20225aw}\n        *   The curated list of public datasets and open-source code facilitates reproducibility and accelerates further development in the domain. \\cite{dong20225aw}",
    "intriguing_abstract": "The explosion of Internet of Things (IoT) devices inundates us with massive, intricate, and inherently non-Euclidean data, overwhelming conventional deep learning architectures. Graph Neural Networks (GNNs) offer a powerful paradigm to model the complex interdependencies within sensor networks and spatio-temporal relationships, yet a systematic synthesis of GNN applications specifically within IoT sensing solutions has remained conspicuously absent. This paper fills that critical void, presenting the first comprehensive and systematic review dedicated to leveraging GNNs for IoT sensing. We introduce a unified framework categorizing IoT sensing paradigms (human, autonomous things, environmental) and propose a novel taxonomy for GNN modeling based on graph representations (multi-agent interaction, human state dynamics, IoT sensor interconnection). Our deep dive analyzes GNN designs across diverse IoT environments, revealing their remarkable success in various applications. Beyond theoretical insights, we compile an invaluable resource of public datasets and source code, maintained in our GNN4IoT GitHub repository. This work serves as a foundational guide, illuminating future research directions and accelerating the development of intelligent, graph-aware IoT solutions.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Internet of Things (IoT) sensing",
      "non-Euclidean data",
      "complex sensor topologies",
      "spatio-temporal dependencies",
      "systematic literature review",
      "unified framework",
      "novel taxonomy",
      "multi-agent interaction",
      "human state dynamics",
      "IoT sensor interconnection",
      "GNN4IoT repository",
      "challenges and future directions"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/d09608593caa20b79a8aaddfe19df7e31513d711.pdf",
    "citation_key": "dong20225aw",
    "metadata": {
      "title": "Graph Neural Networks in IoT: A Survey",
      "authors": [
        "Guimin Dong",
        "Mingyue Tang",
        "Zhiyuan Wang",
        "Jiechao Gao",
        "Sikun Guo",
        "Lihua Cai",
        "Robert Gutierrez",
        "Brad Campbell",
        "Laura E. Barnes",
        "M. Boukhechba"
      ],
      "published_date": "2022",
      "abstract": "The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.",
      "file_path": "paper_data/Graph_Neural_Networks/d09608593caa20b79a8aaddfe19df7e31513d711.pdf",
      "venue": "ACM Trans. Sens. Networks",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{dong20225aw} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of effectively processing and learning from the massive, complex, and often non-Euclidean data generated by heterogeneous Internet of Things (IoT) systems. Traditional deep learning models (e.g., CNNs, RNNs) struggle to fully capture the intricate interactions and interdependencies within sensor topologies and among data instances over time in IoT environments. \\cite{dong20225aw}\n    *   **Importance and Challenge:** The IoT boom generates vast amounts of multi-modal sensory data. Handcrafted feature engineering is inefficient, requires significant domain knowledge, and lacks generalizability. While CNNs and RNNs are used, they are limited to fixed-size grid or line graph structures, failing to leverage the inherent complex, non-Euclidean graph-like relationships (e.g., sensor networks, device interactions, spatio-temporal dependencies) prevalent in many IoT scenarios. Graph Neural Networks (GNNs) offer a promising solution to model these complex relationships. \\cite{dong20225aw}\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous survey papers on GNNs have focused on their theoretical foundations, methodologies, and general applications across various domains like traffic forecasting, social recommendation, natural language processing, and action recognition. \\cite{dong20225aw}\n    *   **Limitations of Previous Solutions:** A significant gap existed where \"few works systematically summarize the applications of GNN models in IoT sensing solutions,\" and specifically, \"no survey work has yet focused on the applications of GNN in IoT.\" \\cite{dong20225aw}\n    *   **Positioning of this Work:** This paper aims to fill this gap by providing the first comprehensive and systematic review specifically connecting GNNs and IoT sensing technology, detailing how GNNs are leveraged to model problems using networked IoT solutions. \\cite{dong20225aw}\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** As a survey paper, the core method is a systematic literature review and synthesis. The authors establish a structured framework for analyzing GNN applications in IoT. \\cite{dong20225aw}\n    *   **Novelty/Difference:** The innovation lies in its focused scope and structured approach to this specific intersection. The authors:\n        *   Propose a unified framework for IoT sensing paradigms, categorizing them into human sensing, autonomous things, and environmental sensing. \\cite{dong20225aw}\n        *   Define a novel taxonomy for GNN modeling in IoT sensing, classifying methods based on how data is represented by graphs: multi-agent interaction, human state dynamics, and IoT sensor interconnection. \\cite{dong20225aw}\n        *   Conduct a deep dive analysis of GNN design within various IoT sensing environments. \\cite{dong20225aw}\n        *   Compile an overarching list of public datasets and source code from collected publications, maintaining a GitHub repository (GNN4IoT). \\cite{dong20225aw}\n\n*   **Key Technical Contributions**\n    *   **System Design/Architectural Innovations (for a survey):**\n        *   Introduction of a unified framework for categorizing IoT sensing paradigms (human, autonomous things, environment). \\cite{dong20225aw}\n        *   Development of a novel taxonomy for GNN applications in IoT sensing, based on graph representation (multi-agent interaction, human state dynamics, IoT sensor interconnection). \\cite{dong20225aw}\n    *   **Theoretical Insights/Analysis:**\n        *   Comprehensive review and synthesis of recent advances in GNN applications within the IoT field. \\cite{dong20225aw}\n        *   Identification and in-depth discussion of important challenges and future research directions for GNNs in IoT sensing. \\cite{dong20225aw}\n    *   **Practical Resources:**\n        *   Compilation of public data resources and open-source implementations, including the creation of a dedicated GitHub repository (GNN4IoT). \\cite{dong20225aw}\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** As a survey, this paper does not conduct new experiments. Instead, it synthesizes the experimental findings from the reviewed literature. \\cite{dong20225aw}\n    *   **Key Performance Metrics and Comparison Results:** The paper reports that GNNs have achieved \"remarkable success with exceeding or comparable performance to existing benchmarks\" across a wide range of IoT sensing applications, including traffic monitoring, remote scene classification, robotic grasping, movement analysis, energy management, and personal health. \\cite{dong20225aw} The specific metrics and comparisons are detailed within the individual papers cited by the survey.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of the survey):** The paper does not explicitly state limitations of its own survey methodology. However, inherent limitations of any survey include the potential for selection bias in reviewed papers and the challenge of keeping pace with the rapidly evolving field of GNNs and IoT.\n    *   **Scope of Applicability:** The survey's scope is strictly focused on the intersection of Graph Neural Networks and IoT sensing technologies, covering applications across human-centric, autonomous things, and environmental sensing paradigms. \\cite{dong20225aw}\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematic review dedicated to GNN applications in IoT sensing. It consolidates fragmented knowledge, offering a structured understanding of the field. \\cite{dong20225aw}\n    *   **Potential Impact on Future Research:**\n        *   It serves as a foundational resource for researchers and practitioners, guiding the application of GNNs to complex IoT problems. \\cite{dong20225aw}\n        *   By highlighting the benefits of GNNs for non-Euclidean IoT data, it encourages further exploration beyond traditional deep learning methods. \\cite{dong20225aw}\n        *   The identified challenges and future directions provide a roadmap for new research, fostering innovation in areas like GNN design for specific IoT environments, data efficiency, and interpretability. \\cite{dong20225aw}\n        *   The curated list of public datasets and open-source code facilitates reproducibility and accelerates further development in the domain. \\cite{dong20225aw}",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Internet of Things (IoT) sensing",
        "non-Euclidean data",
        "complex sensor topologies",
        "spatio-temporal dependencies",
        "systematic literature review",
        "unified framework",
        "novel taxonomy",
        "multi-agent interaction",
        "human state dynamics",
        "IoT sensor interconnection",
        "GNN4IoT repository",
        "challenges and future directions"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "d09608593caa20b79a8aaddfe19df7e31513d711.pdf"
  },
  {
    "success": true,
    "doc_id": "e5901c4f5c545c2a404a8e24d683b871",
    "summary": "Here's a focused summary of the paper \\cite{fan2019k6u} for a literature review:\n\n### Technical Paper Analysis: Graph Neural Networks for Social Recommendation \\cite{fan2019k6u}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses challenges in building social recommender systems using Graph Neural Networks (GNNs). Specifically, it aims to effectively integrate diverse information sources—user-item interactions, associated opinions, and heterogeneous social relations—within a unified GNN framework to learn better user and item latent factors for recommendation.\n    *   **Importance and Challenge**:\n        *   **Importance**: Social relations are crucial for boosting recommendation performance, as users are influenced by their social circles. GNNs offer a powerful paradigm for learning representations on graph data, which naturally represents social and user-item interactions.\n        *   **Challenges**:\n            1.  **Dual Graph Integration**: Users are involved in two distinct graphs: a user-user social graph and a user-item interaction graph. Coherently combining information from both is challenging.\n            2.  **Interaction and Opinion Capture**: The user-item graph contains not just interactions but also users' explicit opinions (e.g., rating scores), which need to be jointly captured.\n            3.  **Heterogeneous Social Strengths**: Social networks often have varied tie strengths (strong vs. weak ties), and treating all social relations equally can degrade recommendation quality. Distinguishing these heterogeneous strengths is vital.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself within the growing field of social recommender systems and deep neural networks for graph data (GNNs).\n    *   **Limitations of Previous Solutions (Implied)**: Traditional social recommendation methods might not fully leverage the topological structure and node information simultaneously, or might struggle with the complex interplay of interactions, opinions, and heterogeneous social ties. Mean-based aggregators in GNNs, for instance, are noted as a limitation for not differentiating the importance of neighbors or interactions. The paper aims to overcome these by introducing attention mechanisms and opinion embeddings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **GraphRec**, a novel Graph Neural Network framework for social recommendations. GraphRec consists of three main components: user modeling, item modeling, and rating prediction.\n    *   **Novelty/Difference**:\n        *   **Unified Dual-Graph Modeling**: GraphRec coherently models both the user-user social graph and the user-item graph by learning user latent factors from two distinct perspectives: \"item-space\" (via user-item interactions) and \"social-space\" (via social connections), which are then combined.\n        *   **Opinion-Aware Interaction Representation**: For both user and item modeling, it introduces \"opinion embedding vectors\" (`er`) for each rating level. These are concatenated with item/user embeddings and processed through an MLP to create \"opinion-aware interaction representations\" (`xia` for items, `fjt` for users), allowing the model to jointly capture interactions and their associated opinions.\n        *   **Attention Mechanisms for Heterogeneity**:\n            *   **Item Attention (`αia`)**: Assigns individualized weights to each user-item interaction when learning item-space user latent factors, allowing different interactions to contribute differently based on their relevance to the target user.\n            *   **Social Attention (`βio`)**: Differentiates the importance (tie strengths) of social friends when learning social-space user latent factors, addressing the challenge of heterogeneous social relations.\n            *   **User Attention (`µjt`)**: Differentiates the importance of users interacting with an item when learning item latent factors, capturing heterogeneous influence from user-item interactions.\n        *   These attention mechanisms replace simpler mean-based aggregators, providing a more nuanced aggregation.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **GraphRec Framework**: A comprehensive GNN architecture specifically designed for social recommendation, integrating dual graphs and various forms of heterogeneity.\n        *   **Opinion Embedding and Opinion-Aware Interaction Representation**: A principled method to jointly capture interaction and opinion information by fusing item/user embeddings with dedicated opinion embeddings via MLPs.\n        *   **Attention-based Aggregation for Heterogeneous Information**: Introduction of three distinct attention mechanisms (item attention, social attention, user attention) to dynamically weigh the contributions of interactions, social friends, and interacting users, respectively, moving beyond uniform aggregation.\n    *   **System Design/Architectural Innovations**:\n        *   **Two-Stream User Modeling**: Learning user latent factors by separately aggregating information from the item-space (user-item graph) and social-space (user-user graph) and then combining them.\n        *   **Multi-Layer Perceptrons (MLPs)**: Used extensively for transforming and combining embeddings, and for parameterizing attention networks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted to demonstrate the effectiveness of the proposed GraphRec framework.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that experiments were performed on \"two real-world datasets\" and demonstrated the \"effectiveness of the proposed framework GraphRec.\" (Specific datasets and metrics are not detailed in the provided snippet but are typically found in the full paper's experimental section).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations or assumptions beyond the challenges it aims to solve. However, GNNs generally assume graph structure is available and can be computationally intensive for very large graphs. The model's reliance on explicit ratings for opinion embeddings might be a limitation in implicit feedback scenarios.\n    *   **Scope of Applicability**: The framework is designed for social recommender systems where both user-item interaction data (with opinions/ratings) and user-user social network data are available.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{fan2019k6u} advances the technical state-of-the-art by providing a principled and comprehensive GNN-based solution that simultaneously addresses three critical challenges in social recommendation: dual-graph integration, joint interaction-opinion modeling, and heterogeneous social tie strengths. Its use of attention mechanisms for fine-grained weighting of different information sources is a significant improvement over simpler aggregation methods.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in:\n        *   Developing more sophisticated attention mechanisms for complex graph structures.\n        *   Extending GNNs to handle other forms of heterogeneity (e.g., different types of items, multi-faceted social relations).\n        *   Applying similar dual-graph and opinion-aware modeling techniques to other domains beyond recommendation where multiple interconnected graphs and nuanced node attributes exist.\n        *   Exploring the interpretability of the learned attention weights to understand user preferences and social influence.",
    "intriguing_abstract": "Social recommender systems grapple with a fundamental challenge: effectively integrating rich user-item interactions, explicit opinions, and complex, heterogeneous social networks. While Graph Neural Networks (GNNs) offer powerful graph representation learning, coherently unifying these dual graphs and discerning the varying strengths of connections remains elusive. We present **GraphRec**, a novel GNN framework that addresses these limitations head-on. GraphRec uniquely learns robust user latent factors by simultaneously modeling information from both user-item and user-user social graphs. A core innovation is our opinion embedding mechanism, which generates opinion-aware interaction representations to jointly capture user preferences and their associated sentiments. Crucially, we introduce three distinct attention mechanisms—item, social, and user attention—to dynamically weigh diverse interactions and heterogeneous social ties, moving far beyond simplistic aggregations. This principled approach significantly enhances the model's ability to capture nuanced social influence and interaction dynamics. Extensive evaluations confirm GraphRec's superior performance, marking a significant advancement in building more intelligent and personalized social recommender systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Social recommender systems",
      "GraphRec framework",
      "Unified dual-graph modeling",
      "Opinion-aware interaction representation",
      "Attention mechanisms",
      "Heterogeneous social relations",
      "Two-stream user modeling",
      "User and item latent factors",
      "Item attention",
      "Social attention",
      "User attention"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf",
    "citation_key": "fan2019k6u",
    "metadata": {
      "title": "Graph Neural Networks for Social Recommendation",
      "authors": [
        "Wenqi Fan",
        "Yao Ma",
        "Qing Li",
        "Yuan He",
        "Y. Zhao",
        "Jiliang Tang",
        "Dawei Yin"
      ],
      "published_date": "2019",
      "abstract": "In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.",
      "file_path": "paper_data/Graph_Neural_Networks/398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf",
      "venue": "The Web Conference",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{fan2019k6u} for a literature review:\n\n### Technical Paper Analysis: Graph Neural Networks for Social Recommendation \\cite{fan2019k6u}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses challenges in building social recommender systems using Graph Neural Networks (GNNs). Specifically, it aims to effectively integrate diverse information sources—user-item interactions, associated opinions, and heterogeneous social relations—within a unified GNN framework to learn better user and item latent factors for recommendation.\n    *   **Importance and Challenge**:\n        *   **Importance**: Social relations are crucial for boosting recommendation performance, as users are influenced by their social circles. GNNs offer a powerful paradigm for learning representations on graph data, which naturally represents social and user-item interactions.\n        *   **Challenges**:\n            1.  **Dual Graph Integration**: Users are involved in two distinct graphs: a user-user social graph and a user-item interaction graph. Coherently combining information from both is challenging.\n            2.  **Interaction and Opinion Capture**: The user-item graph contains not just interactions but also users' explicit opinions (e.g., rating scores), which need to be jointly captured.\n            3.  **Heterogeneous Social Strengths**: Social networks often have varied tie strengths (strong vs. weak ties), and treating all social relations equally can degrade recommendation quality. Distinguishing these heterogeneous strengths is vital.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself within the growing field of social recommender systems and deep neural networks for graph data (GNNs).\n    *   **Limitations of Previous Solutions (Implied)**: Traditional social recommendation methods might not fully leverage the topological structure and node information simultaneously, or might struggle with the complex interplay of interactions, opinions, and heterogeneous social ties. Mean-based aggregators in GNNs, for instance, are noted as a limitation for not differentiating the importance of neighbors or interactions. The paper aims to overcome these by introducing attention mechanisms and opinion embeddings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **GraphRec**, a novel Graph Neural Network framework for social recommendations. GraphRec consists of three main components: user modeling, item modeling, and rating prediction.\n    *   **Novelty/Difference**:\n        *   **Unified Dual-Graph Modeling**: GraphRec coherently models both the user-user social graph and the user-item graph by learning user latent factors from two distinct perspectives: \"item-space\" (via user-item interactions) and \"social-space\" (via social connections), which are then combined.\n        *   **Opinion-Aware Interaction Representation**: For both user and item modeling, it introduces \"opinion embedding vectors\" (`er`) for each rating level. These are concatenated with item/user embeddings and processed through an MLP to create \"opinion-aware interaction representations\" (`xia` for items, `fjt` for users), allowing the model to jointly capture interactions and their associated opinions.\n        *   **Attention Mechanisms for Heterogeneity**:\n            *   **Item Attention (`αia`)**: Assigns individualized weights to each user-item interaction when learning item-space user latent factors, allowing different interactions to contribute differently based on their relevance to the target user.\n            *   **Social Attention (`βio`)**: Differentiates the importance (tie strengths) of social friends when learning social-space user latent factors, addressing the challenge of heterogeneous social relations.\n            *   **User Attention (`µjt`)**: Differentiates the importance of users interacting with an item when learning item latent factors, capturing heterogeneous influence from user-item interactions.\n        *   These attention mechanisms replace simpler mean-based aggregators, providing a more nuanced aggregation.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **GraphRec Framework**: A comprehensive GNN architecture specifically designed for social recommendation, integrating dual graphs and various forms of heterogeneity.\n        *   **Opinion Embedding and Opinion-Aware Interaction Representation**: A principled method to jointly capture interaction and opinion information by fusing item/user embeddings with dedicated opinion embeddings via MLPs.\n        *   **Attention-based Aggregation for Heterogeneous Information**: Introduction of three distinct attention mechanisms (item attention, social attention, user attention) to dynamically weigh the contributions of interactions, social friends, and interacting users, respectively, moving beyond uniform aggregation.\n    *   **System Design/Architectural Innovations**:\n        *   **Two-Stream User Modeling**: Learning user latent factors by separately aggregating information from the item-space (user-item graph) and social-space (user-user graph) and then combining them.\n        *   **Multi-Layer Perceptrons (MLPs)**: Used extensively for transforming and combining embeddings, and for parameterizing attention networks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted to demonstrate the effectiveness of the proposed GraphRec framework.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that experiments were performed on \"two real-world datasets\" and demonstrated the \"effectiveness of the proposed framework GraphRec.\" (Specific datasets and metrics are not detailed in the provided snippet but are typically found in the full paper's experimental section).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations or assumptions beyond the challenges it aims to solve. However, GNNs generally assume graph structure is available and can be computationally intensive for very large graphs. The model's reliance on explicit ratings for opinion embeddings might be a limitation in implicit feedback scenarios.\n    *   **Scope of Applicability**: The framework is designed for social recommender systems where both user-item interaction data (with opinions/ratings) and user-user social network data are available.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{fan2019k6u} advances the technical state-of-the-art by providing a principled and comprehensive GNN-based solution that simultaneously addresses three critical challenges in social recommendation: dual-graph integration, joint interaction-opinion modeling, and heterogeneous social tie strengths. Its use of attention mechanisms for fine-grained weighting of different information sources is a significant improvement over simpler aggregation methods.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in:\n        *   Developing more sophisticated attention mechanisms for complex graph structures.\n        *   Extending GNNs to handle other forms of heterogeneity (e.g., different types of items, multi-faceted social relations).\n        *   Applying similar dual-graph and opinion-aware modeling techniques to other domains beyond recommendation where multiple interconnected graphs and nuanced node attributes exist.\n        *   Exploring the interpretability of the learned attention weights to understand user preferences and social influence.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Social recommender systems",
        "GraphRec framework",
        "Unified dual-graph modeling",
        "Opinion-aware interaction representation",
        "Attention mechanisms",
        "Heterogeneous social relations",
        "Two-stream user modeling",
        "User and item latent factors",
        "Item attention",
        "Social attention",
        "User attention"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we present a novel graph neural network framework (graphrec) for social recommendations\" and \"propose the framework graphrec\".\n*   it discusses addressing challenges by providing a \"principled approach\" and coherently modeling graphs.\n*   it mentions \"extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework graphrec,\" indicating validation of the new method.\n*   the introduction sets the stage for the problem (social recommendation, gnns) and the need for a solution.\n\nthese points strongly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and where the abstract mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\". while it also includes empirical validation, the primary contribution described is the *development and presentation of a novel framework*.\n\n**classification: technical**"
    },
    "file_name": "398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf"
  },
  {
    "success": true,
    "doc_id": "01b5483170da6eb56a82786ac37164ef",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Current Graph Neural Network (GNN) research primarily focuses on proposing and evaluating specific architectural designs (e.g., GCN, GIN, GAT) rather than systematically studying the broader GNN design space. Additionally, GNN designs are often specialized to a single task, making it challenging to quickly identify or transfer effective architectures for novel tasks or datasets.\n    *   **Importance and Challenge**: The rapid proliferation of GNN architectures and applications creates a vast design space (a Cartesian product of design dimensions). Re-exploring this entire space for every new task is prohibitively expensive. Existing GNN evaluation practices are often limited to a small, non-diverse set of tasks, hindering generalizability and the discovery of optimal designs. There is also a lack of a unified platform for extensive and reproducible GNN design exploration \\cite{you2020drv}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Graph Architecture Search**: Previous works applied architecture search to GNNs but focused only on intra-layer design and evaluated on a limited number of node classification tasks \\cite{you2020drv}.\n        *   **Evaluation of GNN Models**: Existing efforts compare specific GNN designs (e.g., GCN, GraphSAGE, GAT) rather than exploring a general design space \\cite{you2020drv}.\n        *   **Transferable Architecture Search**: Studied in computer vision, but often assumes tasks follow the same distribution or that a single architecture performs well across tasks, an assumption that does not hold for the high variety of graph learning tasks \\cite{you2020drv}.\n    *   **Limitations of Previous Solutions**: Prior work suffers from a narrow scope in architecture exploration (intra-layer only), limited task diversity for evaluation, and an inability to effectively transfer GNN designs across the highly varied landscape of graph tasks \\cite{you2020drv}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a systematic study of the GNN architectural design space and the GNN task space. It introduces three key components: (1) a general GNN design space, (2) a GNN task space with a similarity metric, and (3) an efficient design space evaluation method \\cite{you2020drv}.\n    *   **Novelty/Difference**:\n        *   **General GNN Design Space**: Defines a comprehensive design space covering intra-layer design (e.g., BatchNorm, Dropout, Activation, Aggregation), inter-layer design (e.g., Layer connectivity, Pre/Post-process layers, Message passing layers), and learning configurations (e.g., Batch size, Learning rate, Optimizer, Epochs). This results in 12 design dimensions and 315,000 possible designs \\cite{you2020drv}.\n        *   **GNN Task Space with Similarity Metric**: Proposes a quantitative task similarity metric based on the Kendall rank correlation of performance rankings of a fixed set of \"anchor models\" across different tasks. This allows for identifying relationships between tasks and transferring designs \\cite{you2020drv}.\n        *   **Efficient Design Space Evaluation**: Develops a \"controlled random search\" technique to efficiently distill insights from the vast (over 10M) model-task combinations, overcoming the infeasibility of full grid search \\cite{you2020drv}.\n        *   **GraphGym Platform**: Introduces GraphGym, a powerful platform featuring modularized GNN implementation, standardized evaluation, and reproducible/scalable experiment management \\cite{you2020drv}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Formal definition and systematic exploration of a general GNN design space, encompassing 12 key architectural and training dimensions \\cite{you2020drv}.\n        *   A quantitative GNN task similarity metric based on Kendall rank correlation of anchor model performance, enabling principled design transfer \\cite{you2020drv}.\n        *   An efficient \"controlled random search\" evaluation procedure for distilling insights from a massive design-task experiment space \\cite{you2020drv}.\n    *   **System Design or Architectural Innovations**:\n        *   The GraphGym platform, providing a modular, standardized, and reproducible environment for GNN research and experimentation \\cite{you2020drv}.\n    *   **Theoretical Insights or Analysis**:\n        *   Demonstrates that while optimal GNN designs vary significantly across tasks, the proposed task similarity metric allows for effective transfer of best designs \\cite{you2020drv}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated the design space across 32 diverse predictive tasks, including 12 synthetic node classification, 8 synthetic graph classification, and 12 real-world tasks (6 node, 6 graph) \\cite{you2020drv}.\n        *   Applied the controlled random search to analyze the impact of individual design choices (e.g., BatchNorm, activation functions, skip connections) across tasks \\cite{you2020drv}.\n        *   Validated the task similarity metric by showing that tasks with higher similarity tend to share similar best-performing GNN designs \\cite{you2020drv}.\n        *   Demonstrated the effectiveness of design transfer by identifying promising GNN designs for a new task from the Open Graph Benchmark \\cite{you2020drv}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Provided a comprehensive set of guidelines for designing well-performing GNNs based on the extensive evaluation \\cite{you2020drv}.\n        *   Showed that models discovered using their design space achieve state-of-the-art performance on a new task \\cite{you2020drv}.\n        *   Empirically demonstrated that 12 anchor models are sufficient to approximate task similarity, making the metric computationally feasible \\cite{you2020drv}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The defined design space, while comprehensive, is not exhaustive and can be expanded as new design dimensions emerge. The focus is primarily on message passing GNNs \\cite{you2020drv}. The \"modest ranges of options\" for each dimension are based on existing literature and experience, not an exhaustive search of all possible values \\cite{you2020drv}.\n    *   **Scope of Applicability**: The framework is general and extendable to new design dimensions and any novel GNN task. The current evaluation focuses on node and graph-level tasks, with link prediction results provided in the appendix \\cite{you2020drv}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This work represents a significant shift from ad-hoc GNN design to a principled, systematic exploration of the GNN design space and task space. It provides a scalable methodology for understanding GNN architectural choices and their interplay with diverse tasks \\cite{you2020drv}.\n    *   **Potential Impact on Future Research**:\n        *   Offers concrete guidelines for GNN architecture design, accelerating development.\n        *   Enables efficient transfer of GNN designs across similar tasks, reducing redundant research efforts.\n        *   Facilitates the identification of truly novel GNN tasks that require new architectural innovations.\n        *   The GraphGym platform promotes reproducible GNN research and standardized evaluation, fostering fair comparisons and faster progress in the field \\cite{you2020drv}.",
    "intriguing_abstract": "The rapid evolution of Graph Neural Networks (GNNs) has created a bewildering array of architectures, yet a systematic understanding of their vast design space and performance across diverse tasks remains elusive. This paper pioneers a principled approach to navigate this complexity. We introduce a comprehensive GNN design space spanning 12 architectural and training dimensions, encompassing over 300,000 unique configurations. To overcome the prohibitive cost of full exploration, we define a novel GNN task space with a quantitative similarity metric based on Kendall rank correlation, enabling effective design transfer. An efficient 'controlled random search' strategy distills critical insights from millions of model-task combinations. Complementing this, we present GraphGym, a modular and reproducible platform for standardized GNN experimentation. Our extensive evaluation across 32 diverse tasks yields concrete design guidelines and demonstrates that optimal GNN architectures, while task-specific, can be efficiently transferred using our similarity metric. This work transforms GNN architecture search from an ad-hoc process into a systematic science, accelerating discovery, fostering reproducibility, and paving the way for state-of-the-art performance in graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN design space",
      "GNN task space",
      "task similarity metric",
      "efficient design space evaluation",
      "controlled random search",
      "GraphGym platform",
      "design transfer",
      "systematic GNN design exploration",
      "reproducible GNN research",
      "GNN architectural guidelines",
      "node and graph classification",
      "message passing GNNs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/0a69c8815536a657668e089e3281ff2e963d947a.pdf",
    "citation_key": "you2020drv",
    "metadata": {
      "title": "Design Space for Graph Neural Networks",
      "authors": [
        "Jiaxuan You",
        "Rex Ying",
        "J. Leskovec"
      ],
      "published_date": "2020",
      "abstract": "The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.",
      "file_path": "paper_data/Graph_Neural_Networks/0a69c8815536a657668e089e3281ff2e963d947a.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Current Graph Neural Network (GNN) research primarily focuses on proposing and evaluating specific architectural designs (e.g., GCN, GIN, GAT) rather than systematically studying the broader GNN design space. Additionally, GNN designs are often specialized to a single task, making it challenging to quickly identify or transfer effective architectures for novel tasks or datasets.\n    *   **Importance and Challenge**: The rapid proliferation of GNN architectures and applications creates a vast design space (a Cartesian product of design dimensions). Re-exploring this entire space for every new task is prohibitively expensive. Existing GNN evaluation practices are often limited to a small, non-diverse set of tasks, hindering generalizability and the discovery of optimal designs. There is also a lack of a unified platform for extensive and reproducible GNN design exploration \\cite{you2020drv}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Graph Architecture Search**: Previous works applied architecture search to GNNs but focused only on intra-layer design and evaluated on a limited number of node classification tasks \\cite{you2020drv}.\n        *   **Evaluation of GNN Models**: Existing efforts compare specific GNN designs (e.g., GCN, GraphSAGE, GAT) rather than exploring a general design space \\cite{you2020drv}.\n        *   **Transferable Architecture Search**: Studied in computer vision, but often assumes tasks follow the same distribution or that a single architecture performs well across tasks, an assumption that does not hold for the high variety of graph learning tasks \\cite{you2020drv}.\n    *   **Limitations of Previous Solutions**: Prior work suffers from a narrow scope in architecture exploration (intra-layer only), limited task diversity for evaluation, and an inability to effectively transfer GNN designs across the highly varied landscape of graph tasks \\cite{you2020drv}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a systematic study of the GNN architectural design space and the GNN task space. It introduces three key components: (1) a general GNN design space, (2) a GNN task space with a similarity metric, and (3) an efficient design space evaluation method \\cite{you2020drv}.\n    *   **Novelty/Difference**:\n        *   **General GNN Design Space**: Defines a comprehensive design space covering intra-layer design (e.g., BatchNorm, Dropout, Activation, Aggregation), inter-layer design (e.g., Layer connectivity, Pre/Post-process layers, Message passing layers), and learning configurations (e.g., Batch size, Learning rate, Optimizer, Epochs). This results in 12 design dimensions and 315,000 possible designs \\cite{you2020drv}.\n        *   **GNN Task Space with Similarity Metric**: Proposes a quantitative task similarity metric based on the Kendall rank correlation of performance rankings of a fixed set of \"anchor models\" across different tasks. This allows for identifying relationships between tasks and transferring designs \\cite{you2020drv}.\n        *   **Efficient Design Space Evaluation**: Develops a \"controlled random search\" technique to efficiently distill insights from the vast (over 10M) model-task combinations, overcoming the infeasibility of full grid search \\cite{you2020drv}.\n        *   **GraphGym Platform**: Introduces GraphGym, a powerful platform featuring modularized GNN implementation, standardized evaluation, and reproducible/scalable experiment management \\cite{you2020drv}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Formal definition and systematic exploration of a general GNN design space, encompassing 12 key architectural and training dimensions \\cite{you2020drv}.\n        *   A quantitative GNN task similarity metric based on Kendall rank correlation of anchor model performance, enabling principled design transfer \\cite{you2020drv}.\n        *   An efficient \"controlled random search\" evaluation procedure for distilling insights from a massive design-task experiment space \\cite{you2020drv}.\n    *   **System Design or Architectural Innovations**:\n        *   The GraphGym platform, providing a modular, standardized, and reproducible environment for GNN research and experimentation \\cite{you2020drv}.\n    *   **Theoretical Insights or Analysis**:\n        *   Demonstrates that while optimal GNN designs vary significantly across tasks, the proposed task similarity metric allows for effective transfer of best designs \\cite{you2020drv}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated the design space across 32 diverse predictive tasks, including 12 synthetic node classification, 8 synthetic graph classification, and 12 real-world tasks (6 node, 6 graph) \\cite{you2020drv}.\n        *   Applied the controlled random search to analyze the impact of individual design choices (e.g., BatchNorm, activation functions, skip connections) across tasks \\cite{you2020drv}.\n        *   Validated the task similarity metric by showing that tasks with higher similarity tend to share similar best-performing GNN designs \\cite{you2020drv}.\n        *   Demonstrated the effectiveness of design transfer by identifying promising GNN designs for a new task from the Open Graph Benchmark \\cite{you2020drv}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Provided a comprehensive set of guidelines for designing well-performing GNNs based on the extensive evaluation \\cite{you2020drv}.\n        *   Showed that models discovered using their design space achieve state-of-the-art performance on a new task \\cite{you2020drv}.\n        *   Empirically demonstrated that 12 anchor models are sufficient to approximate task similarity, making the metric computationally feasible \\cite{you2020drv}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The defined design space, while comprehensive, is not exhaustive and can be expanded as new design dimensions emerge. The focus is primarily on message passing GNNs \\cite{you2020drv}. The \"modest ranges of options\" for each dimension are based on existing literature and experience, not an exhaustive search of all possible values \\cite{you2020drv}.\n    *   **Scope of Applicability**: The framework is general and extendable to new design dimensions and any novel GNN task. The current evaluation focuses on node and graph-level tasks, with link prediction results provided in the appendix \\cite{you2020drv}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This work represents a significant shift from ad-hoc GNN design to a principled, systematic exploration of the GNN design space and task space. It provides a scalable methodology for understanding GNN architectural choices and their interplay with diverse tasks \\cite{you2020drv}.\n    *   **Potential Impact on Future Research**:\n        *   Offers concrete guidelines for GNN architecture design, accelerating development.\n        *   Enables efficient transfer of GNN designs across similar tasks, reducing redundant research efforts.\n        *   Facilitates the identification of truly novel GNN tasks that require new architectural innovations.\n        *   The GraphGym platform promotes reproducible GNN research and standardized evaluation, fostering fair comparisons and faster progress in the field \\cite{you2020drv}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN design space",
        "GNN task space",
        "task similarity metric",
        "efficient design space evaluation",
        "controlled random search",
        "GraphGym platform",
        "design transfer",
        "systematic GNN design exploration",
        "reproducible GNN research",
        "GNN architectural guidelines",
        "node and graph classification",
        "message passing GNNs"
      ],
      "paper_type": "this paper presents new methods, frameworks, and a system for studying graph neural networks (gnns).\n\nhere's a breakdown of why it fits the **technical** classification:\n\n*   **abstract mentions:**\n    *   \"here we deﬁne and systematically study the architectural design space for gnns...\" - \"define\" implies proposing a new framework.\n    *   \"our approach features three key innovations: (1) a general gnn design space; (2) a gnn task space with a similarity metric; (3) an efﬁcient and effective design space evaluation method...\" - these are explicit mentions of proposing new methods and frameworks.\n    *   \"finally, we release graphgym, a powerful platform for exploring different gnn designs and tasks.\" - developing and releasing a platform/system is a strong indicator of a technical paper.\n*   **introduction discusses:**\n    *   \"issues in gnn architecture design\" and \"issues in gnn evaluation\" - these are technical problems that the paper aims to solve by proposing a new approach.\n\nwhile the paper also involves a large-scale \"systematic study\" (315,000 designs over 32 tasks) and presents \"key results\" and \"guidelines\" (which are empirical findings), these empirical aspects are largely a demonstration and validation of the *new methods, frameworks, and system* (design space, task space, evaluation method, graphgym platform) that the paper proposes. the primary contribution is the development of these technical innovations to enable such systematic study.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "0a69c8815536a657668e089e3281ff2e963d947a.pdf"
  },
  {
    "success": true,
    "doc_id": "8f34c6158b25bf16d92ec7de5c12ecae",
    "summary": "Here's a focused summary of the paper \\cite{cai2020k4b} for a literature review:\n\n### Technical Paper Analysis: A Note on Over-Smoothing for Graph Neural Networks \\cite{cai2020k4b}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the \"over-smoothing\" effect in Graph Neural Networks (GNNs), where increasing the number of layers leads to a degradation in performance. This occurs because node embeddings converge to similar values, causing a loss of discriminative power.\n    *   **Why important and challenging**: Over-smoothing is a key limitation preventing GNNs from achieving greater depth and expressive power, despite their success on graph-structured data. Existing theoretical analyses of this phenomenon are often limited to linear GNNs or specific non-linearities (e.g., ReLU), making it challenging to generalize to the diverse GNN architectures used in practice.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon previous theoretical analyses, particularly `Oono & Suzuki, 2019`, which was a pioneering effort to extend over-smoothing analysis to non-linear GNNs.\n    *   **Limitations of previous solutions**: `Oono & Suzuki, 2019` was primarily limited to ReLU activation functions, noting that extending their analysis to other non-linearities like Sigmoid or Leaky ReLU was non-trivial. Additionally, their analysis often assumed fixed embedding dimensions across layers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a novel analytical technique based on tracking the **Dirichlet energy** of node embeddings across layers. It proves that under certain conditions, this Dirichlet energy exponentially converges to zero as the number of layers increases.\n    *   **What makes this approach novel or different**:\n        *   Utilizing Dirichlet energy provides a \"conceptually clean\" measure of embedding \"expressiveness\" (or lack thereof when it approaches zero), leading to simpler proofs.\n        *   This approach allows for the analysis of a broader range of non-linearities, specifically including Leaky ReLU, and for regular graphs, even Tanh and Sigmoid, which was a noted difficulty for prior methods \\cite{cai2020k4b}.\n        *   The proof can handle GNN layers where the embedding dimension changes, unlike some previous analyses.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   A new theoretical framework for analyzing over-smoothing in general GNN architectures by monitoring the Dirichlet energy of node embeddings.\n        *   Formal proofs demonstrating how each component of a GNN layer (graph convolution, weight matrix, non-linearity) impacts the Dirichlet energy.\n    *   **Theoretical insights or analysis**:\n        *   The paper proves that the Dirichlet energy `E(X(l))` of node embeddings at layer `l` converges exponentially to zero, i.e., `E(X(l)) = O((s * λ)^l)`, where `s` relates to the singular values of weight matrices and `λ` relates to the smallest non-zero eigenvalue of the augmented normalized Laplacian \\cite{cai2020k4b}. This convergence signifies the loss of discriminative power.\n        *   Specifically, it shows:\n            *   `E(PX) ≤ (1 - λ_min)^2 E(X)`, where `λ_min` is the smallest non-zero eigenvalue of the augmented normalized Laplacian `~Δ`.\n            *   `E(XW) ≤ ||W^T||_2^2 E(X)`.\n            *   `E(σ(X)) ≤ E(X)` for ReLU and Leaky ReLU, and for regular graphs, this extends to Tanh and Sigmoid.\n        *   This analysis provides a more general understanding of the conditions under which over-smoothing occurs.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: The authors conducted extensive experiments on both synthetic graphs (Random, Random Geometric, Stochastic Block Model, Barabasi-Albert) and real-world benchmarks (Cora, CiteSeer). They investigated how basic edge operations—specifically, randomly dropping edges and increasing edge weights—affect the eigenvalues of the augmented normalized Laplacian and the Dirichlet energy of node signals.\n    *   **Key performance metrics and comparison results**:\n        *   **Dropping edges**: For most graphs and ratios, dropping edges was observed to *increase* the Dirichlet energy of node signals. This finding empirically supports the idea that edge dropping (as in techniques like DropEdge) helps mitigate over-smoothing by preserving discriminative power \\cite{cai2020k4b}.\n        *   **Increasing edge weights**: Increasing the weight of a few edges to a very high value was found to have a \"dual\" effect to dropping edges, also generally increasing Dirichlet energy. This suggests a potential new direction for relieving over-smoothing.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The theoretical proof relies on assumptions regarding the norm of the weight matrices. While the analysis extends to Leaky ReLU, further generalization to more complex GNN components (e.g., other non-linearities, normalization strategies, attention mechanisms, graphs with node/edge features) remains an open challenge.\n    *   **Scope of applicability**: The paper primarily focuses on the theoretical understanding of over-smoothing in GCN-like architectures. It analyzes the Dirichlet energy `tr(X^T ~Δ X)` but acknowledges that analyzing the \"real\" smoothness, defined by the Rayleigh quotient `tr(X^T ~Δ X) / ||X||_2^2`, for deep GNNs is still an open and important question.\n\n7.  **Technical Significance**\n    *   **How this advances the technical state-of-the-art**: The paper significantly advances the theoretical understanding of over-smoothing in GNNs by providing a more general, conceptually cleaner, and simpler analytical framework using Dirichlet energy \\cite{cai2020k4b}. It extends the applicability of such analyses to a wider range of non-linearities and varying embedding dimensions, overcoming limitations of prior work.\n    *   **Potential impact on future research**:\n        *   The use of Dirichlet energy offers a valuable tool for practitioners to monitor GNN behavior during training and for researchers to analyze GNN properties.\n        *   The empirical findings on edge operations provide concrete insights for developing new architectural designs or regularization techniques (e.g., graph sparsification, edge reweighting) to combat over-smoothing.\n        *   It clearly outlines several open research directions, including extending the analysis to more complex GNN components and understanding the interplay between learning dynamics and over-smoothing.",
    "intriguing_abstract": "Deep Graph Neural Networks (GNNs) are powerful, yet their depth is severely limited by the pervasive \"over-smoothing\" phenomenon, where node embeddings converge, eroding discriminative power. This paper presents a novel theoretical framework to precisely quantify and understand this critical bottleneck. We introduce the **Dirichlet energy** of node embeddings as a conceptually clean and powerful metric, proving its exponential convergence to zero across layers, `E(X(l)) = O((s * λ)^l)`.\n\nOur approach significantly extends prior analyses by rigorously demonstrating how graph convolution, weight matrices, and a broad spectrum of non-linearities (including Leaky ReLU, Tanh, and Sigmoid for regular graphs) contribute to this decay, even accommodating varying embedding dimensions. Crucially, empirical investigations on diverse graphs reveal that operations like randomly dropping edges and increasing edge weights *increase* Dirichlet energy, offering concrete insights into mitigating over-smoothing. This work provides a general, rigorous understanding of GNN depth limitations, offering a vital analytical tool and paving the way for designing more robust and expressive deep GNN architectures through targeted graph sparsification and edge reweighting strategies.",
    "keywords": [
      "Over-smoothing",
      "Graph Neural Networks (GNNs)",
      "Dirichlet energy",
      "node embeddings",
      "augmented normalized Laplacian",
      "theoretical framework",
      "exponential convergence",
      "non-linearities",
      "discriminative power",
      "edge dropping",
      "edge reweighting",
      "deep GNNs",
      "analytical technique",
      "graph convolution"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf",
    "citation_key": "cai2020k4b",
    "metadata": {
      "title": "A Note on Over-Smoothing for Graph Neural Networks",
      "authors": [
        "Chen Cai",
        "Yusu Wang"
      ],
      "published_date": "2020",
      "abstract": "Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \\cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure \"expressiveness\" of embedding is conceptually clean; it leads to simpler proofs than \\cite{oono2019graph} and can handle more non-linearities.",
      "file_path": "paper_data/Graph_Neural_Networks/c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \\cite{cai2020k4b} for a literature review:\n\n### Technical Paper Analysis: A Note on Over-Smoothing for Graph Neural Networks \\cite{cai2020k4b}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the \"over-smoothing\" effect in Graph Neural Networks (GNNs), where increasing the number of layers leads to a degradation in performance. This occurs because node embeddings converge to similar values, causing a loss of discriminative power.\n    *   **Why important and challenging**: Over-smoothing is a key limitation preventing GNNs from achieving greater depth and expressive power, despite their success on graph-structured data. Existing theoretical analyses of this phenomenon are often limited to linear GNNs or specific non-linearities (e.g., ReLU), making it challenging to generalize to the diverse GNN architectures used in practice.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon previous theoretical analyses, particularly `Oono & Suzuki, 2019`, which was a pioneering effort to extend over-smoothing analysis to non-linear GNNs.\n    *   **Limitations of previous solutions**: `Oono & Suzuki, 2019` was primarily limited to ReLU activation functions, noting that extending their analysis to other non-linearities like Sigmoid or Leaky ReLU was non-trivial. Additionally, their analysis often assumed fixed embedding dimensions across layers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a novel analytical technique based on tracking the **Dirichlet energy** of node embeddings across layers. It proves that under certain conditions, this Dirichlet energy exponentially converges to zero as the number of layers increases.\n    *   **What makes this approach novel or different**:\n        *   Utilizing Dirichlet energy provides a \"conceptually clean\" measure of embedding \"expressiveness\" (or lack thereof when it approaches zero), leading to simpler proofs.\n        *   This approach allows for the analysis of a broader range of non-linearities, specifically including Leaky ReLU, and for regular graphs, even Tanh and Sigmoid, which was a noted difficulty for prior methods \\cite{cai2020k4b}.\n        *   The proof can handle GNN layers where the embedding dimension changes, unlike some previous analyses.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   A new theoretical framework for analyzing over-smoothing in general GNN architectures by monitoring the Dirichlet energy of node embeddings.\n        *   Formal proofs demonstrating how each component of a GNN layer (graph convolution, weight matrix, non-linearity) impacts the Dirichlet energy.\n    *   **Theoretical insights or analysis**:\n        *   The paper proves that the Dirichlet energy `E(X(l))` of node embeddings at layer `l` converges exponentially to zero, i.e., `E(X(l)) = O((s * λ)^l)`, where `s` relates to the singular values of weight matrices and `λ` relates to the smallest non-zero eigenvalue of the augmented normalized Laplacian \\cite{cai2020k4b}. This convergence signifies the loss of discriminative power.\n        *   Specifically, it shows:\n            *   `E(PX) ≤ (1 - λ_min)^2 E(X)`, where `λ_min` is the smallest non-zero eigenvalue of the augmented normalized Laplacian `~Δ`.\n            *   `E(XW) ≤ ||W^T||_2^2 E(X)`.\n            *   `E(σ(X)) ≤ E(X)` for ReLU and Leaky ReLU, and for regular graphs, this extends to Tanh and Sigmoid.\n        *   This analysis provides a more general understanding of the conditions under which over-smoothing occurs.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: The authors conducted extensive experiments on both synthetic graphs (Random, Random Geometric, Stochastic Block Model, Barabasi-Albert) and real-world benchmarks (Cora, CiteSeer). They investigated how basic edge operations—specifically, randomly dropping edges and increasing edge weights—affect the eigenvalues of the augmented normalized Laplacian and the Dirichlet energy of node signals.\n    *   **Key performance metrics and comparison results**:\n        *   **Dropping edges**: For most graphs and ratios, dropping edges was observed to *increase* the Dirichlet energy of node signals. This finding empirically supports the idea that edge dropping (as in techniques like DropEdge) helps mitigate over-smoothing by preserving discriminative power \\cite{cai2020k4b}.\n        *   **Increasing edge weights**: Increasing the weight of a few edges to a very high value was found to have a \"dual\" effect to dropping edges, also generally increasing Dirichlet energy. This suggests a potential new direction for relieving over-smoothing.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The theoretical proof relies on assumptions regarding the norm of the weight matrices. While the analysis extends to Leaky ReLU, further generalization to more complex GNN components (e.g., other non-linearities, normalization strategies, attention mechanisms, graphs with node/edge features) remains an open challenge.\n    *   **Scope of applicability**: The paper primarily focuses on the theoretical understanding of over-smoothing in GCN-like architectures. It analyzes the Dirichlet energy `tr(X^T ~Δ X)` but acknowledges that analyzing the \"real\" smoothness, defined by the Rayleigh quotient `tr(X^T ~Δ X) / ||X||_2^2`, for deep GNNs is still an open and important question.\n\n7.  **Technical Significance**\n    *   **How this advances the technical state-of-the-art**: The paper significantly advances the theoretical understanding of over-smoothing in GNNs by providing a more general, conceptually cleaner, and simpler analytical framework using Dirichlet energy \\cite{cai2020k4b}. It extends the applicability of such analyses to a wider range of non-linearities and varying embedding dimensions, overcoming limitations of prior work.\n    *   **Potential impact on future research**:\n        *   The use of Dirichlet energy offers a valuable tool for practitioners to monitor GNN behavior during training and for researchers to analyze GNN properties.\n        *   The empirical findings on edge operations provide concrete insights for developing new architectural designs or regularization techniques (e.g., graph sparsification, edge reweighting) to combat over-smoothing.\n        *   It clearly outlines several open research directions, including extending the analysis to more complex GNN components and understanding the interplay between learning dynamics and over-smoothing.",
      "keywords": [
        "Over-smoothing",
        "Graph Neural Networks (GNNs)",
        "Dirichlet energy",
        "node embeddings",
        "augmented normalized Laplacian",
        "theoretical framework",
        "exponential convergence",
        "non-linearities",
        "discriminative power",
        "edge dropping",
        "edge reweighting",
        "deep GNNs",
        "analytical technique",
        "graph convolution"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the paper explicitly states: \"we build upon previous results... to further **analyze** the over-smoothing effect in the general graph neural network architecture.\"\n*   it then says: \"we **show** when the weight matrix satisﬁes the conditions determined by the spectrum of augmented normalized laplacian, the dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power.\" this is a clear statement of a mathematical result or proof.\n*   it highlights the use of \"dirichlet energy to measure 'expressiveness' of embedding\" and that \"it leads to **simpler proofs** than (oono & suzuki, 2019) and can handle more non-linearities.\" this emphasizes the mathematical rigor and the improvement in the theoretical framework.\n*   the introduction sets up the problem of over-smoothing as a phenomenon to be understood and analyzed, rather than proposing a new system or conducting an experiment.\n\nthese points strongly align with the criteria for a **theoretical** paper, which focuses on mathematical analysis, proofs, and formal models to explain phenomena or establish properties.\n\n**classification: theoretical**"
    },
    "file_name": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf"
  },
  {
    "success": true,
    "doc_id": "591b0814c8683d16939ac8ad88685c20",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Adversarial training has not yet proven to be an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations, unlike its success in the image domain \\cite{gosch20237yi}.\n    *   **Importance & Challenge**: Robustness of GNNs is crucial for real-world applications where graph data can be maliciously altered. The problem is challenging because prior work suffered from biased evaluation settings and GNN architectures struggled to learn robust representations \\cite{gosch20237yi}. Additionally, existing adversarial attacks for GNNs often used overly permissive and unrealistic perturbation sets \\cite{gosch20237yi}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on adversarial training for GNNs (e.g., \\cite{xu2019topology, zhang2020adversarial}) reported some robustness gains \\cite{gosch20237yi}.\n    *   **Limitations of Previous Solutions**:\n        *   **Biased Learning Setting**: Prior studies primarily used transductive learning settings where clean validation and test nodes were known during training. This allowed models to achieve \"perfect robustness\" by memorizing the training graph, leading to a false impression of robustness \\cite{gosch20237yi}. The authors empirically show that self-training, often used in these settings, was the main cause for reported robustness gains, not adversarial training itself \\cite{gosch20237yi}.\n        *   **Limited GNN Architectures**: GNNs like GCN or APPNP, commonly studied, have fixed message passing schemes with limited ability to adapt to adversarial perturbations \\cite{gosch20237yi}.\n        *   **Unrealistic Perturbations**: Existing attacks often only constrained the global number of edge changes, allowing for degenerate perturbations that could drastically alter node neighborhoods, despite node classification being an inherently local task \\cite{gosch20237yi}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-faceted approach to make adversarial training effective for GNNs \\cite{gosch20237yi}:\n        1.  **Revisiting Learning Setting**: They advocate for and evaluate adversarial training in a *fully inductive setting*, where validation/test nodes are unseen during training, to avoid the evaluation pitfalls of transductive settings \\cite{gosch20237yi}.\n        2.  **Flexible GNN Architectures**: They propose using *learnable graph diffusion models* (e.g., GPRGNN, ChebNetII) that can approximate any spectral graph filter. This allows the GNN to adapt its message passing scheme to counteract adversarial perturbations \\cite{gosch20237yi}.\n        3.  **Realistic Adversarial Attack**: They introduce *Locally constrained Randomized Block Coordinate Descent (LR-BCD)*, the first attack for structure perturbations that can target multiple nodes simultaneously while enforcing both global (graph-level) and local (node-level) constraints on edge changes \\cite{gosch20237yi}.\n    *   **Novelty/Difference**:\n        *   The theoretical and empirical demonstration of the fundamental flaws in prior transductive adversarial training settings for GNNs \\cite{gosch20237yi}.\n        *   The integration of flexible, learnable graph diffusion models with adversarial training, leading to \"robust diffusion\" that can adapt its filter characteristics \\cite{gosch20237yi}.\n        *   The development of LR-BCD, which fills a critical gap by providing a more realistic and constrained multi-node attack for GNNs \\cite{gosch20237yi}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical & Empirical Analysis**: Proving and demonstrating that in the transductive setting, perfect robustness can be trivially achieved through memorization, highlighting the need for inductive evaluation \\cite{gosch20237yi}.\n    *   **Novel GNN Architecture for Robustness**: Introduction of \"robust diffusion\" by leveraging flexible GNNs based on learnable graph diffusion (e.g., GPRGNN, ChebNetII) for adversarial training, significantly improving robustness \\cite{gosch20237yi}.\n    *   **Novel Adversarial Attack**: Development of LR-BCD, the first attack for structure perturbations that can handle both global and local constraints when targeting multiple nodes \\cite{gosch20237yi}.\n    *   **Interpretability**: Providing insights into the learned robust representations from polynomial, spectral, and spatial perspectives through the analysis of diffusion coefficients and matrices \\cite{gosch20237yi}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comparison of GCN and GPRGNN with and without adversarial training on inductive Cora-ML to demonstrate robustness gains (Figure 1) \\cite{gosch20237yi}.\n        *   Analysis of the impact of self-training versus adversarial training on GCN robustness in the transductive setting (Figure 2) \\cite{gosch20237yi}.\n        *   Demonstration of GPRGNN achieving near-perfect robustness in the transductive setting, exposing its limitations (Figure 3) \\cite{gosch20237yi}.\n        *   Visualization of the learned diffusion coefficients and matrices (spatial perspective) for GPRGNN under different training strategies on Karate Club (Figure 4) \\cite{gosch20237yi}.\n        *   Analysis of polynomial and spectral characteristics of robust diffusion (Figures 6, 7) \\cite{gosch20237yi}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Test accuracy under varying percentages of perturbed edges (global and local constraints) \\cite{gosch20237yi}.\n        *   Robust diffusion (GPRGNN) with adversarial training showed a 4-fold increased robustness over standard training on Cora-ML (Figure 1) \\cite{gosch20237yi}.\n        *   Empirically showed that self-training, not adversarial training, was the primary driver of robustness in prior transductive settings (Figure 2) \\cite{gosch20237yi}.\n        *   Demonstrated that adversarial training with robust diffusion in the inductive setting leads to state-of-the-art defense against adversarial structure perturbations \\cite{gosch20237yi}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily addresses and overcomes the limitations of *prior work's evaluation settings and architectures*, rather than detailing limitations of its own proposed solutions. The inductive setting is assumed to be a more realistic and challenging scenario for evaluating robustness \\cite{gosch20237yi}.\n    *   **Scope of Applicability**: The work focuses on node classification tasks and graph structure perturbations. The proposed methods are applicable to GNNs that can incorporate learnable diffusion schemes \\cite{gosch20237yi}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper fundamentally re-evaluates and corrects the understanding of adversarial training for GNNs, establishing it as a state-of-the-art defense when applied correctly \\cite{gosch20237yi}. It significantly boosts empirical and certifiable adversarial robustness \\cite{gosch20237yi}.\n    *   **Potential Impact on Future Research**:\n        *   Provides a robust and realistic framework for evaluating GNN robustness, shifting focus to inductive settings and locally constrained attacks \\cite{gosch20237yi}.\n        *   Introduces flexible, interpretable GNN architectures (robust diffusion) that can adapt to adversarial changes, opening new avenues for designing robust GNNs \\cite{gosch20237yi}.\n        *   The LR-BCD attack can serve as a stronger, more realistic benchmark for future GNN robustness research \\cite{gosch20237yi}.",
    "intriguing_abstract": "Adversarial training, a cornerstone of robust deep learning, has surprisingly faltered in defending Graph Neural Networks (GNNs) against structural attacks. Previous attempts were plagued by biased transductive evaluation settings, limited GNN architectures, and unrealistic perturbation models, leading to a false sense of security. We fundamentally re-evaluate this paradigm, demonstrating that true adversarial robustness for GNNs is achievable in a fully *inductive learning setting*. Our novel approach integrates *learnable graph diffusion models*, creating \"robust diffusion\" that adaptively filters adversarial noise. Complementing this, we introduce *Locally constrained Randomized Block Coordinate Descent (LR-BCD)*, the first multi-node attack enforcing both global and crucial *local perturbation constraints*. Through rigorous empirical and theoretical analysis, we prove the pitfalls of prior transductive evaluations and establish adversarial training as a state-of-the-art defense, boosting robustness four-fold. Our work not only provides a realistic framework for GNN robustness but also offers interpretable insights into robust representations, setting new benchmarks for future research.",
    "keywords": [
      "Adversarial training for GNNs",
      "graph structure perturbations",
      "GNN robustness",
      "inductive learning setting",
      "transductive setting bias",
      "learnable graph diffusion models",
      "robust diffusion",
      "Locally constrained Randomized Block Coordinate Descent (LR-BCD)",
      "global and local edge constraints",
      "node classification",
      "theoretical and empirical analysis",
      "state-of-the-art defense"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf",
    "citation_key": "gosch20237yi",
    "metadata": {
      "title": "Adversarial Training for Graph Neural Networks",
      "authors": [
        "Lukas Gosch",
        "Simon Geisler",
        "Daniel Sturm",
        "Bertrand Charpentier",
        "Daniel Zugner",
        "Stephan Gunnemann"
      ],
      "published_date": "2023",
      "abstract": "Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.",
      "file_path": "paper_data/Graph_Neural_Networks/ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Adversarial training has not yet proven to be an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations, unlike its success in the image domain \\cite{gosch20237yi}.\n    *   **Importance & Challenge**: Robustness of GNNs is crucial for real-world applications where graph data can be maliciously altered. The problem is challenging because prior work suffered from biased evaluation settings and GNN architectures struggled to learn robust representations \\cite{gosch20237yi}. Additionally, existing adversarial attacks for GNNs often used overly permissive and unrealistic perturbation sets \\cite{gosch20237yi}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on adversarial training for GNNs (e.g., \\cite{xu2019topology, zhang2020adversarial}) reported some robustness gains \\cite{gosch20237yi}.\n    *   **Limitations of Previous Solutions**:\n        *   **Biased Learning Setting**: Prior studies primarily used transductive learning settings where clean validation and test nodes were known during training. This allowed models to achieve \"perfect robustness\" by memorizing the training graph, leading to a false impression of robustness \\cite{gosch20237yi}. The authors empirically show that self-training, often used in these settings, was the main cause for reported robustness gains, not adversarial training itself \\cite{gosch20237yi}.\n        *   **Limited GNN Architectures**: GNNs like GCN or APPNP, commonly studied, have fixed message passing schemes with limited ability to adapt to adversarial perturbations \\cite{gosch20237yi}.\n        *   **Unrealistic Perturbations**: Existing attacks often only constrained the global number of edge changes, allowing for degenerate perturbations that could drastically alter node neighborhoods, despite node classification being an inherently local task \\cite{gosch20237yi}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-faceted approach to make adversarial training effective for GNNs \\cite{gosch20237yi}:\n        1.  **Revisiting Learning Setting**: They advocate for and evaluate adversarial training in a *fully inductive setting*, where validation/test nodes are unseen during training, to avoid the evaluation pitfalls of transductive settings \\cite{gosch20237yi}.\n        2.  **Flexible GNN Architectures**: They propose using *learnable graph diffusion models* (e.g., GPRGNN, ChebNetII) that can approximate any spectral graph filter. This allows the GNN to adapt its message passing scheme to counteract adversarial perturbations \\cite{gosch20237yi}.\n        3.  **Realistic Adversarial Attack**: They introduce *Locally constrained Randomized Block Coordinate Descent (LR-BCD)*, the first attack for structure perturbations that can target multiple nodes simultaneously while enforcing both global (graph-level) and local (node-level) constraints on edge changes \\cite{gosch20237yi}.\n    *   **Novelty/Difference**:\n        *   The theoretical and empirical demonstration of the fundamental flaws in prior transductive adversarial training settings for GNNs \\cite{gosch20237yi}.\n        *   The integration of flexible, learnable graph diffusion models with adversarial training, leading to \"robust diffusion\" that can adapt its filter characteristics \\cite{gosch20237yi}.\n        *   The development of LR-BCD, which fills a critical gap by providing a more realistic and constrained multi-node attack for GNNs \\cite{gosch20237yi}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical & Empirical Analysis**: Proving and demonstrating that in the transductive setting, perfect robustness can be trivially achieved through memorization, highlighting the need for inductive evaluation \\cite{gosch20237yi}.\n    *   **Novel GNN Architecture for Robustness**: Introduction of \"robust diffusion\" by leveraging flexible GNNs based on learnable graph diffusion (e.g., GPRGNN, ChebNetII) for adversarial training, significantly improving robustness \\cite{gosch20237yi}.\n    *   **Novel Adversarial Attack**: Development of LR-BCD, the first attack for structure perturbations that can handle both global and local constraints when targeting multiple nodes \\cite{gosch20237yi}.\n    *   **Interpretability**: Providing insights into the learned robust representations from polynomial, spectral, and spatial perspectives through the analysis of diffusion coefficients and matrices \\cite{gosch20237yi}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comparison of GCN and GPRGNN with and without adversarial training on inductive Cora-ML to demonstrate robustness gains (Figure 1) \\cite{gosch20237yi}.\n        *   Analysis of the impact of self-training versus adversarial training on GCN robustness in the transductive setting (Figure 2) \\cite{gosch20237yi}.\n        *   Demonstration of GPRGNN achieving near-perfect robustness in the transductive setting, exposing its limitations (Figure 3) \\cite{gosch20237yi}.\n        *   Visualization of the learned diffusion coefficients and matrices (spatial perspective) for GPRGNN under different training strategies on Karate Club (Figure 4) \\cite{gosch20237yi}.\n        *   Analysis of polynomial and spectral characteristics of robust diffusion (Figures 6, 7) \\cite{gosch20237yi}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Test accuracy under varying percentages of perturbed edges (global and local constraints) \\cite{gosch20237yi}.\n        *   Robust diffusion (GPRGNN) with adversarial training showed a 4-fold increased robustness over standard training on Cora-ML (Figure 1) \\cite{gosch20237yi}.\n        *   Empirically showed that self-training, not adversarial training, was the primary driver of robustness in prior transductive settings (Figure 2) \\cite{gosch20237yi}.\n        *   Demonstrated that adversarial training with robust diffusion in the inductive setting leads to state-of-the-art defense against adversarial structure perturbations \\cite{gosch20237yi}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily addresses and overcomes the limitations of *prior work's evaluation settings and architectures*, rather than detailing limitations of its own proposed solutions. The inductive setting is assumed to be a more realistic and challenging scenario for evaluating robustness \\cite{gosch20237yi}.\n    *   **Scope of Applicability**: The work focuses on node classification tasks and graph structure perturbations. The proposed methods are applicable to GNNs that can incorporate learnable diffusion schemes \\cite{gosch20237yi}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper fundamentally re-evaluates and corrects the understanding of adversarial training for GNNs, establishing it as a state-of-the-art defense when applied correctly \\cite{gosch20237yi}. It significantly boosts empirical and certifiable adversarial robustness \\cite{gosch20237yi}.\n    *   **Potential Impact on Future Research**:\n        *   Provides a robust and realistic framework for evaluating GNN robustness, shifting focus to inductive settings and locally constrained attacks \\cite{gosch20237yi}.\n        *   Introduces flexible, interpretable GNN architectures (robust diffusion) that can adapt to adversarial changes, opening new avenues for designing robust GNNs \\cite{gosch20237yi}.\n        *   The LR-BCD attack can serve as a stronger, more realistic benchmark for future GNN robustness research \\cite{gosch20237yi}.",
      "keywords": [
        "Adversarial training for GNNs",
        "graph structure perturbations",
        "GNN robustness",
        "inductive learning setting",
        "transductive setting bias",
        "learnable graph diffusion models",
        "robust diffusion",
        "Locally constrained Randomized Block Coordinate Descent (LR-BCD)",
        "global and local edge constraints",
        "node classification",
        "theoretical and empirical analysis",
        "state-of-the-art defense"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"(1)we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2)we reveal that flexible gnns based on learnable graph diffusion are able to adjust to adversarial perturbations...; (3)we introduce the first attack for structure perturbations...\"\n*   the introduction discusses a technical problem (\"adversarial training... is not yet established as an effective method to defend predictions of graph neural networks (gnns) against graph structure perturbations\"), identifies shortcomings in previous approaches, and proposes a new approach (\"revisit adversarial training for node classification under structure perturbations in a fully inductive setting\").\n*   the phrase \"introduce the first attack\" is a strong indicator of presenting a new method or algorithm.\n*   the paper aims to demonstrate that adversarial training *can be* a state-of-the-art defense, implying the development and evaluation of a new system/methodology.\n\nthese points align perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\""
    },
    "file_name": "ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf"
  },
  {
    "success": true,
    "doc_id": "7a76d2cbe6757974c7928aed31032535",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to the citation requirements:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) achieve remarkable performance but are highly vulnerable to adversarial attacks, specifically small, unnoticeable perturbations of the discrete graph structure \\cite{zhang2020jrt}.\n    *   These \"poisoning attacks\" during training can catastrophically reduce GNN performance by contaminating local node neighborhoods and degrading learned representations \\cite{zhang2020jrt}.\n    *   The problem is critical due to potential impacts on public trust, human decision-making, and health, highlighting the vital need for robust GNNs \\cite{zhang2020jrt}.\n\n*   **Related Work & Positioning**\n    *   Existing defense methods on graphs include GNN-Jaccard \\cite{zhang2020jrt} (pre-processes adjacency matrix), RobustGCN \\cite{zhang2020jrt} (uses Gaussian distributions for hidden representations), GNN-SVD \\cite{zhang2020jrt} (low-rank approximation of adjacency matrix), and transfer learning approaches \\cite{zhang2020jrt}.\n    *   Limitations of previous solutions include:\n        *   Many are not general and cannot be effortlessly combined with *any* GNN architecture \\cite{zhang2020jrt}.\n        *   They primarily focus on homophily graphs and do not consider defending heterophily graphs against adversarial attacks \\cite{zhang2020jrt}.\n        *   Some require several unperturbed graphs from a similar domain during training \\cite{zhang2020jrt}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is GNNGUARD, a general algorithm designed to defend against training-time attacks that perturb discrete graph structure \\cite{zhang2020jrt}.\n    *   GNNGUARD's principle is to detect and quantify the relationship between graph structure and node features, then exploit this relationship to mitigate attack effects \\cite{zhang2020jrt}.\n    *   It learns to assign higher weights to edges connecting similar nodes and prunes edges between unrelated nodes, revising the message passing architecture for robust propagation \\cite{zhang2020jrt}.\n    *   The approach is novel because it introduces two key components: **neighbor importance estimation** and **layer-wise graph memory**, which are empirically shown to be necessary for successful defense \\cite{zhang2020jrt}.\n\n*   **Key Technical Contributions**\n    *   **Neighbor Importance Estimation:** This component dynamically adjusts the relevance of nodes' local network neighborhoods. It quantifies similarity between nodes (using cosine similarity on node embeddings) to assign importance weights to edges, pruning likely fake edges and assigning less weight to suspicious ones based on network homophily principles \\cite{zhang2020jrt}.\n    *   **Layer-Wise Graph Memory:** This unit stabilizes GNN training by keeping a partial memory of the pruned graph structure from the previous layer. This smooths the evolution of edge pruning and allows for robust estimation of importance weights, preventing destabilization if many edges are pruned in a single layer \\cite{zhang2020jrt}.\n    *   **General Framework:** GNNGUARD is designed to be straightforwardly incorporated into *any* GNN architecture by modifying its neural message passing operators (MSG, AGG, UPD) \\cite{zhang2020jrt}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted across five GNN models, three state-of-the-art defense methods, and four datasets, including a challenging human disease graph \\cite{zhang2020jrt}.\n    *   GNNGUARD was evaluated against a variety of training-time poisoning attacks, including direct targeted, influence targeted, and non-targeted attacks \\cite{zhang2020jrt}.\n    *   **Key performance metrics and comparison results:** GNNGUARD outperformed existing defense approaches by 15.3% on average \\cite{zhang2020jrt}.\n    *   It effectively restored state-of-the-art performance of GNNs in the face of various adversarial attacks \\cite{zhang2020jrt}.\n    *   Crucially, GNNGUARD demonstrated successful defense on heterophily graphs, a limitation of previous defenders \\cite{zhang2020jrt}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on defending against **training-time poisoning attacks** that perturb the **discrete graph structure** \\cite{zhang2020jrt}. It does not explicitly address evasion attacks (testing-time) or attacks on node features.\n    *   The defense mechanism is learned for **semi-supervised node classification** \\cite{zhang2020jrt}, though the authors suggest it's a general framework applicable to other tasks like link prediction.\n    *   The effectiveness relies on the hypothesis that damaging attacks add fake edges between dissimilar nodes, which GNNGUARD exploits \\cite{zhang2020jrt}.\n\n*   **Technical Significance**\n    *   GNNGUARD significantly advances the technical state-of-the-art by providing a general, plug-and-play defense mechanism that can be integrated into *any* GNN architecture \\cite{zhang2020jrt}.\n    *   It is the first technique to successfully defend GNNs against adversarial attacks on **heterophily graphs**, expanding the applicability of robust GNNs beyond homophily assumptions \\cite{zhang2020jrt}.\n    *   Its ability to effectively restore state-of-the-art performance of even strong GNNs under various attacks demonstrates its broad applicability and relevance, paving the way for more robust graph machine learning systems \\cite{zhang2020jrt}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) achieve remarkable performance but are dangerously susceptible to adversarial poisoning attacks on their discrete graph structure, leading to catastrophic performance degradation and threatening critical applications. We introduce GNNGUARD, a pioneering, generalizable defense mechanism designed to restore GNN robustness against training-time structural perturbations. GNNGUARD innovatively learns to detect and exploit the relationship between graph structure and node features, dynamically revising message passing by assigning importance weights to edges and pruning suspicious connections.\n\nIts core contributions include a novel **neighbor importance estimation** component, which quantifies node similarity for robust edge weighting, and a **layer-wise graph memory** that stabilizes training by smoothing the evolution of graph pruning. Crucially, GNNGUARD is a plug-and-play framework, seamlessly integrating into *any* GNN architecture. Our extensive experiments demonstrate GNNGUARD's unprecedented ability to outperform state-of-the-art defenses by 15.3% on average, effectively restoring GNN performance across diverse datasets and attack types, including the challenging domain of **heterophily graphs** where previous methods fail. This work significantly advances the field, paving the way for truly robust and trustworthy graph machine learning systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "adversarial attacks",
      "poisoning attacks",
      "discrete graph structure",
      "GNNGUARD",
      "neighbor importance estimation",
      "layer-wise graph memory",
      "heterophily graphs defense",
      "general GNN defense framework",
      "training-time attacks",
      "edge pruning",
      "robust graph machine learning",
      "semi-supervised node classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/73366d75289c5e37481639fb54fdee28a664e2b3.pdf",
    "citation_key": "zhang2020jrt",
    "metadata": {
      "title": "GNNGuard: Defending Graph Neural Networks against Adversarial Attacks",
      "authors": [
        "Xiang Zhang",
        "M. Zitnik"
      ],
      "published_date": "2020",
      "abstract": "Deep learning methods for graphs achieve remarkable performance on many tasks. However, despite the proliferation of such methods and their success, recent findings indicate that small, unnoticeable perturbations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop GNNGuard, a general defense approach against a variety of training-time attacks that perturb the discrete graph structure. GNNGuard can be straightforwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack. GNNGuard uses network theory of homophily to learn how best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges then allow the underlying GNN to robustly propagate neural messages in the graph. GNNGuard introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across five GNNs, three defense methods, and four datasets, including a challenging human disease graph, experiments show that GNNGuard outperforms existing defense approaches by 15.3% on average. Remarkably, GNNGuard can effectively restore the state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks.",
      "file_path": "paper_data/Graph_Neural_Networks/73366d75289c5e37481639fb54fdee28a664e2b3.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the citation requirements:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) achieve remarkable performance but are highly vulnerable to adversarial attacks, specifically small, unnoticeable perturbations of the discrete graph structure \\cite{zhang2020jrt}.\n    *   These \"poisoning attacks\" during training can catastrophically reduce GNN performance by contaminating local node neighborhoods and degrading learned representations \\cite{zhang2020jrt}.\n    *   The problem is critical due to potential impacts on public trust, human decision-making, and health, highlighting the vital need for robust GNNs \\cite{zhang2020jrt}.\n\n*   **Related Work & Positioning**\n    *   Existing defense methods on graphs include GNN-Jaccard \\cite{zhang2020jrt} (pre-processes adjacency matrix), RobustGCN \\cite{zhang2020jrt} (uses Gaussian distributions for hidden representations), GNN-SVD \\cite{zhang2020jrt} (low-rank approximation of adjacency matrix), and transfer learning approaches \\cite{zhang2020jrt}.\n    *   Limitations of previous solutions include:\n        *   Many are not general and cannot be effortlessly combined with *any* GNN architecture \\cite{zhang2020jrt}.\n        *   They primarily focus on homophily graphs and do not consider defending heterophily graphs against adversarial attacks \\cite{zhang2020jrt}.\n        *   Some require several unperturbed graphs from a similar domain during training \\cite{zhang2020jrt}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is GNNGUARD, a general algorithm designed to defend against training-time attacks that perturb discrete graph structure \\cite{zhang2020jrt}.\n    *   GNNGUARD's principle is to detect and quantify the relationship between graph structure and node features, then exploit this relationship to mitigate attack effects \\cite{zhang2020jrt}.\n    *   It learns to assign higher weights to edges connecting similar nodes and prunes edges between unrelated nodes, revising the message passing architecture for robust propagation \\cite{zhang2020jrt}.\n    *   The approach is novel because it introduces two key components: **neighbor importance estimation** and **layer-wise graph memory**, which are empirically shown to be necessary for successful defense \\cite{zhang2020jrt}.\n\n*   **Key Technical Contributions**\n    *   **Neighbor Importance Estimation:** This component dynamically adjusts the relevance of nodes' local network neighborhoods. It quantifies similarity between nodes (using cosine similarity on node embeddings) to assign importance weights to edges, pruning likely fake edges and assigning less weight to suspicious ones based on network homophily principles \\cite{zhang2020jrt}.\n    *   **Layer-Wise Graph Memory:** This unit stabilizes GNN training by keeping a partial memory of the pruned graph structure from the previous layer. This smooths the evolution of edge pruning and allows for robust estimation of importance weights, preventing destabilization if many edges are pruned in a single layer \\cite{zhang2020jrt}.\n    *   **General Framework:** GNNGUARD is designed to be straightforwardly incorporated into *any* GNN architecture by modifying its neural message passing operators (MSG, AGG, UPD) \\cite{zhang2020jrt}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted across five GNN models, three state-of-the-art defense methods, and four datasets, including a challenging human disease graph \\cite{zhang2020jrt}.\n    *   GNNGUARD was evaluated against a variety of training-time poisoning attacks, including direct targeted, influence targeted, and non-targeted attacks \\cite{zhang2020jrt}.\n    *   **Key performance metrics and comparison results:** GNNGUARD outperformed existing defense approaches by 15.3% on average \\cite{zhang2020jrt}.\n    *   It effectively restored state-of-the-art performance of GNNs in the face of various adversarial attacks \\cite{zhang2020jrt}.\n    *   Crucially, GNNGUARD demonstrated successful defense on heterophily graphs, a limitation of previous defenders \\cite{zhang2020jrt}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on defending against **training-time poisoning attacks** that perturb the **discrete graph structure** \\cite{zhang2020jrt}. It does not explicitly address evasion attacks (testing-time) or attacks on node features.\n    *   The defense mechanism is learned for **semi-supervised node classification** \\cite{zhang2020jrt}, though the authors suggest it's a general framework applicable to other tasks like link prediction.\n    *   The effectiveness relies on the hypothesis that damaging attacks add fake edges between dissimilar nodes, which GNNGUARD exploits \\cite{zhang2020jrt}.\n\n*   **Technical Significance**\n    *   GNNGUARD significantly advances the technical state-of-the-art by providing a general, plug-and-play defense mechanism that can be integrated into *any* GNN architecture \\cite{zhang2020jrt}.\n    *   It is the first technique to successfully defend GNNs against adversarial attacks on **heterophily graphs**, expanding the applicability of robust GNNs beyond homophily assumptions \\cite{zhang2020jrt}.\n    *   Its ability to effectively restore state-of-the-art performance of even strong GNNs under various attacks demonstrates its broad applicability and relevance, paving the way for more robust graph machine learning systems \\cite{zhang2020jrt}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "adversarial attacks",
        "poisoning attacks",
        "discrete graph structure",
        "GNNGUARD",
        "neighbor importance estimation",
        "layer-wise graph memory",
        "heterophily graphs defense",
        "general GNN defense framework",
        "training-time attacks",
        "edge pruning",
        "robust graph machine learning",
        "semi-supervised node classification"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"here, we **develop gnnguard, a general algorithm** to defend against a variety of training-time attacks... gnnguard **introduces two novel components**...\" it then describes the mechanism of this algorithm.\n*   the introduction sets up a technical problem (gnns' vulnerability to adversarial attacks) that the proposed solution (gnnguard) aims to address.\n*   while the abstract also mentions empirical evaluation (\"experiments show that gnnguard outperforms existing defense approaches...\"), this evaluation is done to demonstrate the effectiveness of the *new algorithm* being presented. the primary contribution is the development of the algorithm itself.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
    },
    "file_name": "73366d75289c5e37481639fb54fdee28a664e2b3.pdf"
  },
  {
    "success": true,
    "doc_id": "9e378174e60f143d7e783360fe9a4bc9",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) struggle to propagate information effectively between distant nodes in a graph, leading to poor performance on tasks requiring long-range interactions \\cite{alon2020fok}.\n    *   **Importance & Challenge:** Many real-world domains (e.g., quantum chemistry, program analysis, biological systems) are naturally represented as graphs and require GNNs to capture dependencies between non-adjacent nodes. The problem is challenging because, as the number of GNN layers increases to capture longer ranges, the receptive field of a node grows exponentially, leading to an information bottleneck \\cite{alon2020fok}. This phenomenon, termed \"over-squashing,\" is distinct from \"over-smoothing\" which primarily affects short-range tasks.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work analyzes the limitations of popular GNN variants like GCN, GIN, GAT, and GGNN, particularly when applied to problems with large \"problem radii\" (requiring long-range interactions) \\cite{alon2020fok}.\n    *   **Limitations of Previous Solutions:** Previous explanations for GNN performance limitations, such as \"over-smoothing,\" are argued to be more relevant for short-range tasks. For long-range tasks, `\\cite{alon2020fok}` posits that over-squashing is the dominant issue, where exponentially growing information is compressed into fixed-size vectors, causing crucial messages from distant nodes to be lost. The paper draws an analogy to the bottleneck in sequential RNN models but highlights that GNNs' exponentially growing receptive fields make their bottleneck asymptotically more severe \\cite{alon2020fok}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's primary contribution is the identification and characterization of the \"over-squashing\" phenomenon as a fundamental bottleneck in GNNs for long-range problems \\cite{alon2020fok}. It does not propose a new GNN architecture but rather explains an inherent limitation.\n    *   **Novelty:**\n        *   Introduces \"over-squashing\" as a novel explanation for GNNs' failure to learn long-range dependencies, distinct from over-smoothing \\cite{alon2020fok}.\n        *   Demonstrates that the exponentially growing receptive field of a node, when compressed into a fixed-length vector, causes information loss.\n        *   Proposes a simple diagnostic and mitigation technique: adding a \"fully-adjacent layer\" (FA) as the final GNN layer. This involves setting the neighborhood `N_v` to include all nodes `V` for that specific layer, allowing direct interaction between all node representations without adding new weights or changing the layer type \\cite{alon2020fok}.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Insights:** Formalization of the \"over-squashing\" bottleneck, explaining how the exponential growth of a node's receptive field leads to information compression and loss in fixed-size message passing vectors \\cite{alon2020fok}.\n    *   **Empirical Methodology:** Design of a synthetic \"NEIGHBORS MATCH\" benchmark to precisely control the problem radius (`r`) and empirically demonstrate the onset and impact of over-squashing on GNN training accuracy \\cite{alon2020fok}.\n    *   **Diagnostic/Mitigation Technique:** Introduction of the \"fully-adjacent layer\" (FA) as a simple, untuned method to alleviate the bottleneck and improve performance on long-range tasks, serving as evidence for the existence and impact of over-squashing \\cite{alon2020fok}.\n    *   **Comparative Analysis:** Shows that GNNs with simpler aggregation schemes (e.g., GCN, GIN) are more susceptible to over-squashing than those incorporating attention or gating mechanisms (e.g., GAT, GGNN) \\cite{alon2020fok}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Synthetic Benchmark (\"NEIGHBORS MATCH\"):** GNNs (GCN, GIN, GAT, GGNN) were tested on a binary tree structure where the problem radius (`r`) was controlled by tree depth.\n        *   **Real-world Datasets:** Extensively tuned GNN models on QM9 (quantum chemistry), ENZYMES, NCI1 (bioinformatics), and VARMISUSE (code analysis) were re-evaluated by adding a fully-adjacent layer (FA) to their final layer, *without any additional hyperparameter tuning* \\cite{alon2020fok}.\n    *   **Key Performance Metrics & Results:**\n        *   **NEIGHBORS MATCH:** GCN and GIN failed to achieve 100% training accuracy starting from `r=4`, while GAT and GGNN failed at `r=5`, demonstrating underfitting due to over-squashing \\cite{alon2020fok}.\n        *   **QM9:** The FA layer significantly reduced average error rates across 13 properties (e.g., -39.54% for R-GIN, -44.58% for R-GAT, -47.42% for GGNN) \\cite{alon2020fok}.\n        *   **ENZYMES & NCI1:** The FA layer reduced error rates by 12% and 4.8% respectively for GIN and GCN models \\cite{alon2020fok}.\n        *   **VARMISUSE:** The FA layer improved accuracy for GCN and GIN models \\cite{alon2020fok}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:** The paper primarily focuses on diagnosing and demonstrating the over-squashing problem rather than proposing a complex, novel GNN architecture. The \"fully-adjacent layer\" is presented as a simple proof-of-concept for breaking the bottleneck, not necessarily an optimized solution \\cite{alon2020fok}.\n    *   **Scope of Applicability:** The analysis and proposed phenomenon are most relevant for problems requiring *long-range interactions* (large problem radii). For tasks with small problem radii, other factors like over-smoothing might still be more dominant \\cite{alon2020fok}.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** This work provides a crucial new theoretical and empirical understanding of a fundamental limitation in GNNs, shifting the focus to \"over-squashing\" as a primary impediment for long-range graph problems \\cite{alon2020fok}. It challenges the prevailing \"over-smoothing\" narrative for these types of tasks.\n    *   **Potential Impact:** It guides future GNN research towards designing architectures that explicitly mitigate information bottlenecks, potentially leading to more robust and effective models for complex graph-structured data requiring extensive message propagation. The simple FA layer also offers a practical diagnostic tool for researchers to assess over-squashing in their own GNN applications \\cite{alon2020fok}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel at learning from graph-structured data, yet their ability to capture long-range dependencies remains a critical bottleneck, hindering performance in vital domains like quantum chemistry and program analysis. This paper unveils \"over-squashing,\" a fundamental limitation distinct from \"over-smoothing,\" where the exponentially growing receptive field of a node forces crucial information from distant neighbors into fixed-size message passing vectors, leading to severe information loss. We formally characterize this phenomenon and demonstrate its impact on popular GNN architectures through a novel synthetic benchmark. Crucially, we introduce the \"fully-adjacent layer\" (FA), a simple, untuned diagnostic and mitigation technique that significantly alleviates over-squashing. Applying the FA layer to state-of-the-art GNNs yields substantial performance improvements on real-world datasets, including QM9, ENZYMES, NCI1, and VARMISUSE. Our findings provide a new theoretical lens for understanding GNN limitations and offer practical guidance for designing more robust GNNs capable of truly global reasoning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "over-squashing",
      "long-range interactions",
      "information bottleneck",
      "receptive field",
      "fully-adjacent layer (FA layer)",
      "synthetic benchmark",
      "GNN limitations",
      "over-smoothing",
      "quantum chemistry",
      "program analysis",
      "biological systems",
      "empirical validation"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf",
    "citation_key": "alon2020fok",
    "metadata": {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "authors": [
        "Uri Alon",
        "Eran Yahav"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.",
      "file_path": "paper_data/Graph_Neural_Networks/3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) struggle to propagate information effectively between distant nodes in a graph, leading to poor performance on tasks requiring long-range interactions \\cite{alon2020fok}.\n    *   **Importance & Challenge:** Many real-world domains (e.g., quantum chemistry, program analysis, biological systems) are naturally represented as graphs and require GNNs to capture dependencies between non-adjacent nodes. The problem is challenging because, as the number of GNN layers increases to capture longer ranges, the receptive field of a node grows exponentially, leading to an information bottleneck \\cite{alon2020fok}. This phenomenon, termed \"over-squashing,\" is distinct from \"over-smoothing\" which primarily affects short-range tasks.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work analyzes the limitations of popular GNN variants like GCN, GIN, GAT, and GGNN, particularly when applied to problems with large \"problem radii\" (requiring long-range interactions) \\cite{alon2020fok}.\n    *   **Limitations of Previous Solutions:** Previous explanations for GNN performance limitations, such as \"over-smoothing,\" are argued to be more relevant for short-range tasks. For long-range tasks, `\\cite{alon2020fok}` posits that over-squashing is the dominant issue, where exponentially growing information is compressed into fixed-size vectors, causing crucial messages from distant nodes to be lost. The paper draws an analogy to the bottleneck in sequential RNN models but highlights that GNNs' exponentially growing receptive fields make their bottleneck asymptotically more severe \\cite{alon2020fok}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's primary contribution is the identification and characterization of the \"over-squashing\" phenomenon as a fundamental bottleneck in GNNs for long-range problems \\cite{alon2020fok}. It does not propose a new GNN architecture but rather explains an inherent limitation.\n    *   **Novelty:**\n        *   Introduces \"over-squashing\" as a novel explanation for GNNs' failure to learn long-range dependencies, distinct from over-smoothing \\cite{alon2020fok}.\n        *   Demonstrates that the exponentially growing receptive field of a node, when compressed into a fixed-length vector, causes information loss.\n        *   Proposes a simple diagnostic and mitigation technique: adding a \"fully-adjacent layer\" (FA) as the final GNN layer. This involves setting the neighborhood `N_v` to include all nodes `V` for that specific layer, allowing direct interaction between all node representations without adding new weights or changing the layer type \\cite{alon2020fok}.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Insights:** Formalization of the \"over-squashing\" bottleneck, explaining how the exponential growth of a node's receptive field leads to information compression and loss in fixed-size message passing vectors \\cite{alon2020fok}.\n    *   **Empirical Methodology:** Design of a synthetic \"NEIGHBORS MATCH\" benchmark to precisely control the problem radius (`r`) and empirically demonstrate the onset and impact of over-squashing on GNN training accuracy \\cite{alon2020fok}.\n    *   **Diagnostic/Mitigation Technique:** Introduction of the \"fully-adjacent layer\" (FA) as a simple, untuned method to alleviate the bottleneck and improve performance on long-range tasks, serving as evidence for the existence and impact of over-squashing \\cite{alon2020fok}.\n    *   **Comparative Analysis:** Shows that GNNs with simpler aggregation schemes (e.g., GCN, GIN) are more susceptible to over-squashing than those incorporating attention or gating mechanisms (e.g., GAT, GGNN) \\cite{alon2020fok}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Synthetic Benchmark (\"NEIGHBORS MATCH\"):** GNNs (GCN, GIN, GAT, GGNN) were tested on a binary tree structure where the problem radius (`r`) was controlled by tree depth.\n        *   **Real-world Datasets:** Extensively tuned GNN models on QM9 (quantum chemistry), ENZYMES, NCI1 (bioinformatics), and VARMISUSE (code analysis) were re-evaluated by adding a fully-adjacent layer (FA) to their final layer, *without any additional hyperparameter tuning* \\cite{alon2020fok}.\n    *   **Key Performance Metrics & Results:**\n        *   **NEIGHBORS MATCH:** GCN and GIN failed to achieve 100% training accuracy starting from `r=4`, while GAT and GGNN failed at `r=5`, demonstrating underfitting due to over-squashing \\cite{alon2020fok}.\n        *   **QM9:** The FA layer significantly reduced average error rates across 13 properties (e.g., -39.54% for R-GIN, -44.58% for R-GAT, -47.42% for GGNN) \\cite{alon2020fok}.\n        *   **ENZYMES & NCI1:** The FA layer reduced error rates by 12% and 4.8% respectively for GIN and GCN models \\cite{alon2020fok}.\n        *   **VARMISUSE:** The FA layer improved accuracy for GCN and GIN models \\cite{alon2020fok}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:** The paper primarily focuses on diagnosing and demonstrating the over-squashing problem rather than proposing a complex, novel GNN architecture. The \"fully-adjacent layer\" is presented as a simple proof-of-concept for breaking the bottleneck, not necessarily an optimized solution \\cite{alon2020fok}.\n    *   **Scope of Applicability:** The analysis and proposed phenomenon are most relevant for problems requiring *long-range interactions* (large problem radii). For tasks with small problem radii, other factors like over-smoothing might still be more dominant \\cite{alon2020fok}.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** This work provides a crucial new theoretical and empirical understanding of a fundamental limitation in GNNs, shifting the focus to \"over-squashing\" as a primary impediment for long-range graph problems \\cite{alon2020fok}. It challenges the prevailing \"over-smoothing\" narrative for these types of tasks.\n    *   **Potential Impact:** It guides future GNN research towards designing architectures that explicitly mitigate information bottlenecks, potentially leading to more robust and effective models for complex graph-structured data requiring extensive message propagation. The simple FA layer also offers a practical diagnostic tool for researchers to assess over-squashing in their own GNN applications \\cite{alon2020fok}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "over-squashing",
        "long-range interactions",
        "information bottleneck",
        "receptive field",
        "fully-adjacent layer (FA layer)",
        "synthetic benchmark",
        "GNN limitations",
        "over-smoothing",
        "quantum chemistry",
        "program analysis",
        "biological systems",
        "empirical validation"
      ],
      "paper_type": "based on the abstract and introduction, this paper is best classified as **empirical**.\n\nhere's why:\n\n*   **proposes a new explanation:** the abstract states, \"we propose a new explanation for this problem: gnns are susceptible to a bottleneck...\" this is a conceptual contribution.\n*   **strong emphasis on demonstration and findings:** however, the majority of the abstract and introduction snippet focuses on *demonstrating* and *showing* the effects of this proposed bottleneck:\n    *   \"...we highlight the inherent problem of over-squashing in gnns: we **demonstrate** that the bottleneck hinders popular gnns from ﬁtting long-range signals in the training data;\"\n    *   \"we further **show** that gnns that absorb incoming edges equally... are more susceptible to over-squashing than gat and ggnn;\"\n    *   \"ﬁnally, we **show** that prior work... suffer from over-squashing, and that breaking the bottleneck **improves their state-of-the-art results** without any tuning or additional weights.\"\n*   **data-driven evidence:** phrases like \"demonstrate,\" \"show,\" \"hinders popular gnns from ﬁtting long-range signals in the training data,\" and \"improves their state-of-the-art results\" strongly indicate that the paper presents findings derived from experiments and data analysis.\n*   **code availability:** \"our code is available at https://github.com/tech-srl/bottleneck/\" further supports the idea of reproducible experiments and empirical validation.\n\nwhile the paper proposes a new *explanation* (which could have theoretical or technical aspects), its primary contribution as described in these sections is the *empirical validation* of this explanation and its practical implications through data-driven studies and comparisons."
    },
    "file_name": "3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf"
  },
  {
    "success": true,
    "doc_id": "3db8b556c6e4b4c510c81f5f71fed52a",
    "summary": "Here's a focused summary of the paper `\\cite{zhu2021zc3}` for a literature review:\n\n### Technical Paper Analysis: Interpreting and Unifying Graph Neural Networks with An Optimization Framework \\cite{zhu2021zc3}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Despite the proliferation of various Graph Neural Networks (GNNs) that largely follow a message passing paradigm, there is a lack of a unified understanding and analysis of their fundamental propagation mechanisms and their essential relations.\n    *   **Importance and Challenge**: Understanding the underlying mathematical principles governing diverse GNN propagation strategies is crucial for providing a macroscopic view, identifying weaknesses in current GNNs, and opening new opportunities for principled GNN design. The challenge lies in abstracting commonalities from seemingly disparate propagation mechanisms.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing theoretical analyses of GNNs primarily focus on spectral filtering characteristics (e.g., low-pass filtering, Laplacian smoothing), the over-smoothing problem, or the expressive power/capability of GNNs.\n    *   **Limitations of Previous Solutions**: Previous works do not theoretically analyze the *intrinsic connections* and unifying principles behind the diverse propagation mechanisms of different GNNs. `\\cite{zhu2021zc3}` aims to fill this gap by establishing a fundamental mathematical guideline that governs these mechanisms.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{zhu2021zc3}` proposes a unified optimization framework that interprets the propagation mechanisms of various GNNs as optimal solutions to a common objective function. This framework consists of two main terms:\n        *   **Feature Fitting Term (`O_fit`)**: `$\\|F_1Z - F_2H\\|_F^2$` which flexibly encodes information from an initial feature transformation `H` into the learned representation `Z` by designing different graph convolutional kernels `F_1` and `F_2`. These kernels can represent all-pass, low-pass, or high-pass filtering capabilities (e.g., `I`, `$\\hat{A}$`, `$\\tilde{L}$`).\n        *   **Graph Laplacian Regularization Term (`O_reg`)**: `$\\xi tr(Z^T \\tilde{L} Z)$` which acts as a feature smoothing term, constraining connected nodes to have similar representations, thereby capturing the homophily property of graphs.\n    *   **Novelty**:\n        *   Establishing a surprising connection that unifies the propagation mechanisms of representative GNNs (GCN, SGC, PPNP, APPNP, JKNet, DAGNN) under a single optimization framework.\n        *   Introducing the concept of *flexible graph convolutional kernels* within the feature fitting term, which allows for the design of GNNs with specific low-pass or high-pass filtering capabilities, moving beyond \"naïve\" kernels used in existing works.\n        *   Providing a principled, interpretable path for designing new GNNs by optimizing an objective function, rather than solely focusing on specific spectral filters or aggregation strategies.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unified objective optimization framework that theoretically unifies the propagation mechanisms of a series of GNNs.\n        *   Two novel deep GNNs, GNN-LF (Low-Frequency) and GNN-HF (High-Frequency), derived from the framework, which utilize flexible low-pass and high-pass filtering kernels, respectively.\n    *   **Theoretical Insights/Analysis**:\n        *   Theoretical proofs demonstrating that the propagation processes of GCN, SGC, PPNP, APPNP, JKNet, and DAGNN can be interpreted as optimizing specific instances of the proposed unified framework.\n        *   Convergence proofs for the newly proposed GNN-LF and GNN-HF models.\n        *   Expressive power comparisons for the proposed models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a series of benchmark datasets (e.g., for node classification tasks).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The proposed GNN-LF and GNN-HF models consistently *outperform state-of-the-art methods* on these benchmarks.\n        *   The experiments also demonstrate that the proposed GNNs have a *good ability to alleviate over-smoothing*, a common challenge in deep GNNs.\n        *   The results empirically verify the feasibility and effectiveness of designing GNNs using the proposed unified optimization framework.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework primarily focuses on interpreting and designing the *propagation mechanisms* of GNNs, particularly those involving feature aggregation and transformation. While it addresses over-smoothing, it doesn't explicitly delve into other GNN challenges like scalability for extremely large graphs, inductive learning across different graph structures, or specific architectural choices beyond the propagation core. The unification is shown for a \"series of GNNs,\" implying it may not cover every single GNN variant, but rather representative and widely used ones.\n    *   **Scope of Applicability**: The framework is broadly applicable to GNNs that can be characterized by a feature fitting term and a graph regularization term, offering a principled way to understand and design GNNs with specific filtering characteristics (low-pass, high-pass, all-pass).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{zhu2021zc3}` significantly advances the technical state-of-the-art by providing a novel, unified, and interpretable mathematical framework for understanding the propagation mechanisms of diverse GNNs. This moves beyond ad-hoc design to a more principled approach.\n    *   **Potential Impact on Future Research**:\n        *   Offers a \"macroscopic view\" for surveying relations between different GNNs, fostering deeper theoretical understanding.\n        *   Opens up new opportunities for flexibly designing novel, interpretable GNNs by formulating them as optimization problems with specific graph kernels and regularization.\n        *   Provides a theoretical foundation for developing GNNs that can explicitly control frequency filtering (low-pass/high-pass) to address issues like over-smoothing, leading to more robust and effective models.",
    "intriguing_abstract": "Despite the remarkable success of Graph Neural Networks (GNNs) across various domains, a unified theoretical understanding of their diverse message passing propagation mechanisms remains elusive. This paper introduces a groundbreaking **unified optimization framework** that fundamentally reinterprets the propagation processes of prominent GNNs, including GCN, SGC, PPNP, APPNP, JKNet, and DAGNN, as optimal solutions to a common objective function. Our framework comprises a flexible **feature fitting term** utilizing novel **graph convolutional kernels** (all-pass, low-pass, high-pass) and a **Graph Laplacian regularization term** for smoothing. This principled approach moves beyond ad-hoc designs, offering a macroscopic view and a systematic methodology for constructing new GNNs. We derive two novel deep GNNs, GNN-LF and GNN-HF, which not only consistently **outperform state-of-the-art methods** on node classification benchmarks but also effectively **alleviate the over-smoothing problem**. This work provides a pivotal theoretical foundation, paving the way for the principled design of interpretable and robust GNNs with explicit control over their spectral filtering characteristics.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "unified optimization framework",
      "GNN propagation mechanisms",
      "feature fitting term",
      "Graph Laplacian regularization",
      "flexible graph convolutional kernels",
      "low-pass/high-pass filtering",
      "over-smoothing alleviation",
      "principled GNN design",
      "GNN-LF and GNN-HF models",
      "theoretical analysis",
      "state-of-the-art performance"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/4b776e7f26464e5b230c1679560f12618730dcc6.pdf",
    "citation_key": "zhu2021zc3",
    "metadata": {
      "title": "Interpreting and Unifying Graph Neural Networks with An Optimization Framework",
      "authors": [
        "Meiqi Zhu",
        "Xiao Wang",
        "C. Shi",
        "Houye Ji",
        "Peng Cui"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naïve graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",
      "file_path": "paper_data/Graph_Neural_Networks/4b776e7f26464e5b230c1679560f12618730dcc6.pdf",
      "venue": "The Web Conference",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper `\\cite{zhu2021zc3}` for a literature review:\n\n### Technical Paper Analysis: Interpreting and Unifying Graph Neural Networks with An Optimization Framework \\cite{zhu2021zc3}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Despite the proliferation of various Graph Neural Networks (GNNs) that largely follow a message passing paradigm, there is a lack of a unified understanding and analysis of their fundamental propagation mechanisms and their essential relations.\n    *   **Importance and Challenge**: Understanding the underlying mathematical principles governing diverse GNN propagation strategies is crucial for providing a macroscopic view, identifying weaknesses in current GNNs, and opening new opportunities for principled GNN design. The challenge lies in abstracting commonalities from seemingly disparate propagation mechanisms.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing theoretical analyses of GNNs primarily focus on spectral filtering characteristics (e.g., low-pass filtering, Laplacian smoothing), the over-smoothing problem, or the expressive power/capability of GNNs.\n    *   **Limitations of Previous Solutions**: Previous works do not theoretically analyze the *intrinsic connections* and unifying principles behind the diverse propagation mechanisms of different GNNs. `\\cite{zhu2021zc3}` aims to fill this gap by establishing a fundamental mathematical guideline that governs these mechanisms.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{zhu2021zc3}` proposes a unified optimization framework that interprets the propagation mechanisms of various GNNs as optimal solutions to a common objective function. This framework consists of two main terms:\n        *   **Feature Fitting Term (`O_fit`)**: `$\\|F_1Z - F_2H\\|_F^2$` which flexibly encodes information from an initial feature transformation `H` into the learned representation `Z` by designing different graph convolutional kernels `F_1` and `F_2`. These kernels can represent all-pass, low-pass, or high-pass filtering capabilities (e.g., `I`, `$\\hat{A}$`, `$\\tilde{L}$`).\n        *   **Graph Laplacian Regularization Term (`O_reg`)**: `$\\xi tr(Z^T \\tilde{L} Z)$` which acts as a feature smoothing term, constraining connected nodes to have similar representations, thereby capturing the homophily property of graphs.\n    *   **Novelty**:\n        *   Establishing a surprising connection that unifies the propagation mechanisms of representative GNNs (GCN, SGC, PPNP, APPNP, JKNet, DAGNN) under a single optimization framework.\n        *   Introducing the concept of *flexible graph convolutional kernels* within the feature fitting term, which allows for the design of GNNs with specific low-pass or high-pass filtering capabilities, moving beyond \"naïve\" kernels used in existing works.\n        *   Providing a principled, interpretable path for designing new GNNs by optimizing an objective function, rather than solely focusing on specific spectral filters or aggregation strategies.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unified objective optimization framework that theoretically unifies the propagation mechanisms of a series of GNNs.\n        *   Two novel deep GNNs, GNN-LF (Low-Frequency) and GNN-HF (High-Frequency), derived from the framework, which utilize flexible low-pass and high-pass filtering kernels, respectively.\n    *   **Theoretical Insights/Analysis**:\n        *   Theoretical proofs demonstrating that the propagation processes of GCN, SGC, PPNP, APPNP, JKNet, and DAGNN can be interpreted as optimizing specific instances of the proposed unified framework.\n        *   Convergence proofs for the newly proposed GNN-LF and GNN-HF models.\n        *   Expressive power comparisons for the proposed models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a series of benchmark datasets (e.g., for node classification tasks).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The proposed GNN-LF and GNN-HF models consistently *outperform state-of-the-art methods* on these benchmarks.\n        *   The experiments also demonstrate that the proposed GNNs have a *good ability to alleviate over-smoothing*, a common challenge in deep GNNs.\n        *   The results empirically verify the feasibility and effectiveness of designing GNNs using the proposed unified optimization framework.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework primarily focuses on interpreting and designing the *propagation mechanisms* of GNNs, particularly those involving feature aggregation and transformation. While it addresses over-smoothing, it doesn't explicitly delve into other GNN challenges like scalability for extremely large graphs, inductive learning across different graph structures, or specific architectural choices beyond the propagation core. The unification is shown for a \"series of GNNs,\" implying it may not cover every single GNN variant, but rather representative and widely used ones.\n    *   **Scope of Applicability**: The framework is broadly applicable to GNNs that can be characterized by a feature fitting term and a graph regularization term, offering a principled way to understand and design GNNs with specific filtering characteristics (low-pass, high-pass, all-pass).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{zhu2021zc3}` significantly advances the technical state-of-the-art by providing a novel, unified, and interpretable mathematical framework for understanding the propagation mechanisms of diverse GNNs. This moves beyond ad-hoc design to a more principled approach.\n    *   **Potential Impact on Future Research**:\n        *   Offers a \"macroscopic view\" for surveying relations between different GNNs, fostering deeper theoretical understanding.\n        *   Opens up new opportunities for flexibly designing novel, interpretable GNNs by formulating them as optimization problems with specific graph kernels and regularization.\n        *   Provides a theoretical foundation for developing GNNs that can explicitly control frequency filtering (low-pass/high-pass) to address issues like over-smoothing, leading to more robust and effective models.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "unified optimization framework",
        "GNN propagation mechanisms",
        "feature fitting term",
        "Graph Laplacian regularization",
        "flexible graph convolutional kernels",
        "low-pass/high-pass filtering",
        "over-smoothing alleviation",
        "principled GNN design",
        "GNN-LF and GNN-HF models",
        "theoretical analysis",
        "state-of-the-art performance"
      ],
      "paper_type": "the paper should be classified as **theoretical**.\n\nhere's why:\n\n1.  **unified optimization framework:** the abstract states, \"we establish a surprising connection between different propagation mechanisms with a unified optimization problem,\" and \"our proposed unified optimization framework... provides a macroscopic view on surveying the relations between different gnns.\" this indicates a focus on developing a formal, mathematical model to understand and unify existing approaches.\n2.  **mathematical analysis and proofs:** the abstract explicitly mentions, \"showing that despite the proliferation of various gnns, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term.\" this is a strong claim derived from mathematical analysis. furthermore, \"moreover, we provide the convergence proofs and expressive power comparisons for the proposed models\" directly aligns with the \"theoretical\" criteria of \"mathematical analysis, proofs, formal models.\"\n3.  **interpretation and unification:** the title \"interpreting and unifying graph neural networks\" suggests a deep analytical and foundational understanding, which is characteristic of theoretical work. the introduction's question \"is there a unified mathematical g...\" further reinforces this.\n4.  **technical and empirical aspects are secondary:** while the paper does \"develop two novel objective functions\" (technical) and conducts \"extensive experiments on benchmark datasets\" (empirical), these aspects serve to validate and demonstrate the utility of the *proposed theoretical framework* and its derived models. the core contribution lies in the formal interpretation, unification, and the proofs of properties."
    },
    "file_name": "4b776e7f26464e5b230c1679560f12618730dcc6.pdf"
  },
  {
    "success": true,
    "doc_id": "0d98d100b2aa2bcbcb643b1a8d498977",
    "summary": "Here's a focused summary of the paper \"TDGIA: Effective Injection Attacks on Graph Neural Networks\" \\cite{zou2021qkz} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the vulnerability of Graph Neural Networks (GNNs) to a realistic adversarial attack scenario called **Graph Injection Attack (GIA)**. In GIA, an adversary cannot modify existing graph structures or node attributes but can inject new adversarial nodes and connect them to the original graph.\n    *   **Importance and Challenge:**\n        *   GNNs are widely used, making their security against adversarial attacks a critical concern.\n        *   GIA is more realistic than Graph Modification Attack (GMA) as injecting new data (e.g., fake accounts, papers) is often more feasible than altering existing, established data.\n        *   GIA poses unique challenges: determining where to connect injected nodes and how to generate their features from scratch.\n        *   Existing GIA solutions (e.g., from KDD-CUP 2020) showed limited effectiveness and lacked principled approaches, especially under challenging black-box and evasion attack settings on large-scale graphs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Previous GNN attacks primarily focused on **Graph Modification Attack (GMA)** \\cite{zou2021qkz} (e.g., Nettack \\cite{zou2021qkz}, Meta-attack \\cite{zou2021qkz}), which involves altering existing edges or node features.\n        *   More recent works introduced GIA, such as NIPA \\cite{zou2021qkz} (reinforcement learning-based) and AFGSM \\cite{zou2021qkz} (Fast Gradient Sign Method-based).\n    *   **Limitations of Previous Solutions:**\n        *   GMA is often unrealistic in real-world scenarios.\n        *   Existing GIA methods (NIPA \\cite{zou2021qkz}, AFGSM \\cite{zou2021qkz}) operate under the **poison attack setting**, requiring model retraining, which is less practical than the **evasion attack setting** (inference-time attack) adopted by \\cite{zou2021qkz}.\n        *   NIPA \\cite{zou2021qkz} is not scalable to large graphs due to its reinforcement learning nature.\n        *   AFGSM \\cite{zou2021qkz} uses a less general approach for leveraging topological information.\n        *   Prior attacks were often evaluated against weaker defense models, whereas \\cite{zou2021qkz} targets robust defenses from KDD-CUP 2020.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (TDGIA - Topological Defective Graph Injection Attack) \\cite{zou2021qkz}:**\n        *   **Theoretical Analysis:** The paper first provides a theoretical foundation, proving that non-structural-ignorant and permutation-invariant GNNs are inherently GIA-attackable (Lemma 4.1) \\cite{zou2021qkz}. It also analyzes how GNN layers aggregate information and how injected nodes can topologically perturb these aggregations.\n        *   **Two-Module Framework:** TDGIA consists of two main components:\n            1.  **Topological Defective Edge Selection:** This module identifies \"topologically vulnerable\" existing nodes in the original graph. These nodes are strategically chosen to connect with the injected adversarial nodes, maximizing the attack's impact.\n            2.  **Smooth Adversarial Optimization:** This module generates the features for the injected nodes. It employs a specially designed smooth loss function to optimize these features, aiming to significantly degrade the target GNN model's performance.\n        *   **Sequential Injection:** Injected nodes are added sequentially, building upon the selected defective edges.\n        *   **Black-box Evasion Attack:** TDGIA operates in a black-box setting by training a surrogate model to generate attacks, which are then transferred to the unknown target GNNs during inference.\n    *   **Novelty & Differentiation:**\n        *   **Realistic Attack Setting:** First principled GIA method designed for the challenging **black-box and evasion attack settings** on large-scale graphs, unlike prior poison attacks.\n        *   **Principled Vulnerability Exploitation:** Leverages a theoretical understanding of GNN topological vulnerabilities to guide both edge selection and feature generation, leading to more effective attacks.\n        *   **Scalability:** Designed to be applicable to large-scale graphs, addressing a key limitation of previous RL-based GIA methods.\n        *   **Integrated Solution:** Systematically addresses both critical aspects of GIA (where to connect and what features to use) with dedicated, optimized modules.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Topological Defective Edge Selection Strategy:** A novel algorithm for identifying optimal existing nodes to connect with injected adversarial nodes, based on graph topological vulnerabilities.\n        *   **Smooth Feature Optimization Objective:** A new loss function specifically designed for generating effective and optimized features for injected nodes.\n    *   **Theoretical Insights/Analysis:**\n        *   Formal proof (Lemma 4.1) demonstrating the inherent GIA-attackability of non-structural-ignorant and permutation-invariant GNN models \\cite{zou2021qkz}.\n        *   Detailed analysis of how GNN layers aggregate information and how topological perturbations from injected nodes propagate to influence node embeddings.\n    *   **System Design:** A robust two-module framework (edge selection and feature optimization) tailored for effective and scalable GIA in black-box, evasion scenarios.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Extensive experiments were performed on **large-scale datasets**, including the KDD-CUP 2020 competition dataset (over 600,000 nodes).\n        *   Evaluated against **dozens of diverse defense GNN models**, including top solutions from KDD-CUP 2020, not just weak baselines.\n        *   Compared against various attack baselines, including the best performing submissions from KDD-CUP 2020.\n        *   Ablation studies were conducted to confirm the effectiveness of each component of TDGIA.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Measured by the **weighted average accuracy** of target GNN models.\n        *   TDGIA \\cite{zou2021qkz} consistently and significantly **outperformed all baselines**.\n        *   **Quantitatively, TDGIA achieved an 8.08% performance drop** on target GNNs, which is more than double the damage (3.7%) caused by the best KDD-CUP 2020 attack solution (u1234) \\cite{zou2021qkz}, representing a 118% increase in attack effectiveness.\n        *   This high attack performance was achieved by injecting only a **limited number of nodes** (e.g., 1% of target nodes).\n        *   Demonstrated strong **transferability** across different GNN models in the black-box setting.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   Relies on the effectiveness of **transfer attacks** from surrogate models, which can vary depending on the similarity between surrogate and target models.\n        *   The theoretical analysis of topological vulnerability uses first-order approximations, which might not fully capture complex non-linearities in very deep GNNs.\n        *   The attack adheres to budget constraints on injected nodes (number, degree, feature norm) to ensure unnoticeability, but the sensitivity to these constraints is not deeply explored.\n    *   **Scope of Applicability:**\n        *   Primarily focused on **node classification tasks** on attributed graphs.\n        *   Specifically designed for **black-box and evasion attack settings** in GIA.\n        *   Demonstrated on large-scale citation networks, suggesting applicability to similar graph types.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:**\n        *   Establishes a new state-of-the-art for **Graph Injection Attacks (GIA)**, particularly in the challenging black-box and evasion settings \\cite{zou2021qkz}.\n        *   Provides a principled and highly effective framework that significantly outperforms previous best solutions, even against robust defense models.\n        *   Offers a scalable solution for GIA on large-scale graphs, addressing a critical gap in existing methods.\n    *   **Potential Impact on Future Research:**\n        *   **GNN Robustness:** The demonstrated high effectiveness of TDGIA \\cite{zou2021qkz} underscores the urgent need for developing more robust GNN architectures and defense mechanisms against realistic injection attacks.\n        *   **Adversarial Machine Learning on Graphs:** Provides a strong benchmark and a novel methodology for future research in graph adversarial attacks, especially for real-world, large-scale applications.\n        *   **Theoretical Understanding:** The theoretical analysis of GNN vulnerability to GIA can guide the design of inherently more secure graph learning models.",
    "intriguing_abstract": "Despite their widespread adoption, Graph Neural Networks (GNNs) face critical vulnerabilities to realistic adversarial attacks. This paper introduces **Graph Injection Attack (GIA)**, a particularly insidious threat where an adversary injects new malicious nodes and connections without altering the original graph—a scenario far more feasible than modifying established data. We present **TDGIA (Topological Defective Graph Injection Attack)**, the first principled and highly effective GIA framework designed for the challenging **black-box and evasion attack settings** on large-scale graphs.\n\nTDGIA is grounded in a novel theoretical analysis proving the inherent GIA-attackability of GNNs. It leverages a sophisticated two-module approach: **Topological Defective Edge Selection** strategically identifies vulnerable connection points, while **Smooth Adversarial Optimization** generates potent features for injected nodes. Our experiments on large datasets, including KDD-CUP 2020, demonstrate TDGIA's unprecedented effectiveness, achieving over double the performance degradation of state-of-the-art baselines by injecting only a limited number of nodes. This work not only sets a new benchmark for adversarial machine learning on graphs but also underscores the urgent need for developing robust GNN architectures against these scalable, real-world threats.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Graph Injection Attack (GIA)",
      "TDGIA",
      "black-box evasion attack",
      "topological vulnerability",
      "Topological Defective Edge Selection",
      "Smooth Adversarial Optimization",
      "large-scale graphs",
      "node classification",
      "surrogate model transferability",
      "GNN robustness",
      "state-of-the-art GIA"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/218223e91f55a1e0186f5b008b55f5e0fe350698.pdf",
    "citation_key": "zou2021qkz",
    "metadata": {
      "title": "TDGIA: Effective Injection Attacks on Graph Neural Networks",
      "authors": [
        "Xu Zou",
        "Qinkai Zheng",
        "Yuxiao Dong",
        "Xinyu Guan",
        "E. Kharlamov",
        "Jialiang Lu",
        "Jie Tang"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks. In this paper, we study a recently-introduced realistic attack scenario on graphs---graph injection attack (GIA). In the GIA scenario, the adversary is not able to modify the existing link structure and node attributes of the input graph, instead the attack is performed by injecting adversarial nodes into it. We present an analysis on the topological vulnerability of GNNs under GIA setting, based on which we propose the Topological Defective Graph Injection Attack (TDGIA) for effective injection attacks. TDGIA first introduces the topological defective edge selection strategy to choose the original nodes for connecting with the injected ones. It then designs the smooth feature optimization objective to generate the features for the injected nodes. Extensive experiments on large-scale datasets show that TDGIA can consistently and significantly outperform various attack baselines in attacking dozens of defense GNN models. Notably, the performance drop on target GNNs resultant from TDGIA is more than double the damage brought by the best attack solution among hundreds of submissions on KDD-CUP 2020.",
      "file_path": "paper_data/Graph_Neural_Networks/218223e91f55a1e0186f5b008b55f5e0fe350698.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"TDGIA: Effective Injection Attacks on Graph Neural Networks\" \\cite{zou2021qkz} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the vulnerability of Graph Neural Networks (GNNs) to a realistic adversarial attack scenario called **Graph Injection Attack (GIA)**. In GIA, an adversary cannot modify existing graph structures or node attributes but can inject new adversarial nodes and connect them to the original graph.\n    *   **Importance and Challenge:**\n        *   GNNs are widely used, making their security against adversarial attacks a critical concern.\n        *   GIA is more realistic than Graph Modification Attack (GMA) as injecting new data (e.g., fake accounts, papers) is often more feasible than altering existing, established data.\n        *   GIA poses unique challenges: determining where to connect injected nodes and how to generate their features from scratch.\n        *   Existing GIA solutions (e.g., from KDD-CUP 2020) showed limited effectiveness and lacked principled approaches, especially under challenging black-box and evasion attack settings on large-scale graphs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Previous GNN attacks primarily focused on **Graph Modification Attack (GMA)** \\cite{zou2021qkz} (e.g., Nettack \\cite{zou2021qkz}, Meta-attack \\cite{zou2021qkz}), which involves altering existing edges or node features.\n        *   More recent works introduced GIA, such as NIPA \\cite{zou2021qkz} (reinforcement learning-based) and AFGSM \\cite{zou2021qkz} (Fast Gradient Sign Method-based).\n    *   **Limitations of Previous Solutions:**\n        *   GMA is often unrealistic in real-world scenarios.\n        *   Existing GIA methods (NIPA \\cite{zou2021qkz}, AFGSM \\cite{zou2021qkz}) operate under the **poison attack setting**, requiring model retraining, which is less practical than the **evasion attack setting** (inference-time attack) adopted by \\cite{zou2021qkz}.\n        *   NIPA \\cite{zou2021qkz} is not scalable to large graphs due to its reinforcement learning nature.\n        *   AFGSM \\cite{zou2021qkz} uses a less general approach for leveraging topological information.\n        *   Prior attacks were often evaluated against weaker defense models, whereas \\cite{zou2021qkz} targets robust defenses from KDD-CUP 2020.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (TDGIA - Topological Defective Graph Injection Attack) \\cite{zou2021qkz}:**\n        *   **Theoretical Analysis:** The paper first provides a theoretical foundation, proving that non-structural-ignorant and permutation-invariant GNNs are inherently GIA-attackable (Lemma 4.1) \\cite{zou2021qkz}. It also analyzes how GNN layers aggregate information and how injected nodes can topologically perturb these aggregations.\n        *   **Two-Module Framework:** TDGIA consists of two main components:\n            1.  **Topological Defective Edge Selection:** This module identifies \"topologically vulnerable\" existing nodes in the original graph. These nodes are strategically chosen to connect with the injected adversarial nodes, maximizing the attack's impact.\n            2.  **Smooth Adversarial Optimization:** This module generates the features for the injected nodes. It employs a specially designed smooth loss function to optimize these features, aiming to significantly degrade the target GNN model's performance.\n        *   **Sequential Injection:** Injected nodes are added sequentially, building upon the selected defective edges.\n        *   **Black-box Evasion Attack:** TDGIA operates in a black-box setting by training a surrogate model to generate attacks, which are then transferred to the unknown target GNNs during inference.\n    *   **Novelty & Differentiation:**\n        *   **Realistic Attack Setting:** First principled GIA method designed for the challenging **black-box and evasion attack settings** on large-scale graphs, unlike prior poison attacks.\n        *   **Principled Vulnerability Exploitation:** Leverages a theoretical understanding of GNN topological vulnerabilities to guide both edge selection and feature generation, leading to more effective attacks.\n        *   **Scalability:** Designed to be applicable to large-scale graphs, addressing a key limitation of previous RL-based GIA methods.\n        *   **Integrated Solution:** Systematically addresses both critical aspects of GIA (where to connect and what features to use) with dedicated, optimized modules.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Topological Defective Edge Selection Strategy:** A novel algorithm for identifying optimal existing nodes to connect with injected adversarial nodes, based on graph topological vulnerabilities.\n        *   **Smooth Feature Optimization Objective:** A new loss function specifically designed for generating effective and optimized features for injected nodes.\n    *   **Theoretical Insights/Analysis:**\n        *   Formal proof (Lemma 4.1) demonstrating the inherent GIA-attackability of non-structural-ignorant and permutation-invariant GNN models \\cite{zou2021qkz}.\n        *   Detailed analysis of how GNN layers aggregate information and how topological perturbations from injected nodes propagate to influence node embeddings.\n    *   **System Design:** A robust two-module framework (edge selection and feature optimization) tailored for effective and scalable GIA in black-box, evasion scenarios.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Extensive experiments were performed on **large-scale datasets**, including the KDD-CUP 2020 competition dataset (over 600,000 nodes).\n        *   Evaluated against **dozens of diverse defense GNN models**, including top solutions from KDD-CUP 2020, not just weak baselines.\n        *   Compared against various attack baselines, including the best performing submissions from KDD-CUP 2020.\n        *   Ablation studies were conducted to confirm the effectiveness of each component of TDGIA.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Measured by the **weighted average accuracy** of target GNN models.\n        *   TDGIA \\cite{zou2021qkz} consistently and significantly **outperformed all baselines**.\n        *   **Quantitatively, TDGIA achieved an 8.08% performance drop** on target GNNs, which is more than double the damage (3.7%) caused by the best KDD-CUP 2020 attack solution (u1234) \\cite{zou2021qkz}, representing a 118% increase in attack effectiveness.\n        *   This high attack performance was achieved by injecting only a **limited number of nodes** (e.g., 1% of target nodes).\n        *   Demonstrated strong **transferability** across different GNN models in the black-box setting.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   Relies on the effectiveness of **transfer attacks** from surrogate models, which can vary depending on the similarity between surrogate and target models.\n        *   The theoretical analysis of topological vulnerability uses first-order approximations, which might not fully capture complex non-linearities in very deep GNNs.\n        *   The attack adheres to budget constraints on injected nodes (number, degree, feature norm) to ensure unnoticeability, but the sensitivity to these constraints is not deeply explored.\n    *   **Scope of Applicability:**\n        *   Primarily focused on **node classification tasks** on attributed graphs.\n        *   Specifically designed for **black-box and evasion attack settings** in GIA.\n        *   Demonstrated on large-scale citation networks, suggesting applicability to similar graph types.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:**\n        *   Establishes a new state-of-the-art for **Graph Injection Attacks (GIA)**, particularly in the challenging black-box and evasion settings \\cite{zou2021qkz}.\n        *   Provides a principled and highly effective framework that significantly outperforms previous best solutions, even against robust defense models.\n        *   Offers a scalable solution for GIA on large-scale graphs, addressing a critical gap in existing methods.\n    *   **Potential Impact on Future Research:**\n        *   **GNN Robustness:** The demonstrated high effectiveness of TDGIA \\cite{zou2021qkz} underscores the urgent need for developing more robust GNN architectures and defense mechanisms against realistic injection attacks.\n        *   **Adversarial Machine Learning on Graphs:** Provides a strong benchmark and a novel methodology for future research in graph adversarial attacks, especially for real-world, large-scale applications.\n        *   **Theoretical Understanding:** The theoretical analysis of GNN vulnerability to GIA can guide the design of inherently more secure graph learning models.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Graph Injection Attack (GIA)",
        "TDGIA",
        "black-box evasion attack",
        "topological vulnerability",
        "Topological Defective Edge Selection",
        "Smooth Adversarial Optimization",
        "large-scale graphs",
        "node classification",
        "surrogate model transferability",
        "GNN robustness",
        "state-of-the-art GIA"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\n**reasoning:**\n\n*   **abstract keywords:** the abstract explicitly states \"we propose the topological defective graph injection attack (tdgia)\", \"tdgia first introduces the topological defective edge selection strategy\", and \"it then designs the smooth feature optimization objective\". these phrases directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n*   **introduction content:** the introduction sets up a technical problem (gnns are vulnerable to adversarial attacks, especially in the graph injection attack scenario) and the need for dedicated strategies, which the proposed tdgia aims to address.\n*   **empirical support:** while the abstract mentions \"extensive experiments on large-scale datasets show that tdgia can consistently and significantly outperform various attack baselines\", this empirical evidence serves to validate the effectiveness of the *proposed technical method*, rather than being the sole or primary focus of the paper as an \"empirical\" study. the core contribution is the new attack method itself."
    },
    "file_name": "218223e91f55a1e0186f5b008b55f5e0fe350698.pdf"
  },
  {
    "success": true,
    "doc_id": "489d2e3279fa97059afe5816ca0f7171",
    "summary": "Here's a focused summary of the paper \"Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective\" by Xu et al. \\cite{xu2019l8n} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Topology Attack and Defense for Graph Neural Networks \\cite{xu2019l8n}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** This paper addresses the lack of adversarial robustness in Graph Neural Networks (GNNs), specifically focusing on \"topology attacks\" where an adversary perturbs the graph structure (adding or deleting edges) to degrade GNN performance.\n    *   **Importance & Challenge:** GNNs have achieved significant success in tasks like semi-supervised node classification on graph-structured data (e.g., social networks, molecules). However, their vulnerability to adversarial attacks, especially in security-sensitive domains, is a critical concern. The primary challenge lies in applying gradient-based adversarial attack methods, which are successful in continuous domains (like images), to the inherently *discrete* nature of graph topology (edge additions/deletions).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous works have explored adversarial attacks on GNNs, including test-time and training-time attacks, and modifications to both discrete structures (edges) and node attributes \\cite{dai2018adversarial, zugner2018adversarial, bojcheski2018adversarial, zugner2019adversarial}.\n    *   **Limitations of Previous Solutions:** Existing methods often rely on static surrogate models, perturbation theory, or greedy search heuristics. Crucially, many suggested that conventional first-order continuous optimization methods do not directly apply to topology attacks due to the discrete nature of graphs. This paper explicitly aims to \"close this gap\" by enabling gradient-based attacks on discrete graph structures.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a novel optimization-based framework for both generating topology attacks and developing robust GNNs.\n        *   **Convex Relaxation:** To overcome the discrete nature of graph topology, the Boolean edge perturbation variables are relaxed to continuous variables within $[0,1]$. This allows the use of gradient-based optimization.\n        *   **Attack Generation:**\n            *   **PGD Topology Attack:** For attacking a pre-defined GNN (fixed weights), a Projected Gradient Descent (PGD) method is used to minimize an attack loss (negative cross-entropy or CW-type loss) subject to a budget on edge perturbations. A random sampling strategy is then used to convert the continuous solution back to a binary edge perturbation.\n            *   **Min-max Topology Attack:** For attacking an interactive GNN (re-trainable weights), a min-max optimization problem is formulated. This is solved using first-order alternating optimization, where the inner maximization updates GNN weights to resist the attack, and the outer minimization updates the perturbation to maximize loss.\n        *   **Robust Training (Defense):** Leveraging the first-order attack generation, the paper proposes the first optimization-based adversarial training for GNNs. This involves solving a min-max problem where GNN weights are optimized to minimize the loss under the worst-case topology perturbation (found via PGD).\n    *   **Novelty/Difference:** The core novelty lies in enabling *gradient-based, first-order optimization* for *discrete topology attacks* on GNNs through convex relaxation and subsequent binarization. This allows for more effective and principled attack generation and, consequently, a more robust adversarial training framework compared to greedy or heuristic approaches.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A general first-order attack generation framework for GNNs under two scenarios: attacking a pre-defined GNN (PGD topology attack) and attacking a re-trainable GNN (min-max topology attack).\n        *   A closed-form solution for the projection operation onto the constraint set for edge perturbations, which is crucial for the PGD algorithm.\n        *   The first optimization-based adversarial training method for GNNs to enhance robustness against topology attacks.\n    *   **Theoretical Insights:** The work builds upon spectral graph theory, first-order optimization, and robust (mini-max) optimization, providing theoretical grounding for the proposed methods. It also discusses the connection between the attack and defense min-max formulations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The methods were evaluated on semi-supervised node classification tasks using Graph Convolutional Networks (GCNs). Experiments covered both attack performance (misclassification rate) and the improved robustness of adversarially trained GNNs.\n    *   **Key Performance Metrics & Results:**\n        *   **Datasets:** Cora and Citeseer.\n        *   **Attack Performance:** The proposed gradient-based attacks (CE-PGD, CW-PGD, CE-min-max, CW-min-max) demonstrated superior performance, achieving a noticeable decrease in classification performance with only a small number of edge perturbations (e.g., 5% of total edges). They outperformed state-of-the-art baselines like DICE, Meta-Self attack, and greedy attacks.\n        *   **Robustness:** The optimization-based adversarial training significantly improved the robustness of GNNs against both gradient-based and greedy topology attacks without sacrificing classification accuracy on the original (unperturbed) graph.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary limitation is the reliance on convex relaxation for discrete variables, which requires a subsequent binarization step (random sampling). While effective, this is an approximation. The min-max problems are also noted to lack saddle point properties (not convex-concave), which can affect convergence stability in practice.\n    *   **Scope of Applicability:** The methods are primarily demonstrated for semi-supervised node classification on unweighted, undirected graphs using GCNs. While the framework is general, its direct applicability to other GNN architectures, graph types (e.g., directed, weighted), or tasks (e.g., graph classification) would require further investigation. The focus is on *topology* attacks, not node feature attacks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the understanding and mitigation of adversarial attacks on GNNs by introducing a principled, gradient-based optimization framework for discrete graph topology perturbations. It bridges the gap between continuous optimization methods and discrete graph attacks.\n    *   **Potential Impact:** The proposed attack methods provide stronger benchmarks for evaluating GNN robustness. More importantly, the optimization-based adversarial training offers a robust defense mechanism, paving the way for developing more secure and reliable GNN models in real-world applications, especially in critical domains where graph data is prevalent.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized data analysis on complex graph structures, yet their profound success is shadowed by a critical vulnerability: \"topology attacks\" that subtly perturb graph connections. A major challenge has been applying powerful gradient-based adversarial methods, typically effective in continuous domains, to the inherently discrete nature of graph topology. This paper introduces a groundbreaking optimization-based framework that bridges this fundamental gap, enabling both potent topology attacks and robust GNN defenses.\n\nWe overcome the discrete barrier through a novel convex relaxation of Boolean edge perturbation variables, allowing the first application of gradient-based optimization, including Projected Gradient Descent (PGD) and min-max formulations, to craft highly effective discrete graph attacks. Leveraging this breakthrough, we propose the first optimization-based adversarial training method for GNNs, significantly enhancing their robustness against structural perturbations. Our experiments demonstrate superior attack performance and substantially improved GNN resilience on node classification tasks, establishing a new paradigm for understanding and mitigating adversarial threats in GNNs and paving the way for secure, real-world graph applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "topology attacks",
      "adversarial robustness",
      "discrete graph structure",
      "gradient-based optimization",
      "convex relaxation",
      "Projected Gradient Descent (PGD)",
      "min-max optimization",
      "adversarial training",
      "semi-supervised node classification",
      "edge perturbations",
      "robust GNNs",
      "first-order attack generation",
      "optimization-based defense"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/341880efaef452f631a4a5cd61bef5dae47741d7.pdf",
    "citation_key": "xu2019l8n",
    "metadata": {
      "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
      "authors": [
        "Kaidi Xu",
        "Hongge Chen",
        "Sijia Liu",
        "Pin-Yu Chen",
        "Tsui-Wei Weng",
        "Mingyi Hong",
        "Xue Lin"
      ],
      "published_date": "2019",
      "abstract": "Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrifice classification accuracy on original graph.",
      "file_path": "paper_data/Graph_Neural_Networks/341880efaef452f631a4a5cd61bef5dae47741d7.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective\" by Xu et al. \\cite{xu2019l8n} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Topology Attack and Defense for Graph Neural Networks \\cite{xu2019l8n}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** This paper addresses the lack of adversarial robustness in Graph Neural Networks (GNNs), specifically focusing on \"topology attacks\" where an adversary perturbs the graph structure (adding or deleting edges) to degrade GNN performance.\n    *   **Importance & Challenge:** GNNs have achieved significant success in tasks like semi-supervised node classification on graph-structured data (e.g., social networks, molecules). However, their vulnerability to adversarial attacks, especially in security-sensitive domains, is a critical concern. The primary challenge lies in applying gradient-based adversarial attack methods, which are successful in continuous domains (like images), to the inherently *discrete* nature of graph topology (edge additions/deletions).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous works have explored adversarial attacks on GNNs, including test-time and training-time attacks, and modifications to both discrete structures (edges) and node attributes \\cite{dai2018adversarial, zugner2018adversarial, bojcheski2018adversarial, zugner2019adversarial}.\n    *   **Limitations of Previous Solutions:** Existing methods often rely on static surrogate models, perturbation theory, or greedy search heuristics. Crucially, many suggested that conventional first-order continuous optimization methods do not directly apply to topology attacks due to the discrete nature of graphs. This paper explicitly aims to \"close this gap\" by enabling gradient-based attacks on discrete graph structures.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a novel optimization-based framework for both generating topology attacks and developing robust GNNs.\n        *   **Convex Relaxation:** To overcome the discrete nature of graph topology, the Boolean edge perturbation variables are relaxed to continuous variables within $[0,1]$. This allows the use of gradient-based optimization.\n        *   **Attack Generation:**\n            *   **PGD Topology Attack:** For attacking a pre-defined GNN (fixed weights), a Projected Gradient Descent (PGD) method is used to minimize an attack loss (negative cross-entropy or CW-type loss) subject to a budget on edge perturbations. A random sampling strategy is then used to convert the continuous solution back to a binary edge perturbation.\n            *   **Min-max Topology Attack:** For attacking an interactive GNN (re-trainable weights), a min-max optimization problem is formulated. This is solved using first-order alternating optimization, where the inner maximization updates GNN weights to resist the attack, and the outer minimization updates the perturbation to maximize loss.\n        *   **Robust Training (Defense):** Leveraging the first-order attack generation, the paper proposes the first optimization-based adversarial training for GNNs. This involves solving a min-max problem where GNN weights are optimized to minimize the loss under the worst-case topology perturbation (found via PGD).\n    *   **Novelty/Difference:** The core novelty lies in enabling *gradient-based, first-order optimization* for *discrete topology attacks* on GNNs through convex relaxation and subsequent binarization. This allows for more effective and principled attack generation and, consequently, a more robust adversarial training framework compared to greedy or heuristic approaches.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A general first-order attack generation framework for GNNs under two scenarios: attacking a pre-defined GNN (PGD topology attack) and attacking a re-trainable GNN (min-max topology attack).\n        *   A closed-form solution for the projection operation onto the constraint set for edge perturbations, which is crucial for the PGD algorithm.\n        *   The first optimization-based adversarial training method for GNNs to enhance robustness against topology attacks.\n    *   **Theoretical Insights:** The work builds upon spectral graph theory, first-order optimization, and robust (mini-max) optimization, providing theoretical grounding for the proposed methods. It also discusses the connection between the attack and defense min-max formulations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The methods were evaluated on semi-supervised node classification tasks using Graph Convolutional Networks (GCNs). Experiments covered both attack performance (misclassification rate) and the improved robustness of adversarially trained GNNs.\n    *   **Key Performance Metrics & Results:**\n        *   **Datasets:** Cora and Citeseer.\n        *   **Attack Performance:** The proposed gradient-based attacks (CE-PGD, CW-PGD, CE-min-max, CW-min-max) demonstrated superior performance, achieving a noticeable decrease in classification performance with only a small number of edge perturbations (e.g., 5% of total edges). They outperformed state-of-the-art baselines like DICE, Meta-Self attack, and greedy attacks.\n        *   **Robustness:** The optimization-based adversarial training significantly improved the robustness of GNNs against both gradient-based and greedy topology attacks without sacrificing classification accuracy on the original (unperturbed) graph.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary limitation is the reliance on convex relaxation for discrete variables, which requires a subsequent binarization step (random sampling). While effective, this is an approximation. The min-max problems are also noted to lack saddle point properties (not convex-concave), which can affect convergence stability in practice.\n    *   **Scope of Applicability:** The methods are primarily demonstrated for semi-supervised node classification on unweighted, undirected graphs using GCNs. While the framework is general, its direct applicability to other GNN architectures, graph types (e.g., directed, weighted), or tasks (e.g., graph classification) would require further investigation. The focus is on *topology* attacks, not node feature attacks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the understanding and mitigation of adversarial attacks on GNNs by introducing a principled, gradient-based optimization framework for discrete graph topology perturbations. It bridges the gap between continuous optimization methods and discrete graph attacks.\n    *   **Potential Impact:** The proposed attack methods provide stronger benchmarks for evaluating GNN robustness. More importantly, the optimization-based adversarial training offers a robust defense mechanism, paving the way for developing more secure and reliable GNN models in real-world applications, especially in critical domains where graph data is prevalent.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "topology attacks",
        "adversarial robustness",
        "discrete graph structure",
        "gradient-based optimization",
        "convex relaxation",
        "Projected Gradient Descent (PGD)",
        "min-max optimization",
        "adversarial training",
        "semi-supervised node classification",
        "edge perturbations",
        "robust GNNs",
        "first-order attack generation",
        "optimization-based defense"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we first present a novel gradient-based attack method\" and \"we propose the first optimization-based adversarial training for gnns.\" these phrases directly align with the \"technical\" classification criterion: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n*   the introduction further elaborates on a technical problem (\"conventional (first-order) continuous optimization methods do not directly apply to attacks using edge manipulations... due to the discrete nature of graphs\") and proposes a solution (\"we close this gap by studying the problem of generating topology attacks via convex relaxation so that gradient-based adversarial attacks become...\"). this also fits the \"technical\" criterion: \"introduction discusses: technical problem, proposed solution\".\n*   while the abstract mentions \"results show that...\" and \"yields higher robustness...\", indicating empirical evaluation, the core contribution described is the *development* of new methods. the empirical findings serve to validate these proposed technical methods.\n\ntherefore, the primary classification is **technical**.\n\n**classification:** technical"
    },
    "file_name": "341880efaef452f631a4a5cd61bef5dae47741d7.pdf"
  },
  {
    "success": true,
    "doc_id": "07a759014165ce6a41cfdff0498a6208",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are highly dependent on local graph structures, leading to significant performance drops when the test graph structure differs from the training graph structure, a phenomenon termed \"structure shift\" \\cite{xia20247w9}.\n    *   This structure shift causes GNNs to be biased towards specific training structure patterns, compromising their generalization ability and reliability in real-world dynamic graph environments (e.g., evolving citation or social networks) \\cite{xia20247w9}.\n    *   The problem is challenging because the underlying graph generation mechanism is often unknown and complex, making it difficult to simulate diverse structural environments or learn invariant representations through traditional methods \\cite{xia20247w9}.\n\n*   **Related Work & Positioning**\n    *   Existing graph Out-Of-Distribution (OOD) methods for node classification typically either assume knowledge of the graph generation process to derive regularization terms (e.g., \\cite{xia20247w9} citing [31,9]) or require sampling unbiased test data to match training distributions (e.g., \\cite{xia20247w9} citing [40]).\n    *   Limitations of previous solutions include heavy reliance on unknown graph generation processes or the inability to apply to whole graph-level structure shifts (inductive learning scenarios) due to the need for pre-sampling test data \\cite{xia20247w9}.\n    *   This work positions itself by proposing a novel mechanism that learns invariant representations without these restrictive assumptions, directly addressing the challenge of structure shift in the embedding space \\cite{xia20247w9}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Cluster Information Transfer (CIT) mechanism to learn invariant representations for GNNs by manipulating node representations in the embedding space \\cite{xia20247w9}.\n    *   **Clustering Process**: GNNs first learn initial node representations. Spectral clustering is then applied to these representations to obtain cluster assignments, optimized using a combination of cut loss (to group strongly connected nodes) and orthogonality loss (to prevent collapse and ensure balanced cluster sizes) \\cite{xia20247w9}.\n    *   **Cluster Information Transfer (CIT)**:\n        *   Cluster information is characterized by its mean (center) and standard deviation (aggregating scope) \\cite{xia20247w9}.\n        *   A node's representation is transferred from its original cluster's statistics to a randomly selected new cluster's statistics, while preserving its \"cluster-independent information\" \\cite{xia20247w9}. The transformation is $Z'^{(l)}_i = \\sigma(H_c^j) \\frac{Z^{(l)}_i - H_c^k}{\\sigma(H_c^k)} + H_c^j$ \\cite{xia20247w9}.\n        *   **Innovation**: Gaussian perturbations are added to the cluster statistics during transfer to increase uncertainty and diversity, generating more varied \"domains\" and enhancing robustness to unknown structure shifts \\cite{xia20247w9}.\n    *   **Objective Function**: The overall objective combines the standard cross-entropy loss on the newly generated representations with the clustering loss \\cite{xia20247w9}.\n    *   **Plug-in Design**: The CIT mechanism is designed as a plug-in, implemented before the GNN's final classification layer, making it backbone-agnostic and easily integrable with most existing GNN architectures \\cite{xia20247w9}.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism**: Introduction of the Cluster Information Transfer (CIT) mechanism for learning invariant representations in GNNs to address graph structure shift \\cite{xia20247w9}.\n    *   **Invariant Representation Learning**: A method to generate diverse local environments in the embedding space by transferring cluster information while preserving cluster-independent node features, without requiring explicit graph structure modification \\cite{xia20247w9}.\n    *   **Theoretical Analysis**: Provides theoretical insights and analysis demonstrating that the CIT mechanism mitigates the impact of changing clusters during structure shift, thereby enhancing model robustness \\cite{xia20247w9}.\n    *   **Architectural Flexibility**: The proposed mechanism is a \"friendly plug-in\" that can be easily integrated into and improve the generalization ability of most current GNNs \\cite{xia20247w9}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive evaluation across three typical structure shift scenarios:\n        1.  **Perturbation on graph structures**: Cora, Citeseer, Pubmed datasets, with randomly added or deleted edges \\cite{xia20247w9}.\n        2.  **Multiplex networks**: ACM, IMDB datasets, using different relation structures \\cite{xia20247w9}.\n        3.  **Multigraph**: Twitch-Explicit dataset, training on one network and testing on others \\cite{xia20247w9}.\n    *   **GNN Backbones & Baselines**: The CIT mechanism was plugged into GCN, GAT, APPNP, and GCNII. Comparisons were made against original GNNs and GNNs augmented with existing graph OOD methods (SR-GNN, EERM) \\cite{xia20247w9}. A variant without Gaussian perturbations (CIT-GNN(w/o)) was also tested \\cite{xia20247w9}.\n    *   **Key Performance Metrics**: Node classification accuracy and Macro-f1 score \\cite{xia20247w9}.\n    *   **Comparison Results**: The proposed CIT mechanism consistently and significantly improved the generalization ability of GNNs across all tested structure shift scenarios and GNN backbones \\cite{xia20247w9}. Paired t-tests confirmed statistical significance of improvements (e.g., * for 0.05 level and ** for 0.01 level) \\cite{xia20247w9}. The inclusion of Gaussian perturbations further enhanced performance, demonstrating their effectiveness in increasing diversity and robustness \\cite{xia20247w9}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis primarily focuses on the core transfer equation (Eq. 8), while the full mechanism includes Gaussian perturbations (Eq. 9) \\cite{xia20247w9}. The initial exploration of structure shift assumes a relatively simple scenario (e.g., two community structures) \\cite{xia20247w9}.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks and is applicable to GNNs that rely on message-passing and are sensitive to local structure \\cite{xia20247w9}. Its plug-in nature suggests broad compatibility with various GNN architectures.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a novel and effective solution to the critical and challenging problem of graph structure shift in GNNs, significantly enhancing their robustness and generalization capabilities in dynamic environments \\cite{xia20247w9}.\n    *   **New Paradigm for OOD on Graphs**: Offers an innovative approach to graph Out-Of-Distribution problems by generating diverse environments in the embedding space, circumventing the need for explicit graph structure modifications or knowledge of complex graph generation processes \\cite{xia20247w9}.\n    *   **Potential Impact on Future Research**: The plug-in design encourages widespread adoption and further research into invariant representation learning on graphs. The concept of cluster information transfer in embedding space could be extended to other graph learning tasks or types of distribution shifts, fostering more robust and reliable graph AI systems \\cite{xia20247w9}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel in structured data, yet their performance plummets in dynamic real-world scenarios due to \"structure shift\"—a critical Out-Of-Distribution (OOD) challenge where test graph topologies diverge from training. Existing OOD methods often rely on restrictive assumptions, limiting their applicability. We introduce **Cluster Information Transfer (CIT)**, a novel, backbone-agnostic mechanism designed to learn invariant representations directly in the embedding space.\n\nCIT leverages **spectral clustering** to identify local structural patterns. It then strategically transfers node representations between cluster statistics, crucially incorporating **Gaussian perturbations** to synthesize diverse structural environments. This innovative approach preserves cluster-independent information, generating robust, generalized features without explicit graph structure modification. Our theoretical analysis and extensive experiments across various structure shift scenarios (e.g., perturbed graphs, multiplex networks) demonstrate that CIT consistently and significantly enhances GNN generalization and robustness for **node classification**. As a friendly plug-in, CIT offers a powerful, assumption-free paradigm for building reliable GNNs in evolving graph environments.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "structure shift",
      "invariant representations",
      "Cluster Information Transfer (CIT) mechanism",
      "embedding space manipulation",
      "Out-Of-Distribution (OOD) generalization",
      "Gaussian perturbations",
      "spectral clustering",
      "plug-in design",
      "dynamic graph environments",
      "node classification",
      "robustness",
      "theoretical analysis",
      "generalization ability"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/c9845a625e2dac5e32db172d353f81d377760a5f.pdf",
    "citation_key": "xia20247w9",
    "metadata": {
      "title": "Learning Invariant Representations of Graph Neural Networks via Cluster Generalization",
      "authors": [
        "Donglin Xia",
        "Xiao Wang",
        "Nian Liu",
        "Chuan Shi"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.",
      "file_path": "paper_data/Graph_Neural_Networks/c9845a625e2dac5e32db172d353f81d377760a5f.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are highly dependent on local graph structures, leading to significant performance drops when the test graph structure differs from the training graph structure, a phenomenon termed \"structure shift\" \\cite{xia20247w9}.\n    *   This structure shift causes GNNs to be biased towards specific training structure patterns, compromising their generalization ability and reliability in real-world dynamic graph environments (e.g., evolving citation or social networks) \\cite{xia20247w9}.\n    *   The problem is challenging because the underlying graph generation mechanism is often unknown and complex, making it difficult to simulate diverse structural environments or learn invariant representations through traditional methods \\cite{xia20247w9}.\n\n*   **Related Work & Positioning**\n    *   Existing graph Out-Of-Distribution (OOD) methods for node classification typically either assume knowledge of the graph generation process to derive regularization terms (e.g., \\cite{xia20247w9} citing [31,9]) or require sampling unbiased test data to match training distributions (e.g., \\cite{xia20247w9} citing [40]).\n    *   Limitations of previous solutions include heavy reliance on unknown graph generation processes or the inability to apply to whole graph-level structure shifts (inductive learning scenarios) due to the need for pre-sampling test data \\cite{xia20247w9}.\n    *   This work positions itself by proposing a novel mechanism that learns invariant representations without these restrictive assumptions, directly addressing the challenge of structure shift in the embedding space \\cite{xia20247w9}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Cluster Information Transfer (CIT) mechanism to learn invariant representations for GNNs by manipulating node representations in the embedding space \\cite{xia20247w9}.\n    *   **Clustering Process**: GNNs first learn initial node representations. Spectral clustering is then applied to these representations to obtain cluster assignments, optimized using a combination of cut loss (to group strongly connected nodes) and orthogonality loss (to prevent collapse and ensure balanced cluster sizes) \\cite{xia20247w9}.\n    *   **Cluster Information Transfer (CIT)**:\n        *   Cluster information is characterized by its mean (center) and standard deviation (aggregating scope) \\cite{xia20247w9}.\n        *   A node's representation is transferred from its original cluster's statistics to a randomly selected new cluster's statistics, while preserving its \"cluster-independent information\" \\cite{xia20247w9}. The transformation is $Z'^{(l)}_i = \\sigma(H_c^j) \\frac{Z^{(l)}_i - H_c^k}{\\sigma(H_c^k)} + H_c^j$ \\cite{xia20247w9}.\n        *   **Innovation**: Gaussian perturbations are added to the cluster statistics during transfer to increase uncertainty and diversity, generating more varied \"domains\" and enhancing robustness to unknown structure shifts \\cite{xia20247w9}.\n    *   **Objective Function**: The overall objective combines the standard cross-entropy loss on the newly generated representations with the clustering loss \\cite{xia20247w9}.\n    *   **Plug-in Design**: The CIT mechanism is designed as a plug-in, implemented before the GNN's final classification layer, making it backbone-agnostic and easily integrable with most existing GNN architectures \\cite{xia20247w9}.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism**: Introduction of the Cluster Information Transfer (CIT) mechanism for learning invariant representations in GNNs to address graph structure shift \\cite{xia20247w9}.\n    *   **Invariant Representation Learning**: A method to generate diverse local environments in the embedding space by transferring cluster information while preserving cluster-independent node features, without requiring explicit graph structure modification \\cite{xia20247w9}.\n    *   **Theoretical Analysis**: Provides theoretical insights and analysis demonstrating that the CIT mechanism mitigates the impact of changing clusters during structure shift, thereby enhancing model robustness \\cite{xia20247w9}.\n    *   **Architectural Flexibility**: The proposed mechanism is a \"friendly plug-in\" that can be easily integrated into and improve the generalization ability of most current GNNs \\cite{xia20247w9}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive evaluation across three typical structure shift scenarios:\n        1.  **Perturbation on graph structures**: Cora, Citeseer, Pubmed datasets, with randomly added or deleted edges \\cite{xia20247w9}.\n        2.  **Multiplex networks**: ACM, IMDB datasets, using different relation structures \\cite{xia20247w9}.\n        3.  **Multigraph**: Twitch-Explicit dataset, training on one network and testing on others \\cite{xia20247w9}.\n    *   **GNN Backbones & Baselines**: The CIT mechanism was plugged into GCN, GAT, APPNP, and GCNII. Comparisons were made against original GNNs and GNNs augmented with existing graph OOD methods (SR-GNN, EERM) \\cite{xia20247w9}. A variant without Gaussian perturbations (CIT-GNN(w/o)) was also tested \\cite{xia20247w9}.\n    *   **Key Performance Metrics**: Node classification accuracy and Macro-f1 score \\cite{xia20247w9}.\n    *   **Comparison Results**: The proposed CIT mechanism consistently and significantly improved the generalization ability of GNNs across all tested structure shift scenarios and GNN backbones \\cite{xia20247w9}. Paired t-tests confirmed statistical significance of improvements (e.g., * for 0.05 level and ** for 0.01 level) \\cite{xia20247w9}. The inclusion of Gaussian perturbations further enhanced performance, demonstrating their effectiveness in increasing diversity and robustness \\cite{xia20247w9}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis primarily focuses on the core transfer equation (Eq. 8), while the full mechanism includes Gaussian perturbations (Eq. 9) \\cite{xia20247w9}. The initial exploration of structure shift assumes a relatively simple scenario (e.g., two community structures) \\cite{xia20247w9}.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks and is applicable to GNNs that rely on message-passing and are sensitive to local structure \\cite{xia20247w9}. Its plug-in nature suggests broad compatibility with various GNN architectures.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a novel and effective solution to the critical and challenging problem of graph structure shift in GNNs, significantly enhancing their robustness and generalization capabilities in dynamic environments \\cite{xia20247w9}.\n    *   **New Paradigm for OOD on Graphs**: Offers an innovative approach to graph Out-Of-Distribution problems by generating diverse environments in the embedding space, circumventing the need for explicit graph structure modifications or knowledge of complex graph generation processes \\cite{xia20247w9}.\n    *   **Potential Impact on Future Research**: The plug-in design encourages widespread adoption and further research into invariant representation learning on graphs. The concept of cluster information transfer in embedding space could be extended to other graph learning tasks or types of distribution shifts, fostering more robust and reliable graph AI systems \\cite{xia20247w9}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "structure shift",
        "invariant representations",
        "Cluster Information Transfer (CIT) mechanism",
        "embedding space manipulation",
        "Out-Of-Distribution (OOD) generalization",
        "Gaussian perturbations",
        "spectral clustering",
        "plug-in design",
        "dynamic graph environments",
        "node classification",
        "robustness",
        "theoretical analysis",
        "generalization ability"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose the cluster information transfer (cit) mechanism\"**: this is a direct indicator of presenting a new method or system.\n2.  **\"the cit mechanism achieves this by combining different cluster information...\"**: explains the technical details of the proposed method.\n3.  **\"additionally, the proposed mechanism is a plug-in that can be easily used to improve existing gnns.\"**: highlights the practical application of the new method.\n4.  **\"we provide a theoretical analysis...\"** and **\"we comprehensively evaluate our proposed method...\"**: while these indicate theoretical and empirical aspects, they serve to support and validate the *newly proposed mechanism*. the core contribution is the mechanism itself.\n\nthe paper's primary focus is on introducing a novel mechanism (cit) to address a specific technical challenge (structure shift in gnns). the theoretical analysis and empirical evaluation are integral parts of presenting and validating this new technical contribution.\n\ntherefore, the most fitting classification is **technical**."
    },
    "file_name": "c9845a625e2dac5e32db172d353f81d377760a5f.pdf"
  },
  {
    "success": true,
    "doc_id": "f9f3ae19cc45cb04a22cf23d39822b37",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The core challenge in recommender systems (RS) is learning effective user/item representations from interactions and side information to alleviate information overload \\cite{wu2020dc8}.\n    *   **Motivation**: Graph Neural Networks (GNNs) have gained significant traction in RS because most information in RS inherently possesses a graph structure (e.g., user-item interactions as bipartite graphs, social relationships, knowledge graphs) and GNNs are highly effective at learning representations from such data \\cite{wu2020dc8}. GNNs can explicitly encode crucial collaborative signals (topological structure) and explore multi-hop relationships, which traditional methods often capture only implicitly or to a limited extent \\cite{wu2020dc8}. The rapid growth and success of GNN-based RS models in both academia and industry necessitate a comprehensive review.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: The paper acknowledges existing surveys on general deep learning in RS, specific RS biases, knowledge graph-based recommendations, and session-based recommendations \\cite{wu2020dc8}. It also notes surveys on GNN techniques that only briefly mention RS as an application \\cite{wu2020dc8}.\n    *   **Limitations of previous solutions**: Existing RS surveys either largely ignore GNNs or only cover a limited subset of GNN applications within specific sub-fields \\cite{wu2020dc8}. A recent relevant survey \\cite{40} is noted for emphasizing RS perspectives but lacking sufficient attention to GNN techniques themselves and detailed discussions on method advantages/limitations \\cite{wu2020dc8}. The most relevant formal survey \\cite{151} is a short paper with only a brief discussion of GNNs in recommendation \\cite{wu2020dc8}.\n    *   **Positioning**: This survey aims to fill these gaps by providing a *comprehensive*, *unified*, and *comprehensible* review specifically focused on GNN-based recommender systems, systematically classifying models, analyzing challenges, and discussing future directions \\cite{wu2020dc8}.\n\n*   **Technical Approach & Innovation (of the Survey)**\n    *   **Core technical method**: The paper employs a systematic literature review approach, analyzing and synthesizing over 100 representative studies on GNN-based recommender systems \\cite{wu2020dc8}. It introduces fundamental concepts of RS and GNNs as a prerequisite for understanding the reviewed models \\cite{wu2020dc8}.\n    *   **Novelty**:\n        *   Proposes a *new systematic taxonomy* for GNN-based recommendation models, categorizing them based on the type of information used and recommendation tasks (e.g., user-item collaborative filtering, sequential recommendation, social recommendation, knowledge graph-based recommendation, and other tasks) \\cite{wu2020dc8}.\n        *   Systematically analyzes the main issues and challenges of applying GNNs to different types of data within each category and illustrates how existing works address these challenges \\cite{wu2020dc8}.\n        *   Discusses the advantages and limitations of current methods in detail \\cite{wu2020dc8}.\n\n*   **Key Technical Contributions (of the Survey)**\n    *   **Novel methods/techniques**: The primary contribution is the *novel classification schema* and the *structured analytical framework* for understanding the landscape of GNN-based RS \\cite{wu2020dc8}.\n    *   **Theoretical insights/analysis**: Provides a deep dive into the motivations behind GNN adoption in RS, explaining how GNNs explicitly capture collaborative signals and leverage multi-hop relationships, which are crucial for effective representation learning \\cite{wu2020dc8}. It also offers a systematic analysis of how various GNN architectures (GCN, GraphSAGE, GAT, GGNN) are adapted for different RS tasks \\cite{wu2020dc8}.\n\n*   **Experimental Validation (Analysis of Reviewed Papers)**\n    *   **Experiments conducted**: As a survey, it does not conduct new experiments. Instead, it *summarizes* the empirical validation reported in the reviewed literature \\cite{wu2020dc8}.\n    *   **Key performance metrics and comparison results**: The survey highlights that GNN-based models have consistently demonstrated superior performance, achieving new state-of-the-art results on public benchmark datasets across various recommendation tasks (e.g., session-based, POI, group, multimedia, bundle recommendation) \\cite{wu2020dc8}. It also notes the successful deployment of GNNs in large-scale industrial recommender systems, leading to substantial improvements in user engagement \\cite{wu2020dc8}. The paper also summarizes mainstream benchmark datasets and widely-adopted evaluation metrics used in the field \\cite{wu2020dc8}.\n\n*   **Limitations & Scope (of the Survey and Reviewed Methods)**\n    *   **Technical limitations/assumptions**: The survey discusses the limitations of *current GNN-based methods* within each category, which implicitly defines the boundaries of the current state-of-the-art \\cite{wu2020dc8}. These limitations often relate to scalability, cold-start problems, handling dynamic graphs, explainability, and robustness \\cite{wu2020dc8}.\n    *   **Scope of applicability**: The survey's scope is broad within GNN-based RS, covering diverse recommendation tasks and data types. It focuses on the application of GNNs, including variants like GCN, GraphSAGE, GAT, and GGNN, to various RS challenges \\cite{wu2020dc8}.\n\n*   **Technical Significance**\n    *   **Advance the technical state-of-the-art**: This survey significantly advances the understanding of GNNs in RS by providing the first comprehensive and structured review of this rapidly evolving field \\cite{wu2020dc8}. It synthesizes a vast body of literature into a coherent framework, making complex information accessible \\cite{wu2020dc8}.\n    *   **Potential impact on future research**:\n        *   Serves as a foundational reference for researchers and practitioners, offering a general understanding of the latest developments \\cite{wu2020dc8}.\n        *   Identifies nine specific potential future research directions and open issues (e.g., scalability, dynamic graphs, explainability, robustness, cold-start, multi-task learning, fairness, privacy, and GNN architecture design for RS), thereby guiding future research efforts and fostering innovation in the field \\cite{wu2020dc8}.\n        *   The accompanying collection of open-source implementations further supports reproducibility and practical application \\cite{wu2020dc8}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are revolutionizing recommender systems (RS) by effectively leveraging the inherent graph structures in user-item interactions and social networks to combat information overload. This surge in GNN-based RS models necessitates a comprehensive and structured understanding, which existing surveys largely lack. We present the first unified and comprehensible review specifically focused on GNN-based recommender systems.\n\nOur paper introduces a novel systematic taxonomy, classifying over 100 representative models based on information type and recommendation tasks, including collaborative filtering, sequential, social, and knowledge graph-based recommendations. We systematically analyze the challenges of applying GNNs to diverse data, illustrating how various GNN architectures (GCN, GraphSAGE, GAT, GGNN) address these issues and capture crucial collaborative signals and multi-hop relationships. This survey provides a deep dive into method advantages, limitations, and summarizes state-of-the-art performance across benchmarks.\n\nCrucially, we identify nine critical future research directions, such as scalability, dynamic graphs, explainability, and cold-start problems, offering a roadmap for innovation. This work serves as an indispensable resource for researchers and practitioners navigating the rapidly evolving landscape of GNN-powered recommendation.",
    "keywords": [
      "GNN-based Recommender Systems",
      "Graph Neural Networks (GNNs)",
      "Representation Learning",
      "Collaborative Filtering",
      "Systematic Taxonomy",
      "Multi-hop Relationships",
      "Scalability",
      "Cold-start Problem",
      "Dynamic Graphs",
      "Future Research Directions",
      "User-Item Interactions",
      "State-of-the-Art Performance",
      "Knowledge Graph-based Recommendation",
      "Explainability"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf",
    "citation_key": "wu2020dc8",
    "metadata": {
      "title": "Graph Neural Networks in Recommender Systems: A Survey",
      "authors": [
        "Shiwen Wu",
        "Fei Sun",
        "Fei Sun",
        "Bin Cui"
      ],
      "published_date": "2020",
      "abstract": "With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.",
      "file_path": "paper_data/Graph_Neural_Networks/3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf",
      "venue": "ACM Computing Surveys",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The core challenge in recommender systems (RS) is learning effective user/item representations from interactions and side information to alleviate information overload \\cite{wu2020dc8}.\n    *   **Motivation**: Graph Neural Networks (GNNs) have gained significant traction in RS because most information in RS inherently possesses a graph structure (e.g., user-item interactions as bipartite graphs, social relationships, knowledge graphs) and GNNs are highly effective at learning representations from such data \\cite{wu2020dc8}. GNNs can explicitly encode crucial collaborative signals (topological structure) and explore multi-hop relationships, which traditional methods often capture only implicitly or to a limited extent \\cite{wu2020dc8}. The rapid growth and success of GNN-based RS models in both academia and industry necessitate a comprehensive review.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: The paper acknowledges existing surveys on general deep learning in RS, specific RS biases, knowledge graph-based recommendations, and session-based recommendations \\cite{wu2020dc8}. It also notes surveys on GNN techniques that only briefly mention RS as an application \\cite{wu2020dc8}.\n    *   **Limitations of previous solutions**: Existing RS surveys either largely ignore GNNs or only cover a limited subset of GNN applications within specific sub-fields \\cite{wu2020dc8}. A recent relevant survey \\cite{40} is noted for emphasizing RS perspectives but lacking sufficient attention to GNN techniques themselves and detailed discussions on method advantages/limitations \\cite{wu2020dc8}. The most relevant formal survey \\cite{151} is a short paper with only a brief discussion of GNNs in recommendation \\cite{wu2020dc8}.\n    *   **Positioning**: This survey aims to fill these gaps by providing a *comprehensive*, *unified*, and *comprehensible* review specifically focused on GNN-based recommender systems, systematically classifying models, analyzing challenges, and discussing future directions \\cite{wu2020dc8}.\n\n*   **Technical Approach & Innovation (of the Survey)**\n    *   **Core technical method**: The paper employs a systematic literature review approach, analyzing and synthesizing over 100 representative studies on GNN-based recommender systems \\cite{wu2020dc8}. It introduces fundamental concepts of RS and GNNs as a prerequisite for understanding the reviewed models \\cite{wu2020dc8}.\n    *   **Novelty**:\n        *   Proposes a *new systematic taxonomy* for GNN-based recommendation models, categorizing them based on the type of information used and recommendation tasks (e.g., user-item collaborative filtering, sequential recommendation, social recommendation, knowledge graph-based recommendation, and other tasks) \\cite{wu2020dc8}.\n        *   Systematically analyzes the main issues and challenges of applying GNNs to different types of data within each category and illustrates how existing works address these challenges \\cite{wu2020dc8}.\n        *   Discusses the advantages and limitations of current methods in detail \\cite{wu2020dc8}.\n\n*   **Key Technical Contributions (of the Survey)**\n    *   **Novel methods/techniques**: The primary contribution is the *novel classification schema* and the *structured analytical framework* for understanding the landscape of GNN-based RS \\cite{wu2020dc8}.\n    *   **Theoretical insights/analysis**: Provides a deep dive into the motivations behind GNN adoption in RS, explaining how GNNs explicitly capture collaborative signals and leverage multi-hop relationships, which are crucial for effective representation learning \\cite{wu2020dc8}. It also offers a systematic analysis of how various GNN architectures (GCN, GraphSAGE, GAT, GGNN) are adapted for different RS tasks \\cite{wu2020dc8}.\n\n*   **Experimental Validation (Analysis of Reviewed Papers)**\n    *   **Experiments conducted**: As a survey, it does not conduct new experiments. Instead, it *summarizes* the empirical validation reported in the reviewed literature \\cite{wu2020dc8}.\n    *   **Key performance metrics and comparison results**: The survey highlights that GNN-based models have consistently demonstrated superior performance, achieving new state-of-the-art results on public benchmark datasets across various recommendation tasks (e.g., session-based, POI, group, multimedia, bundle recommendation) \\cite{wu2020dc8}. It also notes the successful deployment of GNNs in large-scale industrial recommender systems, leading to substantial improvements in user engagement \\cite{wu2020dc8}. The paper also summarizes mainstream benchmark datasets and widely-adopted evaluation metrics used in the field \\cite{wu2020dc8}.\n\n*   **Limitations & Scope (of the Survey and Reviewed Methods)**\n    *   **Technical limitations/assumptions**: The survey discusses the limitations of *current GNN-based methods* within each category, which implicitly defines the boundaries of the current state-of-the-art \\cite{wu2020dc8}. These limitations often relate to scalability, cold-start problems, handling dynamic graphs, explainability, and robustness \\cite{wu2020dc8}.\n    *   **Scope of applicability**: The survey's scope is broad within GNN-based RS, covering diverse recommendation tasks and data types. It focuses on the application of GNNs, including variants like GCN, GraphSAGE, GAT, and GGNN, to various RS challenges \\cite{wu2020dc8}.\n\n*   **Technical Significance**\n    *   **Advance the technical state-of-the-art**: This survey significantly advances the understanding of GNNs in RS by providing the first comprehensive and structured review of this rapidly evolving field \\cite{wu2020dc8}. It synthesizes a vast body of literature into a coherent framework, making complex information accessible \\cite{wu2020dc8}.\n    *   **Potential impact on future research**:\n        *   Serves as a foundational reference for researchers and practitioners, offering a general understanding of the latest developments \\cite{wu2020dc8}.\n        *   Identifies nine specific potential future research directions and open issues (e.g., scalability, dynamic graphs, explainability, robustness, cold-start, multi-task learning, fairness, privacy, and GNN architecture design for RS), thereby guiding future research efforts and fostering innovation in the field \\cite{wu2020dc8}.\n        *   The accompanying collection of open-source implementations further supports reproducibility and practical application \\cite{wu2020dc8}.",
      "keywords": [
        "GNN-based Recommender Systems",
        "Graph Neural Networks (GNNs)",
        "Representation Learning",
        "Collaborative Filtering",
        "Systematic Taxonomy",
        "Multi-hop Relationships",
        "Scalability",
        "Cold-start Problem",
        "Dynamic Graphs",
        "Future Research Directions",
        "User-Item Interactions",
        "State-of-the-Art Performance",
        "Knowledge Graph-based Recommendation",
        "Explainability"
      ],
      "paper_type": "based on the provided abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"graph neural networks in recommender systems: a **survey**\" - the title explicitly states it's a survey.\n*   **venue:** \"acm computing **surveys**\" - the journal itself is dedicated to survey articles.\n*   **abstract mentions:**\n    *   \"differences between this **survey** and existing ones.\"\n    *   \"very few comprehensive **reviews** that position existing works and current progress...\"\n    *   \"to the extent of our knowledge, the most relevant **survey** published formally is a short paper...\"\n    *   \"this **survey** summarizes the literature on the advances of gnn-based recommendation and discusses open issues or future directions...\"\n    *   \"contribution of this **survey**.\"\n    *   \"we propose a systematic classification schema to organize the existing gnn-based recommendation models.\" (this aligns with \"literature organization, classification schemes\" for surveys).\n    *   \"comprehensive **review**.\"\n    *   \"we discuss the limitations of current methods and propose nine potential future directions.\" (common in surveys).\n*   **introduction discusses:**\n    *   the evolution of recommender systems and the rise of gnns, providing context for the field being surveyed.\n    *   the structure of the paper, which involves categorizing and summarizing existing works.\n\nall these points directly match the criteria for a **survey** paper."
    },
    "file_name": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf"
  },
  {
    "success": true,
    "doc_id": "fe59427e1da246ecdcdb1b95213fd203",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{bianchi20239ee}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of understanding regarding how graph pooling operators impact the expressive power of Graph Neural Networks (GNNs) \\cite{bianchi20239ee}. Specifically, there is no principled, theoretically grounded criterion to compare or design these operators.\n    *   **Importance & Challenge**: GNNs' ability to distinguish non-isomorphic graphs (their expressive power) is crucial for their performance. Hierarchical pooling is vital for building deep GNNs and learning abstract graph representations. However, current pooling evaluation relies heavily on empirical downstream task performance, which is indirect, influenced by many external factors, and doesn't directly assess expressiveness \\cite{bianchi20239ee}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has extensively characterized the expressive power of message-passing (MP) layers in GNNs, often linking it to the Weisfeiler-Lehman (WL) isomorphism test \\cite{bianchi20239ee}.\n    *   **Limitations of Previous Solutions**: These expressiveness studies are largely confined to \"flat GNNs\" (stacks of MP layers followed by a readout), overlooking the role and impact of hierarchical pooling operators \\cite{bianchi20239ee}. Existing empirical pooling evaluation methods are indirect and lack theoretical grounding, while other proposed criteria (e.g., spectral similarity) can be inconsistent \\cite{bianchi20239ee}. This work positions itself to fill this theoretical gap for hierarchical GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to derive sufficient conditions under which a graph pooling operator can fully preserve the expressive power of the preceding MP layers \\cite{bianchi20239ee}. This ensures that the combined GNN architecture remains injective, capable of distinguishing non-isomorphic graphs.\n    *   **The conditions (Theorem 1) are**:\n        1.  The preceding MP layers must produce distinct feature sums for WL-distinguishable graphs (i.e., `sum(XL_i)` != `sum(YL_i)` for non-isomorphic G1, G2) \\cite{bianchi20239ee}.\n        2.  The `SEL` (selection) function of the pooling operator must ensure that the sum of membership scores for each original node across all supernodes is a positive constant (`sum_j(s_ij) = lambda > 0`). This implies all original nodes contribute to the coarsened graph \\cite{bianchi20239ee}.\n        3.  The `RED` (reduction) function must compute pooled node features as a linear transformation of the original node features using the cluster assignment matrix (`XP = S^T XL`), effectively a convex combination \\cite{bianchi20239ee}.\n    *   **Novelty**: This work introduces the first principled, theoretically grounded criterion for evaluating and designing graph pooling operators based on their expressiveness, moving beyond indirect empirical performance metrics \\cite{bianchi20239ee}. It provides clear design principles and a diagnostic tool for existing methods.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Framework**: Formal derivation of sufficient conditions (Theorem 1) for graph pooling operators to preserve the expressive power of preceding MP layers, ensuring the GNN's ability to distinguish non-isomorphic graphs \\cite{bianchi20239ee}.\n    *   **Universal Criterion**: Introduction of a theoretically grounded and universal criterion for comparing and designing graph pooling operators, focusing on their expressiveness rather than indirect downstream task performance \\cite{bianchi20239ee}.\n    *   **Analysis of Existing Operators**: Identification of commonly used pooling operators (e.g., Top-k, ASAPool, SAGPool, PanPool) that fail to meet these expressiveness conditions, explaining their potential limitations. Conversely, dense pooling operators (e.g., DiffPool, MinCutPool, DMoN) and certain sparse ones (e.g., ECPool, k-MISPool) are shown to satisfy the conditions \\cite{bianchi20239ee}.\n    *   **Empirical Validation Setup**: Proposal of a simple yet effective experimental setup to empirically measure the expressive power of any GNN (including those with pooling layers) by directly testing its capability to perform a graph isomorphism test \\cite{bianchi20239ee}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper introduces a novel experimental setup designed to empirically verify the expressive power of GNNs equipped with pooling layers \\cite{bianchi20239ee}.\n    *   **Key Performance Metrics and Comparison Results**: The primary objective of this setup is to measure the GNN's capability to perform a graph isomorphism test \\cite{bianchi20239ee}. While the paper details the *design* of this validation method, specific experimental results, datasets, or comparative performance metrics are not provided in the given text. The focus is on establishing a direct method for expressiveness assessment.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The derived conditions are *sufficient* but not *necessary*, implying that other mechanisms for preserving expressiveness might exist outside this framework \\cite{bianchi20239ee}. The analysis primarily focuses on standard MP-GNNs, which are at most as powerful as the WL test. The `CON` (connection) function of the pooling operator, while not affecting expressiveness under these conditions, can still compromise the effectiveness of subsequent MP layers \\cite{bianchi20239ee}.\n    *   **Scope of Applicability**: The framework is applicable to hierarchical GNNs that interleave MP layers with pooling operators, providing guidance for both selecting existing pooling operators and designing new ones \\cite{bianchi20239ee}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the theoretical understanding of GNN expressiveness by extending the analysis to hierarchical architectures that incorporate pooling layers, a previously underexplored area \\cite{bianchi20239ee}. It provides a much-needed theoretical foundation for evaluating pooling operators, moving beyond purely empirical assessments.\n    *   **Potential Impact on Future Research**:\n        *   **Informed Design**: Offers a principled criterion for designing new, expressive graph pooling operators and for selecting appropriate existing ones, leading to more powerful and reliable GNN architectures \\cite{bianchi20239ee}.\n        *   **Debunking Misconceptions**: Provides a theoretical basis to understand why certain popular pooling operators might fail in specific scenarios, potentially clarifying common criticisms or misconceptions about graph pooling \\cite{bianchi20239ee}.\n        *   **Future Research Directions**: Opens avenues for exploring necessary conditions for pooling expressiveness and for developing pooling operators that explicitly satisfy these conditions while optimizing other factors like computational efficiency or specific task performance \\cite{bianchi20239ee}.",
    "intriguing_abstract": "The true expressive power of deep Graph Neural Networks (GNNs) hinges on their ability to distinguish non-isomorphic graphs, a capability often compromised by the very mechanism designed to scale them: graph pooling. Despite their critical role in hierarchical GNNs, the theoretical impact of pooling operators on GNN expressiveness remains largely unexplored, leading to empirical, indirect evaluations that obscure fundamental limitations.\n\nThis paper presents a groundbreaking theoretical framework, offering the first principled criterion to assess and design graph pooling operators based on their expressive power. We rigorously derive sufficient conditions under which a pooling operator can fully preserve the injectivity of preceding message-passing (MP) layers, ensuring the overall GNN maintains its Weisfeiler-Lehman (WL) test equivalent distinguishing capabilities. Our conditions delineate specific requirements for feature summation, node contribution, and feature reduction mechanisms. Crucially, this work identifies popular pooling operators that inherently fail these expressiveness conditions, while highlighting others that succeed. We also propose a novel empirical setup to directly measure a GNN's graph isomorphism testing capability. This research provides an indispensable diagnostic tool and a clear roadmap for designing truly expressive hierarchical GNNs, fundamentally advancing our understanding and development of robust graph learning architectures.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "graph pooling operators",
      "expressive power",
      "hierarchical GNNs",
      "Weisfeiler-Lehman (WL) test",
      "theoretical framework",
      "sufficient conditions",
      "expressiveness preservation",
      "principled criterion for pooling design",
      "graph isomorphism test",
      "empirical validation setup",
      "message-passing GNNs",
      "analysis of existing pooling operators"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf",
    "citation_key": "bianchi20239ee",
    "metadata": {
      "title": "The expressive power of pooling in Graph Neural Networks",
      "authors": [
        "F. Bianchi",
        "Veronica Lachi"
      ],
      "published_date": "2023",
      "abstract": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.",
      "file_path": "paper_data/Graph_Neural_Networks/018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{bianchi20239ee}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of understanding regarding how graph pooling operators impact the expressive power of Graph Neural Networks (GNNs) \\cite{bianchi20239ee}. Specifically, there is no principled, theoretically grounded criterion to compare or design these operators.\n    *   **Importance & Challenge**: GNNs' ability to distinguish non-isomorphic graphs (their expressive power) is crucial for their performance. Hierarchical pooling is vital for building deep GNNs and learning abstract graph representations. However, current pooling evaluation relies heavily on empirical downstream task performance, which is indirect, influenced by many external factors, and doesn't directly assess expressiveness \\cite{bianchi20239ee}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has extensively characterized the expressive power of message-passing (MP) layers in GNNs, often linking it to the Weisfeiler-Lehman (WL) isomorphism test \\cite{bianchi20239ee}.\n    *   **Limitations of Previous Solutions**: These expressiveness studies are largely confined to \"flat GNNs\" (stacks of MP layers followed by a readout), overlooking the role and impact of hierarchical pooling operators \\cite{bianchi20239ee}. Existing empirical pooling evaluation methods are indirect and lack theoretical grounding, while other proposed criteria (e.g., spectral similarity) can be inconsistent \\cite{bianchi20239ee}. This work positions itself to fill this theoretical gap for hierarchical GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to derive sufficient conditions under which a graph pooling operator can fully preserve the expressive power of the preceding MP layers \\cite{bianchi20239ee}. This ensures that the combined GNN architecture remains injective, capable of distinguishing non-isomorphic graphs.\n    *   **The conditions (Theorem 1) are**:\n        1.  The preceding MP layers must produce distinct feature sums for WL-distinguishable graphs (i.e., `sum(XL_i)` != `sum(YL_i)` for non-isomorphic G1, G2) \\cite{bianchi20239ee}.\n        2.  The `SEL` (selection) function of the pooling operator must ensure that the sum of membership scores for each original node across all supernodes is a positive constant (`sum_j(s_ij) = lambda > 0`). This implies all original nodes contribute to the coarsened graph \\cite{bianchi20239ee}.\n        3.  The `RED` (reduction) function must compute pooled node features as a linear transformation of the original node features using the cluster assignment matrix (`XP = S^T XL`), effectively a convex combination \\cite{bianchi20239ee}.\n    *   **Novelty**: This work introduces the first principled, theoretically grounded criterion for evaluating and designing graph pooling operators based on their expressiveness, moving beyond indirect empirical performance metrics \\cite{bianchi20239ee}. It provides clear design principles and a diagnostic tool for existing methods.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Framework**: Formal derivation of sufficient conditions (Theorem 1) for graph pooling operators to preserve the expressive power of preceding MP layers, ensuring the GNN's ability to distinguish non-isomorphic graphs \\cite{bianchi20239ee}.\n    *   **Universal Criterion**: Introduction of a theoretically grounded and universal criterion for comparing and designing graph pooling operators, focusing on their expressiveness rather than indirect downstream task performance \\cite{bianchi20239ee}.\n    *   **Analysis of Existing Operators**: Identification of commonly used pooling operators (e.g., Top-k, ASAPool, SAGPool, PanPool) that fail to meet these expressiveness conditions, explaining their potential limitations. Conversely, dense pooling operators (e.g., DiffPool, MinCutPool, DMoN) and certain sparse ones (e.g., ECPool, k-MISPool) are shown to satisfy the conditions \\cite{bianchi20239ee}.\n    *   **Empirical Validation Setup**: Proposal of a simple yet effective experimental setup to empirically measure the expressive power of any GNN (including those with pooling layers) by directly testing its capability to perform a graph isomorphism test \\cite{bianchi20239ee}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper introduces a novel experimental setup designed to empirically verify the expressive power of GNNs equipped with pooling layers \\cite{bianchi20239ee}.\n    *   **Key Performance Metrics and Comparison Results**: The primary objective of this setup is to measure the GNN's capability to perform a graph isomorphism test \\cite{bianchi20239ee}. While the paper details the *design* of this validation method, specific experimental results, datasets, or comparative performance metrics are not provided in the given text. The focus is on establishing a direct method for expressiveness assessment.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The derived conditions are *sufficient* but not *necessary*, implying that other mechanisms for preserving expressiveness might exist outside this framework \\cite{bianchi20239ee}. The analysis primarily focuses on standard MP-GNNs, which are at most as powerful as the WL test. The `CON` (connection) function of the pooling operator, while not affecting expressiveness under these conditions, can still compromise the effectiveness of subsequent MP layers \\cite{bianchi20239ee}.\n    *   **Scope of Applicability**: The framework is applicable to hierarchical GNNs that interleave MP layers with pooling operators, providing guidance for both selecting existing pooling operators and designing new ones \\cite{bianchi20239ee}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the theoretical understanding of GNN expressiveness by extending the analysis to hierarchical architectures that incorporate pooling layers, a previously underexplored area \\cite{bianchi20239ee}. It provides a much-needed theoretical foundation for evaluating pooling operators, moving beyond purely empirical assessments.\n    *   **Potential Impact on Future Research**:\n        *   **Informed Design**: Offers a principled criterion for designing new, expressive graph pooling operators and for selecting appropriate existing ones, leading to more powerful and reliable GNN architectures \\cite{bianchi20239ee}.\n        *   **Debunking Misconceptions**: Provides a theoretical basis to understand why certain popular pooling operators might fail in specific scenarios, potentially clarifying common criticisms or misconceptions about graph pooling \\cite{bianchi20239ee}.\n        *   **Future Research Directions**: Opens avenues for exploring necessary conditions for pooling expressiveness and for developing pooling operators that explicitly satisfy these conditions while optimizing other factors like computational efficiency or specific task performance \\cite{bianchi20239ee}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "graph pooling operators",
        "expressive power",
        "hierarchical GNNs",
        "Weisfeiler-Lehman (WL) test",
        "theoretical framework",
        "sufficient conditions",
        "expressiveness preservation",
        "principled criterion for pooling design",
        "graph isomorphism test",
        "empirical validation setup",
        "message-passing GNNs",
        "analysis of existing pooling operators"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **abstract analysis:**\n    *   \"a study on how graph pooling affects the expressiveness of a gnn is still lacking.\" - identifies a gap in theoretical understanding.\n    *   \"there is not a principled criterion to compare them.\" - identifies a lack of theoretical framework.\n    *   \"we **derive sufficient conditions** for a pooling operator to fully preserve the expressive power...\" - this is a strong indicator of mathematical analysis and formal models.\n    *   \"these conditions serve as a universal and **theoretically grounded criterion**...\" - explicitly states the theoretical nature.\n    *   \"based on our **theoretical findings**, we analyze several existing pooling operators...\" - reinforces the theoretical foundation.\n    *   \"finally, we introduce an experimental setup to verify empirically...\" - indicates an empirical component, but it's presented as a verification of the theoretical findings, not the primary contribution.\n\n2.  **introduction analysis:**\n    *   \"significant effort has been devoted to characterizing the expressive power of graph neural networks (gnns) in terms of their capabilities for testing graph isomorphism...\" - sets the context in a theoretical domain (expressive power, graph isomorphism, wl test).\n    *   the discussion of wl test and k-wl test further points to a theoretical framework.\n    *   it highlights the gap in understanding the expressive power of gnns *with pooling*, which is a theoretical problem.\n\n**conclusion:**\nthe core contribution of the paper, as stated in the abstract, is the derivation of \"sufficient conditions\" and a \"theoretically grounded criterion\" based on \"theoretical findings.\" while there is an empirical component, its purpose is to \"verify empirically\" the theoretical work. this strongly aligns with the definition of a **theoretical** paper.\n\n**classification: theoretical**"
    },
    "file_name": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf"
  },
  {
    "success": true,
    "doc_id": "843f988f1466ea05ab0c500195f606b7",
    "summary": "Here's a focused summary of the paper `\\cite{vu2020zkj}` for a literature review:\n\n### PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks \\cite{vu2020zkj}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the predictions of Graph Neural Networks (GNNs) is challenging due to their complex graph structures and non-linear integration of input features.\n    *   **Importance & Challenge**: Understanding GNN decisions is crucial for model transparency, building trust, identifying failure scenarios (safety), and detecting biases (fairness/privacy). Existing explanation methods for conventional neural networks are not directly applicable or sufficient for GNNs, and current GNN-specific explainers have significant limitations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **GNNExplainer \\cite{vu2020zkj}**: A pioneer in GNN explanation using a mutual-information approach.\n        *   **Gradient-based methods \\cite{vu2020zkj}**: Adaptations of CNN explanation techniques (e.g., Grad-CAM, Integrated Gradients) to GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   **GNNExplainer**: Quality is unclear, and it requires the explained model to evaluate predictions on fractional adjacency matrices, which is not supported by many GNN libraries (e.g., PyTorch, DGL).\n        *   **Gradient-based methods**: Require knowledge of the GNN's internal parameters and are not specifically designed for GNNs' unique graph structure.\n        *   **Common Limitation**: Both GNNExplainer and gradient-based methods fall into the class of *additive feature attribution methods*. These methods implicitly assume linear independence of explained features. However, GNNs integrate input features non-linearly, leading to high dependencies among features, which can significantly degrade the quality of explanations based on this linear independence assumption.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: PGM-Explainer is a model-agnostic explainer for GNNs that generates explanations in the form of a simpler, interpretable Bayesian Network (a type of Probabilistic Graphical Model, PGM).\n    *   **Three Major Steps**:\n        1.  **Data Generation**: Perturbs the input graph `G` (e.g., by setting node features to mean values) to generate a set of input-output pairs (sampled data `Dt`) from the GNN's target prediction function. For node classification, it focuses on `L`-hop neighbors; for graph classification, all nodes.\n        2.  **Variables Selection**: Addresses the intractability of learning PGMs from a large number of variables. It identifies a small subset of important variables `U(t)` that is *guaranteed to contain the Markov-blanket* of the target prediction `t`. This is achieved using only *pairwise dependence tests*, significantly more efficient and robust than conditional dependence tests used in conventional Markov-blanket algorithms.\n        3.  **Structure Learning**: Learns the Bayesian Network `B` from the filtered data `Dt[U(t)]` using the BIC score as an objective function. A hill-climbing algorithm is employed to find good local optimal solutions, balancing optimality with computational feasibility.\n    *   **Novelty**:\n        *   **PGM-based Explanations**: Unlike existing methods that rely on linear functions or feature attributions, PGM-Explainer uses Bayesian networks to explicitly model and demonstrate the *dependencies* of explained features in terms of *conditional probabilities*. This provides deeper, more nuanced insights (e.g., \"feature A influences prediction only under specific conditions of feature B\").\n        *   **Model-Agnosticism**: Operates as a black-box explainer, requiring only query access to the GNN's predictions, without needing internal model parameters or back-propagation.\n        *   **Efficient Variable Selection**: Introduces novel theoretical results (Theorem 1 and 2) to efficiently identify a minimal set of relevant variables (`U(t)`) by leveraging pairwise dependence tests, avoiding the computational complexity and data requirements of conditional dependence tests.\n        *   **Support for \"No-Child\" Constraint**: Allows for an optional constraint where the target variable `t` is a leaf in the Bayesian network, making conditional probability queries more efficient and the PGM more intuitive.\n\n*   **Key Technical Contributions**\n    *   **Novel Explanation Framework**: Proposes PGM-Explainer, a novel framework for GNN interpretability based on Probabilistic Graphical Models.\n    *   **Dependency Modeling**: Introduces Bayesian networks as an interpretable domain to explicitly capture and represent complex, non-linear dependencies among graph components contributing to a GNN's prediction, moving beyond additive feature attribution.\n    *   **Theoretical Guarantees for Variable Selection**: Provides theoretical proofs (Theorem 1 and Theorem 2) guaranteeing that the selected subset of variables `U(t)` contains the Markov-blanket of the target prediction, ensuring that the resulting PGM explanation retains all relevant statistical information.\n    *   **Efficient Markov-Blanket Approximation**: Develops a method for variable selection that relies solely on pairwise dependence tests, making the process computationally feasible for large graphs and less prone to errors with limited data compared to methods requiring conditional independence tests.\n    *   **Conditional Probability Insights**: Enables the generation of explanations that quantify contributions via conditional probabilities, offering a richer understanding of feature interactions.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on both synthetic datasets (Barabási-Albert graphs with motifs) and real-world datasets for both node classification (Cora, Citeseer) and graph classification (MUTAG, PROTEINS).\n    *   **Key Performance Metrics**:\n        *   **Fidelity**: How well the explanation reflects the GNN's behavior.\n        *   **Sparsity/Compactness**: The size of the explanation (number of features).\n        *   **Ground-truth agreement**: For synthetic datasets, the ability to correctly identify known explanatory graph components (e.g., all nodes in a motif).\n        *   **Human-subjective tests**: Evaluation of the intuitiveness and interpretability of explanations by human subjects.\n    *   **Comparison Results**: PGM-Explainer was compared against several existing explainers, including GNNExplainer \\cite{vu2020zkj}, PGExplainer, Grad-CAM, Integrated Gradients, and GNN-LRP.\n    *   **Key Findings**: PGM-Explainer consistently achieved better performance than existing explainers across many benchmark tasks, demonstrating superior accuracy in identifying ground-truth explanations (e.g., correctly identifying all nodes in a motif, unlike GNNExplainer which included irrelevant nodes) and providing more intuitive explanations in human-subjective tests.\n\n*   **Limitations & Scope**\n    *   **Intractability of Optimal PGM Learning**: Finding an *optimal* PGM from sampled data is intractable; PGM-Explainer uses hill-climbing to find good *local optimal* solutions.\n    *   **Assumption of Perfect Map**: The theoretical guarantees for Markov-blanket inclusion rely on the assumption that a perfect map exists for the distribution of random variables.\n    *   **Black-Box Assumption**: While a strength (model-agnostic), it means the explainer cannot leverage internal model specifics if available.\n    *   **Scope of Applicability**: Primarily demonstrated for node and graph classification tasks on various graph types.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: PGM-Explainer significantly advances the technical state-of-the-art in GNN interpretability by moving beyond the limitations of additive feature attribution methods. It provides a principled way to model and explain complex, non-linear feature dependencies inherent in GNNs.\n    *   **Richer Explanations**: Offers a new paradigm for GNN explanations by providing conditional probabilities and illustrating feature interactions, leading to deeper and more intuitive insights into GNN decision-making.\n    *   **Potential Impact**: This work opens new avenues for research in interpretable AI for graph data, particularly in developing more sophisticated PGM-based explanation models, exploring different perturbation strategies, and extending theoretical guarantees under weaker assumptions. It can increase trust and facilitate the deployment of GNNs in critical applications.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) deliver unparalleled performance across graph-structured data, yet their inherent opacity severely limits trust and deployment in critical applications. Existing GNN explainability methods, predominantly relying on additive feature attribution, fundamentally fail to capture the complex, non-linear dependencies crucial to GNN decision-making, yielding superficial and often misleading insights.\n\nWe introduce PGM-Explainer, a novel, model-agnostic framework that revolutionizes GNN interpretability by leveraging **Probabilistic Graphical Models (PGMs)**. Unlike prior work, PGM-Explainer generates explanations as interpretable **Bayesian Networks**, explicitly modeling intricate **feature interactions** and **dependencies** through **conditional probabilities**. Our approach features a theoretically-grounded, efficient variable selection mechanism that identifies the **Markov blanket** of the target prediction using only **pairwise dependence tests**, overcoming the computational hurdles of conditional independence. Extensive experiments demonstrate PGM-Explainer's superior fidelity, sparsity, and human interpretability compared to state-of-the-art explainers. By providing deep, nuanced insights into GNN decision-making, PGM-Explainer significantly advances **transparent AI** for graph data, fostering greater trust and enabling responsible GNN deployment.",
    "keywords": [
      "PGM-Explainer",
      "Graph Neural Networks (GNNs)",
      "GNN Interpretability",
      "Probabilistic Graphical Models (PGMs)",
      "Bayesian Networks",
      "Feature Dependencies",
      "Model-Agnostic Explainer",
      "Markov-blanket",
      "Pairwise Dependence Tests",
      "Conditional Probabilities",
      "Theoretical Guarantees",
      "Additive Feature Attribution",
      "Node and Graph Classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf",
    "citation_key": "vu2020zkj",
    "metadata": {
      "title": "PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks",
      "authors": [
        "Minh N. Vu",
        "M. Thai"
      ],
      "published_date": "2020",
      "abstract": "In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper `\\cite{vu2020zkj}` for a literature review:\n\n### PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks \\cite{vu2020zkj}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Explaining the predictions of Graph Neural Networks (GNNs) is challenging due to their complex graph structures and non-linear integration of input features.\n    *   **Importance & Challenge**: Understanding GNN decisions is crucial for model transparency, building trust, identifying failure scenarios (safety), and detecting biases (fairness/privacy). Existing explanation methods for conventional neural networks are not directly applicable or sufficient for GNNs, and current GNN-specific explainers have significant limitations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **GNNExplainer \\cite{vu2020zkj}**: A pioneer in GNN explanation using a mutual-information approach.\n        *   **Gradient-based methods \\cite{vu2020zkj}**: Adaptations of CNN explanation techniques (e.g., Grad-CAM, Integrated Gradients) to GNNs.\n    *   **Limitations of Previous Solutions**:\n        *   **GNNExplainer**: Quality is unclear, and it requires the explained model to evaluate predictions on fractional adjacency matrices, which is not supported by many GNN libraries (e.g., PyTorch, DGL).\n        *   **Gradient-based methods**: Require knowledge of the GNN's internal parameters and are not specifically designed for GNNs' unique graph structure.\n        *   **Common Limitation**: Both GNNExplainer and gradient-based methods fall into the class of *additive feature attribution methods*. These methods implicitly assume linear independence of explained features. However, GNNs integrate input features non-linearly, leading to high dependencies among features, which can significantly degrade the quality of explanations based on this linear independence assumption.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: PGM-Explainer is a model-agnostic explainer for GNNs that generates explanations in the form of a simpler, interpretable Bayesian Network (a type of Probabilistic Graphical Model, PGM).\n    *   **Three Major Steps**:\n        1.  **Data Generation**: Perturbs the input graph `G` (e.g., by setting node features to mean values) to generate a set of input-output pairs (sampled data `Dt`) from the GNN's target prediction function. For node classification, it focuses on `L`-hop neighbors; for graph classification, all nodes.\n        2.  **Variables Selection**: Addresses the intractability of learning PGMs from a large number of variables. It identifies a small subset of important variables `U(t)` that is *guaranteed to contain the Markov-blanket* of the target prediction `t`. This is achieved using only *pairwise dependence tests*, significantly more efficient and robust than conditional dependence tests used in conventional Markov-blanket algorithms.\n        3.  **Structure Learning**: Learns the Bayesian Network `B` from the filtered data `Dt[U(t)]` using the BIC score as an objective function. A hill-climbing algorithm is employed to find good local optimal solutions, balancing optimality with computational feasibility.\n    *   **Novelty**:\n        *   **PGM-based Explanations**: Unlike existing methods that rely on linear functions or feature attributions, PGM-Explainer uses Bayesian networks to explicitly model and demonstrate the *dependencies* of explained features in terms of *conditional probabilities*. This provides deeper, more nuanced insights (e.g., \"feature A influences prediction only under specific conditions of feature B\").\n        *   **Model-Agnosticism**: Operates as a black-box explainer, requiring only query access to the GNN's predictions, without needing internal model parameters or back-propagation.\n        *   **Efficient Variable Selection**: Introduces novel theoretical results (Theorem 1 and 2) to efficiently identify a minimal set of relevant variables (`U(t)`) by leveraging pairwise dependence tests, avoiding the computational complexity and data requirements of conditional dependence tests.\n        *   **Support for \"No-Child\" Constraint**: Allows for an optional constraint where the target variable `t` is a leaf in the Bayesian network, making conditional probability queries more efficient and the PGM more intuitive.\n\n*   **Key Technical Contributions**\n    *   **Novel Explanation Framework**: Proposes PGM-Explainer, a novel framework for GNN interpretability based on Probabilistic Graphical Models.\n    *   **Dependency Modeling**: Introduces Bayesian networks as an interpretable domain to explicitly capture and represent complex, non-linear dependencies among graph components contributing to a GNN's prediction, moving beyond additive feature attribution.\n    *   **Theoretical Guarantees for Variable Selection**: Provides theoretical proofs (Theorem 1 and Theorem 2) guaranteeing that the selected subset of variables `U(t)` contains the Markov-blanket of the target prediction, ensuring that the resulting PGM explanation retains all relevant statistical information.\n    *   **Efficient Markov-Blanket Approximation**: Develops a method for variable selection that relies solely on pairwise dependence tests, making the process computationally feasible for large graphs and less prone to errors with limited data compared to methods requiring conditional independence tests.\n    *   **Conditional Probability Insights**: Enables the generation of explanations that quantify contributions via conditional probabilities, offering a richer understanding of feature interactions.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on both synthetic datasets (Barabási-Albert graphs with motifs) and real-world datasets for both node classification (Cora, Citeseer) and graph classification (MUTAG, PROTEINS).\n    *   **Key Performance Metrics**:\n        *   **Fidelity**: How well the explanation reflects the GNN's behavior.\n        *   **Sparsity/Compactness**: The size of the explanation (number of features).\n        *   **Ground-truth agreement**: For synthetic datasets, the ability to correctly identify known explanatory graph components (e.g., all nodes in a motif).\n        *   **Human-subjective tests**: Evaluation of the intuitiveness and interpretability of explanations by human subjects.\n    *   **Comparison Results**: PGM-Explainer was compared against several existing explainers, including GNNExplainer \\cite{vu2020zkj}, PGExplainer, Grad-CAM, Integrated Gradients, and GNN-LRP.\n    *   **Key Findings**: PGM-Explainer consistently achieved better performance than existing explainers across many benchmark tasks, demonstrating superior accuracy in identifying ground-truth explanations (e.g., correctly identifying all nodes in a motif, unlike GNNExplainer which included irrelevant nodes) and providing more intuitive explanations in human-subjective tests.\n\n*   **Limitations & Scope**\n    *   **Intractability of Optimal PGM Learning**: Finding an *optimal* PGM from sampled data is intractable; PGM-Explainer uses hill-climbing to find good *local optimal* solutions.\n    *   **Assumption of Perfect Map**: The theoretical guarantees for Markov-blanket inclusion rely on the assumption that a perfect map exists for the distribution of random variables.\n    *   **Black-Box Assumption**: While a strength (model-agnostic), it means the explainer cannot leverage internal model specifics if available.\n    *   **Scope of Applicability**: Primarily demonstrated for node and graph classification tasks on various graph types.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: PGM-Explainer significantly advances the technical state-of-the-art in GNN interpretability by moving beyond the limitations of additive feature attribution methods. It provides a principled way to model and explain complex, non-linear feature dependencies inherent in GNNs.\n    *   **Richer Explanations**: Offers a new paradigm for GNN explanations by providing conditional probabilities and illustrating feature interactions, leading to deeper and more intuitive insights into GNN decision-making.\n    *   **Potential Impact**: This work opens new avenues for research in interpretable AI for graph data, particularly in developing more sophisticated PGM-based explanation models, exploring different perturbation strategies, and extending theoretical guarantees under weaker assumptions. It can increase trust and facilitate the deployment of GNNs in critical applications.",
      "keywords": [
        "PGM-Explainer",
        "Graph Neural Networks (GNNs)",
        "GNN Interpretability",
        "Probabilistic Graphical Models (PGMs)",
        "Bayesian Networks",
        "Feature Dependencies",
        "Model-Agnostic Explainer",
        "Markov-blanket",
        "Pairwise Dependence Tests",
        "Conditional Probabilities",
        "Theoretical Guarantees",
        "Additive Feature Attribution",
        "Node and Graph Classification"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose pgm-explainer\"**: this is a direct indicator of a new method or system being introduced.\n2.  the abstract describes *what* pgm-explainer does (\"identiﬁes crucial graph components and generates an explanation in form of a pgm\").\n3.  it highlights the novelty compared to existing explainers.\n4.  while it mentions \"theoretical analysis\" and \"experiments,\" these are typically components of a technical paper that proposes a new method, providing its justification and evaluation. the core contribution is the *development and presentation* of pgm-explainer.\n5.  the introduction sets up the problem (lack of gnn explainers) and the need for a solution, which the paper then provides.\n\ntherefore, the paper primarily presents a new method/system.\n\n**classification: technical**"
    },
    "file_name": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf"
  },
  {
    "success": true,
    "doc_id": "e10c6a56be95ba00a4d8f764d80bfc10",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions \\cite{gao20213kp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the need for a systematic and comprehensive review of the rapidly evolving field of Graph Neural Networks (GNNs) applied to Recommender Systems (RS) \\cite{gao20213kp}. It aims to consolidate existing research, identify key challenges, categorize methods, and outline future directions.\n    *   **Importance and Challenge:** Recommender systems are crucial for personalized information services, and GNNs have emerged as state-of-the-art approaches due to their ability to capture high-order structural information in user-item interaction data. However, effectively applying GNNs to RS presents several challenges:\n        *   **Graph Construction:** Properly transforming diverse RS data into suitable graph structures.\n        *   **Embedding Propagation/Aggregation:** Designing adaptive GNN components for specific recommendation tasks.\n        *   **Model Optimization:** Aligning GNN optimization goals, loss functions, and data sampling with RS requirements.\n        *   **Computation Efficiency:** Addressing the high computational cost of GNNs in large-scale RS deployments \\cite{gao20213kp}.\n        *   The rapid development of this area necessitates an up-to-date and structured overview.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This paper is a survey that systematically reviews and categorizes existing GNN-based recommender systems. It positions itself as a comprehensive and up-to-date resource in a field where GNNs have become the new state-of-the-art for RS \\cite{gao20213kp}.\n    *   **Limitations of Previous Solutions:** The authors highlight several limitations of prior surveys on GNN-based or graph-based recommender systems:\n        *   **Incomplete Scope:** Previous surveys often focus on specific sub-areas (e.g., collaborative filtering, HINs, traditional graph embeddings) and do not provide a holistic view \\cite{gao20213kp}.\n        *   **Limited Taxonomy:** Existing categorizations are often inconsistent or lack the breadth to cover various recommendation stages, scenarios, and objectives comprehensively \\cite{gao20213kp}.\n        *   **Outdated Coverage:** Due to the rapid advancements in the field, many recent papers in top-tier venues are not covered by older surveys \\cite{gao20213kp}.\n        *   **Ignored Aspects:** Important aspects like recommendation stages (ranking, re-ranking), specific scenarios (cross-domain, multi-behavior), and beyond-accuracy objectives (diversity, explainability, fairness, security/privacy) are often overlooked \\cite{gao20213kp}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The core \"method\" of this paper, as a survey, is a systematic and comprehensive literature review. It introduces background on both RS (stages, scenarios, objectives, applications) and GNNs (spectral vs. spatial models), discusses motivations for integrating GNNs into RS, analyzes key challenges, and then provides a detailed overview of existing GNN-based RS works \\cite{gao20213kp}.\n    *   **Novelty/Difference:** The innovation lies in its structured approach and comprehensive scope:\n        *   **Novel Taxonomy:** It proposes a unique and systematic taxonomy for categorizing GNN-based recommender systems based on four perspectives: recommendation **stage** (matching, ranking, re-ranking), **scenario** (social, sequential, session-based, bundle, cross-domain, multi-behavior), **objective** (accuracy, diversity, novelty, explainability, fairness, security/privacy), and **application** \\cite{gao20213kp}.\n        *   **Up-to-date Coverage:** It incorporates many recent papers from top-tier venues, addressing the rapid development of the field \\cite{gao20213kp}.\n        *   **Holistic View:** It provides a holistic understanding by discussing motivations, challenges, methods, and future directions in a unified framework \\cite{gao20213kp}.\n\n4.  **Key Technical Contributions**\n    *   **Systematic Taxonomy:** Introduction of a novel, multi-faceted taxonomy for classifying GNN-based recommender systems, which is more comprehensive than previous categorizations \\cite{gao20213kp}.\n    *   **Identification of Challenges:** A clear articulation of four critical technical challenges in applying GNNs to RS: graph construction, embedding propagation/aggregation, model optimization, and computation efficiency \\cite{gao20213kp}.\n    *   **Motivation Analysis:** Detailed discussion of the core motivations for using GNNs in RS, including leveraging high-order connectivity, structural properties of data, and enhanced supervision signals \\cite{gao20213kp}.\n    *   **Comprehensive Overview:** A structured review of a multitude of existing GNN-based RS works, organized according to the proposed taxonomy \\cite{gao20213kp}.\n    *   **Future Directions:** Identification of open problems and promising future research directions in the field \\cite{gao20213kp}.\n    *   **Resource Provision:** Compilation of representative papers and their code repositories, made available online, serving as a valuable resource for researchers \\cite{gao20213kp}.\n\n5.  **Experimental Validation**\n    *   As a survey paper, \\cite{gao20213kp} does not conduct its own experiments or present new empirical results. Instead, it synthesizes and discusses the experimental validations and performance metrics reported in the numerous research papers it reviews. The paper's contribution is in its analysis and organization of existing knowledge, not in new empirical findings.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper's scope is inherently limited to GNN-based recommender systems. While comprehensive within this domain, it does not delve into other types of recommendation models (e.g., traditional matrix factorization, deep learning models without GNNs) except for historical context \\cite{gao20213kp}.\n    *   **Scope of Applicability:** The survey is highly applicable to researchers, practitioners, and students working on or interested in recommender systems, graph representation learning, and deep learning. It provides a foundational understanding, a structured overview of the state-of-the-art, and a roadmap for future research in GNN-based RS \\cite{gao20213kp}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the most extensive and up-to-date consolidation of research on GNN-based recommender systems \\cite{gao20213kp}. It structures a rapidly growing and complex field, making it more accessible and understandable.\n    *   **Potential Impact on Future Research:**\n        *   It serves as a critical reference for new researchers entering the field, offering a clear overview of existing methods, challenges, and motivations \\cite{gao20213kp}.\n        *   By clearly articulating open problems and promising future directions (e.g., addressing beyond-accuracy objectives, improving computational efficiency, advanced graph construction), it guides and stimulates future research efforts \\cite{gao20213kp}.\n        *   The proposed taxonomy provides a standardized framework for classifying and comparing new GNN-based RS models, fostering more systematic development and evaluation in the community \\cite{gao20213kp}.",
    "intriguing_abstract": "The landscape of Recommender Systems (RS) is being profoundly reshaped by the advent of Graph Neural Networks (GNNs), which excel at capturing intricate high-order structural information within user-item interaction data. As this field rapidly evolves, a comprehensive and systematic overview is critically needed to navigate its complexities and unlock its full potential. This paper presents an exhaustive survey, addressing the limitations of prior reviews by offering an up-to-date and holistic perspective on GNN-based RS.\n\nWe introduce a novel, multi-faceted taxonomy that systematically categorizes existing research based on recommendation *stages*, diverse *scenarios* (e.g., social, sequential, cross-domain), various *objectives* beyond accuracy (e.g., diversity, explainability, fairness), and practical *applications*. Furthermore, we meticulously identify and analyze four pivotal technical challenges: robust graph construction, adaptive embedding propagation, effective model optimization, and scalable computational efficiency. By consolidating state-of-the-art methods and outlining promising future directions, this survey serves as an indispensable resource, guiding researchers and practitioners through the intricate world of GNN-powered recommender systems and stimulating innovation in this dynamic domain.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Recommender Systems (RS)",
      "GNN-based Recommender Systems",
      "Systematic Literature Review",
      "Novel Taxonomy",
      "Graph Construction",
      "Embedding Propagation/Aggregation",
      "Model Optimization",
      "Computational Efficiency",
      "High-order Structural Information",
      "Recommendation Stages and Scenarios",
      "Beyond-Accuracy Objectives",
      "Future Research Directions"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/071e053890765ecc2ff8ef9054e9c75ec135e167.pdf",
    "citation_key": "gao20213kp",
    "metadata": {
      "title": "A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions",
      "authors": [
        "Chen Gao",
        "Yu Zheng",
        "Nian Li",
        "Yinfeng Li",
        "Yingrong Qin",
        "J. Piao",
        "Yuhan Quan",
        "Jianxin Chang",
        "Depeng Jin",
        "Xiangnan He",
        "Yong Li"
      ],
      "published_date": "2021",
      "abstract": "Recommender system is one of the most important information services on today’s Internet. Recently, graph neural networks have become the new state-of-the-art approach to recommender systems. In this survey, we conduct a comprehensive review of the literature on graph neural network-based recommender systems. We first introduce the background and the history of the development of both recommender systems and graph neural networks. For recommender systems, in general, there are four aspects for categorizing existing works: stage, scenario, objective, and application. For graph neural networks, the existing methods consist of two categories: spectral models and spatial ones. We then discuss the motivation of applying graph neural networks into recommender systems, mainly consisting of the high-order connectivity, the structural property of data and the enhanced supervision signal. We then systematically analyze the challenges in graph construction, embedding propagation/aggregation, model optimization, and computation efficiency. Afterward and primarily, we provide a comprehensive overview of a multitude of existing works of graph neural network-based recommender systems, following the taxonomy above. Finally, we raise discussions on the open problems and promising future directions in this area. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/GNN-Recommender-Systems.",
      "file_path": "paper_data/Graph_Neural_Networks/071e053890765ecc2ff8ef9054e9c75ec135e167.pdf",
      "venue": "Trans. Recomm. Syst.",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions \\cite{gao20213kp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the need for a systematic and comprehensive review of the rapidly evolving field of Graph Neural Networks (GNNs) applied to Recommender Systems (RS) \\cite{gao20213kp}. It aims to consolidate existing research, identify key challenges, categorize methods, and outline future directions.\n    *   **Importance and Challenge:** Recommender systems are crucial for personalized information services, and GNNs have emerged as state-of-the-art approaches due to their ability to capture high-order structural information in user-item interaction data. However, effectively applying GNNs to RS presents several challenges:\n        *   **Graph Construction:** Properly transforming diverse RS data into suitable graph structures.\n        *   **Embedding Propagation/Aggregation:** Designing adaptive GNN components for specific recommendation tasks.\n        *   **Model Optimization:** Aligning GNN optimization goals, loss functions, and data sampling with RS requirements.\n        *   **Computation Efficiency:** Addressing the high computational cost of GNNs in large-scale RS deployments \\cite{gao20213kp}.\n        *   The rapid development of this area necessitates an up-to-date and structured overview.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This paper is a survey that systematically reviews and categorizes existing GNN-based recommender systems. It positions itself as a comprehensive and up-to-date resource in a field where GNNs have become the new state-of-the-art for RS \\cite{gao20213kp}.\n    *   **Limitations of Previous Solutions:** The authors highlight several limitations of prior surveys on GNN-based or graph-based recommender systems:\n        *   **Incomplete Scope:** Previous surveys often focus on specific sub-areas (e.g., collaborative filtering, HINs, traditional graph embeddings) and do not provide a holistic view \\cite{gao20213kp}.\n        *   **Limited Taxonomy:** Existing categorizations are often inconsistent or lack the breadth to cover various recommendation stages, scenarios, and objectives comprehensively \\cite{gao20213kp}.\n        *   **Outdated Coverage:** Due to the rapid advancements in the field, many recent papers in top-tier venues are not covered by older surveys \\cite{gao20213kp}.\n        *   **Ignored Aspects:** Important aspects like recommendation stages (ranking, re-ranking), specific scenarios (cross-domain, multi-behavior), and beyond-accuracy objectives (diversity, explainability, fairness, security/privacy) are often overlooked \\cite{gao20213kp}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The core \"method\" of this paper, as a survey, is a systematic and comprehensive literature review. It introduces background on both RS (stages, scenarios, objectives, applications) and GNNs (spectral vs. spatial models), discusses motivations for integrating GNNs into RS, analyzes key challenges, and then provides a detailed overview of existing GNN-based RS works \\cite{gao20213kp}.\n    *   **Novelty/Difference:** The innovation lies in its structured approach and comprehensive scope:\n        *   **Novel Taxonomy:** It proposes a unique and systematic taxonomy for categorizing GNN-based recommender systems based on four perspectives: recommendation **stage** (matching, ranking, re-ranking), **scenario** (social, sequential, session-based, bundle, cross-domain, multi-behavior), **objective** (accuracy, diversity, novelty, explainability, fairness, security/privacy), and **application** \\cite{gao20213kp}.\n        *   **Up-to-date Coverage:** It incorporates many recent papers from top-tier venues, addressing the rapid development of the field \\cite{gao20213kp}.\n        *   **Holistic View:** It provides a holistic understanding by discussing motivations, challenges, methods, and future directions in a unified framework \\cite{gao20213kp}.\n\n4.  **Key Technical Contributions**\n    *   **Systematic Taxonomy:** Introduction of a novel, multi-faceted taxonomy for classifying GNN-based recommender systems, which is more comprehensive than previous categorizations \\cite{gao20213kp}.\n    *   **Identification of Challenges:** A clear articulation of four critical technical challenges in applying GNNs to RS: graph construction, embedding propagation/aggregation, model optimization, and computation efficiency \\cite{gao20213kp}.\n    *   **Motivation Analysis:** Detailed discussion of the core motivations for using GNNs in RS, including leveraging high-order connectivity, structural properties of data, and enhanced supervision signals \\cite{gao20213kp}.\n    *   **Comprehensive Overview:** A structured review of a multitude of existing GNN-based RS works, organized according to the proposed taxonomy \\cite{gao20213kp}.\n    *   **Future Directions:** Identification of open problems and promising future research directions in the field \\cite{gao20213kp}.\n    *   **Resource Provision:** Compilation of representative papers and their code repositories, made available online, serving as a valuable resource for researchers \\cite{gao20213kp}.\n\n5.  **Experimental Validation**\n    *   As a survey paper, \\cite{gao20213kp} does not conduct its own experiments or present new empirical results. Instead, it synthesizes and discusses the experimental validations and performance metrics reported in the numerous research papers it reviews. The paper's contribution is in its analysis and organization of existing knowledge, not in new empirical findings.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper's scope is inherently limited to GNN-based recommender systems. While comprehensive within this domain, it does not delve into other types of recommendation models (e.g., traditional matrix factorization, deep learning models without GNNs) except for historical context \\cite{gao20213kp}.\n    *   **Scope of Applicability:** The survey is highly applicable to researchers, practitioners, and students working on or interested in recommender systems, graph representation learning, and deep learning. It provides a foundational understanding, a structured overview of the state-of-the-art, and a roadmap for future research in GNN-based RS \\cite{gao20213kp}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the most extensive and up-to-date consolidation of research on GNN-based recommender systems \\cite{gao20213kp}. It structures a rapidly growing and complex field, making it more accessible and understandable.\n    *   **Potential Impact on Future Research:**\n        *   It serves as a critical reference for new researchers entering the field, offering a clear overview of existing methods, challenges, and motivations \\cite{gao20213kp}.\n        *   By clearly articulating open problems and promising future directions (e.g., addressing beyond-accuracy objectives, improving computational efficiency, advanced graph construction), it guides and stimulates future research efforts \\cite{gao20213kp}.\n        *   The proposed taxonomy provides a standardized framework for classifying and comparing new GNN-based RS models, fostering more systematic development and evaluation in the community \\cite{gao20213kp}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Recommender Systems (RS)",
        "GNN-based Recommender Systems",
        "Systematic Literature Review",
        "Novel Taxonomy",
        "Graph Construction",
        "Embedding Propagation/Aggregation",
        "Model Optimization",
        "Computational Efficiency",
        "High-order Structural Information",
        "Recommendation Stages and Scenarios",
        "Beyond-Accuracy Objectives",
        "Future Research Directions"
      ],
      "paper_type": "**survey**"
    },
    "file_name": "071e053890765ecc2ff8ef9054e9c75ec135e167.pdf"
  },
  {
    "success": true,
    "doc_id": "6f2e09f99db49585906115058f82288a",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of effectively analyzing and learning from brain connectivity data, which is inherently non-Euclidean (graph-structured), for tasks like understanding brain function, disease diagnosis, and synthesizing missing data. Existing connectomic datasets often suffer from missing observations (e.g., modalities, resolutions, timepoints) due to high acquisition costs and complex preprocessing \\cite{bessadok2021bfy}.\n    *   **Importance and Challenge:** Understanding brain connectivity is crucial for mapping neuronal activity, tracking brain changes over time, and early diagnosis of neurological disorders (e.g., Alzheimer's, autism) \\cite{bessadok2021bfy}. Traditional deep learning methods struggle with the non-Euclidean nature of brain graphs, leading to a significant loss of inherent topological properties and limiting their effectiveness \\cite{bessadok2021bfy}. There is a need for methods that can generate comprehensive brain mappings from limited data and robustly learn from complex graph structures.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions Graph Neural Networks (GNNs) as a superior alternative to traditional machine learning and deep learning (e.g., CNNs) for analyzing brain graphs \\cite{bessadok2021bfy}. It reviews existing approaches such as graph generative models for synthesizing missing brain data, graph integration models for creating population templates, and brain graph embedding techniques (e.g., in hyperbolic space) for visualizing topology and detecting pathological changes \\cite{bessadok2021bfy}.\n    *   **Limitations of Previous Solutions:** Traditional deep learning methods (like CNNs) do not generalize well to non-Euclidean graph data, overlooking relationships between nodes and local connectedness patterns, which results in a loss of crucial topological information \\cite{bessadok2021bfy}. Existing review papers on GNNs or neuroscience typically do not specifically discuss GNN-based methods for solving neuroscience problems, highlighting a gap this review aims to fill \\cite{bessadok2021bfy}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a review that highlights GNNs as the core technical approach for network neuroscience. GNNs are presented as a solution that preserves graph topological properties while learning to perform tasks \\cite{bessadok2021bfy}. The review details the propagation rule for Graph Convolutional Networks (GCNs), a prominent GNN model, as `Z = φ(eD^(-1/2) * eA * eD^(-1/2) * F * Θ)`, where `eA = A + I` and `eD` is a diagonal matrix \\cite{bessadok2021bfy}. It categorizes GNNs into GCNs, Graph Attention Networks (GATs), and message-passing mechanisms \\cite{bessadok2021bfy}. It also provides an overview of how brain graphs (morphological, functional, structural) are constructed \\cite{bessadok2021bfy}.\n    *   **Novelty/Difference (of the review itself):** The paper's innovation lies in providing a comprehensive, focused review and systematic taxonomy of GNN-based methods specifically applied to network neuroscience, distinguishing between \"graph-based\" (nodes are ROIs) and \"population-based\" (nodes are subjects) models \\cite{bessadok2021bfy}. This addresses a gap in existing literature by outlining current and future applications of GNNs in connectomics \\cite{bessadok2021bfy}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (reviewed):** The paper highlights the utility of GNNs (e.g., GCNs, GATs) for handling non-Euclidean brain graph data, preserving topological properties, and enabling tasks such as:\n        *   Brain graph prediction (e.g., synthesizing missing modalities, super-resolution, temporal evolution) \\cite{bessadok2021bfy}.\n        *   Brain graph integration (e.g., generating population-level brain templates) \\cite{bessadok2021bfy}.\n        *   Disease classification based on brain graphs \\cite{bessadok2021bfy}.\n    *   **Theoretical Insights or Analysis (of the review):** The paper provides a systematic categorization of GNN-based methods in network neuroscience and a detailed explanation of different brain graph types and their construction, emphasizing why GNNs are a natural fit for this domain \\cite{bessadok2021bfy}.\n\n*   **Experimental Validation**\n    *   As a review paper, it does not present new experimental results. However, it notes that the GNN-based frameworks discussed have been evaluated using existing connectomic datasets such as the Human Connectome Project (HCP), Baby Connectome Project (BCP), and Connectome Related to Human Disease (CRHD) \\cite{bessadok2021bfy}. The abstract mentions that GNNs lead to \"enhanced performance in various network neuroscience tasks\" \\cite{bessadok2021bfy}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of GNNs in the field, as discussed):** Only a limited number of GNN architectures have been applied to brain connectomes so far, with GCN being the most frequently used \\cite{bessadok2021bfy}.\n    *   **Scope of Applicability:** The review focuses on papers published between January 2017 and December 2020, specifically excluding non-GNN based methods (e.g., CNNs, traditional ML) for brain graph analysis \\cite{bessadok2021bfy}. The authors suggest the insights could generalize to broader \"omics\" fields like genomics \\cite{bessadok2021bfy}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** The paper advances the technical state-of-the-art by establishing GNNs as the leading approach for learning deep graph structures in network neuroscience, effectively overcoming the limitations of traditional deep learning on non-Euclidean data \\cite{bessadok2021bfy}. It provides a crucial structured overview and taxonomy, making GNN applications more accessible to researchers in the field \\cite{bessadok2021bfy}.\n    *   **Potential Impact on Future Research:** It charts a path for future research, guiding the better application of GNN models for neurological disorder diagnosis and population graph integration \\cite{bessadok2021bfy}. The insights can lead to novel diagnostic tools, reduce the need for extensive clinical scans and processing by enabling prediction of missing brain graph data, and promote the adoption of GNNs in network neuroscience and related \"omics\" fields \\cite{bessadok2021bfy}.",
    "intriguing_abstract": "Unlocking the intricate secrets of the human brain demands advanced analytical tools, yet the inherently non-Euclidean nature of brain connectivity data poses a formidable challenge for traditional deep learning. Existing connectomic datasets often suffer from missing observations, hindering comprehensive understanding and precise diagnosis of neurological disorders like Alzheimer's. This paper presents a timely and systematic review, positioning Graph Neural Networks (GNNs) as a transformative paradigm for network neuroscience.\n\nWe delve into how GNNs, including Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), elegantly preserve crucial topological properties of brain graphs, a feat impossible for conventional methods. Our comprehensive taxonomy distinguishes between graph-based and population-based GNN models, highlighting their unparalleled utility in tasks such as synthesizing missing brain modalities, generating robust population templates, and achieving enhanced disease classification. By overcoming the limitations of prior approaches, GNNs are poised to revolutionize connectomics, paving the way for novel diagnostic tools, reducing data acquisition burdens, and accelerating our understanding of brain function and pathology.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "network neuroscience",
      "brain connectivity data",
      "non-Euclidean graph structures",
      "topological properties preservation",
      "brain graph analysis",
      "disease diagnosis",
      "brain graph prediction",
      "brain graph integration",
      "systematic taxonomy",
      "connectomics",
      "neurological disorders",
      "Graph Convolutional Networks (GCNs)"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/db5d583782264529456a475ce8e9a90823b3a2b5.pdf",
    "citation_key": "bessadok2021bfy",
    "metadata": {
      "title": "Graph Neural Networks in Network Neuroscience",
      "authors": [
        "Alaa Bessadok",
        "M. Mahjoub",
        "I. Rekik"
      ],
      "published_date": "2021",
      "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain –namely brain graph. Relying on its non-euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience.",
      "file_path": "paper_data/Graph_Neural_Networks/db5d583782264529456a475ce8e9a90823b3a2b5.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of effectively analyzing and learning from brain connectivity data, which is inherently non-Euclidean (graph-structured), for tasks like understanding brain function, disease diagnosis, and synthesizing missing data. Existing connectomic datasets often suffer from missing observations (e.g., modalities, resolutions, timepoints) due to high acquisition costs and complex preprocessing \\cite{bessadok2021bfy}.\n    *   **Importance and Challenge:** Understanding brain connectivity is crucial for mapping neuronal activity, tracking brain changes over time, and early diagnosis of neurological disorders (e.g., Alzheimer's, autism) \\cite{bessadok2021bfy}. Traditional deep learning methods struggle with the non-Euclidean nature of brain graphs, leading to a significant loss of inherent topological properties and limiting their effectiveness \\cite{bessadok2021bfy}. There is a need for methods that can generate comprehensive brain mappings from limited data and robustly learn from complex graph structures.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions Graph Neural Networks (GNNs) as a superior alternative to traditional machine learning and deep learning (e.g., CNNs) for analyzing brain graphs \\cite{bessadok2021bfy}. It reviews existing approaches such as graph generative models for synthesizing missing brain data, graph integration models for creating population templates, and brain graph embedding techniques (e.g., in hyperbolic space) for visualizing topology and detecting pathological changes \\cite{bessadok2021bfy}.\n    *   **Limitations of Previous Solutions:** Traditional deep learning methods (like CNNs) do not generalize well to non-Euclidean graph data, overlooking relationships between nodes and local connectedness patterns, which results in a loss of crucial topological information \\cite{bessadok2021bfy}. Existing review papers on GNNs or neuroscience typically do not specifically discuss GNN-based methods for solving neuroscience problems, highlighting a gap this review aims to fill \\cite{bessadok2021bfy}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a review that highlights GNNs as the core technical approach for network neuroscience. GNNs are presented as a solution that preserves graph topological properties while learning to perform tasks \\cite{bessadok2021bfy}. The review details the propagation rule for Graph Convolutional Networks (GCNs), a prominent GNN model, as `Z = φ(eD^(-1/2) * eA * eD^(-1/2) * F * Θ)`, where `eA = A + I` and `eD` is a diagonal matrix \\cite{bessadok2021bfy}. It categorizes GNNs into GCNs, Graph Attention Networks (GATs), and message-passing mechanisms \\cite{bessadok2021bfy}. It also provides an overview of how brain graphs (morphological, functional, structural) are constructed \\cite{bessadok2021bfy}.\n    *   **Novelty/Difference (of the review itself):** The paper's innovation lies in providing a comprehensive, focused review and systematic taxonomy of GNN-based methods specifically applied to network neuroscience, distinguishing between \"graph-based\" (nodes are ROIs) and \"population-based\" (nodes are subjects) models \\cite{bessadok2021bfy}. This addresses a gap in existing literature by outlining current and future applications of GNNs in connectomics \\cite{bessadok2021bfy}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (reviewed):** The paper highlights the utility of GNNs (e.g., GCNs, GATs) for handling non-Euclidean brain graph data, preserving topological properties, and enabling tasks such as:\n        *   Brain graph prediction (e.g., synthesizing missing modalities, super-resolution, temporal evolution) \\cite{bessadok2021bfy}.\n        *   Brain graph integration (e.g., generating population-level brain templates) \\cite{bessadok2021bfy}.\n        *   Disease classification based on brain graphs \\cite{bessadok2021bfy}.\n    *   **Theoretical Insights or Analysis (of the review):** The paper provides a systematic categorization of GNN-based methods in network neuroscience and a detailed explanation of different brain graph types and their construction, emphasizing why GNNs are a natural fit for this domain \\cite{bessadok2021bfy}.\n\n*   **Experimental Validation**\n    *   As a review paper, it does not present new experimental results. However, it notes that the GNN-based frameworks discussed have been evaluated using existing connectomic datasets such as the Human Connectome Project (HCP), Baby Connectome Project (BCP), and Connectome Related to Human Disease (CRHD) \\cite{bessadok2021bfy}. The abstract mentions that GNNs lead to \"enhanced performance in various network neuroscience tasks\" \\cite{bessadok2021bfy}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of GNNs in the field, as discussed):** Only a limited number of GNN architectures have been applied to brain connectomes so far, with GCN being the most frequently used \\cite{bessadok2021bfy}.\n    *   **Scope of Applicability:** The review focuses on papers published between January 2017 and December 2020, specifically excluding non-GNN based methods (e.g., CNNs, traditional ML) for brain graph analysis \\cite{bessadok2021bfy}. The authors suggest the insights could generalize to broader \"omics\" fields like genomics \\cite{bessadok2021bfy}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** The paper advances the technical state-of-the-art by establishing GNNs as the leading approach for learning deep graph structures in network neuroscience, effectively overcoming the limitations of traditional deep learning on non-Euclidean data \\cite{bessadok2021bfy}. It provides a crucial structured overview and taxonomy, making GNN applications more accessible to researchers in the field \\cite{bessadok2021bfy}.\n    *   **Potential Impact on Future Research:** It charts a path for future research, guiding the better application of GNN models for neurological disorder diagnosis and population graph integration \\cite{bessadok2021bfy}. The insights can lead to novel diagnostic tools, reduce the need for extensive clinical scans and processing by enabling prediction of missing brain graph data, and promote the adoption of GNNs in network neuroscience and related \"omics\" fields \\cite{bessadok2021bfy}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "network neuroscience",
        "brain connectivity data",
        "non-Euclidean graph structures",
        "topological properties preservation",
        "brain graph analysis",
        "disease diagnosis",
        "brain graph prediction",
        "brain graph integration",
        "systematic taxonomy",
        "connectomics",
        "neurological disorders",
        "Graph Convolutional Networks (GCNs)"
      ],
      "paper_type": "the paper explicitly states its nature multiple times:\n\n*   **abstract:** \"here we **review current gnn-based methods**, highlighting the ways that they have been used in several applications related to brain graphs...\"\n*   **introduction:** \"therefore, it is necessary to provide a high-quality **review that analyzes the trends and highlights the future directions** for the applications of gnns to the ﬁeld of connectomics...\"\n*   **introduction:** \"we particularly conduct in this **survey a comprehensive review** of graph learning tasks such as graph prediction, classiﬁcation and integration with application to network neuroscience where the main data structure is brain graph.\"\n*   **introduction:** \"the rest of this **review is organized as follows**. in section 2, we ﬁrst give an overview of existing gnn models. next, we propose a taxonomy of existing models and discuss the limitations of each category. finally, in section 3 we propose several future research directions and we conclude by summarizing our ﬁndings from this study.\"\n*   **introduction:** \"this **review paper is both a position paper and a primer**, promoting awareness and application of gnns in network neuroscience along with giving a systematic taxonomy of existing models.\" (while it mentions \"position paper,\" its primary and most emphasized function, as described, is a comprehensive review and taxonomy).\n\nthe content clearly aligns with the criteria for a **survey** paper: it reviews existing literature comprehensively, discusses literature organization (taxonomy), and highlights future directions based on this review.\n\n**classification:** survey"
    },
    "file_name": "db5d583782264529456a475ce8e9a90823b3a2b5.pdf"
  },
  {
    "success": true,
    "doc_id": "6146f17f5d80fbd96c09290ba72b6082",
    "summary": "Here's a focused summary of the paper \"Be Conﬁdent! Towards Trustworthy Graph Neural Networks via Conﬁdence Calibration\" \\cite{wang20214ku} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Graph Neural Networks (GNNs) achieve high accuracy, their trustworthiness, specifically the reliability of their prediction confidence, remains largely unexplored. The paper discovers that GNNs are primarily *under-confident* in their predictions, a unique characteristic contrasting with the *over-confidence* typically observed in other modern neural networks \\cite{wang20214ku}.\n    *   **Importance and Challenge**: Trustworthy models are crucial for safety-critical applications (e.g., autonomous driving, medical diagnosis) where predictions must be accompanied by reliable confidence scores. GNNs being under-confident means many accurate predictions are assigned low confidence, limiting their applicability in such scenarios. Existing calibration methods (e.g., Temperature Scaling) are not designed for graph data, ignoring topology, often employing only linear transformations, and potentially degrading classification accuracy \\cite{wang20214ku}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work extends the concept of confidence calibration, previously studied in general deep learning, specifically to GNNs in a semi-supervised setting \\cite{wang20214ku}.\n    *   **Limitations of Previous Solutions**:\n        *   Prior calibration research for deep neural networks primarily identified *over-confidence*, whereas this paper reveals *under-confidence* as a distinct issue for GNNs \\cite{wang20214ku}.\n        *   Traditional calibration methods (e.g., Temperature Scaling, OP-families) are topology-agnostic, failing to account for the relational structure of graph data, which can lead to miscalibration (e.g., assigning the same confidence to nodes with identical logits but different neighborhood contexts) \\cite{wang20214ku}.\n        *   Many existing methods explore only linear calibration functions, which may be insufficient for complex GNN landscapes, and some non-linear methods can degrade the original model's classification accuracy \\cite{wang20214ku}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel trustworthy GNN model called **CaGCN (Calibration GCN)**, which employs a topology-aware, non-linear, and accuracy-preserving post-hoc calibration function \\cite{wang20214ku}.\n        *   **Homophily of Confidence**: The authors first verify that confidence distribution in a graph exhibits a homophily property, meaning neighboring nodes should have similar confidence if the model is well-calibrated \\cite{wang20214ku}.\n        *   **CaGCN as Calibration Function**: Inspired by this, CaGCN uses another GCN to learn a unique, node-specific temperature ($t_i$) for each node $i$. This GCN propagates the original GNN's logits, allowing the calibration function to be topology-aware and non-linear, smoothing confidence among neighbors \\cite{wang20214ku}.\n        *   **Accuracy-Preserving Property**: The calibration function is designed as $h(v_i, t_i) = [v_{i,1}/t_i, \\dots, v_{i,K}/t_i]$, which ensures that the order of class probabilities for a node remains unchanged, thereby preserving the original GNN's classification accuracy \\cite{wang20214ku}.\n        *   **Optimization Objective**: CaGCN is optimized using a Negative Log-Likelihood (NLL) loss combined with a regularization term ($L_{cal}$) that encourages increasing confidence for correct predictions and decreasing it for incorrect ones \\cite{wang20214ku}.\n        *   **Self-training Integration (CaGCN-st)**: The calibrated confidence from CaGCN is then applied to a self-training framework (CaGCN-st) to generate more trustworthy pseudo-labels, which further improves the GNN's performance \\cite{wang20214ku}.\n    *   **Novelty/Differentiation**:\n        *   First to identify and characterize the *under-confidence* phenomenon as a unique challenge for GNNs \\cite{wang20214ku}.\n        *   Introduces the concept of *topology-aware* confidence calibration for GNNs by leveraging a GCN itself as the calibration function \\cite{wang20214ku}.\n        *   Achieves a calibration function that is simultaneously *topology-aware*, *non-linear*, and *accuracy-preserving*, addressing key limitations of prior methods \\cite{wang20214ku}.\n        *   Demonstrates a practical application of calibrated confidence to enhance self-training in GNNs \\cite{wang20214ku}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Discovery and empirical validation of the *under-confidence* characteristic in existing GNNs (GCN, GAT) \\cite{wang20214ku}.\n        *   **CaGCN**: A novel post-hoc calibration method for GNNs that learns node-specific temperatures via a GCN, making the calibration topology-aware, non-linear, and accuracy-preserving \\cite{wang20214ku}.\n        *   **CaGCN-st**: A calibrated self-training framework that utilizes the more reliable confidence scores from CaGCN to generate higher-quality pseudo-labels, leading to improved semi-supervised learning performance \\cite{wang20214ku}.\n    *   **Theoretical Insights or Analysis**:\n        *   Formal definition of perfect calibration for GNNs \\cite{wang20214ku}.\n        *   Proposition 1: Proves the conditions for an accuracy-preserving calibration function \\cite{wang20214ku}.\n        *   Proposition 2: Demonstrates that the proposed CaGCN can achieve any confidence value within the valid range (1/K, 1), implying its capacity for perfect calibration \\cite{wang20214ku}.\n        *   Empirical verification of the *homophily property* of confidence in graph structures \\cite{wang20214ku}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Extensive analysis of GNN (GCN, GAT) calibration on node classification across four benchmark datasets (Cora, Citeseer, Pubmed, CoraFull) with varying label rates \\cite{wang20214ku}.\n        *   Comparison of CaGCN against uncalibrated GNNs and Temperature Scaling (TS) in terms of calibration error and accuracy \\cite{wang20214ku}.\n        *   Evaluation of CaGCN-st's performance in self-training scenarios against baseline self-training methods \\cite{wang20214ku}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Calibration**: Reliability Diagrams, Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Total Variation of Confidence. Results consistently show GNNs are under-confident, and CaGCN significantly reduces ECE and MCE, outperforming TS \\cite{wang20214ku}.\n        *   **Accuracy**: Node classification accuracy. CaGCN not only improves calibration but also preserves or slightly enhances the original GNN's accuracy. CaGCN-st demonstrates further improvements in accuracy in self-training settings, especially at higher label rates \\cite{wang20214ku}.\n        *   **Homophily Verification**: Experiments show that the total variation of confidence between neighboring nodes decreases after calibration, supporting the homophily assumption \\cite{wang20214ku}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The method is primarily evaluated for semi-supervised node classification tasks on transductive settings.\n        *   The assumption in Proposition 2 that logits do not approach infinity is generally met in practice but is a theoretical boundary condition \\cite{wang20214ku}.\n        *   The regularization term's direct impact on calibration versus accuracy refinement could be further disentangled.\n    *   **Scope of Applicability**: The proposed CaGCN and CaGCN-st are designed for GNNs, particularly GCN-like architectures. While the core idea of topology-aware calibration is general, its direct application to other graph tasks (e.g., graph classification, link prediction) or more complex GNN architectures (e.g., heterogeneous GNNs) would require further adaptation and validation \\cite{wang20214ku}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**:\n        *   Fundamentally shifts the understanding of GNN confidence by identifying and characterizing the *under-confidence* problem, distinguishing GNNs from other deep learning models \\cite{wang20214ku}.\n        *   Introduces the first comprehensive solution for GNN confidence calibration that is *topology-aware*, *non-linear*, and *accuracy-preserving*, setting a new standard for trustworthy GNNs \\cite{wang20214ku}.\n        *   Demonstrates a practical and effective way to leverage calibrated confidence to improve downstream tasks like semi-supervised learning via self-training \\cite{wang20214ku}.\n    *   **Potential Impact on Future Research**:\n        *   Paves the way for more reliable and trustworthy GNN deployments in critical applications by providing accurate confidence estimates \\cite{wang20214ku}.\n        *   Opens new research avenues in GNN robustness, uncertainty quantification, and explainability, encouraging exploration of calibration for various GNN architectures and graph learning tasks \\cite{wang20214ku}.\n        *   Could lead to the development of more sophisticated self-training and semi-supervised learning techniques for graph data by ensuring higher quality pseudo-labels \\cite{wang20214ku}.",
    "intriguing_abstract": "While Graph Neural Networks (GNNs) excel in graph-structured data, their trustworthiness, particularly the reliability of prediction confidence, remains a critical, underexplored frontier. This paper unveils a paradoxical characteristic: GNNs are predominantly *under-confident* in their predictions, a stark contrast to the well-documented over-confidence in other deep learning models. Traditional confidence calibration methods, designed for Euclidean data, prove inadequate for graphs, failing to account for topology and often degrading classification accuracy.\n\nTo address this, we introduce **CaGCN**, a pioneering post-hoc calibration framework. CaGCN leverages a novel *topology-aware*, *non-linear*, and *accuracy-preserving* function, learning node-specific temperatures via an auxiliary GCN. This design inherently respects the graph structure, smoothing confidence based on neighborhood context and ensuring the original model's accuracy is maintained. Furthermore, we demonstrate how this calibrated confidence can be harnessed in **CaGCN-st**, a self-training framework that generates significantly more trustworthy pseudo-labels, leading to superior semi-supervised learning performance. Extensive experiments confirm CaGCN's unprecedented ability to drastically reduce Expected Calibration Error (ECE) while preserving or enhancing accuracy across benchmark datasets. This work paves the way for truly trustworthy GNNs, crucial for safety-critical applications and advancing robust graph AI.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Confidence Calibration",
      "Under-confidence",
      "CaGCN",
      "Topology-aware Calibration",
      "Accuracy-preserving Calibration",
      "Node-specific Temperature",
      "Homophily of Confidence",
      "Self-training Framework",
      "Trustworthy Pseudo-labels",
      "Semi-supervised Node Classification",
      "Expected Calibration Error (ECE)",
      "Safety-critical Applications"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf",
    "citation_key": "wang20214ku",
    "metadata": {
      "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration",
      "authors": [
        "Xiao Wang",
        "Hongrui Liu",
        "Chuan Shi",
        "Cheng Yang"
      ],
      "published_date": "2021",
      "abstract": "Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy, whether the results are trustworthy is still unexplored. Previous studies suggest that many modern neural networks are over-confident on the predictions, however, surprisingly, we discover that GNNs are primarily in the opposite direction, i.e., GNNs are under-confident. Therefore, the confidence calibration for GNNs is highly desired. In this paper, we propose a novel trustworthy GNN model by designing a topology-aware post-hoc calibration function. Specifically, we first verify that the confidence distribution in a graph has homophily property, and this finding inspires us to design a calibration GNN model (CaGCN) to learn the calibration function. CaGCN is able to obtain a unique transformation from logits of GNNs to the calibrated confidence for each node, meanwhile, such transformation is able to preserve the order between classes, satisfying the accuracy-preserving property. Moreover, we apply the calibration GNN to self-training framework, showing that more trustworthy pseudo labels can be obtained with the calibrated confidence and further improve the performance. Extensive experiments demonstrate the effectiveness of our proposed model in terms of both calibration and accuracy.",
      "file_path": "paper_data/Graph_Neural_Networks/3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Be Conﬁdent! Towards Trustworthy Graph Neural Networks via Conﬁdence Calibration\" \\cite{wang20214ku} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Graph Neural Networks (GNNs) achieve high accuracy, their trustworthiness, specifically the reliability of their prediction confidence, remains largely unexplored. The paper discovers that GNNs are primarily *under-confident* in their predictions, a unique characteristic contrasting with the *over-confidence* typically observed in other modern neural networks \\cite{wang20214ku}.\n    *   **Importance and Challenge**: Trustworthy models are crucial for safety-critical applications (e.g., autonomous driving, medical diagnosis) where predictions must be accompanied by reliable confidence scores. GNNs being under-confident means many accurate predictions are assigned low confidence, limiting their applicability in such scenarios. Existing calibration methods (e.g., Temperature Scaling) are not designed for graph data, ignoring topology, often employing only linear transformations, and potentially degrading classification accuracy \\cite{wang20214ku}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work extends the concept of confidence calibration, previously studied in general deep learning, specifically to GNNs in a semi-supervised setting \\cite{wang20214ku}.\n    *   **Limitations of Previous Solutions**:\n        *   Prior calibration research for deep neural networks primarily identified *over-confidence*, whereas this paper reveals *under-confidence* as a distinct issue for GNNs \\cite{wang20214ku}.\n        *   Traditional calibration methods (e.g., Temperature Scaling, OP-families) are topology-agnostic, failing to account for the relational structure of graph data, which can lead to miscalibration (e.g., assigning the same confidence to nodes with identical logits but different neighborhood contexts) \\cite{wang20214ku}.\n        *   Many existing methods explore only linear calibration functions, which may be insufficient for complex GNN landscapes, and some non-linear methods can degrade the original model's classification accuracy \\cite{wang20214ku}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel trustworthy GNN model called **CaGCN (Calibration GCN)**, which employs a topology-aware, non-linear, and accuracy-preserving post-hoc calibration function \\cite{wang20214ku}.\n        *   **Homophily of Confidence**: The authors first verify that confidence distribution in a graph exhibits a homophily property, meaning neighboring nodes should have similar confidence if the model is well-calibrated \\cite{wang20214ku}.\n        *   **CaGCN as Calibration Function**: Inspired by this, CaGCN uses another GCN to learn a unique, node-specific temperature ($t_i$) for each node $i$. This GCN propagates the original GNN's logits, allowing the calibration function to be topology-aware and non-linear, smoothing confidence among neighbors \\cite{wang20214ku}.\n        *   **Accuracy-Preserving Property**: The calibration function is designed as $h(v_i, t_i) = [v_{i,1}/t_i, \\dots, v_{i,K}/t_i]$, which ensures that the order of class probabilities for a node remains unchanged, thereby preserving the original GNN's classification accuracy \\cite{wang20214ku}.\n        *   **Optimization Objective**: CaGCN is optimized using a Negative Log-Likelihood (NLL) loss combined with a regularization term ($L_{cal}$) that encourages increasing confidence for correct predictions and decreasing it for incorrect ones \\cite{wang20214ku}.\n        *   **Self-training Integration (CaGCN-st)**: The calibrated confidence from CaGCN is then applied to a self-training framework (CaGCN-st) to generate more trustworthy pseudo-labels, which further improves the GNN's performance \\cite{wang20214ku}.\n    *   **Novelty/Differentiation**:\n        *   First to identify and characterize the *under-confidence* phenomenon as a unique challenge for GNNs \\cite{wang20214ku}.\n        *   Introduces the concept of *topology-aware* confidence calibration for GNNs by leveraging a GCN itself as the calibration function \\cite{wang20214ku}.\n        *   Achieves a calibration function that is simultaneously *topology-aware*, *non-linear*, and *accuracy-preserving*, addressing key limitations of prior methods \\cite{wang20214ku}.\n        *   Demonstrates a practical application of calibrated confidence to enhance self-training in GNNs \\cite{wang20214ku}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Discovery and empirical validation of the *under-confidence* characteristic in existing GNNs (GCN, GAT) \\cite{wang20214ku}.\n        *   **CaGCN**: A novel post-hoc calibration method for GNNs that learns node-specific temperatures via a GCN, making the calibration topology-aware, non-linear, and accuracy-preserving \\cite{wang20214ku}.\n        *   **CaGCN-st**: A calibrated self-training framework that utilizes the more reliable confidence scores from CaGCN to generate higher-quality pseudo-labels, leading to improved semi-supervised learning performance \\cite{wang20214ku}.\n    *   **Theoretical Insights or Analysis**:\n        *   Formal definition of perfect calibration for GNNs \\cite{wang20214ku}.\n        *   Proposition 1: Proves the conditions for an accuracy-preserving calibration function \\cite{wang20214ku}.\n        *   Proposition 2: Demonstrates that the proposed CaGCN can achieve any confidence value within the valid range (1/K, 1), implying its capacity for perfect calibration \\cite{wang20214ku}.\n        *   Empirical verification of the *homophily property* of confidence in graph structures \\cite{wang20214ku}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Extensive analysis of GNN (GCN, GAT) calibration on node classification across four benchmark datasets (Cora, Citeseer, Pubmed, CoraFull) with varying label rates \\cite{wang20214ku}.\n        *   Comparison of CaGCN against uncalibrated GNNs and Temperature Scaling (TS) in terms of calibration error and accuracy \\cite{wang20214ku}.\n        *   Evaluation of CaGCN-st's performance in self-training scenarios against baseline self-training methods \\cite{wang20214ku}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Calibration**: Reliability Diagrams, Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Total Variation of Confidence. Results consistently show GNNs are under-confident, and CaGCN significantly reduces ECE and MCE, outperforming TS \\cite{wang20214ku}.\n        *   **Accuracy**: Node classification accuracy. CaGCN not only improves calibration but also preserves or slightly enhances the original GNN's accuracy. CaGCN-st demonstrates further improvements in accuracy in self-training settings, especially at higher label rates \\cite{wang20214ku}.\n        *   **Homophily Verification**: Experiments show that the total variation of confidence between neighboring nodes decreases after calibration, supporting the homophily assumption \\cite{wang20214ku}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The method is primarily evaluated for semi-supervised node classification tasks on transductive settings.\n        *   The assumption in Proposition 2 that logits do not approach infinity is generally met in practice but is a theoretical boundary condition \\cite{wang20214ku}.\n        *   The regularization term's direct impact on calibration versus accuracy refinement could be further disentangled.\n    *   **Scope of Applicability**: The proposed CaGCN and CaGCN-st are designed for GNNs, particularly GCN-like architectures. While the core idea of topology-aware calibration is general, its direct application to other graph tasks (e.g., graph classification, link prediction) or more complex GNN architectures (e.g., heterogeneous GNNs) would require further adaptation and validation \\cite{wang20214ku}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**:\n        *   Fundamentally shifts the understanding of GNN confidence by identifying and characterizing the *under-confidence* problem, distinguishing GNNs from other deep learning models \\cite{wang20214ku}.\n        *   Introduces the first comprehensive solution for GNN confidence calibration that is *topology-aware*, *non-linear*, and *accuracy-preserving*, setting a new standard for trustworthy GNNs \\cite{wang20214ku}.\n        *   Demonstrates a practical and effective way to leverage calibrated confidence to improve downstream tasks like semi-supervised learning via self-training \\cite{wang20214ku}.\n    *   **Potential Impact on Future Research**:\n        *   Paves the way for more reliable and trustworthy GNN deployments in critical applications by providing accurate confidence estimates \\cite{wang20214ku}.\n        *   Opens new research avenues in GNN robustness, uncertainty quantification, and explainability, encouraging exploration of calibration for various GNN architectures and graph learning tasks \\cite{wang20214ku}.\n        *   Could lead to the development of more sophisticated self-training and semi-supervised learning techniques for graph data by ensuring higher quality pseudo-labels \\cite{wang20214ku}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Confidence Calibration",
        "Under-confidence",
        "CaGCN",
        "Topology-aware Calibration",
        "Accuracy-preserving Calibration",
        "Node-specific Temperature",
        "Homophily of Confidence",
        "Self-training Framework",
        "Trustworthy Pseudo-labels",
        "Semi-supervised Node Classification",
        "Expected Calibration Error (ECE)",
        "Safety-critical Applications"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose a novel trustworthy gnn model by designing a topology-aware post-hoc calibration function**.\" and \"this ﬁnding inspires us to **design a calibration gnn model (cagcn)** to learn the calibration function.\"\n*   the introduction sets up a problem (lack of trustworthiness in gnns) and the need for a solution (confidence calibration).\n*   the paper then describes the proposed model (cagcn) and its capabilities.\n*   the abstract concludes with \"extensive **experiments demonstrate the effectiveness of our proposed model**,\" indicating that empirical evaluation is used to validate the *newly proposed method*.\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it includes empirical validation, the primary contribution is the development and presentation of a novel model.\n\n**classification: technical**"
    },
    "file_name": "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf"
  },
  {
    "success": true,
    "doc_id": "d6ea21be119ac6ebd60aea6b35b296b9",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{geisler2024wli}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Spatial Message Passing Graph Neural Networks (MPGNNs) suffer from a limited \"receptive field\" (typically ℓ-hop neighborhood) and \"over-squashing,\" which severely restricts information exchange between distant nodes. This limits their expressivity and ability to model long-range interactions.\n    *   **Importance and Challenge**: Modeling long-range interactions is crucial for many graph-based tasks, as evidenced by the success of global models like transformers. Over-squashing makes MPGNNs ineffective for problems requiring global information, posing a significant challenge to their applicability in complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of MPGNNs, which have achieved breakthroughs in various domains. It draws inspiration from similar synergistic compositions in molecular point clouds (Kosmala et al., 2023) and sequence models like Mamba (Gu & Dao, 2023) or Hyena (Poli et al., 2023), which offer transformer-like properties with superior scalability.\n    *   **Limitations of Previous Solutions**: MPGNNs are inherently limited by their local message-passing scheme and the over-squashing phenomenon. The design space for spectral GNNs, in contrast to spatial MPGNNs, has been largely unexplored, leaving potential for novel architectural advancements.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Spatio-Spectral Graph Neural Networks (S2GNNs) \\cite{geisler2024wli}, a new modeling paradigm that synergistically combines spatially and spectrally parametrized graph filters.\n        *   S2GNNs utilize a partial eigendecomposition to enable spectral filters that are *spectrally bounded* (operating on a truncated frequency spectrum) but *spatially unbounded*, allowing for global information propagation.\n        *   The architecture can combine spatial and spectral filters additively or sequentially.\n        *   The spectral filter is defined as `Spectral(l)(H(l−1);V,λ) =V [ˆg(l)ϑ(λ)⊙ [V⊤f(l)θ(H(l−1))]]` (Eq. 3), ensuring permutation equivariance.\n    *   **Novelty/Difference**:\n        *   **Synergistic Combination**: The core innovation is the principled combination of local spatial message passing with global spectral filtering, addressing the limitations of each individually.\n        *   **Spectral Domain Neural Network**: The paper proposes the first neural network designed to operate directly in the spectral domain, allowing for data-dependent filtering and channel mixing with negligible computational cost for truncated spectra \\cite{geisler2024wli}.\n        *   **Directed Graph Filters**: S2GNNs generalize spectrally parametrized filters to directed graphs, expanding their applicability.\n        *   **Expressive Positional Encodings**: They introduce stable positional encodings (PEs) derived almost \"for free\" from the partial eigendecomposition, making S2GNNs strictly more expressive than the 1-Weisfeiler-Lehman (WL) test \\cite{geisler2024wli}.\n        *   **Over-squashing Mitigation**: The global nature of spectral filters inherently vanquishes over-squashing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The S2GNN architecture itself, which integrates spatial and spectral filtering for enhanced graph learning \\cite{geisler2024wli}.\n        *   A method for parametrizing spectral filters using Gaussian smearing and linear transformations, designed for stability and expressivity.\n        *   The concept and implementation of a neural network operating directly within the spectral domain.\n        *   Adaptation of spectral filters for directed graphs.\n        *   Novel, cost-effective positional encodings derived from the graph's eigendecomposition.\n    *   **Theoretical Insights or Analysis**:\n        *   **Over-squashing Vanquished**: Theorem 2 formally proves that S2GNNs overcome over-squashing by demonstrating a uniformly lower-bounded Jacobian sensitivity, ensuring effective long-range information exchange.\n        *   **Superior Approximation Bounds**: Theorems 3 and 4 establish strictly tighter approximation-theoretic error bounds for S2GNNs compared to MPGNNs. This is particularly significant for approximating discontinuous or unsmooth ground truth filters, where MPGNNs converge exceedingly slowly.\n        *   **Locality and Spectral Smoothness**: The analysis connects the locality of a filter to the smoothness of its Fourier transform, providing a complementary perspective on MPGNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Empirical verification of MPGNN shortcomings (e.g., over-squashing on \"Clique Path\" graphs) and how S2GNNs overcome them.\n        *   Approximation quality comparison of S2GNNs against purely spatial and spectral filters for a discontinuous target filter.\n        *   Performance evaluation on the challenging peptides-func long-range benchmark tasks (Dwivedi et al., 2022).\n        *   Scalability tests on large graphs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   S2GNNs achieved a new state-of-the-art on the peptides-func benchmark, outperforming MPGNNs and graph transformers with approximately 40% fewer parameters \\cite{geisler2024wli}.\n        *   They demonstrated competitive performance with state-of-the-art sequence models.\n        *   Empirical results on \"Clique Path\" graphs confirmed that spectral filters (and thus S2GNNs) do not exhibit over-squashing (Fig. 5).\n        *   S2GNNs scaled efficiently to millions of nodes on a 40 GB GPU with vanilla full-graph training, exhibiting runtime and space complexity equivalent to MPGNNs \\cite{geisler2024wli}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The efficiency of spectral filters relies on truncating the frequency spectrum (parameter `k` or `λcut`).\n        *   Theoretical guarantees for approximation assume the spectral filter is bandlimited and a universal approximator on a specific interval.\n        *   The default choice for the spectral filter's frequency band is low frequencies, though the method is adaptable.\n    *   **Scope of Applicability**: S2GNNs are designed as general-purpose GNNs, particularly well-suited for tasks requiring effective long-range interactions and where over-squashing is a concern. They are applicable to both undirected and directed graphs.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: S2GNNs introduce a novel and theoretically grounded GNN paradigm that fundamentally addresses the long-standing issues of over-squashing and limited receptive fields in MPGNNs \\cite{geisler2024wli}. By synergistically combining spatial and spectral approaches, they achieve superior expressivity and approximation capabilities, validated by state-of-the-art empirical performance on challenging long-range tasks.\n    *   **Potential Impact on Future Research**: This work opens up a significant new design space for GNN architectures, encouraging further exploration into hybrid spatial-spectral models. The proposed spectral domain neural network, directed graph filters, and \"free\" positional encodings offer concrete avenues for future research, potentially leading to more powerful, efficient, and expressive GNNs for a wider range of applications.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel in many domains, yet their prevalent spatial message passing paradigm inherently suffers from limited receptive fields and \"over-squashing,\" severely impeding long-range information exchange. We introduce Spatio-Spectral Graph Neural Networks (S2GNNs), a novel architecture that fundamentally overcomes these limitations by synergistically combining local spatial message passing with global spectral filters. S2GNNs leverage partial eigendecomposition to enable spectrally bounded yet spatially unbounded filters, allowing for seamless long-range interactions. This work presents the first neural network designed to operate directly in the spectral domain, offering data-dependent filtering and channel mixing. Theoretically, we prove S2GNNs formally vanquish over-squashing and achieve superior approximation bounds, making them strictly more expressive than the 1-Weisfeiler-Lehman test. Empirically, S2GNNs achieve state-of-the-art performance on challenging long-range graph tasks like peptides-func, outperforming existing GNNs and graph transformers with significantly fewer parameters, while scaling efficiently to millions of nodes. S2GNNs establish a new paradigm for graph learning, unlocking unprecedented capabilities for complex graph-based problems.",
    "keywords": [
      "Spatio-Spectral Graph Neural Networks (S2GNNs)",
      "Over-squashing mitigation",
      "Long-range interactions",
      "Spectral domain neural network",
      "Partial eigendecomposition",
      "Directed graph filters",
      "Expressive positional encodings",
      "Superior approximation bounds",
      "Peptides-func benchmark",
      "Scalable graph learning",
      "Hybrid spatial-spectral GNNs",
      "Message Passing Graph Neural Networks (MPGNNs)"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf",
    "citation_key": "geisler2024wli",
    "metadata": {
      "title": "Spatio-Spectral Graph Neural Networks",
      "authors": [
        "Simon Geisler",
        "Arthur Kosmala",
        "Daniel Herbst",
        "Stephan Gunnemann"
      ],
      "published_date": "2024",
      "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
      "file_path": "paper_data/Graph_Neural_Networks/edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{geisler2024wli}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Spatial Message Passing Graph Neural Networks (MPGNNs) suffer from a limited \"receptive field\" (typically ℓ-hop neighborhood) and \"over-squashing,\" which severely restricts information exchange between distant nodes. This limits their expressivity and ability to model long-range interactions.\n    *   **Importance and Challenge**: Modeling long-range interactions is crucial for many graph-based tasks, as evidenced by the success of global models like transformers. Over-squashing makes MPGNNs ineffective for problems requiring global information, posing a significant challenge to their applicability in complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of MPGNNs, which have achieved breakthroughs in various domains. It draws inspiration from similar synergistic compositions in molecular point clouds (Kosmala et al., 2023) and sequence models like Mamba (Gu & Dao, 2023) or Hyena (Poli et al., 2023), which offer transformer-like properties with superior scalability.\n    *   **Limitations of Previous Solutions**: MPGNNs are inherently limited by their local message-passing scheme and the over-squashing phenomenon. The design space for spectral GNNs, in contrast to spatial MPGNNs, has been largely unexplored, leaving potential for novel architectural advancements.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Spatio-Spectral Graph Neural Networks (S2GNNs) \\cite{geisler2024wli}, a new modeling paradigm that synergistically combines spatially and spectrally parametrized graph filters.\n        *   S2GNNs utilize a partial eigendecomposition to enable spectral filters that are *spectrally bounded* (operating on a truncated frequency spectrum) but *spatially unbounded*, allowing for global information propagation.\n        *   The architecture can combine spatial and spectral filters additively or sequentially.\n        *   The spectral filter is defined as `Spectral(l)(H(l−1);V,λ) =V [ˆg(l)ϑ(λ)⊙ [V⊤f(l)θ(H(l−1))]]` (Eq. 3), ensuring permutation equivariance.\n    *   **Novelty/Difference**:\n        *   **Synergistic Combination**: The core innovation is the principled combination of local spatial message passing with global spectral filtering, addressing the limitations of each individually.\n        *   **Spectral Domain Neural Network**: The paper proposes the first neural network designed to operate directly in the spectral domain, allowing for data-dependent filtering and channel mixing with negligible computational cost for truncated spectra \\cite{geisler2024wli}.\n        *   **Directed Graph Filters**: S2GNNs generalize spectrally parametrized filters to directed graphs, expanding their applicability.\n        *   **Expressive Positional Encodings**: They introduce stable positional encodings (PEs) derived almost \"for free\" from the partial eigendecomposition, making S2GNNs strictly more expressive than the 1-Weisfeiler-Lehman (WL) test \\cite{geisler2024wli}.\n        *   **Over-squashing Mitigation**: The global nature of spectral filters inherently vanquishes over-squashing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The S2GNN architecture itself, which integrates spatial and spectral filtering for enhanced graph learning \\cite{geisler2024wli}.\n        *   A method for parametrizing spectral filters using Gaussian smearing and linear transformations, designed for stability and expressivity.\n        *   The concept and implementation of a neural network operating directly within the spectral domain.\n        *   Adaptation of spectral filters for directed graphs.\n        *   Novel, cost-effective positional encodings derived from the graph's eigendecomposition.\n    *   **Theoretical Insights or Analysis**:\n        *   **Over-squashing Vanquished**: Theorem 2 formally proves that S2GNNs overcome over-squashing by demonstrating a uniformly lower-bounded Jacobian sensitivity, ensuring effective long-range information exchange.\n        *   **Superior Approximation Bounds**: Theorems 3 and 4 establish strictly tighter approximation-theoretic error bounds for S2GNNs compared to MPGNNs. This is particularly significant for approximating discontinuous or unsmooth ground truth filters, where MPGNNs converge exceedingly slowly.\n        *   **Locality and Spectral Smoothness**: The analysis connects the locality of a filter to the smoothness of its Fourier transform, providing a complementary perspective on MPGNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Empirical verification of MPGNN shortcomings (e.g., over-squashing on \"Clique Path\" graphs) and how S2GNNs overcome them.\n        *   Approximation quality comparison of S2GNNs against purely spatial and spectral filters for a discontinuous target filter.\n        *   Performance evaluation on the challenging peptides-func long-range benchmark tasks (Dwivedi et al., 2022).\n        *   Scalability tests on large graphs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   S2GNNs achieved a new state-of-the-art on the peptides-func benchmark, outperforming MPGNNs and graph transformers with approximately 40% fewer parameters \\cite{geisler2024wli}.\n        *   They demonstrated competitive performance with state-of-the-art sequence models.\n        *   Empirical results on \"Clique Path\" graphs confirmed that spectral filters (and thus S2GNNs) do not exhibit over-squashing (Fig. 5).\n        *   S2GNNs scaled efficiently to millions of nodes on a 40 GB GPU with vanilla full-graph training, exhibiting runtime and space complexity equivalent to MPGNNs \\cite{geisler2024wli}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The efficiency of spectral filters relies on truncating the frequency spectrum (parameter `k` or `λcut`).\n        *   Theoretical guarantees for approximation assume the spectral filter is bandlimited and a universal approximator on a specific interval.\n        *   The default choice for the spectral filter's frequency band is low frequencies, though the method is adaptable.\n    *   **Scope of Applicability**: S2GNNs are designed as general-purpose GNNs, particularly well-suited for tasks requiring effective long-range interactions and where over-squashing is a concern. They are applicable to both undirected and directed graphs.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: S2GNNs introduce a novel and theoretically grounded GNN paradigm that fundamentally addresses the long-standing issues of over-squashing and limited receptive fields in MPGNNs \\cite{geisler2024wli}. By synergistically combining spatial and spectral approaches, they achieve superior expressivity and approximation capabilities, validated by state-of-the-art empirical performance on challenging long-range tasks.\n    *   **Potential Impact on Future Research**: This work opens up a significant new design space for GNN architectures, encouraging further exploration into hybrid spatial-spectral models. The proposed spectral domain neural network, directed graph filters, and \"free\" positional encodings offer concrete avenues for future research, potentially leading to more powerful, efficient, and expressive GNNs for a wider range of applications.",
      "keywords": [
        "Spatio-Spectral Graph Neural Networks (S2GNNs)",
        "Over-squashing mitigation",
        "Long-range interactions",
        "Spectral domain neural network",
        "Partial eigendecomposition",
        "Directed graph filters",
        "Expressive positional encodings",
        "Superior approximation bounds",
        "Peptides-func benchmark",
        "Scalable graph learning",
        "Hybrid spatial-spectral GNNs",
        "Message Passing Graph Neural Networks (MPGNNs)"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose** spatio-spectral graph neural networks (s2gnns) – a **new modeling paradigm** for graph neural networks (gnns)\".\n*   it describes the components of this new paradigm (\"synergistically combines spatially and spectrally parametrized graph filters\").\n*   it discusses the advantages and capabilities of this proposed system (\"vanquish over-squashing\", \"yield strictly tighter approximation-theoretic error bounds\", \"strictly more expressive than the 1-weisfeiler-lehman (wl) test\", \"outperform spatial mpgnns\", \"scale to millions of nodes\").\n*   the introduction further reinforces this by stating: \"we **propose** spatio-spectral graph neural networks (s2gnns) , a **new approach** for tackling the aforementioned limitations\".\n\nwhile the paper includes theoretical analysis (error bounds, expressivity) and empirical results (outperformance on benchmarks), these are presented as evidence and validation for the **new method/system** being proposed. the core contribution is the introduction and description of s2gnns.\n\ntherefore, this paper is best classified as **technical**."
    },
    "file_name": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf"
  },
  {
    "success": true,
    "doc_id": "2d1a23978f7df82718ad3a97586d47ce",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Conventional Graph Neural Networks (GNNs) are fundamentally limited by the expressive power of the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \\cite{zeng20237gv}.\n    *   This limitation prevents GNNs from perceiving crucial higher-order substructures, which are vital for many downstream tasks (e.g., functional groups in organic chemistry) \\cite{zeng20237gv}.\n    *   The problem is challenging because directly constructing higher-order GNNs leads to scalability and complexity issues, while using predefined hand-crafted substructures compromises generalization ability \\cite{zeng20237gv}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to overcome 1-WL limitations include: higher-order GNNs (suffering from high computational cost), predefined hand-crafted substructures (lacking generalization), inductive coloring methods (often task-specific), and positional encoding methods (facing issues like global sign ambiguity or limited inductive generalization) \\cite{zeng20237gv}.\n    *   This work positions itself by introducing a novel framework, Substructure Aware Graph Neural Networks (SAGNN), that leverages subgraphs to enhance GNN expressiveness. It aims to overcome the 1-WL barrier without incurring the high complexity of higher-order GNNs or the generalization issues of fixed substructures, by focusing on flexible subgraph extraction and encoding \\cite{zeng20237gv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the Substructure Aware Graph Neural Network (SAGNN) framework, which injects subgraph-level structural information into node features during message passing \\cite{zeng20237gv}.\n    *   **Novel Subgraph Extraction:** Introduces the \"Cut subgraph,\" which is obtained from the original graph by continuously and selectively removing edges with the highest Edge Betweenness Centrality (EBC) until the graph is split into a specified number of connected blocks \\cite{zeng20237gv}. This allows for capturing structural information at various granularities.\n    *   **Novel Subgraph Encoding:** Extends the random walk encoding paradigm to compute \"return probabilities\" of a rooted node within a subgraph for a given number of steps \\cite{zeng20237gv}. This method efficiently captures rich structural information and is used as a node feature.\n    *   **Subgraph Information Injection:** The encoded subgraph features (from both Ego-networks and Cut subgraphs) are concatenated with initial node features and original graph structural representations. These enhanced features are then propagated through two parallel message passing channels (Ego channel and Cut channel) \\cite{zeng20237gv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Subgraph Definition:** The \"Cut subgraph\" extraction strategy, which dynamically partitions the graph based on edge centrality to reveal meaningful substructures \\cite{zeng20237gv}.\n    *   **Novel Subgraph Encoding Method:** An efficient random walk return probability encoding for subgraphs, designed to capture structural information with reduced time complexity and improved expressiveness \\cite{zeng20237gv}.\n    *   **Framework Design:** The Substructure Aware Graph Neural Network (SAGNN) framework, which seamlessly integrates these novel subgraph features into standard GNN message passing architectures \\cite{zeng20237gv}.\n    *   **Theoretical Insight:** Provides theoretical proof that any 1-WL GNN equipped with components of the SAGNN framework is strictly more powerful than 1-WL \\cite{zeng20237gv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various well-proven graph tasks, including graph classification on TUDatasets (MUTAG, PTC, PROTEINS, NCI1, IMDB-B) and graph regression for drug constrained solubility prediction (ZINC-FULL) \\cite{zeng20237gv}.\n    *   **Key Performance Metrics:** Accuracy for classification tasks and Mean Absolute Error (MAE) for regression tasks \\cite{zeng20237gv}.\n    *   **Comparison Results:**\n        *   Achieved state-of-the-art (SOTA) performance across a variety of datasets and when integrated with different base GNN models (e.g., GIN, PNA) \\cite{zeng20237gv}.\n        *   Demonstrated a maximum performance improvement of 83% compared to base models and 32% compared to previous SOTA methods \\cite{zeng20237gv}.\n        *   GNNs equipped with SAGNN performed flawlessly even on graphs that are known to fail the 3-WL test, indicating significantly enhanced structure perception \\cite{zeng20237gv}.\n\n*   **Limitations & Scope**\n    *   The provided abstract and introduction do not explicitly detail specific technical limitations or assumptions inherent to the SAGNN framework itself.\n    *   The scope of applicability is broad, covering general graph learning tasks such as graph classification and regression, particularly in domains where higher-order structural information is crucial, like chemo-informatics \\cite{zeng20237gv}. The method is designed to enhance the expressiveness of existing 1-WL GNNs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** Significantly pushes the expressive power of GNNs beyond the 1-WL limit by leveraging subgraph information, offering a more scalable alternative to complex higher-order GNNs \\cite{zeng20237gv}.\n    *   **Improved Structure Perception:** Enables GNNs to perceive and utilize higher-order substructures that were previously indistinguishable, leading to superior performance on challenging graph tasks and even on graphs that fail the 3-WL test \\cite{zeng20237gv}.\n    *   **Practical Impact:** Provides a general and flexible framework that can be integrated with various existing GNN architectures, leading to substantial performance gains on real-world graph learning tasks \\cite{zeng20237gv}.\n    *   **Future Research:** The novel subgraph extraction and encoding methods open new avenues for exploring dynamic and expressive ways to incorporate substructural information into graph learning models \\cite{zeng20237gv}.",
    "intriguing_abstract": "Conventional Graph Neural Networks (GNNs) are fundamentally constrained by the 1-Weisfeiler-Leman (1-WL) test, severely limiting their ability to perceive crucial higher-order substructures vital for complex graph tasks. We introduce **Substructure Aware Graph Neural Networks (SAGNN)**, a novel framework that shatters this expressive barrier by seamlessly integrating rich subgraph-level information into message passing. SAGNN pioneers the **\"Cut subgraph\"** extraction, dynamically partitioning graphs based on **Edge Betweenness Centrality (EBC)** to reveal meaningful substructures. Complementing this, we propose an efficient **random walk return probability encoding**, capturing intricate structural details with reduced complexity. The framework theoretically proves strict superiority over 1-WL GNNs. Experimentally, SAGNN achieves **state-of-the-art (SOTA)** performance across diverse **graph classification** and **regression** benchmarks, demonstrating up to 83% improvement and flawlessly distinguishing graphs that even challenge the 3-WL test. This generalizable approach significantly advances GNN **expressiveness**, offering a scalable solution for tasks requiring deep structural understanding, from **chemo-informatics** to social network analysis, paving the way for more powerful graph learning.",
    "keywords": [
      "Substructure Aware Graph Neural Networks (SAGNN)",
      "1-Weisfeiler-Leman (1-WL) limitation",
      "higher-order substructures",
      "Cut subgraph extraction",
      "random walk return probability encoding",
      "subgraph information injection",
      "enhanced GNN expressiveness beyond 1-WL",
      "theoretical proof of expressiveness",
      "state-of-the-art performance",
      "graph classification",
      "graph regression",
      "Edge Betweenness Centrality",
      "improved structure perception",
      "3-WL test failing graphs"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf",
    "citation_key": "zeng20237gv",
    "metadata": {
      "title": "Substructure Aware Graph Neural Networks",
      "authors": [
        "DingYi Zeng",
        "Wanlong Liu",
        "Wenyu Chen",
        "Li Zhou",
        "Malu Zhang",
        "Hong Qu"
      ],
      "published_date": "2023",
      "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
      "file_path": "paper_data/Graph_Neural_Networks/f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Conventional Graph Neural Networks (GNNs) are fundamentally limited by the expressive power of the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \\cite{zeng20237gv}.\n    *   This limitation prevents GNNs from perceiving crucial higher-order substructures, which are vital for many downstream tasks (e.g., functional groups in organic chemistry) \\cite{zeng20237gv}.\n    *   The problem is challenging because directly constructing higher-order GNNs leads to scalability and complexity issues, while using predefined hand-crafted substructures compromises generalization ability \\cite{zeng20237gv}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to overcome 1-WL limitations include: higher-order GNNs (suffering from high computational cost), predefined hand-crafted substructures (lacking generalization), inductive coloring methods (often task-specific), and positional encoding methods (facing issues like global sign ambiguity or limited inductive generalization) \\cite{zeng20237gv}.\n    *   This work positions itself by introducing a novel framework, Substructure Aware Graph Neural Networks (SAGNN), that leverages subgraphs to enhance GNN expressiveness. It aims to overcome the 1-WL barrier without incurring the high complexity of higher-order GNNs or the generalization issues of fixed substructures, by focusing on flexible subgraph extraction and encoding \\cite{zeng20237gv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the Substructure Aware Graph Neural Network (SAGNN) framework, which injects subgraph-level structural information into node features during message passing \\cite{zeng20237gv}.\n    *   **Novel Subgraph Extraction:** Introduces the \"Cut subgraph,\" which is obtained from the original graph by continuously and selectively removing edges with the highest Edge Betweenness Centrality (EBC) until the graph is split into a specified number of connected blocks \\cite{zeng20237gv}. This allows for capturing structural information at various granularities.\n    *   **Novel Subgraph Encoding:** Extends the random walk encoding paradigm to compute \"return probabilities\" of a rooted node within a subgraph for a given number of steps \\cite{zeng20237gv}. This method efficiently captures rich structural information and is used as a node feature.\n    *   **Subgraph Information Injection:** The encoded subgraph features (from both Ego-networks and Cut subgraphs) are concatenated with initial node features and original graph structural representations. These enhanced features are then propagated through two parallel message passing channels (Ego channel and Cut channel) \\cite{zeng20237gv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Subgraph Definition:** The \"Cut subgraph\" extraction strategy, which dynamically partitions the graph based on edge centrality to reveal meaningful substructures \\cite{zeng20237gv}.\n    *   **Novel Subgraph Encoding Method:** An efficient random walk return probability encoding for subgraphs, designed to capture structural information with reduced time complexity and improved expressiveness \\cite{zeng20237gv}.\n    *   **Framework Design:** The Substructure Aware Graph Neural Network (SAGNN) framework, which seamlessly integrates these novel subgraph features into standard GNN message passing architectures \\cite{zeng20237gv}.\n    *   **Theoretical Insight:** Provides theoretical proof that any 1-WL GNN equipped with components of the SAGNN framework is strictly more powerful than 1-WL \\cite{zeng20237gv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various well-proven graph tasks, including graph classification on TUDatasets (MUTAG, PTC, PROTEINS, NCI1, IMDB-B) and graph regression for drug constrained solubility prediction (ZINC-FULL) \\cite{zeng20237gv}.\n    *   **Key Performance Metrics:** Accuracy for classification tasks and Mean Absolute Error (MAE) for regression tasks \\cite{zeng20237gv}.\n    *   **Comparison Results:**\n        *   Achieved state-of-the-art (SOTA) performance across a variety of datasets and when integrated with different base GNN models (e.g., GIN, PNA) \\cite{zeng20237gv}.\n        *   Demonstrated a maximum performance improvement of 83% compared to base models and 32% compared to previous SOTA methods \\cite{zeng20237gv}.\n        *   GNNs equipped with SAGNN performed flawlessly even on graphs that are known to fail the 3-WL test, indicating significantly enhanced structure perception \\cite{zeng20237gv}.\n\n*   **Limitations & Scope**\n    *   The provided abstract and introduction do not explicitly detail specific technical limitations or assumptions inherent to the SAGNN framework itself.\n    *   The scope of applicability is broad, covering general graph learning tasks such as graph classification and regression, particularly in domains where higher-order structural information is crucial, like chemo-informatics \\cite{zeng20237gv}. The method is designed to enhance the expressiveness of existing 1-WL GNNs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** Significantly pushes the expressive power of GNNs beyond the 1-WL limit by leveraging subgraph information, offering a more scalable alternative to complex higher-order GNNs \\cite{zeng20237gv}.\n    *   **Improved Structure Perception:** Enables GNNs to perceive and utilize higher-order substructures that were previously indistinguishable, leading to superior performance on challenging graph tasks and even on graphs that fail the 3-WL test \\cite{zeng20237gv}.\n    *   **Practical Impact:** Provides a general and flexible framework that can be integrated with various existing GNN architectures, leading to substantial performance gains on real-world graph learning tasks \\cite{zeng20237gv}.\n    *   **Future Research:** The novel subgraph extraction and encoding methods open new avenues for exploring dynamic and expressive ways to incorporate substructural information into graph learning models \\cite{zeng20237gv}.",
      "keywords": [
        "Substructure Aware Graph Neural Networks (SAGNN)",
        "1-Weisfeiler-Leman (1-WL) limitation",
        "higher-order substructures",
        "Cut subgraph extraction",
        "random walk return probability encoding",
        "subgraph information injection",
        "enhanced GNN expressiveness beyond 1-WL",
        "theoretical proof of expressiveness",
        "state-of-the-art performance",
        "graph classification",
        "graph regression",
        "Edge Betweenness Centrality",
        "improved structure perception",
        "3-WL test failing graphs"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose a novel framework neural network framework called substructure aware graph neural networks (sagnn) to address these issues.\"** - this directly indicates the presentation of a new system/method.\n2.  **\"we first propose a cut subgraph...\"** and **\"then we extend the random walk encoding paradigm...\"** - these describe new methods and algorithms.\n3.  the introduction discusses a \"foundational problem that limits the expressive power of gnns,\" which is a technical problem that the proposed solution aims to address.\n4.  while the paper also includes theoretical proofs (\"we theoretically prove that our framework is more powerful than 1-wl\") and empirical experiments (\"our extensive experiments demonstrate the effectiveness... achieving state-of-the-art performance\"), these elements serve to validate the **new framework/method** being proposed. the core contribution is the development of sagnn.\n\ntherefore, the paper primarily falls under the **technical** category.\n\n**classification: technical**"
    },
    "file_name": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf"
  },
  {
    "success": true,
    "doc_id": "2f36806267de15c66c6a50899b7a39ae",
    "summary": "Here's a focused summary of the paper \"Graph Structure Learning for Robust Graph Neural Networks\" \\cite{jin2020dh4} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) are highly vulnerable to adversarial attacks, where small, carefully-crafted perturbations (e.g., modifying graph structure) can significantly degrade their performance on downstream tasks like node classification.\n    *   **Importance & Challenge:** This vulnerability raises serious concerns for applying GNNs in safety-critical applications. Existing defense mechanisms often involve a two-stage process (preprocess then train) or require additional clean data, which may not be effective against complex global attacks or practical in real-world scenarios. The challenge lies in effectively cleaning the perturbed graph and learning a robust GNN model simultaneously, especially when adversarial attacks violate intrinsic graph properties.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work relates to two main categories of GNN defense: (1) learning robust networks (e.g., RGCN, PA-GNN) and (2) preprocessing perturbed graphs (e.g., removing dissimilar links, low-rank approximations).\n    *   **Limitations of Previous Solutions:**\n        *   Methods like PA-GNN require additional supervision knowledge from clean graphs, which is often unavailable.\n        *   Two-stage preprocessing methods (first clean, then train GNN) are often too simplistic and may fail to counteract complex global adversarial attacks.\n        *   Many existing attacks and defenses focus on targeted attacks, while global poisoning attacks are more challenging to defend against.\n    *   **Positioning:** \\cite{jin2020dh4} differentiates itself by proposing a *simultaneous* approach to recover the clean graph structure and learn GNN parameters, guided by intrinsic graph properties, rather than relying on separate stages or external clean data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes Pro-GNN, a general framework that jointly learns a robust graph structure `S` and the GNN model parameters `θ` from a perturbed graph. This is achieved through an alternating optimization scheme.\n    *   **Novelty/Difference:**\n        *   **Joint Optimization:** Unlike prior two-stage methods, Pro-GNN simultaneously optimizes both the graph structure and the GNN parameters within a single framework.\n        *   **Property-Guided Structure Learning:** It explicitly leverages three intrinsic properties of real-world graphs—low-rank, sparsity, and feature smoothness—as regularization terms to guide the reconstruction of a clean graph structure `S` from the adversarial one.\n        *   **Objective Function:** The overall objective combines the standard GNN loss with a structure learning loss that includes terms for Frobenius norm (proximity to original graph), L1-norm (sparsity), nuclear norm (low-rank), and a trace term based on the normalized graph Laplacian (feature smoothness).\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework (Pro-GNN):** A unified framework for robust GNNs that integrates graph structure learning and GNN parameter learning.\n    *   **Property-Driven Regularization:** Formalization of low-rank, sparsity, and feature smoothness properties into a comprehensive regularization term for graph structure learning.\n    *   **Alternating Optimization Algorithm:** Development of an alternating optimization strategy to solve the non-convex joint objective function, iteratively updating the graph structure `S` and GNN parameters `θ`. The `S` update involves solving a problem with L1-norm and nuclear norm, often tackled with ADMM-like approaches involving singular value shrinkage and thresholding.\n    *   **Empirical Evidence of Property Violation:** Demonstrates through empirical analysis (e.g., Figure 1) how adversarial attacks (like Metattack) significantly violate these intrinsic graph properties, motivating their use for defense.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various real-world graph datasets (e.g., Cora, Citeseer, Polblogs, as implied by Figure 1). The evaluation focuses on defending against poisoning adversarial attacks on graph structure.\n    *   **Key Performance Metrics:** Node classification accuracy is the primary metric used to evaluate robustness.\n    *   **Comparison Results:** Pro-GNN achieves significantly better performance compared to state-of-the-art defense methods, even when the graph is heavily perturbed by adversarial attacks. The paper claims superior robustness across different attack types.\n    *   **Reproducibility:** The implementation of Pro-GNN is released as part of the DeepRobust repository, with specific experimental settings available on GitHub.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The current work focuses on poisoning adversarial attacks on graph structure, assuming node features remain unperturbed.\n        *   While the framework is general, the specific GNN model used for demonstration is GCN, though the authors state it's straightforward to extend to other GNNs.\n        *   The paper acknowledges that \"there could be more properties to be explored\" beyond low-rank, sparsity, and feature smoothness, leaving this for future work.\n    *   **Scope of Applicability:** Primarily applicable to scenarios where GNNs are deployed on graphs susceptible to structural adversarial perturbations, particularly poisoning attacks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Pro-GNN represents a significant advancement by introducing a principled, property-guided framework for simultaneously learning robust graph structures and GNN models, moving beyond the limitations of sequential preprocessing or data-intensive defense strategies.\n    *   **Potential Impact:** This work provides a strong foundation for developing more robust GNNs, crucial for their reliable deployment in sensitive applications. It highlights the importance of leveraging intrinsic graph properties for adversarial defense and opens avenues for exploring other graph properties or more complex joint optimization schemes in future research.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized data analysis on complex data, yet their pervasive vulnerability to adversarial attacks, particularly structural perturbations, remains a critical impediment to their deployment in safety-critical applications. Existing defense mechanisms often fall short, relying on simplistic two-stage processes or unavailable clean data. We introduce Pro-GNN, a novel and principled framework that fundamentally rethinks GNN robustness by *jointly* learning a clean graph structure and robust GNN parameters directly from a perturbed graph.\n\nOur key innovation lies in leveraging three intrinsic properties of real-world graphs—low-rank, sparsity, and feature smoothness—as powerful regularization terms to guide the reconstruction of the underlying clean structure. Through an alternating optimization scheme, Pro-GNN simultaneously purifies the graph and optimizes the GNN, effectively counteracting complex poisoning attacks. Extensive experiments demonstrate Pro-GNN's superior performance in node classification tasks, significantly outperforming state-of-the-art defenses even under heavy adversarial perturbations. This work offers a paradigm shift for building truly robust GNNs, paving the way for their reliable and secure application in diverse domains.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Adversarial attacks on graph structure",
      "Robust GNNs",
      "Pro-GNN framework",
      "Joint optimization",
      "Graph structure learning",
      "Property-guided regularization",
      "Low-rank graph property",
      "Sparsity graph property",
      "Feature smoothness graph property",
      "Alternating optimization algorithm",
      "Node classification",
      "Superior robustness"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf",
    "citation_key": "jin2020dh4",
    "metadata": {
      "title": "Graph Structure Learning for Robust Graph Neural Networks",
      "authors": [
        "Wei Jin",
        "Yao Ma",
        "Xiaorui Liu",
        "Xianfeng Tang",
        "Suhang Wang",
        "Jiliang Tang"
      ],
      "published_date": "2020",
      "abstract": "Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses. The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",
      "file_path": "paper_data/Graph_Neural_Networks/011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Graph Structure Learning for Robust Graph Neural Networks\" \\cite{jin2020dh4} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs) are highly vulnerable to adversarial attacks, where small, carefully-crafted perturbations (e.g., modifying graph structure) can significantly degrade their performance on downstream tasks like node classification.\n    *   **Importance & Challenge:** This vulnerability raises serious concerns for applying GNNs in safety-critical applications. Existing defense mechanisms often involve a two-stage process (preprocess then train) or require additional clean data, which may not be effective against complex global attacks or practical in real-world scenarios. The challenge lies in effectively cleaning the perturbed graph and learning a robust GNN model simultaneously, especially when adversarial attacks violate intrinsic graph properties.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work relates to two main categories of GNN defense: (1) learning robust networks (e.g., RGCN, PA-GNN) and (2) preprocessing perturbed graphs (e.g., removing dissimilar links, low-rank approximations).\n    *   **Limitations of Previous Solutions:**\n        *   Methods like PA-GNN require additional supervision knowledge from clean graphs, which is often unavailable.\n        *   Two-stage preprocessing methods (first clean, then train GNN) are often too simplistic and may fail to counteract complex global adversarial attacks.\n        *   Many existing attacks and defenses focus on targeted attacks, while global poisoning attacks are more challenging to defend against.\n    *   **Positioning:** \\cite{jin2020dh4} differentiates itself by proposing a *simultaneous* approach to recover the clean graph structure and learn GNN parameters, guided by intrinsic graph properties, rather than relying on separate stages or external clean data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes Pro-GNN, a general framework that jointly learns a robust graph structure `S` and the GNN model parameters `θ` from a perturbed graph. This is achieved through an alternating optimization scheme.\n    *   **Novelty/Difference:**\n        *   **Joint Optimization:** Unlike prior two-stage methods, Pro-GNN simultaneously optimizes both the graph structure and the GNN parameters within a single framework.\n        *   **Property-Guided Structure Learning:** It explicitly leverages three intrinsic properties of real-world graphs—low-rank, sparsity, and feature smoothness—as regularization terms to guide the reconstruction of a clean graph structure `S` from the adversarial one.\n        *   **Objective Function:** The overall objective combines the standard GNN loss with a structure learning loss that includes terms for Frobenius norm (proximity to original graph), L1-norm (sparsity), nuclear norm (low-rank), and a trace term based on the normalized graph Laplacian (feature smoothness).\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework (Pro-GNN):** A unified framework for robust GNNs that integrates graph structure learning and GNN parameter learning.\n    *   **Property-Driven Regularization:** Formalization of low-rank, sparsity, and feature smoothness properties into a comprehensive regularization term for graph structure learning.\n    *   **Alternating Optimization Algorithm:** Development of an alternating optimization strategy to solve the non-convex joint objective function, iteratively updating the graph structure `S` and GNN parameters `θ`. The `S` update involves solving a problem with L1-norm and nuclear norm, often tackled with ADMM-like approaches involving singular value shrinkage and thresholding.\n    *   **Empirical Evidence of Property Violation:** Demonstrates through empirical analysis (e.g., Figure 1) how adversarial attacks (like Metattack) significantly violate these intrinsic graph properties, motivating their use for defense.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various real-world graph datasets (e.g., Cora, Citeseer, Polblogs, as implied by Figure 1). The evaluation focuses on defending against poisoning adversarial attacks on graph structure.\n    *   **Key Performance Metrics:** Node classification accuracy is the primary metric used to evaluate robustness.\n    *   **Comparison Results:** Pro-GNN achieves significantly better performance compared to state-of-the-art defense methods, even when the graph is heavily perturbed by adversarial attacks. The paper claims superior robustness across different attack types.\n    *   **Reproducibility:** The implementation of Pro-GNN is released as part of the DeepRobust repository, with specific experimental settings available on GitHub.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The current work focuses on poisoning adversarial attacks on graph structure, assuming node features remain unperturbed.\n        *   While the framework is general, the specific GNN model used for demonstration is GCN, though the authors state it's straightforward to extend to other GNNs.\n        *   The paper acknowledges that \"there could be more properties to be explored\" beyond low-rank, sparsity, and feature smoothness, leaving this for future work.\n    *   **Scope of Applicability:** Primarily applicable to scenarios where GNNs are deployed on graphs susceptible to structural adversarial perturbations, particularly poisoning attacks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Pro-GNN represents a significant advancement by introducing a principled, property-guided framework for simultaneously learning robust graph structures and GNN models, moving beyond the limitations of sequential preprocessing or data-intensive defense strategies.\n    *   **Potential Impact:** This work provides a strong foundation for developing more robust GNNs, crucial for their reliable deployment in sensitive applications. It highlights the importance of leveraging intrinsic graph properties for adversarial defense and opens avenues for exploring other graph properties or more complex joint optimization schemes in future research.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Adversarial attacks on graph structure",
        "Robust GNNs",
        "Pro-GNN framework",
        "Joint optimization",
        "Graph structure learning",
        "Property-guided regularization",
        "Low-rank graph property",
        "Sparsity graph property",
        "Feature smoothness graph property",
        "Alternating optimization algorithm",
        "Node classification",
        "Superior robustness"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract analysis:** the abstract explicitly states, \"we propose a general framework pro-gnn\". it then describes this framework as being able to \"jointly learn a structural graph and a robust graph neural network model.\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems. the subsequent mention of \"extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance\" indicates an empirical evaluation of the *proposed* technical solution, which is standard for technical papers.\n*   **introduction analysis:** the introduction sets the stage by discussing the problem (gnn vulnerability to adversarial attacks) and the need for robust algorithms, leading into the proposed solution.\n*   **exclusion of other types:**\n    *   it's not a **survey** as it proposes a new method, rather than reviewing existing literature.\n    *   it's not purely **theoretical** as its focus is on a practical framework and its empirical validation, not mathematical proofs or formal models as the primary contribution.\n    *   while it includes **empirical** work, the core contribution is the *proposal* of a new framework (pro-gnn), making it primarily technical. the empirical part serves to validate the technical contribution.\n    *   it's not a **case_study** as it's not a detailed analysis of a specific application scenario.\n    *   it's not a **position** paper as it presents a concrete solution, not just an argument or vision.\n    *   it's not a **short** paper, given the kdd venue and 9-page length."
    },
    "file_name": "011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf"
  },
  {
    "success": true,
    "doc_id": "c96851eb4e52b1e78ebe17e88a7c7093",
    "summary": "Here's a focused summary of the paper \"Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information\" \\cite{dai2020p5t} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit significant bias on protected sensitive attributes (e.g., gender, region) in node classification tasks. This bias is not only inherited from historical data but is also *magnified* by graph structures (homophily) and the message-passing mechanism, where nodes with similar sensitive attributes tend to connect.\n    *   **Importance and Challenge**: This discrimination severely limits the adoption of powerful GNNs in sensitive real-world applications (e.g., crime prediction, job applicant ranking). Existing fair classification methods are primarily designed for independently and identically distributed (i.i.d.) data and do not account for graph structures. Furthermore, a critical practical challenge is the *scarcity of sensitive attribute information* (sparse annotations), as individuals are often unwilling to share such data, rendering many existing fair model approaches (which require full sensitive attribute knowledge) impractical.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon general fairness in machine learning (pre-processing, in-processing, post-processing) and recent efforts in learning fair graph embeddings.\n    *   **Limitations of Previous Solutions**:\n        *   Most fair classification methods are for i.i.d. data and cannot be directly applied to graph data due to the unique bias introduced by graph structures.\n        *   Existing fair graph embedding works (e.g., Fairwalk \\cite{dai2020p5t} [36], Bose and Hamilton \\cite{dai2020p5t} [4]) often focus on plain graphs without node attributes, aim for fair representations rather than fair node classification, and crucially, *require sensitive attributes for all nodes*, which is a significant practical limitation.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **FairGNN**, a novel framework for fair node classification with limited sensitive attribute information. It comprises three main components:\n        *   **GNN Classifier ($f_G$)**: A standard GNN for node classification.\n        *   **GCN-based Sensitive Attribute Estimator ($f_E$)**: A GCN model trained to predict sensitive attributes for nodes where this information is unknown, effectively addressing the data scarcity problem.\n        *   **Adversary ($f_A$)**: An adversarial network that attempts to predict the (known or estimated) sensitive attributes from the node representations learned by $f_G$. The GNN classifier ($f_G$) is trained to fool this adversary, ensuring its predictions are independent of sensitive attributes.\n        *   **Fairness Constraint**: An additional regularization term is introduced to stabilize the training process and further enforce that predictions are invariant with respect to the estimated sensitive attributes.\n    *   **Novelty**:\n        *   It is the first work to tackle the problem of learning fair GNNs specifically under the practical constraint of *limited sensitive attribute information*.\n        *   The introduction of a sensitive attribute estimator ($f_E$) is a key innovation, enabling the application of adversarial debiasing and fairness constraints even when sensitive attributes are largely unknown.\n        *   The framework combines adversarial learning with a direct fairness constraint for robust debiasing in GNNs.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The FairGNN framework, which integrates a GNN classifier, a sensitive attribute estimator, an adversary, and a fairness constraint to achieve fair node classification.\n    *   **System Design/Architectural Innovations**: A modular architecture that effectively leverages limited sensitive attribute information by estimating missing values, making adversarial debiasing feasible in real-world scenarios.\n    *   **Theoretical Insights**: The paper provides theoretical analysis demonstrating that FairGNN can ensure fairness at the global minimum, even when relying on *estimated* sensitive attributes.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on three real-world datasets: Pokec-z, Pokec-n (social networks with region as sensitive attribute), and NBA (sports network with nationality as sensitive attribute).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Classification Performance**: Measured by Accuracy (ACC) and Area Under the Curve (AUC).\n        *   **Fairness Metrics**: Measured by the absolute difference in Statistical Parity ($\\Delta SP$) and Equal Opportunity ($\\Delta EO$), where lower values indicate better fairness.\n        *   **Results**: Preliminary analysis showed that GNNs (GCN, GAT) achieve higher accuracy than MLP-based models but exhibit significantly worse fairness (higher $\\Delta SP$ and $\\Delta EO$), confirming the problem. FairGNN consistently demonstrated superior performance, effectively *eliminating discrimination* (significantly reducing $\\Delta SP$ and $\\Delta EO$) while *maintaining high node classification accuracy* across all datasets, outperforming baseline GNNs and MLP variants.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The current work focuses on binary class and binary sensitive attribute settings. Its effectiveness relies on the quality of the sensitive attribute estimator.\n    *   **Scope of Applicability**: Applicable to node classification tasks on attributed graphs where sensitive attribute information is sparse.\n\n*   **7. Technical Significance**\n    *   **Advances the Technical State-of-the-Art**: This paper is the first to address the critical and practical problem of learning fair GNNs when sensitive attribute information is limited. It provides a robust and theoretically grounded framework (FairGNN) that effectively mitigates bias in GNNs while preserving predictive performance.\n    *   **Potential Impact on Future Research**: FairGNN paves the way for the ethical deployment of GNNs in sensitive domains. It opens avenues for future research into extending fairness to multi-class/multi-sensitive attribute settings, exploring more advanced sensitive attribute estimation techniques, and investigating other fairness definitions in graph contexts.",
    "intriguing_abstract": "Graph Neural Networks (GNNs), while powerful, are notorious for inheriting and *magnifying* biases on protected sensitive attributes within node classification tasks, severely limiting their ethical deployment in critical real-world applications. This discrimination is exacerbated by inherent graph structures like homophily and the message-passing mechanism. A significant practical hurdle, however, is the widespread scarcity of sensitive attribute information, rendering most existing fair machine learning methods ineffective.\n\nWe introduce **FairGNN**, a pioneering framework designed to achieve fair node classification even with *limited sensitive attribute information*. FairGNN innovatively integrates a GNN classifier, a GCN-based sensitive attribute estimator to infer missing data, and an adversarial network that debiases representations by making them independent of sensitive attributes. An additional fairness constraint further stabilizes training. This novel approach is the first to tackle fair GNNs under such practical data constraints, offering theoretical guarantees of fairness even with estimated attributes. Extensive experiments on real-world datasets demonstrate FairGNN's ability to effectively *eliminate discrimination* (significantly reducing $\\Delta SP$ and $\\Delta EO$) while *maintaining high classification accuracy*, outperforming baseline GNNs. FairGNN paves the way for robust, ethical GNN deployment, advancing the state-of-the-art in fair graph learning.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "sensitive attribute bias",
      "limited sensitive attribute information",
      "FairGNN framework",
      "GCN-based Sensitive Attribute Estimator",
      "adversarial debiasing",
      "fairness constraint",
      "node classification",
      "homophily",
      "Statistical Parity",
      "Equal Opportunity",
      "mitigating discrimination",
      "predictive performance preservation"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf",
    "citation_key": "dai2020p5t",
    "metadata": {
      "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information",
      "authors": [
        "Enyan Dai",
        "Suhang Wang"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy.",
      "file_path": "paper_data/Graph_Neural_Networks/0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf",
      "venue": "Web Search and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information\" \\cite{dai2020p5t} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit significant bias on protected sensitive attributes (e.g., gender, region) in node classification tasks. This bias is not only inherited from historical data but is also *magnified* by graph structures (homophily) and the message-passing mechanism, where nodes with similar sensitive attributes tend to connect.\n    *   **Importance and Challenge**: This discrimination severely limits the adoption of powerful GNNs in sensitive real-world applications (e.g., crime prediction, job applicant ranking). Existing fair classification methods are primarily designed for independently and identically distributed (i.i.d.) data and do not account for graph structures. Furthermore, a critical practical challenge is the *scarcity of sensitive attribute information* (sparse annotations), as individuals are often unwilling to share such data, rendering many existing fair model approaches (which require full sensitive attribute knowledge) impractical.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon general fairness in machine learning (pre-processing, in-processing, post-processing) and recent efforts in learning fair graph embeddings.\n    *   **Limitations of Previous Solutions**:\n        *   Most fair classification methods are for i.i.d. data and cannot be directly applied to graph data due to the unique bias introduced by graph structures.\n        *   Existing fair graph embedding works (e.g., Fairwalk \\cite{dai2020p5t} [36], Bose and Hamilton \\cite{dai2020p5t} [4]) often focus on plain graphs without node attributes, aim for fair representations rather than fair node classification, and crucially, *require sensitive attributes for all nodes*, which is a significant practical limitation.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **FairGNN**, a novel framework for fair node classification with limited sensitive attribute information. It comprises three main components:\n        *   **GNN Classifier ($f_G$)**: A standard GNN for node classification.\n        *   **GCN-based Sensitive Attribute Estimator ($f_E$)**: A GCN model trained to predict sensitive attributes for nodes where this information is unknown, effectively addressing the data scarcity problem.\n        *   **Adversary ($f_A$)**: An adversarial network that attempts to predict the (known or estimated) sensitive attributes from the node representations learned by $f_G$. The GNN classifier ($f_G$) is trained to fool this adversary, ensuring its predictions are independent of sensitive attributes.\n        *   **Fairness Constraint**: An additional regularization term is introduced to stabilize the training process and further enforce that predictions are invariant with respect to the estimated sensitive attributes.\n    *   **Novelty**:\n        *   It is the first work to tackle the problem of learning fair GNNs specifically under the practical constraint of *limited sensitive attribute information*.\n        *   The introduction of a sensitive attribute estimator ($f_E$) is a key innovation, enabling the application of adversarial debiasing and fairness constraints even when sensitive attributes are largely unknown.\n        *   The framework combines adversarial learning with a direct fairness constraint for robust debiasing in GNNs.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The FairGNN framework, which integrates a GNN classifier, a sensitive attribute estimator, an adversary, and a fairness constraint to achieve fair node classification.\n    *   **System Design/Architectural Innovations**: A modular architecture that effectively leverages limited sensitive attribute information by estimating missing values, making adversarial debiasing feasible in real-world scenarios.\n    *   **Theoretical Insights**: The paper provides theoretical analysis demonstrating that FairGNN can ensure fairness at the global minimum, even when relying on *estimated* sensitive attributes.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on three real-world datasets: Pokec-z, Pokec-n (social networks with region as sensitive attribute), and NBA (sports network with nationality as sensitive attribute).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Classification Performance**: Measured by Accuracy (ACC) and Area Under the Curve (AUC).\n        *   **Fairness Metrics**: Measured by the absolute difference in Statistical Parity ($\\Delta SP$) and Equal Opportunity ($\\Delta EO$), where lower values indicate better fairness.\n        *   **Results**: Preliminary analysis showed that GNNs (GCN, GAT) achieve higher accuracy than MLP-based models but exhibit significantly worse fairness (higher $\\Delta SP$ and $\\Delta EO$), confirming the problem. FairGNN consistently demonstrated superior performance, effectively *eliminating discrimination* (significantly reducing $\\Delta SP$ and $\\Delta EO$) while *maintaining high node classification accuracy* across all datasets, outperforming baseline GNNs and MLP variants.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The current work focuses on binary class and binary sensitive attribute settings. Its effectiveness relies on the quality of the sensitive attribute estimator.\n    *   **Scope of Applicability**: Applicable to node classification tasks on attributed graphs where sensitive attribute information is sparse.\n\n*   **7. Technical Significance**\n    *   **Advances the Technical State-of-the-Art**: This paper is the first to address the critical and practical problem of learning fair GNNs when sensitive attribute information is limited. It provides a robust and theoretically grounded framework (FairGNN) that effectively mitigates bias in GNNs while preserving predictive performance.\n    *   **Potential Impact on Future Research**: FairGNN paves the way for the ethical deployment of GNNs in sensitive domains. It opens avenues for future research into extending fairness to multi-class/multi-sensitive attribute settings, exploring more advanced sensitive attribute estimation techniques, and investigating other fairness definitions in graph contexts.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "sensitive attribute bias",
        "limited sensitive attribute information",
        "FairGNN framework",
        "GCN-based Sensitive Attribute Estimator",
        "adversarial debiasing",
        "fairness constraint",
        "node classification",
        "homophily",
        "Statistical Parity",
        "Equal Opportunity",
        "mitigating discrimination",
        "predictive performance preservation"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"fairgnn is **proposed** to eliminate the bias of gnns whilst maintaining high node classification accuracy...\"\n*   it discusses a \"novel and important problem of learning fair gnns\" and then presents a solution.\n*   while it mentions \"theoretical analysis\" and \"extensive experiments,\" these are presented as supporting evidence for the effectiveness of the **proposed** method (fairgnn).\n\nthis strongly aligns with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\"\n\n**classification: technical**"
    },
    "file_name": "0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf"
  },
  {
    "success": true,
    "doc_id": "12a446b6e36c21abd1a1ccda15e7388d",
    "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### GemNet: Universal Directional Graph Neural Networks for Molecules \\cite{klicpera20215fk}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) for molecular property prediction suffer from theoretical limitations, being only as powerful as the 1-Weisfeiler Lehman test, meaning they cannot distinguish certain types of graphs or molecules. Furthermore, rotationally invariant layers in GNNs can lose crucial relative spatial information (the \"Picasso problem\"), hindering accurate predictions for properties that depend on molecular geometry.\n    *   **Importance & Challenge**: Accurately predicting molecular interactions and quantum mechanical properties is crucial for accelerating molecular dynamics simulations by orders of magnitude, which could revolutionize chemical simulations. The challenge lies in developing GNNs that are both highly expressive (universal approximators) and computationally efficient, while respecting fundamental symmetries (invariance to translation, equivariance to permutation and rotation) inherent in molecular systems.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and extends the field of directional GNNs, which explicitly represent directional information (e.g., angles, dihedral angles) to achieve equivariance/invariance without relying on complex group representations. It also relates to equivariant neural networks that leverage SO(3) group representations (e.g., Tensor Field Networks \\cite{klicpera20215fk}) and prior work on GNN expressiveness.\n    *   **Limitations of Previous Solutions**:\n        *   Regular GNNs are limited in expressiveness (1-Weisfeiler Lehman test).\n        *   SO(3)-based equivariant models, while powerful, use four-dimensional S3 sphere representations, making them significantly more computationally expensive than models based on the three-dimensional S2 sphere (spherical representations).\n        *   Previous GNNs often rely on one-hop message passing, potentially limiting their ability to capture complex geometric relationships like dihedral angles.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Universality Proof for Spherical Representations**: The paper first theoretically demonstrates that GNNs utilizing spherical (S2) representations are universal approximators for predictions that are invariant to translation and equivariant to permutation and rotation. This is achieved by showing equivalence to SO(3)-based Tensor Field Networks \\cite{klicpera20215fk} for invariant predictions and then generalizing to equivariant predictions using a result from Villar et al. \\cite{klicpera20215fk}.\n        *   **Discretization to Directional Message Passing**: Spherical representations are discretized by sampling directions to neighboring atoms, which are interpreted as directed edge embeddings.\n        *   **Geometric Message Passing**: This discretization naturally leads to a **two-hop message passing** scheme. Messages are passed between edge embeddings (e.g., `m_ca` and `m_db`) via an intermediate edge (e.g., `x_ba`), allowing the model to incorporate full geometrical information: interatomic distances, angles (`φ_cab`, `φ_abd`), and crucially, **dihedral angles** (`θ_cabd`).\n        *   **GemNet Architecture**: The Geometric Message Passing Neural Network (GemNet) incorporates multiple structural enhancements:\n            *   Uses spherical Fourier-Bessel bases with polynomial radial envelopes for representing geometric information.\n            *   Employs symmetric message passing.\n            *   Includes residual connections between layers.\n            *   Utilizes predetermined scaling factors for variance stabilization instead of standard normalization layers.\n            *   Applies gated activation functions for non-linearities.\n    *   **Novelty/Difference**:\n        *   **Theoretical Foundation**: Proving the universality of *spherical (S2) representations* for rotationally equivariant predictions, demonstrating they offer the same expressivity as more expensive SO(3) representations. This closes a significant theoretical gap.\n        *   **Geometric Information Integration**: The explicit and systematic incorporation of distances, angles, and especially **dihedral angles** through a novel **two-hop message passing** scheme based on directed edge embeddings.\n        *   **Architectural Enhancements**: The combination of specific structural improvements (variance stabilization, gated activations, residual connections) tailored for molecular data, leading to a highly accurate and sample-efficient model.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insight**: Proof of universality for GNNs with spherical (S2) representations for rotationally equivariant predictions, showing they are sufficient without needing more complex SO(3) representations.\n    *   **Novel Algorithm/Method**:\n        *   **Geometric Message Passing**: A novel two-hop message passing scheme that leverages directed edge embeddings and explicitly incorporates interatomic distances, angles, and dihedral angles into message computations.\n        *   Discretization of spherical representations into a tractable GNN framework.\n    *   **System Design/Architectural Innovation**:\n        *   **GemNet**: A robust and high-performing GNN architecture that integrates geometric message passing with specific enhancements like variance stabilization via predetermined scaling factors, residual connections, and gated activations.\n        *   Use of spherical Fourier-Bessel bases for geometric feature encoding.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Ablation Studies**: Comprehensive ablation studies were performed to evaluate the individual contribution of each proposed improvement (geometric message passing, residual connections, variance stabilization, gated activation).\n        *   **Benchmark Comparisons**: GemNet was benchmarked against previous state-of-the-art models on challenging molecular dynamics datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Primarily Mean Absolute Error (MAE) for force predictions.\n        *   **Datasets**: COLL, MD17, and OC20.\n        *   **Results**: GemNet significantly outperformed previous models:\n            *   34% improvement on COLL.\n            *   41% improvement on MD17.\n            *   20% improvement on OC20.\n        *   **Specific Strengths**: The largest improvements were observed for the most challenging molecules, particularly those exhibiting dynamic, non-planar geometries, indicating GemNet's superior ability to capture complex 3D structural information. Ablation studies confirmed that each proposed enhancement contributed significantly to reducing model error with minimal computational overhead.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The universality proof relies on the assumption that the set of relative vectors does not span a (d-1)-dimensional space (Theorem 3). The discretization scheme for spherical representations might not always provide a universal approximation guarantee depending on the specific mesh. The model's performance is tied to the interaction cutoff `c_emb`.\n    *   **Scope of Applicability**: Primarily focused on predicting energy and quantum mechanical properties of molecules, particularly for accelerating molecular dynamics simulations. Applicable to tasks requiring accurate representation of 3D molecular geometry and interactions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GemNet significantly advances the technical state-of-the-art in molecular property prediction by providing a theoretically grounded and empirically superior GNN architecture. It resolves a long-standing theoretical limitation regarding GNN expressiveness for molecular data by proving the sufficiency of S2 representations.\n    *   **Potential Impact on Future Research**:\n        *   **Foundation for Equivariant GNNs**: The universality proof for S2 representations provides a more efficient theoretical foundation for designing equivariant GNNs, potentially leading to simpler yet equally powerful models.\n        *   **Improved Molecular Simulations**: The substantial performance gains, especially on challenging molecules, suggest GemNet can enable more accurate and faster *ab initio* molecular dynamics simulations, opening new avenues for drug discovery, materials science, and chemical engineering.\n        *   **Geometric Feature Learning**: The explicit and effective integration of dihedral angles and two-hop message passing highlights the importance of higher-order geometric features, encouraging further research into sophisticated geometric feature learning in GNNs.",
    "intriguing_abstract": "Accurately predicting molecular properties is paramount for accelerating *ab initio* molecular dynamics, yet existing Graph Neural Networks (GNNs) struggle with limited expressiveness and capturing crucial 3D geometric information like dihedral angles. We introduce GemNet, a novel Universal Directional Graph Neural Network that overcomes these limitations. Theoretically, we prove that GNNs utilizing efficient spherical (S2) representations are universal approximators for rotationally equivariant predictions, matching the power of more complex SO(3) models. Our core innovation lies in a two-hop geometric message passing scheme that explicitly incorporates interatomic distances, angles, and critically, dihedral angles, enabling a richer understanding of molecular geometry. GemNet integrates these advancements with spherical Fourier-Bessel bases and variance stabilization, achieving unprecedented accuracy. Benchmarking on challenging datasets like COLL, MD17, and OC20, GemNet delivers up to 41% improvement over state-of-the-art models, particularly for dynamic, non-planar molecules. This breakthrough significantly advances molecular property prediction, paving the way for revolutionary progress in drug discovery and materials science.",
    "keywords": [
      "GemNet",
      "Universal Directional Graph Neural Networks",
      "Spherical (S2) representations universality",
      "Geometric Message Passing",
      "Two-hop message passing",
      "Dihedral angles integration",
      "Molecular property prediction",
      "Molecular dynamics simulations acceleration",
      "Equivariant GNNs",
      "State-of-the-art force predictions",
      "Ablation studies",
      "Directed edge embeddings",
      "Quantum mechanical properties"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf",
    "citation_key": "klicpera20215fk",
    "metadata": {
      "title": "GemNet: Universal Directional Graph Neural Networks for Molecules",
      "authors": [
        "Johannes Klicpera",
        "Florian Becker",
        "Stephan Gunnemann"
      ],
      "published_date": "2021",
      "abstract": "Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",
      "file_path": "paper_data/Graph_Neural_Networks/5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### GemNet: Universal Directional Graph Neural Networks for Molecules \\cite{klicpera20215fk}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) for molecular property prediction suffer from theoretical limitations, being only as powerful as the 1-Weisfeiler Lehman test, meaning they cannot distinguish certain types of graphs or molecules. Furthermore, rotationally invariant layers in GNNs can lose crucial relative spatial information (the \"Picasso problem\"), hindering accurate predictions for properties that depend on molecular geometry.\n    *   **Importance & Challenge**: Accurately predicting molecular interactions and quantum mechanical properties is crucial for accelerating molecular dynamics simulations by orders of magnitude, which could revolutionize chemical simulations. The challenge lies in developing GNNs that are both highly expressive (universal approximators) and computationally efficient, while respecting fundamental symmetries (invariance to translation, equivariance to permutation and rotation) inherent in molecular systems.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and extends the field of directional GNNs, which explicitly represent directional information (e.g., angles, dihedral angles) to achieve equivariance/invariance without relying on complex group representations. It also relates to equivariant neural networks that leverage SO(3) group representations (e.g., Tensor Field Networks \\cite{klicpera20215fk}) and prior work on GNN expressiveness.\n    *   **Limitations of Previous Solutions**:\n        *   Regular GNNs are limited in expressiveness (1-Weisfeiler Lehman test).\n        *   SO(3)-based equivariant models, while powerful, use four-dimensional S3 sphere representations, making them significantly more computationally expensive than models based on the three-dimensional S2 sphere (spherical representations).\n        *   Previous GNNs often rely on one-hop message passing, potentially limiting their ability to capture complex geometric relationships like dihedral angles.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Universality Proof for Spherical Representations**: The paper first theoretically demonstrates that GNNs utilizing spherical (S2) representations are universal approximators for predictions that are invariant to translation and equivariant to permutation and rotation. This is achieved by showing equivalence to SO(3)-based Tensor Field Networks \\cite{klicpera20215fk} for invariant predictions and then generalizing to equivariant predictions using a result from Villar et al. \\cite{klicpera20215fk}.\n        *   **Discretization to Directional Message Passing**: Spherical representations are discretized by sampling directions to neighboring atoms, which are interpreted as directed edge embeddings.\n        *   **Geometric Message Passing**: This discretization naturally leads to a **two-hop message passing** scheme. Messages are passed between edge embeddings (e.g., `m_ca` and `m_db`) via an intermediate edge (e.g., `x_ba`), allowing the model to incorporate full geometrical information: interatomic distances, angles (`φ_cab`, `φ_abd`), and crucially, **dihedral angles** (`θ_cabd`).\n        *   **GemNet Architecture**: The Geometric Message Passing Neural Network (GemNet) incorporates multiple structural enhancements:\n            *   Uses spherical Fourier-Bessel bases with polynomial radial envelopes for representing geometric information.\n            *   Employs symmetric message passing.\n            *   Includes residual connections between layers.\n            *   Utilizes predetermined scaling factors for variance stabilization instead of standard normalization layers.\n            *   Applies gated activation functions for non-linearities.\n    *   **Novelty/Difference**:\n        *   **Theoretical Foundation**: Proving the universality of *spherical (S2) representations* for rotationally equivariant predictions, demonstrating they offer the same expressivity as more expensive SO(3) representations. This closes a significant theoretical gap.\n        *   **Geometric Information Integration**: The explicit and systematic incorporation of distances, angles, and especially **dihedral angles** through a novel **two-hop message passing** scheme based on directed edge embeddings.\n        *   **Architectural Enhancements**: The combination of specific structural improvements (variance stabilization, gated activations, residual connections) tailored for molecular data, leading to a highly accurate and sample-efficient model.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insight**: Proof of universality for GNNs with spherical (S2) representations for rotationally equivariant predictions, showing they are sufficient without needing more complex SO(3) representations.\n    *   **Novel Algorithm/Method**:\n        *   **Geometric Message Passing**: A novel two-hop message passing scheme that leverages directed edge embeddings and explicitly incorporates interatomic distances, angles, and dihedral angles into message computations.\n        *   Discretization of spherical representations into a tractable GNN framework.\n    *   **System Design/Architectural Innovation**:\n        *   **GemNet**: A robust and high-performing GNN architecture that integrates geometric message passing with specific enhancements like variance stabilization via predetermined scaling factors, residual connections, and gated activations.\n        *   Use of spherical Fourier-Bessel bases for geometric feature encoding.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Ablation Studies**: Comprehensive ablation studies were performed to evaluate the individual contribution of each proposed improvement (geometric message passing, residual connections, variance stabilization, gated activation).\n        *   **Benchmark Comparisons**: GemNet was benchmarked against previous state-of-the-art models on challenging molecular dynamics datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Primarily Mean Absolute Error (MAE) for force predictions.\n        *   **Datasets**: COLL, MD17, and OC20.\n        *   **Results**: GemNet significantly outperformed previous models:\n            *   34% improvement on COLL.\n            *   41% improvement on MD17.\n            *   20% improvement on OC20.\n        *   **Specific Strengths**: The largest improvements were observed for the most challenging molecules, particularly those exhibiting dynamic, non-planar geometries, indicating GemNet's superior ability to capture complex 3D structural information. Ablation studies confirmed that each proposed enhancement contributed significantly to reducing model error with minimal computational overhead.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The universality proof relies on the assumption that the set of relative vectors does not span a (d-1)-dimensional space (Theorem 3). The discretization scheme for spherical representations might not always provide a universal approximation guarantee depending on the specific mesh. The model's performance is tied to the interaction cutoff `c_emb`.\n    *   **Scope of Applicability**: Primarily focused on predicting energy and quantum mechanical properties of molecules, particularly for accelerating molecular dynamics simulations. Applicable to tasks requiring accurate representation of 3D molecular geometry and interactions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: GemNet significantly advances the technical state-of-the-art in molecular property prediction by providing a theoretically grounded and empirically superior GNN architecture. It resolves a long-standing theoretical limitation regarding GNN expressiveness for molecular data by proving the sufficiency of S2 representations.\n    *   **Potential Impact on Future Research**:\n        *   **Foundation for Equivariant GNNs**: The universality proof for S2 representations provides a more efficient theoretical foundation for designing equivariant GNNs, potentially leading to simpler yet equally powerful models.\n        *   **Improved Molecular Simulations**: The substantial performance gains, especially on challenging molecules, suggest GemNet can enable more accurate and faster *ab initio* molecular dynamics simulations, opening new avenues for drug discovery, materials science, and chemical engineering.\n        *   **Geometric Feature Learning**: The explicit and effective integration of dihedral angles and two-hop message passing highlights the importance of higher-order geometric features, encouraging further research into sophisticated geometric feature learning in GNNs.",
      "keywords": [
        "GemNet",
        "Universal Directional Graph Neural Networks",
        "Spherical (S2) representations universality",
        "Geometric Message Passing",
        "Two-hop message passing",
        "Dihedral angles integration",
        "Molecular property prediction",
        "Molecular dynamics simulations acceleration",
        "Equivariant GNNs",
        "State-of-the-art force predictions",
        "Ablation studies",
        "Directed edge embeddings",
        "Quantum mechanical properties"
      ],
      "paper_type": "the paper presents a new graph neural network (gnn) architecture called gemnet. it explicitly describes the development of this new system (\"we then discretize such gnns via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (gemnet)\"). this aligns directly with the \"technical\" classification criteria of presenting new methods, algorithms, or systems.\n\nwhile the paper also includes significant theoretical contributions (\"we show that gnns with spherical representations are indeed universal approximators for predictions...\" and \"proving sufficient conditions for universality...\"), these theoretical findings serve as the foundation and justification for the design of the new gemnet architecture. similarly, the empirical results (\"gemnet outperforms previous models on the coll, md17, and oc20 datasets...\") are used to validate the effectiveness of the proposed gemnet.\n\nin the context of machine learning conferences like neurips, papers that introduce a novel model or algorithm, supported by theoretical analysis and empirical validation, are typically classified as \"technical\" or \"methodology\" papers, as their primary contribution is the development of a new technique or system. the theoretical and empirical aspects are crucial components that support and validate the technical contribution.\n\nthe final classification is: **technical**"
    },
    "file_name": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf"
  },
  {
    "success": true,
    "doc_id": "76503d9cfbcf0b9b5d40d08f76524f34",
    "summary": "Here's a focused summary of the paper \"GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS\" by Dwivedi et al. for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses a major limitation of standard Graph Neural Networks (GNNs), particularly message-passing GNNs (MP-GNNs): their inability to distinguish isomorphic nodes and other graph symmetries due to the absence of canonical positional information \\cite{dwivedi2021af0}. This limits their representation power, as node representations primarily depend on local structure, making it difficult to differentiate nodes with identical local neighborhoods but distinct global positions or roles.\n    *   This problem is critical because it restricts the theoretical expressivity of MP-GNNs, bounding them by the 1-Weisfeiler-Leman (WL) test, leading to poor performance on graphs with symmetries \\cite{dwivedi2021af0}. Existing solutions like stacking layers (prone to over-squashing) or higher-order GNNs (computationally expensive) have their own drawbacks.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to introduce positional encoding (PE) in GNNs often concatenate PEs with input node features, similar to Transformers \\cite{dwivedi2021af0}.\n    *   **Limitations of previous solutions**:\n        *   **Laplacian Eigenvectors (LapPE)**: While forming a meaningful coordinate system, they suffer from sign ambiguity (2^k possible sign values for k eigenvectors) and potential instability due to eigenvalue multiplicities, requiring the network to learn invariance \\cite{dwivedi2021af0}.\n        *   **Learnable position-aware embeddings**: Some rely on random anchor sets, which can limit generalizability on inductive tasks \\cite{dwivedi2021af0}.\n        *   **Pre-computed substructure information**: Methods encoding prior information (e.g., rings for molecules) require pre-computation and can have high computational complexity (O(n^k) for k-tuple substructures) \\cite{dwivedi2021af0}.\n        *   **Transformer-based GNNs**: While addressing long-range interactions, many use non-learnable PEs or inject learned PEs based on Laplacian eigenvectors, inheriting their limitations \\cite{dwivedi2021af0}.\n    *   This work positions itself by proposing a novel framework that *decouples* structural and positional representations, allowing both to be learned simultaneously and adaptively, addressing the limitations of fixed or ambiguously defined PEs \\cite{dwivedi2021af0}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **LSPE (Learnable Structural and Positional Encodings)**, a generic architecture that decouples structural and positional representations. Instead of merely injecting PEs at the input, LSPE allows both structural node features (`h`) and positional features (`p`) to be updated independently through message-passing layers \\cite{dwivedi2021af0}.\n    *   The update equations for LSPE are:\n        *   `h^(l+1)_i = f_h([h^l_i; p^l_i], {[h^l_j; p^l_j] | j ∈ N(i)}, e^l_ij)`\n        *   `e^(l+1)_ij = f_e(h^l_i, h^l_j, e^l_ij)`\n        *   `p^(l+1)_i = f_p(p^l_i, {p^l_j | j ∈ N(i)}, e^l_ij)`\n        where `f_h`, `f_e`, `f_p` are learnable functions, and `[;]` denotes concatenation \\cite{dwivedi2021af0}.\n    *   **Novelty**:\n        *   **Decoupled Learning**: The key innovation is the explicit separation and simultaneous learning of structural and positional representations throughout the GNN layers, rather than just injecting static PEs at the input \\cite{dwivedi2021af0}. This allows the network to adaptively adjust positional information to the task.\n        *   **Random Walk Positional Encoding (RWPE)**: A novel initialization scheme for PEs based on the k-step random walk diffusion process, specifically using the landing probability of a node to itself (`RW^k_ii`). This avoids the sign ambiguity issues of LapPE and provides unique node representations for many real-world graphs \\cite{dwivedi2021af0}.\n        *   **Positional Loss**: The framework incorporates an optional `Loss_LapEig` term alongside the task loss, encouraging the learned PEs to form a coordinate system constrained by graph topology, similar to Laplacian eigenvectors \\cite{dwivedi2021af0}.\n\n*   **Key Technical Contributions**\n    *   **Novel Architecture (LSPE)**: A generic framework that enables GNNs to learn and update both structural and positional representations concurrently, enhancing expressivity while maintaining linear complexity \\cite{dwivedi2021af0}.\n    *   **Random Walk Positional Encoding (RWPE)**: A new, sign-ambiguity-free method for initializing positional embeddings, shown to be effective and outperform LapPE in experiments \\cite{dwivedi2021af0}.\n    *   **Positional Loss Integration**: The ability to incorporate a `Loss_LapEig` to guide the learning of positional embeddings, ensuring they capture meaningful graph topology \\cite{dwivedi2021af0}.\n    *   **Generalizability**: The LSPE framework is demonstrated to be applicable to various GNN architectures, including sparse MP-GNNs (GatedGCN, PNA) and fully-connected Transformer-based GNNs (SAN, GraphiT) \\cite{dwivedi2021af0}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed LSPE architecture was evaluated by instantiating it with both sparse MP-GNNs (GatedGCN-LSPE, PNA-LSPE) and Transformer-based GNNs (SAN-LSPE, GraphiT-LSPE) \\cite{dwivedi2021af0}.\n    *   **Datasets**: Experiments were performed on three standard molecular benchmarks (ZINC, QM9, PCQM4Mv2) and three non-molecular benchmarks (CSL, Pattern, Cluster) \\cite{dwivedi2021af0}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   LSPE consistently improved performance across various GNN classes, showing performance increases from 1.79% up to 64.14% on molecular datasets \\cite{dwivedi2021af0}.\n        *   On molecular benchmarks, LSPE instances surpassed previous state-of-the-art (SOTA) on one dataset (PCQM4Mv2) by a considerable margin (26.23%) and achieved SOTA-comparable scores on the other two \\cite{dwivedi2021af0}.\n        *   RWPE consistently outperformed LapPE, suggesting that learning sign invariance is more challenging than dealing with potential non-uniqueness for some nodes \\cite{dwivedi2021af0}.\n        *   Sparse MP-GNNs with LSPE were found to outperform fully-connected Transformer-based GNNs with LSPE, suggesting a path towards efficient yet powerful graph architectures \\cite{dwivedi2021af0}.\n\n*   **Limitations & Scope**\n    *   **RWPE Uniqueness Assumption**: While RWPE generally provides unique node representations for real-world graphs (e.g., ZINC molecules), it may not be unique for all nodes in synthetic strongly regular graphs (e.g., CSL graphs, where all nodes are isomorphic) \\cite{dwivedi2021af0}. However, it can still distinguish isomorphic graph classes.\n    *   **LapPE Sign Ambiguity**: The paper acknowledges that LapPE's inherent sign ambiguity (2^k possibilities) makes it harder for networks to learn, which is a limitation of using LapPE as an initial PE \\cite{dwivedi2021af0}.\n    *   **Scope of Applicability**: The framework is generic and applicable to any MP-GNN architecture, including sparse and fully-connected variants, making it broadly useful for various graph learning tasks \\cite{dwivedi2021af0}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: LSPE significantly enhances the expressivity and performance of GNNs by providing a principled way to learn and adapt positional information, addressing a fundamental limitation of MP-GNNs \\cite{dwivedi2021af0}.\n    *   **Improved Discriminative Power**: By decoupling and learning positional representations, GNNs can better differentiate between structurally similar but positionally distinct nodes, overcoming the 1-WL test limitations without resorting to computationally expensive higher-order methods \\cite{dwivedi2021af0}.\n    *   **Potential Impact**: This work opens new avenues for designing more powerful and efficient GNNs, particularly for tasks requiring fine-grained understanding of node roles and global graph structure (e.g., molecular property prediction, drug discovery). The finding that sparse MP-GNNs with LSPE can outperform fully-connected Transformer-GNNs suggests a promising direction for developing highly efficient yet expressive models \\cite{dwivedi2021af0}.",
    "intriguing_abstract": "Message-passing Graph Neural Networks (GNNs) are fundamentally limited by their inability to distinguish isomorphic nodes and capture global positional information, restricting their theoretical expressivity to the 1-Weisfeiler-Leman test. This bottleneck hinders their performance on graphs with inherent symmetries, a pervasive challenge in real-world applications.\n\nWe introduce **Learnable Structural and Positional Encodings (LSPE)**, a generic and groundbreaking framework that decouples and dynamically learns both structural and positional representations throughout the GNN layers. Unlike prior methods that inject static positional encodings, LSPE allows these crucial features to co-evolve, adaptively enhancing the network's understanding of graph topology. A key innovation is our **Random Walk Positional Encoding (RWPE)**, a novel, sign-ambiguity-free initialization that consistently outperforms traditional Laplacian-based approaches.\n\nLSPE significantly boosts GNN expressivity and performance, achieving state-of-the-art or comparable results across diverse molecular and non-molecular benchmarks, with performance gains up to 64%. Remarkably, sparse message-passing GNNs augmented with LSPE even surpass fully-connected graph transformers, demonstrating a path towards highly efficient yet powerful graph learning models. This work fundamentally advances GNN capabilities, opening new frontiers for complex tasks in areas like drug discovery and materials science.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Message-Passing GNNs (MP-GNNs)",
      "Positional Encoding",
      "Structural Representations",
      "Learnable Structural and Positional Encodings (LSPE)",
      "Decoupled Learning",
      "Random Walk Positional Encoding (RWPE)",
      "Laplacian Eigenvectors (LapPE)",
      "1-Weisfeiler-Leman (WL) test",
      "Positional Loss",
      "GNN Expressivity",
      "Sign Ambiguity",
      "Molecular Property Prediction",
      "Sparse MP-GNNs",
      "State-of-the-Art Performance"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/454304628bf10f02aba1c2cfc95891e94d09208e.pdf",
    "citation_key": "dwivedi2021af0",
    "metadata": {
      "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
      "authors": [
        "Vijay Prakash Dwivedi",
        "A. Luu",
        "T. Laurent",
        "Yoshua Bengio",
        "X. Bresson"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.",
      "file_path": "paper_data/Graph_Neural_Networks/454304628bf10f02aba1c2cfc95891e94d09208e.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS\" by Dwivedi et al. for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses a major limitation of standard Graph Neural Networks (GNNs), particularly message-passing GNNs (MP-GNNs): their inability to distinguish isomorphic nodes and other graph symmetries due to the absence of canonical positional information \\cite{dwivedi2021af0}. This limits their representation power, as node representations primarily depend on local structure, making it difficult to differentiate nodes with identical local neighborhoods but distinct global positions or roles.\n    *   This problem is critical because it restricts the theoretical expressivity of MP-GNNs, bounding them by the 1-Weisfeiler-Leman (WL) test, leading to poor performance on graphs with symmetries \\cite{dwivedi2021af0}. Existing solutions like stacking layers (prone to over-squashing) or higher-order GNNs (computationally expensive) have their own drawbacks.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to introduce positional encoding (PE) in GNNs often concatenate PEs with input node features, similar to Transformers \\cite{dwivedi2021af0}.\n    *   **Limitations of previous solutions**:\n        *   **Laplacian Eigenvectors (LapPE)**: While forming a meaningful coordinate system, they suffer from sign ambiguity (2^k possible sign values for k eigenvectors) and potential instability due to eigenvalue multiplicities, requiring the network to learn invariance \\cite{dwivedi2021af0}.\n        *   **Learnable position-aware embeddings**: Some rely on random anchor sets, which can limit generalizability on inductive tasks \\cite{dwivedi2021af0}.\n        *   **Pre-computed substructure information**: Methods encoding prior information (e.g., rings for molecules) require pre-computation and can have high computational complexity (O(n^k) for k-tuple substructures) \\cite{dwivedi2021af0}.\n        *   **Transformer-based GNNs**: While addressing long-range interactions, many use non-learnable PEs or inject learned PEs based on Laplacian eigenvectors, inheriting their limitations \\cite{dwivedi2021af0}.\n    *   This work positions itself by proposing a novel framework that *decouples* structural and positional representations, allowing both to be learned simultaneously and adaptively, addressing the limitations of fixed or ambiguously defined PEs \\cite{dwivedi2021af0}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **LSPE (Learnable Structural and Positional Encodings)**, a generic architecture that decouples structural and positional representations. Instead of merely injecting PEs at the input, LSPE allows both structural node features (`h`) and positional features (`p`) to be updated independently through message-passing layers \\cite{dwivedi2021af0}.\n    *   The update equations for LSPE are:\n        *   `h^(l+1)_i = f_h([h^l_i; p^l_i], {[h^l_j; p^l_j] | j ∈ N(i)}, e^l_ij)`\n        *   `e^(l+1)_ij = f_e(h^l_i, h^l_j, e^l_ij)`\n        *   `p^(l+1)_i = f_p(p^l_i, {p^l_j | j ∈ N(i)}, e^l_ij)`\n        where `f_h`, `f_e`, `f_p` are learnable functions, and `[;]` denotes concatenation \\cite{dwivedi2021af0}.\n    *   **Novelty**:\n        *   **Decoupled Learning**: The key innovation is the explicit separation and simultaneous learning of structural and positional representations throughout the GNN layers, rather than just injecting static PEs at the input \\cite{dwivedi2021af0}. This allows the network to adaptively adjust positional information to the task.\n        *   **Random Walk Positional Encoding (RWPE)**: A novel initialization scheme for PEs based on the k-step random walk diffusion process, specifically using the landing probability of a node to itself (`RW^k_ii`). This avoids the sign ambiguity issues of LapPE and provides unique node representations for many real-world graphs \\cite{dwivedi2021af0}.\n        *   **Positional Loss**: The framework incorporates an optional `Loss_LapEig` term alongside the task loss, encouraging the learned PEs to form a coordinate system constrained by graph topology, similar to Laplacian eigenvectors \\cite{dwivedi2021af0}.\n\n*   **Key Technical Contributions**\n    *   **Novel Architecture (LSPE)**: A generic framework that enables GNNs to learn and update both structural and positional representations concurrently, enhancing expressivity while maintaining linear complexity \\cite{dwivedi2021af0}.\n    *   **Random Walk Positional Encoding (RWPE)**: A new, sign-ambiguity-free method for initializing positional embeddings, shown to be effective and outperform LapPE in experiments \\cite{dwivedi2021af0}.\n    *   **Positional Loss Integration**: The ability to incorporate a `Loss_LapEig` to guide the learning of positional embeddings, ensuring they capture meaningful graph topology \\cite{dwivedi2021af0}.\n    *   **Generalizability**: The LSPE framework is demonstrated to be applicable to various GNN architectures, including sparse MP-GNNs (GatedGCN, PNA) and fully-connected Transformer-based GNNs (SAN, GraphiT) \\cite{dwivedi2021af0}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed LSPE architecture was evaluated by instantiating it with both sparse MP-GNNs (GatedGCN-LSPE, PNA-LSPE) and Transformer-based GNNs (SAN-LSPE, GraphiT-LSPE) \\cite{dwivedi2021af0}.\n    *   **Datasets**: Experiments were performed on three standard molecular benchmarks (ZINC, QM9, PCQM4Mv2) and three non-molecular benchmarks (CSL, Pattern, Cluster) \\cite{dwivedi2021af0}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   LSPE consistently improved performance across various GNN classes, showing performance increases from 1.79% up to 64.14% on molecular datasets \\cite{dwivedi2021af0}.\n        *   On molecular benchmarks, LSPE instances surpassed previous state-of-the-art (SOTA) on one dataset (PCQM4Mv2) by a considerable margin (26.23%) and achieved SOTA-comparable scores on the other two \\cite{dwivedi2021af0}.\n        *   RWPE consistently outperformed LapPE, suggesting that learning sign invariance is more challenging than dealing with potential non-uniqueness for some nodes \\cite{dwivedi2021af0}.\n        *   Sparse MP-GNNs with LSPE were found to outperform fully-connected Transformer-based GNNs with LSPE, suggesting a path towards efficient yet powerful graph architectures \\cite{dwivedi2021af0}.\n\n*   **Limitations & Scope**\n    *   **RWPE Uniqueness Assumption**: While RWPE generally provides unique node representations for real-world graphs (e.g., ZINC molecules), it may not be unique for all nodes in synthetic strongly regular graphs (e.g., CSL graphs, where all nodes are isomorphic) \\cite{dwivedi2021af0}. However, it can still distinguish isomorphic graph classes.\n    *   **LapPE Sign Ambiguity**: The paper acknowledges that LapPE's inherent sign ambiguity (2^k possibilities) makes it harder for networks to learn, which is a limitation of using LapPE as an initial PE \\cite{dwivedi2021af0}.\n    *   **Scope of Applicability**: The framework is generic and applicable to any MP-GNN architecture, including sparse and fully-connected variants, making it broadly useful for various graph learning tasks \\cite{dwivedi2021af0}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: LSPE significantly enhances the expressivity and performance of GNNs by providing a principled way to learn and adapt positional information, addressing a fundamental limitation of MP-GNNs \\cite{dwivedi2021af0}.\n    *   **Improved Discriminative Power**: By decoupling and learning positional representations, GNNs can better differentiate between structurally similar but positionally distinct nodes, overcoming the 1-WL test limitations without resorting to computationally expensive higher-order methods \\cite{dwivedi2021af0}.\n    *   **Potential Impact**: This work opens new avenues for designing more powerful and efficient GNNs, particularly for tasks requiring fine-grained understanding of node roles and global graph structure (e.g., molecular property prediction, drug discovery). The finding that sparse MP-GNNs with LSPE can outperform fully-connected Transformer-GNNs suggests a promising direction for developing highly efficient yet expressive models \\cite{dwivedi2021af0}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Message-Passing GNNs (MP-GNNs)",
        "Positional Encoding",
        "Structural Representations",
        "Learnable Structural and Positional Encodings (LSPE)",
        "Decoupled Learning",
        "Random Walk Positional Encoding (RWPE)",
        "Laplacian Eigenvectors (LapPE)",
        "1-Weisfeiler-Leman (WL) test",
        "Positional Loss",
        "GNN Expressivity",
        "Sign Ambiguity",
        "Molecular Property Prediction",
        "Sparse MP-GNNs",
        "State-of-the-Art Performance"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this work, we **propose** to decouple structural and positional representations...\" and \"we **introduce a novel generic architecture** which we call lspe (learnable structural and positional encodings).\"\n*   the introduction reiterates this: \"we **propose** to decouple structural and positional representations...\" and \"we **introduce a novel generic architecture** which we call lspe...\"\n*   it discusses a technical problem (absence of canonical positional information in gnns) and presents a proposed solution (lspe).\n*   it also mentions investigating different gnns and observing performance increases, indicating an empirical evaluation of the *proposed method*.\n\nthese phrases strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it includes empirical findings, these are presented as evidence for the effectiveness of the *new architecture/method* being proposed, making it primarily a technical contribution.\n\n**classification: technical**"
    },
    "file_name": "454304628bf10f02aba1c2cfc95891e94d09208e.pdf"
  },
  {
    "success": true,
    "doc_id": "890f10220eef1094cd402b25424bfe85",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the fundamental limitation of most popular Graph Neural Networks (GNNs), which rely on 1-hop message passing and are bounded in expressive power by the 1-dimensional Weisfeiler-Lehman (1-WL) test \\cite{feng20225sa}. This means they cannot distinguish many non-isomorphic graph structures.\n    *   While K-hop message passing (aggregating information from K-hop neighbors) has been proposed, there was no theoretical work characterizing its expressive power, leaving a gap in understanding its capabilities and limitations \\cite{feng20225sa}.\n\n*   **Related Work & Positioning**\n    *   Existing GNNs primarily use 1-hop message passing, which is known to be limited by the 1-WL test \\cite{feng20225sa}.\n    *   Previous works extended to K-hop message passing (e.g., GPR-GNN, MixHop, GINE+, Graphormer) but often misused or interchanged different definitions of K-hop kernels (shortest path distance vs. graph diffusion) without theoretical analysis of their distinct expressive powers \\cite{feng20225sa}.\n    *   The paper positions itself as the first to theoretically characterize the expressive power of K-hop message passing, differentiate its kernels, identify its limitations, and propose an enhancement.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach involves a rigorous theoretical characterization of K-hop message passing GNNs.\n    *   **Differentiation of K-hop Kernels**: Formally distinguishes two kernels for K-hop neighbors: shortest path distance (spd) and graph diffusion (gd) \\cite{feng20225sa}.\n    *   **Expressive Power Analysis**: Proves that K-hop message passing is strictly more powerful than 1-WL and can distinguish almost all regular graphs, but also shows it's bounded by the 3-WL test and fails on some simple regular graphs \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Introduces the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework. This novel approach enhances K-hop message passing by incorporating information from the \"peripheral subgraph\" (subgraph induced by neighbors in a specific hop) in addition to individual neighbor features \\cite{feng20225sa}. This allows the model to learn more expressive local structural features.\n\n*   **Key Technical Contributions**\n    *   **Formal Kernel Definitions**: Provides formal definitions and differentiation of shortest path distance (spd) and graph diffusion (gd) kernels for K-hop neighbors, clarifying their distinct impacts on expressive power \\cite{feng20225sa}.\n    *   **Theoretical Expressive Power Bounds**:\n        *   Proves that K-hop message passing (with K > 1) is strictly more powerful than 1-hop message passing and the 1-WL test \\cite{feng20225sa}.\n        *   Demonstrates that K-hop GNNs can distinguish almost all regular graphs with a modest K \\cite{feng20225sa}.\n        *   Establishes that the expressive power of K-hop message passing, regardless of the kernel, is bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Proposes KP-GNN, which enhances K-hop message passing by integrating peripheral subgraph information (edges within a hop's neighbors). This framework significantly improves expressive power, enabling it to distinguish many distance regular graphs that even 3-WL or previous distance encoding methods could not \\cite{feng20225sa}.\n    *   **Flexibility and Efficiency**: KP-GNN can be applied to most existing K-hop GNNs with minor modifications and adds only little computational complexity \\cite{feng20225sa}.\n\n*   **Experimental Validation**\n    *   The paper states that experimental results verify the expressive power and effectiveness of KP-GNN \\cite{feng20225sa}.\n    *   KP-GNN achieves competitive results across all benchmark datasets, demonstrating its practical utility \\cite{feng20225sa}. (Specific datasets or metrics are not detailed in the provided abstract/introduction).\n\n*   **Limitations & Scope**\n    *   **K-hop Limitations**: The choice of K-hop kernel (spd vs. gd) affects expressive power, and neither can distinguish all simple non-isomorphic structures \\cite{feng20225sa}. The expressive power of K-hop message passing is fundamentally bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **Scope**: The analysis primarily focuses on distinguishing non-isomorphic graphs and node configurations based on local structural information, assuming uniform node features for theoretical clarity.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing the first comprehensive theoretical characterization of K-hop message passing GNNs \\cite{feng20225sa}.\n    *   It clarifies the impact of different K-hop kernels and establishes precise bounds on their expressive power, offering crucial insights for GNN design \\cite{feng20225sa}.\n    *   The introduction of KP-GNN provides a practical and theoretically grounded method to overcome the limitations of standard K-hop GNNs, paving the way for more powerful and discriminative GNN architectures in future research \\cite{feng20225sa}.",
    "intriguing_abstract": "The quest for more powerful Graph Neural Networks (GNNs) is often hampered by the fundamental expressive limitations of 1-hop message passing, bounded by the 1-Weisfeiler-Lehman (1-WL) test. While K-hop message passing offers an intuitive path to richer structural understanding, its theoretical expressive power and the distinct impacts of various K-hop kernels have remained largely uncharacterized—until now.\n\nThis paper presents the first comprehensive theoretical framework for K-hop message passing GNNs. We rigorously differentiate between shortest path distance (spd) and graph diffusion (gd) kernels, proving that K-hop GNNs are strictly more powerful than 1-WL and can distinguish almost all regular graphs, yet are ultimately bounded by the 3-WL test. To overcome these inherent limitations, we introduce the novel K-hop Peripheral-subgraph-enhanced GNN (KP-GNN) framework. By ingeniously incorporating information from peripheral subgraphs—the intricate structures formed by K-hop neighbors—KP-GNN dramatically boosts expressive power, enabling it to distinguish many distance regular graphs that even 3-WL or previous methods fail to resolve. This flexible and efficient framework offers a critical advancement, paving the way for GNNs with unprecedented discriminative capabilities for complex graph structures.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "K-hop message passing",
      "expressive power",
      "Weisfeiler-Lehman (WL) test",
      "theoretical characterization",
      "K-hop kernels",
      "shortest path distance kernel",
      "graph diffusion kernel",
      "KP-GNN framework",
      "peripheral subgraph enhancement",
      "expressive power bounds",
      "distance regular graphs",
      "GNN architecture design"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/caf8927cf3c872698a0e97591a1205ba577bbba5.pdf",
    "citation_key": "feng20225sa",
    "metadata": {
      "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
      "authors": [
        "Jiarui Feng",
        "Yixin Chen",
        "Fuhai Li",
        "Anindya Sarkar",
        "Muhan Zhang"
      ],
      "published_date": "2022",
      "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing -- aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/caf8927cf3c872698a0e97591a1205ba577bbba5.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the fundamental limitation of most popular Graph Neural Networks (GNNs), which rely on 1-hop message passing and are bounded in expressive power by the 1-dimensional Weisfeiler-Lehman (1-WL) test \\cite{feng20225sa}. This means they cannot distinguish many non-isomorphic graph structures.\n    *   While K-hop message passing (aggregating information from K-hop neighbors) has been proposed, there was no theoretical work characterizing its expressive power, leaving a gap in understanding its capabilities and limitations \\cite{feng20225sa}.\n\n*   **Related Work & Positioning**\n    *   Existing GNNs primarily use 1-hop message passing, which is known to be limited by the 1-WL test \\cite{feng20225sa}.\n    *   Previous works extended to K-hop message passing (e.g., GPR-GNN, MixHop, GINE+, Graphormer) but often misused or interchanged different definitions of K-hop kernels (shortest path distance vs. graph diffusion) without theoretical analysis of their distinct expressive powers \\cite{feng20225sa}.\n    *   The paper positions itself as the first to theoretically characterize the expressive power of K-hop message passing, differentiate its kernels, identify its limitations, and propose an enhancement.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach involves a rigorous theoretical characterization of K-hop message passing GNNs.\n    *   **Differentiation of K-hop Kernels**: Formally distinguishes two kernels for K-hop neighbors: shortest path distance (spd) and graph diffusion (gd) \\cite{feng20225sa}.\n    *   **Expressive Power Analysis**: Proves that K-hop message passing is strictly more powerful than 1-WL and can distinguish almost all regular graphs, but also shows it's bounded by the 3-WL test and fails on some simple regular graphs \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Introduces the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework. This novel approach enhances K-hop message passing by incorporating information from the \"peripheral subgraph\" (subgraph induced by neighbors in a specific hop) in addition to individual neighbor features \\cite{feng20225sa}. This allows the model to learn more expressive local structural features.\n\n*   **Key Technical Contributions**\n    *   **Formal Kernel Definitions**: Provides formal definitions and differentiation of shortest path distance (spd) and graph diffusion (gd) kernels for K-hop neighbors, clarifying their distinct impacts on expressive power \\cite{feng20225sa}.\n    *   **Theoretical Expressive Power Bounds**:\n        *   Proves that K-hop message passing (with K > 1) is strictly more powerful than 1-hop message passing and the 1-WL test \\cite{feng20225sa}.\n        *   Demonstrates that K-hop GNNs can distinguish almost all regular graphs with a modest K \\cite{feng20225sa}.\n        *   Establishes that the expressive power of K-hop message passing, regardless of the kernel, is bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Proposes KP-GNN, which enhances K-hop message passing by integrating peripheral subgraph information (edges within a hop's neighbors). This framework significantly improves expressive power, enabling it to distinguish many distance regular graphs that even 3-WL or previous distance encoding methods could not \\cite{feng20225sa}.\n    *   **Flexibility and Efficiency**: KP-GNN can be applied to most existing K-hop GNNs with minor modifications and adds only little computational complexity \\cite{feng20225sa}.\n\n*   **Experimental Validation**\n    *   The paper states that experimental results verify the expressive power and effectiveness of KP-GNN \\cite{feng20225sa}.\n    *   KP-GNN achieves competitive results across all benchmark datasets, demonstrating its practical utility \\cite{feng20225sa}. (Specific datasets or metrics are not detailed in the provided abstract/introduction).\n\n*   **Limitations & Scope**\n    *   **K-hop Limitations**: The choice of K-hop kernel (spd vs. gd) affects expressive power, and neither can distinguish all simple non-isomorphic structures \\cite{feng20225sa}. The expressive power of K-hop message passing is fundamentally bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **Scope**: The analysis primarily focuses on distinguishing non-isomorphic graphs and node configurations based on local structural information, assuming uniform node features for theoretical clarity.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing the first comprehensive theoretical characterization of K-hop message passing GNNs \\cite{feng20225sa}.\n    *   It clarifies the impact of different K-hop kernels and establishes precise bounds on their expressive power, offering crucial insights for GNN design \\cite{feng20225sa}.\n    *   The introduction of KP-GNN provides a practical and theoretically grounded method to overcome the limitations of standard K-hop GNNs, paving the way for more powerful and discriminative GNN architectures in future research \\cite{feng20225sa}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "K-hop message passing",
        "expressive power",
        "Weisfeiler-Lehman (WL) test",
        "theoretical characterization",
        "K-hop kernels",
        "shortest path distance kernel",
        "graph diffusion kernel",
        "KP-GNN framework",
        "peripheral subgraph enhancement",
        "expressive power bounds",
        "distance regular graphs",
        "GNN architecture design"
      ],
      "paper_type": "based on the abstract and introduction, this paper primarily falls under the **theoretical** type.\n\nhere's why:\n\n1.  **explicit theoretical focus:** both the abstract and introduction explicitly state the paper's main goal is to \"theoretically characterize the expressive power of k-hop message passing.\"\n2.  **key theoretical concepts:** the paper discusses \"expressive power,\" \"weisfeiler-lehman (1-wl) test,\" \"3-wl,\" \"formally differentiate,\" and \"bounded by,\" which are all strong indicators of a theoretical analysis of computational models.\n3.  **mathematical/formal analysis:** the language \"formally differentiate two different kernels\" and \"characterize the expressive power by showing that it is more powerful than 1-wl and can distinguish almost all regular graphs\" points to formal mathematical analysis and proofs (even if not explicitly using the word \"prove\" in the snippet, the characterization implies it).\n4.  **new method as a consequence:** while the paper introduces a \"kp-gnn framework\" and mentions \"experimental results,\" these appear to be a direct consequence of and a solution to the theoretical limitations identified. the kp-gnn is introduced \"to further enhance its expressive power,\" building upon the theoretical characterization. the empirical results \"verify the expressive power and effectiveness,\" serving to validate the theoretical claims.\n\nthe core contribution, as stated, is the theoretical understanding of k-hop message passing. the technical and empirical aspects support and demonstrate the implications of this theoretical work."
    },
    "file_name": "caf8927cf3c872698a0e97591a1205ba577bbba5.pdf"
  },
  {
    "success": true,
    "doc_id": "578386c4b52bbf8d40ae54ca2ca0e7e2",
    "summary": "Here's a focused summary of the paper \"E(n) Equivariant Graph Neural Networks\" \\cite{satorras2021pzl} for a literature review:\n\n---\n\n### E(n) Equivariant Graph Neural Networks \\cite{satorras2021pzl}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Developing Graph Neural Networks (GNNs) that are equivariant to Euclidean transformations (rotations, translations, reflections, and permutations) in n-dimensional spaces, particularly for tasks involving 3D structures (e.g., molecules, point clouds, N-body systems).\n    *   **Importance & Challenge**: Many physical and geometric problems exhibit these symmetries, and enforcing equivariance acts as a powerful inductive bias, improving model generalization and data efficiency. Existing methods often rely on computationally expensive higher-order representations (e.g., spherical harmonics) and are typically limited to 3-dimensional spaces, making them less scalable and efficient.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Higher-order representations**: Methods like Tensor Field Networks (TFN) \\cite{satorras2021pzl} and SE(3) Transformer \\cite{satorras2021pzl} use spherical harmonics to handle transformations, allowing for higher-order representations.\n        *   **Lie Algebra methods**: Parametrize transformations by mapping kernels on the Lie Algebra \\cite{satorras2021pzl}.\n        *   **Radial Field**: An E(n) equivariant network for 3D point clouds, but limited to positional data without propagating node features \\cite{satorras2021pzl}.\n        *   **E(n)-invariant GNNs**: Methods like Schnet \\cite{satorras2021pzl} achieve E(n) invariance by using relative distances, but do not maintain equivariance for vector outputs.\n    *   **Limitations of Previous Solutions**:\n        *   **Computational expense**: Higher-order representations (e.g., spherical harmonics) require costly computations.\n        *   **Dimensionality restriction**: Many methods are limited to 3-dimensional spaces (E(3) or SE(3) equivariance).\n        *   **Scope**: Some methods only handle positional data or achieve invariance rather than full equivariance for vector quantities.\n    *   **Positioning**: The proposed E(n)-Equivariant Graph Neural Network (EGNN) offers a simpler architecture that avoids spherical harmonics, scales to higher dimensions, and achieves competitive or superior performance compared to existing methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method (Equivariant Graph Convolutional Layer - EGCL)**:\n        *   **Node Features & Coordinates**: Operates on both node embeddings ($h_i$) and n-dimensional coordinates ($x_i$).\n        *   **Edge Operation (Eq. 3)**: Computes messages ($m_{ij}$) by taking as input the node embeddings ($h_i, h_j$), edge attributes ($a_{ij}$), and crucially, the *squared relative distance* between coordinates ($||x_i - x_j||^2$). This ensures E(n) invariance for messages.\n        *   **Coordinate Update (Eq. 4)**: Updates the position of each particle ($x_i$) by adding a weighted sum of relative differences ($x_i - x_j$) from neighbors. The weights are derived from the edge embeddings ($m_{ij}$) via a scalar function $\\phi_x$. This operation is key to preserving E(n) equivariance for coordinates.\n        *   **Node Embedding Update (Eqs. 5 & 6)**: Aggregates messages ($m_i$) and updates node embeddings ($h_i$) in a standard GNN fashion, ensuring $h_i$ remains E(n) invariant.\n    *   **Novelty**:\n        *   **Direct Coordinate Update**: Integrates coordinate updates directly into the message-passing framework using relative differences, which naturally preserves E(n) equivariance without complex transformations.\n        *   **Avoidance of Higher-Order Representations**: Does not require spherical harmonics or other computationally expensive higher-order representations, simplifying the model.\n        *   **Scalability to N-dimensions**: The formulation is inherently applicable to arbitrary n-dimensional spaces, unlike many prior E(3)-specific methods.\n        *   **Simplicity and Flexibility**: Achieves strong equivariance with a relatively simple set of equations, allowing the edge embedding to carry rich information.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the E(n)-Equivariant Graph Neural Network (EGNN) architecture, specifically the Equivariant Graph Convolutional Layer (EGCL).\n    *   **Efficiency & Scalability**: A method for achieving E(n) equivariance that avoids computationally expensive higher-order representations and is easily scaled to higher-dimensional spaces.\n    *   **System Design**: A message-passing framework that simultaneously updates both invariant node features and equivariant coordinates, with information exchange between them.\n    *   **Extensions**:\n        *   A variant for explicitly tracking and updating particle velocities, useful for dynamical systems.\n        *   A mechanism for inferring graph edges when not explicitly provided, by learning soft edge values.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Dynamical Systems Modeling (N-body system)**: Forecasting particle positions in a 3D charged N-body system over 1000 timesteps.\n        *   (Mentioned in abstract but not detailed in provided text: Representation learning in graph autoencoders, predicting molecular properties on QM9 dataset).\n    *   **Key Performance Metrics**: Mean Squared Error (MSE) for position prediction, and average forward pass time.\n    *   **Comparison Results (N-body system)**:\n        *   **EGNN achieved the lowest MSE (0.0071)**, significantly outperforming:\n            *   Linear baseline (0.0819)\n            *   SE(3) Transformer (0.0244)\n            *   Tensor Field Network (0.0155)\n            *   Graph Neural Network (non-equivariant) (0.0107)\n            *   Radial Field (0.0104)\n        *   **Forward Pass Time**: EGNN had a competitive forward pass time (0.0062s), faster than SE(3) Transformer (0.1346s) and TFN (0.0343s), and comparable to GNN (0.0032s) and Radial Field (0.0039s).\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The proof of equivariance relies on the assumption that initial node features ($h_l$) are E(n) invariant for the subsequent $h_{l+1}$ to remain E(n) invariant.\n    *   **General GNN Scaling**: While the core EGNN mechanism scales to higher dimensions, the general challenge of scaling fully connected graphs to very large point clouds (due to message aggregation) remains, though the paper proposes an edge inference mechanism to address this.\n    *   **Scope of Applicability**: Primarily designed for tasks where E(n) equivariance is desired for both scalar (type-0) and vector (type-1) representations, such as physical simulations, molecular dynamics, and 3D geometric learning.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a more efficient, simpler, and scalable approach to E(n) equivariant GNNs compared to methods relying on complex higher-order representations or being restricted to 3D.\n    *   **Improved Performance**: Demonstrates superior or competitive performance on challenging equivariant tasks like N-body simulations.\n    *   **Potential Impact**: Offers a robust and practical framework for developing deep learning models in fields requiring geometric reasoning and symmetry exploitation, including physics, chemistry, materials science, and robotics, by enabling more accurate and generalizable predictions.",
    "intriguing_abstract": "Unlocking the full potential of Graph Neural Networks (GNNs) for physical and geometric problems hinges on effectively encoding Euclidean symmetries. Current E(n) equivariant GNNs often rely on computationally expensive higher-order representations like spherical harmonics, limiting their scalability and applicability beyond 3D. We introduce the E(n)-Equivariant Graph Neural Network (EGNN), a novel message-passing architecture that elegantly addresses these challenges.\n\nOur EGNN achieves full E(n) equivariance through a remarkably simple yet powerful mechanism: direct coordinate updates based on weighted relative differences, coupled with invariant node feature updates. By leveraging only squared relative distances and avoiding complex higher-order tensors, EGNN offers unprecedented computational efficiency and scales seamlessly to arbitrary n-dimensional spaces. Experiments demonstrate superior performance on challenging tasks like N-body system dynamics and competitive results on molecular property prediction (QM9), significantly outperforming state-of-the-art methods while being substantially faster. This work provides a robust, generalizable, and highly efficient framework, poised to accelerate discovery in fields ranging from materials science and chemistry to robotics and physics, by enabling more accurate and data-efficient geometric deep learning.",
    "keywords": [
      "E(n) Equivariant Graph Neural Networks (EGNN)",
      "Euclidean equivariance",
      "message-passing framework",
      "direct coordinate update",
      "avoidance of higher-order representations",
      "scalability to N-dimensions",
      "N-body systems modeling",
      "molecular properties prediction",
      "computational efficiency",
      "superior performance",
      "inductive bias",
      "relative distance",
      "Equivariant Graph Convolutional Layer (EGCL)",
      "geometric reasoning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf",
    "citation_key": "satorras2021pzl",
    "metadata": {
      "title": "E(n) Equivariant Graph Neural Networks",
      "authors": [
        "Victor Garcia Satorras",
        "Emiel Hoogeboom",
        "M. Welling"
      ],
      "published_date": "2021",
      "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",
      "file_path": "paper_data/Graph_Neural_Networks/8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"E(n) Equivariant Graph Neural Networks\" \\cite{satorras2021pzl} for a literature review:\n\n---\n\n### E(n) Equivariant Graph Neural Networks \\cite{satorras2021pzl}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Developing Graph Neural Networks (GNNs) that are equivariant to Euclidean transformations (rotations, translations, reflections, and permutations) in n-dimensional spaces, particularly for tasks involving 3D structures (e.g., molecules, point clouds, N-body systems).\n    *   **Importance & Challenge**: Many physical and geometric problems exhibit these symmetries, and enforcing equivariance acts as a powerful inductive bias, improving model generalization and data efficiency. Existing methods often rely on computationally expensive higher-order representations (e.g., spherical harmonics) and are typically limited to 3-dimensional spaces, making them less scalable and efficient.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Higher-order representations**: Methods like Tensor Field Networks (TFN) \\cite{satorras2021pzl} and SE(3) Transformer \\cite{satorras2021pzl} use spherical harmonics to handle transformations, allowing for higher-order representations.\n        *   **Lie Algebra methods**: Parametrize transformations by mapping kernels on the Lie Algebra \\cite{satorras2021pzl}.\n        *   **Radial Field**: An E(n) equivariant network for 3D point clouds, but limited to positional data without propagating node features \\cite{satorras2021pzl}.\n        *   **E(n)-invariant GNNs**: Methods like Schnet \\cite{satorras2021pzl} achieve E(n) invariance by using relative distances, but do not maintain equivariance for vector outputs.\n    *   **Limitations of Previous Solutions**:\n        *   **Computational expense**: Higher-order representations (e.g., spherical harmonics) require costly computations.\n        *   **Dimensionality restriction**: Many methods are limited to 3-dimensional spaces (E(3) or SE(3) equivariance).\n        *   **Scope**: Some methods only handle positional data or achieve invariance rather than full equivariance for vector quantities.\n    *   **Positioning**: The proposed E(n)-Equivariant Graph Neural Network (EGNN) offers a simpler architecture that avoids spherical harmonics, scales to higher dimensions, and achieves competitive or superior performance compared to existing methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method (Equivariant Graph Convolutional Layer - EGCL)**:\n        *   **Node Features & Coordinates**: Operates on both node embeddings ($h_i$) and n-dimensional coordinates ($x_i$).\n        *   **Edge Operation (Eq. 3)**: Computes messages ($m_{ij}$) by taking as input the node embeddings ($h_i, h_j$), edge attributes ($a_{ij}$), and crucially, the *squared relative distance* between coordinates ($||x_i - x_j||^2$). This ensures E(n) invariance for messages.\n        *   **Coordinate Update (Eq. 4)**: Updates the position of each particle ($x_i$) by adding a weighted sum of relative differences ($x_i - x_j$) from neighbors. The weights are derived from the edge embeddings ($m_{ij}$) via a scalar function $\\phi_x$. This operation is key to preserving E(n) equivariance for coordinates.\n        *   **Node Embedding Update (Eqs. 5 & 6)**: Aggregates messages ($m_i$) and updates node embeddings ($h_i$) in a standard GNN fashion, ensuring $h_i$ remains E(n) invariant.\n    *   **Novelty**:\n        *   **Direct Coordinate Update**: Integrates coordinate updates directly into the message-passing framework using relative differences, which naturally preserves E(n) equivariance without complex transformations.\n        *   **Avoidance of Higher-Order Representations**: Does not require spherical harmonics or other computationally expensive higher-order representations, simplifying the model.\n        *   **Scalability to N-dimensions**: The formulation is inherently applicable to arbitrary n-dimensional spaces, unlike many prior E(3)-specific methods.\n        *   **Simplicity and Flexibility**: Achieves strong equivariance with a relatively simple set of equations, allowing the edge embedding to carry rich information.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the E(n)-Equivariant Graph Neural Network (EGNN) architecture, specifically the Equivariant Graph Convolutional Layer (EGCL).\n    *   **Efficiency & Scalability**: A method for achieving E(n) equivariance that avoids computationally expensive higher-order representations and is easily scaled to higher-dimensional spaces.\n    *   **System Design**: A message-passing framework that simultaneously updates both invariant node features and equivariant coordinates, with information exchange between them.\n    *   **Extensions**:\n        *   A variant for explicitly tracking and updating particle velocities, useful for dynamical systems.\n        *   A mechanism for inferring graph edges when not explicitly provided, by learning soft edge values.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Dynamical Systems Modeling (N-body system)**: Forecasting particle positions in a 3D charged N-body system over 1000 timesteps.\n        *   (Mentioned in abstract but not detailed in provided text: Representation learning in graph autoencoders, predicting molecular properties on QM9 dataset).\n    *   **Key Performance Metrics**: Mean Squared Error (MSE) for position prediction, and average forward pass time.\n    *   **Comparison Results (N-body system)**:\n        *   **EGNN achieved the lowest MSE (0.0071)**, significantly outperforming:\n            *   Linear baseline (0.0819)\n            *   SE(3) Transformer (0.0244)\n            *   Tensor Field Network (0.0155)\n            *   Graph Neural Network (non-equivariant) (0.0107)\n            *   Radial Field (0.0104)\n        *   **Forward Pass Time**: EGNN had a competitive forward pass time (0.0062s), faster than SE(3) Transformer (0.1346s) and TFN (0.0343s), and comparable to GNN (0.0032s) and Radial Field (0.0039s).\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The proof of equivariance relies on the assumption that initial node features ($h_l$) are E(n) invariant for the subsequent $h_{l+1}$ to remain E(n) invariant.\n    *   **General GNN Scaling**: While the core EGNN mechanism scales to higher dimensions, the general challenge of scaling fully connected graphs to very large point clouds (due to message aggregation) remains, though the paper proposes an edge inference mechanism to address this.\n    *   **Scope of Applicability**: Primarily designed for tasks where E(n) equivariance is desired for both scalar (type-0) and vector (type-1) representations, such as physical simulations, molecular dynamics, and 3D geometric learning.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a more efficient, simpler, and scalable approach to E(n) equivariant GNNs compared to methods relying on complex higher-order representations or being restricted to 3D.\n    *   **Improved Performance**: Demonstrates superior or competitive performance on challenging equivariant tasks like N-body simulations.\n    *   **Potential Impact**: Offers a robust and practical framework for developing deep learning models in fields requiring geometric reasoning and symmetry exploitation, including physics, chemistry, materials science, and robotics, by enabling more accurate and generalizable predictions.",
      "keywords": [
        "E(n) Equivariant Graph Neural Networks (EGNN)",
        "Euclidean equivariance",
        "message-passing framework",
        "direct coordinate update",
        "avoidance of higher-order representations",
        "scalability to N-dimensions",
        "N-body systems modeling",
        "molecular properties prediction",
        "computational efficiency",
        "superior performance",
        "inductive bias",
        "relative distance",
        "Equivariant Graph Convolutional Layer (EGCL)",
        "geometric reasoning"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly states: \"this paper introduces a new model to learn graph neural networks... called e(n)-equivariant graph neural networks (egnns).\" it then contrasts this new model with existing methods and highlights its advantages and scalability. finally, it mentions demonstrating \"the effectiveness of our method\" on various applications.\n*   the **introduction** sets the stage by discussing the importance of exploiting symmetry and enforcing equivariance in deep neural networks, leading up to the problem that the new model addresses.\n\nthese phrases strongly align with the criteria for a **technical** paper:\n*   abstract mentions: \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" (specifically \"introduces a new model\" and \"our method\").\n*   introduction discusses: technical problem, proposed solution.\n\ntherefore, the paper is a **technical** paper."
    },
    "file_name": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf"
  },
  {
    "success": true,
    "doc_id": "06b72380f7586607c0ade3d0b181b27a",
    "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit a \"performance disparity\" on real-world graphs, performing well on nodes that align with the graph's dominant structural pattern (e.g., homophilic nodes in a homophilic graph) but struggling on nodes exhibiting the opposite pattern (e.g., heterophilic nodes in a homophilic graph) \\cite{mao202313j}. This disparity arises because real-world graphs inherently possess \"structural disparity,\" meaning they are a mixture of both homophilic and heterophilic node patterns.\n    *   **Importance & Challenge**: Existing GNN research often focuses on overall graph performance, overlooking the nuanced behavior of GNNs on these diverse node subgroups. Understanding this performance disparity is critical for developing GNNs that are robust and effective across all node types in complex, real-world graph structures \\cite{mao202313j}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior studies on GNN effectiveness primarily analyze performance on the entire graph and typically focus on graphs that are predominantly either homophilic or heterophilic \\cite{mao202313j}.\n    *   **Limitations of previous solutions**: These approaches fail to account for the common real-world scenario where graphs contain a mixture of homophilic and heterophilic patterns. They do not provide insights into GNN performance on specific node subgroups, potentially masking scenarios where GNNs perform poorly on minority patterns despite achieving good overall results \\cite{mao202313j}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Empirical Disparity Analysis**: Systematically investigates GNN performance by categorizing testing nodes based on their local homophily ratios, comparing a vanilla GNN (GCN) against MLP-based models (vanilla MLP and GLNN) \\cite{mao202313j}.\n        *   **Theoretical Analysis of Aggregation**: Examines how the GNN aggregation mechanism differentially impacts nodes with varying structural patterns. This involves introducing a novel graph generation model, CSBM-Structure (CSBM-S), and deriving theoretical insights into aggregated feature distances and class probabilities \\cite{mao202313j}.\n        *   **Generalization Bound**: Develops a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs to formally explain the underlying causes of the observed performance disparity \\cite{mao202313j}.\n        *   **Discriminative Ability Quantification**: Empirically analyzes the discriminative ability of GNNs on majority versus minority nodes using a \"relative discriminative ratio\" based on class prototypes \\cite{mao202313j}.\n    *   **Novelty/Difference**:\n        *   **First Rigorous Disparity Analysis**: This work is novel in its focused and systematic investigation of GNN performance on *subgroups* of nodes defined by their local structural patterns (homophilic vs. heterophilic), moving beyond aggregate performance metrics \\cite{mao202313j}.\n        *   **Novel CSBM-Structure (CSBM-S) Model**: Introduces a new variant of the Contextual Stochastic Block Model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes within a single graph, providing a more realistic testbed for structural disparity \\cite{mao202313j}.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound**: Presents a novel theoretical framework that precisely identifies aggregated feature distance and homophily ratio differences between training and testing nodes as critical factors driving performance disparity \\cite{mao202313j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Framework**: A non-i.i.d PAC-Bayesian generalization bound for GNNs that rigorously explains performance disparity by identifying aggregated feature distance and homophily ratio differences between training and testing nodes as key factors \\cite{mao202313j}.\n    *   **Novel Graph Generation Model**: Introduction of CSBM-Structure (CSBM-S), a variant of CSBM, which enables controlled experimental studies of graphs exhibiting mixed homophilic and heterophilic patterns \\cite{mao202313j}.\n    *   **Theoretical Insights into Aggregation**: Demonstrates that GNN aggregation creates a significant feature distance between homophilic and heterophilic node subgroups *within the same class*, and that differences in homophily ratio directly influence class prediction probabilities \\cite{mao202313j}.\n    *   **Practical Implications**: Provides insights into the effectiveness of deeper GNNs and identifies an overlooked distribution shift factor, leading to a new scenario for the graph out-of-distribution problem \\cite{mao202313j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Structural Disparity Verification**: Analyzed node homophily ratio distributions on real-world homophilic (Ogbn-arxiv, Pubmed) and heterophilic (Chameleon, Squirrel) datasets, confirming the pervasive existence of mixed structural patterns \\cite{mao202313j}.\n        *   **Subgroup Performance Comparison**: Conducted experiments comparing the accuracy of GCN against vanilla MLP and GLNN on test nodes, specifically grouped by their homophily ratio ranges \\cite{mao202313j}.\n        *   **Discriminative Ratio Analysis**: Empirically calculated a \"relative discriminative ratio\" to quantify the ease of prediction for majority versus minority nodes, observing its variation with the number of aggregation layers \\cite{mao202313j}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Accuracy Differences**: Demonstrated that in homophilic graphs, MLP-based models often outperform GCN on heterophilic (minority) nodes, while GCN performs better on homophilic (majority) nodes. Conversely, in heterophilic graphs, MLP models frequently outperform GCN on homophilic (minority) nodes, with GCN excelling on heterophilic (majority) nodes \\cite{mao202313j}.\n        *   **Discriminative Ratio**: The analysis aimed to illustrate how aggregation influences the discriminative ability, with a lower ratio indicating that majority nodes are easier to predict than minority nodes \\cite{mao202313j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical analysis (e.g., Lemma 1) employs simplifying assumptions like balanced class distribution and shared aggregated feature variance for mathematical elegance \\cite{mao202313j}. The CSBM-S model assumes consistent degree distributions and specific probability relationships \\cite{mao202313j}.\n    *   **Scope of Applicability**: The empirical validation primarily focuses on GCN and MLP-based models, and the findings are most directly applicable to semi-supervised node classification tasks \\cite{mao202313j}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of GNN behavior by providing a detailed, subgroup-level analysis of performance, directly addressing the challenges posed by structural disparity in real-world graphs \\cite{mao202313j}.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The identified factors (aggregated feature distance, homophily ratio differences) and the proposed PAC-Bayesian bound offer a theoretical foundation to guide the development of new GNN architectures and training strategies that are more robust and equitable across diverse node patterns \\cite{mao202313j}.\n        *   **Graph Out-of-Distribution (OOD) Problem**: By highlighting an overlooked distribution shift factor related to structural patterns, the work introduces a new dimension to the graph OOD problem, opening new avenues for research in OOD generalization for graphs \\cite{mao202313j}.\n        *   **Deeper GNNs**: Provides novel insights into the mechanisms behind the effectiveness of deeper GNNs in the context of structural disparity \\cite{mao202313j}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their performance on real-world graphs often masks a critical 'structural disparity': a mix of homophilic and heterophilic node patterns. We uncover a significant *performance disparity*, demonstrating that GNNs excel on dominant structural patterns but severely underperform on minority ones, a crucial oversight in current research.\n\nThis paper presents the first rigorous, subgroup-level investigation into this phenomenon. We introduce **CSBM-Structure (CSBM-S)**, a novel graph generation model, to precisely simulate mixed structural environments. Our core contribution is a **non-i.i.d PAC-Bayesian generalization bound** for GNNs, which theoretically identifies aggregated feature distance and homophily ratio differences between training and testing nodes as the fundamental drivers of this disparity. Empirical validation on diverse datasets confirms that GNN aggregation mechanisms exacerbate these issues, leading to poor discriminative ability for minority nodes.\n\nThese insights are vital for designing truly robust and equitable GNNs, offering a new perspective on the **graph out-of-distribution (OOD) problem** and guiding the development of architectures effective across all node types. This work compels a re-evaluation of GNN evaluation paradigms, paving the way for next-generation graph learning models.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "performance disparity",
      "structural disparity",
      "homophilic and heterophilic nodes",
      "GNN aggregation mechanism",
      "CSBM-Structure (CSBM-S)",
      "non-i.i.d PAC-Bayesian generalization bound",
      "aggregated feature distance",
      "homophily ratio differences",
      "subgroup performance analysis",
      "real-world graphs",
      "graph out-of-distribution (OOD) problem",
      "node classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/707142f242ee4e40489062870ca53810cb33d404.pdf",
    "citation_key": "mao202313j",
    "metadata": {
      "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
      "authors": [
        "Haitao Mao",
        "Zhikai Chen",
        "Wei Jin",
        "Haoyu Han",
        "Yao Ma",
        "Tong Zhao",
        "Neil Shah",
        "Jiliang Tang"
      ],
      "published_date": "2023",
      "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
      "file_path": "paper_data/Graph_Neural_Networks/707142f242ee4e40489062870ca53810cb33d404.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit a \"performance disparity\" on real-world graphs, performing well on nodes that align with the graph's dominant structural pattern (e.g., homophilic nodes in a homophilic graph) but struggling on nodes exhibiting the opposite pattern (e.g., heterophilic nodes in a homophilic graph) \\cite{mao202313j}. This disparity arises because real-world graphs inherently possess \"structural disparity,\" meaning they are a mixture of both homophilic and heterophilic node patterns.\n    *   **Importance & Challenge**: Existing GNN research often focuses on overall graph performance, overlooking the nuanced behavior of GNNs on these diverse node subgroups. Understanding this performance disparity is critical for developing GNNs that are robust and effective across all node types in complex, real-world graph structures \\cite{mao202313j}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior studies on GNN effectiveness primarily analyze performance on the entire graph and typically focus on graphs that are predominantly either homophilic or heterophilic \\cite{mao202313j}.\n    *   **Limitations of previous solutions**: These approaches fail to account for the common real-world scenario where graphs contain a mixture of homophilic and heterophilic patterns. They do not provide insights into GNN performance on specific node subgroups, potentially masking scenarios where GNNs perform poorly on minority patterns despite achieving good overall results \\cite{mao202313j}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Empirical Disparity Analysis**: Systematically investigates GNN performance by categorizing testing nodes based on their local homophily ratios, comparing a vanilla GNN (GCN) against MLP-based models (vanilla MLP and GLNN) \\cite{mao202313j}.\n        *   **Theoretical Analysis of Aggregation**: Examines how the GNN aggregation mechanism differentially impacts nodes with varying structural patterns. This involves introducing a novel graph generation model, CSBM-Structure (CSBM-S), and deriving theoretical insights into aggregated feature distances and class probabilities \\cite{mao202313j}.\n        *   **Generalization Bound**: Develops a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs to formally explain the underlying causes of the observed performance disparity \\cite{mao202313j}.\n        *   **Discriminative Ability Quantification**: Empirically analyzes the discriminative ability of GNNs on majority versus minority nodes using a \"relative discriminative ratio\" based on class prototypes \\cite{mao202313j}.\n    *   **Novelty/Difference**:\n        *   **First Rigorous Disparity Analysis**: This work is novel in its focused and systematic investigation of GNN performance on *subgroups* of nodes defined by their local structural patterns (homophilic vs. heterophilic), moving beyond aggregate performance metrics \\cite{mao202313j}.\n        *   **Novel CSBM-Structure (CSBM-S) Model**: Introduces a new variant of the Contextual Stochastic Block Model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes within a single graph, providing a more realistic testbed for structural disparity \\cite{mao202313j}.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound**: Presents a novel theoretical framework that precisely identifies aggregated feature distance and homophily ratio differences between training and testing nodes as critical factors driving performance disparity \\cite{mao202313j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Framework**: A non-i.i.d PAC-Bayesian generalization bound for GNNs that rigorously explains performance disparity by identifying aggregated feature distance and homophily ratio differences between training and testing nodes as key factors \\cite{mao202313j}.\n    *   **Novel Graph Generation Model**: Introduction of CSBM-Structure (CSBM-S), a variant of CSBM, which enables controlled experimental studies of graphs exhibiting mixed homophilic and heterophilic patterns \\cite{mao202313j}.\n    *   **Theoretical Insights into Aggregation**: Demonstrates that GNN aggregation creates a significant feature distance between homophilic and heterophilic node subgroups *within the same class*, and that differences in homophily ratio directly influence class prediction probabilities \\cite{mao202313j}.\n    *   **Practical Implications**: Provides insights into the effectiveness of deeper GNNs and identifies an overlooked distribution shift factor, leading to a new scenario for the graph out-of-distribution problem \\cite{mao202313j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Structural Disparity Verification**: Analyzed node homophily ratio distributions on real-world homophilic (Ogbn-arxiv, Pubmed) and heterophilic (Chameleon, Squirrel) datasets, confirming the pervasive existence of mixed structural patterns \\cite{mao202313j}.\n        *   **Subgroup Performance Comparison**: Conducted experiments comparing the accuracy of GCN against vanilla MLP and GLNN on test nodes, specifically grouped by their homophily ratio ranges \\cite{mao202313j}.\n        *   **Discriminative Ratio Analysis**: Empirically calculated a \"relative discriminative ratio\" to quantify the ease of prediction for majority versus minority nodes, observing its variation with the number of aggregation layers \\cite{mao202313j}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Accuracy Differences**: Demonstrated that in homophilic graphs, MLP-based models often outperform GCN on heterophilic (minority) nodes, while GCN performs better on homophilic (majority) nodes. Conversely, in heterophilic graphs, MLP models frequently outperform GCN on homophilic (minority) nodes, with GCN excelling on heterophilic (majority) nodes \\cite{mao202313j}.\n        *   **Discriminative Ratio**: The analysis aimed to illustrate how aggregation influences the discriminative ability, with a lower ratio indicating that majority nodes are easier to predict than minority nodes \\cite{mao202313j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical analysis (e.g., Lemma 1) employs simplifying assumptions like balanced class distribution and shared aggregated feature variance for mathematical elegance \\cite{mao202313j}. The CSBM-S model assumes consistent degree distributions and specific probability relationships \\cite{mao202313j}.\n    *   **Scope of Applicability**: The empirical validation primarily focuses on GCN and MLP-based models, and the findings are most directly applicable to semi-supervised node classification tasks \\cite{mao202313j}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of GNN behavior by providing a detailed, subgroup-level analysis of performance, directly addressing the challenges posed by structural disparity in real-world graphs \\cite{mao202313j}.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The identified factors (aggregated feature distance, homophily ratio differences) and the proposed PAC-Bayesian bound offer a theoretical foundation to guide the development of new GNN architectures and training strategies that are more robust and equitable across diverse node patterns \\cite{mao202313j}.\n        *   **Graph Out-of-Distribution (OOD) Problem**: By highlighting an overlooked distribution shift factor related to structural patterns, the work introduces a new dimension to the graph OOD problem, opening new avenues for research in OOD generalization for graphs \\cite{mao202313j}.\n        *   **Deeper GNNs**: Provides novel insights into the mechanisms behind the effectiveness of deeper GNNs in the context of structural disparity \\cite{mao202313j}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "performance disparity",
        "structural disparity",
        "homophilic and heterophilic nodes",
        "GNN aggregation mechanism",
        "CSBM-Structure (CSBM-S)",
        "non-i.i.d PAC-Bayesian generalization bound",
        "aggregated feature distance",
        "homophily ratio differences",
        "subgroup performance analysis",
        "real-world graphs",
        "graph out-of-distribution (OOD) problem",
        "node classification"
      ],
      "paper_type": "based on the abstract and introduction, this paper is best classified as **theoretical**.\n\nhere's why:\n\n*   **strong theoretical indicators:** the abstract explicitly states: \"we theoretically and empirically identify effects of gnns...\" and then, crucially, \"we then propose a rigorous, non-i.i.d pac-bayesian generalization bound for gnns, revealing reasons for the performance disparity...\" the proposal of a \"rigorous... generalization bound\" is a hallmark of theoretical work, involving mathematical analysis and formal models to explain phenomena.\n*   **analysis and explanation:** the paper aims to \"demystify\" structural disparity and \"reveal reasons\" for performance disparity, which aligns with the goal of theoretical papers to provide deeper understanding and formal explanations.\n*   **empirical support:** while the abstract also mentions \"empirically identify effects\" and \"demonstrate the practical implications,\" the empirical work appears to support and validate the theoretical findings derived from the proposed generalization bound, rather than being the sole or primary contribution.\n\nthe paper presents a new formal model (the pac-bayesian bound) to analyze and explain gnn behavior, making \"theoretical\" the most appropriate classification."
    },
    "file_name": "707142f242ee4e40489062870ca53810cb33d404.pdf"
  },
  {
    "success": true,
    "doc_id": "248612ea31ca0363e26ef9a8ca282e60",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{zgner2019bbi}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: The paper addresses the lack of understanding regarding the robustness of Graph Neural Networks (GNNs) to adversarial attacks, specifically focusing on *global poisoning attacks* that perturb the discrete graph structure at training time.\n    *   **Importance & Challenge**: GNNs have achieved state-of-the-art performance in many graph tasks (e.g., node classification), but their inherent propagation effects make them vulnerable to indirect adversarial manipulations. Existing attacks are predominantly *targeted* (aiming to misclassify a single node), leaving the problem of degrading overall model performance unexplored. The core challenge lies in solving a bilevel optimization problem for poisoning attacks on discrete graph structures, which is computationally intensive due to the vast discrete action space and non-differentiability.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on adversarial attacks for graphs includes test-time (evasion) and targeted poisoning attacks (e.g., \\cite{zgner2018}, Dai et al., 2018). Meta-learning has been used for hyperparameter optimization (Bengio, 2000) and for training-time attacks on simple linear models (Muñoz-González et al., 2017).\n    *   **Limitations of Previous Solutions**:\n        *   Most prior graph attacks are *targeted*, failing to address the more severe threat of compromising a model's global performance.\n        *   Previous poisoning attacks often circumvent the complex bilevel optimization problem by using static surrogate models, rather than explicitly optimizing the attacker's loss after the target model's training.\n        *   Adversarial attacks on deep neural networks typically assume independent and continuous data, which does not hold for discrete graph structures, making direct application difficult.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper formulates poisoning attacks as a bilevel optimization problem, where the attacker aims to maximize the classification loss of a target model *after* it has been trained on the perturbed graph. This problem is solved using **meta-gradients**, treating the discrete graph structure (adjacency matrix) as a hyperparameter to optimize.\n    *   **Novelty**:\n        *   **First Global Poisoning Attack**: It introduces the first algorithm designed to compromise the *global* node classification performance of GNNs, moving beyond single-node targeted attacks.\n        *   **Meta-Learning for Graph Adversarial Attacks**: It innovatively adapts meta-learning, traditionally used for hyperparameter optimization, to generate adversarial perturbations by backpropagating through the entire training process of the target model.\n        *   **Greedy Discrete Updates**: To overcome the challenge of discrete graph structures and maintain sparsity, the method relaxes the discreteness condition for gradient computation but performs greedy, discrete edge insertions/deletions based on a score function derived from the meta-gradients.\n        *   **Limited-Knowledge Setting**: The attacks are designed to operate without assuming knowledge of the target classifier's architecture or trained weights, relying on a surrogate model.\n\n4.  **Key Technical Contributions**\n    *   **Formalization of global poisoning attacks on GNNs as a bilevel optimization problem.**\n    *   **Introduction and application of meta-gradients to solve this bilevel problem for discrete graph structures**, effectively treating the graph as a learnable hyperparameter for adversarial purposes.\n    *   **Development of a greedy algorithm for discrete graph perturbations** guided by meta-gradients, which adheres to practical unnoticeability constraints (e.g., budget on changes, no disconnected nodes, preservation of degree distribution).\n    *   **Investigation and adaptation of meta-gradient approximations** (first-order and heuristic) to address the significant computational and memory costs associated with full meta-gradient computation, making the approach practical.\n    *   **Empirical demonstration of attack transferability** across different GNN architectures and even to unsupervised embeddings, highlighting a fundamental vulnerability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed meta-gradient-based poisoning attacks were evaluated on standard semi-supervised node classification tasks.\n    *   **Datasets**: Experiments were performed on benchmark citation networks: CORA-ML and CITESEER.\n    *   **Target Models**: The attacks were tested against Graph Convolutional Networks (GCNs) and CLN (a variant of GNNs).\n    *   **Attack Settings**: Attacks were conducted under a limited budget of 5% perturbed edges and in a limited-knowledge setting (using a linearized GCN as a surrogate model).\n    *   **Key Performance Metrics**: The primary metric was the misclassification rate (1 - accuracy) on the test nodes.\n    *   **Comparison Results**:\n        *   The attacks consistently and significantly increased the misclassification rate compared to clean (unperturbed) graphs. For instance, on CORA-ML, the GCN's misclassification rate increased from 16.6% (clean) to 22.5% under the A-Meta-Both attack \\cite{zgner2019bbi}.\n        *   The perturbations were shown to be highly effective, even causing GNNs to perform *worse than a simple baseline that ignores all relational information*.\n        *   The attacks demonstrated transferability, successfully degrading the performance of target models different from the surrogate model used to generate the perturbations.\n        *   Both variants of the attacker's loss function (`L_atk = -L_train` and `L_atk = -L_self`) proved effective, with `L_atk = -L_self` often yielding slightly stronger attacks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   Computing full meta-gradients is computationally and memory intensive, necessitating the use of approximations, which, while effective, are not exact.\n        *   The greedy approach for discrete updates, while practical, is a heuristic and may not find the globally optimal set of perturbations.\n        *   The unnoticeability constraints, while crucial for realistic attacks, add complexity to the search space.\n    *   **Scope of Applicability**:\n        *   The work primarily focuses on poisoning attacks on the *graph structure* for semi-supervised node classification.\n        *   While the algorithm can be extended to modify node features, the current study concentrates on structural perturbations.\n        *   Attacks are evaluated in a limited-knowledge setting, relying on a surrogate model.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper makes a significant advancement by introducing the first principled framework for global poisoning attacks on GNNs, addressing a critical gap in graph adversarial research. It pioneers the use of meta-gradients for optimizing discrete graph structures in an adversarial context.\n    *   **Highlights Fundamental Vulnerabilities**: The research unequivocally demonstrates that GNNs are highly susceptible to small, unnoticeable structural perturbations, leading to severe performance degradation that can render them less effective than non-relational baselines. This vulnerability is shown to be robust across different GNN architectures and even to unsupervised embeddings.\n    *   **Potential Impact on Future Research**: This work provides a crucial foundation for developing more robust GNN architectures and effective defense mechanisms against global poisoning attacks. It also opens new research directions for applying meta-learning to other graph-based optimization problems, both adversarial and beneficial.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized graph-based learning, yet their fundamental robustness to adversarial manipulation remains critically underexplored, particularly concerning *global poisoning attacks*. While prior work focused on targeted node misclassification, we unveil a far more insidious threat: attacks that degrade the entire model's performance by subtly perturbing the discrete graph structure during training.\n\nThis paper introduces the first principled framework for **global poisoning attacks** on GNNs, formulating the problem as a challenging **bilevel optimization** where the attacker optimizes for maximum model degradation. Our novel solution leverages **meta-gradients**, innovatively treating the discrete graph structure as a hyperparameter to be adversarially optimized. Through greedy, meta-gradient-guided edge modifications, we demonstrate that GNNs are alarmingly susceptible to small, unnoticeable structural changes. Experiments on benchmark datasets reveal that these attacks can consistently and significantly cripple GNN performance, even causing them to perform worse than non-relational baselines. Crucially, these perturbations exhibit strong **transferability** across diverse GNN architectures, highlighting a pervasive vulnerability. This work underscores an urgent need for robust GNN designs and defense mechanisms against such potent, global threats.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "global poisoning attacks",
      "bilevel optimization",
      "meta-gradients",
      "discrete graph structures",
      "meta-learning for adversarial attacks",
      "greedy discrete perturbations",
      "node classification",
      "attack transferability",
      "GNN vulnerability",
      "surrogate models",
      "computational complexity"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf",
    "citation_key": "zgner2019bbi",
    "metadata": {
      "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
      "authors": [
        "Daniel Zügner",
        "Stephan Günnemann"
      ],
      "published_date": "2019",
      "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
      "file_path": "paper_data/Graph_Neural_Networks/6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{zgner2019bbi}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: The paper addresses the lack of understanding regarding the robustness of Graph Neural Networks (GNNs) to adversarial attacks, specifically focusing on *global poisoning attacks* that perturb the discrete graph structure at training time.\n    *   **Importance & Challenge**: GNNs have achieved state-of-the-art performance in many graph tasks (e.g., node classification), but their inherent propagation effects make them vulnerable to indirect adversarial manipulations. Existing attacks are predominantly *targeted* (aiming to misclassify a single node), leaving the problem of degrading overall model performance unexplored. The core challenge lies in solving a bilevel optimization problem for poisoning attacks on discrete graph structures, which is computationally intensive due to the vast discrete action space and non-differentiability.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on adversarial attacks for graphs includes test-time (evasion) and targeted poisoning attacks (e.g., \\cite{zgner2018}, Dai et al., 2018). Meta-learning has been used for hyperparameter optimization (Bengio, 2000) and for training-time attacks on simple linear models (Muñoz-González et al., 2017).\n    *   **Limitations of Previous Solutions**:\n        *   Most prior graph attacks are *targeted*, failing to address the more severe threat of compromising a model's global performance.\n        *   Previous poisoning attacks often circumvent the complex bilevel optimization problem by using static surrogate models, rather than explicitly optimizing the attacker's loss after the target model's training.\n        *   Adversarial attacks on deep neural networks typically assume independent and continuous data, which does not hold for discrete graph structures, making direct application difficult.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper formulates poisoning attacks as a bilevel optimization problem, where the attacker aims to maximize the classification loss of a target model *after* it has been trained on the perturbed graph. This problem is solved using **meta-gradients**, treating the discrete graph structure (adjacency matrix) as a hyperparameter to optimize.\n    *   **Novelty**:\n        *   **First Global Poisoning Attack**: It introduces the first algorithm designed to compromise the *global* node classification performance of GNNs, moving beyond single-node targeted attacks.\n        *   **Meta-Learning for Graph Adversarial Attacks**: It innovatively adapts meta-learning, traditionally used for hyperparameter optimization, to generate adversarial perturbations by backpropagating through the entire training process of the target model.\n        *   **Greedy Discrete Updates**: To overcome the challenge of discrete graph structures and maintain sparsity, the method relaxes the discreteness condition for gradient computation but performs greedy, discrete edge insertions/deletions based on a score function derived from the meta-gradients.\n        *   **Limited-Knowledge Setting**: The attacks are designed to operate without assuming knowledge of the target classifier's architecture or trained weights, relying on a surrogate model.\n\n4.  **Key Technical Contributions**\n    *   **Formalization of global poisoning attacks on GNNs as a bilevel optimization problem.**\n    *   **Introduction and application of meta-gradients to solve this bilevel problem for discrete graph structures**, effectively treating the graph as a learnable hyperparameter for adversarial purposes.\n    *   **Development of a greedy algorithm for discrete graph perturbations** guided by meta-gradients, which adheres to practical unnoticeability constraints (e.g., budget on changes, no disconnected nodes, preservation of degree distribution).\n    *   **Investigation and adaptation of meta-gradient approximations** (first-order and heuristic) to address the significant computational and memory costs associated with full meta-gradient computation, making the approach practical.\n    *   **Empirical demonstration of attack transferability** across different GNN architectures and even to unsupervised embeddings, highlighting a fundamental vulnerability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed meta-gradient-based poisoning attacks were evaluated on standard semi-supervised node classification tasks.\n    *   **Datasets**: Experiments were performed on benchmark citation networks: CORA-ML and CITESEER.\n    *   **Target Models**: The attacks were tested against Graph Convolutional Networks (GCNs) and CLN (a variant of GNNs).\n    *   **Attack Settings**: Attacks were conducted under a limited budget of 5% perturbed edges and in a limited-knowledge setting (using a linearized GCN as a surrogate model).\n    *   **Key Performance Metrics**: The primary metric was the misclassification rate (1 - accuracy) on the test nodes.\n    *   **Comparison Results**:\n        *   The attacks consistently and significantly increased the misclassification rate compared to clean (unperturbed) graphs. For instance, on CORA-ML, the GCN's misclassification rate increased from 16.6% (clean) to 22.5% under the A-Meta-Both attack \\cite{zgner2019bbi}.\n        *   The perturbations were shown to be highly effective, even causing GNNs to perform *worse than a simple baseline that ignores all relational information*.\n        *   The attacks demonstrated transferability, successfully degrading the performance of target models different from the surrogate model used to generate the perturbations.\n        *   Both variants of the attacker's loss function (`L_atk = -L_train` and `L_atk = -L_self`) proved effective, with `L_atk = -L_self` often yielding slightly stronger attacks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   Computing full meta-gradients is computationally and memory intensive, necessitating the use of approximations, which, while effective, are not exact.\n        *   The greedy approach for discrete updates, while practical, is a heuristic and may not find the globally optimal set of perturbations.\n        *   The unnoticeability constraints, while crucial for realistic attacks, add complexity to the search space.\n    *   **Scope of Applicability**:\n        *   The work primarily focuses on poisoning attacks on the *graph structure* for semi-supervised node classification.\n        *   While the algorithm can be extended to modify node features, the current study concentrates on structural perturbations.\n        *   Attacks are evaluated in a limited-knowledge setting, relying on a surrogate model.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This paper makes a significant advancement by introducing the first principled framework for global poisoning attacks on GNNs, addressing a critical gap in graph adversarial research. It pioneers the use of meta-gradients for optimizing discrete graph structures in an adversarial context.\n    *   **Highlights Fundamental Vulnerabilities**: The research unequivocally demonstrates that GNNs are highly susceptible to small, unnoticeable structural perturbations, leading to severe performance degradation that can render them less effective than non-relational baselines. This vulnerability is shown to be robust across different GNN architectures and even to unsupervised embeddings.\n    *   **Potential Impact on Future Research**: This work provides a crucial foundation for developing more robust GNN architectures and effective defense mechanisms against global poisoning attacks. It also opens new research directions for applying meta-learning to other graph-based optimization problems, both adversarial and beneficial.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "global poisoning attacks",
        "bilevel optimization",
        "meta-gradients",
        "discrete graph structures",
        "meta-learning for adversarial attacks",
        "greedy discrete perturbations",
        "node classification",
        "attack transferability",
        "GNN vulnerability",
        "surrogate models",
        "computational complexity"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract states: \"we investigate training time attacks on graph neural networks... our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.\" this clearly describes a **new method or approach** being proposed.\n*   it further mentions: \"our experiments show that small graph perturbations consistently lead to a strong decrease in performance... the perturbations created by **our algorithm** can misguide...\" this indicates the development of an algorithm and its empirical evaluation.\n*   the introduction sets the context by discussing the problem of adversarial attacks on graph neural networks and then leads into their proposed solution.\n\nthese elements strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it also includes empirical findings (\"our experiments show...\"), the core contribution described is the development and presentation of a novel attack method using meta-gradients.\n\n**classification: technical**"
    },
    "file_name": "6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf"
  },
  {
    "success": true,
    "doc_id": "c671a1dd70c6df7df9571f1798c12e51",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analysis of \"Explainability in Graph Neural Networks: A Taxonomic Survey\" \\cite{yuan2020fnk}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs), despite their high performance, operate as \"black-boxes,\" lacking inherent interpretability. This opacity prevents understanding *why* a GNN makes a particular prediction, hindering trust and safe deployment.\n    *   **Why Important & Challenging:**\n        *   **Trust & Safety:** In critical applications (e.g., finance, biology, medicine), understanding the reasoning behind GNN predictions is crucial for fairness, privacy, and safety.\n        *   **Unique Graph Challenges:** Unlike images or text, graph data is non-grid-like, features discrete adjacency matrices, and its structural information (e.g., substructures, message passing paths) is paramount. Existing explanation methods for other data types are often not directly applicable.\n        *   **Lack of Standardization:** The rapidly developing field of GNN explainability suffers from a lack of unified treatment, standard benchmarks, and consistent evaluation metrics, making it difficult to compare and advance methods.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Significant progress has been made in explaining deep models for image and text data, utilizing techniques like gradient-based methods, feature mapping, and perturbation analysis.\n    *   **Limitations of Previous Solutions:**\n        *   Prior surveys on deep learning explainability *exclusively focused on image and text domains*, neglecting the unique challenges of graph data.\n        *   Image/text explanation methods are often *unsuitable for graphs* due to their distinct topological structure, discrete nature of connections, and the critical role of structural patterns (e.g., network motifs) that are not captured by pixel or word importance.\n        *   Existing GNN explanation methods (e.g., XGNN, GNNExplainer) are disparate, developed from \"different angles,\" and lack a \"unified treatment\" or \"standard benchmark and testbed for evaluations\" \\cite{yuan2020fnk}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive *survey* and *meta-analysis* rather than proposing a single new algorithm. Its core approach involves:\n        *   Developing a novel, unified **taxonomy** to systematically categorize existing GNN explanation methods.\n        *   Providing a **systematic review** of these methods, detailing their methodologies, advantages, and drawbacks.\n        *   Constructing a **standardized testbed** for GNN explainability, including datasets, common algorithms, and evaluation metrics.\n        *   Conducting **comprehensive experiments** to compare and analyze the performance of various techniques.\n    *   **Novelty/Difference:**\n        *   It is presented as the *first and only systematic and comprehensive survey* specifically dedicated to GNN explainability \\cite{yuan2020fnk}.\n        *   It introduces a novel, structured taxonomy that clarifies the commonalities and differences among diverse GNN explanation techniques.\n        *   It addresses the critical need for standardization by providing a unified framework, a publicly available testbed (including new human-understandable datasets), and an open-source library.\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy:** A comprehensive, two-tiered taxonomy categorizing GNN explanation methods into **instance-level** (gradients/features-based, perturbation-based, decomposition, surrogate) and **model-level** explanations \\cite{yuan2020fnk}.\n    *   **Standardized Testbed:** Creation of a publicly available testbed for GNN explainability, comprising commonly used datasets, recommended evaluation metrics, and three novel human-understandable datasets derived from text \\cite{yuan2020fnk}.\n    *   **Open-Source Library:** Development of an open-source library that includes implementations of several existing GNN explanation techniques, common datasets, and evaluation metrics, fostering reproducibility and new method development \\cite{yuan2020fnk}.\n    *   **Empirical Analysis:** Comprehensive experimental comparison and analysis of various GNN explanation techniques, providing insights into their performance and guiding future research directions \\cite{yuan2020fnk}.\n    *   **Unified Methodological Treatment:** Offers a structured understanding of the diverse GNN explanation methods, highlighting their underlying principles and distinctions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The paper states that it \"conduct[s] comprehensive experiments to compare and analyze the performance of many techniques\" \\cite{yuan2020fnk}. These experiments involve evaluating the surveyed explanation methods using the provided testbed.\n    *   **Key Performance Metrics & Comparison Results:** The paper summarizes \"commonly employed datasets and evaluation metrics for GNN explanation tasks\" and discusses their limitations, recommending \"several convincing metrics\" \\cite{yuan2020fnk}. The results of the comprehensive experiments are intended to \"provide insights for new researchers about the development of the research field and baseline selections\" \\cite{yuan2020fnk}. (Specific metrics and detailed results are not provided in the excerpt but are part of the full paper's contribution).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The paper notes that \"model-level methods\" for GNN explainability are \"still less explored\" compared to instance-level methods \\cite{yuan2020fnk}.\n        *   It acknowledges that explanations from model-level methods \"may not be human-intelligible since the obtained graph patterns may not even exist in the real-world\" \\cite{yuan2020fnk}.\n        *   As a survey, it represents a snapshot of the field at the time of publication, and new methods will continuously emerge.\n    *   **Scope of Applicability:** The survey focuses exclusively on **explainability in Graph Neural Networks (GNNs)**, covering both instance-level (input-dependent) and model-level (input-independent) explanations. The provided testbed and library are designed to support research across various graph-related tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work establishes a foundational and systematic framework for the nascent field of GNN explainability. By providing the first comprehensive taxonomy and review, it significantly advances the understanding and organization of existing methods \\cite{yuan2020fnk}. It also contributes to standardizing terminology by differentiating \"explainability\" and \"interpretability.\"\n    *   **Potential Impact on Future Research:**\n        *   Serves as an essential reference for researchers, offering a structured entry point and overview of the field.\n        *   The proposed taxonomy \"sheds lights on the commonalities and differences of existing methods and sets the stage for further methodological developments\" \\cite{yuan2020fnk}.\n        *   The open-source library and standardized testbed (datasets, metrics) will accelerate research, facilitate fair comparisons, and promote reproducibility of new GNN explanation techniques.\n        *   The empirical analysis provides valuable baselines and insights, guiding the development of more effective and robust GNN explanation methods.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are revolutionizing AI, yet their inherent \"black-box\" nature severely limits trust and deployment in critical domains like medicine and finance. Understanding *why* a GNN makes a prediction is paramount, especially given the unique, non-Euclidean challenges of graph data, where structural information and message passing are key. The nascent field of GNN explainability suffers from disparate approaches and a critical lack of unified treatment, hindering progress.\n\nThis paper presents the *first comprehensive and systematic survey* of GNN explainability, addressing this void. We introduce a novel, two-tiered **taxonomy** that meticulously categorizes existing methods into **instance-level** and **model-level** explanations, clarifying their underlying principles and distinctions. To accelerate research and foster reproducibility, we establish a **standardized testbed**—featuring diverse datasets, recommended metrics, and novel human-understandable graph datasets—alongside an accompanying **open-source library** of implemented techniques. Our extensive empirical analysis provides crucial insights and baselines. This foundational work unifies the GNN explainability landscape, offering an indispensable resource for researchers, promoting fair comparisons, and paving the way for trustworthy and interpretable GNN applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "GNN Explainability",
      "Black-box models",
      "Novel Taxonomy",
      "Systematic Review",
      "Standardized Testbed",
      "Open-Source Library",
      "Instance-level explanations",
      "Model-level explanations",
      "Graph data challenges",
      "Unified framework",
      "Evaluation metrics",
      "Reproducibility",
      "Critical applications"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/6ae2967bb0a5e57cc545176120a4845576e068a3.pdf",
    "citation_key": "yuan2020fnk",
    "metadata": {
      "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
      "authors": [
        "Hao Yuan",
        "Haiyang Yu",
        "Shurui Gui",
        "Shuiwang Ji"
      ],
      "published_date": "2020",
      "abstract": "Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
      "file_path": "paper_data/Graph_Neural_Networks/6ae2967bb0a5e57cc545176120a4845576e068a3.pdf",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analysis of \"Explainability in Graph Neural Networks: A Taxonomic Survey\" \\cite{yuan2020fnk}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Graph Neural Networks (GNNs), despite their high performance, operate as \"black-boxes,\" lacking inherent interpretability. This opacity prevents understanding *why* a GNN makes a particular prediction, hindering trust and safe deployment.\n    *   **Why Important & Challenging:**\n        *   **Trust & Safety:** In critical applications (e.g., finance, biology, medicine), understanding the reasoning behind GNN predictions is crucial for fairness, privacy, and safety.\n        *   **Unique Graph Challenges:** Unlike images or text, graph data is non-grid-like, features discrete adjacency matrices, and its structural information (e.g., substructures, message passing paths) is paramount. Existing explanation methods for other data types are often not directly applicable.\n        *   **Lack of Standardization:** The rapidly developing field of GNN explainability suffers from a lack of unified treatment, standard benchmarks, and consistent evaluation metrics, making it difficult to compare and advance methods.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Significant progress has been made in explaining deep models for image and text data, utilizing techniques like gradient-based methods, feature mapping, and perturbation analysis.\n    *   **Limitations of Previous Solutions:**\n        *   Prior surveys on deep learning explainability *exclusively focused on image and text domains*, neglecting the unique challenges of graph data.\n        *   Image/text explanation methods are often *unsuitable for graphs* due to their distinct topological structure, discrete nature of connections, and the critical role of structural patterns (e.g., network motifs) that are not captured by pixel or word importance.\n        *   Existing GNN explanation methods (e.g., XGNN, GNNExplainer) are disparate, developed from \"different angles,\" and lack a \"unified treatment\" or \"standard benchmark and testbed for evaluations\" \\cite{yuan2020fnk}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive *survey* and *meta-analysis* rather than proposing a single new algorithm. Its core approach involves:\n        *   Developing a novel, unified **taxonomy** to systematically categorize existing GNN explanation methods.\n        *   Providing a **systematic review** of these methods, detailing their methodologies, advantages, and drawbacks.\n        *   Constructing a **standardized testbed** for GNN explainability, including datasets, common algorithms, and evaluation metrics.\n        *   Conducting **comprehensive experiments** to compare and analyze the performance of various techniques.\n    *   **Novelty/Difference:**\n        *   It is presented as the *first and only systematic and comprehensive survey* specifically dedicated to GNN explainability \\cite{yuan2020fnk}.\n        *   It introduces a novel, structured taxonomy that clarifies the commonalities and differences among diverse GNN explanation techniques.\n        *   It addresses the critical need for standardization by providing a unified framework, a publicly available testbed (including new human-understandable datasets), and an open-source library.\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy:** A comprehensive, two-tiered taxonomy categorizing GNN explanation methods into **instance-level** (gradients/features-based, perturbation-based, decomposition, surrogate) and **model-level** explanations \\cite{yuan2020fnk}.\n    *   **Standardized Testbed:** Creation of a publicly available testbed for GNN explainability, comprising commonly used datasets, recommended evaluation metrics, and three novel human-understandable datasets derived from text \\cite{yuan2020fnk}.\n    *   **Open-Source Library:** Development of an open-source library that includes implementations of several existing GNN explanation techniques, common datasets, and evaluation metrics, fostering reproducibility and new method development \\cite{yuan2020fnk}.\n    *   **Empirical Analysis:** Comprehensive experimental comparison and analysis of various GNN explanation techniques, providing insights into their performance and guiding future research directions \\cite{yuan2020fnk}.\n    *   **Unified Methodological Treatment:** Offers a structured understanding of the diverse GNN explanation methods, highlighting their underlying principles and distinctions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The paper states that it \"conduct[s] comprehensive experiments to compare and analyze the performance of many techniques\" \\cite{yuan2020fnk}. These experiments involve evaluating the surveyed explanation methods using the provided testbed.\n    *   **Key Performance Metrics & Comparison Results:** The paper summarizes \"commonly employed datasets and evaluation metrics for GNN explanation tasks\" and discusses their limitations, recommending \"several convincing metrics\" \\cite{yuan2020fnk}. The results of the comprehensive experiments are intended to \"provide insights for new researchers about the development of the research field and baseline selections\" \\cite{yuan2020fnk}. (Specific metrics and detailed results are not provided in the excerpt but are part of the full paper's contribution).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The paper notes that \"model-level methods\" for GNN explainability are \"still less explored\" compared to instance-level methods \\cite{yuan2020fnk}.\n        *   It acknowledges that explanations from model-level methods \"may not be human-intelligible since the obtained graph patterns may not even exist in the real-world\" \\cite{yuan2020fnk}.\n        *   As a survey, it represents a snapshot of the field at the time of publication, and new methods will continuously emerge.\n    *   **Scope of Applicability:** The survey focuses exclusively on **explainability in Graph Neural Networks (GNNs)**, covering both instance-level (input-dependent) and model-level (input-independent) explanations. The provided testbed and library are designed to support research across various graph-related tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work establishes a foundational and systematic framework for the nascent field of GNN explainability. By providing the first comprehensive taxonomy and review, it significantly advances the understanding and organization of existing methods \\cite{yuan2020fnk}. It also contributes to standardizing terminology by differentiating \"explainability\" and \"interpretability.\"\n    *   **Potential Impact on Future Research:**\n        *   Serves as an essential reference for researchers, offering a structured entry point and overview of the field.\n        *   The proposed taxonomy \"sheds lights on the commonalities and differences of existing methods and sets the stage for further methodological developments\" \\cite{yuan2020fnk}.\n        *   The open-source library and standardized testbed (datasets, metrics) will accelerate research, facilitate fair comparisons, and promote reproducibility of new GNN explanation techniques.\n        *   The empirical analysis provides valuable baselines and insights, guiding the development of more effective and robust GNN explanation methods.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "GNN Explainability",
        "Black-box models",
        "Novel Taxonomy",
        "Systematic Review",
        "Standardized Testbed",
        "Open-Source Library",
        "Instance-level explanations",
        "Model-level explanations",
        "Graph data challenges",
        "Unified framework",
        "Evaluation metrics",
        "Reproducibility",
        "Critical applications"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"explainability in graph neural networks: a taxonomic survey\" - the word \"survey\" is explicitly in the title, and \"taxonomic\" indicates a classification and organization of existing knowledge.\n*   **abstract:**\n    *   \"in this survey, we provide a uniﬁed and taxonomic view of current gnn explainability methods.\" - directly states it is a survey and its purpose.\n    *   \"our uniﬁed and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods...\" - focuses on analyzing and comparing *existing* methods.\n    *   \"furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques.\" - this empirical work is done to evaluate *existing* techniques, which is common in surveys to provide a benchmark.\n*   **introduction:**\n    *   \"in this survey, we provide a uniﬁed and taxonomic view of current gnn explainability methods.\" - repeats the core purpose.\n    *   \"recent survey works... provides good systematic reviews and taxonomies of these methods. however, these surveys only focus on the explanation methods for image and text domains and ignore the explainability of deep graph models.\" - positions itself as filling a gap in *survey literature*.\n    *   \"hence, it raises the need of a systematic survey covering the methods and evaluations of gnn explanation techniques.\" - explicitly states the need for a \"systematic survey\".\n    *   **contributions list:** the first contribution explicitly states: \"this survey provides a systematic and comprehensive review of existing explanation techniques for deep graph models.\" other contributions like proposing a taxonomy, introducing each method in detail, summarizing datasets and metrics, and conducting experiments are all in service of this comprehensive review and analysis of *existing* work.\n*   **index terms:** includes \"survey\" and \"taxonomy\".\n\nwhile the paper also includes empirical studies and develops new datasets/a library, these are presented as supporting elements for the comprehensive review and comparison of *existing* gnn explainability methods, rather than being the primary, novel contribution of a new method or a standalone empirical study. the overarching goal is to review, organize, and analyze the current state of the art."
    },
    "file_name": "6ae2967bb0a5e57cc545176120a4845576e068a3.pdf"
  },
  {
    "success": true,
    "doc_id": "784b79804cf85048bc72761c776d8b05",
    "summary": "Here's a focused summary of the paper \"Cooperative Graph Neural Networks\" by Finkelshtein et al. for a literature review:\n\n### Cooperative Graph Neural Networks \\cite{finkelshtein202301z}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs) rely on a fixed, synchronous message-passing paradigm where every node \"listens and broadcasts\" to all its neighbors at every layer. This rigid information flow leads to several well-known limitations.\n    *   **Importance and Challenge**:\n        *   **Information Bottlenecks**: Standard message passing struggles with long-range dependencies, requiring many layers which can lead to exponential growth of receptive fields and subsequent information loss (over-squashing) \\cite{finkelshtein202301z}.\n        *   **Over-smoothing**: Node features can become increasingly similar with more layers, hindering discriminative power \\cite{finkelshtein202301z}.\n        *   **Fixed & Synchronous Updates**: The static and synchronous nature of message passing prevents nodes from reacting individually to messages or adapting information flow based on their state or the task, which is not always optimal \\cite{finkelshtein202301z}.\n        *   **Limited Expressive Power**: Most GNNs are upper-bounded by the 1-Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs \\cite{finkelshtein202301z}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work generalizes the widely adopted message-passing paradigm (e.g., GCN, GIN, GAT, GraphSAGE) \\cite{finkelshtein202301z}.\n    *   **Limitations of Previous Solutions**:\n        *   Existing approaches to improve expressivity often involve higher-order structures, subgraph counting, or unique identifiers, which can increase complexity \\cite{finkelshtein202301z}.\n        *   Solutions for long-range dependencies typically involve graph rewiring or direct connections between distant nodes, which modify the graph topology globally or based on fixed rules \\cite{finkelshtein202301z}.\n        *   Some works explore dynamic layer counts per node or learned topologies, but these are often fixed across layers or focus on different objectives than dynamic, node-specific communication strategies \\cite{finkelshtein202301z}.\n        *   Attention mechanisms (e.g., GAT) weigh neighbor contributions but do not fundamentally alter the communication strategy or address limitations like counting node degrees \\cite{finkelshtein202301z}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Cooperative Graph Neural Networks (CO-GNNs), a novel framework where each node is viewed as a \"player\" that can dynamically choose one of four actions at every layer:\n        *   **STANDARD (S)**: Broadcast to neighbors that listen and listen to neighbors that broadcast (recovers standard message passing).\n        *   **LISTEN (L)**: Listen to neighbors that broadcast.\n        *   **BROADCAST (B)**: Broadcast to neighbors that listen.\n        *   **ISOLATE (I)**: Neither listen nor broadcast.\n    *   **Novelty/Difference**:\n        *   **Two-Stage Process**: CO-GNNs comprise two jointly trained \"cooperating\" message-passing neural networks:\n            *   An **action network (π)**: Predicts a probability distribution over the four actions for each node based on its current state and neighbors.\n            *   An **environment network (η)**: Updates node representations based on the sampled actions, effectively operating on a dynamically induced computational graph \\cite{finkelshtein202301z}.\n        *   **Dynamic & Asynchronous Message Passing**: Unlike standard GNNs, CO-GNNs allow nodes to determine their own communication strategy (listen, broadcast, or both) based on their state, leading to a flexible, dynamic, and asynchronous information flow \\cite{finkelshtein202301z}.\n        *   **Directed Information Flow**: The interplay of actions can induce directed edges in the computational graph at each layer, allowing for more nuanced information propagation than fixed undirected edges \\cite{finkelshtein202301z}.\n        *   **Differentiable Action Sampling**: Utilizes the Straight-through Gumbel-softmax estimator to enable gradient-based optimization for the categorical action choices \\cite{finkelshtein202301z}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of the CO-GNN framework with its unique two-network (action and environment) architecture and dynamic, node-specific action selection mechanism \\cite{finkelshtein202301z}.\n    *   **System Design/Architectural Innovations**: A message-passing scheme that decouples the input graph from the computational graph, allowing for layer-wise, node-specific \"rewiring\" of information flow \\cite{finkelshtein202301z}.\n    *   **Theoretical Insights/Analysis**:\n        *   **Enhanced Expressive Power**: Theoretically shown to be more expressive than the 1-WL algorithm (Proposition 5.1), capable of distinguishing non-isomorphic graphs that 1-WL cannot, due to the variance introduced by action sampling \\cite{finkelshtein202301z}.\n        *   **Suitability for Long-Range Tasks**: Adaptive nature makes them better suited for long-range tasks by allowing nodes to selectively propagate information \\cite{finkelshtein202301z}.\n        *   **Conceptual Properties**: Identified as task-specific, directed, dynamic, feature and structure-based, asynchronous, and employing conditional aggregation. It is also orthogonal to attention mechanisms and mitigates over-smoothing \\cite{finkelshtein202301z}.\n        *   **Efficiency**: Demonstrated to be efficient in runtime and parameter-efficient by sharing the action network across layers \\cite{finkelshtein202301z}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Validation on a synthetic task to demonstrate the ability to learn specific communication patterns (e.g., listening only to neighbors with a certain degree) \\cite{finkelshtein202301z}.\n        *   Extensive experiments on real-world datasets for various graph machine learning tasks (details in Section 6.2 and Appendix C.3) \\cite{finkelshtein202301z}.\n        *   Analysis of action trends on homophilic and heterophilic graphs \\cite{finkelshtein202301z}.\n        *   Visualization of actions on a heterophilic graph \\cite{finkelshtein202301z}.\n        *   Ablation studies on the choices of action and environment networks \\cite{finkelshtein202301z}.\n        *   Additional experiments on expressive power (Appendix C.1), long-range tasks (Appendix C.2), and over-smoothing (Appendix C.5) \\cite{finkelshtein202301z}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   CO-GNNs consistently improve performance compared to their baseline models (e.g., SUMGNNs, MEANGNNs, GCN, GIN, GAT) \\cite{finkelshtein202301z}.\n        *   Achieved multiple state-of-the-art results on real-world datasets \\cite{finkelshtein202301z}.\n        *   Empirically validated mitigation of over-smoothing \\cite{finkelshtein202301z}.\n        *   Demonstrated ability to solve tasks that GAT alone cannot, when GAT is used as the environment network within a CO-GNN \\cite{finkelshtein202301z}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   While CO-GNNs can mitigate over-smoothing, the optimization becomes increasingly difficult as the number of layers grows very large \\cite{finkelshtein202301z}.\n        *   The framework introduces additional complexity by requiring the training of two networks (action and environment), although it is parameter-efficient \\cite{finkelshtein202301z}.\n    *   **Scope of Applicability**: Applicable to various graph machine learning tasks (node-level, graph-level) and can integrate different base GNN architectures for its action and environment networks \\cite{finkelshtein202301z}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: Introduces a fundamentally new and more flexible message-passing paradigm for GNNs, moving beyond the static and synchronous limitations of traditional approaches \\cite{finkelshtein202301z}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for designing GNNs that can dynamically adapt their computational graph and information flow based on node states and task requirements.\n        *   Provides a robust framework to address long-standing challenges in GNNs such as limited expressive power, over-squashing, and over-smoothing through adaptive communication strategies.\n        *   The concept of \"cooperating\" networks for GNNs could inspire further research into multi-agent or game-theoretic approaches for graph learning \\cite{finkelshtein202301z}.",
    "intriguing_abstract": "Traditional Graph Neural Networks (GNNs) are fundamentally limited by their rigid, synchronous message-passing paradigm, leading to critical issues like over-squashing, over-smoothing, and restricted expressive power. We introduce Cooperative Graph Neural Networks (CO-GNNs), a novel framework that empowers each node to dynamically choose its communication strategy at every layer.\n\nCO-GNNs feature two jointly trained, cooperating networks: an **action network (π)** predicts node-specific actions (e.g., LISTEN, BROADCAST, ISOLATE), while an **environment network (η)** updates representations based on these dynamic choices. This innovative approach decouples the input graph from a dynamically induced computational graph, enabling flexible, asynchronous information flow. Theoretically, CO-GNNs surpass the 1-Weisfeiler-Leman test, demonstrating enhanced expressive power. Empirically, our model mitigates over-squashing and over-smoothing, significantly improving performance on long-range dependency tasks and achieving state-of-the-art results across diverse benchmarks. CO-GNNs represent a fundamental shift in GNN design, offering a powerful, adaptive solution to long-standing challenges in graph representation learning.",
    "keywords": [
      "Cooperative Graph Neural Networks (CO-GNNs)",
      "Dynamic and asynchronous message passing",
      "Over-squashing",
      "Over-smoothing",
      "Enhanced expressive power",
      "1-Weisfeiler-Leman algorithm",
      "Action network",
      "Environment network",
      "Differentiable action sampling",
      "Long-range dependencies",
      "Computational graph rewiring",
      "Graph machine learning",
      "State-of-the-art results"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf",
    "citation_key": "finkelshtein202301z",
    "metadata": {
      "title": "Cooperative Graph Neural Networks",
      "authors": [
        "Ben Finkelshtein",
        "Xingyue Huang",
        "Michael M. Bronstein",
        "I. Ceylan"
      ],
      "published_date": "2023",
      "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Cooperative Graph Neural Networks\" by Finkelshtein et al. for a literature review:\n\n### Cooperative Graph Neural Networks \\cite{finkelshtein202301z}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs) rely on a fixed, synchronous message-passing paradigm where every node \"listens and broadcasts\" to all its neighbors at every layer. This rigid information flow leads to several well-known limitations.\n    *   **Importance and Challenge**:\n        *   **Information Bottlenecks**: Standard message passing struggles with long-range dependencies, requiring many layers which can lead to exponential growth of receptive fields and subsequent information loss (over-squashing) \\cite{finkelshtein202301z}.\n        *   **Over-smoothing**: Node features can become increasingly similar with more layers, hindering discriminative power \\cite{finkelshtein202301z}.\n        *   **Fixed & Synchronous Updates**: The static and synchronous nature of message passing prevents nodes from reacting individually to messages or adapting information flow based on their state or the task, which is not always optimal \\cite{finkelshtein202301z}.\n        *   **Limited Expressive Power**: Most GNNs are upper-bounded by the 1-Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs \\cite{finkelshtein202301z}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work generalizes the widely adopted message-passing paradigm (e.g., GCN, GIN, GAT, GraphSAGE) \\cite{finkelshtein202301z}.\n    *   **Limitations of Previous Solutions**:\n        *   Existing approaches to improve expressivity often involve higher-order structures, subgraph counting, or unique identifiers, which can increase complexity \\cite{finkelshtein202301z}.\n        *   Solutions for long-range dependencies typically involve graph rewiring or direct connections between distant nodes, which modify the graph topology globally or based on fixed rules \\cite{finkelshtein202301z}.\n        *   Some works explore dynamic layer counts per node or learned topologies, but these are often fixed across layers or focus on different objectives than dynamic, node-specific communication strategies \\cite{finkelshtein202301z}.\n        *   Attention mechanisms (e.g., GAT) weigh neighbor contributions but do not fundamentally alter the communication strategy or address limitations like counting node degrees \\cite{finkelshtein202301z}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Cooperative Graph Neural Networks (CO-GNNs), a novel framework where each node is viewed as a \"player\" that can dynamically choose one of four actions at every layer:\n        *   **STANDARD (S)**: Broadcast to neighbors that listen and listen to neighbors that broadcast (recovers standard message passing).\n        *   **LISTEN (L)**: Listen to neighbors that broadcast.\n        *   **BROADCAST (B)**: Broadcast to neighbors that listen.\n        *   **ISOLATE (I)**: Neither listen nor broadcast.\n    *   **Novelty/Difference**:\n        *   **Two-Stage Process**: CO-GNNs comprise two jointly trained \"cooperating\" message-passing neural networks:\n            *   An **action network (π)**: Predicts a probability distribution over the four actions for each node based on its current state and neighbors.\n            *   An **environment network (η)**: Updates node representations based on the sampled actions, effectively operating on a dynamically induced computational graph \\cite{finkelshtein202301z}.\n        *   **Dynamic & Asynchronous Message Passing**: Unlike standard GNNs, CO-GNNs allow nodes to determine their own communication strategy (listen, broadcast, or both) based on their state, leading to a flexible, dynamic, and asynchronous information flow \\cite{finkelshtein202301z}.\n        *   **Directed Information Flow**: The interplay of actions can induce directed edges in the computational graph at each layer, allowing for more nuanced information propagation than fixed undirected edges \\cite{finkelshtein202301z}.\n        *   **Differentiable Action Sampling**: Utilizes the Straight-through Gumbel-softmax estimator to enable gradient-based optimization for the categorical action choices \\cite{finkelshtein202301z}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of the CO-GNN framework with its unique two-network (action and environment) architecture and dynamic, node-specific action selection mechanism \\cite{finkelshtein202301z}.\n    *   **System Design/Architectural Innovations**: A message-passing scheme that decouples the input graph from the computational graph, allowing for layer-wise, node-specific \"rewiring\" of information flow \\cite{finkelshtein202301z}.\n    *   **Theoretical Insights/Analysis**:\n        *   **Enhanced Expressive Power**: Theoretically shown to be more expressive than the 1-WL algorithm (Proposition 5.1), capable of distinguishing non-isomorphic graphs that 1-WL cannot, due to the variance introduced by action sampling \\cite{finkelshtein202301z}.\n        *   **Suitability for Long-Range Tasks**: Adaptive nature makes them better suited for long-range tasks by allowing nodes to selectively propagate information \\cite{finkelshtein202301z}.\n        *   **Conceptual Properties**: Identified as task-specific, directed, dynamic, feature and structure-based, asynchronous, and employing conditional aggregation. It is also orthogonal to attention mechanisms and mitigates over-smoothing \\cite{finkelshtein202301z}.\n        *   **Efficiency**: Demonstrated to be efficient in runtime and parameter-efficient by sharing the action network across layers \\cite{finkelshtein202301z}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Validation on a synthetic task to demonstrate the ability to learn specific communication patterns (e.g., listening only to neighbors with a certain degree) \\cite{finkelshtein202301z}.\n        *   Extensive experiments on real-world datasets for various graph machine learning tasks (details in Section 6.2 and Appendix C.3) \\cite{finkelshtein202301z}.\n        *   Analysis of action trends on homophilic and heterophilic graphs \\cite{finkelshtein202301z}.\n        *   Visualization of actions on a heterophilic graph \\cite{finkelshtein202301z}.\n        *   Ablation studies on the choices of action and environment networks \\cite{finkelshtein202301z}.\n        *   Additional experiments on expressive power (Appendix C.1), long-range tasks (Appendix C.2), and over-smoothing (Appendix C.5) \\cite{finkelshtein202301z}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   CO-GNNs consistently improve performance compared to their baseline models (e.g., SUMGNNs, MEANGNNs, GCN, GIN, GAT) \\cite{finkelshtein202301z}.\n        *   Achieved multiple state-of-the-art results on real-world datasets \\cite{finkelshtein202301z}.\n        *   Empirically validated mitigation of over-smoothing \\cite{finkelshtein202301z}.\n        *   Demonstrated ability to solve tasks that GAT alone cannot, when GAT is used as the environment network within a CO-GNN \\cite{finkelshtein202301z}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   While CO-GNNs can mitigate over-smoothing, the optimization becomes increasingly difficult as the number of layers grows very large \\cite{finkelshtein202301z}.\n        *   The framework introduces additional complexity by requiring the training of two networks (action and environment), although it is parameter-efficient \\cite{finkelshtein202301z}.\n    *   **Scope of Applicability**: Applicable to various graph machine learning tasks (node-level, graph-level) and can integrate different base GNN architectures for its action and environment networks \\cite{finkelshtein202301z}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: Introduces a fundamentally new and more flexible message-passing paradigm for GNNs, moving beyond the static and synchronous limitations of traditional approaches \\cite{finkelshtein202301z}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for designing GNNs that can dynamically adapt their computational graph and information flow based on node states and task requirements.\n        *   Provides a robust framework to address long-standing challenges in GNNs such as limited expressive power, over-squashing, and over-smoothing through adaptive communication strategies.\n        *   The concept of \"cooperating\" networks for GNNs could inspire further research into multi-agent or game-theoretic approaches for graph learning \\cite{finkelshtein202301z}.",
      "keywords": [
        "Cooperative Graph Neural Networks (CO-GNNs)",
        "Dynamic and asynchronous message passing",
        "Over-squashing",
        "Over-smoothing",
        "Enhanced expressive power",
        "1-Weisfeiler-Leman algorithm",
        "Action network",
        "Environment network",
        "Differentiable action sampling",
        "Long-range dependencies",
        "Computational graph rewiring",
        "Graph machine learning",
        "State-of-the-art results"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose a novel framework for training graph neural networks...\" and describes this new framework in detail. this directly aligns with the \"technical\" criterion: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n*   the introduction discusses the limitations of the current message-passing paradigm, setting the stage for the proposed solution, which is characteristic of a technical paper.\n*   while the abstract also mentions \"theoretical analysis\" and \"extensive empirical analysis,\" these are presented as supporting elements for the *proposed novel framework*. the core contribution is the framework itself.\n\ntherefore, the primary classification is **technical**."
    },
    "file_name": "46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf"
  },
  {
    "success": true,
    "doc_id": "e82ed009807783fb89eec4054f407b48",
    "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: Existing methods for explaining Graph Neural Network (GNN) predictions primarily focus on identifying relevant subgraphs, but they are not *counterfactual* (CF). This means they don't explain *how* a prediction can be changed to achieve an alternative outcome.\n    *   **Importance and challenge**: Understanding *why* an ML model makes a prediction is crucial for debugging, decision-making, and regulatory compliance. CF explanations provide actionable insights (e.g., identifying minimal changes to a molecule's structure to achieve a desired property in drug discovery). Existing subgraph methods require users to pre-specify the explanation size and are shown to be ineffective at producing valid, accurate CF explanations for GNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**:\n        *   **GNN Explainability**: Most prior work focuses on generating subgraphs relevant to a prediction \\cite{lucic2021p70}, analogous to LIME or SHAP for other data types.\n        *   **Counterfactual Explanations**: Existing CF methods are designed for tabular, image, or text data, perturbing feature values, and are not equipped to handle graph data with relationships (edges).\n        *   **Adversarial Attacks**: While also involving minimal perturbations to change predictions, their intent differs (fooling the model vs. explaining it), and they typically focus on degrading overall model performance rather than explaining individual node predictions.\n    *   **Limitations of previous solutions**:\n        *   Subgraph-generating methods: Not counterfactual in nature; require users to specify the subgraph size in advance; identify relevant but not necessarily *minimal* subgraphs; and are not well-suited for the CF explanation problem \\cite{lucic2021p70}.\n        *   Existing CF methods: Cannot handle graph data, specifically perturbing graph structure (edges).\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: \\cite{lucic2021p70} proposes **CF-GNNExplainer**, a method for generating CF explanations for GNNs by finding the *minimal perturbation* to the input graph (specifically, through edge deletions) such that the GNN's prediction for a given node changes.\n    *   **Novelty**:\n        *   It iteratively removes edges from the original adjacency matrix using matrix sparsification techniques.\n        *   It tracks perturbations that lead to a prediction change and returns the one with the smallest number of edge deletions.\n        *   A \"CF model\" `g` is introduced, based on the original GNN `f`, but parameterized by a binary perturbation matrix `P` (instead of the GNN's weights `W`). `P` sparsifies the adjacency matrix `Av`.\n        *   The method optimizes a loss function `L = L_pred + λL_dist`, where `L_pred` encourages the prediction to change (using negative log-likelihood) and `L_dist` encourages minimality (number of edges removed).\n        *   `P` is derived from an intermediate real-valued matrix `^P` (optimized via gradients) which is then sigmoid-transformed and thresholded to obtain the binary `P`.\n\n*   **Key Technical Contributions**\n    *   Formalization of the problem of generating counterfactual explanations specifically for GNNs.\n    *   **CF-GNNExplainer**: A novel, instance-level method that generates CF explanations for GNNs by minimally perturbing the graph structure (edge deletions).\n    *   Adaptation of matrix sparsification techniques to identify and remove crucial edges for a GNN's prediction.\n    *   Introduction of a \"CF model\" `g` that generates perturbed graph inputs (CF examples) while keeping the underlying GNN model weights constant.\n    *   A comprehensive experimental setup and metrics for evaluating CF explanations in GNNs.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Evaluated on three widely used synthetic datasets for GNN explanation tasks: `tree-cycles`, `tree-grids`, and `ba-shapes`. These datasets provide ground-truth explanations for node classification.\n    *   **Key performance metrics**: Fidelity, explanation size (number of edges removed), sparsity, and accuracy (percentage of instances for which a valid CF explanation is found).\n    *   **Comparison results**:\n        *   CF-GNNExplainer successfully generates CF examples for the *majority* of instances.\n        *   It achieves this by removing *less than 3 edges on average*.\n        *   It demonstrates *at least 94% accuracy* in generating valid CF explanations.\n        *   The paper claims that even adapted existing subgraph-generating methods (by varying the subgraph size `S`) are unable to produce valid and accurate CF explanations, highlighting the superiority of CF-GNNExplainer for this specific problem.\n\n*   **Limitations & Scope**\n    *   **Technical limitations**:\n        *   The method exclusively focuses on *edge deletions* as perturbations, not considering edge additions or node feature modifications.\n        *   It has a time complexity of O(KN^2) (where N is the number of nodes in the subgraph neighborhood and K is the number of iterations), which can be high, though common for local XAI methods.\n    *   **Scope of applicability**:\n        *   Primarily designed and evaluated for *node classification* tasks on GNNs.\n        *   Validation is performed on *synthetic datasets* with known ground-truth explanations, acknowledging the current lack of real-world node classification datasets with such ground truth.\n\n*   **Technical Significance**\n    *   **Advances state-of-the-art**: CF-GNNExplainer is the first method to specifically address counterfactual explanations for GNNs by perturbing graph structure, filling a critical gap in GNN explainability beyond simply identifying relevant subgraphs.\n    *   **Potential impact**:\n        *   Enables more actionable and human-understandable insights into GNN predictions, particularly in applications where minimal structural changes are meaningful (e.g., drug discovery, materials science).\n        *   Shifts the paradigm of GNN explainability from \"what was important for this prediction?\" to \"what needs to change for a different prediction?\".\n        *   The proposed methodology and evaluation framework can serve as a foundation for future research in counterfactual XAI for graph-structured data.\n        *   The ability to achieve high accuracy with very few edge removals suggests it effectively identifies the most critical structural elements influencing a GNN's decision.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel in complex data, yet their black-box nature hinders trust and actionable insights. Current explainability methods often reveal *what* influenced a prediction, but critically fail to answer *how* to alter it for a desired outcome. This absence of *counterfactual explanations* (CF explanations) for GNNs leaves a significant void, particularly where minimal structural changes hold profound implications, such as in drug discovery or materials science.\n\nWe introduce **CF-GNNExplainer**, a pioneering instance-level method that formalizes and addresses this challenge. Our approach uniquely generates CF explanations for GNNs by identifying the *minimal set of edge deletions* required to flip a GNN's prediction for a specific node. Leveraging novel adaptations of *matrix sparsification* techniques, CF-GNNExplainer iteratively prunes the graph's *adjacency matrix*, optimizing for both prediction change and minimality.\n\nExtensive experiments on *node classification* tasks demonstrate CF-GNNExplainer's remarkable efficacy, consistently generating valid CF examples for the majority of instances by removing *fewer than three edges on average* with over 94% accuracy. This work represents a crucial paradigm shift in *GNN Explainable AI (XAI)*, moving beyond mere attribution to provide truly actionable intelligence and paving the way for more interpretable and controllable graph-based machine learning applications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Counterfactual Explanations",
      "CF-GNNExplainer",
      "Minimal Perturbation",
      "Edge Deletions",
      "Matrix Sparsification",
      "GNN Explainability",
      "Graph Structure Perturbation",
      "Node Classification",
      "Loss Function Optimization",
      "High Accuracy (CF Explanations)",
      "Actionable Insights",
      "Synthetic Datasets",
      "Drug Discovery"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/11b9f4729c8e355dec7122993076f6e2788c03c4.pdf",
    "citation_key": "lucic2021p70",
    "metadata": {
      "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
      "authors": [
        "Ana Lucic",
        "Maartje ter Hoeve",
        "Gabriele Tolomei",
        "M. de Rijke",
        "F. Silvestri"
      ],
      "published_date": "2021",
      "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
      "file_path": "paper_data/Graph_Neural_Networks/11b9f4729c8e355dec7122993076f6e2788c03c4.pdf",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: Existing methods for explaining Graph Neural Network (GNN) predictions primarily focus on identifying relevant subgraphs, but they are not *counterfactual* (CF). This means they don't explain *how* a prediction can be changed to achieve an alternative outcome.\n    *   **Importance and challenge**: Understanding *why* an ML model makes a prediction is crucial for debugging, decision-making, and regulatory compliance. CF explanations provide actionable insights (e.g., identifying minimal changes to a molecule's structure to achieve a desired property in drug discovery). Existing subgraph methods require users to pre-specify the explanation size and are shown to be ineffective at producing valid, accurate CF explanations for GNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**:\n        *   **GNN Explainability**: Most prior work focuses on generating subgraphs relevant to a prediction \\cite{lucic2021p70}, analogous to LIME or SHAP for other data types.\n        *   **Counterfactual Explanations**: Existing CF methods are designed for tabular, image, or text data, perturbing feature values, and are not equipped to handle graph data with relationships (edges).\n        *   **Adversarial Attacks**: While also involving minimal perturbations to change predictions, their intent differs (fooling the model vs. explaining it), and they typically focus on degrading overall model performance rather than explaining individual node predictions.\n    *   **Limitations of previous solutions**:\n        *   Subgraph-generating methods: Not counterfactual in nature; require users to specify the subgraph size in advance; identify relevant but not necessarily *minimal* subgraphs; and are not well-suited for the CF explanation problem \\cite{lucic2021p70}.\n        *   Existing CF methods: Cannot handle graph data, specifically perturbing graph structure (edges).\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: \\cite{lucic2021p70} proposes **CF-GNNExplainer**, a method for generating CF explanations for GNNs by finding the *minimal perturbation* to the input graph (specifically, through edge deletions) such that the GNN's prediction for a given node changes.\n    *   **Novelty**:\n        *   It iteratively removes edges from the original adjacency matrix using matrix sparsification techniques.\n        *   It tracks perturbations that lead to a prediction change and returns the one with the smallest number of edge deletions.\n        *   A \"CF model\" `g` is introduced, based on the original GNN `f`, but parameterized by a binary perturbation matrix `P` (instead of the GNN's weights `W`). `P` sparsifies the adjacency matrix `Av`.\n        *   The method optimizes a loss function `L = L_pred + λL_dist`, where `L_pred` encourages the prediction to change (using negative log-likelihood) and `L_dist` encourages minimality (number of edges removed).\n        *   `P` is derived from an intermediate real-valued matrix `^P` (optimized via gradients) which is then sigmoid-transformed and thresholded to obtain the binary `P`.\n\n*   **Key Technical Contributions**\n    *   Formalization of the problem of generating counterfactual explanations specifically for GNNs.\n    *   **CF-GNNExplainer**: A novel, instance-level method that generates CF explanations for GNNs by minimally perturbing the graph structure (edge deletions).\n    *   Adaptation of matrix sparsification techniques to identify and remove crucial edges for a GNN's prediction.\n    *   Introduction of a \"CF model\" `g` that generates perturbed graph inputs (CF examples) while keeping the underlying GNN model weights constant.\n    *   A comprehensive experimental setup and metrics for evaluating CF explanations in GNNs.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Evaluated on three widely used synthetic datasets for GNN explanation tasks: `tree-cycles`, `tree-grids`, and `ba-shapes`. These datasets provide ground-truth explanations for node classification.\n    *   **Key performance metrics**: Fidelity, explanation size (number of edges removed), sparsity, and accuracy (percentage of instances for which a valid CF explanation is found).\n    *   **Comparison results**:\n        *   CF-GNNExplainer successfully generates CF examples for the *majority* of instances.\n        *   It achieves this by removing *less than 3 edges on average*.\n        *   It demonstrates *at least 94% accuracy* in generating valid CF explanations.\n        *   The paper claims that even adapted existing subgraph-generating methods (by varying the subgraph size `S`) are unable to produce valid and accurate CF explanations, highlighting the superiority of CF-GNNExplainer for this specific problem.\n\n*   **Limitations & Scope**\n    *   **Technical limitations**:\n        *   The method exclusively focuses on *edge deletions* as perturbations, not considering edge additions or node feature modifications.\n        *   It has a time complexity of O(KN^2) (where N is the number of nodes in the subgraph neighborhood and K is the number of iterations), which can be high, though common for local XAI methods.\n    *   **Scope of applicability**:\n        *   Primarily designed and evaluated for *node classification* tasks on GNNs.\n        *   Validation is performed on *synthetic datasets* with known ground-truth explanations, acknowledging the current lack of real-world node classification datasets with such ground truth.\n\n*   **Technical Significance**\n    *   **Advances state-of-the-art**: CF-GNNExplainer is the first method to specifically address counterfactual explanations for GNNs by perturbing graph structure, filling a critical gap in GNN explainability beyond simply identifying relevant subgraphs.\n    *   **Potential impact**:\n        *   Enables more actionable and human-understandable insights into GNN predictions, particularly in applications where minimal structural changes are meaningful (e.g., drug discovery, materials science).\n        *   Shifts the paradigm of GNN explainability from \"what was important for this prediction?\" to \"what needs to change for a different prediction?\".\n        *   The proposed methodology and evaluation framework can serve as a foundation for future research in counterfactual XAI for graph-structured data.\n        *   The ability to achieve high accuracy with very few edge removals suggests it effectively identifies the most critical structural elements influencing a GNN's decision.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Counterfactual Explanations",
        "CF-GNNExplainer",
        "Minimal Perturbation",
        "Edge Deletions",
        "Matrix Sparsification",
        "GNN Explainability",
        "Graph Structure Perturbation",
        "Node Classification",
        "Loss Function Optimization",
        "High Accuracy (CF Explanations)",
        "Actionable Insights",
        "Synthetic Datasets",
        "Drug Discovery"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we **propose a method** for generating cf explanations for gnns\" and names the method \"**cf-gnnexplainer**\". it describes the core idea of the method (\"minimal perturbation to the input (graph) data such that the prediction changes\") and then presents results validating this method (\"across three widely used datasets\", \"removing less than 3 edges on average, with at least 94% accuracy\").\n*   **introduction:** sets up a technical problem (lack of counterfactual explanations for gnns) and the need for such explanations, leading into the proposed solution.\n*   **classification criteria match:** this aligns perfectly with the \"technical\" criteria: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\". while it includes empirical evaluation, the central contribution is the *development and proposal of a new method*."
    },
    "file_name": "11b9f4729c8e355dec7122993076f6e2788c03c4.pdf"
  },
  {
    "success": true,
    "doc_id": "cf5dcbe1498e75c6832008ff252bc0aa",
    "summary": "This paper, \"Graph Neural Networks for Graphs with Heterophily: A Survey\" by Zheng et al., provides a comprehensive review of Graph Neural Networks (GNNs) specifically designed to address the challenge of heterophily in graphs \\cite{zheng2022qxr}.\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the significant performance degradation of traditional GNNs when applied to graphs exhibiting heterophily. Most GNNs operate under the homophily assumption (nodes with similar labels are connected), which is violated in many real-world scenarios where nodes with different labels tend to be linked \\cite{zheng2022qxr}.\n    *   **Importance & Challenge**: Heterophily is a ubiquitous graph property (e.g., online transaction networks, molecular networks), making it crucial to develop GNNs capable of handling it. The core challenges for homophilic GNNs in heterophilic settings are:\n        *   **Undiscovered Non-local Neighbors**: Homophilic GNNs' local aggregation fails to explore non-local topology where heterophilic nodes of the same class might be situated, hindering the identification of informative nodes.\n        *   **Indistinguishable Node Representation Learning**: Uniform local neighbor aggregation struggles to capture the discrepancy between similar non-local neighbors and dissimilar local neighbors, leading to non-discriminative node representations \\cite{zheng2022qxr}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that systematically reviews and categorizes existing GNNs developed to tackle heterophily. It positions itself as a foundational resource to organize the rapidly growing body of research in this area \\cite{zheng2022qxr}.\n    *   **Limitations of Previous Solutions (as identified by the survey)**: The primary limitation of *conventional GNNs* is their inherent reliance on the homophily assumption. For *heterophilic GNNs*, the survey identifies a lack of a unified framework and systematic taxonomy to understand and compare diverse methods. It also points to ongoing challenges in interpretability, robustness, scalability, and comprehensive data exploration for existing heterophilic GNNs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper's core technical approach is a systematic review and analysis of existing heterophilic GNN models. It proposes a unified framework for heterophilic GNN design based on three key principles: (P1) Non-locality of Neighbor Sets, (P2) Class Distinguishability, and (P3) Depth Fusion of Multi-layer Information \\cite{zheng2022qxr}.\n    *   **Novelty/Difference (of the survey)**: The main innovation is the introduction of a systematic taxonomy that categorizes existing heterophilic GNN models into three classes: (1) Non-local neighbor extension methods, (2) GNN architecture refinement methods, and (3) Hybrid methods. This taxonomy provides a structured lens for understanding and comparing diverse approaches \\cite{zheng2022qxr}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (from the survey)**:\n        *   **Systematic Taxonomy**: A novel three-fold taxonomy for heterophilic GNNs, providing a structured understanding of the field \\cite{zheng2022qxr}.\n        *   **Unified Framework**: Identification of three key design principles (Non-locality, Class Distinguishability, Depth Fusion) that guide the development of heterophilic GNNs.\n    *   **Theoretical Insights or Analysis (from the survey)**:\n        *   **Correlation Analysis**: A thorough discussion on the intricate correlations between graph heterophily and other critical graph research domains, including model robustness, over-smoothing, and graph anomaly detection \\cite{zheng2022qxr}.\n        *   **Heterophily Measurement Review**: A detailed review and comparison of various metrics for quantifying graph heterophily and homophily (e.g., node homophily, edge homophily, adjusted homophily, label informativeness), highlighting their respective characteristics and suitability.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work does not present its own experimental validation of novel GNN models. Instead, it synthesizes and discusses the empirical findings from the literature it reviews.\n    *   The paper indicates that \"More details of real-world heterophilic graph dataset benchmarks, open-source codes, and the overall development timeline of heterophilic GNNs can be found in the appendix\" \\cite{zheng2022qxr}, implying a discussion of common datasets, metrics, and results from the surveyed papers.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of existing heterophilic GNNs, as identified by the survey)**: The paper highlights several open challenges for current heterophilic GNNs, including limitations in interpretability, robustness, scalability, and the need for more comprehensive heterophilic graph data exploration \\cite{zheng2022qxr}.\n    *   **Scope of Applicability**: The survey focuses specifically on GNNs designed for graphs exhibiting heterophily, primarily within the context of semi-supervised node classification, which is identified as the main learning task.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first comprehensive and systematic review of GNNs for heterophilic graphs. It structures a rapidly evolving field, making it more accessible and understandable for researchers \\cite{zheng2022qxr}.\n    *   **Potential Impact on Future Research**:\n        *   **Guidance for Model Development**: The proposed taxonomy and design principles offer a clear framework for developing more effective heterophilic GNNs.\n        *   **Identification of Research Gaps**: By discussing limitations and future directions, the paper highlights critical open problems and stimulates new research avenues in areas like interpretability, robustness, scalability, and data exploration \\cite{zheng2022qxr}.\n        *   **Interdisciplinary Connections**: The discussion on correlations between heterophily and other graph domains encourages cross-domain research and broader applicability of heterophilic GNNs.",
    "intriguing_abstract": "The pervasive challenge of heterophily, where dissimilar nodes frequently connect, fundamentally undermines the performance of conventional Graph Neural Networks (GNNs) designed under the homophily assumption. As real-world graphs increasingly defy this premise, developing robust GNNs for heterophilic settings has become paramount. This survey provides the first comprehensive and systematic review of GNNs specifically engineered to overcome heterophily. We propose a novel three-fold taxonomy categorizing existing models into non-local neighbor extension, GNN architecture refinement, and hybrid methods. Furthermore, we introduce a unified design framework grounded in three key principles: Non-locality of Neighbor Sets, Class Distinguishability, and Depth Fusion of Multi-layer Information. Beyond model classification, we delve into the intricate correlations between heterophily and critical graph research domains like robustness, over-smoothing, and anomaly detection, alongside a thorough review of heterophily measurement metrics. This work not only structures a rapidly evolving field but also identifies crucial open challenges in interpretability, scalability, and data exploration. By offering a foundational resource, this survey aims to guide the development of next-generation heterophilic GNNs, fostering innovation and expanding their applicability across diverse real-world scenarios.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Heterophily",
      "Homophily assumption",
      "Heterophilic GNNs",
      "Systematic survey",
      "Unified framework",
      "Taxonomy",
      "Design principles",
      "Node representation learning",
      "Non-local neighbor extension",
      "Heterophily measurement",
      "Interpretability",
      "Robustness",
      "Scalability",
      "Semi-supervised node classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf",
    "citation_key": "zheng2022qxr",
    "metadata": {
      "title": "Graph Neural Networks for Graphs with Heterophily: A Survey",
      "authors": [
        "Xin Zheng",
        "Yixin Liu",
        "Shirui Pan",
        "Miao Zhang",
        "Di Jin",
        "Philip S. Yu"
      ],
      "published_date": "2022",
      "abstract": "Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we discuss the correlation between graph heterophily and various graph research domains, aiming to facilitate the development of more effective GNNs across a spectrum of practical applications and learning tasks in the graph research community. In the end, we point out the potential directions to advance and stimulate more future research and applications on heterophilic graph learning with GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper, \"Graph Neural Networks for Graphs with Heterophily: A Survey\" by Zheng et al., provides a comprehensive review of Graph Neural Networks (GNNs) specifically designed to address the challenge of heterophily in graphs \\cite{zheng2022qxr}.\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the significant performance degradation of traditional GNNs when applied to graphs exhibiting heterophily. Most GNNs operate under the homophily assumption (nodes with similar labels are connected), which is violated in many real-world scenarios where nodes with different labels tend to be linked \\cite{zheng2022qxr}.\n    *   **Importance & Challenge**: Heterophily is a ubiquitous graph property (e.g., online transaction networks, molecular networks), making it crucial to develop GNNs capable of handling it. The core challenges for homophilic GNNs in heterophilic settings are:\n        *   **Undiscovered Non-local Neighbors**: Homophilic GNNs' local aggregation fails to explore non-local topology where heterophilic nodes of the same class might be situated, hindering the identification of informative nodes.\n        *   **Indistinguishable Node Representation Learning**: Uniform local neighbor aggregation struggles to capture the discrepancy between similar non-local neighbors and dissimilar local neighbors, leading to non-discriminative node representations \\cite{zheng2022qxr}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that systematically reviews and categorizes existing GNNs developed to tackle heterophily. It positions itself as a foundational resource to organize the rapidly growing body of research in this area \\cite{zheng2022qxr}.\n    *   **Limitations of Previous Solutions (as identified by the survey)**: The primary limitation of *conventional GNNs* is their inherent reliance on the homophily assumption. For *heterophilic GNNs*, the survey identifies a lack of a unified framework and systematic taxonomy to understand and compare diverse methods. It also points to ongoing challenges in interpretability, robustness, scalability, and comprehensive data exploration for existing heterophilic GNNs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper's core technical approach is a systematic review and analysis of existing heterophilic GNN models. It proposes a unified framework for heterophilic GNN design based on three key principles: (P1) Non-locality of Neighbor Sets, (P2) Class Distinguishability, and (P3) Depth Fusion of Multi-layer Information \\cite{zheng2022qxr}.\n    *   **Novelty/Difference (of the survey)**: The main innovation is the introduction of a systematic taxonomy that categorizes existing heterophilic GNN models into three classes: (1) Non-local neighbor extension methods, (2) GNN architecture refinement methods, and (3) Hybrid methods. This taxonomy provides a structured lens for understanding and comparing diverse approaches \\cite{zheng2022qxr}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (from the survey)**:\n        *   **Systematic Taxonomy**: A novel three-fold taxonomy for heterophilic GNNs, providing a structured understanding of the field \\cite{zheng2022qxr}.\n        *   **Unified Framework**: Identification of three key design principles (Non-locality, Class Distinguishability, Depth Fusion) that guide the development of heterophilic GNNs.\n    *   **Theoretical Insights or Analysis (from the survey)**:\n        *   **Correlation Analysis**: A thorough discussion on the intricate correlations between graph heterophily and other critical graph research domains, including model robustness, over-smoothing, and graph anomaly detection \\cite{zheng2022qxr}.\n        *   **Heterophily Measurement Review**: A detailed review and comparison of various metrics for quantifying graph heterophily and homophily (e.g., node homophily, edge homophily, adjusted homophily, label informativeness), highlighting their respective characteristics and suitability.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work does not present its own experimental validation of novel GNN models. Instead, it synthesizes and discusses the empirical findings from the literature it reviews.\n    *   The paper indicates that \"More details of real-world heterophilic graph dataset benchmarks, open-source codes, and the overall development timeline of heterophilic GNNs can be found in the appendix\" \\cite{zheng2022qxr}, implying a discussion of common datasets, metrics, and results from the surveyed papers.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of existing heterophilic GNNs, as identified by the survey)**: The paper highlights several open challenges for current heterophilic GNNs, including limitations in interpretability, robustness, scalability, and the need for more comprehensive heterophilic graph data exploration \\cite{zheng2022qxr}.\n    *   **Scope of Applicability**: The survey focuses specifically on GNNs designed for graphs exhibiting heterophily, primarily within the context of semi-supervised node classification, which is identified as the main learning task.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first comprehensive and systematic review of GNNs for heterophilic graphs. It structures a rapidly evolving field, making it more accessible and understandable for researchers \\cite{zheng2022qxr}.\n    *   **Potential Impact on Future Research**:\n        *   **Guidance for Model Development**: The proposed taxonomy and design principles offer a clear framework for developing more effective heterophilic GNNs.\n        *   **Identification of Research Gaps**: By discussing limitations and future directions, the paper highlights critical open problems and stimulates new research avenues in areas like interpretability, robustness, scalability, and data exploration \\cite{zheng2022qxr}.\n        *   **Interdisciplinary Connections**: The discussion on correlations between heterophily and other graph domains encourages cross-domain research and broader applicability of heterophilic GNNs.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Heterophily",
        "Homophily assumption",
        "Heterophilic GNNs",
        "Systematic survey",
        "Unified framework",
        "Taxonomy",
        "Design principles",
        "Node representation learning",
        "Non-local neighbor extension",
        "Heterophily measurement",
        "Interpretability",
        "Robustness",
        "Scalability",
        "Semi-supervised node classification"
      ],
      "paper_type": "based on the abstract and introduction:\n\nthe paper explicitly states its purpose multiple times:\n*   \"in this paper, we provide a **comprehensive review** of gnns for heterophilic graphs.\" (abstract)\n*   \"specifically, we propose a **systematic taxonomy** that essentially governs existing heterophilic gnn models, along with a general **summary and detailed analysis**.\" (abstract)\n*   \"furthermore, we **discuss the correlation** between graph heterophily and various graph research domains...\" (abstract)\n*   \"in the end, we point out the **potential directions** to advance and stimulate more future research...\" (abstract)\n*   \"in this paper, we present a **comprehensive and systematic review** of gnns for heterophilic graphs, aiming to provide a general blueprint of heterophilic graph research.\" (introduction)\n*   the \"contributions\" section lists: \"comprehensive overview\", \"systematic taxonomy\", \"thorough discussion\", and \"future directions\".\n*   section 2 explicitly states \"notations used in this **survey**.\"\n\nthese phrases directly match the criteria for a **survey** paper:\n*   **survey** - reviews existing literature comprehensively\n    *   abstract mentions: \"survey\", \"review\", \"comprehensive analysis\", \"state-of-the-art\"\n    *   introduction discusses: literature organization, classification schemes\n\ntherefore, the paper is a:\n\n**survey**"
    },
    "file_name": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf"
  },
  {
    "success": true,
    "doc_id": "f8ed2e9494e4dc978842ad03046aa4a1",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Unnoticeable Backdoor Attacks on Graph Neural Networks \\cite{dai2023tuj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Existing backdoor attacks on Graph Neural Networks (GNNs) are ineffective under limited attack budgets and their injected triggers are easily detectable and mitigable \\cite{dai2023tuj}. This paper addresses the novel problem of developing *unnoticeable* graph backdoor attacks with *limited attack budget* \\cite{dai2023tuj}.\n    *   **Why important and challenging:**\n        *   GNNs are vulnerable to adversarial attacks, but traditional graph manipulation attacks are computationally expensive (O(N^2)) and impractical for large graphs or when attackers cannot control existing nodes \\cite{dai2023tuj}.\n        *   Backdoor attacks offer an efficient alternative, but prior efforts \\cite{zhang2021subgraph, xi2022graph} have two critical unnoticeability flaws: (i) they require a large number of poisoned nodes to be effective, increasing the risk of detection; and (ii) their triggers often violate graph homophily (similar nodes connect), making them easily identifiable and removable by simple pruning strategies \\cite{dai2023tuj}.\n        *   The challenge lies in simultaneously achieving high attack success rates with minimal poisoned nodes and generating triggers that blend seamlessly into the graph structure to evade detection \\cite{dai2023tuj}.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches:**\n        *   **Graph manipulation attacks:** Focus on perturbing graph structures/node features (e.g., Nettack \\cite{zhao2020graph}) but are computationally intensive and impractical for large-scale graphs \\cite{dai2023tuj}.\n        *   **Prior graph backdoor attacks:**\n            *   SBA \\cite{zhang2021subgraph}: Injects randomly generated subgraphs as universal triggers \\cite{dai2023tuj}.\n            *   GTA \\cite{xi2022graph}: Employs a trigger generator to learn sample-specific adaptive triggers \\cite{dai2023tuj}.\n            *   Other works consider node selection based on centrality \\cite{sheng2022graph} or unnoticeability by not changing labels \\cite{xu2022improving} \\cite{dai2023tuj}.\n    *   **Limitations of previous solutions:**\n        *   Empirical analysis in \\cite{dai2023tuj} shows that existing backdoor methods (SBA-Gene, SBA-Samp, GTA) require a large budget of poisoned nodes to achieve effective attack success rates, making them easily noticeable \\cite{dai2023tuj}.\n        *   Their generated triggers can be easily detected and mitigated. Specifically, they often violate the homophily property of real-world graphs, allowing simple defense strategies like pruning edges with low cosine similarity (between trigger and poisoned nodes, or within the trigger) to drastically reduce attack success rates with minimal impact on clean accuracy \\cite{dai2023tuj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method (UGBA - Unnoticeable Graph Backdoor Attack):**\n        *   **Deliberate Poisoned Node Selection:** UGBA proposes a novel poisoned node selection algorithm to identify \"crucial representative nodes\" for trigger injection, rather than random selection \\cite{dai2023tuj}. This strategy aims to maximize the utilization of a limited attack budget by avoiding redundant poisoning (e.g., nodes from the same cluster) \\cite{dai2023tuj}.\n        *   **Adaptive Unnoticeable Trigger Generation:** An adaptive trigger generator is deployed to create triggers that are both powerful and difficult to detect \\cite{dai2023tuj}. This generator is designed to ensure:\n            *   High feature similarity between the generated trigger and the attached poisoned/target nodes \\cite{dai2023tuj}.\n            *   Homophily within the trigger structure itself, meaning edges within the trigger and between the trigger and the poisoned node should link similar nodes. This is formalized by a constraint: `min (u,v) in E_i^B sim(u,v) >= T`, where `sim` is cosine similarity and `T` is a high threshold \\cite{dai2023tuj}.\n    *   **Novelty/Difference:** UGBA is novel in its integrated approach to simultaneously address both limited attack budget and trigger unnoticeability \\cite{dai2023tuj}. It innovates by combining a strategic, non-random poisoned node selection with an adaptive trigger generation mechanism that explicitly enforces homophily-like properties to evade detection, a feature lacking in prior adaptive trigger methods \\cite{dai2023tuj}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation:** First to explicitly study and address the problem of promoting unnoticeability in graph backdoor attacks, considering both the characteristics of generated triggers and the efficiency of attack budget utilization \\cite{dai2023tuj}.\n    *   **Empirical Verification of Vulnerabilities:** Provided empirical evidence that existing graph backdoor attacks are highly susceptible to simple, yet effective, defense strategies based on homophily violations (edge pruning and label discarding) \\cite{dai2023tuj}.\n    *   **Novel Algorithms/Methods:**\n        *   A novel poisoned node selection algorithm for efficient utilization of limited attack budget \\cite{dai2023tuj}.\n        *   An adaptive trigger generator capable of learning effective and unnoticeable triggers that maintain high similarity with target nodes and adhere to homophily principles \\cite{dai2023tuj}.\n    *   **System Design:** The UGBA framework integrates these two novel components to achieve effective and unnoticeable attacks under budget constraints \\cite{dai2023tuj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:**\n        *   Analyzed the impact of the number of poisoned nodes (|V_P|) on the Attack Success Rate (ASR) of state-of-the-art backdoor attacks (SBA-Gene, SBA-Samp, GTA) \\cite{dai2023tuj}.\n        *   Evaluated two defense strategies (\"Prune\" and \"Prune+LD\" – Prune + Label Discarding) against existing backdoor attacks, measuring ASR and clean accuracy \\cite{dai2023tuj}.\n        *   Extensive experiments on real-world datasets (e.g., OGB-arxiv) against various GNN models (e.g., GraphSage) and defense strategies to demonstrate UGBA's effectiveness \\cite{dai2023tuj}.\n    *   **Key performance metrics and comparison results:**\n        *   **ASR:** Existing methods showed very low ASR (e.g., <10% for SBA-Samp/Gen, <40% for GTA) with limited budgets (|V_P|=80 or 240) on OGB-arxiv, confirming budget inefficiency \\cite{dai2023tuj}.\n        *   **Defense Effectiveness:** \"Prune\" and \"Prune+LD\" strategies drastically reduced ASR for existing attacks (e.g., GTA ASR dropped from 94.8% to 0.04% with Prune+LD) while negligibly affecting clean accuracy, confirming the detectability of existing triggers \\cite{dai2023tuj}.\n        *   UGBA is shown to achieve effective backdoor attacks while being unnoticeable and operating under limited attack budgets \\cite{dai2023tuj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:**\n        *   The work primarily focuses on semi-supervised node classification in the inductive setting \\cite{dai2023tuj}.\n        *   The attacker is assumed to have access to training data, can attach triggers and labels within a budget during training, and can attach triggers during inference. The target GNN model architecture is unknown to the attacker \\cite{dai2023tuj}.\n        *   The unnoticeability constraint relies on cosine similarity and a tunable threshold `T` for feature similarity \\cite{dai2023tuj}.\n    *   **Scope of applicability:** Most relevant for GNNs operating on graphs exhibiting homophily, particularly in inductive node classification scenarios where new nodes are introduced \\cite{dai2023tuj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:**\n        *   Elevates the discussion of graph backdoor attacks beyond mere effectiveness to crucial real-world considerations of unnoticeability and budget efficiency \\cite{dai2023tuj}.\n        *   Introduces novel, structurally-aware attack mechanisms (deliberate node selection and homophily-enforcing trigger generation) that significantly enhance the stealth and resource efficiency of graph backdoor attacks \\cite{dai2023tuj}.\n        *   Provides a strong empirical foundation by demonstrating the vulnerabilities of existing attacks to simple defenses, thereby establishing a new baseline for robust attack and defense research \\cite{dai2023tuj}.\n    *   **Potential impact on future research:**\n        *   Will likely stimulate further research into more sophisticated unnoticeable attack strategies and, critically, more robust and adaptive defense mechanisms against such stealthy attacks on GNNs \\cite{dai2023tuj}.\n        *   Emphasizes the importance of considering graph structural properties (like homophily) in the design of both attacks and defenses for GNNs \\cite{dai2023tuj}.\n        *   Encourages the development of advanced poisoned node selection algorithms and trigger generation techniques that can operate effectively under severe budget constraints \\cite{dai2023tuj}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) face a critical, yet often underestimated, threat: stealthy backdoor attacks. While prior methods demonstrate GNN vulnerability, they are largely ineffective under limited attack budgets and easily detected due to their conspicuous triggers that violate graph homophily. We introduce UGBA (Unnoticeable Graph Backdoor Attack), a novel framework that fundamentally redefines the landscape of GNN adversarial attacks by achieving unprecedented stealth and efficiency. UGBA pioneers a deliberate poisoned node selection algorithm, strategically identifying crucial representative nodes to maximize attack efficacy with minimal budget. Crucially, it employs an an adaptive trigger generator that crafts triggers exhibiting high feature similarity with attached nodes and strictly adhering to graph homophily, rendering them virtually indistinguishable from benign graph structures. Our extensive empirical analysis demonstrates that state-of-the-art defenses, effective against prior attacks by exploiting homophily violations, are rendered impotent against UGBA. This work not only exposes a significant new threat vector in GNN security but also establishes a new benchmark for truly unnoticeable adversarial attacks, compelling the development of more sophisticated and robust defense mechanisms against this evolving form of graph manipulation.",
    "keywords": [
      "Unnoticeable Graph Backdoor Attacks",
      "Graph Neural Networks (GNNs)",
      "Limited Attack Budget",
      "Graph Homophily",
      "Poisoned Node Selection",
      "Adaptive Trigger Generation",
      "Semi-supervised Node Classification",
      "Trigger Detectability",
      "GNN Vulnerabilities",
      "Defense Strategies",
      "Attack Success Rate (ASR)"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf",
    "citation_key": "dai2023tuj",
    "metadata": {
      "title": "Unnoticeable Backdoor Attacks on Graph Neural Networks",
      "authors": [
        "Enyan Dai",
        "M. Lin",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "published_date": "2023",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.",
      "file_path": "paper_data/Graph_Neural_Networks/530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf",
      "venue": "The Web Conference",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Unnoticeable Backdoor Attacks on Graph Neural Networks \\cite{dai2023tuj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Existing backdoor attacks on Graph Neural Networks (GNNs) are ineffective under limited attack budgets and their injected triggers are easily detectable and mitigable \\cite{dai2023tuj}. This paper addresses the novel problem of developing *unnoticeable* graph backdoor attacks with *limited attack budget* \\cite{dai2023tuj}.\n    *   **Why important and challenging:**\n        *   GNNs are vulnerable to adversarial attacks, but traditional graph manipulation attacks are computationally expensive (O(N^2)) and impractical for large graphs or when attackers cannot control existing nodes \\cite{dai2023tuj}.\n        *   Backdoor attacks offer an efficient alternative, but prior efforts \\cite{zhang2021subgraph, xi2022graph} have two critical unnoticeability flaws: (i) they require a large number of poisoned nodes to be effective, increasing the risk of detection; and (ii) their triggers often violate graph homophily (similar nodes connect), making them easily identifiable and removable by simple pruning strategies \\cite{dai2023tuj}.\n        *   The challenge lies in simultaneously achieving high attack success rates with minimal poisoned nodes and generating triggers that blend seamlessly into the graph structure to evade detection \\cite{dai2023tuj}.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches:**\n        *   **Graph manipulation attacks:** Focus on perturbing graph structures/node features (e.g., Nettack \\cite{zhao2020graph}) but are computationally intensive and impractical for large-scale graphs \\cite{dai2023tuj}.\n        *   **Prior graph backdoor attacks:**\n            *   SBA \\cite{zhang2021subgraph}: Injects randomly generated subgraphs as universal triggers \\cite{dai2023tuj}.\n            *   GTA \\cite{xi2022graph}: Employs a trigger generator to learn sample-specific adaptive triggers \\cite{dai2023tuj}.\n            *   Other works consider node selection based on centrality \\cite{sheng2022graph} or unnoticeability by not changing labels \\cite{xu2022improving} \\cite{dai2023tuj}.\n    *   **Limitations of previous solutions:**\n        *   Empirical analysis in \\cite{dai2023tuj} shows that existing backdoor methods (SBA-Gene, SBA-Samp, GTA) require a large budget of poisoned nodes to achieve effective attack success rates, making them easily noticeable \\cite{dai2023tuj}.\n        *   Their generated triggers can be easily detected and mitigated. Specifically, they often violate the homophily property of real-world graphs, allowing simple defense strategies like pruning edges with low cosine similarity (between trigger and poisoned nodes, or within the trigger) to drastically reduce attack success rates with minimal impact on clean accuracy \\cite{dai2023tuj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method (UGBA - Unnoticeable Graph Backdoor Attack):**\n        *   **Deliberate Poisoned Node Selection:** UGBA proposes a novel poisoned node selection algorithm to identify \"crucial representative nodes\" for trigger injection, rather than random selection \\cite{dai2023tuj}. This strategy aims to maximize the utilization of a limited attack budget by avoiding redundant poisoning (e.g., nodes from the same cluster) \\cite{dai2023tuj}.\n        *   **Adaptive Unnoticeable Trigger Generation:** An adaptive trigger generator is deployed to create triggers that are both powerful and difficult to detect \\cite{dai2023tuj}. This generator is designed to ensure:\n            *   High feature similarity between the generated trigger and the attached poisoned/target nodes \\cite{dai2023tuj}.\n            *   Homophily within the trigger structure itself, meaning edges within the trigger and between the trigger and the poisoned node should link similar nodes. This is formalized by a constraint: `min (u,v) in E_i^B sim(u,v) >= T`, where `sim` is cosine similarity and `T` is a high threshold \\cite{dai2023tuj}.\n    *   **Novelty/Difference:** UGBA is novel in its integrated approach to simultaneously address both limited attack budget and trigger unnoticeability \\cite{dai2023tuj}. It innovates by combining a strategic, non-random poisoned node selection with an adaptive trigger generation mechanism that explicitly enforces homophily-like properties to evade detection, a feature lacking in prior adaptive trigger methods \\cite{dai2023tuj}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation:** First to explicitly study and address the problem of promoting unnoticeability in graph backdoor attacks, considering both the characteristics of generated triggers and the efficiency of attack budget utilization \\cite{dai2023tuj}.\n    *   **Empirical Verification of Vulnerabilities:** Provided empirical evidence that existing graph backdoor attacks are highly susceptible to simple, yet effective, defense strategies based on homophily violations (edge pruning and label discarding) \\cite{dai2023tuj}.\n    *   **Novel Algorithms/Methods:**\n        *   A novel poisoned node selection algorithm for efficient utilization of limited attack budget \\cite{dai2023tuj}.\n        *   An adaptive trigger generator capable of learning effective and unnoticeable triggers that maintain high similarity with target nodes and adhere to homophily principles \\cite{dai2023tuj}.\n    *   **System Design:** The UGBA framework integrates these two novel components to achieve effective and unnoticeable attacks under budget constraints \\cite{dai2023tuj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:**\n        *   Analyzed the impact of the number of poisoned nodes (|V_P|) on the Attack Success Rate (ASR) of state-of-the-art backdoor attacks (SBA-Gene, SBA-Samp, GTA) \\cite{dai2023tuj}.\n        *   Evaluated two defense strategies (\"Prune\" and \"Prune+LD\" – Prune + Label Discarding) against existing backdoor attacks, measuring ASR and clean accuracy \\cite{dai2023tuj}.\n        *   Extensive experiments on real-world datasets (e.g., OGB-arxiv) against various GNN models (e.g., GraphSage) and defense strategies to demonstrate UGBA's effectiveness \\cite{dai2023tuj}.\n    *   **Key performance metrics and comparison results:**\n        *   **ASR:** Existing methods showed very low ASR (e.g., <10% for SBA-Samp/Gen, <40% for GTA) with limited budgets (|V_P|=80 or 240) on OGB-arxiv, confirming budget inefficiency \\cite{dai2023tuj}.\n        *   **Defense Effectiveness:** \"Prune\" and \"Prune+LD\" strategies drastically reduced ASR for existing attacks (e.g., GTA ASR dropped from 94.8% to 0.04% with Prune+LD) while negligibly affecting clean accuracy, confirming the detectability of existing triggers \\cite{dai2023tuj}.\n        *   UGBA is shown to achieve effective backdoor attacks while being unnoticeable and operating under limited attack budgets \\cite{dai2023tuj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:**\n        *   The work primarily focuses on semi-supervised node classification in the inductive setting \\cite{dai2023tuj}.\n        *   The attacker is assumed to have access to training data, can attach triggers and labels within a budget during training, and can attach triggers during inference. The target GNN model architecture is unknown to the attacker \\cite{dai2023tuj}.\n        *   The unnoticeability constraint relies on cosine similarity and a tunable threshold `T` for feature similarity \\cite{dai2023tuj}.\n    *   **Scope of applicability:** Most relevant for GNNs operating on graphs exhibiting homophily, particularly in inductive node classification scenarios where new nodes are introduced \\cite{dai2023tuj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:**\n        *   Elevates the discussion of graph backdoor attacks beyond mere effectiveness to crucial real-world considerations of unnoticeability and budget efficiency \\cite{dai2023tuj}.\n        *   Introduces novel, structurally-aware attack mechanisms (deliberate node selection and homophily-enforcing trigger generation) that significantly enhance the stealth and resource efficiency of graph backdoor attacks \\cite{dai2023tuj}.\n        *   Provides a strong empirical foundation by demonstrating the vulnerabilities of existing attacks to simple defenses, thereby establishing a new baseline for robust attack and defense research \\cite{dai2023tuj}.\n    *   **Potential impact on future research:**\n        *   Will likely stimulate further research into more sophisticated unnoticeable attack strategies and, critically, more robust and adaptive defense mechanisms against such stealthy attacks on GNNs \\cite{dai2023tuj}.\n        *   Emphasizes the importance of considering graph structural properties (like homophily) in the design of both attacks and defenses for GNNs \\cite{dai2023tuj}.\n        *   Encourages the development of advanced poisoned node selection algorithms and trigger generation techniques that can operate effectively under severe budget constraints \\cite{dai2023tuj}.",
      "keywords": [
        "Unnoticeable Graph Backdoor Attacks",
        "Graph Neural Networks (GNNs)",
        "Limited Attack Budget",
        "Graph Homophily",
        "Poisoned Node Selection",
        "Adaptive Trigger Generation",
        "Semi-supervised Node Classification",
        "Trigger Detectability",
        "GNN Vulnerabilities",
        "Defense Strategies",
        "Attack Success Rate (ASR)"
      ],
      "paper_type": "based on the abstract and introduction, this paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:**\n    *   \"we **study a novel problem** of unnoticeable graph backdoor attacks with limited attack budget.\" (identifies a technical problem)\n    *   \"we **propose** to deliberately select the nodes to inject triggers...\" (directly indicates a new method/approach)\n    *   \"an adaptive trigger generator is deployed to obtain effective triggers...\" (describes a component of their proposed system/method)\n    *   \"extensive experiments... demonstrate the effectiveness of **our proposed method**...\" (confirms the existence and evaluation of a new method)\n*   **introduction discusses:**\n    *   the vulnerability of gnns to adversarial attacks.\n    *   critiques existing attack methods for their limitations (e.g., \"require a large attack budget\", \"easily detected and pruned\", \"unaffordable time and space complexity\"). this sets the stage for their *new, improved technical solution*.\n\nthe paper's core contribution is the development and presentation of a new method for unnoticeable backdoor attacks on gnns, which is then validated empirically. the empirical analysis serves to demonstrate the effectiveness of their proposed technical solution."
    },
    "file_name": "530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf"
  },
  {
    "success": true,
    "doc_id": "89ebd0d68282c98bf5aaba4e32e1baa8",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Accurately forecasting the evolution patterns of complex spatio-temporal data in urban environments. This involves capturing intricate spatial correlations (especially on non-Euclidean structures like road networks) and dynamic temporal dependencies simultaneously.\n    *   **Importance & Challenge**: This problem is crucial for intelligent management decisions in smart cities (e.g., transportation, environment, public safety, healthcare) and for enabling advanced technologies like digital twin cities. It is challenging because urban spatio-temporal data exhibits complex correlation and heterogeneity, making traditional feature engineering difficult. Furthermore, the non-Euclidean nature of many urban systems (e.g., vehicle flows over road networks) limits the direct applicability of grid-based deep learning methods like CNNs and RNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Spatio-Temporal Graph Neural Networks (STGNNs) are proposed as an advancement over traditional statistical methods (e.g., SVR, Random Forest, GBDT) and earlier deep learning models (e.g., hybrid CNN/RNN like ConvLSTM, PredRNN).\n    *   **Limitations of Previous Solutions**: Traditional statistical methods are less effective due to the complex, correlated, and heterogeneous nature of urban spatio-temporal data. Hybrid CNN/RNN models, while an improvement, are fundamentally limited by their inability to directly learn from non-Euclidean data structures prevalent in urban systems (e.g., road networks, urban knowledge graphs).\n    *   **Positioning of *This Survey***: This paper \\cite{jin2023e18} distinguishes itself as the first comprehensive survey systematically reviewing recent studies that use STGNNs specifically for *predictive learning in urban computing*. It covers both application and methodology perspectives, including recent advancements and future directions, unlike prior surveys that were older, domain-specific (e.g., only traffic), or too broad.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: STGNNs integrate Graph Neural Networks (GNNs) with various temporal learning methods (e.g., RNNs, CNNs, attention mechanisms) to simultaneously capture complex spatial dependencies and dynamic temporal patterns in non-Euclidean spatio-temporal data.\n    *   **Novelty/Difference**: The core innovation of STGNNs lies in their ability to directly process and learn from graph-structured data, which is ubiquitous in urban systems (e.g., road networks, sensor networks). This overcomes the limitations of traditional deep learning methods that assume Euclidean data structures, by combining graph convolution operations for spatial feature extraction with temporal sequence modeling techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (of STGNNs)**:\n        *   **Spatio-Temporal Graph Construction**: Methods for building graphs from urban data, including topology-based (e.g., road networks), distance-based (e.g., Gaussian radial basis), similarity-based (e.g., Pearson Correlation Coefficient, Dynamic Time Warping), and interaction-based (e.g., flow between nodes) approaches. The survey also mentions adaptive learning for implicit relations.\n        *   **Integrated Learning**: The fundamental concept of integrating GNNs for spatial feature extraction with temporal models (RNNs, CNNs, attention) for sequence learning.\n    *   **Contributions *of this survey***:\n        *   Provides a systematic taxonomy and comprehensive review of STGNNs for predictive learning in urban computing.\n        *   Categorizes primary application domains (transportation, environment, safety, public health) and specific predictive learning tasks (e.g., traffic state/demand/incident/travel time/trajectory prediction, air quality prediction).\n        *   Analyzes spatio-temporal graph construction methods and fundamental/advanced STGNN architectures.\n        *   Identifies limitations of current STGNN research and proposes future directions.\n\n*   **Experimental Validation**\n    *   This paper \\cite{jin2023e18} is a survey and does not present its own experimental validation.\n    *   However, it extensively discusses the application domains where STGNNs have been empirically validated by other research. These include:\n        *   **Transportation**: Network-based and region-based traffic state prediction (flow, speed), traffic demand prediction (origin, destination, OD matrices for taxi, rail, bike-sharing), traffic incident prediction, travel time prediction, and trajectory prediction for humans and vehicles.\n        *   **Environment**: Air quality prediction (e.g., AQI, PM2.5, emissions).\n    *   The survey highlights that STGNNs have shown \"great promise\" and \"significant advantages\" in these tasks, implying superior performance over previous methods, typically demonstrated through standard predictive metrics on real-world urban datasets.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of STGNNs, as identified by the survey)**: The survey concludes by highlighting limitations of existing STGNN research, which include challenges related to scalability, dynamic graph learning, interpretability, robustness, and data scarcity, among others.\n    *   **Scope of Applicability (of STGNNs)**: STGNNs are primarily applicable to predictive learning tasks involving spatio-temporal data that can be represented as graphs, particularly in urban computing contexts where non-Euclidean spatial relationships and dynamic temporal patterns are crucial.\n    *   **Scope of *This Survey***: The survey focuses specifically on STGNNs for *predictive learning* in *urban computing*, covering spatio-temporal graph construction, prevalent deep learning architectures, primary application domains, design scrutiny, integration with advanced technologies, and future research directions.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: STGNNs significantly advance the technical state-of-the-art by providing a powerful and flexible framework to model complex spatio-temporal dependencies in non-Euclidean urban data. This capability addresses a major limitation of previous statistical and deep learning methods, enabling more accurate and robust predictions in dynamic urban environments.\n    *   **Potential Impact on Future Research**: STGNNs open new avenues for developing more sophisticated urban intelligence systems, leading to improved traffic management, environmental monitoring, public safety, and resource allocation. The survey itself aims to guide future research by systematically identifying current limitations and suggesting promising directions, fostering further innovation in areas like adaptive graph learning, explainability, and handling data heterogeneity.",
    "intriguing_abstract": "Unlocking the predictive power of urban intelligence hinges on accurately forecasting complex spatio-temporal phenomena across dynamic, non-Euclidean environments like road networks. Traditional deep learning methods falter here, struggling to capture intricate spatial correlations and evolving temporal dependencies simultaneously. This paper presents the first comprehensive survey systematically reviewing **Spatio-Temporal Graph Neural Networks (STGNNs)** as the transformative paradigm for **predictive learning in urban computing**.\n\nWe delve into how STGNNs integrate **Graph Neural Networks** with advanced temporal models (RNNs, CNNs, attention mechanisms) to robustly process graph-structured urban data, overcoming the limitations of Euclidean assumptions. This survey offers a meticulous taxonomy of STGNN architectures, explores diverse spatio-temporal graph construction methods, and categorizes their profound impact across critical application domains, including transportation, environmental monitoring, and public safety. By scrutinizing current research limitations and charting promising future directions, this work serves as an indispensable guide for researchers, propelling the development of more intelligent, resilient **smart cities** and **digital twin** initiatives. Discover how STGNNs are redefining urban forecasting.",
    "keywords": [
      "Spatio-Temporal Graph Neural Networks (STGNNs)",
      "Urban Computing",
      "Predictive Learning",
      "Non-Euclidean Spatio-Temporal Data",
      "Graph Convolution",
      "Dynamic Temporal Dependencies",
      "Spatio-Temporal Graph Construction",
      "Smart Cities",
      "Traffic Prediction",
      "Air Quality Prediction",
      "Systematic Review",
      "Limitations and Future Directions",
      "Integrated Learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/252351936bd6fabf4b6cd2962fa0ee613772278d.pdf",
    "citation_key": "jin2023e18",
    "metadata": {
      "title": "Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey",
      "authors": [
        "G. Jin",
        "Yuxuan Liang",
        "Yuchen Fang",
        "Zezhi Shao",
        "Jincai Huang",
        "Junbo Zhang",
        "Yu Zheng"
      ],
      "published_date": "2023",
      "abstract": "With recent advances in sensing technologies, a myriad of spatio-temporal data has been generated and recorded in smart cities. Forecasting the evolution patterns of spatio-temporal data is an important yet demanding aspect of urban computing, which can enhance intelligent management decisions in various fields, including transportation, environment, climate, public safety, healthcare, and others. Traditional statistical and deep learning methods struggle to capture complex correlations in urban spatio-temporal data. To this end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed, achieving great promise in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. In this manuscript, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. Firstly, we provide a brief introduction to the construction methods of spatio-temporal graph data and the prevalent deep-learning architectures used in STGNNs. We then sort out the primary application domains and specific predictive learning tasks based on existing literature. Afterward, we scrutinize the design of STGNNs and their combination with some advanced technologies in recent years. Finally, we conclude the limitations of existing research and suggest potential directions for future work.",
      "file_path": "paper_data/Graph_Neural_Networks/252351936bd6fabf4b6cd2962fa0ee613772278d.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Accurately forecasting the evolution patterns of complex spatio-temporal data in urban environments. This involves capturing intricate spatial correlations (especially on non-Euclidean structures like road networks) and dynamic temporal dependencies simultaneously.\n    *   **Importance & Challenge**: This problem is crucial for intelligent management decisions in smart cities (e.g., transportation, environment, public safety, healthcare) and for enabling advanced technologies like digital twin cities. It is challenging because urban spatio-temporal data exhibits complex correlation and heterogeneity, making traditional feature engineering difficult. Furthermore, the non-Euclidean nature of many urban systems (e.g., vehicle flows over road networks) limits the direct applicability of grid-based deep learning methods like CNNs and RNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Spatio-Temporal Graph Neural Networks (STGNNs) are proposed as an advancement over traditional statistical methods (e.g., SVR, Random Forest, GBDT) and earlier deep learning models (e.g., hybrid CNN/RNN like ConvLSTM, PredRNN).\n    *   **Limitations of Previous Solutions**: Traditional statistical methods are less effective due to the complex, correlated, and heterogeneous nature of urban spatio-temporal data. Hybrid CNN/RNN models, while an improvement, are fundamentally limited by their inability to directly learn from non-Euclidean data structures prevalent in urban systems (e.g., road networks, urban knowledge graphs).\n    *   **Positioning of *This Survey***: This paper \\cite{jin2023e18} distinguishes itself as the first comprehensive survey systematically reviewing recent studies that use STGNNs specifically for *predictive learning in urban computing*. It covers both application and methodology perspectives, including recent advancements and future directions, unlike prior surveys that were older, domain-specific (e.g., only traffic), or too broad.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: STGNNs integrate Graph Neural Networks (GNNs) with various temporal learning methods (e.g., RNNs, CNNs, attention mechanisms) to simultaneously capture complex spatial dependencies and dynamic temporal patterns in non-Euclidean spatio-temporal data.\n    *   **Novelty/Difference**: The core innovation of STGNNs lies in their ability to directly process and learn from graph-structured data, which is ubiquitous in urban systems (e.g., road networks, sensor networks). This overcomes the limitations of traditional deep learning methods that assume Euclidean data structures, by combining graph convolution operations for spatial feature extraction with temporal sequence modeling techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (of STGNNs)**:\n        *   **Spatio-Temporal Graph Construction**: Methods for building graphs from urban data, including topology-based (e.g., road networks), distance-based (e.g., Gaussian radial basis), similarity-based (e.g., Pearson Correlation Coefficient, Dynamic Time Warping), and interaction-based (e.g., flow between nodes) approaches. The survey also mentions adaptive learning for implicit relations.\n        *   **Integrated Learning**: The fundamental concept of integrating GNNs for spatial feature extraction with temporal models (RNNs, CNNs, attention) for sequence learning.\n    *   **Contributions *of this survey***:\n        *   Provides a systematic taxonomy and comprehensive review of STGNNs for predictive learning in urban computing.\n        *   Categorizes primary application domains (transportation, environment, safety, public health) and specific predictive learning tasks (e.g., traffic state/demand/incident/travel time/trajectory prediction, air quality prediction).\n        *   Analyzes spatio-temporal graph construction methods and fundamental/advanced STGNN architectures.\n        *   Identifies limitations of current STGNN research and proposes future directions.\n\n*   **Experimental Validation**\n    *   This paper \\cite{jin2023e18} is a survey and does not present its own experimental validation.\n    *   However, it extensively discusses the application domains where STGNNs have been empirically validated by other research. These include:\n        *   **Transportation**: Network-based and region-based traffic state prediction (flow, speed), traffic demand prediction (origin, destination, OD matrices for taxi, rail, bike-sharing), traffic incident prediction, travel time prediction, and trajectory prediction for humans and vehicles.\n        *   **Environment**: Air quality prediction (e.g., AQI, PM2.5, emissions).\n    *   The survey highlights that STGNNs have shown \"great promise\" and \"significant advantages\" in these tasks, implying superior performance over previous methods, typically demonstrated through standard predictive metrics on real-world urban datasets.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of STGNNs, as identified by the survey)**: The survey concludes by highlighting limitations of existing STGNN research, which include challenges related to scalability, dynamic graph learning, interpretability, robustness, and data scarcity, among others.\n    *   **Scope of Applicability (of STGNNs)**: STGNNs are primarily applicable to predictive learning tasks involving spatio-temporal data that can be represented as graphs, particularly in urban computing contexts where non-Euclidean spatial relationships and dynamic temporal patterns are crucial.\n    *   **Scope of *This Survey***: The survey focuses specifically on STGNNs for *predictive learning* in *urban computing*, covering spatio-temporal graph construction, prevalent deep learning architectures, primary application domains, design scrutiny, integration with advanced technologies, and future research directions.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: STGNNs significantly advance the technical state-of-the-art by providing a powerful and flexible framework to model complex spatio-temporal dependencies in non-Euclidean urban data. This capability addresses a major limitation of previous statistical and deep learning methods, enabling more accurate and robust predictions in dynamic urban environments.\n    *   **Potential Impact on Future Research**: STGNNs open new avenues for developing more sophisticated urban intelligence systems, leading to improved traffic management, environmental monitoring, public safety, and resource allocation. The survey itself aims to guide future research by systematically identifying current limitations and suggesting promising directions, fostering further innovation in areas like adaptive graph learning, explainability, and handling data heterogeneity.",
      "keywords": [
        "Spatio-Temporal Graph Neural Networks (STGNNs)",
        "Urban Computing",
        "Predictive Learning",
        "Non-Euclidean Spatio-Temporal Data",
        "Graph Convolution",
        "Dynamic Temporal Dependencies",
        "Spatio-Temporal Graph Construction",
        "Smart Cities",
        "Traffic Prediction",
        "Air Quality Prediction",
        "Systematic Review",
        "Limitations and Future Directions",
        "Integrated Learning"
      ],
      "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** the title explicitly states \"spatio-temporal graph neural networks for predictive learning in urban computing: a **survey**\".\n*   **abstract:** it clearly states, \"in this manuscript, we provide a **comprehensive survey** on recent progress on stgnn technologies for predictive learning in urban computing.\" it then outlines how it will organize and present existing knowledge (\"provide a brief introduction to the construction methods,\" \"sort out the primary application domains,\" \"scrutinize the design,\" \"conclude the limitations... and suggest potential directions\").\n*   **introduction:** the introduction further elaborates on the structure of the survey, detailing how it will review existing methods, applications, and designs, and identify future research directions.\n\nthese points directly align with the criteria for a **survey** paper: \"reviews existing literature comprehensively\" and \"abstract mentions: 'survey', 'review', 'comprehensive analysis', 'state-of-the-art'\"."
    },
    "file_name": "252351936bd6fabf4b6cd2962fa0ee613772278d.pdf"
  },
  {
    "success": true,
    "doc_id": "e23de16f7739fd5762f342324ba10f91",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Scaling Graph Convolutional Networks (GCNs) to web-scale recommender systems, which involve billions of items and hundreds of millions of users.\n    *   **Importance and Challenge:** While GCNs achieve state-of-the-art performance on recommender system benchmarks, their deployment in real-world production environments is challenging. Existing GCNs typically require operating on the full graph Laplacian during training, which is infeasible for massive, constantly evolving graphs with billions of nodes and edges due to memory and computational constraints.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon recent advancements in deep learning for graph-structured data, particularly Graph Convolutional Networks (GCNs) and is most closely related to the GraphSAGE algorithm \\cite{ying20189jc}.\n    *   **Limitations of Previous Solutions:**\n        *   Early graph neural networks (e.g., Gori et al., Scarselli et al.) were computationally expensive, requiring message-passing to convergence, limiting them to small graphs (<10,000 nodes).\n        *   Traditional GCNs (spectral and approximation-based) require operating on the *entire graph Laplacian* during training, making them impractical for graphs with billions of nodes and edges.\n        *   GraphSAGE \\cite{ying20189jc} improved inductive capabilities but still implicitly assumed the entire graph could be stored in GPU memory or accessed efficiently, which is not scalable to Pinterest's graph size.\n        *   Graph embedding methods like node2vec and DeepWalk are unsupervised, cannot incorporate node feature information, and have a number of parameters linear with graph size, making them prohibitive for web-scale applications.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces PinSage \\cite{ying20189jc}, a data-efficient Graph Convolutional Network (GCN) algorithm. PinSage combines efficient random walks and graph convolutions to generate embeddings of nodes (items) that incorporate both graph structure and node feature information.\n    *   **Novelty and Differentiation:**\n        *   **On-the-fly Convolutions:** Unlike traditional GCNs that use the full graph Laplacian, PinSage performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph for each minibatch.\n        *   **Producer-Consumer Minibatch Construction:** A CPU-bound producer efficiently samples node neighborhoods and fetches features, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs for stochastic gradient descent, maximizing GPU utilization.\n        *   **Efficient MapReduce Inference:** A designed MapReduce pipeline distributes the trained model to generate embeddings for billions of nodes, minimizing repeated computations.\n        *   **Constructing Convolutions via Random Walks:** Instead of random sampling, PinSage uses short random walks to sample the computation graph, which also provides importance scores for neighbors.\n        *   **Importance Pooling:** A novel aggregation method where neighbor features are weighted based on their L1-normalized visit counts from random walks, improving the quality of aggregated information.\n        *   **Curriculum Training:** A training strategy where the algorithm is fed progressively \"harder\" examples during training to improve model robustness and convergence.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   PinSage \\cite{ying20189jc}: A highly scalable GCN framework specifically designed for web-scale recommender systems.\n        *   Importance-based neighborhood definition and sampling using random walks.\n        *   Importance pooling: A weighted aggregation function for GCNs based on random walk visit counts.\n        *   Curriculum training scheme for GCNs.\n    *   **System Design or Architectural Innovations:**\n        *   Dynamic, on-the-fly computation graph construction for localized convolutions, eliminating the need for the full graph Laplacian.\n        *   Producer-consumer architecture for efficient minibatch generation and GPU utilization.\n        *   MapReduce pipeline for scalable inference of billions of node embeddings.\n    *   **Theoretical Insights or Analysis:** The paper primarily focuses on practical engineering innovations for scalability and empirical performance gains rather than deep theoretical analysis.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   PinSage \\cite{ying20189jc} was developed and deployed at Pinterest, operating on a graph with 3 billion nodes (pins and boards) and 18 billion edges, trained on 7.5 billion examples.\n        *   Evaluated for item-item recommendation (related-pin recommendation) and \"homefeed\" recommendation tasks.\n        *   Validation included extensive offline metrics, controlled user studies, and A/B tests.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Offline Ranking Metrics:** PinSage improved over the best performing baseline by more than 40%.\n        *   **Human Evaluations:** In head-to-head comparisons, PinSage recommendations were preferred about 60% of the time.\n        *   **A/B Tests:** Showed 30% to 100% improvements in user engagement across various settings.\n        *   **Impact of Innovations:** Importance pooling led to a 46% performance gain in offline evaluation metrics, and curriculum training resulted in a 12% performance gain.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The paper primarily addresses the limitations of *prior* GCN approaches rather than explicitly detailing limitations of PinSage itself. It assumes the availability of rich node features (e.g., visual, textual for pins). The approach is tailored for bipartite graphs (pins and boards) but is presented as generalizable to item-context graphs.\n    *   **Scope of Applicability:** Applicable to web-scale recommender systems, content discovery platforms, and any domain requiring scalable deep graph embeddings for massive graph-structured data.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** PinSage \\cite{ying20189jc} represents the largest-ever application of deep graph embeddings to date, successfully scaling GCNs to a production environment with billions of nodes and edges. It demonstrates that GCNs can overcome previous scalability barriers and achieve state-of-the-art performance in real-world recommendation tasks, significantly outperforming existing deep learning and graph-based alternatives.\n    *   **Potential Impact on Future Research:** This work paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures, opening up GCNs for broader industrial applications beyond academic benchmarks. It provides a blueprint for designing scalable GCNs for massive graphs.",
    "intriguing_abstract": "Scaling Graph Convolutional Networks (GCNs) to web-scale recommender systems, involving billions of nodes and edges, has remained a formidable challenge due to the computational and memory demands of operating on the full graph Laplacian. This paper introduces **PinSage**, a pioneering data-efficient GCN framework that revolutionizes deep graph embeddings for massive, dynamic graphs. PinSage overcomes these barriers by employing novel techniques: it performs efficient, localized convolutions through random walk-based neighborhood sampling, dynamically constructing computation graphs for each minibatch. This approach, coupled with **Importance Pooling** for weighted aggregation and a **curriculum training** scheme, significantly enhances information propagation and model robustness. Furthermore, an optimized producer-consumer architecture and a scalable MapReduce inference pipeline enable the generation of billions of node embeddings. Deployed at Pinterest on a graph with 3 billion nodes and 18 billion edges, PinSage achieved unprecedented performance, improving offline metrics by over 40% and boosting user engagement by 30-100% in A/B tests. This work represents the largest-ever application of deep graph embeddings, demonstrating GCNs' viability for real-world, web-scale recommendation and paving the way for future industrial applications.",
    "keywords": [
      "PinSage",
      "Graph Convolutional Networks (GCNs)",
      "web-scale recommender systems",
      "scalability",
      "node embeddings",
      "importance pooling",
      "curriculum training",
      "dynamic computation graph",
      "random walks",
      "producer-consumer architecture",
      "MapReduce inference",
      "full graph Laplacian limitation",
      "user engagement",
      "A/B testing"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf",
    "citation_key": "ying20189jc",
    "metadata": {
      "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
      "authors": [
        "Rex Ying",
        "Ruining He",
        "Kaifeng Chen",
        "Pong Eksombatchai",
        "William L. Hamilton",
        "J. Leskovec"
      ],
      "published_date": "2018",
      "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
      "file_path": "paper_data/Graph_Neural_Networks/6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Scaling Graph Convolutional Networks (GCNs) to web-scale recommender systems, which involve billions of items and hundreds of millions of users.\n    *   **Importance and Challenge:** While GCNs achieve state-of-the-art performance on recommender system benchmarks, their deployment in real-world production environments is challenging. Existing GCNs typically require operating on the full graph Laplacian during training, which is infeasible for massive, constantly evolving graphs with billions of nodes and edges due to memory and computational constraints.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon recent advancements in deep learning for graph-structured data, particularly Graph Convolutional Networks (GCNs) and is most closely related to the GraphSAGE algorithm \\cite{ying20189jc}.\n    *   **Limitations of Previous Solutions:**\n        *   Early graph neural networks (e.g., Gori et al., Scarselli et al.) were computationally expensive, requiring message-passing to convergence, limiting them to small graphs (<10,000 nodes).\n        *   Traditional GCNs (spectral and approximation-based) require operating on the *entire graph Laplacian* during training, making them impractical for graphs with billions of nodes and edges.\n        *   GraphSAGE \\cite{ying20189jc} improved inductive capabilities but still implicitly assumed the entire graph could be stored in GPU memory or accessed efficiently, which is not scalable to Pinterest's graph size.\n        *   Graph embedding methods like node2vec and DeepWalk are unsupervised, cannot incorporate node feature information, and have a number of parameters linear with graph size, making them prohibitive for web-scale applications.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces PinSage \\cite{ying20189jc}, a data-efficient Graph Convolutional Network (GCN) algorithm. PinSage combines efficient random walks and graph convolutions to generate embeddings of nodes (items) that incorporate both graph structure and node feature information.\n    *   **Novelty and Differentiation:**\n        *   **On-the-fly Convolutions:** Unlike traditional GCNs that use the full graph Laplacian, PinSage performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph for each minibatch.\n        *   **Producer-Consumer Minibatch Construction:** A CPU-bound producer efficiently samples node neighborhoods and fetches features, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs for stochastic gradient descent, maximizing GPU utilization.\n        *   **Efficient MapReduce Inference:** A designed MapReduce pipeline distributes the trained model to generate embeddings for billions of nodes, minimizing repeated computations.\n        *   **Constructing Convolutions via Random Walks:** Instead of random sampling, PinSage uses short random walks to sample the computation graph, which also provides importance scores for neighbors.\n        *   **Importance Pooling:** A novel aggregation method where neighbor features are weighted based on their L1-normalized visit counts from random walks, improving the quality of aggregated information.\n        *   **Curriculum Training:** A training strategy where the algorithm is fed progressively \"harder\" examples during training to improve model robustness and convergence.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   PinSage \\cite{ying20189jc}: A highly scalable GCN framework specifically designed for web-scale recommender systems.\n        *   Importance-based neighborhood definition and sampling using random walks.\n        *   Importance pooling: A weighted aggregation function for GCNs based on random walk visit counts.\n        *   Curriculum training scheme for GCNs.\n    *   **System Design or Architectural Innovations:**\n        *   Dynamic, on-the-fly computation graph construction for localized convolutions, eliminating the need for the full graph Laplacian.\n        *   Producer-consumer architecture for efficient minibatch generation and GPU utilization.\n        *   MapReduce pipeline for scalable inference of billions of node embeddings.\n    *   **Theoretical Insights or Analysis:** The paper primarily focuses on practical engineering innovations for scalability and empirical performance gains rather than deep theoretical analysis.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   PinSage \\cite{ying20189jc} was developed and deployed at Pinterest, operating on a graph with 3 billion nodes (pins and boards) and 18 billion edges, trained on 7.5 billion examples.\n        *   Evaluated for item-item recommendation (related-pin recommendation) and \"homefeed\" recommendation tasks.\n        *   Validation included extensive offline metrics, controlled user studies, and A/B tests.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Offline Ranking Metrics:** PinSage improved over the best performing baseline by more than 40%.\n        *   **Human Evaluations:** In head-to-head comparisons, PinSage recommendations were preferred about 60% of the time.\n        *   **A/B Tests:** Showed 30% to 100% improvements in user engagement across various settings.\n        *   **Impact of Innovations:** Importance pooling led to a 46% performance gain in offline evaluation metrics, and curriculum training resulted in a 12% performance gain.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The paper primarily addresses the limitations of *prior* GCN approaches rather than explicitly detailing limitations of PinSage itself. It assumes the availability of rich node features (e.g., visual, textual for pins). The approach is tailored for bipartite graphs (pins and boards) but is presented as generalizable to item-context graphs.\n    *   **Scope of Applicability:** Applicable to web-scale recommender systems, content discovery platforms, and any domain requiring scalable deep graph embeddings for massive graph-structured data.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** PinSage \\cite{ying20189jc} represents the largest-ever application of deep graph embeddings to date, successfully scaling GCNs to a production environment with billions of nodes and edges. It demonstrates that GCNs can overcome previous scalability barriers and achieve state-of-the-art performance in real-world recommendation tasks, significantly outperforming existing deep learning and graph-based alternatives.\n    *   **Potential Impact on Future Research:** This work paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures, opening up GCNs for broader industrial applications beyond academic benchmarks. It provides a blueprint for designing scalable GCNs for massive graphs.",
      "keywords": [
        "PinSage",
        "Graph Convolutional Networks (GCNs)",
        "web-scale recommender systems",
        "scalability",
        "node embeddings",
        "importance pooling",
        "curriculum training",
        "dynamic computation graph",
        "random walks",
        "producer-consumer architecture",
        "MapReduce inference",
        "full graph Laplacian limitation",
        "user engagement",
        "A/B testing"
      ],
      "paper_type": "this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we **developed** and deployed\", \"we **develop** a data-efficient graph convolutional network (gcn) **algorithm** pinsage\", \"we **develop a novel method**\", \"design a **novel training strategy**\". these phrases directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n*   **introduction discusses:** the introduction sets up the problem of scaling deep learning methods for recommender systems, which the paper then addresses with its proposed solution (pinsage).\n*   while the paper also includes significant **empirical** evaluation (deployment at pinterest, 7.5 billion examples, offline metrics, user studies, a/b tests) and could be considered a **case_study** due to its focus on pinterest, the core contribution highlighted is the *development of a new algorithm and novel methods*. the empirical results serve to validate this technical contribution. when a paper introduces a new method and then evaluates it, the primary classification often leans towards \"technical.\""
    },
    "file_name": "6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf"
  },
  {
    "success": true,
    "doc_id": "6d41936bf668209370fb51e84407910c",
    "summary": "Here's a focused summary of the technical paper \"GPT-GNN: Generative Pre-Training of Graph Neural Networks\" by Hu et al. \\cite{hu2020u8o} for a literature review:\n\n### GPT-GNN: Generative Pre-Training of Graph Neural Networks \\cite{hu2020u8o}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Training Graph Neural Networks (GNNs) typically requires abundant task-specific labeled data, which is often prohibitively expensive or infeasible to obtain, especially for large-scale graphs. This hinders the deployment of GNNs across various downstream tasks on the same graph.\n    *   **Importance and Challenge**: The problem is critical because real-world graphs are often massive and dynamic, making manual labeling impractical. Different tasks on the same graph usually demand distinct sets of labels, leading to repetitive and costly data acquisition. The challenge lies in developing a generalizable GNN model that can capture intrinsic graph properties from unlabeled data and adapt to diverse downstream tasks with minimal fine-tuning.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work is inspired by the success of pre-training paradigms in Natural Language Processing (e.g., BERT, GPT) and Computer Vision, which leverage large unlabeled datasets to learn general representations.\n    *   **Limitations of Previous Solutions**:\n        *   **Network/Graph Embedding**: These methods (e.g., DeepWalk, Node2Vec) learn direct node embedding vectors, which are not designed for initializing and fine-tuning GNN models for transfer learning.\n        *   **GNN Pre-training (Unsupervised)**: Earlier GNN pre-training attempts (e.g., Variational Graph Auto-Encoders, GraphSAGE unsupervised, Graph Infomax) often focus on simpler objectives like reconstructing graph structure or maximizing mutual information between node/graph representations. They tend to ignore rich semantics, higher-order graph structures, or the crucial dependency between node attributes and graph structure, which is fundamental to GNNs' convolutional aggregation.\n        *   **Neural Graph Generation**: Existing graph generation techniques are typically designed for small graphs and often focus solely on generating graph structure without considering node attributes, making them unsuitable for pre-training GNNs on large, attributed graphs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{hu2020u8o} proposes GPT-GNN, a generative pre-training framework for GNNs. It pre-trains a GNN by modeling the likelihood of an input attributed graph through a self-supervised *attributed graph generation* task. The pre-trained GNN then serves as a robust initialization for various downstream tasks.\n    *   **Novelty/Difference**:\n        *   **Dependency-Aware Factorization**: Unlike prior methods, \\cite{hu2020u8o} introduces a novel factorization of the graph generation objective into two *coupled* components: 1) **Attribute Generation** (`pθ(Xi|Ei,o,X<i,E<i)`) and 2) **Edge Generation** (`pθ(Ei,¬o|Ei,o,X≤i,E<i)`). This explicitly models the inherent dependency between node attributes and graph structure during the generative process, which is crucial for GNNs.\n        *   **Efficient Large-Scale Pre-training**: The framework is designed for efficiency, allowing simultaneous calculation of attribute and edge generation losses for each node in a single GNN run. It also incorporates techniques like sub-graph sampling for large-scale graphs and an adaptive embedding queue to mitigate issues with negative sampling.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a self-supervised *attributed graph generation* task for GNN pre-training, which aims to reconstruct both the structure and attributes of the input graph.\n        *   A dependency-aware factorization mechanism that decomposes the graph generation objective into Attribute Generation (predicting node attributes given partial edges) and Edge Generation (predicting remaining edges given partial edges and generated attributes). This joint optimization captures the attribute-structure interplay.\n    *   **System Design/Architectural Innovations**:\n        *   The GPT-GNN framework efficiently computes both attribute and edge generation losses simultaneously, requiring only one GNN pass per graph, which is critical for scalability.\n        *   Incorporation of sub-graph sampling to handle large-scale graphs that cannot fit into memory.\n        *   An adaptive embedding queue mechanism to improve the accuracy of negative sampling, which is often used in link prediction tasks.\n    *   **Theoretical Insights/Analysis**: The paper formalizes the graph distribution `p(G;θ)` as an expected likelihood over all possible node permutations and autoregressively factorizes the log-likelihood, providing a principled generative objective.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{hu2020u8o} conducted comprehensive experiments by pre-training GNNs on two real-world, large-scale datasets and evaluating their performance on various downstream tasks after fine-tuning.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**:\n            *   Open Academic Graph (OAG): A massive graph with 179 million nodes and 2 billion edges.\n            *   Amazon recommendation data: Comprising 113 million nodes.\n        *   **Downstream Tasks**: Node classification and link prediction.\n        *   **Results**: GPT-GNN significantly outperformed state-of-the-art GNN models *without pre-training*. Specifically, on the OAG dataset, it achieved an average performance lift of up to **9.1%** across node classification and link prediction tasks. The framework consistently improved the performance of different base GNN architectures (e.g., GCN, GraphSAGE, GAT, HGT) under various settings, demonstrating its robustness and generalizability.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The autoregressive generation process assumes a specific node ordering (permutation), and while the paper averages over permutations conceptually, practical implementations often rely on a fixed or sampled order. The use of negative sampling, even with mitigation, can introduce approximations.\n    *   **Scope of Applicability**: The framework is primarily designed for node-level transfer learning on a *single (large-scale) graph* or graphs within the *same domain*. While powerful for these scenarios, its direct applicability to graph-level tasks or transfer across vastly different graph domains might require further adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{hu2020u8o} makes a significant advancement by introducing the first generative pre-training framework for GNNs that explicitly models the intricate dependency between node attributes and graph structure on large-scale, attributed graphs. This addresses a critical bottleneck in GNN deployment.\n    *   **Potential Impact on Future Research**: GPT-GNN establishes a new paradigm for training GNNs, shifting from purely supervised, task-specific training to a more generalizable, self-supervised pre-training approach. This can lead to:\n        *   More robust and data-efficient GNN models, especially in domains with scarce labeled data.\n        *   Further research into more sophisticated attributed graph generation models.\n        *   Exploration of pre-training for other graph learning tasks and across different graph types.\n        *   Bridging the gap between the success of pre-training in NLP/CV and the graph domain.",
    "intriguing_abstract": "Training Graph Neural Networks (GNNs) on large, real-world graphs is often hampered by the scarcity of labeled data, a critical bottleneck for their widespread deployment. Inspired by the success of pre-training in NLP and CV, we introduce GPT-GNN, a pioneering generative pre-training framework for GNNs. GPT-GNN addresses this challenge by learning generalizable representations through a novel self-supervised *attributed graph generation* task.\n\nOur core innovation lies in a *dependency-aware factorization* of the graph generation objective, explicitly modeling the intricate interplay between node attributes and graph structure during the generative process. This unique approach allows the GNN to reconstruct both structural and semantic information efficiently. Pre-trained on massive unlabeled graphs like Open Academic Graph (OAG), GPT-GNN provides a robust initialization, significantly boosting performance on diverse downstream tasks such as *node classification* and *link prediction* after minimal fine-tuning. We demonstrate an average performance lift of up to 9.1% on OAG, establishing a new paradigm for GNN training and unlocking their potential in data-scarce environments.",
    "keywords": [
      "GPT-GNN",
      "Generative Pre-Training",
      "Graph Neural Networks (GNNs)",
      "Attributed Graph Generation",
      "Dependency-Aware Factorization",
      "Self-Supervised Learning",
      "Large-Scale Graphs",
      "Node Classification",
      "Link Prediction",
      "Sub-Graph Sampling",
      "Adaptive Embedding Queue",
      "Performance Lift",
      "Transfer Learning"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf",
    "citation_key": "hu2020u8o",
    "metadata": {
      "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
      "authors": [
        "Ziniu Hu",
        "Yuxiao Dong",
        "Kuansan Wang",
        "Kai-Wei Chang",
        "Yizhou Sun"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks?",
      "file_path": "paper_data/Graph_Neural_Networks/04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \"GPT-GNN: Generative Pre-Training of Graph Neural Networks\" by Hu et al. \\cite{hu2020u8o} for a literature review:\n\n### GPT-GNN: Generative Pre-Training of Graph Neural Networks \\cite{hu2020u8o}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Training Graph Neural Networks (GNNs) typically requires abundant task-specific labeled data, which is often prohibitively expensive or infeasible to obtain, especially for large-scale graphs. This hinders the deployment of GNNs across various downstream tasks on the same graph.\n    *   **Importance and Challenge**: The problem is critical because real-world graphs are often massive and dynamic, making manual labeling impractical. Different tasks on the same graph usually demand distinct sets of labels, leading to repetitive and costly data acquisition. The challenge lies in developing a generalizable GNN model that can capture intrinsic graph properties from unlabeled data and adapt to diverse downstream tasks with minimal fine-tuning.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work is inspired by the success of pre-training paradigms in Natural Language Processing (e.g., BERT, GPT) and Computer Vision, which leverage large unlabeled datasets to learn general representations.\n    *   **Limitations of Previous Solutions**:\n        *   **Network/Graph Embedding**: These methods (e.g., DeepWalk, Node2Vec) learn direct node embedding vectors, which are not designed for initializing and fine-tuning GNN models for transfer learning.\n        *   **GNN Pre-training (Unsupervised)**: Earlier GNN pre-training attempts (e.g., Variational Graph Auto-Encoders, GraphSAGE unsupervised, Graph Infomax) often focus on simpler objectives like reconstructing graph structure or maximizing mutual information between node/graph representations. They tend to ignore rich semantics, higher-order graph structures, or the crucial dependency between node attributes and graph structure, which is fundamental to GNNs' convolutional aggregation.\n        *   **Neural Graph Generation**: Existing graph generation techniques are typically designed for small graphs and often focus solely on generating graph structure without considering node attributes, making them unsuitable for pre-training GNNs on large, attributed graphs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{hu2020u8o} proposes GPT-GNN, a generative pre-training framework for GNNs. It pre-trains a GNN by modeling the likelihood of an input attributed graph through a self-supervised *attributed graph generation* task. The pre-trained GNN then serves as a robust initialization for various downstream tasks.\n    *   **Novelty/Difference**:\n        *   **Dependency-Aware Factorization**: Unlike prior methods, \\cite{hu2020u8o} introduces a novel factorization of the graph generation objective into two *coupled* components: 1) **Attribute Generation** (`pθ(Xi|Ei,o,X<i,E<i)`) and 2) **Edge Generation** (`pθ(Ei,¬o|Ei,o,X≤i,E<i)`). This explicitly models the inherent dependency between node attributes and graph structure during the generative process, which is crucial for GNNs.\n        *   **Efficient Large-Scale Pre-training**: The framework is designed for efficiency, allowing simultaneous calculation of attribute and edge generation losses for each node in a single GNN run. It also incorporates techniques like sub-graph sampling for large-scale graphs and an adaptive embedding queue to mitigate issues with negative sampling.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a self-supervised *attributed graph generation* task for GNN pre-training, which aims to reconstruct both the structure and attributes of the input graph.\n        *   A dependency-aware factorization mechanism that decomposes the graph generation objective into Attribute Generation (predicting node attributes given partial edges) and Edge Generation (predicting remaining edges given partial edges and generated attributes). This joint optimization captures the attribute-structure interplay.\n    *   **System Design/Architectural Innovations**:\n        *   The GPT-GNN framework efficiently computes both attribute and edge generation losses simultaneously, requiring only one GNN pass per graph, which is critical for scalability.\n        *   Incorporation of sub-graph sampling to handle large-scale graphs that cannot fit into memory.\n        *   An adaptive embedding queue mechanism to improve the accuracy of negative sampling, which is often used in link prediction tasks.\n    *   **Theoretical Insights/Analysis**: The paper formalizes the graph distribution `p(G;θ)` as an expected likelihood over all possible node permutations and autoregressively factorizes the log-likelihood, providing a principled generative objective.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{hu2020u8o} conducted comprehensive experiments by pre-training GNNs on two real-world, large-scale datasets and evaluating their performance on various downstream tasks after fine-tuning.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**:\n            *   Open Academic Graph (OAG): A massive graph with 179 million nodes and 2 billion edges.\n            *   Amazon recommendation data: Comprising 113 million nodes.\n        *   **Downstream Tasks**: Node classification and link prediction.\n        *   **Results**: GPT-GNN significantly outperformed state-of-the-art GNN models *without pre-training*. Specifically, on the OAG dataset, it achieved an average performance lift of up to **9.1%** across node classification and link prediction tasks. The framework consistently improved the performance of different base GNN architectures (e.g., GCN, GraphSAGE, GAT, HGT) under various settings, demonstrating its robustness and generalizability.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The autoregressive generation process assumes a specific node ordering (permutation), and while the paper averages over permutations conceptually, practical implementations often rely on a fixed or sampled order. The use of negative sampling, even with mitigation, can introduce approximations.\n    *   **Scope of Applicability**: The framework is primarily designed for node-level transfer learning on a *single (large-scale) graph* or graphs within the *same domain*. While powerful for these scenarios, its direct applicability to graph-level tasks or transfer across vastly different graph domains might require further adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{hu2020u8o} makes a significant advancement by introducing the first generative pre-training framework for GNNs that explicitly models the intricate dependency between node attributes and graph structure on large-scale, attributed graphs. This addresses a critical bottleneck in GNN deployment.\n    *   **Potential Impact on Future Research**: GPT-GNN establishes a new paradigm for training GNNs, shifting from purely supervised, task-specific training to a more generalizable, self-supervised pre-training approach. This can lead to:\n        *   More robust and data-efficient GNN models, especially in domains with scarce labeled data.\n        *   Further research into more sophisticated attributed graph generation models.\n        *   Exploration of pre-training for other graph learning tasks and across different graph types.\n        *   Bridging the gap between the success of pre-training in NLP/CV and the graph domain.",
      "keywords": [
        "GPT-GNN",
        "Generative Pre-Training",
        "Graph Neural Networks (GNNs)",
        "Attributed Graph Generation",
        "Dependency-Aware Factorization",
        "Self-Supervised Learning",
        "Large-Scale Graphs",
        "Node Classification",
        "Link Prediction",
        "Sub-Graph Sampling",
        "Adaptive Embedding Queue",
        "Performance Lift",
        "Transfer Learning"
      ],
      "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   **abstract:** explicitly states \"we present the gpt-gnn framework,\" \"gpt-gnn introduces a self-supervised attributed graph generation task,\" and describes the technical components (\"factorize the likelihood... into two components: 1) attribute generation and 2) edge generation\"). it focuses on proposing a new method/system.\n*   **introduction:** discusses a technical problem (\"training gnns usually requires abundant task-specific labeled data, which is often arduously expensive to obtain\") and sets the stage for a proposed solution (pre-training, which gpt-gnn implements).\n*   **keywords:** \"generative pre-training; graph neural networks; graph representation learning; network embedding; gnn pre-training\" all point to a focus on developing new techniques within these areas.\n*   while it includes \"comprehensive experiments... demonstrate that gpt-gnn significantly outperforms,\" this empirical evaluation serves to validate the *new method* being presented, making the primary classification \"technical\" rather than purely \"empirical.\" the core contribution is the development of the gpt-gnn framework."
    },
    "file_name": "04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf"
  },
  {
    "success": true,
    "doc_id": "696fe9896aacc6cc9ada5bb38ab72fdc",
    "summary": "Here's a focused summary of the paper \"Revisiting Heterophily For Graph Neural Networks\" by Luan et al. \\cite{luan202272y} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are typically built on the homophily assumption (connected nodes share similar attributes), which is a relational inductive bias.\n    *   However, GNNs often perform unsatisfactorily, sometimes even worse than simple Multi-Layer Perceptrons (MLPs), on a non-trivial set of real-world datasets.\n    *   Heterophily, where connected nodes have dissimilar attributes, is widely considered the main cause of this performance degradation.\n    *   Existing homophily metrics (edge, node, class homophily) are limited as they only consider graph-label consistency and fail to capture the true impact of heterophily on aggregation-based GNNs.\n\n*   **Related Work & Positioning**\n    *   The paper acknowledges numerous existing works addressing heterophily in GNNs \\cite{luan202272y}.\n    *   **Limitations of previous solutions**:\n        *   Existing homophily metrics are shown to be insufficient, as they can misleadingly indicate strong heterophily even when nodes remain distinguishable after aggregation (e.g., the bipartite graph example in Fig. 1).\n        *   Traditional adaptive filterbanks use scalar weights shared by all nodes, failing to account for diverse *local* heterophily.\n        *   Many existing heterophily-addressing methods rely on high-order filters or global properties of high-frequency signals, which can be computationally expensive.\n        *   Other methods focus on learning filters with high expressive power, which can be less flexible.\n\n*   **Technical Approach & Innovation**\n    *   **Revisiting Homophily Metrics**: The paper re-examines heterophily from the perspective of **post-aggregation node similarity**. It defines a **post-aggregation node similarity matrix** $S(\\hat{A}, X) = \\hat{A}X(\\hat{A}X)^T$ and, based on this, introduces new homophily metrics: **Aggregation Similarity Score** ($S_{agg}$) and **Graph Aggregation Homophily** ($H_{agg}(G)$ and $H^M_{agg}(G)$). These new metrics are shown to be more informative and better reflect GNN performance than existing ones.\n    *   **Diversification Operation**: It theoretically proves that a **local diversification operation** (implemented as a high-pass filter, $I - \\hat{A}$) can effectively address certain harmful cases of heterophily. This is quantified by a new metric called **Diversification Distinguishability (DD)**.\n    *   **Adaptive Channel Mixing (ACM) Framework**: Based on the insights from aggregation and diversification, the paper proposes ACM, a novel framework that augments baseline GNNs. ACM adaptively exploits three distinct channels—**aggregation (low-pass), diversification (high-pass), and identity**—**node-wisely and locally** in each layer. This allows GNNs to extract richer localized information tailored to diverse node heterophily situations.\n\n*   **Key Technical Contributions**\n    *   **Novel Homophily Metrics**: Introduction of post-aggregation node similarity-based metrics ($H_{agg}(G)$, $H^M_{agg}(G)$) that provide a more accurate indication of how graph structure affects GNN performance under heterophily \\cite{luan202272y}.\n    *   **Theoretical Justification for Diversification**: A theoretical proof (Theorem 1) demonstrating the effectiveness of local diversification (high-pass filtering) in addressing harmful heterophily under specific conditions.\n    *   **Adaptive Channel Mixing (ACM) Framework**: A novel, flexible, and efficient GNN architectural framework that adaptively combines aggregation, diversification, and identity channels node-wisely and locally, enhancing GNN robustness to heterophily.\n    *   **Ease of Implementation**: ACM is designed to be easily integrated into existing baseline GNN layers.\n\n*   **Experimental Validation**\n    *   **Synthetic Graphs**: Experiments on synthetic graphs with varying homophily levels demonstrated that the proposed $H^M_{agg}(G)$ metric exhibited a nearly monotonic relationship with GNN performance (SGC and GCN), unlike the U-shaped curves observed with existing homophily metrics \\cite{luan202272y}.\n    *   **Real-world Node Classification**: ACM-augmented baselines were evaluated on 10 benchmark node classification tasks (7 heterophilic, 3 homophilic).\n        *   On heterophilic graphs, ACM consistently achieved significant performance gains (2.04% to 27.5%) over uni-channel baselines and surpassed state-of-the-art GNNs on most tasks.\n        *   On homophilic graphs, ACM-augmented GNNs performed comparably to or better than baselines and were competitive with SOTA models.\n        *   The improvements were achieved without incurring significant computational burden.\n\n*   **Limitations & Scope**\n    *   The theoretical proof for diversification's effectiveness (Theorem 1) is demonstrated under specific conditions (e.g., 2 classes, features equal to labels, specific aggregation operator), suggesting its generalizability might require further investigation.\n    *   The primary experimental validation focuses on node classification tasks, and its applicability to other graph learning tasks (e.g., graph classification, link prediction) is not explicitly detailed.\n    *   ACM is presented as an augmentation framework for existing uni-channel GNNs rather than a standalone GNN architecture.\n\n*   **Technical Significance**\n    *   The paper significantly advances the understanding of heterophily in GNNs by introducing a more nuanced perspective based on post-aggregation node similarity, moving beyond simple graph-label consistency.\n    *   ACM provides a highly effective and computationally efficient method to improve GNN performance on challenging heterophilic graphs, addressing a critical limitation of traditional GNNs.\n    *   The adaptive, node-wise channel mixing approach represents a novel architectural paradigm for GNNs, offering greater flexibility and local adaptability to diverse graph structures and homophily levels.\n    *   This work lays a foundation for designing more robust and versatile GNNs that can perform well across a wider spectrum of real-world graph data.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) often falter on real-world heterophilic graphs, where connected nodes are dissimilar, due to their inherent homophily assumption. This paper critically re-examines heterophily, revealing that traditional metrics fail to capture its true impact on GNN aggregation. We introduce novel **post-aggregation node similarity** metrics, demonstrating their superior correlation with GNN performance and offering a deeper understanding of this pervasive challenge. Theoretically, we prove that a **local diversification operation** (high-pass filtering) can effectively mitigate harmful heterophily. Building on these insights, we propose **Adaptive Channel Mixing (ACM)**, a novel framework that augments GNNs by adaptively combining aggregation (low-pass), diversification (high-pass), and identity channels **node-wisely and locally** in each layer. ACM significantly boosts performance on challenging heterophilic **node classification** tasks, outperforming state-of-the-art models while remaining competitive on homophilic datasets. This work offers a flexible, efficient, and theoretically grounded paradigm for designing robust GNNs capable of thriving across diverse graph structures.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Heterophily",
      "Homophily assumption",
      "Post-aggregation node similarity",
      "Novel homophily metrics",
      "Local diversification operation",
      "High-pass filtering",
      "Adaptive Channel Mixing (ACM) framework",
      "Node-wise adaptive channels",
      "Theoretical justification",
      "Node classification",
      "Performance improvement",
      "Computational efficiency"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf",
    "citation_key": "luan202272y",
    "metadata": {
      "title": "Revisiting Heterophily For Graph Neural Networks",
      "authors": [
        "Sitao Luan",
        "Chenqing Hua",
        "Qincheng Lu",
        "Jiaqi Zhu",
        "Mingde Zhao",
        "Shuyuan Zhang",
        "Xiaoming Chang",
        "Doina Precup"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
      "file_path": "paper_data/Graph_Neural_Networks/4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Revisiting Heterophily For Graph Neural Networks\" by Luan et al. \\cite{luan202272y} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are typically built on the homophily assumption (connected nodes share similar attributes), which is a relational inductive bias.\n    *   However, GNNs often perform unsatisfactorily, sometimes even worse than simple Multi-Layer Perceptrons (MLPs), on a non-trivial set of real-world datasets.\n    *   Heterophily, where connected nodes have dissimilar attributes, is widely considered the main cause of this performance degradation.\n    *   Existing homophily metrics (edge, node, class homophily) are limited as they only consider graph-label consistency and fail to capture the true impact of heterophily on aggregation-based GNNs.\n\n*   **Related Work & Positioning**\n    *   The paper acknowledges numerous existing works addressing heterophily in GNNs \\cite{luan202272y}.\n    *   **Limitations of previous solutions**:\n        *   Existing homophily metrics are shown to be insufficient, as they can misleadingly indicate strong heterophily even when nodes remain distinguishable after aggregation (e.g., the bipartite graph example in Fig. 1).\n        *   Traditional adaptive filterbanks use scalar weights shared by all nodes, failing to account for diverse *local* heterophily.\n        *   Many existing heterophily-addressing methods rely on high-order filters or global properties of high-frequency signals, which can be computationally expensive.\n        *   Other methods focus on learning filters with high expressive power, which can be less flexible.\n\n*   **Technical Approach & Innovation**\n    *   **Revisiting Homophily Metrics**: The paper re-examines heterophily from the perspective of **post-aggregation node similarity**. It defines a **post-aggregation node similarity matrix** $S(\\hat{A}, X) = \\hat{A}X(\\hat{A}X)^T$ and, based on this, introduces new homophily metrics: **Aggregation Similarity Score** ($S_{agg}$) and **Graph Aggregation Homophily** ($H_{agg}(G)$ and $H^M_{agg}(G)$). These new metrics are shown to be more informative and better reflect GNN performance than existing ones.\n    *   **Diversification Operation**: It theoretically proves that a **local diversification operation** (implemented as a high-pass filter, $I - \\hat{A}$) can effectively address certain harmful cases of heterophily. This is quantified by a new metric called **Diversification Distinguishability (DD)**.\n    *   **Adaptive Channel Mixing (ACM) Framework**: Based on the insights from aggregation and diversification, the paper proposes ACM, a novel framework that augments baseline GNNs. ACM adaptively exploits three distinct channels—**aggregation (low-pass), diversification (high-pass), and identity**—**node-wisely and locally** in each layer. This allows GNNs to extract richer localized information tailored to diverse node heterophily situations.\n\n*   **Key Technical Contributions**\n    *   **Novel Homophily Metrics**: Introduction of post-aggregation node similarity-based metrics ($H_{agg}(G)$, $H^M_{agg}(G)$) that provide a more accurate indication of how graph structure affects GNN performance under heterophily \\cite{luan202272y}.\n    *   **Theoretical Justification for Diversification**: A theoretical proof (Theorem 1) demonstrating the effectiveness of local diversification (high-pass filtering) in addressing harmful heterophily under specific conditions.\n    *   **Adaptive Channel Mixing (ACM) Framework**: A novel, flexible, and efficient GNN architectural framework that adaptively combines aggregation, diversification, and identity channels node-wisely and locally, enhancing GNN robustness to heterophily.\n    *   **Ease of Implementation**: ACM is designed to be easily integrated into existing baseline GNN layers.\n\n*   **Experimental Validation**\n    *   **Synthetic Graphs**: Experiments on synthetic graphs with varying homophily levels demonstrated that the proposed $H^M_{agg}(G)$ metric exhibited a nearly monotonic relationship with GNN performance (SGC and GCN), unlike the U-shaped curves observed with existing homophily metrics \\cite{luan202272y}.\n    *   **Real-world Node Classification**: ACM-augmented baselines were evaluated on 10 benchmark node classification tasks (7 heterophilic, 3 homophilic).\n        *   On heterophilic graphs, ACM consistently achieved significant performance gains (2.04% to 27.5%) over uni-channel baselines and surpassed state-of-the-art GNNs on most tasks.\n        *   On homophilic graphs, ACM-augmented GNNs performed comparably to or better than baselines and were competitive with SOTA models.\n        *   The improvements were achieved without incurring significant computational burden.\n\n*   **Limitations & Scope**\n    *   The theoretical proof for diversification's effectiveness (Theorem 1) is demonstrated under specific conditions (e.g., 2 classes, features equal to labels, specific aggregation operator), suggesting its generalizability might require further investigation.\n    *   The primary experimental validation focuses on node classification tasks, and its applicability to other graph learning tasks (e.g., graph classification, link prediction) is not explicitly detailed.\n    *   ACM is presented as an augmentation framework for existing uni-channel GNNs rather than a standalone GNN architecture.\n\n*   **Technical Significance**\n    *   The paper significantly advances the understanding of heterophily in GNNs by introducing a more nuanced perspective based on post-aggregation node similarity, moving beyond simple graph-label consistency.\n    *   ACM provides a highly effective and computationally efficient method to improve GNN performance on challenging heterophilic graphs, addressing a critical limitation of traditional GNNs.\n    *   The adaptive, node-wise channel mixing approach represents a novel architectural paradigm for GNNs, offering greater flexibility and local adaptability to diverse graph structures and homophily levels.\n    *   This work lays a foundation for designing more robust and versatile GNNs that can perform well across a wider spectrum of real-world graph data.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Heterophily",
        "Homophily assumption",
        "Post-aggregation node similarity",
        "Novel homophily metrics",
        "Local diversification operation",
        "High-pass filtering",
        "Adaptive Channel Mixing (ACM) framework",
        "Node-wise adaptive channels",
        "Theoretical justification",
        "Node classification",
        "Performance improvement",
        "Computational efficiency"
      ],
      "paper_type": "the paper's abstract and introduction clearly indicate the development and evaluation of a new method.\n\nhere's a breakdown of the key elements:\n\n1.  **critique and analysis:** the paper \"revisit[s] the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming.\" it also \"study[s] heterophily from the perspective of post-aggregation node similarity and deﬁne new homophily metrics.\" this involves analysis and new definitions.\n2.  **theoretical contribution:** \"based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversiﬁcation operation.\" this is a clear theoretical component.\n3.  **proposed method/framework:** \"then, we propose the adaptive channel mixing (acm), a framework to adaptively exploit aggregation, diversiﬁcation and identity channels node-wisely to extract richer localized information for diverse node heterophily situations.\" this is the core technical contribution.\n4.  **empirical evaluation:** \"when evaluated on 10 benchmark node classiﬁcation tasks, acm-augmented baselines consistently achieve signiﬁcant performance gain, exceeding state-of-the-art gnns on most tasks without incurring signiﬁcant computational burden.\" this demonstrates the empirical validation of the proposed method.\n\nwhile the paper contains theoretical elements (proofs, new definitions) and strong empirical evaluation, its primary focus is on **proposing a new framework (acm)** to address the heterophily problem in gnns. the theoretical analysis informs the design of this framework, and the empirical evaluation validates its effectiveness. the act of \"proposing\" a \"framework\" directly aligns with the definition of a **technical** paper.\n\ntherefore, the most appropriate classification is **technical**."
    },
    "file_name": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf"
  },
  {
    "success": true,
    "doc_id": "24e064957a92ee3e2fd770e37f1408f1",
    "summary": "Here's a focused summary of the paper \"PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK\" \\cite{klicpera20186xu} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   Existing neural message passing algorithms (e.g., GCNs) for semi-supervised graph classification are limited to considering only a small, local neighborhood (a few propagation steps) for node classification.\n    *   Extending this neighborhood size is challenging due to two main issues:\n        *   **Oversmoothing:** Too many propagation layers lead to Laplacian smoothing, causing node representations to become indistinguishable and losing local information. This is linked to GCNs converging to a global, root-node-independent limit distribution.\n        *   **Increased Complexity:** Most message passing schemes use learnable weight matrices in each layer, meaning a larger neighborhood requires a deeper neural network with more learnable parameters, leading to computational and memory overhead.\n    *   This fixed relationship between neighborhood size and neural network depth is a strong limitation, forcing compromises in model design.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself against various deep learning approaches on graphs, including node embedding methods (e.g., Node2Vec, DeepWalk) and supervised methods using both graph structure and node features.\n    *   It specifically focuses on message passing (or neighbor aggregation) algorithms (e.g., GCN, GAT, GraphSAGE) which have gained significant attention.\n    *   Previous attempts to improve these algorithms include attention mechanisms, random walks, edge features, and scalability improvements, but these still suffer from limited neighborhood range.\n    *   Methods using skip connections (e.g., residual connections) or batch normalization in deep GNNs have been proposed to mitigate oversmoothing, but these often complicate the model and introduce additional hyperparameters.\n    *   This work differentiates itself by addressing the range limitation through a fundamental architectural change rather than ad-hoc techniques or simply adding more layers/connections.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea:** The paper proposes to decouple the neural network's feature transformation (prediction) from the graph's propagation scheme. It introduces Personalized Propagation of Neural Predictions (PPNP) and its fast approximation, APPNP.\n    *   **Connection to PageRank:** It highlights the connection between GCN's limit distribution and PageRank, then leverages personalized PageRank to overcome the oversmoothing issue.\n    *   **Personalized PageRank for Propagation:** The propagation scheme is derived from personalized PageRank, which includes a \"teleport probability\" ($\\alpha$) to return to the root node. This ensures that the propagation process retains locality and avoids converging to a global, root-node-independent distribution, even with \"infinitely many\" propagation steps.\n    *   **PPNP Model:**\n        *   First, a neural network $f$ (e.g., a simple MLP) generates initial predictions $H$ for each node independently based on its features $X$.\n        *   These predictions $H$ are then propagated using a personalized PageRank matrix: $Z_{PPNP}=\\text{softmax} \\left( \\alpha \\left( I_n - (1-\\alpha)\\tilde{A} \\right)^{-1} H \\right)$ \\cite{klicpera20186xu}.\n        *   The model is trained end-to-end, allowing gradients to flow through the propagation scheme.\n    *   **APPNP (Approximate PPNP):** To address the $O(n^2)$ computational complexity of PPNP for large graphs, APPNP approximates the personalized PageRank via power iteration (a random walk with restarts). This iterative scheme maintains sparsity and achieves linear complexity $O(mK)$ where $m$ is the number of edges and $K$ is the number of power iterations.\n        *   $Z^{(0)}=H=f(X)$; $Z^{(k+1)}=(1-\\alpha)\\tilde{A}Z^{(k)}+\\alpha H$; $Z^{(K)}=\\text{softmax} \\left( (1-\\alpha)\\tilde{A}Z^{(K-1)}+\\alpha H \\right)$ \\cite{klicpera20186xu}.\n    *   **Adjustable Neighborhood:** The teleport probability $\\alpha$ allows for fine-tuning the balance between preserving locality and leveraging information from a large neighborhood, adapting to different graph types.\n\n4.  **Key Technical Contributions**\n    *   **Novel Propagation Scheme:** Derivation and application of a propagation scheme based on personalized PageRank, directly addressing the oversmoothing problem in GCNs.\n    *   **Decoupled Architecture:** Introduction of PPNP and APPNP, which explicitly separate the neural network's feature transformation from the graph's propagation. This allows for arbitrary propagation depth without increasing the neural network's parameter count or depth.\n    *   **Scalable Approximation:** APPNP provides an efficient, linear-time approximation of PPNP, making the approach practical for large graphs by avoiding dense matrix inversions.\n    *   **Adjustable Range:** The teleport probability $\\alpha$ offers a hyperparameter to control the effective range of propagation, enabling the model to leverage large neighborhoods while preserving local information.\n    *   **Flexibility:** The decoupled design allows PPNP/APPNP to be combined with any state-of-the-art neural network for initial feature prediction.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated PPNP and APPNP against several state-of-the-art GCN-like models (V.GCN, GCN, N-GCN, GAT, JK, Bt.FP) for semi-supervised node classification.\n        *   Tested on four standard text-classification graph datasets: Citeseer, Cora-ML, Pubmed, and MS Academic.\n        *   Employed a rigorous evaluation protocol: 100 runs on multiple random splits and initializations, a fixed visible/test set split, hyperparameter optimization only on Citeseer and Cora-ML (then applied across datasets), early stopping, and statistical significance testing (bootstrapping for confidence intervals, paired t-tests for p-values).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Accuracy:** PPNP and APPNP consistently and significantly outperformed all compared models across all datasets (e.g., on Cora-ML, APPNP achieved 85.09% vs. GAT's 84.37% and GCN's 83.41%).\n        *   **Efficiency:** APPNP demonstrated training times on par with or faster than previous models, with a comparable or lower number of parameters. PPNP, due to its $O(n^2)$ complexity, ran out of memory on larger datasets (Pubmed, MS Academic), validating the necessity of APPNP.\n        *   The rigorous evaluation highlighted that many previously reported improvements of other models \"vanish\" under careful statistical scrutiny, underscoring the robustness of PPNP/APPNP's gains.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The original PPNP formulation is computationally intensive ($O(n^2)$) and memory-demanding, making it impractical for large graphs without approximation. This is mitigated by APPNP.\n        *   The choice of teleport probability $\\alpha$ is a hyperparameter that needs tuning, although the paper shows it allows for flexibility.\n    *   **Scope of Applicability:**\n        *   Primarily focused on semi-supervised node classification on graphs.\n        *   Applicable to various graph types, especially those where long-range dependencies are important.\n        *   The initial feature transformation can be performed by \"any neural network,\" offering broad compatibility.\n\n7.  **Technical Significance**\n    *   **State-of-the-Art Advancement:** PPNP/APPNP achieves new state-of-the-art results in semi-supervised node classification, demonstrating superior accuracy and efficiency compared to existing GCN-like models.\n    *   **Fundamental Problem Resolution:** It provides a principled solution to the long-standing \"oversmoothing\" and limited range problems in graph neural networks by leveraging personalized PageRank.\n    *   **Architectural Paradigm Shift:** The decoupling of prediction and propagation offers a novel architectural design principle for GNNs, allowing for independent optimization of feature learning and graph diffusion.\n    *   **Improved Efficiency and Scalability:** APPNP's linear complexity and parameter efficiency make it highly scalable and practical for real-world large-scale graph applications.\n    *   **Enhanced Evaluation Standards:** The paper's emphasis on a rigorous experimental protocol sets a higher bar for future research in graph neural networks, promoting more reliable and reproducible results.",
    "intriguing_abstract": "Deep Graph Neural Networks (GNNs) face a fundamental dilemma: increasing propagation depth for broader neighborhood information often leads to **oversmoothing**, blurring node identities and limiting their representational power. We introduce a novel architectural paradigm that decouples the neural network's **feature transformation** from the graph's **propagation scheme**. Our method, **Personalized Propagation of Neural Predictions (PPNP)**, leverages a propagation derived from **Personalized PageRank** to fundamentally address oversmoothing, enabling arbitrary **propagation depth** without increasing model parameters. This approach preserves local information through a 'teleport probability' while integrating global context. To overcome PPNP's computational intensity for large graphs, we present **APPNP**, an efficient, linear-time approximation via power iteration, making deep propagation practical and scalable. Extensive experiments on **semi-supervised node classification** demonstrate that PPNP and APPNP consistently achieve new state-of-the-art performance across multiple benchmarks, offering a principled, robust, and efficient solution to a core challenge in **Graph Neural Networks**. This work redefines how **GNNs** can effectively utilize vast graph structures.",
    "keywords": [
      "Graph Neural Networks",
      "Oversmoothing problem",
      "Personalized PageRank",
      "PPNP (Personalized Propagation of Neural Predictions)",
      "APPNP (Approximate PPNP)",
      "Decoupled architecture",
      "Semi-supervised node classification",
      "Novel propagation scheme",
      "Teleport probability",
      "Scalable approximation",
      "State-of-the-art results",
      "Long-range dependencies"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/ac225094aab9e7b629bc5b3343e026dea0200c70.pdf",
    "citation_key": "klicpera20186xu",
    "metadata": {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
      "authors": [
        "Johannes Klicpera",
        "Aleksandar Bojchevski",
        "Stephan Günnemann"
      ],
      "published_date": "2018",
      "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
      "file_path": "paper_data/Graph_Neural_Networks/ac225094aab9e7b629bc5b3343e026dea0200c70.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK\" \\cite{klicpera20186xu} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   Existing neural message passing algorithms (e.g., GCNs) for semi-supervised graph classification are limited to considering only a small, local neighborhood (a few propagation steps) for node classification.\n    *   Extending this neighborhood size is challenging due to two main issues:\n        *   **Oversmoothing:** Too many propagation layers lead to Laplacian smoothing, causing node representations to become indistinguishable and losing local information. This is linked to GCNs converging to a global, root-node-independent limit distribution.\n        *   **Increased Complexity:** Most message passing schemes use learnable weight matrices in each layer, meaning a larger neighborhood requires a deeper neural network with more learnable parameters, leading to computational and memory overhead.\n    *   This fixed relationship between neighborhood size and neural network depth is a strong limitation, forcing compromises in model design.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself against various deep learning approaches on graphs, including node embedding methods (e.g., Node2Vec, DeepWalk) and supervised methods using both graph structure and node features.\n    *   It specifically focuses on message passing (or neighbor aggregation) algorithms (e.g., GCN, GAT, GraphSAGE) which have gained significant attention.\n    *   Previous attempts to improve these algorithms include attention mechanisms, random walks, edge features, and scalability improvements, but these still suffer from limited neighborhood range.\n    *   Methods using skip connections (e.g., residual connections) or batch normalization in deep GNNs have been proposed to mitigate oversmoothing, but these often complicate the model and introduce additional hyperparameters.\n    *   This work differentiates itself by addressing the range limitation through a fundamental architectural change rather than ad-hoc techniques or simply adding more layers/connections.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea:** The paper proposes to decouple the neural network's feature transformation (prediction) from the graph's propagation scheme. It introduces Personalized Propagation of Neural Predictions (PPNP) and its fast approximation, APPNP.\n    *   **Connection to PageRank:** It highlights the connection between GCN's limit distribution and PageRank, then leverages personalized PageRank to overcome the oversmoothing issue.\n    *   **Personalized PageRank for Propagation:** The propagation scheme is derived from personalized PageRank, which includes a \"teleport probability\" ($\\alpha$) to return to the root node. This ensures that the propagation process retains locality and avoids converging to a global, root-node-independent distribution, even with \"infinitely many\" propagation steps.\n    *   **PPNP Model:**\n        *   First, a neural network $f$ (e.g., a simple MLP) generates initial predictions $H$ for each node independently based on its features $X$.\n        *   These predictions $H$ are then propagated using a personalized PageRank matrix: $Z_{PPNP}=\\text{softmax} \\left( \\alpha \\left( I_n - (1-\\alpha)\\tilde{A} \\right)^{-1} H \\right)$ \\cite{klicpera20186xu}.\n        *   The model is trained end-to-end, allowing gradients to flow through the propagation scheme.\n    *   **APPNP (Approximate PPNP):** To address the $O(n^2)$ computational complexity of PPNP for large graphs, APPNP approximates the personalized PageRank via power iteration (a random walk with restarts). This iterative scheme maintains sparsity and achieves linear complexity $O(mK)$ where $m$ is the number of edges and $K$ is the number of power iterations.\n        *   $Z^{(0)}=H=f(X)$; $Z^{(k+1)}=(1-\\alpha)\\tilde{A}Z^{(k)}+\\alpha H$; $Z^{(K)}=\\text{softmax} \\left( (1-\\alpha)\\tilde{A}Z^{(K-1)}+\\alpha H \\right)$ \\cite{klicpera20186xu}.\n    *   **Adjustable Neighborhood:** The teleport probability $\\alpha$ allows for fine-tuning the balance between preserving locality and leveraging information from a large neighborhood, adapting to different graph types.\n\n4.  **Key Technical Contributions**\n    *   **Novel Propagation Scheme:** Derivation and application of a propagation scheme based on personalized PageRank, directly addressing the oversmoothing problem in GCNs.\n    *   **Decoupled Architecture:** Introduction of PPNP and APPNP, which explicitly separate the neural network's feature transformation from the graph's propagation. This allows for arbitrary propagation depth without increasing the neural network's parameter count or depth.\n    *   **Scalable Approximation:** APPNP provides an efficient, linear-time approximation of PPNP, making the approach practical for large graphs by avoiding dense matrix inversions.\n    *   **Adjustable Range:** The teleport probability $\\alpha$ offers a hyperparameter to control the effective range of propagation, enabling the model to leverage large neighborhoods while preserving local information.\n    *   **Flexibility:** The decoupled design allows PPNP/APPNP to be combined with any state-of-the-art neural network for initial feature prediction.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated PPNP and APPNP against several state-of-the-art GCN-like models (V.GCN, GCN, N-GCN, GAT, JK, Bt.FP) for semi-supervised node classification.\n        *   Tested on four standard text-classification graph datasets: Citeseer, Cora-ML, Pubmed, and MS Academic.\n        *   Employed a rigorous evaluation protocol: 100 runs on multiple random splits and initializations, a fixed visible/test set split, hyperparameter optimization only on Citeseer and Cora-ML (then applied across datasets), early stopping, and statistical significance testing (bootstrapping for confidence intervals, paired t-tests for p-values).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Accuracy:** PPNP and APPNP consistently and significantly outperformed all compared models across all datasets (e.g., on Cora-ML, APPNP achieved 85.09% vs. GAT's 84.37% and GCN's 83.41%).\n        *   **Efficiency:** APPNP demonstrated training times on par with or faster than previous models, with a comparable or lower number of parameters. PPNP, due to its $O(n^2)$ complexity, ran out of memory on larger datasets (Pubmed, MS Academic), validating the necessity of APPNP.\n        *   The rigorous evaluation highlighted that many previously reported improvements of other models \"vanish\" under careful statistical scrutiny, underscoring the robustness of PPNP/APPNP's gains.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The original PPNP formulation is computationally intensive ($O(n^2)$) and memory-demanding, making it impractical for large graphs without approximation. This is mitigated by APPNP.\n        *   The choice of teleport probability $\\alpha$ is a hyperparameter that needs tuning, although the paper shows it allows for flexibility.\n    *   **Scope of Applicability:**\n        *   Primarily focused on semi-supervised node classification on graphs.\n        *   Applicable to various graph types, especially those where long-range dependencies are important.\n        *   The initial feature transformation can be performed by \"any neural network,\" offering broad compatibility.\n\n7.  **Technical Significance**\n    *   **State-of-the-Art Advancement:** PPNP/APPNP achieves new state-of-the-art results in semi-supervised node classification, demonstrating superior accuracy and efficiency compared to existing GCN-like models.\n    *   **Fundamental Problem Resolution:** It provides a principled solution to the long-standing \"oversmoothing\" and limited range problems in graph neural networks by leveraging personalized PageRank.\n    *   **Architectural Paradigm Shift:** The decoupling of prediction and propagation offers a novel architectural design principle for GNNs, allowing for independent optimization of feature learning and graph diffusion.\n    *   **Improved Efficiency and Scalability:** APPNP's linear complexity and parameter efficiency make it highly scalable and practical for real-world large-scale graph applications.\n    *   **Enhanced Evaluation Standards:** The paper's emphasis on a rigorous experimental protocol sets a higher bar for future research in graph neural networks, promoting more reliable and reproducible results.",
      "keywords": [
        "Graph Neural Networks",
        "Oversmoothing problem",
        "Personalized PageRank",
        "PPNP (Personalized Propagation of Neural Predictions)",
        "APPNP (Approximate PPNP)",
        "Decoupled architecture",
        "Semi-supervised node classification",
        "Novel propagation scheme",
        "Teleport probability",
        "Scalable approximation",
        "State-of-the-art results",
        "Long-range dependencies"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we use the relationship between graph convolutional networks (gcn) and pagerank to **derive an improved propagation scheme** based on personalized pagerank. we utilize this propagation procedure to **construct a simple model**, personalized propagation of neural predic-tions (ppnp), and its fast approximation, appnp.\"\n*   it mentions the model's characteristics: \"our model’s training time is on par or faster and its number of parameters on par or lower than previous models.\"\n*   it also mentions empirical validation: \"we show that this model outperforms several recently proposed methods for semi-supervised classiﬁcation in the most thorough study done so far for gc\". while there's an empirical component, it's to validate the *newly proposed model*.\n*   the introduction further emphasizes the new model's performance and availability (\"our implementation is available online.\").\n\nthis content directly aligns with the \"technical\" classification criteria: \"presents new methods, algorithms, or systems\" and mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\". the paper's primary contribution is the development and presentation of the ppnp and appnp models.\n\n**classification: technical**"
    },
    "file_name": "ac225094aab9e7b629bc5b3343e026dea0200c70.pdf"
  },
  {
    "success": true,
    "doc_id": "b85db5b40aff356ffd13fedb6fa54698",
    "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analysis of \"Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View\" \\cite{chen2019s47}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the \"over-smoothing\" issue in Graph Neural Networks (GNNs), where node representations from different classes become indistinguishable after stacking multiple layers.\n    *   **Why important and challenging:** Over-smoothing severely degrades GNN model performance (e.g., classification accuracy). Despite its prevalence, there has been limited systematic and quantitative study on *why* and *how* over-smoothing occurs, making it challenging to effectively mitigate.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** While GNNs have shown strong performance in various graph-based tasks, the over-smoothing problem is a known limitation.\n    *   **Limitations of previous solutions:** Prior research primarily focused on designing novel GNN architectures. This work distinguishes itself by conducting a systematic, quantitative study of the over-smoothing phenomenon itself, proposing new metrics, and offering solutions from a \"topological view\" rather than solely architectural modifications.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm:**\n        *   **Quantitative Measurement:** Introduces two novel metrics: Mean Average Distance (MAD) to measure general smoothness of node representations, and MADGap (difference between MAD of remote and neighboring nodes) to specifically quantify over-smoothness.\n        *   **Problem Diagnosis:** Empirically verifies that smoothing is an inherent nature of GNNs. Identifies the \"low information-to-noise ratio\" in message passing, largely influenced by graph topology (e.g., many inter-class edges), as the critical factor leading to over-smoothing.\n        *   **Alleviation Methods (Topological View):**\n            *   **MADReg:** A regularization term based on MADGap is added to the GNN training objective to directly encourage a larger gap between intra-class and inter-class similarities.\n            *   **AdaEdge:** An adaptive edge optimization method that iteratively adjusts the graph topology by removing/adding edges based on model predictions to better align the graph structure with the downstream task.\n    *   **What makes this approach novel or different:** The novelty lies in providing a systematic and quantitative framework (MAD, MADGap) to understand and measure over-smoothing, identifying the information-to-noise ratio and graph topology as key causal factors, and proposing topology-aware solutions (MADReg, AdaEdge) rather than just architectural changes.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   **MAD (Mean Average Distance):** A quantitative metric to measure the global smoothness of graph node representations.\n        *   **MADGap:** A quantitative metric to measure the over-smoothness of graph representations by differentiating between remote and neighboring node similarities, showing high correlation with model performance.\n        *   **MADReg:** A novel regularization technique that incorporates MADGap into the GNN training loss to mitigate over-smoothing.\n        *   **AdaEdge:** An adaptive graph topology optimization method that dynamically modifies graph edges based on model predictions to improve the information-to-noise ratio.\n    *   **Theoretical insights or analysis:** Empirically demonstrates that smoothing is fundamental to GNNs and that a low information-to-noise ratio, heavily influenced by graph topology, is the primary cause of over-smoothing.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on 7 widely-used graph datasets (citation, coauthor, Amazon product networks) and 10 typical GNN models (e.g., GCN, GAT, GraphSAGE) to ensure generalizability.\n    *   **Key performance metrics and comparison results:**\n        *   The correlation between MADGap and prediction accuracy was statistically validated (e.g., Pearson coefficient > 0.9 for GCN on CORA).\n        *   The impact of information-to-noise ratio on MADGap and model performance was demonstrated.\n        *   Both MADReg and AdaEdge were shown to significantly reduce over-smoothness (increase MADGap) and consistently improve the performance (e.g., classification accuracy) of various GNN models across different datasets.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The analysis and proposed methods are primarily validated for the node classification task. The MADGap metric relies on topological distance to approximate node categories, which might not always perfectly align with semantic classes.\n    *   **Scope of applicability:** While the methods are shown to be effective across various GNN architectures, their direct applicability might be most pronounced in scenarios where graph topology can be modified or regularized during training.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art:** This paper provides a foundational quantitative framework for understanding and measuring the over-smoothing problem in GNNs, moving beyond qualitative observations. By identifying graph topology and information-to-noise ratio as key factors, it shifts the focus from solely GNN architecture design to also considering and optimizing the underlying graph structure.\n    *   **Potential impact on future research:** The proposed metrics (MAD, MADGap) can serve as valuable diagnostic tools for future GNN research. The topological view and methods (MADReg, AdaEdge) open new avenues for developing more robust GNNs by actively managing graph structure and message quality, potentially leading to GNNs that can effectively leverage deeper architectures without suffering from over-smoothing \\cite{chen2019s47}.",
    "intriguing_abstract": "Deep Graph Neural Networks (GNNs) promise powerful insights into complex graph data, yet their potential is profoundly limited by the ubiquitous \"over-smoothing\" problem, where node representations from different classes become indistinguishable after stacking multiple layers. This paper presents a pioneering systematic and quantitative investigation into over-smoothing from a novel topological perspective. We introduce two crucial metrics, Mean Average Distance (MAD) and MADGap, to precisely measure global smoothness and over-smoothness, demonstrating MADGap's strong correlation with GNN performance. Our analysis reveals that a low information-to-noise ratio during message passing, heavily influenced by graph topology, is the primary culprit. To combat this, we propose MADReg, a regularization term leveraging MADGap, and AdaEdge, an adaptive graph topology optimization method that dynamically refines edges. Extensive experiments across diverse datasets and GNN architectures confirm that our topology-aware solutions significantly mitigate over-smoothing, boosting classification accuracy. This work provides foundational diagnostic tools and opens new avenues for designing robust, deeper GNNs by actively managing graph structure, moving beyond purely architectural modifications.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "over-smoothing problem",
      "node representations",
      "graph topology",
      "information-to-noise ratio",
      "Mean Average Distance (MAD)",
      "MADGap",
      "MADReg (regularization)",
      "AdaEdge (graph optimization)",
      "quantitative framework",
      "node classification",
      "message passing"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/94194703e83b5447f519fd8bcbb903916e05aaf9.pdf",
    "citation_key": "chen2019s47",
    "metadata": {
      "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View",
      "authors": [
        "Deli Chen",
        "Yankai Lin",
        "Wei Li",
        "Peng Li",
        "Jie Zhou",
        "Xu Sun"
      ],
      "published_date": "2019",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.",
      "file_path": "paper_data/Graph_Neural_Networks/94194703e83b5447f519fd8bcbb903916e05aaf9.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analysis of \"Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View\" \\cite{chen2019s47}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the \"over-smoothing\" issue in Graph Neural Networks (GNNs), where node representations from different classes become indistinguishable after stacking multiple layers.\n    *   **Why important and challenging:** Over-smoothing severely degrades GNN model performance (e.g., classification accuracy). Despite its prevalence, there has been limited systematic and quantitative study on *why* and *how* over-smoothing occurs, making it challenging to effectively mitigate.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** While GNNs have shown strong performance in various graph-based tasks, the over-smoothing problem is a known limitation.\n    *   **Limitations of previous solutions:** Prior research primarily focused on designing novel GNN architectures. This work distinguishes itself by conducting a systematic, quantitative study of the over-smoothing phenomenon itself, proposing new metrics, and offering solutions from a \"topological view\" rather than solely architectural modifications.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm:**\n        *   **Quantitative Measurement:** Introduces two novel metrics: Mean Average Distance (MAD) to measure general smoothness of node representations, and MADGap (difference between MAD of remote and neighboring nodes) to specifically quantify over-smoothness.\n        *   **Problem Diagnosis:** Empirically verifies that smoothing is an inherent nature of GNNs. Identifies the \"low information-to-noise ratio\" in message passing, largely influenced by graph topology (e.g., many inter-class edges), as the critical factor leading to over-smoothing.\n        *   **Alleviation Methods (Topological View):**\n            *   **MADReg:** A regularization term based on MADGap is added to the GNN training objective to directly encourage a larger gap between intra-class and inter-class similarities.\n            *   **AdaEdge:** An adaptive edge optimization method that iteratively adjusts the graph topology by removing/adding edges based on model predictions to better align the graph structure with the downstream task.\n    *   **What makes this approach novel or different:** The novelty lies in providing a systematic and quantitative framework (MAD, MADGap) to understand and measure over-smoothing, identifying the information-to-noise ratio and graph topology as key causal factors, and proposing topology-aware solutions (MADReg, AdaEdge) rather than just architectural changes.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   **MAD (Mean Average Distance):** A quantitative metric to measure the global smoothness of graph node representations.\n        *   **MADGap:** A quantitative metric to measure the over-smoothness of graph representations by differentiating between remote and neighboring node similarities, showing high correlation with model performance.\n        *   **MADReg:** A novel regularization technique that incorporates MADGap into the GNN training loss to mitigate over-smoothing.\n        *   **AdaEdge:** An adaptive graph topology optimization method that dynamically modifies graph edges based on model predictions to improve the information-to-noise ratio.\n    *   **Theoretical insights or analysis:** Empirically demonstrates that smoothing is fundamental to GNNs and that a low information-to-noise ratio, heavily influenced by graph topology, is the primary cause of over-smoothing.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on 7 widely-used graph datasets (citation, coauthor, Amazon product networks) and 10 typical GNN models (e.g., GCN, GAT, GraphSAGE) to ensure generalizability.\n    *   **Key performance metrics and comparison results:**\n        *   The correlation between MADGap and prediction accuracy was statistically validated (e.g., Pearson coefficient > 0.9 for GCN on CORA).\n        *   The impact of information-to-noise ratio on MADGap and model performance was demonstrated.\n        *   Both MADReg and AdaEdge were shown to significantly reduce over-smoothness (increase MADGap) and consistently improve the performance (e.g., classification accuracy) of various GNN models across different datasets.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The analysis and proposed methods are primarily validated for the node classification task. The MADGap metric relies on topological distance to approximate node categories, which might not always perfectly align with semantic classes.\n    *   **Scope of applicability:** While the methods are shown to be effective across various GNN architectures, their direct applicability might be most pronounced in scenarios where graph topology can be modified or regularized during training.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art:** This paper provides a foundational quantitative framework for understanding and measuring the over-smoothing problem in GNNs, moving beyond qualitative observations. By identifying graph topology and information-to-noise ratio as key factors, it shifts the focus from solely GNN architecture design to also considering and optimizing the underlying graph structure.\n    *   **Potential impact on future research:** The proposed metrics (MAD, MADGap) can serve as valuable diagnostic tools for future GNN research. The topological view and methods (MADReg, AdaEdge) open new avenues for developing more robust GNNs by actively managing graph structure and message quality, potentially leading to GNNs that can effectively leverage deeper architectures without suffering from over-smoothing \\cite{chen2019s47}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "over-smoothing problem",
        "node representations",
        "graph topology",
        "information-to-noise ratio",
        "Mean Average Distance (MAD)",
        "MADGap",
        "MADReg (regularization)",
        "AdaEdge (graph optimization)",
        "quantitative framework",
        "node classification",
        "message passing"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the paper identifies a \"severe limitation of gnns\" (over-smoothing issue).\n*   it \"introduce[s] two quantitative metrics, mad and madgap,\" which are new tools/methods for measurement.\n*   it \"propose[s] two methods to alleviate the over-smoothing issue from the topological view: (1) madreg... (2) adaedge.\" these are new algorithms/systems.\n*   it then describes \"extensive experiments on 7widely-used graph datasets with 10typical gnn models show that the two proposed methods are effective.\" this is the empirical validation of the proposed technical solutions.\n*   the introduction further emphasizes the \"systematic and quantitative study\" and the aim to \"reveal what is the crucial factor\" and \"find out a reasonable dir[ection],\" which leads to the proposed methods.\n\nthe primary contribution is the development of new metrics and new methods/algorithms to solve a specific technical problem in graph neural networks, which are then empirically validated. this aligns perfectly with the definition of a **technical** paper. while it includes significant empirical work, that work serves to validate the *new methods* proposed, making it a technical paper with strong empirical components, rather than purely empirical.\n\n**classification: technical**"
    },
    "file_name": "94194703e83b5447f519fd8bcbb903916e05aaf9.pdf"
  },
  {
    "success": true,
    "doc_id": "f8c41d42b8df10a0f7ded05dcf2b3998",
    "summary": "Here's a focused summary of the paper \"Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage\" \\cite{wang2022531} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) often inherit historical biases from training data, leading to discriminatory predictions. Existing fair GNN methods typically overlook a critical issue: \"sensitive attribute leakage\" caused by feature propagation. This leakage occurs when feature propagation in GNNs varies the correlation of previously innocuous non-sensitive features to sensitive ones, causing them to encode more sensitive information and exacerbate discrimination.\n    *   **Importance and Challenge**: Ensuring fairness in GNNs is crucial for their responsible deployment in sensitive applications. The challenge lies in the dynamic nature of feature correlations within GNNs; features that appear non-sensitive initially can become highly correlated with sensitive attributes after propagation, making static debiasing approaches insufficient.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on fair GNNs primarily borrows techniques from traditional fair representation learning (e.g., adversarial debiasing, contrastive learning) or directly debiases node features and graph topology.\n    *   **Limitations of Previous Solutions**: These existing methods fail to account for the \"sensitive attribute leakage\" phenomenon, where feature propagation dynamically alters feature correlations, leading to non-sensitive features becoming sensitive-correlated. This oversight means they may not effectively mitigate bias introduced or amplified by the GNN's propagation mechanism.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Fair View Graph Neural Network (FairVGNN), a principled framework designed to learn fair node representations by mitigating sensitive attribute leakage. It comprises two main modules:\n        1.  **Generative Adversarial Debiasing Module**: A fair view generator learns to automatically identify and mask sensitive-correlated features. This module generates multiple \"fair views\" of features by sampling learnable masks (using Gumbel-Softmax for differentiability) that consider how feature correlations change after propagation.\n        2.  **Adaptive Weight Clamping Module**: This module adaptively clamps the weights of the GNN encoder for sensitive-related feature channels. This mechanism is theoretically justified as minimizing the upper bound of the representation difference between different sensitive groups.\n    *   **Novelty/Difference**: FairVGNN's novelty lies in its explicit recognition and mitigation of \"sensitive attribute leakage\" and \"correlation variation\" during GNN feature propagation. Unlike prior work, it dynamically learns which features become sensitive-correlated *after* propagation and employs a two-pronged approach (feature masking via adversarial learning and adaptive weight clamping) to address this.\n\n*   **Key Technical Contributions**\n    *   **Novel Phenomenon Identification**: The paper formally identifies and empirically verifies the \"sensitive attribute leakage\" phenomenon in GNNs, where feature propagation causes correlation variation, leading to innocuous features becoming sensitive-correlated and exacerbating discrimination.\n    *   **Novel Algorithm (FairVGNN)**:\n        *   A **Generative Adversarial Debiasing Module** that learns a fair view generator to automatically identify and mask sensitive-correlated feature channels, accounting for correlation changes due to feature propagation.\n        *   An **Adaptive Weight Clamping Module** that clamps GNN encoder weights for sensitive-related channels, with theoretical justification for minimizing representation differences between sensitive groups.\n    *   **Empirical Insight**: Demonstrates that masking features based on *propagated* correlations (rather than just original correlations) is crucial for achieving better fairness, highlighting the importance of considering feature propagation's effect on bias.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Initial empirical studies on German and Credit datasets to demonstrate sensitive attribute leakage and correlation variation (Figure 1, Table 1).\n        *   Extensive experiments evaluating FairVGNN's performance on real-world datasets (German, Credit) against various baselines (MLP, GCN, GIN with different masking strategies).\n    *   **Key Performance Metrics**: Model utility is measured by AUC and F1 score. Fairness is evaluated using Δsp (difference of statistical parity) and Δeo (difference of equal opportunity).\n    *   **Comparison Results**: FairVGNN consistently achieves a better trade-off between model utility and fairness compared to baseline methods. The results show that considering correlation variation (e.g., masking based on propagated correlations) significantly improves fairness, especially on datasets where such variation is prominent.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The current framework primarily focuses on group fairness with binary sensitive attributes, though the authors suggest generalization to multi-sensitive groups as future work. The generative adversarial debiasing module currently assumes a fixed network topology, leaving joint generation of fair feature and topological views for future research.\n    *   **Scope of Applicability**: FairVGNN is applicable to GNN-based node representation learning tasks where fairness is a concern, particularly in scenarios where feature propagation might inadvertently amplify or introduce bias through sensitive attribute leakage.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art in fair GNNs by identifying and providing a novel solution to the previously overlooked problem of sensitive attribute leakage due to feature propagation. It offers a more nuanced understanding of how bias propagates and manifests in GNNs.\n    *   **Potential Impact**: FairVGNN provides a robust and principled framework for building fairer GNN models. Its insights into dynamic feature correlation and the proposed mitigation strategies could inspire future research in debiasing complex graph-based models, leading to more trustworthy and ethically sound AI systems in various domains. The public availability of the code further facilitates its adoption and extension.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful, yet their widespread adoption is critically hampered by inherent biases leading to discriminatory predictions. A pervasive, often overlooked, challenge is \"sensitive attribute leakage,\" where feature propagation dynamically alters feature correlations, causing initially innocuous non-sensitive features to encode sensitive information and exacerbate bias. Existing fair GNN methods largely fail to account for this dynamic \"correlation variation.\"\n\nWe introduce FairVGNN, a novel and principled framework designed to mitigate sensitive attribute leakage and learn truly fair node representations. FairVGNN comprises two innovative modules: a Generative Adversarial Debiasing Module that intelligently identifies and masks sensitive-correlated features *after* propagation, and an Adaptive Weight Clamping Module theoretically justified to minimize representation differences between sensitive groups. Our work formally identifies and empirically verifies this leakage phenomenon, demonstrating that debiasing based on propagated correlations is crucial. FairVGNN consistently achieves a superior utility-fairness trade-off, significantly advancing the state-of-the-art in fair GNNs and paving the way for more trustworthy and ethically sound AI systems.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "sensitive attribute leakage",
      "feature propagation",
      "fairness in GNNs",
      "correlation variation",
      "FairVGNN",
      "Generative Adversarial Debiasing Module",
      "Adaptive Weight Clamping Module",
      "feature masking",
      "dynamic feature correlation",
      "mitigating bias",
      "node representation learning",
      "utility-fairness trade-off",
      "group fairness"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf",
    "citation_key": "wang2022531",
    "metadata": {
      "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage",
      "authors": [
        "Yu Wang",
        "Yuying Zhao",
        "Yushun Dong",
        "Huiyuan Chen",
        "Jundong Li",
        "Tyler Derr"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) have shown great power in learning node representations on graphs. However, they may inherit historical prejudices from training data, leading to discriminatory bias in predictions. Although some work has developed fair GNNs, most of them directly borrow fair representation learning techniques from non-graph domains without considering the potential problem of sensitive attribute leakage caused by feature propagation in GNNs. However, we empirically observe that feature propagation could vary the correlation of previously innocuous non-sensitive features to the sensitive ones. This can be viewed as a leakage of sensitive information which could further exacerbate discrimination in predictions. Thus, we design two feature masking strategies according to feature correlations to highlight the importance of considering feature propagation and correlation variation in alleviating discrimination. Motivated by our analysis, we propose Fair View Graph Neural Network (FairVGNN) to generate fair views of features by automatically identifying and masking sensitive-correlated features considering correlation variation after feature propagation. Given the learned fair views, we adaptively clamp weights of the encoder to avoid using sensitive-related features. Experiments on real-world datasets demonstrate that FairVGNN enjoys a better trade-off between model utility and fairness.",
      "file_path": "paper_data/Graph_Neural_Networks/0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the paper \"Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage\" \\cite{wang2022531} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) often inherit historical biases from training data, leading to discriminatory predictions. Existing fair GNN methods typically overlook a critical issue: \"sensitive attribute leakage\" caused by feature propagation. This leakage occurs when feature propagation in GNNs varies the correlation of previously innocuous non-sensitive features to sensitive ones, causing them to encode more sensitive information and exacerbate discrimination.\n    *   **Importance and Challenge**: Ensuring fairness in GNNs is crucial for their responsible deployment in sensitive applications. The challenge lies in the dynamic nature of feature correlations within GNNs; features that appear non-sensitive initially can become highly correlated with sensitive attributes after propagation, making static debiasing approaches insufficient.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on fair GNNs primarily borrows techniques from traditional fair representation learning (e.g., adversarial debiasing, contrastive learning) or directly debiases node features and graph topology.\n    *   **Limitations of Previous Solutions**: These existing methods fail to account for the \"sensitive attribute leakage\" phenomenon, where feature propagation dynamically alters feature correlations, leading to non-sensitive features becoming sensitive-correlated. This oversight means they may not effectively mitigate bias introduced or amplified by the GNN's propagation mechanism.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Fair View Graph Neural Network (FairVGNN), a principled framework designed to learn fair node representations by mitigating sensitive attribute leakage. It comprises two main modules:\n        1.  **Generative Adversarial Debiasing Module**: A fair view generator learns to automatically identify and mask sensitive-correlated features. This module generates multiple \"fair views\" of features by sampling learnable masks (using Gumbel-Softmax for differentiability) that consider how feature correlations change after propagation.\n        2.  **Adaptive Weight Clamping Module**: This module adaptively clamps the weights of the GNN encoder for sensitive-related feature channels. This mechanism is theoretically justified as minimizing the upper bound of the representation difference between different sensitive groups.\n    *   **Novelty/Difference**: FairVGNN's novelty lies in its explicit recognition and mitigation of \"sensitive attribute leakage\" and \"correlation variation\" during GNN feature propagation. Unlike prior work, it dynamically learns which features become sensitive-correlated *after* propagation and employs a two-pronged approach (feature masking via adversarial learning and adaptive weight clamping) to address this.\n\n*   **Key Technical Contributions**\n    *   **Novel Phenomenon Identification**: The paper formally identifies and empirically verifies the \"sensitive attribute leakage\" phenomenon in GNNs, where feature propagation causes correlation variation, leading to innocuous features becoming sensitive-correlated and exacerbating discrimination.\n    *   **Novel Algorithm (FairVGNN)**:\n        *   A **Generative Adversarial Debiasing Module** that learns a fair view generator to automatically identify and mask sensitive-correlated feature channels, accounting for correlation changes due to feature propagation.\n        *   An **Adaptive Weight Clamping Module** that clamps GNN encoder weights for sensitive-related channels, with theoretical justification for minimizing representation differences between sensitive groups.\n    *   **Empirical Insight**: Demonstrates that masking features based on *propagated* correlations (rather than just original correlations) is crucial for achieving better fairness, highlighting the importance of considering feature propagation's effect on bias.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Initial empirical studies on German and Credit datasets to demonstrate sensitive attribute leakage and correlation variation (Figure 1, Table 1).\n        *   Extensive experiments evaluating FairVGNN's performance on real-world datasets (German, Credit) against various baselines (MLP, GCN, GIN with different masking strategies).\n    *   **Key Performance Metrics**: Model utility is measured by AUC and F1 score. Fairness is evaluated using Δsp (difference of statistical parity) and Δeo (difference of equal opportunity).\n    *   **Comparison Results**: FairVGNN consistently achieves a better trade-off between model utility and fairness compared to baseline methods. The results show that considering correlation variation (e.g., masking based on propagated correlations) significantly improves fairness, especially on datasets where such variation is prominent.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The current framework primarily focuses on group fairness with binary sensitive attributes, though the authors suggest generalization to multi-sensitive groups as future work. The generative adversarial debiasing module currently assumes a fixed network topology, leaving joint generation of fair feature and topological views for future research.\n    *   **Scope of Applicability**: FairVGNN is applicable to GNN-based node representation learning tasks where fairness is a concern, particularly in scenarios where feature propagation might inadvertently amplify or introduce bias through sensitive attribute leakage.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art in fair GNNs by identifying and providing a novel solution to the previously overlooked problem of sensitive attribute leakage due to feature propagation. It offers a more nuanced understanding of how bias propagates and manifests in GNNs.\n    *   **Potential Impact**: FairVGNN provides a robust and principled framework for building fairer GNN models. Its insights into dynamic feature correlation and the proposed mitigation strategies could inspire future research in debiasing complex graph-based models, leading to more trustworthy and ethically sound AI systems in various domains. The public availability of the code further facilitates its adoption and extension.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "sensitive attribute leakage",
        "feature propagation",
        "fairness in GNNs",
        "correlation variation",
        "FairVGNN",
        "Generative Adversarial Debiasing Module",
        "Adaptive Weight Clamping Module",
        "feature masking",
        "dynamic feature correlation",
        "mitigating bias",
        "node representation learning",
        "utility-fairness trade-off",
        "group fairness"
      ],
      "paper_type": "based on the abstract and introduction, this paper should be classified as **technical**.\n\nhere's the reasoning:\n\n1.  **strong indicators for \"technical\":**\n    *   the abstract explicitly states: \"thus, we **design** two feature masking strategies...\" and \"we **propose** fair view graph neural network (fairvgnn) to generate fair views of features...\" this directly aligns with the \"technical\" criterion: \"presents new methods, algorithms, or systems.\"\n    *   it discusses a \"proposed solution\" (fairvgnn) to a \"technical problem\" (discriminatory bias in gnns due to sensitive attribute leakage).\n\n2.  **presence of \"empirical\" elements:**\n    *   the abstract mentions: \"however, we **empirically observe** that feature propagation could vary the correlation...\" and \"**experiments on real-world datasets demonstrate** that fairvgnn enjoys a better trade-off...\" these are strong indicators of empirical work.\n\n3.  **why \"technical\" is the primary classification over \"empirical\":**\n    while the paper clearly involves empirical observations and experiments, these serve to *motivate* the design of a new system (fairvgnn) and to *validate* its effectiveness. the core contribution of the paper is the *development and proposal* of this novel method/system. many technical papers include empirical evaluation as a necessary component to demonstrate the utility of their proposed methods. if the paper were purely empirical, it might focus solely on analyzing existing methods, datasets, or phenomena without proposing a new algorithmic solution. here, the new solution (fairvgnn) is central.\n\ntherefore, the paper's primary contribution is the introduction of a new technical solution, supported by empirical findings and evaluations."
    },
    "file_name": "0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf"
  },
  {
    "success": true,
    "doc_id": "c38a55ce664786ff6d33ddcd3e4be185",
    "summary": "Here's a focused summary of the technical paper \\cite{zhou20213lg} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) suffer from the \"over-smoothing\" issue, where node embeddings converge to indistinguishable vectors when stacking many layers \\cite{zhou20213lg}. This significantly degrades performance.\n    *   **Importance and Challenge**: Over-smoothing prevents GNNs from leveraging deep architectures to model dependencies to high-order neighbors, which are crucial for many tasks. The challenge lies in the lack of a generalizable and theoretical principle to guide the design and training of deep GNNs, as existing solutions are often heuristic or borrowed from CNNs without graph-specific theoretical grounding \\cite{zhou20213lg}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous methods to alleviate over-smoothing include embedding normalization, residual connections, and random data augmentation \\cite{zhou20213lg}.\n    *   **Limitations of Previous Solutions**: Many are motivated by techniques from Convolutional Neural Networks (CNNs) or rely on heuristic strategies. Most only achieve comparable or even worse performance than their shallow counterparts. There is a lack of a generalizable principle and theoretical analysis to guide their design and selection \\cite{zhou20213lg}. While Dirichlet energy has been used to quantify over-smoothing, no methods leveraged it to overcome the issue.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a \"Dirichlet energy constrained learning\" principle to guide the training of deep GNNs by regularizing the Dirichlet energy of node embeddings at each layer \\cite{zhou20213lg}. This principle defines an appropriate lower and upper range for Dirichlet energy to prevent both over-smoothing (energy too small) and over-separating (energy too large).\n    *   **Novelty**: The approach is novel because it provides a theoretical and generalizable principle based on Dirichlet energy, which is then directly translated into a practical deep GNN framework called Energetic Graph Neural Networks (EGNN) \\cite{zhou20213lg}. EGNN incorporates three specific components designed to satisfy these energy constraints: orthogonal weight controlling, lower-bounded residual connection, and Shifted ReLU (SReLU) activation.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Dirichlet Energy Constrained Learning Principle**: A theoretical principle that defines appropriate lower and upper limits for Dirichlet energy at each layer to guide deep GNN training, overcoming over-smoothing and over-separating \\cite{zhou20213lg}.\n        *   **Orthogonal Weight Controlling**: Initializes trainable weights as diagonal matrices with explicit singular values and regularizes them to meet the upper energy limit during training \\cite{zhou20213lg}.\n        *   **Lower-bounded Residual Connection**: Introduces residual connections with strengths (`alpha` and `beta`) determined by the lower energy limit, ensuring `E(X(k)) >= c_min * E(X(k-1))` and preventing over-smoothing \\cite{zhou20213lg}.\n        *   **SReLU Activation**: Employs a Shifted ReLU (SReLU) with a trainable shift `b` to provide a trade-off between non-linear and linear mappings, addressing the issue of ReLU further reducing Dirichlet energy \\cite{zhou20213lg}.\n    *   **Theoretical Insights**: Provides theoretical analysis (Lemmas 1-6) on how Dirichlet energy is bounded and affected by GNN components (weights, activations), forming the basis for the proposed principle and EGNN design \\cite{zhou20213lg}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted on benchmark node classification datasets (e.g., Cora, Citeseer, PubMed, Coauthor CS, Amazon Computers/Photo) to evaluate EGNN's performance with increasing layers \\cite{zhou20213lg}.\n    *   **Key Performance Metrics and Comparison Results**: EGNN achieves state-of-the-art performance by effectively utilizing deep layers (up to 64 layers). The paper demonstrates that EGNN can be easily trained to reach 64 layers and achieves surprisingly competitive performance on benchmarks, outperforming existing deep GNN methods \\cite{zhou20213lg}. The optimal energy value is shown to locate within the proposed range.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical derivations for Dirichlet energy bounds initially simplify GCN by removing non-linear activation, which is later addressed in the model design \\cite{zhou20213lg}. The conditions for Lemma 4 and 5 (e.g., `c_max >= c_min / (2*c_min - 1)^2`) impose specific relationships between hyperparameters.\n    *   **Scope of Applicability**: The framework is primarily demonstrated for node classification tasks. While the principle is generalizable, its direct application and hyperparameter tuning might vary for other graph-based tasks.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing a theoretically grounded and generalizable principle (Dirichlet energy constrained learning) for designing and training deep GNNs \\cite{zhou20213lg}. It moves beyond heuristic solutions by offering a unified framework that can explain and improve existing techniques.\n    *   **Potential Impact on Future Research**: The proposed principle and EGNN framework offer a robust foundation for future research into deep GNN architectures. It opens avenues for exploring other energy-based regularization techniques, novel activation functions, and weight initialization strategies tailored for deep graph learning, potentially enabling GNNs to effectively model even higher-order dependencies in complex networks \\cite{zhou20213lg}.",
    "intriguing_abstract": "Deep Graph Neural Networks (GNNs) hold immense promise for modeling complex relational data, yet their potential is severely hampered by the pervasive \"over-smoothing\" problem, where node embeddings become indistinguishable in deeper layers. This critical limitation prevents GNNs from effectively capturing high-order dependencies, often forcing reliance on shallow, sub-optimal architectures. We introduce a groundbreaking theoretical principle: **Dirichlet energy constrained learning**, offering a principled solution to this long-standing challenge.\n\nOur novel approach leverages Dirichlet energy not merely as a diagnostic tool, but as a direct regularization mechanism to guide the training of deep GNNs. By defining appropriate lower and upper bounds for Dirichlet energy at each layer, we prevent both over-smoothing and over-separating. This principle is instantiated in **Energetic Graph Neural Networks (EGNN)**, a robust framework incorporating orthogonal weight controlling, lower-bounded residual connections, and Shifted ReLU (SReLU) activation. EGNN demonstrably enables the training of GNNs up to 64 layers, achieving state-of-the-art performance on benchmark node classification tasks. This work provides a generalizable, theoretically-grounded foundation for truly deep graph learning, moving beyond heuristic solutions and paving the way for future advancements in complex network analysis.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "over-smoothing issue",
      "Dirichlet energy",
      "Dirichlet energy constrained learning",
      "Energetic Graph Neural Networks (EGNN)",
      "deep GNNs",
      "orthogonal weight controlling",
      "lower-bounded residual connection",
      "Shifted ReLU (SReLU)",
      "theoretical analysis",
      "node classification",
      "state-of-the-art performance",
      "over-separating"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/faa6fce9a16925eb3091271281f923bc95291ebb.pdf",
    "citation_key": "zhou20213lg",
    "metadata": {
      "title": "Dirichlet Energy Constrained Learning for Deep Graph Neural Networks",
      "authors": [
        "Kaixiong Zhou",
        "Xiao Huang",
        "D. Zha",
        "Rui Chen",
        "Li Li",
        "Soo-Hyun Choi",
        "Xia Hu"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) integrate deep architectures and topological structure modeling in an effective way. However, the performance of existing GNNs would decrease significantly when they stack many layers, because of the over-smoothing issue. Node embeddings tend to converge to similar vectors when GNNs keep recursively aggregating the representations of neighbors. To enable deep GNNs, several methods have been explored recently. But they are developed from either techniques in convolutional neural networks or heuristic strategies. There is no generalizable and theoretical principle to guide the design of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by leveraging the Dirichlet energy of node embeddings, and propose a generalizable principle to guide the training of deep GNNs. Based on it, a novel deep GNN framework -- EGNN is designed. It could provide lower and upper constraints in terms of Dirichlet energy at each layer to avoid over-smoothing. Experimental results demonstrate that EGNN achieves state-of-the-art performance by using deep layers.",
      "file_path": "paper_data/Graph_Neural_Networks/faa6fce9a16925eb3091271281f923bc95291ebb.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{zhou20213lg} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Graph Neural Networks (GNNs) suffer from the \"over-smoothing\" issue, where node embeddings converge to indistinguishable vectors when stacking many layers \\cite{zhou20213lg}. This significantly degrades performance.\n    *   **Importance and Challenge**: Over-smoothing prevents GNNs from leveraging deep architectures to model dependencies to high-order neighbors, which are crucial for many tasks. The challenge lies in the lack of a generalizable and theoretical principle to guide the design and training of deep GNNs, as existing solutions are often heuristic or borrowed from CNNs without graph-specific theoretical grounding \\cite{zhou20213lg}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous methods to alleviate over-smoothing include embedding normalization, residual connections, and random data augmentation \\cite{zhou20213lg}.\n    *   **Limitations of Previous Solutions**: Many are motivated by techniques from Convolutional Neural Networks (CNNs) or rely on heuristic strategies. Most only achieve comparable or even worse performance than their shallow counterparts. There is a lack of a generalizable principle and theoretical analysis to guide their design and selection \\cite{zhou20213lg}. While Dirichlet energy has been used to quantify over-smoothing, no methods leveraged it to overcome the issue.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a \"Dirichlet energy constrained learning\" principle to guide the training of deep GNNs by regularizing the Dirichlet energy of node embeddings at each layer \\cite{zhou20213lg}. This principle defines an appropriate lower and upper range for Dirichlet energy to prevent both over-smoothing (energy too small) and over-separating (energy too large).\n    *   **Novelty**: The approach is novel because it provides a theoretical and generalizable principle based on Dirichlet energy, which is then directly translated into a practical deep GNN framework called Energetic Graph Neural Networks (EGNN) \\cite{zhou20213lg}. EGNN incorporates three specific components designed to satisfy these energy constraints: orthogonal weight controlling, lower-bounded residual connection, and Shifted ReLU (SReLU) activation.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Dirichlet Energy Constrained Learning Principle**: A theoretical principle that defines appropriate lower and upper limits for Dirichlet energy at each layer to guide deep GNN training, overcoming over-smoothing and over-separating \\cite{zhou20213lg}.\n        *   **Orthogonal Weight Controlling**: Initializes trainable weights as diagonal matrices with explicit singular values and regularizes them to meet the upper energy limit during training \\cite{zhou20213lg}.\n        *   **Lower-bounded Residual Connection**: Introduces residual connections with strengths (`alpha` and `beta`) determined by the lower energy limit, ensuring `E(X(k)) >= c_min * E(X(k-1))` and preventing over-smoothing \\cite{zhou20213lg}.\n        *   **SReLU Activation**: Employs a Shifted ReLU (SReLU) with a trainable shift `b` to provide a trade-off between non-linear and linear mappings, addressing the issue of ReLU further reducing Dirichlet energy \\cite{zhou20213lg}.\n    *   **Theoretical Insights**: Provides theoretical analysis (Lemmas 1-6) on how Dirichlet energy is bounded and affected by GNN components (weights, activations), forming the basis for the proposed principle and EGNN design \\cite{zhou20213lg}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted on benchmark node classification datasets (e.g., Cora, Citeseer, PubMed, Coauthor CS, Amazon Computers/Photo) to evaluate EGNN's performance with increasing layers \\cite{zhou20213lg}.\n    *   **Key Performance Metrics and Comparison Results**: EGNN achieves state-of-the-art performance by effectively utilizing deep layers (up to 64 layers). The paper demonstrates that EGNN can be easily trained to reach 64 layers and achieves surprisingly competitive performance on benchmarks, outperforming existing deep GNN methods \\cite{zhou20213lg}. The optimal energy value is shown to locate within the proposed range.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical derivations for Dirichlet energy bounds initially simplify GCN by removing non-linear activation, which is later addressed in the model design \\cite{zhou20213lg}. The conditions for Lemma 4 and 5 (e.g., `c_max >= c_min / (2*c_min - 1)^2`) impose specific relationships between hyperparameters.\n    *   **Scope of Applicability**: The framework is primarily demonstrated for node classification tasks. While the principle is generalizable, its direct application and hyperparameter tuning might vary for other graph-based tasks.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing a theoretically grounded and generalizable principle (Dirichlet energy constrained learning) for designing and training deep GNNs \\cite{zhou20213lg}. It moves beyond heuristic solutions by offering a unified framework that can explain and improve existing techniques.\n    *   **Potential Impact on Future Research**: The proposed principle and EGNN framework offer a robust foundation for future research into deep GNN architectures. It opens avenues for exploring other energy-based regularization techniques, novel activation functions, and weight initialization strategies tailored for deep graph learning, potentially enabling GNNs to effectively model even higher-order dependencies in complex networks \\cite{zhou20213lg}.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "over-smoothing issue",
        "Dirichlet energy",
        "Dirichlet energy constrained learning",
        "Energetic Graph Neural Networks (EGNN)",
        "deep GNNs",
        "orthogonal weight controlling",
        "lower-bounded residual connection",
        "Shifted ReLU (SReLU)",
        "theoretical analysis",
        "node classification",
        "state-of-the-art performance",
        "over-separating"
      ],
      "paper_type": "based on the abstract and introduction:\n\nthe paper identifies a key limitation of existing graph neural networks (gnns) (over-smoothing issue), critiques current heuristic solutions, and then explicitly states: \"we analyze the bottleneck of deep gnns by leveraging the dirichlet energy of node embeddings, and **propose a generalizable principle** to guide the training of deep gnns. based on it, **a novel deep gnn framework – egnn is designed**.\" it also mentions \"experimental results demonstrate that egnn achieves state-of-the-art performance.\"\n\nthis aligns perfectly with the criteria for a **technical** paper:\n*   abstract mentions: \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" (specifically \"propose a generalizable principle\" and \"a novel deep gnn framework – egnn is designed\").\n*   introduction discusses: \"technical problem\" (over-smoothing), \"proposed solution\" (the new principle and framework).\n\nwhile it has theoretical elements (\"leveraging the dirichlet energy\", \"theoretical principle\"), the ultimate goal and output described are the design of a \"novel framework,\" which is a new system/method. the experimental results serve to validate this new method.\n\n**classification: technical**"
    },
    "file_name": "faa6fce9a16925eb3091271281f923bc95291ebb.pdf"
  },
  {
    "success": true,
    "doc_id": "eee0af6ec949d41f40f850539ae78021",
    "summary": "This paper \\cite{jegelka20222lq} provides a theoretical survey of Graph Neural Networks (GNNs), focusing on their representational power, generalization capabilities, and extrapolation under distribution shifts. It primarily analyzes Message Passing GNNs (MPNNs) and higher-order GNNs, summarizing key mathematical connections and theoretical results from the literature.\n\nHere's a focused summary for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the theoretical understanding of Graph Neural Networks (GNNs), particularly their ability to learn rich representations of graph-structured data for various prediction tasks (node, graph, or point configurations). It investigates fundamental questions regarding what GNNs can represent, how well they generalize, and their capacity to extrapolate to unseen data distributions.\n    *   **Importance and Challenge**: GNNs have achieved widespread empirical success in diverse applications (social networks, recommender systems, molecular property prediction, combinatorial optimization). However, a deep theoretical understanding of *why* they work, their inherent limitations, and how to design more powerful architectures is crucial. This understanding is challenging due to the complex, non-Euclidean nature of graph data and the permutation invariance/equivariance requirements.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions GNNs as a powerful, adaptable alternative to traditional graph embeddings (e.g., spectral embeddings, graph kernels) because they can adapt embeddings to specific tasks, generalize across graphs, and incorporate attributes. It specifically focuses on Message Passing GNNs (MPNNs) as a popular spatial GNN architecture.\n    *   **Limitations of Previous Solutions (and standard MPNNs)**:\n        *   Traditional graph embeddings often lack adaptability to specific tasks or the ability to incorporate rich node/edge attributes.\n        *   Standard MPNNs, while powerful, are limited in their discriminative power, often being no more powerful than the 1-Weisfeiler-Leman (1-WL) graph isomorphism test. This means they cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or decide complex structural properties like girth, diameter, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   These limitations motivate the exploration of higher-order GNNs and augmentations like node IDs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to survey and synthesize theoretical results, primarily by connecting GNN capabilities to established computational models and mathematical theories.\n        *   **MPNN Framework**: It formally defines MPNNs as an iterative message-passing scheme (Equations 1.3-1.9 \\cite{jegelka20222lq}), emphasizing the aggregation and update functions, and the requirement for permutation invariance/equivariance.\n        *   **WL Test Analogy**: A central approach is to relate the discriminative power of MPNNs to the 1-Weisfeiler-Leman (1-WL) algorithm for graph isomorphism testing (Theorem 1 \\cite{jegelka20222lq}).\n        *   **Multiset Function Approximation**: It leverages universal approximation theorems for multiset functions (Theorem 2 \\cite{jegelka20222lq}) to explain how GNNs can represent permutation-invariant aggregations.\n        *   **Local Algorithms Analogy**: It draws parallels between GNNs (especially with node IDs) and local algorithms in distributed computing (e.g., CONGEST, LOCAL models) to derive computational power and lower bounds.\n    *   **Novelty/Difference (as discussed in the paper)**:\n        *   **Graph Isomorphism Network (GIN)** \\cite{jegelka20222lq}: Highlighted as an MPNN that explicitly implements the sum decomposition for injective multiset functions, achieving the maximum discriminative power of 1-WL.\n        *   **Node ID Augmentations**: Discusses augmenting MPNNs with unique or random node identifiers to overcome 1-WL limitations, leading to increased representational power and even Turing completeness (Theorem 4 \\cite{jegelka20222lq}).\n        *   **Higher-Order GNNs**: Explores architectures that operate on k-tuples of nodes (instead of single nodes), providing a unified tensor-based framework to increase expressive power beyond 1-WL.\n        *   **CPNGNN**: A specific MPNN model that incorporates port numbering from local algorithms to improve approximation capabilities for certain combinatorial problems (Theorem 3 \\cite{jegelka20222lq}).\n\n4.  **Key Technical Contributions (as summarized/presented by the paper)**\n    *   **Theoretical Insights & Analysis**:\n        *   Formal connection between MPNN discriminative power and the 1-Weisfeiler-Leman algorithm (Theorem 1 \\cite{jegelka20222lq}).\n        *   Universal approximation theorems for multiset functions, explaining the design principles for powerful aggregation functions (Theorem 2 \\cite{jegelka20222lq}).\n        *   Identification of specific graph properties that MPNNs *cannot* decide (Proposition 2 \\cite{jegelka20222lq}).\n        *   Proof of Turing completeness for MPNNs augmented with unique node IDs, establishing equivalence to the LOCAL model of distributed computing (Theorem 4, Corollary 1 \\cite{jegelka20222lq}).\n        *   Derivation of lower bounds on GNN depth and width for solving various combinatorial problems, based on analogies to the CONGEST model (Theorem 5 \\cite{jegelka20222lq}).\n        *   Probabilistic universal approximation results for MPNNs with random node IDs (Theorem 6 \\cite{jegelka20222lq}).\n    *   **Novel Algorithms/Methods (discussed as advancements in the field)**:\n        *   **Graph Isomorphism Network (GIN)**: An MPNN designed for maximal 1-WL discriminative power.\n        *   **CPNGNN**: A GNN model incorporating port numbering for improved approximation algorithms.\n        *   **Higher-order GNNs**: Architectures that process k-tuples of nodes to enhance expressive power.\n\n5.  **Experimental Validation**\n    *   This paper is a *theoretical survey* and *does not present new experimental validation*. Instead, it synthesizes and summarizes theoretical results and mathematical connections from existing literature. The \"key performance metrics and comparison results\" discussed are theoretical bounds and equivalences, rather than empirical benchmarks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations of MPNNs**:\n        *   Cannot distinguish regular graphs or graphs with identical computation trees (Figure 1 \\cite{jegelka20222lq}).\n        *   Inability to decide properties like girth, diameter, circumference, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   Limited approximation capabilities for certain combinatorial problems (e.g., maximum matching for CPNGNNs, Theorem 3 \\cite{jegelka20222lq}).\n        *   Computational power loss for bounded size GNNs without unique node IDs (Theorem 5 \\cite{jegelka20222lq}).\n    *   **Assumptions**: Universal approximation results for multiset functions often assume continuous functions and sufficient latent dimension (at least maximum degree). Turing completeness requires Turing complete aggregation/update functions and unbounded width.\n    *   **Scope of Applicability**: The survey primarily focuses on Message Passing GNNs and higher-order GNNs, with a brief mention of spectral GNNs but not in detail. It covers representation, generalization, and extrapolation, acknowledging that GNNs are a rapidly evolving field and not all works can be included.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing a structured and comprehensive theoretical framework for understanding GNNs. It clarifies the fundamental limits of standard MPNNs and highlights architectural innovations (node IDs, higher-order processing) that overcome these limitations.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The theoretical insights (e.g., WL equivalence, multiset approximation) provide guiding principles for designing more expressive and powerful GNN architectures.\n        *   **Understanding Generalization**: The discussion on generalization and extrapolation under distribution shifts is critical for developing robust and reliable GNNs for real-world applications.\n        *   **Combinatorial Optimization**: The connections to local algorithms and lower bounds offer a roadmap for applying GNNs to hard combinatorial problems and understanding their inherent computational limits.\n        *   **Foundation for Further Theory**: By synthesizing existing theoretical results, the paper establishes a strong foundation for future theoretical investigations into GNNs, including their connections to logic, distributed computing, and graph theory.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have revolutionized machine learning on graph-structured data, yet their fundamental theoretical underpinnings often remain opaque. This survey demystifies the core capabilities and limitations of GNNs, particularly Message Passing GNNs (MPNNs), by synthesizing crucial theoretical advancements. We rigorously analyze their representational power, establishing formal connections to the 1-Weisfeiler-Leman (WL) test and universal approximation theorems for multiset functions. Crucially, we unveil the inherent discriminative limitations of standard MPNNs, which struggle with properties like girth or induced subgraphs. To overcome these, we explore the profound impact of higher-order GNNs and node identifier augmentations, demonstrating how these architectural innovations can achieve Turing completeness and unlock significantly enhanced expressive power, bridging GNNs with distributed algorithms. By providing a comprehensive theoretical framework for representational capacity, generalization, and extrapolation under distribution shifts, this work offers critical insights for designing more powerful, robust, and interpretable GNN architectures. It lays a foundational roadmap for future research in graph learning and combinatorial optimization.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Message Passing GNNs (MPNNs)",
      "representational power",
      "generalization capabilities",
      "extrapolation",
      "distribution shifts",
      "Weisfeiler-Leman (WL) test",
      "graph isomorphism",
      "higher-order GNNs",
      "node ID augmentations",
      "Turing completeness",
      "local algorithms",
      "combinatorial optimization",
      "theoretical survey"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/2a85846fd827a157b624ee012e75cbe37344281c.pdf",
    "citation_key": "jegelka20222lq",
    "metadata": {
      "title": "Theory of Graph Neural Networks: Representation and Learning",
      "authors": [
        "S. Jegelka"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections.",
      "file_path": "paper_data/Graph_Neural_Networks/2a85846fd827a157b624ee012e75cbe37344281c.pdf",
      "venue": "arXiv.org",
      "citationCount": 0,
      "score": 0,
      "summary": "This paper \\cite{jegelka20222lq} provides a theoretical survey of Graph Neural Networks (GNNs), focusing on their representational power, generalization capabilities, and extrapolation under distribution shifts. It primarily analyzes Message Passing GNNs (MPNNs) and higher-order GNNs, summarizing key mathematical connections and theoretical results from the literature.\n\nHere's a focused summary for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the theoretical understanding of Graph Neural Networks (GNNs), particularly their ability to learn rich representations of graph-structured data for various prediction tasks (node, graph, or point configurations). It investigates fundamental questions regarding what GNNs can represent, how well they generalize, and their capacity to extrapolate to unseen data distributions.\n    *   **Importance and Challenge**: GNNs have achieved widespread empirical success in diverse applications (social networks, recommender systems, molecular property prediction, combinatorial optimization). However, a deep theoretical understanding of *why* they work, their inherent limitations, and how to design more powerful architectures is crucial. This understanding is challenging due to the complex, non-Euclidean nature of graph data and the permutation invariance/equivariance requirements.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions GNNs as a powerful, adaptable alternative to traditional graph embeddings (e.g., spectral embeddings, graph kernels) because they can adapt embeddings to specific tasks, generalize across graphs, and incorporate attributes. It specifically focuses on Message Passing GNNs (MPNNs) as a popular spatial GNN architecture.\n    *   **Limitations of Previous Solutions (and standard MPNNs)**:\n        *   Traditional graph embeddings often lack adaptability to specific tasks or the ability to incorporate rich node/edge attributes.\n        *   Standard MPNNs, while powerful, are limited in their discriminative power, often being no more powerful than the 1-Weisfeiler-Leman (1-WL) graph isomorphism test. This means they cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or decide complex structural properties like girth, diameter, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   These limitations motivate the exploration of higher-order GNNs and augmentations like node IDs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to survey and synthesize theoretical results, primarily by connecting GNN capabilities to established computational models and mathematical theories.\n        *   **MPNN Framework**: It formally defines MPNNs as an iterative message-passing scheme (Equations 1.3-1.9 \\cite{jegelka20222lq}), emphasizing the aggregation and update functions, and the requirement for permutation invariance/equivariance.\n        *   **WL Test Analogy**: A central approach is to relate the discriminative power of MPNNs to the 1-Weisfeiler-Leman (1-WL) algorithm for graph isomorphism testing (Theorem 1 \\cite{jegelka20222lq}).\n        *   **Multiset Function Approximation**: It leverages universal approximation theorems for multiset functions (Theorem 2 \\cite{jegelka20222lq}) to explain how GNNs can represent permutation-invariant aggregations.\n        *   **Local Algorithms Analogy**: It draws parallels between GNNs (especially with node IDs) and local algorithms in distributed computing (e.g., CONGEST, LOCAL models) to derive computational power and lower bounds.\n    *   **Novelty/Difference (as discussed in the paper)**:\n        *   **Graph Isomorphism Network (GIN)** \\cite{jegelka20222lq}: Highlighted as an MPNN that explicitly implements the sum decomposition for injective multiset functions, achieving the maximum discriminative power of 1-WL.\n        *   **Node ID Augmentations**: Discusses augmenting MPNNs with unique or random node identifiers to overcome 1-WL limitations, leading to increased representational power and even Turing completeness (Theorem 4 \\cite{jegelka20222lq}).\n        *   **Higher-Order GNNs**: Explores architectures that operate on k-tuples of nodes (instead of single nodes), providing a unified tensor-based framework to increase expressive power beyond 1-WL.\n        *   **CPNGNN**: A specific MPNN model that incorporates port numbering from local algorithms to improve approximation capabilities for certain combinatorial problems (Theorem 3 \\cite{jegelka20222lq}).\n\n4.  **Key Technical Contributions (as summarized/presented by the paper)**\n    *   **Theoretical Insights & Analysis**:\n        *   Formal connection between MPNN discriminative power and the 1-Weisfeiler-Leman algorithm (Theorem 1 \\cite{jegelka20222lq}).\n        *   Universal approximation theorems for multiset functions, explaining the design principles for powerful aggregation functions (Theorem 2 \\cite{jegelka20222lq}).\n        *   Identification of specific graph properties that MPNNs *cannot* decide (Proposition 2 \\cite{jegelka20222lq}).\n        *   Proof of Turing completeness for MPNNs augmented with unique node IDs, establishing equivalence to the LOCAL model of distributed computing (Theorem 4, Corollary 1 \\cite{jegelka20222lq}).\n        *   Derivation of lower bounds on GNN depth and width for solving various combinatorial problems, based on analogies to the CONGEST model (Theorem 5 \\cite{jegelka20222lq}).\n        *   Probabilistic universal approximation results for MPNNs with random node IDs (Theorem 6 \\cite{jegelka20222lq}).\n    *   **Novel Algorithms/Methods (discussed as advancements in the field)**:\n        *   **Graph Isomorphism Network (GIN)**: An MPNN designed for maximal 1-WL discriminative power.\n        *   **CPNGNN**: A GNN model incorporating port numbering for improved approximation algorithms.\n        *   **Higher-order GNNs**: Architectures that process k-tuples of nodes to enhance expressive power.\n\n5.  **Experimental Validation**\n    *   This paper is a *theoretical survey* and *does not present new experimental validation*. Instead, it synthesizes and summarizes theoretical results and mathematical connections from existing literature. The \"key performance metrics and comparison results\" discussed are theoretical bounds and equivalences, rather than empirical benchmarks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations of MPNNs**:\n        *   Cannot distinguish regular graphs or graphs with identical computation trees (Figure 1 \\cite{jegelka20222lq}).\n        *   Inability to decide properties like girth, diameter, circumference, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   Limited approximation capabilities for certain combinatorial problems (e.g., maximum matching for CPNGNNs, Theorem 3 \\cite{jegelka20222lq}).\n        *   Computational power loss for bounded size GNNs without unique node IDs (Theorem 5 \\cite{jegelka20222lq}).\n    *   **Assumptions**: Universal approximation results for multiset functions often assume continuous functions and sufficient latent dimension (at least maximum degree). Turing completeness requires Turing complete aggregation/update functions and unbounded width.\n    *   **Scope of Applicability**: The survey primarily focuses on Message Passing GNNs and higher-order GNNs, with a brief mention of spectral GNNs but not in detail. It covers representation, generalization, and extrapolation, acknowledging that GNNs are a rapidly evolving field and not all works can be included.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing a structured and comprehensive theoretical framework for understanding GNNs. It clarifies the fundamental limits of standard MPNNs and highlights architectural innovations (node IDs, higher-order processing) that overcome these limitations.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The theoretical insights (e.g., WL equivalence, multiset approximation) provide guiding principles for designing more expressive and powerful GNN architectures.\n        *   **Understanding Generalization**: The discussion on generalization and extrapolation under distribution shifts is critical for developing robust and reliable GNNs for real-world applications.\n        *   **Combinatorial Optimization**: The connections to local algorithms and lower bounds offer a roadmap for applying GNNs to hard combinatorial problems and understanding their inherent computational limits.\n        *   **Foundation for Further Theory**: By synthesizing existing theoretical results, the paper establishes a strong foundation for future theoretical investigations into GNNs, including their connections to logic, distributed computing, and graph theory.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Message Passing GNNs (MPNNs)",
        "representational power",
        "generalization capabilities",
        "extrapolation",
        "distribution shifts",
        "Weisfeiler-Leman (WL) test",
        "graph isomorphism",
        "higher-order GNNs",
        "node ID augmentations",
        "Turing completeness",
        "local algorithms",
        "combinatorial optimization",
        "theoretical survey"
      ],
      "paper_type": "based on the abstract and introduction:\n\n1.  **abstract analysis:**\n    *   \"this article **summarizes a selection of the emerging theoretical results** on approximation and learning properties of widely used message passing gnns and higher-order gnns, focusing on representation, generalization and extrapolation.\"\n    *   \"along the way, it **summarizes mathematical connections**.\"\n    *   the keywords \"summarizes a selection\" and \"summarizes mathematical connections\" strongly point towards a review or survey.\n\n2.  **introduction analysis:**\n    *   \"...due to space limits, **this survey focuses on** the popular message passing (spatial) g...\"\n    *   the explicit use of the word \"survey\" by the authors themselves is a definitive indicator.\n\ncomparing this to the classification criteria:\n\n*   **survey**: \"reviews existing literature comprehensively\" - abstract mentions: \"summarizes\", \"selection of emerging theoretical results\". introduction discusses: \"this survey focuses on...\". this is a direct match.\n\nwhile the *subject matter* is theoretical (\"theory of graph neural networks\"), the *paper type* is a review or summary of that theory, not the presentation of new theoretical work by the authors of this paper.\n\ntherefore, the paper is a **survey**."
    },
    "file_name": "2a85846fd827a157b624ee012e75cbe37344281c.pdf"
  },
  {
    "success": true,
    "doc_id": "c0ddb78a6cff90decb9aebbf77224929",
    "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Training Graph Neural Networks (GNNs) on large-scale graphs (millions of nodes/edges) presents significant computational challenges regarding storage and training time. This problem is exacerbated by the need for frequent retraining (e.g., incremental learning, hyperparameter search, neural architecture search).\n    *   **Importance & Challenge**: Real-world data is increasingly graph-structured and large. Existing graph simplification methods like sparsification (reduces edges) and coarsening (reduces nodes) have limitations: sparsification doesn't reduce node attributes, and both primarily aim to preserve general graph properties (e.g., eigenvalues, distances) which may not be optimal for downstream GNN performance. The core challenge is to *drastically reduce graph size* while *preserving the GNN's predictive performance* on the downstream task.\n\n*   **Related Work & Positioning**\n    *   **Dataset Distillation (DD) & Condensation (DC)**: `\\cite{jin2021pf0}` is motivated by these methods, which generate small synthetic datasets for deep neural networks. However, DD/DC are designed for Euclidean image data and are not directly applicable to non-Euclidean, interdependent graph-structured data. `\\cite{jin2021pf0}` extends this concept to graphs, jointly learning synthetic node features and graph structure.\n    *   **Coreset Methods**: These select representative samples from the original dataset. `\\cite{jin2021pf0}` differs by *learning synthetic* nodes and connections, rather than merely selecting existing ones, which can lead to suboptimal performance if truly representative samples are scarce.\n    *   **Graph Sparsification & Coarsening**: These reduce graph size by removing edges or grouping nodes. `\\cite{jin2021pf0}` distinguishes itself by learning *synthetic* nodes and connections in a *supervised* manner, specifically optimizing for GNN performance, unlike the unsupervised grouping or property-preserving objectives of traditional coarsening/sparsification.\n    *   **Graph Structure Learning**: While related to learning graph structures, existing methods are not designed to learn graphs of *smaller size*, making them unsuitable for graph condensation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: `\\cite{jin2021pf0}` proposes **Graph Condensation (GCOND)**, which aims to learn a small, synthetic, and highly-informative graph `S = {A', X', Y'}` from a large original graph `T = {A, X, Y}`. The objective is for GNNs trained on `S` to achieve comparable performance to those trained on `T`.\n    *   **Optimization Strategy (Gradient Matching)**: To tackle the challenging bi-level optimization problem, `\\cite{jin2021pf0}` adapts a gradient matching scheme. It minimizes the distance between the gradients of GNN parameters computed on the condensed graph `S` and the original graph `T` at each training step. This ensures that the GNN's training trajectory on the small synthetic graph mimics that on the large real graph, leading to similar converged model parameters.\n    *   **Novel Graph Structure Parameterization**: A key innovation is modeling the condensed graph structure `A'` as a *function of the condensed node features `X'`* (i.e., `A' = g(X')`). Specifically, `A'_ij` is determined by `Sigmoid(MLP([x'_i; x'_j]) + MLP([x'_j; x'_i])) / 2`.\n        *   This approach significantly reduces the number of parameters needed to model the graph structure (avoiding `O(N'^2)` parameters) and inherently leverages the correlation between node features and connections.\n    *   **Alternating Optimization**: `\\cite{jin2021pf0}` employs an alternating optimization schema for `X'` (synthetic node features) and `g` (the MLP parameters for structure generation).\n    *   **Graph Sampling & Class-wise Gradient Matching**: To handle large graphs and ease optimization, `\\cite{jin2021pf0}` uses mini-batch training with neighbor sampling for GNNs and calculates the gradient matching loss separately for nodes from different classes.\n    *   **\"Graphless\" Model Variant (GCOND-X)**: An ablation study introduces `GCOND-X`, which learns only `X'` while fixing `A'` as an identity matrix. This variant still achieves competitive performance, highlighting the power of the gradient matching loss in embedding structural information into features.\n\n*   **Key Technical Contributions**\n    *   **Problem Formulation**: First to propose and formally define the problem of graph condensation for GNNs.\n    *   **GCOND Framework**: Introduced a novel framework that condenses large graphs into small synthetic ones for GNN training.\n    *   **Structure-Feature Interdependence**: Pioneered parameterizing the condensed graph structure as a function of condensed node features, significantly reducing parameter complexity and leveraging implicit graph properties.\n    *   **Gradient Matching Adaptation**: Successfully adapted gradient matching for the unique challenges of graph-structured data and GNNs.\n    *   **Empirical Validation**: Demonstrated the practical effectiveness and generalization capabilities of the condensed graphs across various GNN architectures and datasets.\n\n*   **Experimental Validation**\n    *   **Experiments**: `\\cite{jin2021pf0}` conducted extensive experiments to validate GCOND's effectiveness in condensing various graph datasets, its generalization to different GNN architectures, and its utility in Neural Architecture Search (NAS).\n    *   **Datasets**: Evaluated on both transductive (Cora, Citeseer, Ogbn-arxiv) and inductive (Flickr, Reddit) datasets.\n    *   **Key Metrics**: Primary metrics include the test accuracy of GNNs trained on the condensed graph compared to the original, and the graph size reduction percentage.\n    *   **Results**:\n        *   GCOND achieved remarkable graph size reduction (e.g., >99.9%) while maintaining high GNN performance. For instance, it approximated original test accuracy by 95.3% on Reddit, 99.8% on Flickr, and 99.0% on Citeseer.\n        *   It consistently outperformed baselines including graph coarsening, coreset methods, and direct dataset condensation adaptations.\n        *   The condensed graphs demonstrated strong generalization capabilities, allowing various GNN architectures (GCN, SGC, APPNP, GraphSAGE) to be trained effectively.\n        *   Reliable correlation was observed between GNN performance on condensed datasets and whole datasets in NAS experiments, suggesting its utility for efficient architecture search.\n\n*   **Limitations & Scope**\n    *   **Fixed Labels**: To simplify the problem, the synthetic node labels `Y'` are fixed to maintain the original class distribution, rather than being learned.\n    *   **Node Classification Focus**: The primary focus and evaluation are on node classification tasks for attributed graphs.\n    *   **Empirical Choices**: The specific `MLP` architecture for `g(X')` and the alternating optimization schedule are empirically determined.\n    *   **Sparsification Threshold**: The choice of threshold for sparsifying the learned adjacency matrix `A'` is a hyperparameter.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{jin2021pf0}` makes the first successful attempt at graph condensation for GNNs, providing a novel and effective solution to the critical computational challenges posed by large-scale graphs.\n    *   **Enabling Technology**: It enables significantly faster GNN training and retraining, which is crucial for applications requiring frequent model updates, hyperparameter tuning, or neural architecture search.\n    *   **Broader Impact**: The proposed framework facilitates more efficient storage, visualization, and retrieval of graph data. The innovative approach of learning graph structure as a function of features could inspire future research in graph representation learning and graph generation.",
    "intriguing_abstract": "Training Graph Neural Networks (GNNs) on massive, real-world graphs presents a formidable computational bottleneck, hindering rapid model development, hyperparameter tuning, and Neural Architecture Search (NAS). We introduce **Graph Condensation (GCOND)**, a pioneering framework that drastically reduces the size of large-scale graphs into a small, synthetic equivalent while meticulously preserving GNN predictive performance.\n\nUnlike traditional graph simplification methods, GCOND learns a highly informative synthetic graph whose GNN training trajectory closely mimics that of the original. Our core innovation lies in adapting a gradient matching scheme for graph-structured data and, crucially, parameterizing the synthetic graph structure as a function of its learned node features. This novel approach significantly reduces parameter complexity and intrinsically captures the intricate relationship between features and connections. GCOND achieves remarkable size reductions, often exceeding 99.9%, while maintaining over 95% of the original GNN accuracy across diverse datasets and architectures. This work offers an unprecedented acceleration for GNN training and development, paving the way for efficient, scalable GNN applications in an era of ever-growing graph data.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "Large-scale graphs",
      "Graph Condensation (GCOND)",
      "Gradient Matching",
      "Synthetic graph structure learning",
      "Structure-feature interdependence",
      "Computational efficiency",
      "Predictive performance preservation",
      "Graph size reduction",
      "Neural Architecture Search (NAS)",
      "Bi-level optimization",
      "Dataset Distillation adaptation",
      "Node classification tasks"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf",
    "citation_key": "jin2021pf0",
    "metadata": {
      "title": "Graph Condensation for Graph Neural Networks",
      "authors": [
        "Wei Jin",
        "Lingxiao Zhao",
        "Shichang Zhang",
        "Yozen Liu",
        "Jiliang Tang",
        "Neil Shah"
      ],
      "published_date": "2021",
      "abstract": "Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures.Code is released at https://github.com/ChandlerBang/GCond.",
      "file_path": "paper_data/Graph_Neural_Networks/5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Training Graph Neural Networks (GNNs) on large-scale graphs (millions of nodes/edges) presents significant computational challenges regarding storage and training time. This problem is exacerbated by the need for frequent retraining (e.g., incremental learning, hyperparameter search, neural architecture search).\n    *   **Importance & Challenge**: Real-world data is increasingly graph-structured and large. Existing graph simplification methods like sparsification (reduces edges) and coarsening (reduces nodes) have limitations: sparsification doesn't reduce node attributes, and both primarily aim to preserve general graph properties (e.g., eigenvalues, distances) which may not be optimal for downstream GNN performance. The core challenge is to *drastically reduce graph size* while *preserving the GNN's predictive performance* on the downstream task.\n\n*   **Related Work & Positioning**\n    *   **Dataset Distillation (DD) & Condensation (DC)**: `\\cite{jin2021pf0}` is motivated by these methods, which generate small synthetic datasets for deep neural networks. However, DD/DC are designed for Euclidean image data and are not directly applicable to non-Euclidean, interdependent graph-structured data. `\\cite{jin2021pf0}` extends this concept to graphs, jointly learning synthetic node features and graph structure.\n    *   **Coreset Methods**: These select representative samples from the original dataset. `\\cite{jin2021pf0}` differs by *learning synthetic* nodes and connections, rather than merely selecting existing ones, which can lead to suboptimal performance if truly representative samples are scarce.\n    *   **Graph Sparsification & Coarsening**: These reduce graph size by removing edges or grouping nodes. `\\cite{jin2021pf0}` distinguishes itself by learning *synthetic* nodes and connections in a *supervised* manner, specifically optimizing for GNN performance, unlike the unsupervised grouping or property-preserving objectives of traditional coarsening/sparsification.\n    *   **Graph Structure Learning**: While related to learning graph structures, existing methods are not designed to learn graphs of *smaller size*, making them unsuitable for graph condensation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: `\\cite{jin2021pf0}` proposes **Graph Condensation (GCOND)**, which aims to learn a small, synthetic, and highly-informative graph `S = {A', X', Y'}` from a large original graph `T = {A, X, Y}`. The objective is for GNNs trained on `S` to achieve comparable performance to those trained on `T`.\n    *   **Optimization Strategy (Gradient Matching)**: To tackle the challenging bi-level optimization problem, `\\cite{jin2021pf0}` adapts a gradient matching scheme. It minimizes the distance between the gradients of GNN parameters computed on the condensed graph `S` and the original graph `T` at each training step. This ensures that the GNN's training trajectory on the small synthetic graph mimics that on the large real graph, leading to similar converged model parameters.\n    *   **Novel Graph Structure Parameterization**: A key innovation is modeling the condensed graph structure `A'` as a *function of the condensed node features `X'`* (i.e., `A' = g(X')`). Specifically, `A'_ij` is determined by `Sigmoid(MLP([x'_i; x'_j]) + MLP([x'_j; x'_i])) / 2`.\n        *   This approach significantly reduces the number of parameters needed to model the graph structure (avoiding `O(N'^2)` parameters) and inherently leverages the correlation between node features and connections.\n    *   **Alternating Optimization**: `\\cite{jin2021pf0}` employs an alternating optimization schema for `X'` (synthetic node features) and `g` (the MLP parameters for structure generation).\n    *   **Graph Sampling & Class-wise Gradient Matching**: To handle large graphs and ease optimization, `\\cite{jin2021pf0}` uses mini-batch training with neighbor sampling for GNNs and calculates the gradient matching loss separately for nodes from different classes.\n    *   **\"Graphless\" Model Variant (GCOND-X)**: An ablation study introduces `GCOND-X`, which learns only `X'` while fixing `A'` as an identity matrix. This variant still achieves competitive performance, highlighting the power of the gradient matching loss in embedding structural information into features.\n\n*   **Key Technical Contributions**\n    *   **Problem Formulation**: First to propose and formally define the problem of graph condensation for GNNs.\n    *   **GCOND Framework**: Introduced a novel framework that condenses large graphs into small synthetic ones for GNN training.\n    *   **Structure-Feature Interdependence**: Pioneered parameterizing the condensed graph structure as a function of condensed node features, significantly reducing parameter complexity and leveraging implicit graph properties.\n    *   **Gradient Matching Adaptation**: Successfully adapted gradient matching for the unique challenges of graph-structured data and GNNs.\n    *   **Empirical Validation**: Demonstrated the practical effectiveness and generalization capabilities of the condensed graphs across various GNN architectures and datasets.\n\n*   **Experimental Validation**\n    *   **Experiments**: `\\cite{jin2021pf0}` conducted extensive experiments to validate GCOND's effectiveness in condensing various graph datasets, its generalization to different GNN architectures, and its utility in Neural Architecture Search (NAS).\n    *   **Datasets**: Evaluated on both transductive (Cora, Citeseer, Ogbn-arxiv) and inductive (Flickr, Reddit) datasets.\n    *   **Key Metrics**: Primary metrics include the test accuracy of GNNs trained on the condensed graph compared to the original, and the graph size reduction percentage.\n    *   **Results**:\n        *   GCOND achieved remarkable graph size reduction (e.g., >99.9%) while maintaining high GNN performance. For instance, it approximated original test accuracy by 95.3% on Reddit, 99.8% on Flickr, and 99.0% on Citeseer.\n        *   It consistently outperformed baselines including graph coarsening, coreset methods, and direct dataset condensation adaptations.\n        *   The condensed graphs demonstrated strong generalization capabilities, allowing various GNN architectures (GCN, SGC, APPNP, GraphSAGE) to be trained effectively.\n        *   Reliable correlation was observed between GNN performance on condensed datasets and whole datasets in NAS experiments, suggesting its utility for efficient architecture search.\n\n*   **Limitations & Scope**\n    *   **Fixed Labels**: To simplify the problem, the synthetic node labels `Y'` are fixed to maintain the original class distribution, rather than being learned.\n    *   **Node Classification Focus**: The primary focus and evaluation are on node classification tasks for attributed graphs.\n    *   **Empirical Choices**: The specific `MLP` architecture for `g(X')` and the alternating optimization schedule are empirically determined.\n    *   **Sparsification Threshold**: The choice of threshold for sparsifying the learned adjacency matrix `A'` is a hyperparameter.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{jin2021pf0}` makes the first successful attempt at graph condensation for GNNs, providing a novel and effective solution to the critical computational challenges posed by large-scale graphs.\n    *   **Enabling Technology**: It enables significantly faster GNN training and retraining, which is crucial for applications requiring frequent model updates, hyperparameter tuning, or neural architecture search.\n    *   **Broader Impact**: The proposed framework facilitates more efficient storage, visualization, and retrieval of graph data. The innovative approach of learning graph structure as a function of features could inspire future research in graph representation learning and graph generation.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "Large-scale graphs",
        "Graph Condensation (GCOND)",
        "Gradient Matching",
        "Synthetic graph structure learning",
        "Structure-feature interdependence",
        "Computational efficiency",
        "Predictive performance preservation",
        "Graph size reduction",
        "Neural Architecture Search (NAS)",
        "Bi-level optimization",
        "Dataset Distillation adaptation",
        "Node classification tasks"
      ],
      "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **technical keywords:** the abstract explicitly states, \"we **propose** and study the problem of graph condensation,\" and \"we approach the condensation problem by imitating... and **design a strategy** to condense node features and structural information simultaneously.\" it also refers to \"the **proposed framework**.\" these phrases directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n2.  **problem and solution:** the introduction sets up a technical problem (computational challenges of large-scale graphs for gnns) and the abstract details the proposed technical solution (graph condensation framework).\n3.  **empirical validation:** while the paper clearly includes \"extensive experiments\" and reports \"findings\" (accuracy percentages, graph size reduction), these empirical results serve to *demonstrate the effectiveness of the proposed technical framework*. in machine learning, papers that introduce novel methods and then validate them through experiments are primarily classified as technical, with the empirical work being a crucial component of proving the technical contribution. if the paper were purely empirical, it might, for example, analyze existing methods or datasets without proposing a new one."
    },
    "file_name": "5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf"
  },
  {
    "success": true,
    "doc_id": "18d27e66abf561d72a676d8b4cd18063",
    "summary": "Here's a focused summary of the technical paper \\cite{geisler2021dcq} for a literature review:\n\n### Focused Summary for Literature Review: Robustness of Graph Neural Networks at Scale \\cite{geisler2021dcq}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing studies on the adversarial robustness of Graph Neural Networks (GNNs) are limited to relatively small graphs, making it challenging to understand and mitigate vulnerabilities in real-world, large-scale GNN deployments.\n    *   **Importance and Challenge**: GNNs are increasingly popular in Internet-scale applications, but their vulnerability to adversarial attacks is well-established. Prior attack methods suffer from prohibitive memory requirements (e.g., quadratic in the number of nodes, `O(n^2)`) and computational costs, making it practically infeasible to study robustness on graphs with millions of nodes. Additionally, common surrogate losses used in attacks are ineffective for global attacks on large graphs, and existing robust GNNs are not scalable.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{geisler2021dcq} builds upon gradient-based adversarial attacks (e.g., PGD, Carlini-Wagner) and robust GNN designs, but critically extends them to operate at significantly larger scales.\n    *   **Limitations of Previous Solutions**:\n        *   **Scalability**: Prior gradient-based attacks optimize over a dense adjacency matrix, leading to `O(n^2)` space complexity, which is impractical for large graphs (e.g., PubMed requiring 20GB for a dense attack). Local attacks like SGA \\cite{li2020sga} are limited to ~200k nodes and specific GNN architectures.\n        *   **Surrogate Losses**: Common surrogate losses like Cross Entropy (CE) or Carlini-Wagner (CW) are shown to be poorly suited for global attacks on GNNs, especially with small budgets on large graphs. They tend to waste budget on already misclassified nodes or confident nodes, rather than focusing on \"fragile\" nodes near the decision boundary.\n        *   **Robust GNNs**: Previous robust GNNs typically lack scalability, making them unsuitable for large-scale deployment.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**:\n        *   **Sparsity-Aware First-Order Optimization Attacks**: Introduces two methods, **Projected Randomized Block Coordinate Descent (PR-BCD)** and **Greedy R-BCD (GR-BCD)**, which leverage Randomized Block Coordinate Descent (R-BCD) to optimize perturbations. These attacks maintain an efficient sparse representation by only optimizing a subset (block) of variables at a time, with memory complexity `O(b)` (block size) instead of `O(n^2)`. For scalable GNNs like PPRGo, the complexity can even be constant in the number of nodes `n`.\n        *   **Novel Surrogate Losses for Global Attacks**: Proposes **Masked Cross Entropy (MCE)** and a **tanh margin loss** to overcome the limitations of standard surrogate losses. MCE focuses only on correctly classified nodes, while the tanh margin loss encourages confident misclassification and prioritizes nodes close to the decision boundary.\n        *   **Scalable Robust Aggregation Function**: Designs **Soft Median**, a computationally efficient, robust, and differentiable aggregation function inspired by differentiable sorting. This function enables robust GNNs to scale effectively.\n    *   **Novelty/Differentiation**:\n        *   **Scalability**: The primary innovation is enabling adversarial robustness studies and defenses for GNNs on graphs orders of magnitude larger than previously possible, by addressing the `O(n^2)` memory bottleneck.\n        *   **Targeted Attack Effectiveness**: The proposed surrogate losses significantly improve the strength of global attacks by focusing the perturbation budget more effectively.\n        *   **Efficient Defense**: Soft Median provides a robust defense mechanism that is also scalable, unlike prior robust GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **PR-BCD and GR-BCD**: Sparsity-aware first-order optimization attacks that enable global GNN attacks on large graphs with `O(b)` memory complexity.\n        *   **Masked Cross Entropy (MCE) and tanh margin loss**: New surrogate loss functions specifically designed for effective global attacks on GNNs, addressing the shortcomings of CE and CW losses.\n        *   **Soft Median**: A novel, differentiable, and scalable robust aggregation function for GNNs, inspired by differentiable sorting.\n    *   **Theoretical Insights**: Formalizes properties for effective surrogate losses for global attacks (Definition 1 & 2, Proposition 1), demonstrating why CE and CW are suboptimal.\n    *   **System Design/Architectural Innovations**: Extends attack and defense techniques to work with scalable GNN architectures like PPRGo, achieving even greater scalability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated attack strength and memory efficiency of PR-BCD against PGD on various datasets (e.g., PubMed, arXiv, Products).\n        *   Compared the effectiveness of proposed surrogate losses (MCE, tanh margin) against CE and CW losses.\n        *   Assessed the robustness and accuracy of GNNs equipped with Soft Median aggregation against various attacks.\n        *   Scaled attacks and defenses to graphs up to 2.5 million nodes for global attacks and 111 million nodes for local attacks, and further to 170 million nodes with PPRGo.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Attack Strength**: Proposed surrogate losses (MCE, tanh margin) can double the attack strength compared to CE on common datasets, with the gap widening on larger graphs.\n        *   **Memory Efficiency**: PR-BCD significantly reduces GPU memory consumption compared to PGD (e.g., Figure 1 shows substantial improvement, allowing attacks on graphs 100 times larger).\n        *   **Scalability**: Successfully attacked and defended GNNs on graphs more than 100 times larger than previous work, and one order of magnitude further by extending techniques to PPRGo.\n        *   **Defense Effectiveness**: Soft Median achieves similar robustness to prior robust GNNs but with a significantly lower memory footprint, making it effective at scale.\n        *   **Block Size Influence**: Demonstrated that PR-BCD performs comparably to dense equivalents over a wide range of block sizes, and increasing epochs can compensate for smaller block sizes on larger graphs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The proposed attacks are approximate and do not provide guarantees on how well they approximate the actual discrete optimization problem, offering only an upper bound on adversarial accuracy.\n        *   Attacks rely on the victim model being (approximately) differentiable.\n        *   The scalability of the attacks is ultimately limited by the scalability of the attacked GNN itself.\n    *   **Scope of Applicability**: Primarily focuses on evasion (test-time) structure attacks for node classification, though methods can be adapted for poisoning attacks. Assumes a white-box threat model.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{geisler2021dcq} sets the foundation for the holistic study of adversarial robustness of GNNs at unprecedented scales, overcoming critical memory and computational bottlenecks. It provides the first scalable attacks and defenses for GNNs on graphs with millions of nodes.\n    *   **Potential Impact on Future Research**: Enables researchers to investigate GNN robustness in more realistic, large-scale settings. The insights into surrogate loss design and scalable robust aggregation functions can guide the development of more effective and practical adversarial machine learning techniques for graph data. It highlights the need for careful deployment of GNNs in real-world applications and encourages further research into scalable robustness.",
    "intriguing_abstract": "The widespread deployment of Graph Neural Networks (GNNs) in critical, large-scale applications underscores an urgent need to understand and mitigate their adversarial vulnerabilities. However, studying GNN adversarial robustness has been severely hampered by prohibitive `O(n^2)` memory and computational costs, limiting research to small graphs. We introduce a suite of novel techniques to overcome these barriers: **Projected Randomized Block Coordinate Descent (PR-BCD)** and **Greedy R-BCD (GR-BCD)**, sparsity-aware first-order optimization attacks that enable global perturbations on graphs with millions of nodes by reducing memory complexity to `O(b)`. We also propose **Masked Cross Entropy (MCE)** and a **tanh margin loss**, significantly enhancing attack strength by focusing perturbation budgets. Furthermore, we design **Soft Median**, a scalable and differentiable robust aggregation function, enabling robust GNN defenses at unprecedented scales. Our methods allow for the first holistic study of GNN adversarial robustness on graphs orders of magnitude larger than previously possible, successfully attacking and defending GNNs on datasets up to 170 million nodes. Experiments demonstrate that our novel surrogate losses can double attack strength, while Soft Median provides effective robustness with minimal memory overhead. This work fundamentally reshapes the landscape of GNN security research, paving the way for the development of truly robust and reliable GNNs essential for real-world, Internet-scale deployments.",
    "keywords": [
      "Graph Neural Networks (GNNs)",
      "adversarial robustness",
      "large-scale graphs",
      "O(n^2) memory bottleneck",
      "sparsity-aware optimization attacks",
      "Projected Randomized Block Coordinate Descent (PR-BCD)",
      "Greedy R-BCD (GR-BCD)",
      "novel surrogate losses",
      "Masked Cross Entropy (MCE)",
      "tanh margin loss",
      "scalable robust aggregation",
      "Soft Median",
      "scalable GNN attacks and defenses",
      "node classification"
    ],
    "file_path": "paper_data/Graph_Neural_Networks/3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf",
    "citation_key": "geisler2021dcq",
    "metadata": {
      "title": "Robustness of Graph Neural Networks at Scale",
      "authors": [
        "Simon Geisler",
        "Tobias Schmidt",
        "Hakan cSirin",
        "Daniel Zugner",
        "Aleksandar Bojchevski",
        "Stephan Gunnemann"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.",
      "file_path": "paper_data/Graph_Neural_Networks/3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 0,
      "score": 0,
      "summary": "Here's a focused summary of the technical paper \\cite{geisler2021dcq} for a literature review:\n\n### Focused Summary for Literature Review: Robustness of Graph Neural Networks at Scale \\cite{geisler2021dcq}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing studies on the adversarial robustness of Graph Neural Networks (GNNs) are limited to relatively small graphs, making it challenging to understand and mitigate vulnerabilities in real-world, large-scale GNN deployments.\n    *   **Importance and Challenge**: GNNs are increasingly popular in Internet-scale applications, but their vulnerability to adversarial attacks is well-established. Prior attack methods suffer from prohibitive memory requirements (e.g., quadratic in the number of nodes, `O(n^2)`) and computational costs, making it practically infeasible to study robustness on graphs with millions of nodes. Additionally, common surrogate losses used in attacks are ineffective for global attacks on large graphs, and existing robust GNNs are not scalable.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{geisler2021dcq} builds upon gradient-based adversarial attacks (e.g., PGD, Carlini-Wagner) and robust GNN designs, but critically extends them to operate at significantly larger scales.\n    *   **Limitations of Previous Solutions**:\n        *   **Scalability**: Prior gradient-based attacks optimize over a dense adjacency matrix, leading to `O(n^2)` space complexity, which is impractical for large graphs (e.g., PubMed requiring 20GB for a dense attack). Local attacks like SGA \\cite{li2020sga} are limited to ~200k nodes and specific GNN architectures.\n        *   **Surrogate Losses**: Common surrogate losses like Cross Entropy (CE) or Carlini-Wagner (CW) are shown to be poorly suited for global attacks on GNNs, especially with small budgets on large graphs. They tend to waste budget on already misclassified nodes or confident nodes, rather than focusing on \"fragile\" nodes near the decision boundary.\n        *   **Robust GNNs**: Previous robust GNNs typically lack scalability, making them unsuitable for large-scale deployment.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**:\n        *   **Sparsity-Aware First-Order Optimization Attacks**: Introduces two methods, **Projected Randomized Block Coordinate Descent (PR-BCD)** and **Greedy R-BCD (GR-BCD)**, which leverage Randomized Block Coordinate Descent (R-BCD) to optimize perturbations. These attacks maintain an efficient sparse representation by only optimizing a subset (block) of variables at a time, with memory complexity `O(b)` (block size) instead of `O(n^2)`. For scalable GNNs like PPRGo, the complexity can even be constant in the number of nodes `n`.\n        *   **Novel Surrogate Losses for Global Attacks**: Proposes **Masked Cross Entropy (MCE)** and a **tanh margin loss** to overcome the limitations of standard surrogate losses. MCE focuses only on correctly classified nodes, while the tanh margin loss encourages confident misclassification and prioritizes nodes close to the decision boundary.\n        *   **Scalable Robust Aggregation Function**: Designs **Soft Median**, a computationally efficient, robust, and differentiable aggregation function inspired by differentiable sorting. This function enables robust GNNs to scale effectively.\n    *   **Novelty/Differentiation**:\n        *   **Scalability**: The primary innovation is enabling adversarial robustness studies and defenses for GNNs on graphs orders of magnitude larger than previously possible, by addressing the `O(n^2)` memory bottleneck.\n        *   **Targeted Attack Effectiveness**: The proposed surrogate losses significantly improve the strength of global attacks by focusing the perturbation budget more effectively.\n        *   **Efficient Defense**: Soft Median provides a robust defense mechanism that is also scalable, unlike prior robust GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **PR-BCD and GR-BCD**: Sparsity-aware first-order optimization attacks that enable global GNN attacks on large graphs with `O(b)` memory complexity.\n        *   **Masked Cross Entropy (MCE) and tanh margin loss**: New surrogate loss functions specifically designed for effective global attacks on GNNs, addressing the shortcomings of CE and CW losses.\n        *   **Soft Median**: A novel, differentiable, and scalable robust aggregation function for GNNs, inspired by differentiable sorting.\n    *   **Theoretical Insights**: Formalizes properties for effective surrogate losses for global attacks (Definition 1 & 2, Proposition 1), demonstrating why CE and CW are suboptimal.\n    *   **System Design/Architectural Innovations**: Extends attack and defense techniques to work with scalable GNN architectures like PPRGo, achieving even greater scalability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated attack strength and memory efficiency of PR-BCD against PGD on various datasets (e.g., PubMed, arXiv, Products).\n        *   Compared the effectiveness of proposed surrogate losses (MCE, tanh margin) against CE and CW losses.\n        *   Assessed the robustness and accuracy of GNNs equipped with Soft Median aggregation against various attacks.\n        *   Scaled attacks and defenses to graphs up to 2.5 million nodes for global attacks and 111 million nodes for local attacks, and further to 170 million nodes with PPRGo.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Attack Strength**: Proposed surrogate losses (MCE, tanh margin) can double the attack strength compared to CE on common datasets, with the gap widening on larger graphs.\n        *   **Memory Efficiency**: PR-BCD significantly reduces GPU memory consumption compared to PGD (e.g., Figure 1 shows substantial improvement, allowing attacks on graphs 100 times larger).\n        *   **Scalability**: Successfully attacked and defended GNNs on graphs more than 100 times larger than previous work, and one order of magnitude further by extending techniques to PPRGo.\n        *   **Defense Effectiveness**: Soft Median achieves similar robustness to prior robust GNNs but with a significantly lower memory footprint, making it effective at scale.\n        *   **Block Size Influence**: Demonstrated that PR-BCD performs comparably to dense equivalents over a wide range of block sizes, and increasing epochs can compensate for smaller block sizes on larger graphs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The proposed attacks are approximate and do not provide guarantees on how well they approximate the actual discrete optimization problem, offering only an upper bound on adversarial accuracy.\n        *   Attacks rely on the victim model being (approximately) differentiable.\n        *   The scalability of the attacks is ultimately limited by the scalability of the attacked GNN itself.\n    *   **Scope of Applicability**: Primarily focuses on evasion (test-time) structure attacks for node classification, though methods can be adapted for poisoning attacks. Assumes a white-box threat model.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{geisler2021dcq} sets the foundation for the holistic study of adversarial robustness of GNNs at unprecedented scales, overcoming critical memory and computational bottlenecks. It provides the first scalable attacks and defenses for GNNs on graphs with millions of nodes.\n    *   **Potential Impact on Future Research**: Enables researchers to investigate GNN robustness in more realistic, large-scale settings. The insights into surrogate loss design and scalable robust aggregation functions can guide the development of more effective and practical adversarial machine learning techniques for graph data. It highlights the need for careful deployment of GNNs in real-world applications and encourages further research into scalable robustness.",
      "keywords": [
        "Graph Neural Networks (GNNs)",
        "adversarial robustness",
        "large-scale graphs",
        "O(n^2) memory bottleneck",
        "sparsity-aware optimization attacks",
        "Projected Randomized Block Coordinate Descent (PR-BCD)",
        "Greedy R-BCD (GR-BCD)",
        "novel surrogate losses",
        "Masked Cross Entropy (MCE)",
        "tanh margin loss",
        "scalable robust aggregation",
        "Soft Median",
        "scalable GNN attacks and defenses",
        "node classification"
      ],
      "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose two sparsity-aware ﬁrst-order optimization attacks\" and \"we design a robust aggregation function, soft median\". it also mentions \"evaluate our attacks and defense\".\n*   the introduction discusses a technical problem (vulnerability of gnns at scale, memory requirements) and highlights the \"substantial improvement of memory efﬁcie ncy of our attack over a popular prior work\".\n\nthese points strongly align with the criteria for a **technical** paper:\n*   **abstract mentions:** \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" (e.g., \"propose two sparsity-aware ﬁrst-order optimization attacks\", \"design a robust aggregation function\").\n*   **introduction discusses:** \"technical problem\" (robustness at scale, memory efficiency), \"proposed solution\" (our attack, our defense).\n\nwhile the paper also involves significant empirical evaluation (\"we evaluate our attacks and defense with standard gnns on graphs more than 100 times larger\"), the core contribution is the *development and proposal* of new methods and systems. empirical studies are often a crucial part of validating technical contributions.\n\ntherefore, the most appropriate classification is **technical**."
    },
    "file_name": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf"
  },
  {
    "success": true,
    "doc_id": "ab2cd81b254e4396a25511eeecd7ed68",
    "summary": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
    "intriguing_abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf",
    "citation_key": "wu20193b0",
    "metadata": {
      "title": "A Comprehensive Survey on Graph Neural Networks",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Fengwen Chen",
        "Guodong Long",
        "Chengqi Zhang",
        "Philip S. Yu"
      ],
      "published_date": "2019",
      "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
      "file_path": "paper_data/Graph_Neural_Networks/info/81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "citationCount": 9135,
      "score": 1522.5,
      "summary": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
      "keywords": []
    },
    "file_name": "81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf"
  },
  {
    "success": true,
    "doc_id": "837833c6ec0362e4916d38d791571649",
    "summary": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf",
    "citation_key": "xu2018c8q",
    "metadata": {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": [
        "Keyulu Xu",
        "Weihua Hu",
        "J. Leskovec",
        "S. Jegelka"
      ],
      "published_date": "2018",
      "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
      "file_path": "paper_data/Graph_Neural_Networks/info/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 8157,
      "score": 1165.2857142857142,
      "summary": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
      "keywords": []
    },
    "file_name": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf"
  },
  {
    "success": true,
    "doc_id": "ca7001e65c61d46974619e9197400964",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf",
    "citation_key": "zhou20188n6",
    "metadata": {
      "title": "Graph Neural Networks: A Review of Methods and Applications",
      "authors": [
        "Jie Zhou",
        "Ganqu Cui",
        "Zhengyan Zhang",
        "Cheng Yang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "published_date": "2018",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf",
      "venue": "AI Open",
      "citationCount": 5934,
      "score": 847.7142857142857,
      "summary": "",
      "keywords": []
    },
    "file_name": "ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf"
  },
  {
    "success": true,
    "doc_id": "743dc7d43b3297a8be576295b2cc3ac3",
    "summary": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
    "intriguing_abstract": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7456dea3a3646f2df6392773a196a5abd0d53b11.pdf",
    "citation_key": "batzner2021t07",
    "metadata": {
      "title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
      "authors": [
        "Simon L. Batzner",
        "Albert Musaelian",
        "Lixin Sun",
        "M. Geiger",
        "J. Mailoa",
        "M. Kornbluth",
        "N. Molinari",
        "T. Smidt",
        "B. Kozinsky"
      ],
      "published_date": "2021",
      "abstract": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7456dea3a3646f2df6392773a196a5abd0d53b11.pdf",
      "venue": "Nature Communications",
      "citationCount": 1496,
      "score": 374.0,
      "summary": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
      "keywords": []
    },
    "file_name": "7456dea3a3646f2df6392773a196a5abd0d53b11.pdf"
  },
  {
    "success": true,
    "doc_id": "57a6a851e12f4e514c7df0b8dbf01f71",
    "summary": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
    "intriguing_abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf",
    "citation_key": "sarlin20198a6",
    "metadata": {
      "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
      "authors": [
        "Paul-Edouard Sarlin",
        "Daniel DeTone",
        "Tomasz Malisiewicz",
        "Andrew Rabinovich"
      ],
      "published_date": "2019",
      "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
      "file_path": "paper_data/Graph_Neural_Networks/info/347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf",
      "venue": "Computer Vision and Pattern Recognition",
      "citationCount": 2131,
      "score": 355.16666666666663,
      "summary": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
      "keywords": []
    },
    "file_name": "347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf"
  },
  {
    "success": true,
    "doc_id": "00d56b3a65cdcd67b123aff74442620a",
    "summary": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
    "intriguing_abstract": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf",
    "citation_key": "wu2020hi3",
    "metadata": {
      "title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Guodong Long",
        "Jing Jiang",
        "Xiaojun Chang",
        "Chengqi Zhang"
      ],
      "published_date": "2020",
      "abstract": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
      "file_path": "paper_data/Graph_Neural_Networks/info/75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 1580,
      "score": 316.0,
      "summary": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
      "keywords": []
    },
    "file_name": "75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf"
  },
  {
    "success": true,
    "doc_id": "83deea6edd51da36325f9a5839b8bca3",
    "summary": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
    "intriguing_abstract": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf",
    "citation_key": "wu2018t43",
    "metadata": {
      "title": "Session-based Recommendation with Graph Neural Networks",
      "authors": [
        "Shu Wu",
        "Yuyuan Tang",
        "Yanqiao Zhu",
        "Liang Wang",
        "Xing Xie",
        "T. Tan"
      ],
      "published_date": "2018",
      "abstract": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 1635,
      "score": 233.57142857142856,
      "summary": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
      "keywords": []
    },
    "file_name": "0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf"
  },
  {
    "success": true,
    "doc_id": "8a0176979e54d7098ad388a4ce5c3446",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf",
    "citation_key": "zhu2020c3j",
    "metadata": {
      "title": "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs",
      "authors": [
        "Jiong Zhu",
        "Yujun Yan",
        "Lingxiao Zhao",
        "Mark Heimann",
        "L. Akoglu",
        "Danai Koutra"
      ],
      "published_date": "2020",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 1079,
      "score": 215.8,
      "summary": "",
      "keywords": []
    },
    "file_name": "21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf"
  },
  {
    "success": true,
    "doc_id": "7396d9c6daadcde482ac719f482c8af5",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/381411d740562de1e766dc8cc833844eb99dde01.pdf",
    "citation_key": "wang2019t4a",
    "metadata": {
      "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.",
      "authors": [
        "Minjie Wang",
        "Da Zheng",
        "Zihao Ye",
        "Quan Gan",
        "Mufei Li",
        "Xiang Song",
        "Jinjing Zhou",
        "Chao Ma",
        "Lingfan Yu",
        "Yujie Gai",
        "Tianjun Xiao",
        "Tong He",
        "G. Karypis",
        "Jinyang Li",
        "Zheng Zhang"
      ],
      "published_date": "2019",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/381411d740562de1e766dc8cc833844eb99dde01.pdf",
      "venue": "",
      "citationCount": 1171,
      "score": 195.16666666666666,
      "summary": "",
      "keywords": []
    },
    "file_name": "381411d740562de1e766dc8cc833844eb99dde01.pdf"
  },
  {
    "success": true,
    "doc_id": "d1019647a9cb205824ea6a32a5b41a20",
    "summary": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks usually utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. Furthermore, existing methods were out at elbows when solving complicated spatial-temporal data: they usually utilize separate modules for spatial and temporal correlations, or they only use independent components capturing localized or global heterogeneous dependencies. To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. First, a data-driven method of generating “temporal graph” is proposed to compensate several genuine correlations that spatial graph may not reflect. STFGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer parallelly, STFGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
    "intriguing_abstract": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks usually utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. Furthermore, existing methods were out at elbows when solving complicated spatial-temporal data: they usually utilize separate modules for spatial and temporal correlations, or they only use independent components capturing localized or global heterogeneous dependencies. To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. First, a data-driven method of generating “temporal graph” is proposed to compensate several genuine correlations that spatial graph may not reflect. STFGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer parallelly, STFGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf",
    "citation_key": "li2020fil",
    "metadata": {
      "title": "Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting",
      "authors": [
        "Mengzhang Li",
        "Zhanxing Zhu"
      ],
      "published_date": "2020",
      "abstract": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks usually utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. Furthermore, existing methods were out at elbows when solving complicated spatial-temporal data: they usually utilize separate modules for spatial and temporal correlations, or they only use independent components capturing localized or global heterogeneous dependencies. To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. First, a data-driven method of generating “temporal graph” is proposed to compensate several genuine correlations that spatial graph may not reflect. STFGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer parallelly, STFGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
      "file_path": "paper_data/Graph_Neural_Networks/info/24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 849,
      "score": 169.8,
      "summary": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks usually utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. Furthermore, existing methods were out at elbows when solving complicated spatial-temporal data: they usually utilize separate modules for spatial and temporal correlations, or they only use independent components capturing localized or global heterogeneous dependencies. To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. First, a data-driven method of generating “temporal graph” is proposed to compensate several genuine correlations that spatial graph may not reflect. STFGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer parallelly, STFGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
      "keywords": []
    },
    "file_name": "24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf"
  },
  {
    "success": true,
    "doc_id": "6a0a2b6fc66ddc34437471ee0f2c618c",
    "summary": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
    "intriguing_abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf",
    "citation_key": "satorras20174cv",
    "metadata": {
      "title": "Few-Shot Learning with Graph Neural Networks",
      "authors": [
        "Victor Garcia Satorras",
        "Joan Bruna"
      ],
      "published_date": "2017",
      "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 1261,
      "score": 157.625,
      "summary": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
      "keywords": []
    },
    "file_name": "572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf"
  },
  {
    "success": true,
    "doc_id": "fb54296013f45e961da296e598a311ce",
    "summary": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",
    "intriguing_abstract": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/00549af4bc3270e0f688acbf694f912d7ee39cad.pdf",
    "citation_key": "zhou20195xo",
    "metadata": {
      "title": "Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks",
      "authors": [
        "Yaqin Zhou",
        "Shangqing Liu",
        "J. Siow",
        "Xiaoning Du",
        "Yang Liu"
      ],
      "published_date": "2019",
      "abstract": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",
      "file_path": "paper_data/Graph_Neural_Networks/info/00549af4bc3270e0f688acbf694f912d7ee39cad.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 849,
      "score": 141.5,
      "summary": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",
      "keywords": []
    },
    "file_name": "00549af4bc3270e0f688acbf694f912d7ee39cad.pdf"
  },
  {
    "success": true,
    "doc_id": "032325a5262803a9eb267c6158c8c2ec",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf",
    "citation_key": "oono2019usb",
    "metadata": {
      "title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification",
      "authors": [
        "Kenta Oono",
        "Taiji Suzuki"
      ],
      "published_date": "2019",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 734,
      "score": 122.33333333333333,
      "summary": "",
      "keywords": []
    },
    "file_name": "4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf"
  },
  {
    "success": true,
    "doc_id": "5533b2d3947590afe7a25ae93479054d",
    "summary": "The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.",
    "intriguing_abstract": "The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/68a024d7b70ef3989a6751678f635cbe754440fc.pdf",
    "citation_key": "shi2019vl4",
    "metadata": {
      "title": "Skeleton-Based Action Recognition With Directed Graph Neural Networks",
      "authors": [
        "Lei Shi",
        "Yifan Zhang",
        "Jian Cheng",
        "Hanqing Lu"
      ],
      "published_date": "2019",
      "abstract": "The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.",
      "file_path": "paper_data/Graph_Neural_Networks/info/68a024d7b70ef3989a6751678f635cbe754440fc.pdf",
      "venue": "Computer Vision and Pattern Recognition",
      "citationCount": 717,
      "score": 119.5,
      "summary": "The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.",
      "keywords": []
    },
    "file_name": "68a024d7b70ef3989a6751678f635cbe754440fc.pdf"
  },
  {
    "success": true,
    "doc_id": "1ba6720ea7d8abb4b90e7978b46b0137",
    "summary": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
    "intriguing_abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2b8a207189bc02d73d1dce850bcde24dbd984483.pdf",
    "citation_key": "wu20221la",
    "metadata": {
      "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention",
      "authors": [
        "Zhanghao Wu",
        "Paras Jain",
        "Matthew A. Wright",
        "Azalia Mirhoseini",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2b8a207189bc02d73d1dce850bcde24dbd984483.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 336,
      "score": 112.0,
      "summary": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
      "keywords": []
    },
    "file_name": "2b8a207189bc02d73d1dce850bcde24dbd984483.pdf"
  },
  {
    "success": true,
    "doc_id": "24074b3fcb832f58ea91ca191504d210",
    "summary": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
    "intriguing_abstract": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf",
    "citation_key": "wang2020khd",
    "metadata": {
      "title": "Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
      "authors": [
        "Ziyang Wang",
        "Wei Wei",
        "G. Cong",
        "Xiaoli Li",
        "Xian-Ling Mao",
        "Minghui Qiu"
      ],
      "published_date": "2020",
      "abstract": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "citationCount": 511,
      "score": 102.2,
      "summary": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
      "keywords": []
    },
    "file_name": "b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf"
  },
  {
    "success": true,
    "doc_id": "3b2c7b5be05dc3aeefee70fe47436265",
    "summary": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
    "intriguing_abstract": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/594dc362b4332ae661e3d71da17d097bb4a357dd.pdf",
    "citation_key": "wang2019vol",
    "metadata": {
      "title": "Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems",
      "authors": [
        "Hongwei Wang",
        "Fuzheng Zhang",
        "Mengdi Zhang",
        "J. Leskovec",
        "Miao Zhao",
        "Wenjie Li",
        "Zhongyuan Wang"
      ],
      "published_date": "2019",
      "abstract": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
      "file_path": "paper_data/Graph_Neural_Networks/info/594dc362b4332ae661e3d71da17d097bb4a357dd.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 583,
      "score": 97.16666666666666,
      "summary": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
      "keywords": []
    },
    "file_name": "594dc362b4332ae661e3d71da17d097bb4a357dd.pdf"
  },
  {
    "success": true,
    "doc_id": "29c4990c1f67d969ddc6a3f118e38a59",
    "summary": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.",
    "intriguing_abstract": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf",
    "citation_key": "zhong2019kka",
    "metadata": {
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "C. Miao"
      ],
      "published_date": "2019",
      "abstract": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf",
      "venue": "IEEE Transactions on Affective Computing",
      "citationCount": 578,
      "score": 96.33333333333333,
      "summary": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.",
      "keywords": []
    },
    "file_name": "0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf"
  },
  {
    "success": true,
    "doc_id": "e02f04d7676d8fc6b8ab42bb1c12287f",
    "summary": "Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority over-sampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority over-sampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, \\method, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.",
    "intriguing_abstract": "Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority over-sampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority over-sampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, \\method, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2b1eae2cceb377cb9267b2c96294228d5e583136.pdf",
    "citation_key": "zhao2021po9",
    "metadata": {
      "title": "GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks",
      "authors": [
        "Tianxiang Zhao",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "published_date": "2021",
      "abstract": "Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority over-sampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority over-sampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, \\method, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2b1eae2cceb377cb9267b2c96294228d5e583136.pdf",
      "venue": "Web Search and Data Mining",
      "citationCount": 364,
      "score": 91.0,
      "summary": "Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority over-sampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority over-sampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, \\method, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.",
      "keywords": []
    },
    "file_name": "2b1eae2cceb377cb9267b2c96294228d5e583136.pdf"
  },
  {
    "success": true,
    "doc_id": "44b15eacb18c293621b343f19d8b3434",
    "summary": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.",
    "intriguing_abstract": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cda969fd7362bdf21aa1f3398078982dcb350d76.pdf",
    "citation_key": "lv20219al",
    "metadata": {
      "title": "Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks",
      "authors": [
        "Qingsong Lv",
        "Ming Ding",
        "Qiang Liu",
        "Yuxiang Chen",
        "Wenzheng Feng",
        "Siming He",
        "Chang Zhou",
        "Jianguo Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published_date": "2021",
      "abstract": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cda969fd7362bdf21aa1f3398078982dcb350d76.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 363,
      "score": 90.75,
      "summary": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.",
      "keywords": []
    },
    "file_name": "cda969fd7362bdf21aa1f3398078982dcb350d76.pdf"
  },
  {
    "success": true,
    "doc_id": "e2ac4635cdfae95a188d9b27ed5296ec",
    "summary": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
    "intriguing_abstract": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf",
    "citation_key": "yu201969a",
    "metadata": {
      "title": "DAG-GNN: DAG Structure Learning with Graph Neural Networks",
      "authors": [
        "Yue Yu",
        "Jie Chen",
        "Tian Gao",
        "Mo Yu"
      ],
      "published_date": "2019",
      "abstract": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 518,
      "score": 86.33333333333333,
      "summary": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
      "keywords": []
    },
    "file_name": "1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf"
  },
  {
    "success": true,
    "doc_id": "8118768c2cf6f98cc92a3c44f54ed6d1",
    "summary": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/53e80869c6582d7f95ef0a351170736afd1742d0.pdf",
    "citation_key": "tang2022g66",
    "metadata": {
      "title": "Rethinking Graph Neural Networks for Anomaly Detection",
      "authors": [
        "Jianheng Tang",
        "Jiajin Li",
        "Zi-Chao Gao",
        "Jia Li"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
      "file_path": "paper_data/Graph_Neural_Networks/info/53e80869c6582d7f95ef0a351170736afd1742d0.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 250,
      "score": 83.33333333333333,
      "summary": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
      "keywords": []
    },
    "file_name": "53e80869c6582d7f95ef0a351170736afd1742d0.pdf"
  },
  {
    "success": true,
    "doc_id": "1e56a7ac51f9eff1a4f0a8c7e5b40a03",
    "summary": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameters learning for classification task. Different from traditional GSL on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph independently. Specifically, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.",
    "intriguing_abstract": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameters learning for classification task. Different from traditional GSL on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph independently. Specifically, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf",
    "citation_key": "zhao2021lls",
    "metadata": {
      "title": "Heterogeneous Graph Structure Learning for Graph Neural Networks",
      "authors": [
        "Jianan Zhao",
        "Xiao Wang",
        "C. Shi",
        "Binbin Hu",
        "Guojie Song",
        "Yanfang Ye"
      ],
      "published_date": "2021",
      "abstract": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameters learning for classification task. Different from traditional GSL on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph independently. Specifically, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 272,
      "score": 68.0,
      "summary": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameters learning for classification task. Different from traditional GSL on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph independently. Specifically, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.",
      "keywords": []
    },
    "file_name": "6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf"
  },
  {
    "success": true,
    "doc_id": "5b161ec3257d039c01ac4c1d9338d975",
    "summary": "We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.",
    "intriguing_abstract": "We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf",
    "citation_key": "keisler2022t7p",
    "metadata": {
      "title": "Forecasting Global Weather with Graph Neural Networks",
      "authors": [
        "R. Keisler"
      ],
      "published_date": "2022",
      "abstract": "We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.",
      "file_path": "paper_data/Graph_Neural_Networks/info/80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf",
      "venue": "arXiv.org",
      "citationCount": 196,
      "score": 65.33333333333333,
      "summary": "We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.",
      "keywords": []
    },
    "file_name": "80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf"
  },
  {
    "success": true,
    "doc_id": "b9d892337fdd3a4033812212731f1f90",
    "summary": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.",
    "intriguing_abstract": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1478c3c0225368419f68aabc6b67033531d3b4c1.pdf",
    "citation_key": "li2020mk1",
    "metadata": {
      "title": "Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction",
      "authors": [
        "Maosen Li",
        "Siheng Chen",
        "Yangheng Zhao",
        "Ya Zhang",
        "Yanfeng Wang",
        "Qi Tian"
      ],
      "published_date": "2020",
      "abstract": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1478c3c0225368419f68aabc6b67033531d3b4c1.pdf",
      "venue": "Computer Vision and Pattern Recognition",
      "citationCount": 322,
      "score": 64.4,
      "summary": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.",
      "keywords": []
    },
    "file_name": "1478c3c0225368419f68aabc6b67033531d3b4c1.pdf"
  },
  {
    "success": true,
    "doc_id": "576aec46dfc1c0b1d1576a556904b346",
    "summary": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
    "intriguing_abstract": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf",
    "citation_key": "wu2022ptq",
    "metadata": {
      "title": "Graph Neural Networks: Foundation, Frontiers and Applications",
      "authors": [
        "Lingfei Wu",
        "P. Cui",
        "Jian Pei",
        "Liang Zhao",
        "Xiaojie Guo"
      ],
      "published_date": "2022",
      "abstract": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 190,
      "score": 63.33333333333333,
      "summary": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
      "keywords": []
    },
    "file_name": "b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf"
  },
  {
    "success": true,
    "doc_id": "61bb6b0b61d04be7e49f5ce6a1665700",
    "summary": "Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.",
    "intriguing_abstract": "Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/030046515a20a4b4f86c290361881923694e458a.pdf",
    "citation_key": "liu2021qyl",
    "metadata": {
      "title": "Combining Graph Neural Networks With Expert Knowledge for Smart Contract Vulnerability Detection",
      "authors": [
        "Zhenguang Liu",
        "Peng Qian",
        "Xiaoyang Wang",
        "Yuan Zhuang",
        "Lin Qiu",
        "Xun Wang"
      ],
      "published_date": "2021",
      "abstract": "Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.",
      "file_path": "paper_data/Graph_Neural_Networks/info/030046515a20a4b4f86c290361881923694e458a.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 243,
      "score": 60.75,
      "summary": "Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.",
      "keywords": []
    },
    "file_name": "030046515a20a4b4f86c290361881923694e458a.pdf"
  },
  {
    "success": true,
    "doc_id": "3cd947a5ea9b2ffff52af1d7b53522be",
    "summary": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.",
    "intriguing_abstract": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf",
    "citation_key": "zhang20211dl",
    "metadata": {
      "title": "From Canonical Correlation Analysis to Self-supervised Graph Neural Networks",
      "authors": [
        "Hengrui Zhang",
        "Qitian Wu",
        "Junchi Yan",
        "D. Wipf",
        "Philip S. Yu"
      ],
      "published_date": "2021",
      "abstract": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 241,
      "score": 60.25,
      "summary": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.",
      "keywords": []
    },
    "file_name": "e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf"
  },
  {
    "success": true,
    "doc_id": "3cb8aba3d650d759159ebdd63a4eb8a5",
    "summary": "Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.",
    "intriguing_abstract": "Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/16351ff232f2e475c8d8347809ef905d67998fa5.pdf",
    "citation_key": "shen202037i",
    "metadata": {
      "title": "Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis",
      "authors": [
        "Yifei Shen",
        "Yuanming Shi",
        "Jun Zhang",
        "K. Letaief"
      ],
      "published_date": "2020",
      "abstract": "Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.",
      "file_path": "paper_data/Graph_Neural_Networks/info/16351ff232f2e475c8d8347809ef905d67998fa5.pdf",
      "venue": "IEEE Journal on Selected Areas in Communications",
      "citationCount": 296,
      "score": 59.2,
      "summary": "Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.",
      "keywords": []
    },
    "file_name": "16351ff232f2e475c8d8347809ef905d67998fa5.pdf"
  },
  {
    "success": true,
    "doc_id": "e2d6f013534548218450651ddb801b39",
    "summary": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
    "intriguing_abstract": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf",
    "citation_key": "zhang2020tdy",
    "metadata": {
      "title": "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks",
      "authors": [
        "Yufeng Zhang",
        "Xueli Yu",
        "Zeyu Cui",
        "Shu Wu",
        "Zhongzheng Wen",
        "Liang Wang"
      ],
      "published_date": "2020",
      "abstract": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "citationCount": 296,
      "score": 59.2,
      "summary": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
      "keywords": []
    },
    "file_name": "23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf"
  },
  {
    "success": true,
    "doc_id": "32908dfdab0c8b9d864a2ba0a7268ee0",
    "summary": "Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f1e5e65941617604923225cc4bf464e370fcae67.pdf",
    "citation_key": "huang20209zd",
    "metadata": {
      "title": "Combining Label Propagation and Simple Models Out-performs Graph Neural Networks",
      "authors": [
        "Qian Huang",
        "Horace He",
        "Abhay Singh",
        "Ser-Nam Lim",
        "Austin R. Benson"
      ],
      "published_date": "2020",
      "abstract": "Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f1e5e65941617604923225cc4bf464e370fcae67.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 293,
      "score": 58.6,
      "summary": "Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL.",
      "keywords": []
    },
    "file_name": "f1e5e65941617604923225cc4bf464e370fcae67.pdf"
  },
  {
    "success": true,
    "doc_id": "004e0a7079fb1b0cd921708f7d501022",
    "summary": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as “static” spatio-temporal graphs, which are inherently “sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, “asynchronous” networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.",
    "intriguing_abstract": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as “static” spatio-temporal graphs, which are inherently “sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, “asynchronous” networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf",
    "citation_key": "schaefer2022rsz",
    "metadata": {
      "title": "AEGNN: Asynchronous Event-based Graph Neural Networks",
      "authors": [
        "S. Schaefer",
        "Daniel Gehrig",
        "D. Scaramuzza"
      ],
      "published_date": "2022",
      "abstract": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as “static” spatio-temporal graphs, which are inherently “sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, “asynchronous” networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf",
      "venue": "Computer Vision and Pattern Recognition",
      "citationCount": 165,
      "score": 55.0,
      "summary": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as “static” spatio-temporal graphs, which are inherently “sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, “asynchronous” networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.",
      "keywords": []
    },
    "file_name": "2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf"
  },
  {
    "success": true,
    "doc_id": "2bb84a9e6265261b54eb12154f46583d",
    "summary": "Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.",
    "intriguing_abstract": "Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf",
    "citation_key": "chen20201cf",
    "metadata": {
      "title": "Handling Information Loss of Graph Neural Networks for Session-based Recommendation",
      "authors": [
        "Tianwen Chen",
        "R. C. Wong"
      ],
      "published_date": "2020",
      "abstract": "Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 267,
      "score": 53.400000000000006,
      "summary": "Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.",
      "keywords": []
    },
    "file_name": "b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf"
  },
  {
    "success": true,
    "doc_id": "5d37e176b26ecfdde73612056e9ee800",
    "summary": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often yield poor performance in large scale networks (i.e., poor scalability) and unseen network settings (i.e., poor generalization). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communications problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-node graph (where the nodes may represent users, base stations, or antennas), GNNs’ generalization error and required number of training samples are <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n)$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n^{2})$ </tex-math></inline-formula> times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and the effectiveness of the proposed design framework.",
    "intriguing_abstract": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often yield poor performance in large scale networks (i.e., poor scalability) and unseen network settings (i.e., poor generalization). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communications problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-node graph (where the nodes may represent users, base stations, or antennas), GNNs’ generalization error and required number of training samples are <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n)$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n^{2})$ </tex-math></inline-formula> times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and the effectiveness of the proposed design framework.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf",
    "citation_key": "shen2022gcz",
    "metadata": {
      "title": "Graph Neural Networks for Wireless Communications: From Theory to Practice",
      "authors": [
        "Yifei Shen",
        "Jun Zhang",
        "Shenghui Song",
        "K. Letaief"
      ],
      "published_date": "2022",
      "abstract": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often yield poor performance in large scale networks (i.e., poor scalability) and unseen network settings (i.e., poor generalization). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communications problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-node graph (where the nodes may represent users, base stations, or antennas), GNNs’ generalization error and required number of training samples are <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n)$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n^{2})$ </tex-math></inline-formula> times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and the effectiveness of the proposed design framework.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf",
      "venue": "IEEE Transactions on Wireless Communications",
      "citationCount": 152,
      "score": 50.666666666666664,
      "summary": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often yield poor performance in large scale networks (i.e., poor scalability) and unseen network settings (i.e., poor generalization). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communications problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-node graph (where the nodes may represent users, base stations, or antennas), GNNs’ generalization error and required number of training samples are <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n)$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n^{2})$ </tex-math></inline-formula> times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and the effectiveness of the proposed design framework.",
      "keywords": []
    },
    "file_name": "1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf"
  },
  {
    "success": true,
    "doc_id": "02517b6c2477efbcddef5058db9fabd4",
    "summary": "Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users’ tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys",
    "intriguing_abstract": "Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users’ tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf",
    "citation_key": "sharma2022liz",
    "metadata": {
      "title": "A Survey of Graph Neural Networks for Social Recommender Systems",
      "authors": [
        "Kartik Sharma",
        "Yeon-Chang Lee",
        "S. Nambi",
        "Aditya Salian",
        "Shlok Shah",
        "Sang-Wook Kim",
        "Srijan Kumar"
      ],
      "published_date": "2022",
      "abstract": "Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users’ tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys",
      "file_path": "paper_data/Graph_Neural_Networks/info/250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf",
      "venue": "ACM Computing Surveys",
      "citationCount": 147,
      "score": 49.0,
      "summary": "Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users’ tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys",
      "keywords": []
    },
    "file_name": "250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf"
  },
  {
    "success": true,
    "doc_id": "c5be310dff49bea5e74a49bea140fb26",
    "summary": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
    "intriguing_abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2028710190373ef893e3055c9113e04274a152d7.pdf",
    "citation_key": "chen2021x8i",
    "metadata": {
      "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
      "authors": [
        "Tianlong Chen",
        "Yongduo Sui",
        "Xuxi Chen",
        "Aston Zhang",
        "Zhangyang Wang"
      ],
      "published_date": "2021",
      "abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2028710190373ef893e3055c9113e04274a152d7.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 188,
      "score": 47.0,
      "summary": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
      "keywords": []
    },
    "file_name": "2028710190373ef893e3055c9113e04274a152d7.pdf"
  },
  {
    "success": true,
    "doc_id": "37619885f0f5e3d380a5a7c42ba9a19d",
    "summary": "Recent graph neural network (GNN) based methods for few-shot learning (FSL) represent the samples of interest as a fully-connected graph and conduct reasoning on the nodes flatly, which ignores the hierarchical correlations among nodes. However, real-world categories may have hierarchical structures, and for FSL, it is important to extract the distinguishing features of the categories from individual samples. To explore this, we propose a novel hierarchical graph neural network (HGNN) for FSL, which consists of three parts, i.e., bottom-up reasoning, top-down reasoning, and skip connections, to enable the efficient learning of multi-level relationships. For the bottom-up reasoning, we design intra-class k-nearest neighbor pooling (intra-class knnPool) and inter-class knnPool layers, to conduct hierarchical learning for both the intra- and inter-class nodes. For the top-down reasoning, we propose to utilize graph unpooling (gUnpool) layers to restore the down-sampled graph into its original size. Skip connections are proposed to fuse multi-level features for the final node classification. The parameters of HGNN are learned by episodic training with the signal of node losses, which aims to train a well-generalizable model for recognizing unseen classes with few labeled data. Experimental results on benchmark datasets have demonstrated that HGNN outperforms other state-of-the-art GNN based methods significantly, for both transductive and non-transductive FSL tasks. The dataset as well as the source code can be downloaded online1",
    "intriguing_abstract": "Recent graph neural network (GNN) based methods for few-shot learning (FSL) represent the samples of interest as a fully-connected graph and conduct reasoning on the nodes flatly, which ignores the hierarchical correlations among nodes. However, real-world categories may have hierarchical structures, and for FSL, it is important to extract the distinguishing features of the categories from individual samples. To explore this, we propose a novel hierarchical graph neural network (HGNN) for FSL, which consists of three parts, i.e., bottom-up reasoning, top-down reasoning, and skip connections, to enable the efficient learning of multi-level relationships. For the bottom-up reasoning, we design intra-class k-nearest neighbor pooling (intra-class knnPool) and inter-class knnPool layers, to conduct hierarchical learning for both the intra- and inter-class nodes. For the top-down reasoning, we propose to utilize graph unpooling (gUnpool) layers to restore the down-sampled graph into its original size. Skip connections are proposed to fuse multi-level features for the final node classification. The parameters of HGNN are learned by episodic training with the signal of node losses, which aims to train a well-generalizable model for recognizing unseen classes with few labeled data. Experimental results on benchmark datasets have demonstrated that HGNN outperforms other state-of-the-art GNN based methods significantly, for both transductive and non-transductive FSL tasks. The dataset as well as the source code can be downloaded online1",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/18b2c7dd5f37818f74407a69985322f8a109f75f.pdf",
    "citation_key": "chen2022ifd",
    "metadata": {
      "title": "Hierarchical Graph Neural Networks for Few-Shot Learning",
      "authors": [
        "Cen Chen",
        "Kenli Li",
        "Wei Wei",
        "Joey Tianyi Zhou",
        "Zeng Zeng"
      ],
      "published_date": "2022",
      "abstract": "Recent graph neural network (GNN) based methods for few-shot learning (FSL) represent the samples of interest as a fully-connected graph and conduct reasoning on the nodes flatly, which ignores the hierarchical correlations among nodes. However, real-world categories may have hierarchical structures, and for FSL, it is important to extract the distinguishing features of the categories from individual samples. To explore this, we propose a novel hierarchical graph neural network (HGNN) for FSL, which consists of three parts, i.e., bottom-up reasoning, top-down reasoning, and skip connections, to enable the efficient learning of multi-level relationships. For the bottom-up reasoning, we design intra-class k-nearest neighbor pooling (intra-class knnPool) and inter-class knnPool layers, to conduct hierarchical learning for both the intra- and inter-class nodes. For the top-down reasoning, we propose to utilize graph unpooling (gUnpool) layers to restore the down-sampled graph into its original size. Skip connections are proposed to fuse multi-level features for the final node classification. The parameters of HGNN are learned by episodic training with the signal of node losses, which aims to train a well-generalizable model for recognizing unseen classes with few labeled data. Experimental results on benchmark datasets have demonstrated that HGNN outperforms other state-of-the-art GNN based methods significantly, for both transductive and non-transductive FSL tasks. The dataset as well as the source code can be downloaded online1",
      "file_path": "paper_data/Graph_Neural_Networks/info/18b2c7dd5f37818f74407a69985322f8a109f75f.pdf",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "citationCount": 137,
      "score": 45.666666666666664,
      "summary": "Recent graph neural network (GNN) based methods for few-shot learning (FSL) represent the samples of interest as a fully-connected graph and conduct reasoning on the nodes flatly, which ignores the hierarchical correlations among nodes. However, real-world categories may have hierarchical structures, and for FSL, it is important to extract the distinguishing features of the categories from individual samples. To explore this, we propose a novel hierarchical graph neural network (HGNN) for FSL, which consists of three parts, i.e., bottom-up reasoning, top-down reasoning, and skip connections, to enable the efficient learning of multi-level relationships. For the bottom-up reasoning, we design intra-class k-nearest neighbor pooling (intra-class knnPool) and inter-class knnPool layers, to conduct hierarchical learning for both the intra- and inter-class nodes. For the top-down reasoning, we propose to utilize graph unpooling (gUnpool) layers to restore the down-sampled graph into its original size. Skip connections are proposed to fuse multi-level features for the final node classification. The parameters of HGNN are learned by episodic training with the signal of node losses, which aims to train a well-generalizable model for recognizing unseen classes with few labeled data. Experimental results on benchmark datasets have demonstrated that HGNN outperforms other state-of-the-art GNN based methods significantly, for both transductive and non-transductive FSL tasks. The dataset as well as the source code can be downloaded online1",
      "keywords": []
    },
    "file_name": "18b2c7dd5f37818f74407a69985322f8a109f75f.pdf"
  },
  {
    "success": true,
    "doc_id": "80ea32b12b1b8ff8d492f6138007e0ab",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5ab6888c67d2877f15c2b065da4216538835d141.pdf",
    "citation_key": "li2022hw4",
    "metadata": {
      "title": "Cell clustering for spatial transcriptomics data with graph neural networks",
      "authors": [
        "Jiachen Li",
        "Siheng Chen",
        "Xiaoyong Pan",
        "Ye Yuan",
        "Hongbin Shen"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/5ab6888c67d2877f15c2b065da4216538835d141.pdf",
      "venue": "Nature Computational Science",
      "citationCount": 136,
      "score": 45.33333333333333,
      "summary": "",
      "keywords": []
    },
    "file_name": "5ab6888c67d2877f15c2b065da4216538835d141.pdf"
  },
  {
    "success": true,
    "doc_id": "ff4ab6f791ac0cf7f535aec18310be21",
    "summary": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/38e320f860d54e4071d68955c774b3e4a091bfe0.pdf",
    "citation_key": "yun2022s4i",
    "metadata": {
      "title": "Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction",
      "authors": [
        "Seongjun Yun",
        "Seoyoon Kim",
        "Junhyun Lee",
        "Jaewoo Kang",
        "Hyunwoo J. Kim"
      ],
      "published_date": "2022",
      "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/38e320f860d54e4071d68955c774b3e4a091bfe0.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 134,
      "score": 44.666666666666664,
      "summary": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs.",
      "keywords": []
    },
    "file_name": "38e320f860d54e4071d68955c774b3e4a091bfe0.pdf"
  },
  {
    "success": true,
    "doc_id": "ca4d86a74b06ccfc69905134e81377cc",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf",
    "citation_key": "wijesinghe20225ms",
    "metadata": {
      "title": "A New Perspective on \"How Graph Neural Networks Go Beyond Weisfeiler-Lehman?\"",
      "authors": [
        "Asiri Wijesinghe",
        "Qing Wang"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 101,
      "score": 33.666666666666664,
      "summary": "",
      "keywords": []
    },
    "file_name": "6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf"
  },
  {
    "success": true,
    "doc_id": "7bb6184acb75e6e53cb844ee09b0ff24",
    "summary": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",
    "intriguing_abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf",
    "citation_key": "cini20213l6",
    "metadata": {
      "title": "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks",
      "authors": [
        "Andrea Cini",
        "Ivan Marisca",
        "C. Alippi"
      ],
      "published_date": "2021",
      "abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 127,
      "score": 31.75,
      "summary": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",
      "keywords": []
    },
    "file_name": "2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf"
  },
  {
    "success": true,
    "doc_id": "2bb3919e6535fa0b247ddab5e8f9a4ce",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf",
    "citation_key": "wu2023zm5",
    "metadata": {
      "title": "Graph Neural Networks for Natural Language Processing: A Survey",
      "authors": [
        "Lingfei Wu",
        "Yu Chen",
        "Kai Shen",
        "Xiaojie Guo",
        "Hanning Gao",
        "Shucheng Li",
        "J. Pei",
        "Bo Long"
      ],
      "published_date": "2023",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf",
      "venue": "Found. Trends Mach. Learn.",
      "citationCount": 325,
      "score": 162.5,
      "summary": "",
      "keywords": []
    },
    "file_name": "da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf"
  },
  {
    "success": true,
    "doc_id": "7ec8bebb7f54ae4f5075953af286e7e5",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf",
    "citation_key": "li2022a34",
    "metadata": {
      "title": "The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study",
      "authors": [
        "Tianfu Li",
        "Zheng Zhou",
        "Sinan Li",
        "Chuang Sun",
        "Ruqiang Yan",
        "Xuefeng Chen"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf",
      "venue": "Mechanical systems and signal processing",
      "citationCount": 413,
      "score": 137.66666666666666,
      "summary": "",
      "keywords": []
    },
    "file_name": "91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf"
  },
  {
    "success": true,
    "doc_id": "1a6483d1190fb078dbccb938428a016c",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf",
    "citation_key": "velickovic2023p4r",
    "metadata": {
      "title": "Everything is Connected: Graph Neural Networks",
      "authors": [
        "Petar Velickovic"
      ],
      "published_date": "2023",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf",
      "venue": "Current Opinion in Structural Biology",
      "citationCount": 218,
      "score": 109.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf"
  },
  {
    "success": true,
    "doc_id": "c8ba21046a90567492f796efbbba9e31",
    "summary": "Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.",
    "intriguing_abstract": "Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf",
    "citation_key": "jiang2020gaq",
    "metadata": {
      "title": "Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models",
      "authors": [
        "Dejun Jiang",
        "Zhenxing Wu",
        "Chang-Yu Hsieh",
        "Guangyong Chen",
        "B. Liao",
        "Zhe Wang",
        "Chao Shen",
        "Dongsheng Cao",
        "Jian Wu",
        "Tingjun Hou"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.",
      "file_path": "paper_data/Graph_Neural_Networks/info/08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf",
      "venue": "Journal of Cheminformatics",
      "citationCount": 499,
      "score": 99.80000000000001,
      "summary": "Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.",
      "keywords": []
    },
    "file_name": "08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf"
  },
  {
    "success": true,
    "doc_id": "e03fdedea4a6bbb1d9efb03e961031ba",
    "summary": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
    "intriguing_abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/80c698688bb4488beaceaab5c64f701a946cb7ae.pdf",
    "citation_key": "sun2023vsl",
    "metadata": {
      "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
      "authors": [
        "Xiangguo Sun",
        "Hongtao Cheng",
        "Jia Li",
        "Bo Liu",
        "J. Guan"
      ],
      "published_date": "2023",
      "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
      "file_path": "paper_data/Graph_Neural_Networks/info/80c698688bb4488beaceaab5c64f701a946cb7ae.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 174,
      "score": 87.0,
      "summary": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
      "keywords": []
    },
    "file_name": "80c698688bb4488beaceaab5c64f701a946cb7ae.pdf"
  },
  {
    "success": true,
    "doc_id": "d7c793d735d016b119fc5d48c1db5747",
    "summary": "Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",
    "intriguing_abstract": "Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf",
    "citation_key": "zhang2021f18",
    "metadata": {
      "title": "Graph Neural Networks and Their Current Applications in Bioinformatics",
      "authors": [
        "Xiao-Meng Zhang",
        "Li Liang",
        "Lin Liu",
        "Mingjing Tang"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",
      "file_path": "paper_data/Graph_Neural_Networks/info/19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf",
      "venue": "Frontiers in Genetics",
      "citationCount": 322,
      "score": 80.5,
      "summary": "Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",
      "keywords": []
    },
    "file_name": "19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf"
  },
  {
    "success": true,
    "doc_id": "e4bac3722a52e4d51bb3b77bffcd32f4",
    "summary": "Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -- many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.",
    "intriguing_abstract": "Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -- many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3da4626411d83c19c9919bb41dba94fff88da90e.pdf",
    "citation_key": "bojchevski2020c51",
    "metadata": {
      "title": "Scaling Graph Neural Networks with Approximate PageRank",
      "authors": [
        "Aleksandar Bojchevski",
        "Johannes Klicpera",
        "Bryan Perozzi",
        "Amol Kapoor",
        "Martin J. Blais",
        "Benedek R'ozemberczki",
        "Michal Lukasik",
        "Stephan Gunnemann"
      ],
      "published_date": "2020",
      "abstract": "Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -- many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3da4626411d83c19c9919bb41dba94fff88da90e.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 392,
      "score": 78.4,
      "summary": "Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -- many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.",
      "keywords": []
    },
    "file_name": "3da4626411d83c19c9919bb41dba94fff88da90e.pdf"
  },
  {
    "success": true,
    "doc_id": "505cd52d4089caedaba78cad51d4b6f3",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f470ac3537339514bb9d88fcad9c075441906d45.pdf",
    "citation_key": "xia2023bpu",
    "metadata": {
      "title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules",
      "authors": [
        "Jun Xia",
        "Chengshuai Zhao",
        "Bozhen Hu",
        "Zhangyang Gao",
        "Cheng Tan",
        "Yue Liu",
        "Siyuan Li",
        "Stan Z. Li"
      ],
      "published_date": "2023",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/f470ac3537339514bb9d88fcad9c075441906d45.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 155,
      "score": 77.5,
      "summary": "",
      "keywords": []
    },
    "file_name": "f470ac3537339514bb9d88fcad9c075441906d45.pdf"
  },
  {
    "success": true,
    "doc_id": "5e063f1b6294522e8452c755ed9e12e9",
    "summary": "Graph neural networks (GNNs) have been extensively used in a wide variety of domains in recent years. Owing to their power in analyzing graph-structured data, they have become broadly popular in intelligent transportation systems (ITS) applications as well. Despite their widespread applications in different transportation domains, there is no comprehensive review of recent advancements and future research directions that covers all transportation areas. Accordingly, in this survey, for the first time, we provide an overview of GNN studies in the general domain of ITS. Unlike previous surveys, which have been limited to traffic forecasting problems, we explore how GNN frameworks have evolved for different ITS applications, including traffic forecasting, demand prediction, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety. Also, we micro-categorize the studies based on their transportation application to identify domain-specific research directions, opportunities, and challenges, which have been missing in previous surveys. Moreover, we identify unique and undiscussed research opportunities and directions, which is the result of reviewing a wide range of transportation applications. The neglected role of edge and graph learning in ITS applications, developing multi-modal models, and exploiting the power of unsupervised and reinforcement learning methods for developing more powerful GNNs are some examples of such new discussions in this survey. Finally, we have identified popular baseline models and datasets in each transportation domain, which facilitate the development and evaluation of future GNN-based frameworks.",
    "intriguing_abstract": "Graph neural networks (GNNs) have been extensively used in a wide variety of domains in recent years. Owing to their power in analyzing graph-structured data, they have become broadly popular in intelligent transportation systems (ITS) applications as well. Despite their widespread applications in different transportation domains, there is no comprehensive review of recent advancements and future research directions that covers all transportation areas. Accordingly, in this survey, for the first time, we provide an overview of GNN studies in the general domain of ITS. Unlike previous surveys, which have been limited to traffic forecasting problems, we explore how GNN frameworks have evolved for different ITS applications, including traffic forecasting, demand prediction, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety. Also, we micro-categorize the studies based on their transportation application to identify domain-specific research directions, opportunities, and challenges, which have been missing in previous surveys. Moreover, we identify unique and undiscussed research opportunities and directions, which is the result of reviewing a wide range of transportation applications. The neglected role of edge and graph learning in ITS applications, developing multi-modal models, and exploiting the power of unsupervised and reinforcement learning methods for developing more powerful GNNs are some examples of such new discussions in this survey. Finally, we have identified popular baseline models and datasets in each transportation domain, which facilitate the development and evaluation of future GNN-based frameworks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf",
    "citation_key": "rahmani2023kh4",
    "metadata": {
      "title": "Graph Neural Networks for Intelligent Transportation Systems: A Survey",
      "authors": [
        "Saeed Rahmani",
        "Asiye Baghbani",
        "N. Bouguila",
        "Z. Patterson"
      ],
      "published_date": "2023",
      "abstract": "Graph neural networks (GNNs) have been extensively used in a wide variety of domains in recent years. Owing to their power in analyzing graph-structured data, they have become broadly popular in intelligent transportation systems (ITS) applications as well. Despite their widespread applications in different transportation domains, there is no comprehensive review of recent advancements and future research directions that covers all transportation areas. Accordingly, in this survey, for the first time, we provide an overview of GNN studies in the general domain of ITS. Unlike previous surveys, which have been limited to traffic forecasting problems, we explore how GNN frameworks have evolved for different ITS applications, including traffic forecasting, demand prediction, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety. Also, we micro-categorize the studies based on their transportation application to identify domain-specific research directions, opportunities, and challenges, which have been missing in previous surveys. Moreover, we identify unique and undiscussed research opportunities and directions, which is the result of reviewing a wide range of transportation applications. The neglected role of edge and graph learning in ITS applications, developing multi-modal models, and exploiting the power of unsupervised and reinforcement learning methods for developing more powerful GNNs are some examples of such new discussions in this survey. Finally, we have identified popular baseline models and datasets in each transportation domain, which facilitate the development and evaluation of future GNN-based frameworks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf",
      "venue": "IEEE transactions on intelligent transportation systems (Print)",
      "citationCount": 154,
      "score": 77.0,
      "summary": "Graph neural networks (GNNs) have been extensively used in a wide variety of domains in recent years. Owing to their power in analyzing graph-structured data, they have become broadly popular in intelligent transportation systems (ITS) applications as well. Despite their widespread applications in different transportation domains, there is no comprehensive review of recent advancements and future research directions that covers all transportation areas. Accordingly, in this survey, for the first time, we provide an overview of GNN studies in the general domain of ITS. Unlike previous surveys, which have been limited to traffic forecasting problems, we explore how GNN frameworks have evolved for different ITS applications, including traffic forecasting, demand prediction, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety. Also, we micro-categorize the studies based on their transportation application to identify domain-specific research directions, opportunities, and challenges, which have been missing in previous surveys. Moreover, we identify unique and undiscussed research opportunities and directions, which is the result of reviewing a wide range of transportation applications. The neglected role of edge and graph learning in ITS applications, developing multi-modal models, and exploiting the power of unsupervised and reinforcement learning methods for developing more powerful GNNs are some examples of such new discussions in this survey. Finally, we have identified popular baseline models and datasets in each transportation domain, which facilitate the development and evaluation of future GNN-based frameworks.",
      "keywords": []
    },
    "file_name": "116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf"
  },
  {
    "success": true,
    "doc_id": "3ded9d7bab4984b5f01fbcfd330852f1",
    "summary": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation grap\" and introduce a revolutionizing MAcro Recommendation Graph (MAG) for billion-scale recommendations to reduce the neighbor count from billions to hundreds in the graph structure infrastructure. Specifically, We group micro nodes (users and items) with similar behavior patterns to form macro nodes and then MAG directly describes the relation between the user/item and the hundred of macro nodes rather than the billions of micro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
    "intriguing_abstract": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation grap\" and introduce a revolutionizing MAcro Recommendation Graph (MAG) for billion-scale recommendations to reduce the neighbor count from billions to hundreds in the graph structure infrastructure. Specifically, We group micro nodes (users and items) with similar behavior patterns to form macro nodes and then MAG directly describes the relation between the user/item and the hundred of macro nodes rather than the billions of micro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf",
    "citation_key": "chen2024gbe",
    "metadata": {
      "title": "Macro Graph Neural Networks for Online Billion-Scale Recommender Systems",
      "authors": [
        "Hao Chen",
        "Yuan-Qi Bei",
        "Qijie Shen",
        "Yue Xu",
        "Sheng Zhou",
        "Wenbing Huang",
        "Feiran Huang",
        "Senzhang Wang",
        "Xiao Huang"
      ],
      "published_date": "2024",
      "abstract": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation grap\" and introduce a revolutionizing MAcro Recommendation Graph (MAG) for billion-scale recommendations to reduce the neighbor count from billions to hundreds in the graph structure infrastructure. Specifically, We group micro nodes (users and items) with similar behavior patterns to form macro nodes and then MAG directly describes the relation between the user/item and the hundred of macro nodes rather than the billions of micro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
      "file_path": "paper_data/Graph_Neural_Networks/info/6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf",
      "venue": "The Web Conference",
      "citationCount": 66,
      "score": 66.0,
      "summary": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation grap\" and introduce a revolutionizing MAcro Recommendation Graph (MAG) for billion-scale recommendations to reduce the neighbor count from billions to hundreds in the graph structure infrastructure. Specifically, We group micro nodes (users and items) with similar behavior patterns to form macro nodes and then MAG directly describes the relation between the user/item and the hundred of macro nodes rather than the billions of micro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
      "keywords": []
    },
    "file_name": "6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf"
  },
  {
    "success": true,
    "doc_id": "ce647be0cf1b658df2a8fced4ad65bfd",
    "summary": "Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",
    "intriguing_abstract": "Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf",
    "citation_key": "liao202120x",
    "metadata": {
      "title": "A Review of Graph Neural Networks and Their Applications in Power Systems",
      "authors": [
        "Wenlong Liao",
        "B. Bak‐Jensen",
        "J. Pillai",
        "Yuelong Wang",
        "Yusen Wang"
      ],
      "published_date": "2021",
      "abstract": "Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf",
      "venue": "Journal of Modern Power Systems and Clean Energy",
      "citationCount": 252,
      "score": 63.0,
      "summary": "Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",
      "keywords": []
    },
    "file_name": "2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf"
  },
  {
    "success": true,
    "doc_id": "bb0650c2ed1e4ac552b11efd9a052ad4",
    "summary": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.",
    "intriguing_abstract": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf",
    "citation_key": "hin2022g19",
    "metadata": {
      "title": "LineVD: Statement-level Vulnerability Detection using Graph Neural Networks",
      "authors": [
        "David Hin",
        "Andrey Kan",
        "Huaming Chen",
        "M. Babar"
      ],
      "published_date": "2022",
      "abstract": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.",
      "file_path": "paper_data/Graph_Neural_Networks/info/94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf",
      "venue": "IEEE Working Conference on Mining Software Repositories",
      "citationCount": 187,
      "score": 62.33333333333333,
      "summary": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.",
      "keywords": []
    },
    "file_name": "94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf"
  },
  {
    "success": true,
    "doc_id": "92d2fa0f03ea45a5d5b03ad2713a0fe8",
    "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf",
    "citation_key": "tsitsulin20209pl",
    "metadata": {
      "title": "Graph Clustering with Graph Neural Networks",
      "authors": [
        "Anton Tsitsulin",
        "John Palowitch",
        "Bryan Perozzi",
        "Emmanuel Müller"
      ],
      "published_date": "2020",
      "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.",
      "file_path": "paper_data/Graph_Neural_Networks/info/81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf",
      "venue": "Journal of machine learning research",
      "citationCount": 309,
      "score": 61.800000000000004,
      "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.",
      "keywords": []
    },
    "file_name": "81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf"
  },
  {
    "success": true,
    "doc_id": "14e1302a7f552e3f5da785b0f9c13d8a",
    "summary": "Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.",
    "intriguing_abstract": "Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/d08167fd8583b0f70ba8a26821c29ea8af420826.pdf",
    "citation_key": "fung20212kw",
    "metadata": {
      "title": "Benchmarking graph neural networks for materials chemistry",
      "authors": [
        "Victor Fung",
        "Jiaxin Zhang",
        "Eric Juarez",
        "B. Sumpter"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.",
      "file_path": "paper_data/Graph_Neural_Networks/info/d08167fd8583b0f70ba8a26821c29ea8af420826.pdf",
      "venue": "npj Computational Materials",
      "citationCount": 246,
      "score": 61.5,
      "summary": "Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.",
      "keywords": []
    },
    "file_name": "d08167fd8583b0f70ba8a26821c29ea8af420826.pdf"
  },
  {
    "success": true,
    "doc_id": "53f9a6563e0bafcf9c1a28f472dfc93a",
    "summary": "Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks.",
    "intriguing_abstract": "Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e243c89ac61aae7330792c6c3f8791f07f40d031.pdf",
    "citation_key": "wang2021mxw",
    "metadata": {
      "title": "Joint Object Detection and Multi-Object Tracking with Graph Neural Networks",
      "authors": [
        "Yongxin Wang",
        "Kris Kitani",
        "Xinshuo Weng"
      ],
      "published_date": "2021",
      "abstract": "Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e243c89ac61aae7330792c6c3f8791f07f40d031.pdf",
      "venue": "IEEE International Conference on Robotics and Automation",
      "citationCount": 240,
      "score": 60.0,
      "summary": "Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks.",
      "keywords": []
    },
    "file_name": "e243c89ac61aae7330792c6c3f8791f07f40d031.pdf"
  },
  {
    "success": true,
    "doc_id": "89e804f87f0611e9e3a35ff5e51802ed",
    "summary": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
    "intriguing_abstract": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf",
    "citation_key": "wang2020nbg",
    "metadata": {
      "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
      "authors": [
        "Danqing Wang",
        "Pengfei Liu",
        "Y. Zheng",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "published_date": "2020",
      "abstract": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "citationCount": 283,
      "score": 56.6,
      "summary": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
      "keywords": []
    },
    "file_name": "3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf"
  },
  {
    "success": true,
    "doc_id": "46156e59ed4cf527970b480a0482495c",
    "summary": "Proteins are the essential biological macromolecules required to perform nearly all biological processes, and cellular functions. Proteins rarely carry out their tasks in isolation but interact with other proteins (known as protein–protein interaction) present in their surroundings to complete biological activities. The knowledge of protein–protein interactions (PPIs) unravels the cellular behavior and its functionality. The computational methods automate the prediction of PPI and are less expensive than experimental methods in terms of resources and time. So far, most of the works on PPI have mainly focused on sequence information. Here, we use graph convolutional network (GCN) and graph attention network (GAT) to predict the interaction between proteins by utilizing protein’s structural information and sequence features. We build the graphs of proteins from their PDB files, which contain 3D coordinates of atoms. The protein graph represents the amino acid network, also known as residue contact network, where each node is a residue. Two nodes are connected if they have a pair of atoms (one from each node) within the threshold distance. To extract the node/residue features, we use the protein language model. The input to the language model is the protein sequence, and the output is the feature vector for each amino acid of the underlying sequence. We validate the predictive capability of the proposed graph-based approach on two PPI datasets: Human and S. cerevisiae. Obtained results demonstrate the effectiveness of the proposed approach as it outperforms the previous leading methods. The source code for training and data to train the model are available at https://github.com/JhaKanchan15/PPI_GNN.git.",
    "intriguing_abstract": "Proteins are the essential biological macromolecules required to perform nearly all biological processes, and cellular functions. Proteins rarely carry out their tasks in isolation but interact with other proteins (known as protein–protein interaction) present in their surroundings to complete biological activities. The knowledge of protein–protein interactions (PPIs) unravels the cellular behavior and its functionality. The computational methods automate the prediction of PPI and are less expensive than experimental methods in terms of resources and time. So far, most of the works on PPI have mainly focused on sequence information. Here, we use graph convolutional network (GCN) and graph attention network (GAT) to predict the interaction between proteins by utilizing protein’s structural information and sequence features. We build the graphs of proteins from their PDB files, which contain 3D coordinates of atoms. The protein graph represents the amino acid network, also known as residue contact network, where each node is a residue. Two nodes are connected if they have a pair of atoms (one from each node) within the threshold distance. To extract the node/residue features, we use the protein language model. The input to the language model is the protein sequence, and the output is the feature vector for each amino acid of the underlying sequence. We validate the predictive capability of the proposed graph-based approach on two PPI datasets: Human and S. cerevisiae. Obtained results demonstrate the effectiveness of the proposed approach as it outperforms the previous leading methods. The source code for training and data to train the model are available at https://github.com/JhaKanchan15/PPI_GNN.git.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf",
    "citation_key": "jha2022cj8",
    "metadata": {
      "title": "Prediction of protein–protein interaction using graph neural networks",
      "authors": [
        "Kanchan Jha",
        "S. Saha",
        "Hiteshi Singh"
      ],
      "published_date": "2022",
      "abstract": "Proteins are the essential biological macromolecules required to perform nearly all biological processes, and cellular functions. Proteins rarely carry out their tasks in isolation but interact with other proteins (known as protein–protein interaction) present in their surroundings to complete biological activities. The knowledge of protein–protein interactions (PPIs) unravels the cellular behavior and its functionality. The computational methods automate the prediction of PPI and are less expensive than experimental methods in terms of resources and time. So far, most of the works on PPI have mainly focused on sequence information. Here, we use graph convolutional network (GCN) and graph attention network (GAT) to predict the interaction between proteins by utilizing protein’s structural information and sequence features. We build the graphs of proteins from their PDB files, which contain 3D coordinates of atoms. The protein graph represents the amino acid network, also known as residue contact network, where each node is a residue. Two nodes are connected if they have a pair of atoms (one from each node) within the threshold distance. To extract the node/residue features, we use the protein language model. The input to the language model is the protein sequence, and the output is the feature vector for each amino acid of the underlying sequence. We validate the predictive capability of the proposed graph-based approach on two PPI datasets: Human and S. cerevisiae. Obtained results demonstrate the effectiveness of the proposed approach as it outperforms the previous leading methods. The source code for training and data to train the model are available at https://github.com/JhaKanchan15/PPI_GNN.git.",
      "file_path": "paper_data/Graph_Neural_Networks/info/fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf",
      "venue": "Scientific Reports",
      "citationCount": 164,
      "score": 54.666666666666664,
      "summary": "Proteins are the essential biological macromolecules required to perform nearly all biological processes, and cellular functions. Proteins rarely carry out their tasks in isolation but interact with other proteins (known as protein–protein interaction) present in their surroundings to complete biological activities. The knowledge of protein–protein interactions (PPIs) unravels the cellular behavior and its functionality. The computational methods automate the prediction of PPI and are less expensive than experimental methods in terms of resources and time. So far, most of the works on PPI have mainly focused on sequence information. Here, we use graph convolutional network (GCN) and graph attention network (GAT) to predict the interaction between proteins by utilizing protein’s structural information and sequence features. We build the graphs of proteins from their PDB files, which contain 3D coordinates of atoms. The protein graph represents the amino acid network, also known as residue contact network, where each node is a residue. Two nodes are connected if they have a pair of atoms (one from each node) within the threshold distance. To extract the node/residue features, we use the protein language model. The input to the language model is the protein sequence, and the output is the feature vector for each amino acid of the underlying sequence. We validate the predictive capability of the proposed graph-based approach on two PPI datasets: Human and S. cerevisiae. Obtained results demonstrate the effectiveness of the proposed approach as it outperforms the previous leading methods. The source code for training and data to train the model are available at https://github.com/JhaKanchan15/PPI_GNN.git.",
      "keywords": []
    },
    "file_name": "fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf"
  },
  {
    "success": true,
    "doc_id": "077a85ffbc0d5184f96a5cd3ceea3f5b",
    "summary": "Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables. Combinatorial optimization, the search for the minimum of an objective function within a finite but very large set of candidate solutions, finds many important and challenging applications in science and industry. A new graph neural network deep learning approach that incorporates concepts from statistical physics is used to develop a robust solver that can tackle a large class of NP-hard combinatorial optimization problems.",
    "intriguing_abstract": "Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables. Combinatorial optimization, the search for the minimum of an objective function within a finite but very large set of candidate solutions, finds many important and challenging applications in science and industry. A new graph neural network deep learning approach that incorporates concepts from statistical physics is used to develop a robust solver that can tackle a large class of NP-hard combinatorial optimization problems.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf",
    "citation_key": "schuetz2021cod",
    "metadata": {
      "title": "Combinatorial optimization with physics-inspired graph neural networks",
      "authors": [
        "M. Schuetz",
        "J. K. Brubaker",
        "H. Katzgraber"
      ],
      "published_date": "2021",
      "abstract": "Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables. Combinatorial optimization, the search for the minimum of an objective function within a finite but very large set of candidate solutions, finds many important and challenging applications in science and industry. A new graph neural network deep learning approach that incorporates concepts from statistical physics is used to develop a robust solver that can tackle a large class of NP-hard combinatorial optimization problems.",
      "file_path": "paper_data/Graph_Neural_Networks/info/84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf",
      "venue": "Nature Machine Intelligence",
      "citationCount": 212,
      "score": 53.0,
      "summary": "Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables. Combinatorial optimization, the search for the minimum of an objective function within a finite but very large set of candidate solutions, finds many important and challenging applications in science and industry. A new graph neural network deep learning approach that incorporates concepts from statistical physics is used to develop a robust solver that can tackle a large class of NP-hard combinatorial optimization problems.",
      "keywords": []
    },
    "file_name": "84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf"
  },
  {
    "success": true,
    "doc_id": "33f6099551ed3443c82444ebb98b7745",
    "summary": "Decentralized Applications (DApps) are increasingly developed and deployed on blockchain platforms such as Ethereum. DApp fingerprinting can identify users’ visits to specific DApps by analyzing the resulting network traffic, revealing much sensitive information about the users, such as their real identities, financial conditions and religious or political preferences. DApps deployed on the same platform usually adopt the same communication interface and similar traffic encryption settings, making the resulting traffic less discriminative. Existing encrypted traffic classification methods either require hand-crafted and fine-tuning features or suffer from low accuracy. It remains a challenging task to conduct DApp fingerprinting in an accurate and efficient way. In this paper, we present GraphDApp, a novel DApp fingerprinting method using Graph Neural Networks (GNNs). We propose a graph structure named Traffic Interaction Graph (TIG) as an information-rich representation of encrypted DApp flows, which implicitly reserves multiple dimensional features in bidirectional client-server interactions. Using TIG, we turn DApp fingerprinting into a graph classification problem and design a powerful GNN-based classifier. We collect real-world traffic datasets from 1,300 DApps with more than 169,000 flows. The experimental results show that GraphDApp is superior to the other state-of-the-art methods in terms of classification accuracy in both closed- and open-world scenarios. In addition, GraphDApp maintains its high accuracy when being applied to the traditional mobile application classification.",
    "intriguing_abstract": "Decentralized Applications (DApps) are increasingly developed and deployed on blockchain platforms such as Ethereum. DApp fingerprinting can identify users’ visits to specific DApps by analyzing the resulting network traffic, revealing much sensitive information about the users, such as their real identities, financial conditions and religious or political preferences. DApps deployed on the same platform usually adopt the same communication interface and similar traffic encryption settings, making the resulting traffic less discriminative. Existing encrypted traffic classification methods either require hand-crafted and fine-tuning features or suffer from low accuracy. It remains a challenging task to conduct DApp fingerprinting in an accurate and efficient way. In this paper, we present GraphDApp, a novel DApp fingerprinting method using Graph Neural Networks (GNNs). We propose a graph structure named Traffic Interaction Graph (TIG) as an information-rich representation of encrypted DApp flows, which implicitly reserves multiple dimensional features in bidirectional client-server interactions. Using TIG, we turn DApp fingerprinting into a graph classification problem and design a powerful GNN-based classifier. We collect real-world traffic datasets from 1,300 DApps with more than 169,000 flows. The experimental results show that GraphDApp is superior to the other state-of-the-art methods in terms of classification accuracy in both closed- and open-world scenarios. In addition, GraphDApp maintains its high accuracy when being applied to the traditional mobile application classification.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf",
    "citation_key": "shen2021sbk",
    "metadata": {
      "title": "Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks",
      "authors": [
        "Meng Shen",
        "Jinpeng Zhang",
        "Liehuang Zhu",
        "Ke Xu",
        "Xiaojiang Du"
      ],
      "published_date": "2021",
      "abstract": "Decentralized Applications (DApps) are increasingly developed and deployed on blockchain platforms such as Ethereum. DApp fingerprinting can identify users’ visits to specific DApps by analyzing the resulting network traffic, revealing much sensitive information about the users, such as their real identities, financial conditions and religious or political preferences. DApps deployed on the same platform usually adopt the same communication interface and similar traffic encryption settings, making the resulting traffic less discriminative. Existing encrypted traffic classification methods either require hand-crafted and fine-tuning features or suffer from low accuracy. It remains a challenging task to conduct DApp fingerprinting in an accurate and efficient way. In this paper, we present GraphDApp, a novel DApp fingerprinting method using Graph Neural Networks (GNNs). We propose a graph structure named Traffic Interaction Graph (TIG) as an information-rich representation of encrypted DApp flows, which implicitly reserves multiple dimensional features in bidirectional client-server interactions. Using TIG, we turn DApp fingerprinting into a graph classification problem and design a powerful GNN-based classifier. We collect real-world traffic datasets from 1,300 DApps with more than 169,000 flows. The experimental results show that GraphDApp is superior to the other state-of-the-art methods in terms of classification accuracy in both closed- and open-world scenarios. In addition, GraphDApp maintains its high accuracy when being applied to the traditional mobile application classification.",
      "file_path": "paper_data/Graph_Neural_Networks/info/94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "citationCount": 211,
      "score": 52.75,
      "summary": "Decentralized Applications (DApps) are increasingly developed and deployed on blockchain platforms such as Ethereum. DApp fingerprinting can identify users’ visits to specific DApps by analyzing the resulting network traffic, revealing much sensitive information about the users, such as their real identities, financial conditions and religious or political preferences. DApps deployed on the same platform usually adopt the same communication interface and similar traffic encryption settings, making the resulting traffic less discriminative. Existing encrypted traffic classification methods either require hand-crafted and fine-tuning features or suffer from low accuracy. It remains a challenging task to conduct DApp fingerprinting in an accurate and efficient way. In this paper, we present GraphDApp, a novel DApp fingerprinting method using Graph Neural Networks (GNNs). We propose a graph structure named Traffic Interaction Graph (TIG) as an information-rich representation of encrypted DApp flows, which implicitly reserves multiple dimensional features in bidirectional client-server interactions. Using TIG, we turn DApp fingerprinting into a graph classification problem and design a powerful GNN-based classifier. We collect real-world traffic datasets from 1,300 DApps with more than 169,000 flows. The experimental results show that GraphDApp is superior to the other state-of-the-art methods in terms of classification accuracy in both closed- and open-world scenarios. In addition, GraphDApp maintains its high accuracy when being applied to the traditional mobile application classification.",
      "keywords": []
    },
    "file_name": "94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf"
  },
  {
    "success": true,
    "doc_id": "9008bd1e6e4934a87764aa1c4a268100",
    "summary": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",
    "intriguing_abstract": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c193011099906126fe7b6cfcb04062cf4591ccf9.pdf",
    "citation_key": "bo2023rwt",
    "metadata": {
      "title": "Specformer: Spectral Graph Neural Networks Meet Transformers",
      "authors": [
        "Deyu Bo",
        "Chuan Shi",
        "Lele Wang",
        "Renjie Liao"
      ],
      "published_date": "2023",
      "abstract": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c193011099906126fe7b6cfcb04062cf4591ccf9.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 102,
      "score": 51.0,
      "summary": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",
      "keywords": []
    },
    "file_name": "c193011099906126fe7b6cfcb04062cf4591ccf9.pdf"
  },
  {
    "success": true,
    "doc_id": "335ef9b8a97e4fb1cdb00e7ae7789a7f",
    "summary": "Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user's preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.",
    "intriguing_abstract": "Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user's preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf",
    "citation_key": "zhang20212ke",
    "metadata": {
      "title": "Dynamic Graph Neural Networks for Sequential Recommendation",
      "authors": [
        "Mengqi Zhang",
        "Shu Wu",
        "Xueli Yu",
        "Liang Wang"
      ],
      "published_date": "2021",
      "abstract": "Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user's preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.",
      "file_path": "paper_data/Graph_Neural_Networks/info/286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 202,
      "score": 50.5,
      "summary": "Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user's preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.",
      "keywords": []
    },
    "file_name": "286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf"
  },
  {
    "success": true,
    "doc_id": "edf1f0498a753d53aeb6fee517b205aa",
    "summary": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
    "intriguing_abstract": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c00673042d8cc539d903c4f30b55a71487f5c701.pdf",
    "citation_key": "wei20246l2",
    "metadata": {
      "title": "Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks",
      "authors": [
        "Jianjun Wei",
        "Yue Liu",
        "Xin Huang",
        "Xin Zhang",
        "Wenyi Liu",
        "Xu Yan"
      ],
      "published_date": "2024",
      "abstract": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c00673042d8cc539d903c4f30b55a71487f5c701.pdf",
      "venue": "2024 5th International Conference on Machine Learning and Computer Application (ICMLCA)",
      "citationCount": 47,
      "score": 47.0,
      "summary": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
      "keywords": []
    },
    "file_name": "c00673042d8cc539d903c4f30b55a71487f5c701.pdf"
  },
  {
    "success": true,
    "doc_id": "252d4f5a1b2b30c0e987c3e1a861adf9",
    "summary": "Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
    "intriguing_abstract": "Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/aac77c36a9a5c24aa135538c32950096e59ba442.pdf",
    "citation_key": "yu2020u32",
    "metadata": {
      "title": "TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation",
      "authors": [
        "Feng Yu",
        "Yanqiao Zhu",
        "Q. Liu",
        "Shu Wu",
        "Liang Wang",
        "T. Tan"
      ],
      "published_date": "2020",
      "abstract": "Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/aac77c36a9a5c24aa135538c32950096e59ba442.pdf",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "citationCount": 234,
      "score": 46.800000000000004,
      "summary": "Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
      "keywords": []
    },
    "file_name": "aac77c36a9a5c24aa135538c32950096e59ba442.pdf"
  },
  {
    "success": true,
    "doc_id": "30b38f522c8afe55a636207721906dd1",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/86ad9d1dd6626921297a8456b048f4bccafe967c.pdf",
    "citation_key": "he2021x8v",
    "metadata": {
      "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks",
      "authors": [
        "Chaoyang He",
        "Keshav Balasubramanian",
        "Emir Ceyani",
        "Yu Rong",
        "P. Zhao",
        "Junzhou Huang",
        "M. Annavaram",
        "S. Avestimehr"
      ],
      "published_date": "2021",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/86ad9d1dd6626921297a8456b048f4bccafe967c.pdf",
      "venue": "arXiv.org",
      "citationCount": 183,
      "score": 45.75,
      "summary": "",
      "keywords": []
    },
    "file_name": "86ad9d1dd6626921297a8456b048f4bccafe967c.pdf"
  },
  {
    "success": true,
    "doc_id": "3d13c2e46e5e98c810ebe686f28c77ae",
    "summary": "The Industrial Internet of Things (IIoT) plays an important role in digital transformation of traditional industries toward Industry 4.0. By connecting sensors, instruments, and other industry devices to the Internet, IIoT facilitates the data collection, data analysis, and automated control, thereby improving the productivity and efficiency of the business as well as the resulting economic benefits. Due to the complex IIoT infrastructure, anomaly detection becomes an important tool to ensure the success of IIoT. Due to the nature of IIoT, graph-level anomaly detection has been a promising means to detect and predict anomalies in many different domains, such as transportation, energy, and factory, as well as for dynamically evolving networks. This article provides a useful investigation on graph neural networks (GNNs) for anomaly detection in IIoT-enabled smart transportation, smart energy, and smart factory. In addition to the GNN-empowered anomaly detection solutions on point, contextual, and collective types of anomalies, useful data sets, challenges, and open issues for each type of anomalies in the three identified industry sectors (i.e., smart transportation, smart energy, and smart factory) are also provided and discussed, which will be useful for future research in this area. To demonstrate the use of GNN in concrete scenarios, we show three case studies in smart transportation, smart energy, and smart factory, respectively.",
    "intriguing_abstract": "The Industrial Internet of Things (IIoT) plays an important role in digital transformation of traditional industries toward Industry 4.0. By connecting sensors, instruments, and other industry devices to the Internet, IIoT facilitates the data collection, data analysis, and automated control, thereby improving the productivity and efficiency of the business as well as the resulting economic benefits. Due to the complex IIoT infrastructure, anomaly detection becomes an important tool to ensure the success of IIoT. Due to the nature of IIoT, graph-level anomaly detection has been a promising means to detect and predict anomalies in many different domains, such as transportation, energy, and factory, as well as for dynamically evolving networks. This article provides a useful investigation on graph neural networks (GNNs) for anomaly detection in IIoT-enabled smart transportation, smart energy, and smart factory. In addition to the GNN-empowered anomaly detection solutions on point, contextual, and collective types of anomalies, useful data sets, challenges, and open issues for each type of anomalies in the three identified industry sectors (i.e., smart transportation, smart energy, and smart factory) are also provided and discussed, which will be useful for future research in this area. To demonstrate the use of GNN in concrete scenarios, we show three case studies in smart transportation, smart energy, and smart factory, respectively.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf",
    "citation_key": "wu20210h4",
    "metadata": {
      "title": "Graph Neural Networks for Anomaly Detection in Industrial Internet of Things",
      "authors": [
        "Yulei Wu",
        "Hongning Dai",
        "Haina Tang"
      ],
      "published_date": "2021",
      "abstract": "The Industrial Internet of Things (IIoT) plays an important role in digital transformation of traditional industries toward Industry 4.0. By connecting sensors, instruments, and other industry devices to the Internet, IIoT facilitates the data collection, data analysis, and automated control, thereby improving the productivity and efficiency of the business as well as the resulting economic benefits. Due to the complex IIoT infrastructure, anomaly detection becomes an important tool to ensure the success of IIoT. Due to the nature of IIoT, graph-level anomaly detection has been a promising means to detect and predict anomalies in many different domains, such as transportation, energy, and factory, as well as for dynamically evolving networks. This article provides a useful investigation on graph neural networks (GNNs) for anomaly detection in IIoT-enabled smart transportation, smart energy, and smart factory. In addition to the GNN-empowered anomaly detection solutions on point, contextual, and collective types of anomalies, useful data sets, challenges, and open issues for each type of anomalies in the three identified industry sectors (i.e., smart transportation, smart energy, and smart factory) are also provided and discussed, which will be useful for future research in this area. To demonstrate the use of GNN in concrete scenarios, we show three case studies in smart transportation, smart energy, and smart factory, respectively.",
      "file_path": "paper_data/Graph_Neural_Networks/info/d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf",
      "venue": "IEEE Internet of Things Journal",
      "citationCount": 182,
      "score": 45.5,
      "summary": "The Industrial Internet of Things (IIoT) plays an important role in digital transformation of traditional industries toward Industry 4.0. By connecting sensors, instruments, and other industry devices to the Internet, IIoT facilitates the data collection, data analysis, and automated control, thereby improving the productivity and efficiency of the business as well as the resulting economic benefits. Due to the complex IIoT infrastructure, anomaly detection becomes an important tool to ensure the success of IIoT. Due to the nature of IIoT, graph-level anomaly detection has been a promising means to detect and predict anomalies in many different domains, such as transportation, energy, and factory, as well as for dynamically evolving networks. This article provides a useful investigation on graph neural networks (GNNs) for anomaly detection in IIoT-enabled smart transportation, smart energy, and smart factory. In addition to the GNN-empowered anomaly detection solutions on point, contextual, and collective types of anomalies, useful data sets, challenges, and open issues for each type of anomalies in the three identified industry sectors (i.e., smart transportation, smart energy, and smart factory) are also provided and discussed, which will be useful for future research in this area. To demonstrate the use of GNN in concrete scenarios, we show three case studies in smart transportation, smart energy, and smart factory, respectively.",
      "keywords": []
    },
    "file_name": "d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf"
  },
  {
    "success": true,
    "doc_id": "fd7e06c15627dddf007498ff3513bf9c",
    "summary": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
    "intriguing_abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/fc580c211689663a64f42e2ba92c864cb134ba9b.pdf",
    "citation_key": "kofinas2024t2b",
    "metadata": {
      "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
      "authors": [
        "Miltiadis Kofinas",
        "Boris Knyazev",
        "Yan Zhang",
        "Yunlu Chen",
        "G. Burghouts",
        "E. Gavves",
        "Cees G. M. Snoek",
        "David W. Zhang"
      ],
      "published_date": "2024",
      "abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/fc580c211689663a64f42e2ba92c864cb134ba9b.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 45,
      "score": 45.0,
      "summary": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
      "keywords": []
    },
    "file_name": "fc580c211689663a64f42e2ba92c864cb134ba9b.pdf"
  },
  {
    "success": true,
    "doc_id": "a5cef3ce07ad8b431a094a909eb0c65b",
    "summary": "Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the biomolecular structural information is not fully utilized. The essential long-range interactions among atoms are also neglected in GNN models. To this end, we propose a structure-aware interactive graph neural network (SIGN) which consists of two components: polar-inspired graph attention layers (PGAL) and pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs the node-edge aggregation process to update embeddings of nodes and edges while preserving the distance and angle information among atoms. Then, PiPool is adopted to gather interactive edges with a subsequent reconstruction loss to reflect the global interactions. Exhaustive experimental study on two benchmarks verifies the superiority of SIGN.",
    "intriguing_abstract": "Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the biomolecular structural information is not fully utilized. The essential long-range interactions among atoms are also neglected in GNN models. To this end, we propose a structure-aware interactive graph neural network (SIGN) which consists of two components: polar-inspired graph attention layers (PGAL) and pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs the node-edge aggregation process to update embeddings of nodes and edges while preserving the distance and angle information among atoms. Then, PiPool is adopted to gather interactive edges with a subsequent reconstruction loss to reflect the global interactions. Exhaustive experimental study on two benchmarks verifies the superiority of SIGN.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ef41b29312860bc284640e35ab499053f4966bbf.pdf",
    "citation_key": "li2021v1l",
    "metadata": {
      "title": "Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity",
      "authors": [
        "Shuangli Li",
        "Jingbo Zhou",
        "Tong Xu",
        "Liang Huang",
        "Fan Wang",
        "Haoyi Xiong",
        "Weili Huang",
        "D. Dou",
        "Hui Xiong"
      ],
      "published_date": "2021",
      "abstract": "Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the biomolecular structural information is not fully utilized. The essential long-range interactions among atoms are also neglected in GNN models. To this end, we propose a structure-aware interactive graph neural network (SIGN) which consists of two components: polar-inspired graph attention layers (PGAL) and pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs the node-edge aggregation process to update embeddings of nodes and edges while preserving the distance and angle information among atoms. Then, PiPool is adopted to gather interactive edges with a subsequent reconstruction loss to reflect the global interactions. Exhaustive experimental study on two benchmarks verifies the superiority of SIGN.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ef41b29312860bc284640e35ab499053f4966bbf.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 179,
      "score": 44.75,
      "summary": "Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the biomolecular structural information is not fully utilized. The essential long-range interactions among atoms are also neglected in GNN models. To this end, we propose a structure-aware interactive graph neural network (SIGN) which consists of two components: polar-inspired graph attention layers (PGAL) and pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs the node-edge aggregation process to update embeddings of nodes and edges while preserving the distance and angle information among atoms. Then, PiPool is adopted to gather interactive edges with a subsequent reconstruction loss to reflect the global interactions. Exhaustive experimental study on two benchmarks verifies the superiority of SIGN.",
      "keywords": []
    },
    "file_name": "ef41b29312860bc284640e35ab499053f4966bbf.pdf"
  },
  {
    "success": true,
    "doc_id": "e0e510b683c292c0ad0238bf69185b58",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf",
    "citation_key": "balcilar2021di1",
    "metadata": {
      "title": "Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective",
      "authors": [
        "M. Balcilar",
        "G. Renton",
        "P. Héroux",
        "Benoit Gaüzère",
        "Sébastien Adam",
        "P. Honeine"
      ],
      "published_date": "2021",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 173,
      "score": 43.25,
      "summary": "",
      "keywords": []
    },
    "file_name": "064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf"
  },
  {
    "success": true,
    "doc_id": "0b2acfef74fd1913ac66fdb6b687ba5b",
    "summary": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
    "intriguing_abstract": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf",
    "citation_key": "zhang2020f4l",
    "metadata": {
      "title": "Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning",
      "authors": [
        "Muhan Zhang",
        "Pan Li",
        "Yinglong Xia",
        "Kai Wang",
        "Long Jin"
      ],
      "published_date": "2020",
      "abstract": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
      "file_path": "paper_data/Graph_Neural_Networks/info/99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 216,
      "score": 43.2,
      "summary": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
      "keywords": []
    },
    "file_name": "99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf"
  },
  {
    "success": true,
    "doc_id": "a5217538f149155770914afceb34bfdb",
    "summary": "Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",
    "intriguing_abstract": "Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/dc5e841197165c3c38940cc9f607fce7b09116cf.pdf",
    "citation_key": "bilot20234ui",
    "metadata": {
      "title": "Graph Neural Networks for Intrusion Detection: A Survey",
      "authors": [
        "Tristan Bilot",
        "Nour El Madhoun",
        "K. A. Agha",
        "Anis Zouaoui"
      ],
      "published_date": "2023",
      "abstract": "Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",
      "file_path": "paper_data/Graph_Neural_Networks/info/dc5e841197165c3c38940cc9f607fce7b09116cf.pdf",
      "venue": "IEEE Access",
      "citationCount": 83,
      "score": 41.5,
      "summary": "Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",
      "keywords": []
    },
    "file_name": "dc5e841197165c3c38940cc9f607fce7b09116cf.pdf"
  },
  {
    "success": true,
    "doc_id": "80991033adc4d984a22cfa74f57e87ff",
    "summary": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
    "intriguing_abstract": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf",
    "citation_key": "wu2023303",
    "metadata": {
      "title": "Energy-based Out-of-Distribution Detection for Graph Neural Networks",
      "authors": [
        "Qitian Wu",
        "Yiting Chen",
        "Chenxiao Yang",
        "Junchi Yan"
      ],
      "published_date": "2023",
      "abstract": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 78,
      "score": 39.0,
      "summary": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
      "keywords": []
    },
    "file_name": "0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf"
  },
  {
    "success": true,
    "doc_id": "0bfddb80b63333b0ba85b39f6805d331",
    "summary": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
    "intriguing_abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5f1913828e30c3070f32c154d2d142ec17e91189.pdf",
    "citation_key": "huang2021lpu",
    "metadata": {
      "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation",
      "authors": [
        "P. Huang",
        "Han-Hung Lee",
        "Hwann-Tzong Chen",
        "Tyng-Luh Liu"
      ],
      "published_date": "2021",
      "abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5f1913828e30c3070f32c154d2d142ec17e91189.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 155,
      "score": 38.75,
      "summary": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
      "keywords": []
    },
    "file_name": "5f1913828e30c3070f32c154d2d142ec17e91189.pdf"
  },
  {
    "success": true,
    "doc_id": "02f04803be506fba8e86ba4be2c46fbf",
    "summary": "Graph neural networks (GNNs) have achieved tremendous success on multiple graph-based learning tasks by fusing network structure and node features. Modern GNN models are built upon iterative aggregation of neighbor's/proximity features by message passing. Its prediction performance has been shown to be strongly bounded by assortative mixing in the graph, a key property wherein nodes with similar attributes mix/connect with each other. We observe that real world networks exhibit heterogeneous or diverse mixing patterns and the conventional global measurement of assortativity, such as global assortativity coefficient, may not be a representative statistic in quantifying this mixing. We adopt a generalized concept, node-level assortativity, one that is based at the node level to better represent the diverse patterns and accurately quantify the learnability of GNNs. We find that the prediction performance of a wide range of GNN models is highly correlated with the node level assortativity. To break this limit, in this work, we focus on transforming the input graph into a computation graph which contains both proximity and structural information as distinct type of edges. The resulted multi-relational graph has an enhanced level of assortativity and, more importantly, preserves rich information from the original graph. We then propose to run GNNs on this computation graph and show that adaptively choosing between structure and proximity leads to improved performance under diverse mixing. Empirically, we show the benefits of adopting our transformation framework for semi-supervised node classification task on a variety of real world graph learning benchmarks.",
    "intriguing_abstract": "Graph neural networks (GNNs) have achieved tremendous success on multiple graph-based learning tasks by fusing network structure and node features. Modern GNN models are built upon iterative aggregation of neighbor's/proximity features by message passing. Its prediction performance has been shown to be strongly bounded by assortative mixing in the graph, a key property wherein nodes with similar attributes mix/connect with each other. We observe that real world networks exhibit heterogeneous or diverse mixing patterns and the conventional global measurement of assortativity, such as global assortativity coefficient, may not be a representative statistic in quantifying this mixing. We adopt a generalized concept, node-level assortativity, one that is based at the node level to better represent the diverse patterns and accurately quantify the learnability of GNNs. We find that the prediction performance of a wide range of GNN models is highly correlated with the node level assortativity. To break this limit, in this work, we focus on transforming the input graph into a computation graph which contains both proximity and structural information as distinct type of edges. The resulted multi-relational graph has an enhanced level of assortativity and, more importantly, preserves rich information from the original graph. We then propose to run GNNs on this computation graph and show that adaptively choosing between structure and proximity leads to improved performance under diverse mixing. Empirically, we show the benefits of adopting our transformation framework for semi-supervised node classification task on a variety of real world graph learning benchmarks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/4bc7d63595d194a6e0930019764216e6b42da0d4.pdf",
    "citation_key": "suresh202191q",
    "metadata": {
      "title": "Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns",
      "authors": [
        "Susheel Suresh",
        "Vinith Budde",
        "Jennifer Neville",
        "Pan Li",
        "Jianzhu Ma"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks (GNNs) have achieved tremendous success on multiple graph-based learning tasks by fusing network structure and node features. Modern GNN models are built upon iterative aggregation of neighbor's/proximity features by message passing. Its prediction performance has been shown to be strongly bounded by assortative mixing in the graph, a key property wherein nodes with similar attributes mix/connect with each other. We observe that real world networks exhibit heterogeneous or diverse mixing patterns and the conventional global measurement of assortativity, such as global assortativity coefficient, may not be a representative statistic in quantifying this mixing. We adopt a generalized concept, node-level assortativity, one that is based at the node level to better represent the diverse patterns and accurately quantify the learnability of GNNs. We find that the prediction performance of a wide range of GNN models is highly correlated with the node level assortativity. To break this limit, in this work, we focus on transforming the input graph into a computation graph which contains both proximity and structural information as distinct type of edges. The resulted multi-relational graph has an enhanced level of assortativity and, more importantly, preserves rich information from the original graph. We then propose to run GNNs on this computation graph and show that adaptively choosing between structure and proximity leads to improved performance under diverse mixing. Empirically, we show the benefits of adopting our transformation framework for semi-supervised node classification task on a variety of real world graph learning benchmarks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/4bc7d63595d194a6e0930019764216e6b42da0d4.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 147,
      "score": 36.75,
      "summary": "Graph neural networks (GNNs) have achieved tremendous success on multiple graph-based learning tasks by fusing network structure and node features. Modern GNN models are built upon iterative aggregation of neighbor's/proximity features by message passing. Its prediction performance has been shown to be strongly bounded by assortative mixing in the graph, a key property wherein nodes with similar attributes mix/connect with each other. We observe that real world networks exhibit heterogeneous or diverse mixing patterns and the conventional global measurement of assortativity, such as global assortativity coefficient, may not be a representative statistic in quantifying this mixing. We adopt a generalized concept, node-level assortativity, one that is based at the node level to better represent the diverse patterns and accurately quantify the learnability of GNNs. We find that the prediction performance of a wide range of GNN models is highly correlated with the node level assortativity. To break this limit, in this work, we focus on transforming the input graph into a computation graph which contains both proximity and structural information as distinct type of edges. The resulted multi-relational graph has an enhanced level of assortativity and, more importantly, preserves rich information from the original graph. We then propose to run GNNs on this computation graph and show that adaptively choosing between structure and proximity leads to improved performance under diverse mixing. Empirically, we show the benefits of adopting our transformation framework for semi-supervised node classification task on a variety of real world graph learning benchmarks.",
      "keywords": []
    },
    "file_name": "4bc7d63595d194a6e0930019764216e6b42da0d4.pdf"
  },
  {
    "success": true,
    "doc_id": "3337112691b3e25bf5b33a62e16b7863",
    "summary": "Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.",
    "intriguing_abstract": "Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/bda290f54791e4719917e44a7e6441d000c43ab3.pdf",
    "citation_key": "liu2022gcg",
    "metadata": {
      "title": "Federated Graph Neural Networks: Overview, Techniques, and Challenges",
      "authors": [
        "R. Liu",
        "Han Yu"
      ],
      "published_date": "2022",
      "abstract": "Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/bda290f54791e4719917e44a7e6441d000c43ab3.pdf",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "citationCount": 110,
      "score": 36.666666666666664,
      "summary": "Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.",
      "keywords": []
    },
    "file_name": "bda290f54791e4719917e44a7e6441d000c43ab3.pdf"
  },
  {
    "success": true,
    "doc_id": "c7a4db90fdcdf4c6e69800d54ae830ab",
    "summary": "Infectious disease forecasting has been a key focus in the recent past owing to the COVID-19 pandemic and has proved to be an important tool in controlling the pandemic. With the advent of reliable spatiotemporal data, graph neural network models have been able to successfully model the inter-relation between the cross-region signals to produce quality forecasts, but like most deep-learning models they do not explicitly incorporate the underlying causal mechanisms. In this work, we employ a causal mechanistic model to guide the learning of the graph embeddings and propose a novel learning framework -- Causal-based Graph Neural Network (CausalGNN) that learns spatiotemporal embedding in a latent space where graph input features and epidemiological context are combined via a mutually learning mechanism using graph-based non-linear transformations. We design an attention-based dynamic GNN module to capture spatial and temporal disease dynamics. A causal module is added to the framework to provide epidemiological context for node embedding via ordinary differential equations. Extensive experiments on forecasting daily new cases of COVID-19 at global, US state, and US county levels show that the proposed method outperforms a broad range of baselines. The learned model which incorporates epidemiological context organizes the embedding in an efficient way by keeping the parameter size small leading to robust and accurate forecasting performance across various datasets.",
    "intriguing_abstract": "Infectious disease forecasting has been a key focus in the recent past owing to the COVID-19 pandemic and has proved to be an important tool in controlling the pandemic. With the advent of reliable spatiotemporal data, graph neural network models have been able to successfully model the inter-relation between the cross-region signals to produce quality forecasts, but like most deep-learning models they do not explicitly incorporate the underlying causal mechanisms. In this work, we employ a causal mechanistic model to guide the learning of the graph embeddings and propose a novel learning framework -- Causal-based Graph Neural Network (CausalGNN) that learns spatiotemporal embedding in a latent space where graph input features and epidemiological context are combined via a mutually learning mechanism using graph-based non-linear transformations. We design an attention-based dynamic GNN module to capture spatial and temporal disease dynamics. A causal module is added to the framework to provide epidemiological context for node embedding via ordinary differential equations. Extensive experiments on forecasting daily new cases of COVID-19 at global, US state, and US county levels show that the proposed method outperforms a broad range of baselines. The learned model which incorporates epidemiological context organizes the embedding in an efficient way by keeping the parameter size small leading to robust and accurate forecasting performance across various datasets.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5b1978e8284c8514165938bff6e3276977088f94.pdf",
    "citation_key": "wang202201n",
    "metadata": {
      "title": "CausalGNN: Causal-Based Graph Neural Networks for Spatio-Temporal Epidemic Forecasting",
      "authors": [
        "Lijing Wang",
        "A. Adiga",
        "Jiangzhuo Chen",
        "A. Sadilek",
        "S. Venkatramanan",
        "M. Marathe"
      ],
      "published_date": "2022",
      "abstract": "Infectious disease forecasting has been a key focus in the recent past owing to the COVID-19 pandemic and has proved to be an important tool in controlling the pandemic. With the advent of reliable spatiotemporal data, graph neural network models have been able to successfully model the inter-relation between the cross-region signals to produce quality forecasts, but like most deep-learning models they do not explicitly incorporate the underlying causal mechanisms. In this work, we employ a causal mechanistic model to guide the learning of the graph embeddings and propose a novel learning framework -- Causal-based Graph Neural Network (CausalGNN) that learns spatiotemporal embedding in a latent space where graph input features and epidemiological context are combined via a mutually learning mechanism using graph-based non-linear transformations. We design an attention-based dynamic GNN module to capture spatial and temporal disease dynamics. A causal module is added to the framework to provide epidemiological context for node embedding via ordinary differential equations. Extensive experiments on forecasting daily new cases of COVID-19 at global, US state, and US county levels show that the proposed method outperforms a broad range of baselines. The learned model which incorporates epidemiological context organizes the embedding in an efficient way by keeping the parameter size small leading to robust and accurate forecasting performance across various datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5b1978e8284c8514165938bff6e3276977088f94.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 107,
      "score": 35.666666666666664,
      "summary": "Infectious disease forecasting has been a key focus in the recent past owing to the COVID-19 pandemic and has proved to be an important tool in controlling the pandemic. With the advent of reliable spatiotemporal data, graph neural network models have been able to successfully model the inter-relation between the cross-region signals to produce quality forecasts, but like most deep-learning models they do not explicitly incorporate the underlying causal mechanisms. In this work, we employ a causal mechanistic model to guide the learning of the graph embeddings and propose a novel learning framework -- Causal-based Graph Neural Network (CausalGNN) that learns spatiotemporal embedding in a latent space where graph input features and epidemiological context are combined via a mutually learning mechanism using graph-based non-linear transformations. We design an attention-based dynamic GNN module to capture spatial and temporal disease dynamics. A causal module is added to the framework to provide epidemiological context for node embedding via ordinary differential equations. Extensive experiments on forecasting daily new cases of COVID-19 at global, US state, and US county levels show that the proposed method outperforms a broad range of baselines. The learned model which incorporates epidemiological context organizes the embedding in an efficient way by keeping the parameter size small leading to robust and accurate forecasting performance across various datasets.",
      "keywords": []
    },
    "file_name": "5b1978e8284c8514165938bff6e3276977088f94.pdf"
  },
  {
    "success": true,
    "doc_id": "c4f92aea80f7aee4e3131da0190aa54f",
    "summary": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf",
    "citation_key": "zhou2021c3l",
    "metadata": {
      "title": "Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay",
      "authors": [
        "Fan Zhou",
        "Chengtai Cao"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
      "file_path": "paper_data/Graph_Neural_Networks/info/8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 139,
      "score": 34.75,
      "summary": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
      "keywords": []
    },
    "file_name": "8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf"
  },
  {
    "success": true,
    "doc_id": "2de73e168b4dd1d41afd07ec22c6a7d6",
    "summary": "Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7× speed-up using a single CPU socket and up to 97× speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.",
    "intriguing_abstract": "Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7× speed-up using a single CPU socket and up to 97× speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b0d62f38592dbae23628d9700490cb11ac873182.pdf",
    "citation_key": "vasimuddin2021x7c",
    "metadata": {
      "title": "DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks",
      "authors": [
        "Vasimuddin",
        "Sanchit Misra",
        "Guixiang Ma",
        "Ramanarayan Mohanty",
        "E. Georganas",
        "A. Heinecke",
        "Dhiraj D. Kalamkar",
        "Nesreen Ahmed",
        "Sasikanth Avancha"
      ],
      "published_date": "2021",
      "abstract": "Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7× speed-up using a single CPU socket and up to 97× speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b0d62f38592dbae23628d9700490cb11ac873182.pdf",
      "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis",
      "citationCount": 139,
      "score": 34.75,
      "summary": "Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7× speed-up using a single CPU socket and up to 97× speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.",
      "keywords": []
    },
    "file_name": "b0d62f38592dbae23628d9700490cb11ac873182.pdf"
  },
  {
    "success": true,
    "doc_id": "688746c281cae11fab1d581689300b84",
    "summary": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
    "intriguing_abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/54ff6c9ad037792e938e05985720d313512539b7.pdf",
    "citation_key": "eliasof202189g",
    "metadata": {
      "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations",
      "authors": [
        "Moshe Eliasof",
        "E. Haber",
        "Eran Treister"
      ],
      "published_date": "2021",
      "abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
      "file_path": "paper_data/Graph_Neural_Networks/info/54ff6c9ad037792e938e05985720d313512539b7.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 139,
      "score": 34.75,
      "summary": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
      "keywords": []
    },
    "file_name": "54ff6c9ad037792e938e05985720d313512539b7.pdf"
  },
  {
    "success": true,
    "doc_id": "89b88255d387976d48d88b00da77d006",
    "summary": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf",
    "citation_key": "huang2023fk1",
    "metadata": {
      "title": "Uncertainty Quantification over Graph with Conformalized Graph Neural Networks",
      "authors": [
        "Kexin Huang",
        "Ying Jin",
        "E. Candès",
        "J. Leskovec"
      ],
      "published_date": "2023",
      "abstract": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features.",
      "file_path": "paper_data/Graph_Neural_Networks/info/569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 69,
      "score": 34.5,
      "summary": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features.",
      "keywords": []
    },
    "file_name": "569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf"
  },
  {
    "success": true,
    "doc_id": "a982075f560d083f76fdf9177a489fe0",
    "summary": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
    "intriguing_abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf",
    "citation_key": "fey2021smn",
    "metadata": {
      "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings",
      "authors": [
        "Matthias Fey",
        "J. E. Lenssen",
        "F. Weichert",
        "J. Leskovec"
      ],
      "published_date": "2021",
      "abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 136,
      "score": 34.0,
      "summary": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
      "keywords": []
    },
    "file_name": "e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf"
  },
  {
    "success": true,
    "doc_id": "fcd73930b0442a1991eb7fe6ee8c3388",
    "summary": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.",
    "intriguing_abstract": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2efc9d5bb114f7114b041d621e002b1562366903.pdf",
    "citation_key": "nguyen2021g12",
    "metadata": {
      "title": "ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection",
      "authors": [
        "Van-Anh Nguyen",
        "D. Q. Nguyen",
        "Van Nguyen",
        "Trung Le",
        "Quan Hung Tran",
        "Dinh Q. Phung"
      ],
      "published_date": "2021",
      "abstract": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2efc9d5bb114f7114b041d621e002b1562366903.pdf",
      "venue": "2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
      "citationCount": 133,
      "score": 33.25,
      "summary": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.",
      "keywords": []
    },
    "file_name": "2efc9d5bb114f7114b041d621e002b1562366903.pdf"
  },
  {
    "success": true,
    "doc_id": "a6e05c47b50292369e765ebc52b7ece5",
    "summary": "Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.",
    "intriguing_abstract": "Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf",
    "citation_key": "innan2023fa7",
    "metadata": {
      "title": "Financial Fraud Detection using Quantum Graph Neural Networks",
      "authors": [
        "Nouhaila Innan",
        "Abhishek Sawaika",
        "Ashim Dhor",
        "Siddhant Dutta",
        "Sairupa Thota",
        "Husayn Gokal",
        "Nandan Patel",
        "Muhammad Al-Zafar Khan",
        "Ioannis Theodonis",
        "Mohamed Bennai"
      ],
      "published_date": "2023",
      "abstract": "Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.",
      "file_path": "paper_data/Graph_Neural_Networks/info/84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf",
      "venue": "Quantum Machine Intelligence",
      "citationCount": 65,
      "score": 32.5,
      "summary": "Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.",
      "keywords": []
    },
    "file_name": "84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf"
  },
  {
    "success": true,
    "doc_id": "cdfc4d64100930282a0761b20ba682a1",
    "summary": "A well-trained deep neural network (DNN) enables real-time resource allocation by learning the relationship between a policy and its impacting parameters. When wireless systems operate in dynamic environments, the DNN has to be re-trained frequently and hence training complexity should be low. A promising approach to deal with this issue is to construct DNNs with prior knowledge. In this paper, we show that the power allocation policy in multi-cell-multi-user systems exhibits a combination of permutation equivariance properties, which can be harnessed by graph neural networks (GNNs). In particular, we construct a heterogeneous graph and resort to heterogeneous GNN for learning the policy, whose outputs are only equivariant to some permutations of vertexes rather than arbitrary permutations as homogeneous GNNs. We prove that the properties of the functions learned by existing heterogeneous GNN for the formulated graph are inconsistent with the properties of the policy. To avoid the performance degradation by embedding wrong priors, we design a parameter sharing scheme for heterogeneous GNN such that the learned relationship satisfies the desired properties. Simulation results show that the sample and computational complexities for training the constructed GNN are much lower than existing DNNs to achieve the same sum rate.",
    "intriguing_abstract": "A well-trained deep neural network (DNN) enables real-time resource allocation by learning the relationship between a policy and its impacting parameters. When wireless systems operate in dynamic environments, the DNN has to be re-trained frequently and hence training complexity should be low. A promising approach to deal with this issue is to construct DNNs with prior knowledge. In this paper, we show that the power allocation policy in multi-cell-multi-user systems exhibits a combination of permutation equivariance properties, which can be harnessed by graph neural networks (GNNs). In particular, we construct a heterogeneous graph and resort to heterogeneous GNN for learning the policy, whose outputs are only equivariant to some permutations of vertexes rather than arbitrary permutations as homogeneous GNNs. We prove that the properties of the functions learned by existing heterogeneous GNN for the formulated graph are inconsistent with the properties of the policy. To avoid the performance degradation by embedding wrong priors, we design a parameter sharing scheme for heterogeneous GNN such that the learned relationship satisfies the desired properties. Simulation results show that the sample and computational complexities for training the constructed GNN are much lower than existing DNNs to achieve the same sum rate.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/91b9fa72da566afc77a07ec856c3d8da23714367.pdf",
    "citation_key": "guo2022hu1",
    "metadata": {
      "title": "Learning Power Allocation for Multi-Cell-Multi-User Systems With Heterogeneous Graph Neural Networks",
      "authors": [
        "Jia Guo",
        "Chenyang Yang"
      ],
      "published_date": "2022",
      "abstract": "A well-trained deep neural network (DNN) enables real-time resource allocation by learning the relationship between a policy and its impacting parameters. When wireless systems operate in dynamic environments, the DNN has to be re-trained frequently and hence training complexity should be low. A promising approach to deal with this issue is to construct DNNs with prior knowledge. In this paper, we show that the power allocation policy in multi-cell-multi-user systems exhibits a combination of permutation equivariance properties, which can be harnessed by graph neural networks (GNNs). In particular, we construct a heterogeneous graph and resort to heterogeneous GNN for learning the policy, whose outputs are only equivariant to some permutations of vertexes rather than arbitrary permutations as homogeneous GNNs. We prove that the properties of the functions learned by existing heterogeneous GNN for the formulated graph are inconsistent with the properties of the policy. To avoid the performance degradation by embedding wrong priors, we design a parameter sharing scheme for heterogeneous GNN such that the learned relationship satisfies the desired properties. Simulation results show that the sample and computational complexities for training the constructed GNN are much lower than existing DNNs to achieve the same sum rate.",
      "file_path": "paper_data/Graph_Neural_Networks/info/91b9fa72da566afc77a07ec856c3d8da23714367.pdf",
      "venue": "IEEE Transactions on Wireless Communications",
      "citationCount": 97,
      "score": 32.33333333333333,
      "summary": "A well-trained deep neural network (DNN) enables real-time resource allocation by learning the relationship between a policy and its impacting parameters. When wireless systems operate in dynamic environments, the DNN has to be re-trained frequently and hence training complexity should be low. A promising approach to deal with this issue is to construct DNNs with prior knowledge. In this paper, we show that the power allocation policy in multi-cell-multi-user systems exhibits a combination of permutation equivariance properties, which can be harnessed by graph neural networks (GNNs). In particular, we construct a heterogeneous graph and resort to heterogeneous GNN for learning the policy, whose outputs are only equivariant to some permutations of vertexes rather than arbitrary permutations as homogeneous GNNs. We prove that the properties of the functions learned by existing heterogeneous GNN for the formulated graph are inconsistent with the properties of the policy. To avoid the performance degradation by embedding wrong priors, we design a parameter sharing scheme for heterogeneous GNN such that the learned relationship satisfies the desired properties. Simulation results show that the sample and computational complexities for training the constructed GNN are much lower than existing DNNs to achieve the same sum rate.",
      "keywords": []
    },
    "file_name": "91b9fa72da566afc77a07ec856c3d8da23714367.pdf"
  },
  {
    "success": true,
    "doc_id": "7c6c2cf8809d5fe94023828bf695be74",
    "summary": "Developing accurate yet fast computational tools to simulate complex physical phenomena is a long-standing problem. Recent advances in machine learning have revolutionized the way simulations are approached, shifting from a purely physics- to AI-based paradigm. Although impressive achievements have been reached, efficiently predicting complex physical phenomena in materials and structures remains a challenge. Here, we present an AI-based general framework, implemented through graph neural networks, able to learn complex mechanical behavior of materials from a few hundreds data. Harnessing the natural mesh-to-graph mapping, our deep learning model predicts deformation, stress, and strain fields in various material systems, like fiber and stratified composites, and lattice metamaterials. The model can capture complex nonlinear phenomena, from plasticity to buckling instability, seemingly learning physical relationships between the predicted physical fields. Owing to its flexibility, this graph-based framework aims at connecting materials’ microstructure, base materials’ properties, and boundary conditions to a physical response, opening new avenues towards graph-AI-based surrogate modeling.",
    "intriguing_abstract": "Developing accurate yet fast computational tools to simulate complex physical phenomena is a long-standing problem. Recent advances in machine learning have revolutionized the way simulations are approached, shifting from a purely physics- to AI-based paradigm. Although impressive achievements have been reached, efficiently predicting complex physical phenomena in materials and structures remains a challenge. Here, we present an AI-based general framework, implemented through graph neural networks, able to learn complex mechanical behavior of materials from a few hundreds data. Harnessing the natural mesh-to-graph mapping, our deep learning model predicts deformation, stress, and strain fields in various material systems, like fiber and stratified composites, and lattice metamaterials. The model can capture complex nonlinear phenomena, from plasticity to buckling instability, seemingly learning physical relationships between the predicted physical fields. Owing to its flexibility, this graph-based framework aims at connecting materials’ microstructure, base materials’ properties, and boundary conditions to a physical response, opening new avenues towards graph-AI-based surrogate modeling.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/066dd731b5aaeede92d129344776783590c338d2.pdf",
    "citation_key": "maurizi202293p",
    "metadata": {
      "title": "Predicting stress, strain and deformation fields in materials and structures with graph neural networks",
      "authors": [
        "M. Maurizi",
        "Chao Gao",
        "F. Berto"
      ],
      "published_date": "2022",
      "abstract": "Developing accurate yet fast computational tools to simulate complex physical phenomena is a long-standing problem. Recent advances in machine learning have revolutionized the way simulations are approached, shifting from a purely physics- to AI-based paradigm. Although impressive achievements have been reached, efficiently predicting complex physical phenomena in materials and structures remains a challenge. Here, we present an AI-based general framework, implemented through graph neural networks, able to learn complex mechanical behavior of materials from a few hundreds data. Harnessing the natural mesh-to-graph mapping, our deep learning model predicts deformation, stress, and strain fields in various material systems, like fiber and stratified composites, and lattice metamaterials. The model can capture complex nonlinear phenomena, from plasticity to buckling instability, seemingly learning physical relationships between the predicted physical fields. Owing to its flexibility, this graph-based framework aims at connecting materials’ microstructure, base materials’ properties, and boundary conditions to a physical response, opening new avenues towards graph-AI-based surrogate modeling.",
      "file_path": "paper_data/Graph_Neural_Networks/info/066dd731b5aaeede92d129344776783590c338d2.pdf",
      "venue": "Scientific Reports",
      "citationCount": 96,
      "score": 32.0,
      "summary": "Developing accurate yet fast computational tools to simulate complex physical phenomena is a long-standing problem. Recent advances in machine learning have revolutionized the way simulations are approached, shifting from a purely physics- to AI-based paradigm. Although impressive achievements have been reached, efficiently predicting complex physical phenomena in materials and structures remains a challenge. Here, we present an AI-based general framework, implemented through graph neural networks, able to learn complex mechanical behavior of materials from a few hundreds data. Harnessing the natural mesh-to-graph mapping, our deep learning model predicts deformation, stress, and strain fields in various material systems, like fiber and stratified composites, and lattice metamaterials. The model can capture complex nonlinear phenomena, from plasticity to buckling instability, seemingly learning physical relationships between the predicted physical fields. Owing to its flexibility, this graph-based framework aims at connecting materials’ microstructure, base materials’ properties, and boundary conditions to a physical response, opening new avenues towards graph-AI-based surrogate modeling.",
      "keywords": []
    },
    "file_name": "066dd731b5aaeede92d129344776783590c338d2.pdf"
  },
  {
    "success": true,
    "doc_id": "df680e2a83c72ee5a1a3e674fe0c610f",
    "summary": "The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.",
    "intriguing_abstract": "The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf",
    "citation_key": "ye20226hn",
    "metadata": {
      "title": "A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs",
      "authors": [
        "Zi Ye",
        "Y. J. Kumar",
        "G. O. Sing",
        "Fengyan Song",
        "Junsong Wang"
      ],
      "published_date": "2022",
      "abstract": "The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.",
      "file_path": "paper_data/Graph_Neural_Networks/info/60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf",
      "venue": "IEEE Access",
      "citationCount": 96,
      "score": 32.0,
      "summary": "The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.",
      "keywords": []
    },
    "file_name": "60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf"
  },
  {
    "success": true,
    "doc_id": "6353b53aae44411ada6916c1372c5677",
    "summary": "The prevalence of graph structures in real-world scenarios enables important tasks such as node classification and link prediction. Graphs in many domains follow a long-tailed distribution in their node degrees, i.e., a significant fraction of nodes are tail nodes with a small degree. Although recent graph neural networks (GNNs) can learn powerful node representations, they treat all nodes uniformly and are not tailored to the large group of tail nodes. In particular, there is limited structural information (i.e., links) on tail nodes, resulting in inferior performance. Toward robust tail node embedding, in this paper we propose a novel graph neural network called Tail-GNN. It hinges on the novel concept of transferable neighborhood translation, to model the variable ties between a target node and its neighbors. On one hand, Tail-GNN learns a neighborhood translation from the structurally rich head nodes (i.e., high-degree nodes), which can be further transferred to the structurally limited tail nodes to enhance their representations. On the other hand, the ties with the neighbors are variable across different parts of the graph, and a global neighborhood translation is inflexible. Thus, we devise a node-wise adaptation to localize the global translation w.r.t. each node. Extensive experiments on five benchmark datasets demonstrate that our proposed Tail-GNN significantly outperforms the state-of-the-art baselines.",
    "intriguing_abstract": "The prevalence of graph structures in real-world scenarios enables important tasks such as node classification and link prediction. Graphs in many domains follow a long-tailed distribution in their node degrees, i.e., a significant fraction of nodes are tail nodes with a small degree. Although recent graph neural networks (GNNs) can learn powerful node representations, they treat all nodes uniformly and are not tailored to the large group of tail nodes. In particular, there is limited structural information (i.e., links) on tail nodes, resulting in inferior performance. Toward robust tail node embedding, in this paper we propose a novel graph neural network called Tail-GNN. It hinges on the novel concept of transferable neighborhood translation, to model the variable ties between a target node and its neighbors. On one hand, Tail-GNN learns a neighborhood translation from the structurally rich head nodes (i.e., high-degree nodes), which can be further transferred to the structurally limited tail nodes to enhance their representations. On the other hand, the ties with the neighbors are variable across different parts of the graph, and a global neighborhood translation is inflexible. Thus, we devise a node-wise adaptation to localize the global translation w.r.t. each node. Extensive experiments on five benchmark datasets demonstrate that our proposed Tail-GNN significantly outperforms the state-of-the-art baselines.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ac44bbb4c62033d558aad57712438e5571069d9c.pdf",
    "citation_key": "liu2021efj",
    "metadata": {
      "title": "Tail-GNN: Tail-Node Graph Neural Networks",
      "authors": [
        "Zemin Liu",
        "Trung-Kien Nguyen",
        "Yuan Fang"
      ],
      "published_date": "2021",
      "abstract": "The prevalence of graph structures in real-world scenarios enables important tasks such as node classification and link prediction. Graphs in many domains follow a long-tailed distribution in their node degrees, i.e., a significant fraction of nodes are tail nodes with a small degree. Although recent graph neural networks (GNNs) can learn powerful node representations, they treat all nodes uniformly and are not tailored to the large group of tail nodes. In particular, there is limited structural information (i.e., links) on tail nodes, resulting in inferior performance. Toward robust tail node embedding, in this paper we propose a novel graph neural network called Tail-GNN. It hinges on the novel concept of transferable neighborhood translation, to model the variable ties between a target node and its neighbors. On one hand, Tail-GNN learns a neighborhood translation from the structurally rich head nodes (i.e., high-degree nodes), which can be further transferred to the structurally limited tail nodes to enhance their representations. On the other hand, the ties with the neighbors are variable across different parts of the graph, and a global neighborhood translation is inflexible. Thus, we devise a node-wise adaptation to localize the global translation w.r.t. each node. Extensive experiments on five benchmark datasets demonstrate that our proposed Tail-GNN significantly outperforms the state-of-the-art baselines.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ac44bbb4c62033d558aad57712438e5571069d9c.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 128,
      "score": 32.0,
      "summary": "The prevalence of graph structures in real-world scenarios enables important tasks such as node classification and link prediction. Graphs in many domains follow a long-tailed distribution in their node degrees, i.e., a significant fraction of nodes are tail nodes with a small degree. Although recent graph neural networks (GNNs) can learn powerful node representations, they treat all nodes uniformly and are not tailored to the large group of tail nodes. In particular, there is limited structural information (i.e., links) on tail nodes, resulting in inferior performance. Toward robust tail node embedding, in this paper we propose a novel graph neural network called Tail-GNN. It hinges on the novel concept of transferable neighborhood translation, to model the variable ties between a target node and its neighbors. On one hand, Tail-GNN learns a neighborhood translation from the structurally rich head nodes (i.e., high-degree nodes), which can be further transferred to the structurally limited tail nodes to enhance their representations. On the other hand, the ties with the neighbors are variable across different parts of the graph, and a global neighborhood translation is inflexible. Thus, we devise a node-wise adaptation to localize the global translation w.r.t. each node. Extensive experiments on five benchmark datasets demonstrate that our proposed Tail-GNN significantly outperforms the state-of-the-art baselines.",
      "keywords": []
    },
    "file_name": "ac44bbb4c62033d558aad57712438e5571069d9c.pdf"
  },
  {
    "success": true,
    "doc_id": "4d3e48139841061d6a4c79e7100b7b12",
    "summary": "Graph Neural Networks (GNNs) are widely used on a variety of graph-based machine learning tasks. For node-level tasks, GNNs have strong power to model the homophily property of graphs (i.e., connected nodes are more similar), while their ability to capture heterophily property is often doubtful. This is partially caused by the design of the feature transformation with the same kernel for the nodes in the same hop and the followed aggregation operator. One kernel cannot model the similarity and the dissimilarity (i.e., the positive and negative correlation) between node features simultaneously even though we use attention mechanisms like Graph Attention Network (GAT), since the weight calculated by attention is always a positive value. In this paper, we propose a novel GNN model based on a bi-kernel feature transformation and a selection gate. Two kernels capture homophily and heterophily information respectively, and the gate is introduced to select which kernel we should use for the given node pairs. We conduct extensive experiments on various datasets with different homophily-heterophily properties. The experimental results show consistent and significant improvements against state-of-the-art GNN methods.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are widely used on a variety of graph-based machine learning tasks. For node-level tasks, GNNs have strong power to model the homophily property of graphs (i.e., connected nodes are more similar), while their ability to capture heterophily property is often doubtful. This is partially caused by the design of the feature transformation with the same kernel for the nodes in the same hop and the followed aggregation operator. One kernel cannot model the similarity and the dissimilarity (i.e., the positive and negative correlation) between node features simultaneously even though we use attention mechanisms like Graph Attention Network (GAT), since the weight calculated by attention is always a positive value. In this paper, we propose a novel GNN model based on a bi-kernel feature transformation and a selection gate. Two kernels capture homophily and heterophily information respectively, and the gate is introduced to select which kernel we should use for the given node pairs. We conduct extensive experiments on various datasets with different homophily-heterophily properties. The experimental results show consistent and significant improvements against state-of-the-art GNN methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cc827043e6c5be8337df4edb155096f9d0006020.pdf",
    "citation_key": "du2021kn9",
    "metadata": {
      "title": "GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily",
      "authors": [
        "Lun Du",
        "Xiaozhou Shi",
        "Qiang Fu",
        "Hengyu Liu",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) are widely used on a variety of graph-based machine learning tasks. For node-level tasks, GNNs have strong power to model the homophily property of graphs (i.e., connected nodes are more similar), while their ability to capture heterophily property is often doubtful. This is partially caused by the design of the feature transformation with the same kernel for the nodes in the same hop and the followed aggregation operator. One kernel cannot model the similarity and the dissimilarity (i.e., the positive and negative correlation) between node features simultaneously even though we use attention mechanisms like Graph Attention Network (GAT), since the weight calculated by attention is always a positive value. In this paper, we propose a novel GNN model based on a bi-kernel feature transformation and a selection gate. Two kernels capture homophily and heterophily information respectively, and the gate is introduced to select which kernel we should use for the given node pairs. We conduct extensive experiments on various datasets with different homophily-heterophily properties. The experimental results show consistent and significant improvements against state-of-the-art GNN methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cc827043e6c5be8337df4edb155096f9d0006020.pdf",
      "venue": "The Web Conference",
      "citationCount": 127,
      "score": 31.75,
      "summary": "Graph Neural Networks (GNNs) are widely used on a variety of graph-based machine learning tasks. For node-level tasks, GNNs have strong power to model the homophily property of graphs (i.e., connected nodes are more similar), while their ability to capture heterophily property is often doubtful. This is partially caused by the design of the feature transformation with the same kernel for the nodes in the same hop and the followed aggregation operator. One kernel cannot model the similarity and the dissimilarity (i.e., the positive and negative correlation) between node features simultaneously even though we use attention mechanisms like Graph Attention Network (GAT), since the weight calculated by attention is always a positive value. In this paper, we propose a novel GNN model based on a bi-kernel feature transformation and a selection gate. Two kernels capture homophily and heterophily information respectively, and the gate is introduced to select which kernel we should use for the given node pairs. We conduct extensive experiments on various datasets with different homophily-heterophily properties. The experimental results show consistent and significant improvements against state-of-the-art GNN methods.",
      "keywords": []
    },
    "file_name": "cc827043e6c5be8337df4edb155096f9d0006020.pdf"
  },
  {
    "success": true,
    "doc_id": "e858b9a6b8c1f1eb20bdadef34664268",
    "summary": "The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts.",
    "intriguing_abstract": "The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/741bf9081fe341c173f36739a50606bf2a159610.pdf",
    "citation_key": "xu20226vc",
    "metadata": {
      "title": "Evidence-aware Fake News Detection with Graph Neural Networks",
      "authors": [
        "Weizhi Xu",
        "Jun Wu",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "published_date": "2022",
      "abstract": "The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts.",
      "file_path": "paper_data/Graph_Neural_Networks/info/741bf9081fe341c173f36739a50606bf2a159610.pdf",
      "venue": "The Web Conference",
      "citationCount": 95,
      "score": 31.666666666666664,
      "summary": "The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts.",
      "keywords": []
    },
    "file_name": "741bf9081fe341c173f36739a50606bf2a159610.pdf"
  },
  {
    "success": true,
    "doc_id": "3a47aaa14c12a072e2734387c67c63dd",
    "summary": "Co-designing hardware platforms and neural network software can help improve the computational efficiency and training affordability of deep learning implementations. A new approach designed for graph learning with echo state neural networks makes use of in-memory computing with resistive memory and shows up to a 35 times improvement in the energy efficiency and 99% reduction in training cost for graph classification on large datasets. Recent years have witnessed a surge of interest in learning representations of graph-structured data, with applications from social networks to drug discovery. However, graph neural networks, the machine learning models for handling graph-structured data, face significant challenges when running on conventional digital hardware, including the slowdown of Moore’s law due to transistor scaling limits and the von Neumann bottleneck incurred by physically separated memory and processing units, as well as a high training cost. Here we present a hardware–software co-design to address these challenges, by designing an echo state graph neural network based on random resistive memory arrays, which are built from low-cost, nanoscale and stackable resistors for efficient in-memory computing. This approach leverages the intrinsic stochasticity of dielectric breakdown in resistive switching to implement random projections in hardware for an echo state network that effectively minimizes the training complexity thanks to its fixed and random weights. The system demonstrates state-of-the-art performance on both graph classification using the MUTAG and COLLAB datasets and node classification using the CORA dataset, achieving 2.16×, 35.42× and 40.37× improvements in energy efficiency for a projected random resistive memory-based hybrid analogue–digital system over a state-of-the-art graphics processing unit and 99.35%, 99.99% and 91.40% reductions of backward pass complexity compared with conventional graph learning. The results point to a promising direction for next-generation artificial intelligence systems for graph learning.",
    "intriguing_abstract": "Co-designing hardware platforms and neural network software can help improve the computational efficiency and training affordability of deep learning implementations. A new approach designed for graph learning with echo state neural networks makes use of in-memory computing with resistive memory and shows up to a 35 times improvement in the energy efficiency and 99% reduction in training cost for graph classification on large datasets. Recent years have witnessed a surge of interest in learning representations of graph-structured data, with applications from social networks to drug discovery. However, graph neural networks, the machine learning models for handling graph-structured data, face significant challenges when running on conventional digital hardware, including the slowdown of Moore’s law due to transistor scaling limits and the von Neumann bottleneck incurred by physically separated memory and processing units, as well as a high training cost. Here we present a hardware–software co-design to address these challenges, by designing an echo state graph neural network based on random resistive memory arrays, which are built from low-cost, nanoscale and stackable resistors for efficient in-memory computing. This approach leverages the intrinsic stochasticity of dielectric breakdown in resistive switching to implement random projections in hardware for an echo state network that effectively minimizes the training complexity thanks to its fixed and random weights. The system demonstrates state-of-the-art performance on both graph classification using the MUTAG and COLLAB datasets and node classification using the CORA dataset, achieving 2.16×, 35.42× and 40.37× improvements in energy efficiency for a projected random resistive memory-based hybrid analogue–digital system over a state-of-the-art graphics processing unit and 99.35%, 99.99% and 91.40% reductions of backward pass complexity compared with conventional graph learning. The results point to a promising direction for next-generation artificial intelligence systems for graph learning.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf",
    "citation_key": "wang2023a6u",
    "metadata": {
      "title": "Echo state graph neural networks with analogue random resistive memory arrays",
      "authors": [
        "Shaocong Wang",
        "Yi Li",
        "Dingchen Wang",
        "Woyu Zhang",
        "X. Chen",
        "Danian Dong",
        "Song-jian Wang",
        "Xumeng Zhang",
        "Peng Lin",
        "Claudio Gallicchio",
        "Xiaoxin Xu",
        "Qi Liu",
        "K. Cheng",
        "Zhongrui Wang",
        "Dashan Shang",
        "Meilin Liu"
      ],
      "published_date": "2023",
      "abstract": "Co-designing hardware platforms and neural network software can help improve the computational efficiency and training affordability of deep learning implementations. A new approach designed for graph learning with echo state neural networks makes use of in-memory computing with resistive memory and shows up to a 35 times improvement in the energy efficiency and 99% reduction in training cost for graph classification on large datasets. Recent years have witnessed a surge of interest in learning representations of graph-structured data, with applications from social networks to drug discovery. However, graph neural networks, the machine learning models for handling graph-structured data, face significant challenges when running on conventional digital hardware, including the slowdown of Moore’s law due to transistor scaling limits and the von Neumann bottleneck incurred by physically separated memory and processing units, as well as a high training cost. Here we present a hardware–software co-design to address these challenges, by designing an echo state graph neural network based on random resistive memory arrays, which are built from low-cost, nanoscale and stackable resistors for efficient in-memory computing. This approach leverages the intrinsic stochasticity of dielectric breakdown in resistive switching to implement random projections in hardware for an echo state network that effectively minimizes the training complexity thanks to its fixed and random weights. The system demonstrates state-of-the-art performance on both graph classification using the MUTAG and COLLAB datasets and node classification using the CORA dataset, achieving 2.16×, 35.42× and 40.37× improvements in energy efficiency for a projected random resistive memory-based hybrid analogue–digital system over a state-of-the-art graphics processing unit and 99.35%, 99.99% and 91.40% reductions of backward pass complexity compared with conventional graph learning. The results point to a promising direction for next-generation artificial intelligence systems for graph learning.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf",
      "venue": "Nature Machine Intelligence",
      "citationCount": 63,
      "score": 31.5,
      "summary": "Co-designing hardware platforms and neural network software can help improve the computational efficiency and training affordability of deep learning implementations. A new approach designed for graph learning with echo state neural networks makes use of in-memory computing with resistive memory and shows up to a 35 times improvement in the energy efficiency and 99% reduction in training cost for graph classification on large datasets. Recent years have witnessed a surge of interest in learning representations of graph-structured data, with applications from social networks to drug discovery. However, graph neural networks, the machine learning models for handling graph-structured data, face significant challenges when running on conventional digital hardware, including the slowdown of Moore’s law due to transistor scaling limits and the von Neumann bottleneck incurred by physically separated memory and processing units, as well as a high training cost. Here we present a hardware–software co-design to address these challenges, by designing an echo state graph neural network based on random resistive memory arrays, which are built from low-cost, nanoscale and stackable resistors for efficient in-memory computing. This approach leverages the intrinsic stochasticity of dielectric breakdown in resistive switching to implement random projections in hardware for an echo state network that effectively minimizes the training complexity thanks to its fixed and random weights. The system demonstrates state-of-the-art performance on both graph classification using the MUTAG and COLLAB datasets and node classification using the CORA dataset, achieving 2.16×, 35.42× and 40.37× improvements in energy efficiency for a projected random resistive memory-based hybrid analogue–digital system over a state-of-the-art graphics processing unit and 99.35%, 99.99% and 91.40% reductions of backward pass complexity compared with conventional graph learning. The results point to a promising direction for next-generation artificial intelligence systems for graph learning.",
      "keywords": []
    },
    "file_name": "f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf"
  },
  {
    "success": true,
    "doc_id": "8cdbf0adc49713bd8399220a8b6aaaaf",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf",
    "citation_key": "bing2022oka",
    "metadata": {
      "title": "Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications",
      "authors": [
        "Rui Bing",
        "Guan Yuan",
        "Mu Zhu",
        "Fanrong Meng",
        "Huifang Ma",
        "Shaojie Qiao"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf",
      "venue": "Artificial Intelligence Review",
      "citationCount": 94,
      "score": 31.333333333333332,
      "summary": "",
      "keywords": []
    },
    "file_name": "e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf"
  },
  {
    "success": true,
    "doc_id": "5d0210d5bc5a9ad439bf35899acfc50f",
    "summary": "Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
    "intriguing_abstract": "Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e46aa831aac38520249dff35916937f0d094f32e.pdf",
    "citation_key": "lyu2023ao0",
    "metadata": {
      "title": "Knowledge Enhanced Graph Neural Networks for Explainable Recommendation",
      "authors": [
        "Ziyu Lyu",
        "Yue Wu",
        "Junjie Lai",
        "Min Yang",
        "Chengming Li",
        "Wei Zhou"
      ],
      "published_date": "2023",
      "abstract": "Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e46aa831aac38520249dff35916937f0d094f32e.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 62,
      "score": 31.0,
      "summary": "Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
      "keywords": []
    },
    "file_name": "e46aa831aac38520249dff35916937f0d094f32e.pdf"
  },
  {
    "success": true,
    "doc_id": "70f41e0cb3cc12e3becc2d499faeb6a0",
    "summary": "Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a52ae33c11309a98887405db21e930a1f298d865.pdf",
    "citation_key": "peng2021gbb",
    "metadata": {
      "title": "Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks",
      "authors": [
        "Hao Peng",
        "Ruitong Zhang",
        "Yingtong Dou",
        "Renyu Yang",
        "Jingyi Zhang",
        "Philip S. Yu"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.",
      "file_path": "paper_data/Graph_Neural_Networks/info/a52ae33c11309a98887405db21e930a1f298d865.pdf",
      "venue": "ACM Trans. Inf. Syst.",
      "citationCount": 123,
      "score": 30.75,
      "summary": "Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.",
      "keywords": []
    },
    "file_name": "a52ae33c11309a98887405db21e930a1f298d865.pdf"
  },
  {
    "success": true,
    "doc_id": "7685313f2633ec266975bd50ef9dcc41",
    "summary": "Abstract Knowledge of the interactions between proteins and nucleic acids is the basis of understanding various biological activities and designing new drugs. How to accurately identify the nucleic-acid-binding residues remains a challenging task. In this paper, we propose an accurate predictor, GraphBind, for identifying nucleic-acid-binding residues on proteins based on an end-to-end graph neural network. Considering that binding sites often behave in highly conservative patterns on local tertiary structures, we first construct graphs based on the structural contexts of target residues and their spatial neighborhood. Then, hierarchical graph neural networks (HGNNs) are used to embed the latent local patterns of structural and bio-physicochemical characteristics for binding residue recognition. We comprehensively evaluate GraphBind on DNA/RNA benchmark datasets. The results demonstrate the superior performance of GraphBind than state-of-the-art methods. Moreover, GraphBind is extended to other ligand-binding residue prediction to verify its generalization capability. Web server of GraphBind is freely available at http://www.csbio.sjtu.edu.cn/bioinf/GraphBind/.",
    "intriguing_abstract": "Abstract Knowledge of the interactions between proteins and nucleic acids is the basis of understanding various biological activities and designing new drugs. How to accurately identify the nucleic-acid-binding residues remains a challenging task. In this paper, we propose an accurate predictor, GraphBind, for identifying nucleic-acid-binding residues on proteins based on an end-to-end graph neural network. Considering that binding sites often behave in highly conservative patterns on local tertiary structures, we first construct graphs based on the structural contexts of target residues and their spatial neighborhood. Then, hierarchical graph neural networks (HGNNs) are used to embed the latent local patterns of structural and bio-physicochemical characteristics for binding residue recognition. We comprehensively evaluate GraphBind on DNA/RNA benchmark datasets. The results demonstrate the superior performance of GraphBind than state-of-the-art methods. Moreover, GraphBind is extended to other ligand-binding residue prediction to verify its generalization capability. Web server of GraphBind is freely available at http://www.csbio.sjtu.edu.cn/bioinf/GraphBind/.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf",
    "citation_key": "xia2021s85",
    "metadata": {
      "title": "GraphBind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues",
      "authors": [
        "Ying Xia",
        "Chun-Qiu Xia",
        "Xiaoyong Pan",
        "Hongbin Shen"
      ],
      "published_date": "2021",
      "abstract": "Abstract Knowledge of the interactions between proteins and nucleic acids is the basis of understanding various biological activities and designing new drugs. How to accurately identify the nucleic-acid-binding residues remains a challenging task. In this paper, we propose an accurate predictor, GraphBind, for identifying nucleic-acid-binding residues on proteins based on an end-to-end graph neural network. Considering that binding sites often behave in highly conservative patterns on local tertiary structures, we first construct graphs based on the structural contexts of target residues and their spatial neighborhood. Then, hierarchical graph neural networks (HGNNs) are used to embed the latent local patterns of structural and bio-physicochemical characteristics for binding residue recognition. We comprehensively evaluate GraphBind on DNA/RNA benchmark datasets. The results demonstrate the superior performance of GraphBind than state-of-the-art methods. Moreover, GraphBind is extended to other ligand-binding residue prediction to verify its generalization capability. Web server of GraphBind is freely available at http://www.csbio.sjtu.edu.cn/bioinf/GraphBind/.",
      "file_path": "paper_data/Graph_Neural_Networks/info/a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf",
      "venue": "Nucleic Acids Research",
      "citationCount": 122,
      "score": 30.5,
      "summary": "Abstract Knowledge of the interactions between proteins and nucleic acids is the basis of understanding various biological activities and designing new drugs. How to accurately identify the nucleic-acid-binding residues remains a challenging task. In this paper, we propose an accurate predictor, GraphBind, for identifying nucleic-acid-binding residues on proteins based on an end-to-end graph neural network. Considering that binding sites often behave in highly conservative patterns on local tertiary structures, we first construct graphs based on the structural contexts of target residues and their spatial neighborhood. Then, hierarchical graph neural networks (HGNNs) are used to embed the latent local patterns of structural and bio-physicochemical characteristics for binding residue recognition. We comprehensively evaluate GraphBind on DNA/RNA benchmark datasets. The results demonstrate the superior performance of GraphBind than state-of-the-art methods. Moreover, GraphBind is extended to other ligand-binding residue prediction to verify its generalization capability. Web server of GraphBind is freely available at http://www.csbio.sjtu.edu.cn/bioinf/GraphBind/.",
      "keywords": []
    },
    "file_name": "a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf"
  },
  {
    "success": true,
    "doc_id": "dd1fc8a42e1d21a18b82322f959ab2b3",
    "summary": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
    "intriguing_abstract": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5fb4947831352af6d6231a830a943f0f2069ee8b.pdf",
    "citation_key": "feng2022914",
    "metadata": {
      "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
      "authors": [
        "Aosong Feng",
        "Chenyu You",
        "Shiqiang Wang",
        "L. Tassiulas"
      ],
      "published_date": "2022",
      "abstract": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5fb4947831352af6d6231a830a943f0f2069ee8b.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 90,
      "score": 30.0,
      "summary": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
      "keywords": []
    },
    "file_name": "5fb4947831352af6d6231a830a943f0f2069ee8b.pdf"
  },
  {
    "success": true,
    "doc_id": "aa3ba339e4fba480181b1db8d3aaa34d",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf",
    "citation_key": "paper2022mw4",
    "metadata": {
      "title": "Graph Neural Networks: Foundations, Frontiers, and Applications",
      "authors": [],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf",
      "venue": "",
      "citationCount": 88,
      "score": 29.333333333333332,
      "summary": "",
      "keywords": []
    },
    "file_name": "cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf"
  },
  {
    "success": true,
    "doc_id": "cf2cf7069859323f473c3c69716377f7",
    "summary": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7544db2ae3140081b1581a99eee88960cc31415a.pdf",
    "citation_key": "luan2021g2p",
    "metadata": {
      "title": "Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?",
      "authors": [
        "Sitao Luan",
        "Chenqing Hua",
        "Qincheng Lu",
        "Jiaqi Zhu",
        "Mingde Zhao",
        "Shuyuan Zhang",
        "Xiaoming Chang",
        "Doina Precup"
      ],
      "published_date": "2021",
      "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7544db2ae3140081b1581a99eee88960cc31415a.pdf",
      "venue": "arXiv.org",
      "citationCount": 117,
      "score": 29.25,
      "summary": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.",
      "keywords": []
    },
    "file_name": "7544db2ae3140081b1581a99eee88960cc31415a.pdf"
  },
  {
    "success": true,
    "doc_id": "c83215bb076ecbc21e774d9e706617d9",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf",
    "citation_key": "waikhom20226fa",
    "metadata": {
      "title": "A survey of graph neural networks in various learning paradigms: methods, applications, and challenges",
      "authors": [
        "Lilapati Waikhom",
        "Ripon Patgiri"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf",
      "venue": "Artificial Intelligence Review",
      "citationCount": 87,
      "score": 29.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf"
  },
  {
    "success": true,
    "doc_id": "441d2787f54c404687a93ab9d0857288",
    "summary": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset, we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types. Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",
    "intriguing_abstract": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset, we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types. Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf",
    "citation_key": "tang2021h2z",
    "metadata": {
      "title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis",
      "authors": [
        "Siyi Tang",
        "Jared A. Dunnmon",
        "Khaled Saab",
        "Xuan Zhang",
        "Qianying Huang",
        "Florian Dubost",
        "D. Rubin",
        "Christopher Lee-Messer"
      ],
      "published_date": "2021",
      "abstract": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset, we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types. Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 115,
      "score": 28.75,
      "summary": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset, we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types. Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",
      "keywords": []
    },
    "file_name": "0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf"
  },
  {
    "success": true,
    "doc_id": "62c5452199124b42abd45a7143a8a444",
    "summary": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \\emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
    "intriguing_abstract": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \\emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf",
    "citation_key": "thost20211ln",
    "metadata": {
      "title": "Directed Acyclic Graph Neural Networks",
      "authors": [
        "Veronika Thost",
        "Jie Chen"
      ],
      "published_date": "2021",
      "abstract": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \\emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 115,
      "score": 28.75,
      "summary": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \\emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
      "keywords": []
    },
    "file_name": "c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf"
  },
  {
    "success": true,
    "doc_id": "2c913e9cf2c0d8989f5a0e8050dbfab9",
    "summary": "Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?\n\n\n\nIn this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.",
    "intriguing_abstract": "Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?\n\n\n\nIn this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf",
    "citation_key": "chai2022nf9",
    "metadata": {
      "title": "Can Abnormality be Detected by Graph Neural Networks?",
      "authors": [
        "Ziwei Chai",
        "Siqi You",
        "Yang Yang",
        "Shiliang Pu",
        "Jiarong Xu",
        "Haoyang Cai",
        "Weihao Jiang"
      ],
      "published_date": "2022",
      "abstract": "Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?\n\n\n\nIn this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 86,
      "score": 28.666666666666664,
      "summary": "Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?\n\n\n\nIn this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.",
      "keywords": []
    },
    "file_name": "b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf"
  },
  {
    "success": true,
    "doc_id": "0cd59dab32ce6609d62c38435754e127",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf",
    "citation_key": "sun20239ly",
    "metadata": {
      "title": "Attention-based graph neural networks: a survey",
      "authors": [
        "Chengcheng Sun",
        "Chenhao Li",
        "Xiang Lin",
        "Tianji Zheng",
        "Fanrong Meng",
        "Xiaobin Rui",
        "Zhixi Wang"
      ],
      "published_date": "2023",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf",
      "venue": "Artificial Intelligence Review",
      "citationCount": 57,
      "score": 28.5,
      "summary": "",
      "keywords": []
    },
    "file_name": "9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf"
  },
  {
    "success": true,
    "doc_id": "ee44ec3f005bb6f20b51e2197e8137b3",
    "summary": "The problem of session-aware recommendation aims to predict users’ next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.",
    "intriguing_abstract": "The problem of session-aware recommendation aims to predict users’ next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf",
    "citation_key": "zhang2022atq",
    "metadata": {
      "title": "Personalized Graph Neural Networks With Attention Mechanism for Session-Aware Recommendation",
      "authors": [
        "Mengqi Zhang",
        "Shu Wu",
        "Meng Gao",
        "Xin Jiang",
        "Ke Xu",
        "Liang Wang"
      ],
      "published_date": "2022",
      "abstract": "The problem of session-aware recommendation aims to predict users’ next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 84,
      "score": 28.0,
      "summary": "The problem of session-aware recommendation aims to predict users’ next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.",
      "keywords": []
    },
    "file_name": "cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf"
  },
  {
    "success": true,
    "doc_id": "98bc904d2c168005bff6a894fcd8e139",
    "summary": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.",
    "intriguing_abstract": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6b72135bf31e78ccee78478228b635201326d217.pdf",
    "citation_key": "munikoti2022k7d",
    "metadata": {
      "title": "Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications",
      "authors": [
        "Sai Munikoti",
        "D. Agarwal",
        "L. Das",
        "M. Halappanavar",
        "Balasubramaniam Natarajan"
      ],
      "published_date": "2022",
      "abstract": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.",
      "file_path": "paper_data/Graph_Neural_Networks/info/6b72135bf31e78ccee78478228b635201326d217.pdf",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "citationCount": 83,
      "score": 27.666666666666664,
      "summary": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.",
      "keywords": []
    },
    "file_name": "6b72135bf31e78ccee78478228b635201326d217.pdf"
  },
  {
    "success": true,
    "doc_id": "8b2edcacf4831069575081abc21a31f7",
    "summary": "Classifying encrypted traffic from emerging applications is important but challenging as many conventional traffic classification approaches are ineffective, thus calling for novel methods for identifying encrypted network flows. Recent machine learning and deep learning-based approaches are severely limited by their feature selection and inherent neural network architecture. More importantly, they overlook the opportunity to capture latent information in the temporal dimension of packets. As network data by nature are of non-Euclidean distance space and carry abundant chronological and temporal relations, we are inspired to utilize geometric deep learning that simultaneously takes into account packet raw bytes, metadata and packet relations for classifying encrypted network traffic. Our proposed graph neural network (GNN) model outperforms the two reference methods, convolutional neural networks (CNN) and recurrent neural networks (RNN) quantitatively as indicated by three metrics: sensitivity, precision and F1 score.",
    "intriguing_abstract": "Classifying encrypted traffic from emerging applications is important but challenging as many conventional traffic classification approaches are ineffective, thus calling for novel methods for identifying encrypted network flows. Recent machine learning and deep learning-based approaches are severely limited by their feature selection and inherent neural network architecture. More importantly, they overlook the opportunity to capture latent information in the temporal dimension of packets. As network data by nature are of non-Euclidean distance space and carry abundant chronological and temporal relations, we are inspired to utilize geometric deep learning that simultaneously takes into account packet raw bytes, metadata and packet relations for classifying encrypted network traffic. Our proposed graph neural network (GNN) model outperforms the two reference methods, convolutional neural networks (CNN) and recurrent neural networks (RNN) quantitatively as indicated by three metrics: sensitivity, precision and F1 score.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf",
    "citation_key": "huoh2023i97",
    "metadata": {
      "title": "Flow-Based Encrypted Network Traffic Classification With Graph Neural Networks",
      "authors": [
        "Ting-Li Huoh",
        "Yan Luo",
        "Peilong Li",
        "Tong Zhang"
      ],
      "published_date": "2023",
      "abstract": "Classifying encrypted traffic from emerging applications is important but challenging as many conventional traffic classification approaches are ineffective, thus calling for novel methods for identifying encrypted network flows. Recent machine learning and deep learning-based approaches are severely limited by their feature selection and inherent neural network architecture. More importantly, they overlook the opportunity to capture latent information in the temporal dimension of packets. As network data by nature are of non-Euclidean distance space and carry abundant chronological and temporal relations, we are inspired to utilize geometric deep learning that simultaneously takes into account packet raw bytes, metadata and packet relations for classifying encrypted network traffic. Our proposed graph neural network (GNN) model outperforms the two reference methods, convolutional neural networks (CNN) and recurrent neural networks (RNN) quantitatively as indicated by three metrics: sensitivity, precision and F1 score.",
      "file_path": "paper_data/Graph_Neural_Networks/info/77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf",
      "venue": "IEEE Transactions on Network and Service Management",
      "citationCount": 55,
      "score": 27.5,
      "summary": "Classifying encrypted traffic from emerging applications is important but challenging as many conventional traffic classification approaches are ineffective, thus calling for novel methods for identifying encrypted network flows. Recent machine learning and deep learning-based approaches are severely limited by their feature selection and inherent neural network architecture. More importantly, they overlook the opportunity to capture latent information in the temporal dimension of packets. As network data by nature are of non-Euclidean distance space and carry abundant chronological and temporal relations, we are inspired to utilize geometric deep learning that simultaneously takes into account packet raw bytes, metadata and packet relations for classifying encrypted network traffic. Our proposed graph neural network (GNN) model outperforms the two reference methods, convolutional neural networks (CNN) and recurrent neural networks (RNN) quantitatively as indicated by three metrics: sensitivity, precision and F1 score.",
      "keywords": []
    },
    "file_name": "77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf"
  },
  {
    "success": true,
    "doc_id": "0d6e01f212dc93e2dff90cfb73b0be4e",
    "summary": "Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation. The prospect for future potential directions is also provided.",
    "intriguing_abstract": "Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation. The prospect for future potential directions is also provided.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf",
    "citation_key": "han20227gn",
    "metadata": {
      "title": "Geometrically Equivariant Graph Neural Networks: A Survey",
      "authors": [
        "Jiaqi Han",
        "Yu Rong",
        "Tingyang Xu",
        "Wenbing Huang"
      ],
      "published_date": "2022",
      "abstract": "Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation. The prospect for future potential directions is also provided.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf",
      "venue": "arXiv.org",
      "citationCount": 82,
      "score": 27.333333333333332,
      "summary": "Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation. The prospect for future potential directions is also provided.",
      "keywords": []
    },
    "file_name": "cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf"
  },
  {
    "success": true,
    "doc_id": "0932ccbdc6df1395e3e44b958ee00e9e",
    "summary": "Graphs are used widely to model complex systems, and detecting anomalies in a graph is an important task in the analysis of complex systems. Graph anomalies are patterns in a graph that do not conform to normal patterns expected of the attributes and/or structures of the graph. In recent years, graph neural networks (GNNs) have been studied extensively and have successfully performed difficult machine learning tasks in node classification, link prediction, and graph classification thanks to the highly expressive capability via message passing in effectively learning graph representations. To solve the graph anomaly detection problem, GNN-based methods leverage information about the graph attributes (or features) and/or structures to learn to score anomalies appropriately. In this survey, we review the recent advances made in detecting graph anomalies using GNN models. Specifically, we summarize GNN-based methods according to the graph type (i.e., static and dynamic), the anomaly type (i.e., node, edge, subgraph, and whole graph), and the network architecture (e.g., graph autoencoder, graph convolutional network). To the best of our knowledge, this survey is the first comprehensive review of graph anomaly detection methods based on GNNs.",
    "intriguing_abstract": "Graphs are used widely to model complex systems, and detecting anomalies in a graph is an important task in the analysis of complex systems. Graph anomalies are patterns in a graph that do not conform to normal patterns expected of the attributes and/or structures of the graph. In recent years, graph neural networks (GNNs) have been studied extensively and have successfully performed difficult machine learning tasks in node classification, link prediction, and graph classification thanks to the highly expressive capability via message passing in effectively learning graph representations. To solve the graph anomaly detection problem, GNN-based methods leverage information about the graph attributes (or features) and/or structures to learn to score anomalies appropriately. In this survey, we review the recent advances made in detecting graph anomalies using GNN models. Specifically, we summarize GNN-based methods according to the graph type (i.e., static and dynamic), the anomaly type (i.e., node, edge, subgraph, and whole graph), and the network architecture (e.g., graph autoencoder, graph convolutional network). To the best of our knowledge, this survey is the first comprehensive review of graph anomaly detection methods based on GNNs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/20309e3990cd612a13e389e1572786e55100f03d.pdf",
    "citation_key": "kim2022yql",
    "metadata": {
      "title": "Graph Anomaly Detection With Graph Neural Networks: Current Status and Challenges",
      "authors": [
        "Hwan Kim",
        "Byung Suk Lee",
        "Won-Yong Shin",
        "Sungsu Lim"
      ],
      "published_date": "2022",
      "abstract": "Graphs are used widely to model complex systems, and detecting anomalies in a graph is an important task in the analysis of complex systems. Graph anomalies are patterns in a graph that do not conform to normal patterns expected of the attributes and/or structures of the graph. In recent years, graph neural networks (GNNs) have been studied extensively and have successfully performed difficult machine learning tasks in node classification, link prediction, and graph classification thanks to the highly expressive capability via message passing in effectively learning graph representations. To solve the graph anomaly detection problem, GNN-based methods leverage information about the graph attributes (or features) and/or structures to learn to score anomalies appropriately. In this survey, we review the recent advances made in detecting graph anomalies using GNN models. Specifically, we summarize GNN-based methods according to the graph type (i.e., static and dynamic), the anomaly type (i.e., node, edge, subgraph, and whole graph), and the network architecture (e.g., graph autoencoder, graph convolutional network). To the best of our knowledge, this survey is the first comprehensive review of graph anomaly detection methods based on GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/20309e3990cd612a13e389e1572786e55100f03d.pdf",
      "venue": "IEEE Access",
      "citationCount": 80,
      "score": 26.666666666666664,
      "summary": "Graphs are used widely to model complex systems, and detecting anomalies in a graph is an important task in the analysis of complex systems. Graph anomalies are patterns in a graph that do not conform to normal patterns expected of the attributes and/or structures of the graph. In recent years, graph neural networks (GNNs) have been studied extensively and have successfully performed difficult machine learning tasks in node classification, link prediction, and graph classification thanks to the highly expressive capability via message passing in effectively learning graph representations. To solve the graph anomaly detection problem, GNN-based methods leverage information about the graph attributes (or features) and/or structures to learn to score anomalies appropriately. In this survey, we review the recent advances made in detecting graph anomalies using GNN models. Specifically, we summarize GNN-based methods according to the graph type (i.e., static and dynamic), the anomaly type (i.e., node, edge, subgraph, and whole graph), and the network architecture (e.g., graph autoencoder, graph convolutional network). To the best of our knowledge, this survey is the first comprehensive review of graph anomaly detection methods based on GNNs.",
      "keywords": []
    },
    "file_name": "20309e3990cd612a13e389e1572786e55100f03d.pdf"
  },
  {
    "success": true,
    "doc_id": "58dad2cbb8a3fcb908454411aa10b36b",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/bd4b8cad70faa48605163eaede13d62fb671f5de.pdf",
    "citation_key": "zhang2022uih",
    "metadata": {
      "title": "Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift",
      "authors": [
        "Zeyang Zhang",
        "Xin Wang",
        "Ziwei Zhang",
        "Haoyang Li",
        "Zhou Qin",
        "Wenwu Zhu"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/bd4b8cad70faa48605163eaede13d62fb671f5de.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 79,
      "score": 26.333333333333332,
      "summary": "",
      "keywords": []
    },
    "file_name": "bd4b8cad70faa48605163eaede13d62fb671f5de.pdf"
  },
  {
    "success": true,
    "doc_id": "2e6e97bdd80c3fafe5025e14daef6d9e",
    "summary": "Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots’ inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots’ viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.",
    "intriguing_abstract": "Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots’ inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots’ viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e925e38c5bade594237439c1d4a77e1376535697.pdf",
    "citation_key": "zhou2022a3h",
    "metadata": {
      "title": "Multi-Robot Collaborative Perception With Graph Neural Networks",
      "authors": [
        "Yang Zhou",
        "Jiuhong Xiao",
        "Yuee Zhou",
        "Giuseppe Loianno"
      ],
      "published_date": "2022",
      "abstract": "Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots’ inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots’ viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e925e38c5bade594237439c1d4a77e1376535697.pdf",
      "venue": "IEEE Robotics and Automation Letters",
      "citationCount": 77,
      "score": 25.666666666666664,
      "summary": "Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots’ inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots’ viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.",
      "keywords": []
    },
    "file_name": "e925e38c5bade594237439c1d4a77e1376535697.pdf"
  },
  {
    "success": true,
    "doc_id": "e23e6f50cf16733330141553c4e5dc58",
    "summary": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
    "intriguing_abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/8b9f01585a679dffe92261ecdec56425db9ef97f.pdf",
    "citation_key": "wu2023aqs",
    "metadata": {
      "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
      "authors": [
        "Xinyi Wu",
        "A. Ajorlou",
        "Zihui Wu",
        "A. Jadbabaie"
      ],
      "published_date": "2023",
      "abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
      "file_path": "paper_data/Graph_Neural_Networks/info/8b9f01585a679dffe92261ecdec56425db9ef97f.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 51,
      "score": 25.5,
      "summary": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
      "keywords": []
    },
    "file_name": "8b9f01585a679dffe92261ecdec56425db9ef97f.pdf"
  },
  {
    "success": true,
    "doc_id": "379b5c28ac4cb5fba7cf77fd222f4058",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ab27a370d87617255455a05cb2d98c268b5fa06b.pdf",
    "citation_key": "long2022l97",
    "metadata": {
      "title": "Pre-training graph neural networks for link prediction in biomedical networks",
      "authors": [
        "Yahui Long",
        "Min Wu",
        "Yong Liu",
        "Yuan Fang",
        "C. Kwoh",
        "Jiawei Luo",
        "Xiaoli Li"
      ],
      "published_date": "2022",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/ab27a370d87617255455a05cb2d98c268b5fa06b.pdf",
      "venue": "Bioinform.",
      "citationCount": 76,
      "score": 25.333333333333332,
      "summary": "",
      "keywords": []
    },
    "file_name": "ab27a370d87617255455a05cb2d98c268b5fa06b.pdf"
  },
  {
    "success": true,
    "doc_id": "2ed84884715af3a8ac1c13d7fc21bc79",
    "summary": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden.",
    "intriguing_abstract": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf",
    "citation_key": "cini2022pjy",
    "metadata": {
      "title": "Scalable Spatiotemporal Graph Neural Networks",
      "authors": [
        "Andrea Cini",
        "Ivan Marisca",
        "F. Bianchi",
        "C. Alippi"
      ],
      "published_date": "2022",
      "abstract": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 73,
      "score": 24.333333333333332,
      "summary": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden.",
      "keywords": []
    },
    "file_name": "5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf"
  },
  {
    "success": true,
    "doc_id": "65df74ddc1ce7c9d50085f5e3f934bf5",
    "summary": "In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.",
    "intriguing_abstract": "In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf",
    "citation_key": "zhang2023ann",
    "metadata": {
      "title": "Factor Graph Neural Networks",
      "authors": [
        "Zhen Zhang",
        "Mohammed Haroon Dupty",
        "Fan Wu",
        "Fan Wu"
      ],
      "published_date": "2023",
      "abstract": "In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf",
      "venue": "Journal of machine learning research",
      "citationCount": 43,
      "score": 21.5,
      "summary": "In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.",
      "keywords": []
    },
    "file_name": "5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf"
  },
  {
    "success": true,
    "doc_id": "36794c497462ec2828c45750647570a7",
    "summary": "Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform's prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Convolutional</italic> <italic>Network</italic>) for prebuilt bundle recommendation, and a BGGN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Generation</italic> <italic>Network</italic>) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user's supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77% to 23.18% and 20.90% to 64.52%, respectively. We have released the datasets and codes at this link: <uri>https://github.com/cjx0525/BGCN</uri>.",
    "intriguing_abstract": "Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform's prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Convolutional</italic> <italic>Network</italic>) for prebuilt bundle recommendation, and a BGGN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Generation</italic> <italic>Network</italic>) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user's supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77% to 23.18% and 20.90% to 64.52%, respectively. We have released the datasets and codes at this link: <uri>https://github.com/cjx0525/BGCN</uri>.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/985a47671c30e2d059c568ba8eb8e2813bab9423.pdf",
    "citation_key": "chang2023ex5",
    "metadata": {
      "title": "Bundle Recommendation and Generation With Graph Neural Networks",
      "authors": [
        "Jianxin Chang",
        "Chen Gao",
        "Xiangnan He",
        "Depeng Jin",
        "Yong Li"
      ],
      "published_date": "2023",
      "abstract": "Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform's prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Convolutional</italic> <italic>Network</italic>) for prebuilt bundle recommendation, and a BGGN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Generation</italic> <italic>Network</italic>) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user's supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77% to 23.18% and 20.90% to 64.52%, respectively. We have released the datasets and codes at this link: <uri>https://github.com/cjx0525/BGCN</uri>.",
      "file_path": "paper_data/Graph_Neural_Networks/info/985a47671c30e2d059c568ba8eb8e2813bab9423.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 43,
      "score": 21.5,
      "summary": "Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform's prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Convolutional</italic> <italic>Network</italic>) for prebuilt bundle recommendation, and a BGGN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Generation</italic> <italic>Network</italic>) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user's supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77% to 23.18% and 20.90% to 64.52%, respectively. We have released the datasets and codes at this link: <uri>https://github.com/cjx0525/BGCN</uri>.",
      "keywords": []
    },
    "file_name": "985a47671c30e2d059c568ba8eb8e2813bab9423.pdf"
  },
  {
    "success": true,
    "doc_id": "43efdbc1fa144e84a6bf7f13a925a489",
    "summary": "In recent years, we have witnessed the developments that deep learning has brought to machine learning. It has solved many\nproblems in the areas of computer vision, speech recognition, natural language processing, and various other tasks with state\nof-the-art performance. However, the data in these tasks is typically represented in Euclidean space. As technology develops,\nmore and more applications are generating data from non-Euclidean domains and representing them as graphs with complex\nrelationships and interdependencies between objects. This poses a significant challenge to deep learning algorithms. This is\nbecause, due to the uniqueness of graphs, applying deep learning to the ubiquitous graph data is not an easy task. To solve\nthe problem in non-Euclidean domains, Graph Neural Networks (GNNs) have emerged. A Graph Neural Network (GNN)\nis a neural model that captures dependencies between graphs by passing messages between graph nodes. This paper\nintroduces commonly used graph neural networks, their learning methods, and common datasets for graph neural networks.\nIt also provides an outlook on the future of Graph Neural Networks.",
    "intriguing_abstract": "In recent years, we have witnessed the developments that deep learning has brought to machine learning. It has solved many\nproblems in the areas of computer vision, speech recognition, natural language processing, and various other tasks with state\nof-the-art performance. However, the data in these tasks is typically represented in Euclidean space. As technology develops,\nmore and more applications are generating data from non-Euclidean domains and representing them as graphs with complex\nrelationships and interdependencies between objects. This poses a significant challenge to deep learning algorithms. This is\nbecause, due to the uniqueness of graphs, applying deep learning to the ubiquitous graph data is not an easy task. To solve\nthe problem in non-Euclidean domains, Graph Neural Networks (GNNs) have emerged. A Graph Neural Network (GNN)\nis a neural model that captures dependencies between graphs by passing messages between graph nodes. This paper\nintroduces commonly used graph neural networks, their learning methods, and common datasets for graph neural networks.\nIt also provides an outlook on the future of Graph Neural Networks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/9208290fd7948ed14ebe55718118c401e8396159.pdf",
    "citation_key": "wang2023zr0",
    "metadata": {
      "title": "A survey on graph neural networks",
      "authors": [
        "J. Wang"
      ],
      "published_date": "2023",
      "abstract": "In recent years, we have witnessed the developments that deep learning has brought to machine learning. It has solved many\nproblems in the areas of computer vision, speech recognition, natural language processing, and various other tasks with state\nof-the-art performance. However, the data in these tasks is typically represented in Euclidean space. As technology develops,\nmore and more applications are generating data from non-Euclidean domains and representing them as graphs with complex\nrelationships and interdependencies between objects. This poses a significant challenge to deep learning algorithms. This is\nbecause, due to the uniqueness of graphs, applying deep learning to the ubiquitous graph data is not an easy task. To solve\nthe problem in non-Euclidean domains, Graph Neural Networks (GNNs) have emerged. A Graph Neural Network (GNN)\nis a neural model that captures dependencies between graphs by passing messages between graph nodes. This paper\nintroduces commonly used graph neural networks, their learning methods, and common datasets for graph neural networks.\nIt also provides an outlook on the future of Graph Neural Networks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/9208290fd7948ed14ebe55718118c401e8396159.pdf",
      "venue": "EAI Endorsed Trans. e Learn.",
      "citationCount": 35,
      "score": 17.5,
      "summary": "In recent years, we have witnessed the developments that deep learning has brought to machine learning. It has solved many\nproblems in the areas of computer vision, speech recognition, natural language processing, and various other tasks with state\nof-the-art performance. However, the data in these tasks is typically represented in Euclidean space. As technology develops,\nmore and more applications are generating data from non-Euclidean domains and representing them as graphs with complex\nrelationships and interdependencies between objects. This poses a significant challenge to deep learning algorithms. This is\nbecause, due to the uniqueness of graphs, applying deep learning to the ubiquitous graph data is not an easy task. To solve\nthe problem in non-Euclidean domains, Graph Neural Networks (GNNs) have emerged. A Graph Neural Network (GNN)\nis a neural model that captures dependencies between graphs by passing messages between graph nodes. This paper\nintroduces commonly used graph neural networks, their learning methods, and common datasets for graph neural networks.\nIt also provides an outlook on the future of Graph Neural Networks.",
      "keywords": []
    },
    "file_name": "9208290fd7948ed14ebe55718118c401e8396159.pdf"
  },
  {
    "success": true,
    "doc_id": "b3c981d362cde04787732c60e067367d",
    "summary": "Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks' structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs' performance in eight brain network disease analysis tasks.",
    "intriguing_abstract": "Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks' structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs' performance in eight brain network disease analysis tasks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf",
    "citation_key": "zhao2022fvg",
    "metadata": {
      "title": "Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis",
      "authors": [
        "Xusheng Zhao",
        "Jia Wu",
        "Hao Peng",
        "A. Beheshti",
        "Jessica J. M. Monaghan",
        "D. McAlpine",
        "Heivet Hernández-Pérez",
        "M. Dras",
        "Qiong Dai",
        "Yangyang Li",
        "Philip S. Yu",
        "Lifang He"
      ],
      "published_date": "2022",
      "abstract": "Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks' structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs' performance in eight brain network disease analysis tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf",
      "venue": "Neural Networks",
      "citationCount": 49,
      "score": 16.333333333333332,
      "summary": "Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks' structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs' performance in eight brain network disease analysis tasks.",
      "keywords": []
    },
    "file_name": "3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf"
  },
  {
    "success": true,
    "doc_id": "b6099723e7504124449c17f72455e93a",
    "summary": "Graph Neural Networks have gained huge interest in the past few years. These powerful algorithms expanded deep learning models to non-Euclidean space and were able to achieve state of art performance in various applications including recommender systems and social networks. However, this performance is based on static graph structures assumption which limits the Graph Neural Networks performance when the data varies with time. Spatiotemporal Graph Neural Networks are extension of Graph Neural Networks that takes the time factor into account. Recently, various Spatiotemporal Graph Neural Network algorithms were proposed and achieved superior performance compared to other deep learning algorithms in several time dependent applications. This survey discusses interesting topics related to Spatiotemporal Graph Neural Networks, including algorithms, applications, and open challenges.",
    "intriguing_abstract": "Graph Neural Networks have gained huge interest in the past few years. These powerful algorithms expanded deep learning models to non-Euclidean space and were able to achieve state of art performance in various applications including recommender systems and social networks. However, this performance is based on static graph structures assumption which limits the Graph Neural Networks performance when the data varies with time. Spatiotemporal Graph Neural Networks are extension of Graph Neural Networks that takes the time factor into account. Recently, various Spatiotemporal Graph Neural Network algorithms were proposed and achieved superior performance compared to other deep learning algorithms in several time dependent applications. This survey discusses interesting topics related to Spatiotemporal Graph Neural Networks, including algorithms, applications, and open challenges.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf",
    "citation_key": "sahili2023f2x",
    "metadata": {
      "title": "Spatio-Temporal Graph Neural Networks: A Survey",
      "authors": [
        "Zahraa Al Sahili",
        "M. Awad"
      ],
      "published_date": "2023",
      "abstract": "Graph Neural Networks have gained huge interest in the past few years. These powerful algorithms expanded deep learning models to non-Euclidean space and were able to achieve state of art performance in various applications including recommender systems and social networks. However, this performance is based on static graph structures assumption which limits the Graph Neural Networks performance when the data varies with time. Spatiotemporal Graph Neural Networks are extension of Graph Neural Networks that takes the time factor into account. Recently, various Spatiotemporal Graph Neural Network algorithms were proposed and achieved superior performance compared to other deep learning algorithms in several time dependent applications. This survey discusses interesting topics related to Spatiotemporal Graph Neural Networks, including algorithms, applications, and open challenges.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf",
      "venue": "arXiv.org",
      "citationCount": 25,
      "score": 12.5,
      "summary": "Graph Neural Networks have gained huge interest in the past few years. These powerful algorithms expanded deep learning models to non-Euclidean space and were able to achieve state of art performance in various applications including recommender systems and social networks. However, this performance is based on static graph structures assumption which limits the Graph Neural Networks performance when the data varies with time. Spatiotemporal Graph Neural Networks are extension of Graph Neural Networks that takes the time factor into account. Recently, various Spatiotemporal Graph Neural Network algorithms were proposed and achieved superior performance compared to other deep learning algorithms in several time dependent applications. This survey discusses interesting topics related to Spatiotemporal Graph Neural Networks, including algorithms, applications, and open challenges.",
      "keywords": []
    },
    "file_name": "ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf"
  },
  {
    "success": true,
    "doc_id": "1e1d949679c98f6dccbc4c4f2ec8aab7",
    "summary": "We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space -- the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they ``look like'' they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.",
    "intriguing_abstract": "We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space -- the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they ``look like'' they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f42898181e56cb6fee860143c96663ed361449e0.pdf",
    "citation_key": "levie2023c1s",
    "metadata": {
      "title": "A graphon-signal analysis of graph neural networks",
      "authors": [
        "R. Levie"
      ],
      "published_date": "2023",
      "abstract": "We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space -- the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they ``look like'' they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f42898181e56cb6fee860143c96663ed361449e0.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 24,
      "score": 12.0,
      "summary": "We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space -- the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they ``look like'' they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.",
      "keywords": []
    },
    "file_name": "f42898181e56cb6fee860143c96663ed361449e0.pdf"
  },
  {
    "success": true,
    "doc_id": "505d1acc6b5890176d26b77ae3dc30ba",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/df519a15af1e83824340212477d9d356f86f15ec.pdf",
    "citation_key": "wang2024nuq",
    "metadata": {
      "title": "Incorporating syntax and semantics with dual graph neural networks for aspect-level sentiment analysis",
      "authors": [
        "Pengcheng Wang",
        "Linping Tao",
        "Mingwei Tang",
        "Liuxuan Wang",
        "Yangsheng Xu",
        "Mingfeng Zhao"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/df519a15af1e83824340212477d9d356f86f15ec.pdf",
      "venue": "Engineering applications of artificial intelligence",
      "citationCount": 20,
      "score": 20.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "df519a15af1e83824340212477d9d356f86f15ec.pdf"
  },
  {
    "success": true,
    "doc_id": "b0c87c4cc4e23050a4e7b4373acc6d18",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/181ff84051e375be829ec230c1e65439a199171c.pdf",
    "citation_key": "dong2024dx0",
    "metadata": {
      "title": "Dynamic link prediction by learning the representation of node-pair via graph neural networks",
      "authors": [
        "Hu Dong",
        "Longjie Li",
        "Dongwen Tian",
        "Yiyang Sun",
        "Yuncong Zhao"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/181ff84051e375be829ec230c1e65439a199171c.pdf",
      "venue": "Expert systems with applications",
      "citationCount": 17,
      "score": 17.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "181ff84051e375be829ec230c1e65439a199171c.pdf"
  },
  {
    "success": true,
    "doc_id": "3029a2739f707a19435f876dd4e4f88a",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cfc041534d57719d893ec5af01a7065621f7c410.pdf",
    "citation_key": "zhao2024oyr",
    "metadata": {
      "title": "Beam layout design of shear wall structures based on graph neural networks",
      "authors": [
        "Pengju Zhao",
        "Wenjie Liao",
        "Yuli Huang",
        "Xinzheng Lu"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/cfc041534d57719d893ec5af01a7065621f7c410.pdf",
      "venue": "Automation in Construction",
      "citationCount": 16,
      "score": 16.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "cfc041534d57719d893ec5af01a7065621f7c410.pdf"
  },
  {
    "success": true,
    "doc_id": "ca39de13dc1414b802be5d585b466299",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf",
    "citation_key": "chen2024h2c",
    "metadata": {
      "title": "Drug-Target Interactions Prediction Based on Signed Heterogeneous Graph Neural Networks",
      "authors": [
        "Ming Chen",
        "Yajian Jiang",
        "Xiujuan Lei",
        "Yi Pan",
        "Chunyan Ji",
        "Wei Jiang",
        "Hongkai Xiong"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf",
      "venue": "Chinese journal of electronics",
      "citationCount": 16,
      "score": 16.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf"
  },
  {
    "success": true,
    "doc_id": "b1e12e6f8d14a0a99c669bd977619c63",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a55c59163cf138d31994afc875d46997d3ef5c4b.pdf",
    "citation_key": "foroutan2024nhg",
    "metadata": {
      "title": "Deep Learning-Based Spatial-Temporal Graph Neural Networks for Price Movement Classification in Crude Oil and Precious Metal Markets",
      "authors": [
        "P. Foroutan",
        "Salim Lahmiri"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/a55c59163cf138d31994afc875d46997d3ef5c4b.pdf",
      "venue": "Machine Learning with Applications",
      "citationCount": 15,
      "score": 15.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "a55c59163cf138d31994afc875d46997d3ef5c4b.pdf"
  },
  {
    "success": true,
    "doc_id": "a6558fad460569ae7531f659738cd38a",
    "summary": "Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.",
    "intriguing_abstract": "Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3492576dae538ad34a6ecae5b631651e8ddebf92.pdf",
    "citation_key": "wander2024nnn",
    "metadata": {
      "title": "CatTSunami: Accelerating Transition State Energy Calculations with Pretrained Graph Neural Networks",
      "authors": [
        "Brook Wander",
        "Muhammed Shuaibi",
        "John R. Kitchin",
        "Zachary W. Ulissi",
        "C. L. Zitnick"
      ],
      "published_date": "2024",
      "abstract": "Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3492576dae538ad34a6ecae5b631651e8ddebf92.pdf",
      "venue": "ACS Catalysis",
      "citationCount": 14,
      "score": 14.0,
      "summary": "Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.",
      "keywords": []
    },
    "file_name": "3492576dae538ad34a6ecae5b631651e8ddebf92.pdf"
  },
  {
    "success": true,
    "doc_id": "ab6feeedf4b112cd8c7f0a95c701ff67",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf",
    "citation_key": "li20248gg",
    "metadata": {
      "title": "Homogeneous graph neural networks for third-party library recommendation",
      "authors": [
        "Duantengchuan Li",
        "Yuxuan Gao",
        "Zhihao Wang",
        "Hua Qiu",
        "Pan Liu",
        "Zhuoran Xiong",
        "Zilong Zhang"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf",
      "venue": "Information Processing & Management",
      "citationCount": 14,
      "score": 14.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf"
  },
  {
    "success": true,
    "doc_id": "6ed694b8c4028c00f4380889b30036a3",
    "summary": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
    "intriguing_abstract": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf",
    "citation_key": "duan2024que",
    "metadata": {
      "title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks",
      "authors": [
        "Yifan Duan",
        "Guibin Zhang",
        "Shilong Wang",
        "Xiaojiang Peng",
        "Ziqi Wang",
        "Junyuan Mao",
        "Hao Wu",
        "Xinke Jiang",
        "Kun Wang"
      ],
      "published_date": "2024",
      "abstract": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf",
      "venue": "arXiv.org",
      "citationCount": 13,
      "score": 13.0,
      "summary": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
      "keywords": []
    },
    "file_name": "ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf"
  },
  {
    "success": true,
    "doc_id": "c5cc358c45fc8f34fb86a4018c072ca9",
    "summary": "One way that technology is changing the legal profession is by increasing the role of neural networks in intellectual property rights (IPR). The present status of intellectual property rights might be drastically changed if neural networks were to be used to improve the efficiency, accuracy, and cost-effectiveness of copyright, patent, and trademark procedures. Neural networks have had a significant influence on several IP-related applications, such as patent analysis and search, copyright infringement detection, and trademark search. Included in the suggested method are model training, feature extraction, and pre-processing. The goal of pre-processing is to eliminate or replace irrelevant or noisy data from each tweet so that sentiment classification can proceed more effectively. Algorithms for sentiment categorization and information content analysis make up feature extraction. The training process always made use of the DGNN model. This cutting-edge approach outperforms CNN and GNN with an average accuracy of ${9 1. 4 5 \\%}$.",
    "intriguing_abstract": "One way that technology is changing the legal profession is by increasing the role of neural networks in intellectual property rights (IPR). The present status of intellectual property rights might be drastically changed if neural networks were to be used to improve the efficiency, accuracy, and cost-effectiveness of copyright, patent, and trademark procedures. Neural networks have had a significant influence on several IP-related applications, such as patent analysis and search, copyright infringement detection, and trademark search. Included in the suggested method are model training, feature extraction, and pre-processing. The goal of pre-processing is to eliminate or replace irrelevant or noisy data from each tweet so that sentiment classification can proceed more effectively. Algorithms for sentiment categorization and information content analysis make up feature extraction. The training process always made use of the DGNN model. This cutting-edge approach outperforms CNN and GNN with an average accuracy of ${9 1. 4 5 \\%}$.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5ca285d36255114938751e1787681fa17073a313.pdf",
    "citation_key": "praveen202498y",
    "metadata": {
      "title": "Enhancing Intellectual Property Rights(IPR) Transparency with Blockchain and Dual Graph Neural Networks",
      "authors": [
        "R. Praveen",
        "Aktalina Torogeldieva",
        "B. Saravanan",
        "Ajay Kumar",
        "Pushpa Rani",
        "B. P. Gajbhare"
      ],
      "published_date": "2024",
      "abstract": "One way that technology is changing the legal profession is by increasing the role of neural networks in intellectual property rights (IPR). The present status of intellectual property rights might be drastically changed if neural networks were to be used to improve the efficiency, accuracy, and cost-effectiveness of copyright, patent, and trademark procedures. Neural networks have had a significant influence on several IP-related applications, such as patent analysis and search, copyright infringement detection, and trademark search. Included in the suggested method are model training, feature extraction, and pre-processing. The goal of pre-processing is to eliminate or replace irrelevant or noisy data from each tweet so that sentiment classification can proceed more effectively. Algorithms for sentiment categorization and information content analysis make up feature extraction. The training process always made use of the DGNN model. This cutting-edge approach outperforms CNN and GNN with an average accuracy of ${9 1. 4 5 \\%}$.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5ca285d36255114938751e1787681fa17073a313.pdf",
      "venue": "2024 First International Conference on Software, Systems and Information Technology (SSITCON)",
      "citationCount": 13,
      "score": 13.0,
      "summary": "One way that technology is changing the legal profession is by increasing the role of neural networks in intellectual property rights (IPR). The present status of intellectual property rights might be drastically changed if neural networks were to be used to improve the efficiency, accuracy, and cost-effectiveness of copyright, patent, and trademark procedures. Neural networks have had a significant influence on several IP-related applications, such as patent analysis and search, copyright infringement detection, and trademark search. Included in the suggested method are model training, feature extraction, and pre-processing. The goal of pre-processing is to eliminate or replace irrelevant or noisy data from each tweet so that sentiment classification can proceed more effectively. Algorithms for sentiment categorization and information content analysis make up feature extraction. The training process always made use of the DGNN model. This cutting-edge approach outperforms CNN and GNN with an average accuracy of ${9 1. 4 5 \\%}$.",
      "keywords": []
    },
    "file_name": "5ca285d36255114938751e1787681fa17073a313.pdf"
  },
  {
    "success": true,
    "doc_id": "ad92e9e469e991009d1b924a41ff549d",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/adf1318ee484fe32d227a5084ed981eedb828c72.pdf",
    "citation_key": "wang2024p88",
    "metadata": {
      "title": "Explanatory subgraph attacks against Graph Neural Networks",
      "authors": [
        "Huiwei Wang",
        "Tianhua Liu",
        "Ziyu Sheng",
        "Huaqing Li"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/adf1318ee484fe32d227a5084ed981eedb828c72.pdf",
      "venue": "Neural Networks",
      "citationCount": 13,
      "score": 13.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "adf1318ee484fe32d227a5084ed981eedb828c72.pdf"
  },
  {
    "success": true,
    "doc_id": "1aafed497da4d94039bb8cb88e1fb1a9",
    "summary": "Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.",
    "intriguing_abstract": "Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7a42822cb3102041bad5ff7058451e35e48fd15f.pdf",
    "citation_key": "jing2024az0",
    "metadata": {
      "title": "Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation",
      "authors": [
        "Baoyu Jing",
        "Dawei Zhou",
        "Kan Ren",
        "Carl Yang"
      ],
      "published_date": "2024",
      "abstract": "Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7a42822cb3102041bad5ff7058451e35e48fd15f.pdf",
      "venue": "International Conference on Information and Knowledge Management",
      "citationCount": 13,
      "score": 13.0,
      "summary": "Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.",
      "keywords": []
    },
    "file_name": "7a42822cb3102041bad5ff7058451e35e48fd15f.pdf"
  },
  {
    "success": true,
    "doc_id": "f0d63249cd32cf0019d66db87c38b46c",
    "summary": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
    "intriguing_abstract": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf",
    "citation_key": "zhang2024370",
    "metadata": {
      "title": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?",
      "authors": [
        "Zhongjian Zhang",
        "Xiao Wang",
        "Huichi Zhou",
        "Yue Yu",
        "Mengmei Zhang",
        "Cheng Yang",
        "Chuan Shi"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
      "keywords": []
    },
    "file_name": "cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf"
  },
  {
    "success": true,
    "doc_id": "52be6e953bdc8afcfd65bd7bfee10b2f",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf",
    "citation_key": "kanatsoulis2024l6i",
    "metadata": {
      "title": "Counting Graph Substructures with Graph Neural Networks",
      "authors": [
        "Charilaos I. Kanatsoulis",
        "Alejandro Ribeiro"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 12,
      "score": 12.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf"
  },
  {
    "success": true,
    "doc_id": "712fbb92a2053e904279bff3c4d5fc80",
    "summary": "Medicine recommendation systems are designed to aid healthcare professionals by analysing a patient’s admission data to recommend safe and effective medications. These systems are categorised into two types: instance-based and longitudinal-based. Instance-based models only consider the current admission, while longitudinal models consider the patient’s medical history. Electronic Health Records are used to incorporate medical history into longitudinal models. This project proposes a novel Knowledge Graph-Driven Medicine Recommendation System using Graph Neural Networks, KGDNet, that utilises longitudinal EHR data along with ontologies and Drug-Drug Interaction knowledge to construct admission-wise clinical and medicine Knowledge Graphs for every patient. Recurrent Neural Networks are employed to model a patient’s historical data, and Graph Neural Networks are used to learn embeddings from the Knowledge Graphs. A Transformer-based Attention mechanism is then used to generate medication recommendations for the patient, considering their current clinical state, medication history, and joint medical records. The model is evaluated on the MIMIC-IV EHR data and outperforms existing methods in terms of precision, recall, F1 score, Jaccard score, and Drug-Drug Interaction control. An ablation study on our models various inputs and components to provide evidence for the importance of each component in providing the best performance. Case study is also performed to demonstrate the real-world effectiveness of KGDNet.",
    "intriguing_abstract": "Medicine recommendation systems are designed to aid healthcare professionals by analysing a patient’s admission data to recommend safe and effective medications. These systems are categorised into two types: instance-based and longitudinal-based. Instance-based models only consider the current admission, while longitudinal models consider the patient’s medical history. Electronic Health Records are used to incorporate medical history into longitudinal models. This project proposes a novel Knowledge Graph-Driven Medicine Recommendation System using Graph Neural Networks, KGDNet, that utilises longitudinal EHR data along with ontologies and Drug-Drug Interaction knowledge to construct admission-wise clinical and medicine Knowledge Graphs for every patient. Recurrent Neural Networks are employed to model a patient’s historical data, and Graph Neural Networks are used to learn embeddings from the Knowledge Graphs. A Transformer-based Attention mechanism is then used to generate medication recommendations for the patient, considering their current clinical state, medication history, and joint medical records. The model is evaluated on the MIMIC-IV EHR data and outperforms existing methods in terms of precision, recall, F1 score, Jaccard score, and Drug-Drug Interaction control. An ablation study on our models various inputs and components to provide evidence for the importance of each component in providing the best performance. Case study is also performed to demonstrate the real-world effectiveness of KGDNet.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c1fbf79a695352b906d8c980608fccb99d3366ee.pdf",
    "citation_key": "mishra2024v89",
    "metadata": {
      "title": "Knowledge graph driven medicine recommendation system using graph neural networks on longitudinal medical records",
      "authors": [
        "Rajat Mishra",
        "S. Shridevi"
      ],
      "published_date": "2024",
      "abstract": "Medicine recommendation systems are designed to aid healthcare professionals by analysing a patient’s admission data to recommend safe and effective medications. These systems are categorised into two types: instance-based and longitudinal-based. Instance-based models only consider the current admission, while longitudinal models consider the patient’s medical history. Electronic Health Records are used to incorporate medical history into longitudinal models. This project proposes a novel Knowledge Graph-Driven Medicine Recommendation System using Graph Neural Networks, KGDNet, that utilises longitudinal EHR data along with ontologies and Drug-Drug Interaction knowledge to construct admission-wise clinical and medicine Knowledge Graphs for every patient. Recurrent Neural Networks are employed to model a patient’s historical data, and Graph Neural Networks are used to learn embeddings from the Knowledge Graphs. A Transformer-based Attention mechanism is then used to generate medication recommendations for the patient, considering their current clinical state, medication history, and joint medical records. The model is evaluated on the MIMIC-IV EHR data and outperforms existing methods in terms of precision, recall, F1 score, Jaccard score, and Drug-Drug Interaction control. An ablation study on our models various inputs and components to provide evidence for the importance of each component in providing the best performance. Case study is also performed to demonstrate the real-world effectiveness of KGDNet.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c1fbf79a695352b906d8c980608fccb99d3366ee.pdf",
      "venue": "Scientific Reports",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Medicine recommendation systems are designed to aid healthcare professionals by analysing a patient’s admission data to recommend safe and effective medications. These systems are categorised into two types: instance-based and longitudinal-based. Instance-based models only consider the current admission, while longitudinal models consider the patient’s medical history. Electronic Health Records are used to incorporate medical history into longitudinal models. This project proposes a novel Knowledge Graph-Driven Medicine Recommendation System using Graph Neural Networks, KGDNet, that utilises longitudinal EHR data along with ontologies and Drug-Drug Interaction knowledge to construct admission-wise clinical and medicine Knowledge Graphs for every patient. Recurrent Neural Networks are employed to model a patient’s historical data, and Graph Neural Networks are used to learn embeddings from the Knowledge Graphs. A Transformer-based Attention mechanism is then used to generate medication recommendations for the patient, considering their current clinical state, medication history, and joint medical records. The model is evaluated on the MIMIC-IV EHR data and outperforms existing methods in terms of precision, recall, F1 score, Jaccard score, and Drug-Drug Interaction control. An ablation study on our models various inputs and components to provide evidence for the importance of each component in providing the best performance. Case study is also performed to demonstrate the real-world effectiveness of KGDNet.",
      "keywords": []
    },
    "file_name": "c1fbf79a695352b906d8c980608fccb99d3366ee.pdf"
  },
  {
    "success": true,
    "doc_id": "94257a46c85f942c91e7160154c3cf05",
    "summary": "The prediction of configurational disorder properties, such as configurational entropy and order-disorder phase transition temperature, of compound materials relies on efficient and accurate evaluations of configurational energies. Previous cluster expansion methods are not applicable to configurationally-complex material systems, including those with atomic distortions and long-range orders. In this work, we propose to leverage the versatile expressive capabilities of graph neural networks (GNNs) for efficient evaluations of configurational energies and present a workflow combining attention-based GNNs and Monte Carlo simulations to calculate the disorder properties. Using the dataset of face-centered tetragonal gold copper without and with local atomic distortions as an example, we demonstrate that the proposed data-driven framework enables the prediction of phase transition temperatures close to experimental values. We also elucidate that the variance of the energy deviations among configurations controls the prediction accuracy of disorder properties and can be used as the target loss function when training and selecting the GNN models. The work serves as a fundamental step toward a data-driven paradigm for the accelerated design of configurationally-complex functional material systems.",
    "intriguing_abstract": "The prediction of configurational disorder properties, such as configurational entropy and order-disorder phase transition temperature, of compound materials relies on efficient and accurate evaluations of configurational energies. Previous cluster expansion methods are not applicable to configurationally-complex material systems, including those with atomic distortions and long-range orders. In this work, we propose to leverage the versatile expressive capabilities of graph neural networks (GNNs) for efficient evaluations of configurational energies and present a workflow combining attention-based GNNs and Monte Carlo simulations to calculate the disorder properties. Using the dataset of face-centered tetragonal gold copper without and with local atomic distortions as an example, we demonstrate that the proposed data-driven framework enables the prediction of phase transition temperatures close to experimental values. We also elucidate that the variance of the energy deviations among configurations controls the prediction accuracy of disorder properties and can be used as the target loss function when training and selecting the GNN models. The work serves as a fundamental step toward a data-driven paradigm for the accelerated design of configurationally-complex functional material systems.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cd90ab144ca439fad38ad952d254ef2036da6d96.pdf",
    "citation_key": "fang2024p34",
    "metadata": {
      "title": "Towards accurate prediction of configurational disorder properties in materials using graph neural networks",
      "authors": [
        "Zhenyao Fang",
        "Qimin Yan"
      ],
      "published_date": "2024",
      "abstract": "The prediction of configurational disorder properties, such as configurational entropy and order-disorder phase transition temperature, of compound materials relies on efficient and accurate evaluations of configurational energies. Previous cluster expansion methods are not applicable to configurationally-complex material systems, including those with atomic distortions and long-range orders. In this work, we propose to leverage the versatile expressive capabilities of graph neural networks (GNNs) for efficient evaluations of configurational energies and present a workflow combining attention-based GNNs and Monte Carlo simulations to calculate the disorder properties. Using the dataset of face-centered tetragonal gold copper without and with local atomic distortions as an example, we demonstrate that the proposed data-driven framework enables the prediction of phase transition temperatures close to experimental values. We also elucidate that the variance of the energy deviations among configurations controls the prediction accuracy of disorder properties and can be used as the target loss function when training and selecting the GNN models. The work serves as a fundamental step toward a data-driven paradigm for the accelerated design of configurationally-complex functional material systems.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cd90ab144ca439fad38ad952d254ef2036da6d96.pdf",
      "venue": "npj Computational Materials",
      "citationCount": 12,
      "score": 12.0,
      "summary": "The prediction of configurational disorder properties, such as configurational entropy and order-disorder phase transition temperature, of compound materials relies on efficient and accurate evaluations of configurational energies. Previous cluster expansion methods are not applicable to configurationally-complex material systems, including those with atomic distortions and long-range orders. In this work, we propose to leverage the versatile expressive capabilities of graph neural networks (GNNs) for efficient evaluations of configurational energies and present a workflow combining attention-based GNNs and Monte Carlo simulations to calculate the disorder properties. Using the dataset of face-centered tetragonal gold copper without and with local atomic distortions as an example, we demonstrate that the proposed data-driven framework enables the prediction of phase transition temperatures close to experimental values. We also elucidate that the variance of the energy deviations among configurations controls the prediction accuracy of disorder properties and can be used as the target loss function when training and selecting the GNN models. The work serves as a fundamental step toward a data-driven paradigm for the accelerated design of configurationally-complex functional material systems.",
      "keywords": []
    },
    "file_name": "cd90ab144ca439fad38ad952d254ef2036da6d96.pdf"
  },
  {
    "success": true,
    "doc_id": "e324c77517f1d00ce876aa218c906fff",
    "summary": "Enhanced sampling simulations make the computational study of rare events feasible. A large family of such methods crucially depends on the definition of some collective variables (CVs) that could provide a low-dimensional representation of the relevant physics of the process. Recently, many methods have been proposed to semiautomatize the CV design by using machine learning tools to learn the variables directly from the simulation data. However, most methods are based on feedforward neural networks and require some user-defined physical descriptors. Here, we propose bypassing this step using a graph neural network to directly use the atomic coordinates as input for the CV model. This way, we achieve a fully automatic approach to CV determination that provides variables invariant under the relevant symmetries, especially the permutational one. Furthermore, we provide different analysis tools to favor the physical interpretation of the final CV. We prove the robustness of our approach using different methods from the literature for the optimization of the CV, and we prove its efficacy on several systems, including a small peptide, an ion dissociation in explicit solvent, and a simple chemical reaction.",
    "intriguing_abstract": "Enhanced sampling simulations make the computational study of rare events feasible. A large family of such methods crucially depends on the definition of some collective variables (CVs) that could provide a low-dimensional representation of the relevant physics of the process. Recently, many methods have been proposed to semiautomatize the CV design by using machine learning tools to learn the variables directly from the simulation data. However, most methods are based on feedforward neural networks and require some user-defined physical descriptors. Here, we propose bypassing this step using a graph neural network to directly use the atomic coordinates as input for the CV model. This way, we achieve a fully automatic approach to CV determination that provides variables invariant under the relevant symmetries, especially the permutational one. Furthermore, we provide different analysis tools to favor the physical interpretation of the final CV. We prove the robustness of our approach using different methods from the literature for the optimization of the CV, and we prove its efficacy on several systems, including a small peptide, an ion dissociation in explicit solvent, and a simple chemical reaction.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1697df4909875d593e1f82aeba49f2861640017a.pdf",
    "citation_key": "zhang202483k",
    "metadata": {
      "title": "Descriptor-Free Collective Variables from Geometric Graph Neural Networks.",
      "authors": [
        "Jintu Zhang",
        "Luigi Bonati",
        "Enrico Trizio",
        "Odin Zhang",
        "Yu Kang",
        "Tingjun Hou",
        "M. Parrinello"
      ],
      "published_date": "2024",
      "abstract": "Enhanced sampling simulations make the computational study of rare events feasible. A large family of such methods crucially depends on the definition of some collective variables (CVs) that could provide a low-dimensional representation of the relevant physics of the process. Recently, many methods have been proposed to semiautomatize the CV design by using machine learning tools to learn the variables directly from the simulation data. However, most methods are based on feedforward neural networks and require some user-defined physical descriptors. Here, we propose bypassing this step using a graph neural network to directly use the atomic coordinates as input for the CV model. This way, we achieve a fully automatic approach to CV determination that provides variables invariant under the relevant symmetries, especially the permutational one. Furthermore, we provide different analysis tools to favor the physical interpretation of the final CV. We prove the robustness of our approach using different methods from the literature for the optimization of the CV, and we prove its efficacy on several systems, including a small peptide, an ion dissociation in explicit solvent, and a simple chemical reaction.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1697df4909875d593e1f82aeba49f2861640017a.pdf",
      "venue": "Journal of Chemical Theory and Computation",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Enhanced sampling simulations make the computational study of rare events feasible. A large family of such methods crucially depends on the definition of some collective variables (CVs) that could provide a low-dimensional representation of the relevant physics of the process. Recently, many methods have been proposed to semiautomatize the CV design by using machine learning tools to learn the variables directly from the simulation data. However, most methods are based on feedforward neural networks and require some user-defined physical descriptors. Here, we propose bypassing this step using a graph neural network to directly use the atomic coordinates as input for the CV model. This way, we achieve a fully automatic approach to CV determination that provides variables invariant under the relevant symmetries, especially the permutational one. Furthermore, we provide different analysis tools to favor the physical interpretation of the final CV. We prove the robustness of our approach using different methods from the literature for the optimization of the CV, and we prove its efficacy on several systems, including a small peptide, an ion dissociation in explicit solvent, and a simple chemical reaction.",
      "keywords": []
    },
    "file_name": "1697df4909875d593e1f82aeba49f2861640017a.pdf"
  },
  {
    "success": true,
    "doc_id": "9ddcc2126cd0be8c7bf0ce7d752e3677",
    "summary": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
    "intriguing_abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cb2a45084f0c7bdc38271e94205603d1237945d8.pdf",
    "citation_key": "yin20241mx",
    "metadata": {
      "title": "Continuous Spiking Graph Neural Networks",
      "authors": [
        "Nan Yin",
        "Mengzhu Wang",
        "Li Shen",
        "Hitesh Laxmichand Patel",
        "Baopu Li",
        "Bin Gu",
        "Huan Xiong"
      ],
      "published_date": "2024",
      "abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cb2a45084f0c7bdc38271e94205603d1237945d8.pdf",
      "venue": "arXiv.org",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
      "keywords": []
    },
    "file_name": "cb2a45084f0c7bdc38271e94205603d1237945d8.pdf"
  },
  {
    "success": true,
    "doc_id": "bddb55bf32ab3400f1a5c83a3db55ee6",
    "summary": "Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.",
    "intriguing_abstract": "Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/21dce0407d0ee3bec185b0361593d73bb26a532e.pdf",
    "citation_key": "yan20240up",
    "metadata": {
      "title": "XGBoost-Enhanced Graph Neural Networks: A New Architecture for Heterogeneous Tabular Data",
      "authors": [
        "Liuxi Yan",
        "Yaoqun Xu"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.",
      "file_path": "paper_data/Graph_Neural_Networks/info/21dce0407d0ee3bec185b0361593d73bb26a532e.pdf",
      "venue": "Applied Sciences",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.",
      "keywords": []
    },
    "file_name": "21dce0407d0ee3bec185b0361593d73bb26a532e.pdf"
  },
  {
    "success": true,
    "doc_id": "6dea6008810a2afb65f694df299e2154",
    "summary": "Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.",
    "intriguing_abstract": "Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf",
    "citation_key": "shen2024exf",
    "metadata": {
      "title": "Graph Rewiring and Preprocessing for Graph Neural Networks Based on Effective Resistance",
      "authors": [
        "Xu Shen",
        "P. Lió",
        "Lintao Yang",
        "Ru Yuan",
        "Yuyang Zhang",
        "Chengbin Peng"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.",
      "file_path": "paper_data/Graph_Neural_Networks/info/bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.",
      "keywords": []
    },
    "file_name": "bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf"
  },
  {
    "success": true,
    "doc_id": "3c2f93669db4a423d5d6f70dba09b55b",
    "summary": "Optimizing the allocation of healthcare resources has never been easier than with the help of Graph Neural Networks (GNNs). In healthcare systems, where complications abound, GNNs may help with the allocation of resources in a way that improves both patient outcomes and operational efficiency. The goal is to use GNNs’ graph-modelling capabilities to describe healthcare networks, with hospitals, doctors, and other medical resources acting as nodes and potential transfers of patients or shared resources as edges. Using both past and present data, the goal is to optimize resource allocation by forecasting the most effective distribution patterns. The decision-making process is improved by using important methods including attention mechanisms, message-passing algorithms, and node embeddings. The accuracy and scalability of healthcare resource management are both enhanced by GNNs, which capture spatial and temporal dynamics. The general efficacy of healthcare delivery systems is improved by this method, which permits more precise forecasts and efficient use of resources. The results highlight how GNNs can change healthcare resource allocation models. From GitHub repository data the short interfering RNAs (siRNAs) and microRNAs (miRNAs) data with efficacy is calculated. Out of the 8 data collected as samples for various siRNAs the efficacy minimum value is 0.07 and the maximum value is 0.946 and in other data samples collected it is ${0. 3 8 6}$ and 0.777. For miRNAs it is 0.934 and 0.394 and in other data sample collected it is 0.738 and 0.432.",
    "intriguing_abstract": "Optimizing the allocation of healthcare resources has never been easier than with the help of Graph Neural Networks (GNNs). In healthcare systems, where complications abound, GNNs may help with the allocation of resources in a way that improves both patient outcomes and operational efficiency. The goal is to use GNNs’ graph-modelling capabilities to describe healthcare networks, with hospitals, doctors, and other medical resources acting as nodes and potential transfers of patients or shared resources as edges. Using both past and present data, the goal is to optimize resource allocation by forecasting the most effective distribution patterns. The decision-making process is improved by using important methods including attention mechanisms, message-passing algorithms, and node embeddings. The accuracy and scalability of healthcare resource management are both enhanced by GNNs, which capture spatial and temporal dynamics. The general efficacy of healthcare delivery systems is improved by this method, which permits more precise forecasts and efficient use of resources. The results highlight how GNNs can change healthcare resource allocation models. From GitHub repository data the short interfering RNAs (siRNAs) and microRNAs (miRNAs) data with efficacy is calculated. Out of the 8 data collected as samples for various siRNAs the efficacy minimum value is 0.07 and the maximum value is 0.946 and in other data samples collected it is ${0. 3 8 6}$ and 0.777. For miRNAs it is 0.934 and 0.394 and in other data sample collected it is 0.738 and 0.432.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/105191ce014da7d36d93d405c920a261dba3e937.pdf",
    "citation_key": "manivannan2024830",
    "metadata": {
      "title": "Graph Neural Networks for Resource Allocation Optimization in Healthcare Industry",
      "authors": [
        "S. K. Manivannan",
        "Venkatesh Kavididevi",
        "D. Muthukumaran",
        "R. Nandhini",
        "V. Vakula",
        "C. Srinivasan"
      ],
      "published_date": "2024",
      "abstract": "Optimizing the allocation of healthcare resources has never been easier than with the help of Graph Neural Networks (GNNs). In healthcare systems, where complications abound, GNNs may help with the allocation of resources in a way that improves both patient outcomes and operational efficiency. The goal is to use GNNs’ graph-modelling capabilities to describe healthcare networks, with hospitals, doctors, and other medical resources acting as nodes and potential transfers of patients or shared resources as edges. Using both past and present data, the goal is to optimize resource allocation by forecasting the most effective distribution patterns. The decision-making process is improved by using important methods including attention mechanisms, message-passing algorithms, and node embeddings. The accuracy and scalability of healthcare resource management are both enhanced by GNNs, which capture spatial and temporal dynamics. The general efficacy of healthcare delivery systems is improved by this method, which permits more precise forecasts and efficient use of resources. The results highlight how GNNs can change healthcare resource allocation models. From GitHub repository data the short interfering RNAs (siRNAs) and microRNAs (miRNAs) data with efficacy is calculated. Out of the 8 data collected as samples for various siRNAs the efficacy minimum value is 0.07 and the maximum value is 0.946 and in other data samples collected it is ${0. 3 8 6}$ and 0.777. For miRNAs it is 0.934 and 0.394 and in other data sample collected it is 0.738 and 0.432.",
      "file_path": "paper_data/Graph_Neural_Networks/info/105191ce014da7d36d93d405c920a261dba3e937.pdf",
      "venue": "2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Optimizing the allocation of healthcare resources has never been easier than with the help of Graph Neural Networks (GNNs). In healthcare systems, where complications abound, GNNs may help with the allocation of resources in a way that improves both patient outcomes and operational efficiency. The goal is to use GNNs’ graph-modelling capabilities to describe healthcare networks, with hospitals, doctors, and other medical resources acting as nodes and potential transfers of patients or shared resources as edges. Using both past and present data, the goal is to optimize resource allocation by forecasting the most effective distribution patterns. The decision-making process is improved by using important methods including attention mechanisms, message-passing algorithms, and node embeddings. The accuracy and scalability of healthcare resource management are both enhanced by GNNs, which capture spatial and temporal dynamics. The general efficacy of healthcare delivery systems is improved by this method, which permits more precise forecasts and efficient use of resources. The results highlight how GNNs can change healthcare resource allocation models. From GitHub repository data the short interfering RNAs (siRNAs) and microRNAs (miRNAs) data with efficacy is calculated. Out of the 8 data collected as samples for various siRNAs the efficacy minimum value is 0.07 and the maximum value is 0.946 and in other data samples collected it is ${0. 3 8 6}$ and 0.777. For miRNAs it is 0.934 and 0.394 and in other data sample collected it is 0.738 and 0.432.",
      "keywords": []
    },
    "file_name": "105191ce014da7d36d93d405c920a261dba3e937.pdf"
  },
  {
    "success": true,
    "doc_id": "7d1cab0e7e833096274d8de8cb5cacbf",
    "summary": "As a way to alleviate the information overload problem arisen with the development of the internet, recommender systems receive a lot of attention from academia and industry. Due to its superiority in graph data, graph neural networks are widely adopted in recommender systems. This survey offers a comprehensive review of the latest research and innovative approaches in GNN-based recommender systems. This survey introduces a new taxonomy by the construction of GNN models and explores the challenges these models face. This paper also discusses new approaches, i.e., using social graphs and knowledge graphs as side information, and evaluates their strengths and limitations. Finally, this paper suggests some potential directions for future research in this field.",
    "intriguing_abstract": "As a way to alleviate the information overload problem arisen with the development of the internet, recommender systems receive a lot of attention from academia and industry. Due to its superiority in graph data, graph neural networks are widely adopted in recommender systems. This survey offers a comprehensive review of the latest research and innovative approaches in GNN-based recommender systems. This survey introduces a new taxonomy by the construction of GNN models and explores the challenges these models face. This paper also discusses new approaches, i.e., using social graphs and knowledge graphs as side information, and evaluates their strengths and limitations. Finally, this paper suggests some potential directions for future research in this field.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf",
    "citation_key": "he202455s",
    "metadata": {
      "title": "Graph neural networks in recommender systems",
      "authors": [
        "Xingyang He"
      ],
      "published_date": "2024",
      "abstract": "As a way to alleviate the information overload problem arisen with the development of the internet, recommender systems receive a lot of attention from academia and industry. Due to its superiority in graph data, graph neural networks are widely adopted in recommender systems. This survey offers a comprehensive review of the latest research and innovative approaches in GNN-based recommender systems. This survey introduces a new taxonomy by the construction of GNN models and explores the challenges these models face. This paper also discusses new approaches, i.e., using social graphs and knowledge graphs as side information, and evaluates their strengths and limitations. Finally, this paper suggests some potential directions for future research in this field.",
      "file_path": "paper_data/Graph_Neural_Networks/info/10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf",
      "venue": "Applied and Computational Engineering",
      "citationCount": 12,
      "score": 12.0,
      "summary": "As a way to alleviate the information overload problem arisen with the development of the internet, recommender systems receive a lot of attention from academia and industry. Due to its superiority in graph data, graph neural networks are widely adopted in recommender systems. This survey offers a comprehensive review of the latest research and innovative approaches in GNN-based recommender systems. This survey introduces a new taxonomy by the construction of GNN models and explores the challenges these models face. This paper also discusses new approaches, i.e., using social graphs and knowledge graphs as side information, and evaluates their strengths and limitations. Finally, this paper suggests some potential directions for future research in this field.",
      "keywords": []
    },
    "file_name": "10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf"
  },
  {
    "success": true,
    "doc_id": "3b33ebace6224e778a5d942f2c1aad7b",
    "summary": "Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.",
    "intriguing_abstract": "Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7779b880700e9e3495557e076d60594d18d69277.pdf",
    "citation_key": "zhao2024qw6",
    "metadata": {
      "title": "A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks",
      "authors": [
        "Zhe Zhao",
        "Pengkun Wang",
        "Haibin Wen",
        "Yudong Zhang",
        "Zhengyang Zhou",
        "Yang Wang"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7779b880700e9e3495557e076d60594d18d69277.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.",
      "keywords": []
    },
    "file_name": "7779b880700e9e3495557e076d60594d18d69277.pdf"
  },
  {
    "success": true,
    "doc_id": "38377a662b69387ef8f87e1f6a08ca47",
    "summary": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
    "intriguing_abstract": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf",
    "citation_key": "yan2024ikq",
    "metadata": {
      "title": "Investigation of Customized Medical Decision Algorithms Utilizing Graph Neural Networks",
      "authors": [
        "Yafeng Yan",
        "Shuyao He",
        "Zhou Yu",
        "Jiajie Yuan",
        "Ziang Liu",
        "Yan Chen"
      ],
      "published_date": "2024",
      "abstract": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
      "file_path": "paper_data/Graph_Neural_Networks/info/599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf",
      "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
      "citationCount": 12,
      "score": 12.0,
      "summary": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
      "keywords": []
    },
    "file_name": "599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf"
  },
  {
    "success": true,
    "doc_id": "0bfd37feb7951707f7caa15f5444ac97",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/687abbb274492f95b2c0fe82137c009754456d4c.pdf",
    "citation_key": "xia2024xc9",
    "metadata": {
      "title": "GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "authors": [
        "Zaishuo Xia",
        "Han Yang",
        "Binghui Wang",
        "Jinyuan Jia"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/687abbb274492f95b2c0fe82137c009754456d4c.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 12,
      "score": 12.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "687abbb274492f95b2c0fe82137c009754456d4c.pdf"
  },
  {
    "success": true,
    "doc_id": "793de67aae86b2b728876cc4f11c4f6c",
    "summary": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
    "intriguing_abstract": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3911024df853ccf11138d35835572ce863df51bf.pdf",
    "citation_key": "zhou2024t2r",
    "metadata": {
      "title": "Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction",
      "authors": [
        "Yicheng Zhou",
        "P. Wang",
        "Hao Dong",
        "Denghui Zhang",
        "Dingqi Yang",
        "Yanjie Fu",
        "Pengyang Wang"
      ],
      "published_date": "2024",
      "abstract": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3911024df853ccf11138d35835572ce863df51bf.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
      "keywords": []
    },
    "file_name": "3911024df853ccf11138d35835572ce863df51bf.pdf"
  },
  {
    "success": true,
    "doc_id": "9bea277f36d4c365d9dba58ed5672bd2",
    "summary": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",
    "intriguing_abstract": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf",
    "citation_key": "lu2024eu9",
    "metadata": {
      "title": "GOAt: Explaining Graph Neural Networks via Graph Output Attribution",
      "authors": [
        "Shengyao Lu",
        "Keith G. Mills",
        "Jiao He",
        "Bang Liu",
        "Di Niu"
      ],
      "published_date": "2024",
      "abstract": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",
      "keywords": []
    },
    "file_name": "cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf"
  },
  {
    "success": true,
    "doc_id": "af71646620c77f921a340740df402801",
    "summary": "Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes -- a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes -- a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf",
    "citation_key": "wang2024cb8",
    "metadata": {
      "title": "A Manifold Perspective on the Statistical Generalization of Graph Neural Networks",
      "authors": [
        "Zhiyang Wang",
        "J. Cerviño",
        "Alejandro Ribeiro"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes -- a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/info/774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf",
      "venue": "arXiv.org",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes -- a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.",
      "keywords": []
    },
    "file_name": "774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf"
  },
  {
    "success": true,
    "doc_id": "598769519e617466769d4feba197f1c5",
    "summary": "With the advancement of 3D sensing technologies, point clouds are gradually becoming the main type of data representation in applications such as autonomous driving, robotics, and augmented reality. Nevertheless, the irregularity inherent in point clouds presents numerous challenges for traditional deep learning frameworks. Graph neural networks (GNNs) have demonstrated their tremendous potential in processing graph-structured data and are widely applied in various domains including social media data analysis, molecular structure calculation, and computer vision. GNNs, with their capability to handle non-Euclidean data, offer a novel approach for addressing these challenges. Additionally, drawing inspiration from the achievements of transformers in natural language processing, graph transformers have propelled models towards global awareness, overcoming the limitations of local aggregation mechanisms inherent in early GNN architectures. This paper provides a comprehensive review of GNNs and graph-based methods in point cloud applications, adopting a task-oriented perspective to analyze this field. We categorize GNN methods for point clouds based on fundamental tasks, such as segmentation, classification, object detection, registration, and other related tasks. For each category, we summarize the existing mainstream methods, conduct a comprehensive analysis of their performance on various datasets, and discuss the development trends and future prospects of graph-based methods.",
    "intriguing_abstract": "With the advancement of 3D sensing technologies, point clouds are gradually becoming the main type of data representation in applications such as autonomous driving, robotics, and augmented reality. Nevertheless, the irregularity inherent in point clouds presents numerous challenges for traditional deep learning frameworks. Graph neural networks (GNNs) have demonstrated their tremendous potential in processing graph-structured data and are widely applied in various domains including social media data analysis, molecular structure calculation, and computer vision. GNNs, with their capability to handle non-Euclidean data, offer a novel approach for addressing these challenges. Additionally, drawing inspiration from the achievements of transformers in natural language processing, graph transformers have propelled models towards global awareness, overcoming the limitations of local aggregation mechanisms inherent in early GNN architectures. This paper provides a comprehensive review of GNNs and graph-based methods in point cloud applications, adopting a task-oriented perspective to analyze this field. We categorize GNN methods for point clouds based on fundamental tasks, such as segmentation, classification, object detection, registration, and other related tasks. For each category, we summarize the existing mainstream methods, conduct a comprehensive analysis of their performance on various datasets, and discuss the development trends and future prospects of graph-based methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/e49178ea82233947837c135ec303852dc776dbde.pdf",
    "citation_key": "li2024yyl",
    "metadata": {
      "title": "Graph Neural Networks in Point Clouds: A Survey",
      "authors": [
        "Dilong Li",
        "Chenghui Lu",
        "Zi-xing Chen",
        "Jianlong Guan",
        "Jing Zhao",
        "Jixiang Du"
      ],
      "published_date": "2024",
      "abstract": "With the advancement of 3D sensing technologies, point clouds are gradually becoming the main type of data representation in applications such as autonomous driving, robotics, and augmented reality. Nevertheless, the irregularity inherent in point clouds presents numerous challenges for traditional deep learning frameworks. Graph neural networks (GNNs) have demonstrated their tremendous potential in processing graph-structured data and are widely applied in various domains including social media data analysis, molecular structure calculation, and computer vision. GNNs, with their capability to handle non-Euclidean data, offer a novel approach for addressing these challenges. Additionally, drawing inspiration from the achievements of transformers in natural language processing, graph transformers have propelled models towards global awareness, overcoming the limitations of local aggregation mechanisms inherent in early GNN architectures. This paper provides a comprehensive review of GNNs and graph-based methods in point cloud applications, adopting a task-oriented perspective to analyze this field. We categorize GNN methods for point clouds based on fundamental tasks, such as segmentation, classification, object detection, registration, and other related tasks. For each category, we summarize the existing mainstream methods, conduct a comprehensive analysis of their performance on various datasets, and discuss the development trends and future prospects of graph-based methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/e49178ea82233947837c135ec303852dc776dbde.pdf",
      "venue": "Remote Sensing",
      "citationCount": 11,
      "score": 11.0,
      "summary": "With the advancement of 3D sensing technologies, point clouds are gradually becoming the main type of data representation in applications such as autonomous driving, robotics, and augmented reality. Nevertheless, the irregularity inherent in point clouds presents numerous challenges for traditional deep learning frameworks. Graph neural networks (GNNs) have demonstrated their tremendous potential in processing graph-structured data and are widely applied in various domains including social media data analysis, molecular structure calculation, and computer vision. GNNs, with their capability to handle non-Euclidean data, offer a novel approach for addressing these challenges. Additionally, drawing inspiration from the achievements of transformers in natural language processing, graph transformers have propelled models towards global awareness, overcoming the limitations of local aggregation mechanisms inherent in early GNN architectures. This paper provides a comprehensive review of GNNs and graph-based methods in point cloud applications, adopting a task-oriented perspective to analyze this field. We categorize GNN methods for point clouds based on fundamental tasks, such as segmentation, classification, object detection, registration, and other related tasks. For each category, we summarize the existing mainstream methods, conduct a comprehensive analysis of their performance on various datasets, and discuss the development trends and future prospects of graph-based methods.",
      "keywords": []
    },
    "file_name": "e49178ea82233947837c135ec303852dc776dbde.pdf"
  },
  {
    "success": true,
    "doc_id": "a596efe1d2854b81f7db4173599288ba",
    "summary": "Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.",
    "intriguing_abstract": "Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf",
    "citation_key": "castroospina2024iy2",
    "metadata": {
      "title": "Graph-Based Audio Classification Using Pre-Trained Models and Graph Neural Networks",
      "authors": [
        "A. Castro-Ospina",
        "M. Solarte-Sanchez",
        "L. Vega-Escobar",
        "C. Isaza",
        "J. D. Martínez-Vargas"
      ],
      "published_date": "2024",
      "abstract": "Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf",
      "venue": "Italian National Conference on Sensors",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.",
      "keywords": []
    },
    "file_name": "0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf"
  },
  {
    "success": true,
    "doc_id": "911f8ec9df60edbd1e5dbcac1be0420e",
    "summary": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/046f6abdbf63fbb80d831102e7889c6801ad3545.pdf",
    "citation_key": "zhao2024g5p",
    "metadata": {
      "title": "Conformalized Link Prediction on Graph Neural Networks",
      "authors": [
        "Tianyi Zhao",
        "Jian Kang",
        "Lu Cheng"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/046f6abdbf63fbb80d831102e7889c6801ad3545.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.",
      "keywords": []
    },
    "file_name": "046f6abdbf63fbb80d831102e7889c6801ad3545.pdf"
  },
  {
    "success": true,
    "doc_id": "7f739ef03e3eb7d9e2ad786d5981f53d",
    "summary": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
    "intriguing_abstract": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf",
    "citation_key": "duan2024efz",
    "metadata": {
      "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
      "authors": [
        "Wei Duan",
        "Jie Lu",
        "Yu Guang Wang",
        "Junyu Xuan"
      ],
      "published_date": "2024",
      "abstract": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
      "file_path": "paper_data/Graph_Neural_Networks/info/fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf",
      "venue": "Trans. Mach. Learn. Res.",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
      "keywords": []
    },
    "file_name": "fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf"
  },
  {
    "success": true,
    "doc_id": "a242376cbc88a318ec1f0da51f504cec",
    "summary": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs.",
    "intriguing_abstract": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cfff81fc166668790f4099cebd785cdd20f25b6d.pdf",
    "citation_key": "luo2024h2k",
    "metadata": {
      "title": "Graph Neural Networks for Brain Graph Learning: A Survey",
      "authors": [
        "Xuexiong Luo",
        "Jia Wu",
        "Jian Yang",
        "Shan Xue",
        "Amin Beheshti",
        "Quan Z. Sheng",
        "David Mcalpine",
        "P. Sowman",
        "Alexis Giral",
        "Philip S. Yu"
      ],
      "published_date": "2024",
      "abstract": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cfff81fc166668790f4099cebd785cdd20f25b6d.pdf",
      "venue": "International Joint Conference on Artificial Intelligence",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs.",
      "keywords": []
    },
    "file_name": "cfff81fc166668790f4099cebd785cdd20f25b6d.pdf"
  },
  {
    "success": true,
    "doc_id": "2b2399aba949c48e43e9528d80ac3246",
    "summary": "Understanding the pharmacokinetics, safety and efficacy of candidate drugs is crucial for their success. One key aspect is the characterization of absorption, distribution, metabolism, excretion and toxicity (ADMET) properties, which require early assessment in the drug discovery and development process. This study aims to present an innovative approach for predicting ADMET properties using attention-based graph neural networks (GNNs). The model utilizes a graph-based representation of molecules directly derived from Simplified Molecular Input Line Entry System (SMILE) notation. Information is processed sequentially, from substructures to the whole molecule, employing a bottom-up approach. The developed GNN is tested and compared with existing approaches using six benchmark datasets and by encompassing regression (lipophilicity and aqueous solubility) and classification (CYP2C9, CYP2C19, CYP2D6 and CYP3A4 inhibition) tasks. Results show the effectiveness of our model, which bypasses the computationally expensive retrieval and selection of molecular descriptors. This approach provides a valuable tool for high-throughput screening, facilitating early assessment of ADMET properties and enhancing the likelihood of drug success in the development pipeline.",
    "intriguing_abstract": "Understanding the pharmacokinetics, safety and efficacy of candidate drugs is crucial for their success. One key aspect is the characterization of absorption, distribution, metabolism, excretion and toxicity (ADMET) properties, which require early assessment in the drug discovery and development process. This study aims to present an innovative approach for predicting ADMET properties using attention-based graph neural networks (GNNs). The model utilizes a graph-based representation of molecules directly derived from Simplified Molecular Input Line Entry System (SMILE) notation. Information is processed sequentially, from substructures to the whole molecule, employing a bottom-up approach. The developed GNN is tested and compared with existing approaches using six benchmark datasets and by encompassing regression (lipophilicity and aqueous solubility) and classification (CYP2C9, CYP2C19, CYP2D6 and CYP3A4 inhibition) tasks. Results show the effectiveness of our model, which bypasses the computationally expensive retrieval and selection of molecular descriptors. This approach provides a valuable tool for high-throughput screening, facilitating early assessment of ADMET properties and enhancing the likelihood of drug success in the development pipeline.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf",
    "citation_key": "carlo2024a3g",
    "metadata": {
      "title": "Predicting ADMET Properties from Molecule SMILE: A Bottom-Up Approach Using Attention-Based Graph Neural Networks",
      "authors": [
        "Alessandro De Carlo",
        "D. Ronchi",
        "Marco Piastra",
        "E. Tosca",
        "P. Magni"
      ],
      "published_date": "2024",
      "abstract": "Understanding the pharmacokinetics, safety and efficacy of candidate drugs is crucial for their success. One key aspect is the characterization of absorption, distribution, metabolism, excretion and toxicity (ADMET) properties, which require early assessment in the drug discovery and development process. This study aims to present an innovative approach for predicting ADMET properties using attention-based graph neural networks (GNNs). The model utilizes a graph-based representation of molecules directly derived from Simplified Molecular Input Line Entry System (SMILE) notation. Information is processed sequentially, from substructures to the whole molecule, employing a bottom-up approach. The developed GNN is tested and compared with existing approaches using six benchmark datasets and by encompassing regression (lipophilicity and aqueous solubility) and classification (CYP2C9, CYP2C19, CYP2D6 and CYP3A4 inhibition) tasks. Results show the effectiveness of our model, which bypasses the computationally expensive retrieval and selection of molecular descriptors. This approach provides a valuable tool for high-throughput screening, facilitating early assessment of ADMET properties and enhancing the likelihood of drug success in the development pipeline.",
      "file_path": "paper_data/Graph_Neural_Networks/info/01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf",
      "venue": "Pharmaceutics",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Understanding the pharmacokinetics, safety and efficacy of candidate drugs is crucial for their success. One key aspect is the characterization of absorption, distribution, metabolism, excretion and toxicity (ADMET) properties, which require early assessment in the drug discovery and development process. This study aims to present an innovative approach for predicting ADMET properties using attention-based graph neural networks (GNNs). The model utilizes a graph-based representation of molecules directly derived from Simplified Molecular Input Line Entry System (SMILE) notation. Information is processed sequentially, from substructures to the whole molecule, employing a bottom-up approach. The developed GNN is tested and compared with existing approaches using six benchmark datasets and by encompassing regression (lipophilicity and aqueous solubility) and classification (CYP2C9, CYP2C19, CYP2D6 and CYP3A4 inhibition) tasks. Results show the effectiveness of our model, which bypasses the computationally expensive retrieval and selection of molecular descriptors. This approach provides a valuable tool for high-throughput screening, facilitating early assessment of ADMET properties and enhancing the likelihood of drug success in the development pipeline.",
      "keywords": []
    },
    "file_name": "01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf"
  },
  {
    "success": true,
    "doc_id": "159c8db32446ae3f8e20961193b53114",
    "summary": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
    "intriguing_abstract": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf",
    "citation_key": "zandi2024dgs",
    "metadata": {
      "title": "Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction",
      "authors": [
        "Sahab Zandi",
        "Kamesh Korangi",
        "Mar'ia 'Oskarsd'ottir",
        "Christophe Mues",
        "Cristi'an Bravo"
      ],
      "published_date": "2024",
      "abstract": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf",
      "venue": "European Journal of Operational Research",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
      "keywords": []
    },
    "file_name": "0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf"
  },
  {
    "success": true,
    "doc_id": "027f37e0fec4622df6768ed03145a125",
    "summary": "Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.",
    "intriguing_abstract": "Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/df2701c0fabf50b511182a287d112dfcc84c59b3.pdf",
    "citation_key": "zhao2024aer",
    "metadata": {
      "title": "Effective Fault Scenario Identification for Communication Networks via Knowledge-Enhanced Graph Neural Networks",
      "authors": [
        "Haihong Zhao",
        "Bo Yang",
        "Jiaxu Cui",
        "Qianli Xing",
        "Jiaxing Shen",
        "Fujin Zhu",
        "Jiannong Cao"
      ],
      "published_date": "2024",
      "abstract": "Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.",
      "file_path": "paper_data/Graph_Neural_Networks/info/df2701c0fabf50b511182a287d112dfcc84c59b3.pdf",
      "venue": "IEEE Transactions on Mobile Computing",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.",
      "keywords": []
    },
    "file_name": "df2701c0fabf50b511182a287d112dfcc84c59b3.pdf"
  },
  {
    "success": true,
    "doc_id": "bb9dd47acb9dff25bc4a4c0cfd821172",
    "summary": "Introduction In recent years, graph neural network has been extensively applied to drug discovery research. Although researchers have made significant progress in this field, there is less research on bibliometrics. The purpose of this study is to conduct a comprehensive bibliometric analysis of graph neural network applications in drug discovery in order to identify current research hotspots and trends, as well as serve as a reference for future research. Methods Publications from 2017 to 2023 about the application of graph neural network in drug discovery were collected from the Web of Science Core Collection. Bibliometrix, VOSviewer, and Citespace were mainly used for bibliometric studies. Results and Discussion In this paper, a total of 652 papers from 48 countries/regions were included. Research interest in this field is continuously increasing. China and the United States have a significant advantage in terms of funding, the number of publications, and collaborations with other institutions and countries. Although some cooperation networks have been formed in this field, extensive worldwide cooperation still needs to be strengthened. The results of the keyword analysis clarified that graph neural network has primarily been applied to drug-target interaction, drug repurposing, and drug-drug interaction, while graph convolutional neural network and its related optimization methods are currently the core algorithms in this field. Data availability and ethical supervision, balancing computing resources, and developing novel graph neural network models with better interpretability are the key technical issues currently faced. This paper analyzes the current state, hot spots, and trends of graph neural network applications in drug discovery through bibliometric approaches, as well as the current issues and challenges in this field. These findings provide researchers with valuable insights on the current status and future directions of this field.",
    "intriguing_abstract": "Introduction In recent years, graph neural network has been extensively applied to drug discovery research. Although researchers have made significant progress in this field, there is less research on bibliometrics. The purpose of this study is to conduct a comprehensive bibliometric analysis of graph neural network applications in drug discovery in order to identify current research hotspots and trends, as well as serve as a reference for future research. Methods Publications from 2017 to 2023 about the application of graph neural network in drug discovery were collected from the Web of Science Core Collection. Bibliometrix, VOSviewer, and Citespace were mainly used for bibliometric studies. Results and Discussion In this paper, a total of 652 papers from 48 countries/regions were included. Research interest in this field is continuously increasing. China and the United States have a significant advantage in terms of funding, the number of publications, and collaborations with other institutions and countries. Although some cooperation networks have been formed in this field, extensive worldwide cooperation still needs to be strengthened. The results of the keyword analysis clarified that graph neural network has primarily been applied to drug-target interaction, drug repurposing, and drug-drug interaction, while graph convolutional neural network and its related optimization methods are currently the core algorithms in this field. Data availability and ethical supervision, balancing computing resources, and developing novel graph neural network models with better interpretability are the key technical issues currently faced. This paper analyzes the current state, hot spots, and trends of graph neural network applications in drug discovery through bibliometric approaches, as well as the current issues and challenges in this field. These findings provide researchers with valuable insights on the current status and future directions of this field.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf",
    "citation_key": "yao2024pyk",
    "metadata": {
      "title": "Knowledge mapping of graph neural networks for drug discovery: a bibliometric and visualized analysis",
      "authors": [
        "Rufan Yao",
        "Zhenhua Shen",
        "Xinyi Xu",
        "Guixia Ling",
        "Rongwu Xiang",
        "Tingyan Song",
        "Fei Zhai",
        "Yuxuan Zhai"
      ],
      "published_date": "2024",
      "abstract": "Introduction In recent years, graph neural network has been extensively applied to drug discovery research. Although researchers have made significant progress in this field, there is less research on bibliometrics. The purpose of this study is to conduct a comprehensive bibliometric analysis of graph neural network applications in drug discovery in order to identify current research hotspots and trends, as well as serve as a reference for future research. Methods Publications from 2017 to 2023 about the application of graph neural network in drug discovery were collected from the Web of Science Core Collection. Bibliometrix, VOSviewer, and Citespace were mainly used for bibliometric studies. Results and Discussion In this paper, a total of 652 papers from 48 countries/regions were included. Research interest in this field is continuously increasing. China and the United States have a significant advantage in terms of funding, the number of publications, and collaborations with other institutions and countries. Although some cooperation networks have been formed in this field, extensive worldwide cooperation still needs to be strengthened. The results of the keyword analysis clarified that graph neural network has primarily been applied to drug-target interaction, drug repurposing, and drug-drug interaction, while graph convolutional neural network and its related optimization methods are currently the core algorithms in this field. Data availability and ethical supervision, balancing computing resources, and developing novel graph neural network models with better interpretability are the key technical issues currently faced. This paper analyzes the current state, hot spots, and trends of graph neural network applications in drug discovery through bibliometric approaches, as well as the current issues and challenges in this field. These findings provide researchers with valuable insights on the current status and future directions of this field.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf",
      "venue": "Frontiers in Pharmacology",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Introduction In recent years, graph neural network has been extensively applied to drug discovery research. Although researchers have made significant progress in this field, there is less research on bibliometrics. The purpose of this study is to conduct a comprehensive bibliometric analysis of graph neural network applications in drug discovery in order to identify current research hotspots and trends, as well as serve as a reference for future research. Methods Publications from 2017 to 2023 about the application of graph neural network in drug discovery were collected from the Web of Science Core Collection. Bibliometrix, VOSviewer, and Citespace were mainly used for bibliometric studies. Results and Discussion In this paper, a total of 652 papers from 48 countries/regions were included. Research interest in this field is continuously increasing. China and the United States have a significant advantage in terms of funding, the number of publications, and collaborations with other institutions and countries. Although some cooperation networks have been formed in this field, extensive worldwide cooperation still needs to be strengthened. The results of the keyword analysis clarified that graph neural network has primarily been applied to drug-target interaction, drug repurposing, and drug-drug interaction, while graph convolutional neural network and its related optimization methods are currently the core algorithms in this field. Data availability and ethical supervision, balancing computing resources, and developing novel graph neural network models with better interpretability are the key technical issues currently faced. This paper analyzes the current state, hot spots, and trends of graph neural network applications in drug discovery through bibliometric approaches, as well as the current issues and challenges in this field. These findings provide researchers with valuable insights on the current status and future directions of this field.",
      "keywords": []
    },
    "file_name": "1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf"
  },
  {
    "success": true,
    "doc_id": "4061b741a7657dbf6b1e415d2d5e1863",
    "summary": "In drug discovery, the search for new and effective medications is often hindered by concerns about toxicity. Numerous promising molecules fail to pass the later phases of drug development due to strict toxicity assessments. This challenge significantly increases the cost, time, and human effort needed to discover new therapeutic molecules. Additionally, a considerable number of drugs already on the market have been withdrawn or re-evaluated because of their unwanted side effects. Among the various types of toxicity, drug-induced heart damage is a severe adverse effect commonly associated with several medications, especially those used in cancer treatments. Although a number of computational approaches have been proposed to identify the cardiotoxicity of molecules, the performance and interpretability of the existing approaches are limited. In our study, we proposed a more effective computational framework to predict the cardiotoxicity of molecules using an attention-based graph neural network. Experimental results indicated that the proposed framework outperformed the other methods. The stability of the model was also confirmed by our experiments. To assist researchers in evaluating the cardiotoxicity of molecules, we have developed an easy-to-use online web server that incorporates our model.",
    "intriguing_abstract": "In drug discovery, the search for new and effective medications is often hindered by concerns about toxicity. Numerous promising molecules fail to pass the later phases of drug development due to strict toxicity assessments. This challenge significantly increases the cost, time, and human effort needed to discover new therapeutic molecules. Additionally, a considerable number of drugs already on the market have been withdrawn or re-evaluated because of their unwanted side effects. Among the various types of toxicity, drug-induced heart damage is a severe adverse effect commonly associated with several medications, especially those used in cancer treatments. Although a number of computational approaches have been proposed to identify the cardiotoxicity of molecules, the performance and interpretability of the existing approaches are limited. In our study, we proposed a more effective computational framework to predict the cardiotoxicity of molecules using an attention-based graph neural network. Experimental results indicated that the proposed framework outperformed the other methods. The stability of the model was also confirmed by our experiments. To assist researchers in evaluating the cardiotoxicity of molecules, we have developed an easy-to-use online web server that incorporates our model.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf",
    "citation_key": "vinh20243q3",
    "metadata": {
      "title": "Predicting Cardiotoxicity of Molecules Using Attention-Based Graph Neural Networks",
      "authors": [
        "Tuan Vinh",
        "Loc Nguyen",
        "Quang H. Trinh",
        "T. Nguyen-Vo",
        "Binh P. Nguyen"
      ],
      "published_date": "2024",
      "abstract": "In drug discovery, the search for new and effective medications is often hindered by concerns about toxicity. Numerous promising molecules fail to pass the later phases of drug development due to strict toxicity assessments. This challenge significantly increases the cost, time, and human effort needed to discover new therapeutic molecules. Additionally, a considerable number of drugs already on the market have been withdrawn or re-evaluated because of their unwanted side effects. Among the various types of toxicity, drug-induced heart damage is a severe adverse effect commonly associated with several medications, especially those used in cancer treatments. Although a number of computational approaches have been proposed to identify the cardiotoxicity of molecules, the performance and interpretability of the existing approaches are limited. In our study, we proposed a more effective computational framework to predict the cardiotoxicity of molecules using an attention-based graph neural network. Experimental results indicated that the proposed framework outperformed the other methods. The stability of the model was also confirmed by our experiments. To assist researchers in evaluating the cardiotoxicity of molecules, we have developed an easy-to-use online web server that incorporates our model.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf",
      "venue": "Journal of Chemical Information and Modeling",
      "citationCount": 11,
      "score": 11.0,
      "summary": "In drug discovery, the search for new and effective medications is often hindered by concerns about toxicity. Numerous promising molecules fail to pass the later phases of drug development due to strict toxicity assessments. This challenge significantly increases the cost, time, and human effort needed to discover new therapeutic molecules. Additionally, a considerable number of drugs already on the market have been withdrawn or re-evaluated because of their unwanted side effects. Among the various types of toxicity, drug-induced heart damage is a severe adverse effect commonly associated with several medications, especially those used in cancer treatments. Although a number of computational approaches have been proposed to identify the cardiotoxicity of molecules, the performance and interpretability of the existing approaches are limited. In our study, we proposed a more effective computational framework to predict the cardiotoxicity of molecules using an attention-based graph neural network. Experimental results indicated that the proposed framework outperformed the other methods. The stability of the model was also confirmed by our experiments. To assist researchers in evaluating the cardiotoxicity of molecules, we have developed an easy-to-use online web server that incorporates our model.",
      "keywords": []
    },
    "file_name": "7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf"
  },
  {
    "success": true,
    "doc_id": "96d54d3e82af93c3e3f64c6be0387796",
    "summary": "Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.",
    "intriguing_abstract": "Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/326430bd401c2ac820fc08a0a198ceacf1cde506.pdf",
    "citation_key": "ashraf202443e",
    "metadata": {
      "title": "Physics-Informed Graph Neural Networks for Water Distribution Systems",
      "authors": [
        "Inaam Ashraf",
        "Janine Strotherm",
        "L. Hermes",
        "Barbara Hammer"
      ],
      "published_date": "2024",
      "abstract": "Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.",
      "file_path": "paper_data/Graph_Neural_Networks/info/326430bd401c2ac820fc08a0a198ceacf1cde506.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.",
      "keywords": []
    },
    "file_name": "326430bd401c2ac820fc08a0a198ceacf1cde506.pdf"
  },
  {
    "success": true,
    "doc_id": "5e5ed526672327a3b85388c149c82d4f",
    "summary": "Identifying and discovering druggable protein binding sites is an important early step in computer-aided drug discovery, but it remains a difficult task where most campaigns rely on a priori knowledge of binding sites from experiments. Here, we present a binding site prediction method called Graph Attention Site Prediction (GrASP) and re-evaluate assumptions in nearly every step in the site prediction workflow from data set preparation to model evaluation. GrASP is able to achieve state-of-the-art performance at recovering binding sites in PDB structures while maintaining a high degree of precision which will minimize wasted computation in downstream tasks such as docking and free energy perturbation.",
    "intriguing_abstract": "Identifying and discovering druggable protein binding sites is an important early step in computer-aided drug discovery, but it remains a difficult task where most campaigns rely on a priori knowledge of binding sites from experiments. Here, we present a binding site prediction method called Graph Attention Site Prediction (GrASP) and re-evaluate assumptions in nearly every step in the site prediction workflow from data set preparation to model evaluation. GrASP is able to achieve state-of-the-art performance at recovering binding sites in PDB structures while maintaining a high degree of precision which will minimize wasted computation in downstream tasks such as docking and free energy perturbation.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf",
    "citation_key": "smith2024q8n",
    "metadata": {
      "title": "Graph Attention Site Prediction (GrASP): Identifying Druggable Binding Sites Using Graph Neural Networks with Attention",
      "authors": [
        "Zachary Smith",
        "Michael Strobel",
        "Bodhi P. Vani",
        "P. Tiwary"
      ],
      "published_date": "2024",
      "abstract": "Identifying and discovering druggable protein binding sites is an important early step in computer-aided drug discovery, but it remains a difficult task where most campaigns rely on a priori knowledge of binding sites from experiments. Here, we present a binding site prediction method called Graph Attention Site Prediction (GrASP) and re-evaluate assumptions in nearly every step in the site prediction workflow from data set preparation to model evaluation. GrASP is able to achieve state-of-the-art performance at recovering binding sites in PDB structures while maintaining a high degree of precision which will minimize wasted computation in downstream tasks such as docking and free energy perturbation.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf",
      "venue": "Journal of Chemical Information and Modeling",
      "citationCount": 11,
      "score": 11.0,
      "summary": "Identifying and discovering druggable protein binding sites is an important early step in computer-aided drug discovery, but it remains a difficult task where most campaigns rely on a priori knowledge of binding sites from experiments. Here, we present a binding site prediction method called Graph Attention Site Prediction (GrASP) and re-evaluate assumptions in nearly every step in the site prediction workflow from data set preparation to model evaluation. GrASP is able to achieve state-of-the-art performance at recovering binding sites in PDB structures while maintaining a high degree of precision which will minimize wasted computation in downstream tasks such as docking and free energy perturbation.",
      "keywords": []
    },
    "file_name": "2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf"
  },
  {
    "success": true,
    "doc_id": "47f58e18a1eb15c1680af90f422c2e33",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf",
    "citation_key": "abadal2024w7e",
    "metadata": {
      "title": "Graph neural networks for electroencephalogram analysis: Alzheimer's disease and epilepsy use cases",
      "authors": [
        "S. Abadal",
        "Pablo Galván",
        "Alberto Mármol",
        "N. Mammone",
        "C. Ieracitano",
        "Michele Lo Giudice",
        "Alessandro Salvini",
        "F. Morabito"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf",
      "venue": "Neural Networks",
      "citationCount": 11,
      "score": 11.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf"
  },
  {
    "success": true,
    "doc_id": "90b312a3c09375612b0b0fcfea99ba05",
    "summary": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion.",
    "intriguing_abstract": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf",
    "citation_key": "pflueger2024qi6",
    "metadata": {
      "title": "Recurrent Graph Neural Networks and Their Connections to Bisimulation and Logic",
      "authors": [
        "Maximilian Pflueger",
        "David J. Tena Cucala",
        "Egor V. Kostylev"
      ],
      "published_date": "2024",
      "abstract": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion.",
      "file_path": "paper_data/Graph_Neural_Networks/info/478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 10,
      "score": 10.0,
      "summary": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion.",
      "keywords": []
    },
    "file_name": "478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf"
  },
  {
    "success": true,
    "doc_id": "f2e82a2e97b86cb13c1d4f328bee014d",
    "summary": "Brain connectivity analysis plays a crucial role in unraveling the complex network dynamics of the human brain, providing insights into cognitive functions, behaviors, and neurological disorders. Traditional graph-theoretical methods, while foundational, often fall short in capturing the high-dimensional and dynamic nature of brain connectivity. Graph Neural Networks (GNNs) have recently emerged as a powerful approach for this purpose, with the potential to improve diagnostics, prognostics, and personalized interventions. This review examines recent studies leveraging GNNs in brain connectivity analysis, focusing on key methodological advancements in multimodal data integration, dynamic connectivity, and interpretability across various imaging modalities, including fMRI, MRI, DTI, PET, and EEG. Findings reveal that GNNs excel in modeling complex, non-linear connectivity patterns and enable the integration of multiple neuroimaging modalities to provide richer insights into both healthy and pathological brain networks. However, challenges remain, particularly in interpretability, data scarcity, and multimodal integration, limiting the full clinical utility of GNNs. Addressing these limitations through enhanced interpretability, optimized multimodal techniques, and expanded labeled datasets is crucial to fully harness the potential of GNNs for neuroscience research and clinical applications.",
    "intriguing_abstract": "Brain connectivity analysis plays a crucial role in unraveling the complex network dynamics of the human brain, providing insights into cognitive functions, behaviors, and neurological disorders. Traditional graph-theoretical methods, while foundational, often fall short in capturing the high-dimensional and dynamic nature of brain connectivity. Graph Neural Networks (GNNs) have recently emerged as a powerful approach for this purpose, with the potential to improve diagnostics, prognostics, and personalized interventions. This review examines recent studies leveraging GNNs in brain connectivity analysis, focusing on key methodological advancements in multimodal data integration, dynamic connectivity, and interpretability across various imaging modalities, including fMRI, MRI, DTI, PET, and EEG. Findings reveal that GNNs excel in modeling complex, non-linear connectivity patterns and enable the integration of multiple neuroimaging modalities to provide richer insights into both healthy and pathological brain networks. However, challenges remain, particularly in interpretability, data scarcity, and multimodal integration, limiting the full clinical utility of GNNs. Addressing these limitations through enhanced interpretability, optimized multimodal techniques, and expanded labeled datasets is crucial to fully harness the potential of GNNs for neuroscience research and clinical applications.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf",
    "citation_key": "mohammadi202476q",
    "metadata": {
      "title": "Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions",
      "authors": [
        "H. Mohammadi",
        "Waldemar Karwowski"
      ],
      "published_date": "2024",
      "abstract": "Brain connectivity analysis plays a crucial role in unraveling the complex network dynamics of the human brain, providing insights into cognitive functions, behaviors, and neurological disorders. Traditional graph-theoretical methods, while foundational, often fall short in capturing the high-dimensional and dynamic nature of brain connectivity. Graph Neural Networks (GNNs) have recently emerged as a powerful approach for this purpose, with the potential to improve diagnostics, prognostics, and personalized interventions. This review examines recent studies leveraging GNNs in brain connectivity analysis, focusing on key methodological advancements in multimodal data integration, dynamic connectivity, and interpretability across various imaging modalities, including fMRI, MRI, DTI, PET, and EEG. Findings reveal that GNNs excel in modeling complex, non-linear connectivity patterns and enable the integration of multiple neuroimaging modalities to provide richer insights into both healthy and pathological brain networks. However, challenges remain, particularly in interpretability, data scarcity, and multimodal integration, limiting the full clinical utility of GNNs. Addressing these limitations through enhanced interpretability, optimized multimodal techniques, and expanded labeled datasets is crucial to fully harness the potential of GNNs for neuroscience research and clinical applications.",
      "file_path": "paper_data/Graph_Neural_Networks/info/817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf",
      "venue": "Brain Science",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Brain connectivity analysis plays a crucial role in unraveling the complex network dynamics of the human brain, providing insights into cognitive functions, behaviors, and neurological disorders. Traditional graph-theoretical methods, while foundational, often fall short in capturing the high-dimensional and dynamic nature of brain connectivity. Graph Neural Networks (GNNs) have recently emerged as a powerful approach for this purpose, with the potential to improve diagnostics, prognostics, and personalized interventions. This review examines recent studies leveraging GNNs in brain connectivity analysis, focusing on key methodological advancements in multimodal data integration, dynamic connectivity, and interpretability across various imaging modalities, including fMRI, MRI, DTI, PET, and EEG. Findings reveal that GNNs excel in modeling complex, non-linear connectivity patterns and enable the integration of multiple neuroimaging modalities to provide richer insights into both healthy and pathological brain networks. However, challenges remain, particularly in interpretability, data scarcity, and multimodal integration, limiting the full clinical utility of GNNs. Addressing these limitations through enhanced interpretability, optimized multimodal techniques, and expanded labeled datasets is crucial to fully harness the potential of GNNs for neuroscience research and clinical applications.",
      "keywords": []
    },
    "file_name": "817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf"
  },
  {
    "success": true,
    "doc_id": "f4e56a76fcd90e540012a5948461ccf9",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf",
    "citation_key": "sui2024xh9",
    "metadata": {
      "title": "Inductive Lottery Ticket Learning for Graph Neural Networks",
      "authors": [
        "Yongduo Sui",
        "Xiang Wang",
        "Tianlong Chen",
        "Meng Wang",
        "Xiang He",
        "Tat-Seng Chua"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf",
      "venue": "Journal of Computational Science and Technology",
      "citationCount": 10,
      "score": 10.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf"
  },
  {
    "success": true,
    "doc_id": "79d4575e59ca78409d2f0d3eeaae889b",
    "summary": "The drastic performance degradation of Graph Neural Networks (GNNs) as the depth of the graph propagation layers exceeds 8-10 is widely attributed to a phenomenon of Over-smoothing. Although recent research suggests that Over-smoothing may not be the dominant reason for such a performance degradation, they have not provided rigorous analysis from a theoretical view, which warrants further investigation In this paper, we systematically analyze the real dominant problem in deep GNNs and identify the issues that these GNNs towards addressing Over-smoothing essentially work on via empirical experiments and theoretical gradient analysis. We theoretically prove that the difficult training problem of deep MLPs is actually the main challenge, and various existing methods that supposedly tackle Over-smoothing actually improve the trainability of MLPs, which is the main reason for their performance gains. Our further investigation into trainability issues reveals that properly constrained smaller upper bounds of gradient flow notably enhance the trainability of GNNs. Experimental results on diverse datasets demonstrate consistency between our theoretical findings and empirical evidence. Our analysis provides new insights in constructing deep graph models.",
    "intriguing_abstract": "The drastic performance degradation of Graph Neural Networks (GNNs) as the depth of the graph propagation layers exceeds 8-10 is widely attributed to a phenomenon of Over-smoothing. Although recent research suggests that Over-smoothing may not be the dominant reason for such a performance degradation, they have not provided rigorous analysis from a theoretical view, which warrants further investigation In this paper, we systematically analyze the real dominant problem in deep GNNs and identify the issues that these GNNs towards addressing Over-smoothing essentially work on via empirical experiments and theoretical gradient analysis. We theoretically prove that the difficult training problem of deep MLPs is actually the main challenge, and various existing methods that supposedly tackle Over-smoothing actually improve the trainability of MLPs, which is the main reason for their performance gains. Our further investigation into trainability issues reveals that properly constrained smaller upper bounds of gradient flow notably enhance the trainability of GNNs. Experimental results on diverse datasets demonstrate consistency between our theoretical findings and empirical evidence. Our analysis provides new insights in constructing deep graph models.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/93ad698088aa72fcbd5004bd59ff38c25054f319.pdf",
    "citation_key": "peng2024t2s",
    "metadata": {
      "title": "Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep Graph Neural Networks",
      "authors": [
        "Jie Peng",
        "Runlin Lei",
        "Zhewei Wei"
      ],
      "published_date": "2024",
      "abstract": "The drastic performance degradation of Graph Neural Networks (GNNs) as the depth of the graph propagation layers exceeds 8-10 is widely attributed to a phenomenon of Over-smoothing. Although recent research suggests that Over-smoothing may not be the dominant reason for such a performance degradation, they have not provided rigorous analysis from a theoretical view, which warrants further investigation In this paper, we systematically analyze the real dominant problem in deep GNNs and identify the issues that these GNNs towards addressing Over-smoothing essentially work on via empirical experiments and theoretical gradient analysis. We theoretically prove that the difficult training problem of deep MLPs is actually the main challenge, and various existing methods that supposedly tackle Over-smoothing actually improve the trainability of MLPs, which is the main reason for their performance gains. Our further investigation into trainability issues reveals that properly constrained smaller upper bounds of gradient flow notably enhance the trainability of GNNs. Experimental results on diverse datasets demonstrate consistency between our theoretical findings and empirical evidence. Our analysis provides new insights in constructing deep graph models.",
      "file_path": "paper_data/Graph_Neural_Networks/info/93ad698088aa72fcbd5004bd59ff38c25054f319.pdf",
      "venue": "International Conference on Information and Knowledge Management",
      "citationCount": 10,
      "score": 10.0,
      "summary": "The drastic performance degradation of Graph Neural Networks (GNNs) as the depth of the graph propagation layers exceeds 8-10 is widely attributed to a phenomenon of Over-smoothing. Although recent research suggests that Over-smoothing may not be the dominant reason for such a performance degradation, they have not provided rigorous analysis from a theoretical view, which warrants further investigation In this paper, we systematically analyze the real dominant problem in deep GNNs and identify the issues that these GNNs towards addressing Over-smoothing essentially work on via empirical experiments and theoretical gradient analysis. We theoretically prove that the difficult training problem of deep MLPs is actually the main challenge, and various existing methods that supposedly tackle Over-smoothing actually improve the trainability of MLPs, which is the main reason for their performance gains. Our further investigation into trainability issues reveals that properly constrained smaller upper bounds of gradient flow notably enhance the trainability of GNNs. Experimental results on diverse datasets demonstrate consistency between our theoretical findings and empirical evidence. Our analysis provides new insights in constructing deep graph models.",
      "keywords": []
    },
    "file_name": "93ad698088aa72fcbd5004bd59ff38c25054f319.pdf"
  },
  {
    "success": true,
    "doc_id": "cb9960952579a13786b3e8048f93a003",
    "summary": "Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.",
    "intriguing_abstract": "Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf",
    "citation_key": "zhao2024e2x",
    "metadata": {
      "title": "Causal Graph Neural Networks for Wildfire Danger Prediction",
      "authors": [
        "Shan Zhao",
        "Ioannis Prapas",
        "Ilektra Karasante",
        "Zhitong Xiong",
        "Ioannis Papoutsis",
        "G. Camps-Valls",
        "Xiao Xiang Zhu"
      ],
      "published_date": "2024",
      "abstract": "Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf",
      "venue": "arXiv.org",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.",
      "keywords": []
    },
    "file_name": "c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf"
  },
  {
    "success": true,
    "doc_id": "49a72e97307273dc5d9c64da1d10a8cb",
    "summary": "Graph Neural Networks (GNNs) have gained significant traction for simulating complex physical systems, with models like MeshGraphNet demonstrating strong performance on unstructured simulation meshes. However, these models face several limitations, including scalability issues, requirement for meshing at inference, and challenges in handling long-range interactions. In this work, we introduce X-MeshGraphNet, a scalable, multi-scale extension of MeshGraphNet designed to address these challenges. X-MeshGraphNet overcomes the scalability bottleneck by partitioning large graphs and incorporating halo regions that enable seamless message passing across partitions. This, combined with gradient aggregation, ensures that training across partitions is equivalent to processing the entire graph at once. To remove the dependency on simulation meshes, X-MeshGraphNet constructs custom graphs directly from tessellated geometry files (e.g., STLs) by generating point clouds on the surface or volume of the object and connecting k-nearest neighbors. Additionally, our model builds multi-scale graphs by iteratively combining coarse and fine-resolution point clouds, where each level refines the previous, allowing for efficient long-range interactions. Our experiments demonstrate that X-MeshGraphNet maintains the predictive accuracy of full-graph GNNs while significantly improving scalability and flexibility. This approach eliminates the need for time-consuming mesh generation at inference, offering a practical solution for real-time simulation across a wide range of applications. The code for reproducing the results presented in this paper is available through NVIDIA Modulus.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have gained significant traction for simulating complex physical systems, with models like MeshGraphNet demonstrating strong performance on unstructured simulation meshes. However, these models face several limitations, including scalability issues, requirement for meshing at inference, and challenges in handling long-range interactions. In this work, we introduce X-MeshGraphNet, a scalable, multi-scale extension of MeshGraphNet designed to address these challenges. X-MeshGraphNet overcomes the scalability bottleneck by partitioning large graphs and incorporating halo regions that enable seamless message passing across partitions. This, combined with gradient aggregation, ensures that training across partitions is equivalent to processing the entire graph at once. To remove the dependency on simulation meshes, X-MeshGraphNet constructs custom graphs directly from tessellated geometry files (e.g., STLs) by generating point clouds on the surface or volume of the object and connecting k-nearest neighbors. Additionally, our model builds multi-scale graphs by iteratively combining coarse and fine-resolution point clouds, where each level refines the previous, allowing for efficient long-range interactions. Our experiments demonstrate that X-MeshGraphNet maintains the predictive accuracy of full-graph GNNs while significantly improving scalability and flexibility. This approach eliminates the need for time-consuming mesh generation at inference, offering a practical solution for real-time simulation across a wide range of applications. The code for reproducing the results presented in this paper is available through NVIDIA Modulus.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/9391738dff06189f64ced951df6c1848311731dc.pdf",
    "citation_key": "nabian2024vto",
    "metadata": {
      "title": "X-MeshGraphNet: Scalable Multi-Scale Graph Neural Networks for Physics Simulation",
      "authors": [
        "M. A. Nabian"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have gained significant traction for simulating complex physical systems, with models like MeshGraphNet demonstrating strong performance on unstructured simulation meshes. However, these models face several limitations, including scalability issues, requirement for meshing at inference, and challenges in handling long-range interactions. In this work, we introduce X-MeshGraphNet, a scalable, multi-scale extension of MeshGraphNet designed to address these challenges. X-MeshGraphNet overcomes the scalability bottleneck by partitioning large graphs and incorporating halo regions that enable seamless message passing across partitions. This, combined with gradient aggregation, ensures that training across partitions is equivalent to processing the entire graph at once. To remove the dependency on simulation meshes, X-MeshGraphNet constructs custom graphs directly from tessellated geometry files (e.g., STLs) by generating point clouds on the surface or volume of the object and connecting k-nearest neighbors. Additionally, our model builds multi-scale graphs by iteratively combining coarse and fine-resolution point clouds, where each level refines the previous, allowing for efficient long-range interactions. Our experiments demonstrate that X-MeshGraphNet maintains the predictive accuracy of full-graph GNNs while significantly improving scalability and flexibility. This approach eliminates the need for time-consuming mesh generation at inference, offering a practical solution for real-time simulation across a wide range of applications. The code for reproducing the results presented in this paper is available through NVIDIA Modulus.",
      "file_path": "paper_data/Graph_Neural_Networks/info/9391738dff06189f64ced951df6c1848311731dc.pdf",
      "venue": "arXiv.org",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Graph Neural Networks (GNNs) have gained significant traction for simulating complex physical systems, with models like MeshGraphNet demonstrating strong performance on unstructured simulation meshes. However, these models face several limitations, including scalability issues, requirement for meshing at inference, and challenges in handling long-range interactions. In this work, we introduce X-MeshGraphNet, a scalable, multi-scale extension of MeshGraphNet designed to address these challenges. X-MeshGraphNet overcomes the scalability bottleneck by partitioning large graphs and incorporating halo regions that enable seamless message passing across partitions. This, combined with gradient aggregation, ensures that training across partitions is equivalent to processing the entire graph at once. To remove the dependency on simulation meshes, X-MeshGraphNet constructs custom graphs directly from tessellated geometry files (e.g., STLs) by generating point clouds on the surface or volume of the object and connecting k-nearest neighbors. Additionally, our model builds multi-scale graphs by iteratively combining coarse and fine-resolution point clouds, where each level refines the previous, allowing for efficient long-range interactions. Our experiments demonstrate that X-MeshGraphNet maintains the predictive accuracy of full-graph GNNs while significantly improving scalability and flexibility. This approach eliminates the need for time-consuming mesh generation at inference, offering a practical solution for real-time simulation across a wide range of applications. The code for reproducing the results presented in this paper is available through NVIDIA Modulus.",
      "keywords": []
    },
    "file_name": "9391738dff06189f64ced951df6c1848311731dc.pdf"
  },
  {
    "success": true,
    "doc_id": "436322d61409b1f3d9835e961b9564b3",
    "summary": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
    "intriguing_abstract": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/d0cd5ede6535f617e40b58517fe593b648b737b0.pdf",
    "citation_key": "cen2024md8",
    "metadata": {
      "title": "Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?",
      "authors": [
        "Jiacheng Cen",
        "Anyi Li",
        "Ning Lin",
        "Yuxiang Ren",
        "Zihe Wang",
        "Wenbing Huang"
      ],
      "published_date": "2024",
      "abstract": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
      "file_path": "paper_data/Graph_Neural_Networks/info/d0cd5ede6535f617e40b58517fe593b648b737b0.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
      "keywords": []
    },
    "file_name": "d0cd5ede6535f617e40b58517fe593b648b737b0.pdf"
  },
  {
    "success": true,
    "doc_id": "37ae4d7616b955b185002dafed08d11d",
    "summary": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/819d9ef75975c78c5ce12e54af93737f4b698f55.pdf",
    "citation_key": "yang2024vy7",
    "metadata": {
      "title": "Graph Neural Networks with Soft Association between Topology and Attribute",
      "authors": [
        "Yachao Yang",
        "Yanfeng Sun",
        "Shaofan Wang",
        "Jipeng Guo",
        "Junbin Gao",
        "Fujiao Ju",
        "Baocai Yin"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA.",
      "file_path": "paper_data/Graph_Neural_Networks/info/819d9ef75975c78c5ce12e54af93737f4b698f55.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA.",
      "keywords": []
    },
    "file_name": "819d9ef75975c78c5ce12e54af93737f4b698f55.pdf"
  },
  {
    "success": true,
    "doc_id": "48c6de76c019f178ced7c7c477a8df77",
    "summary": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction...",
    "intriguing_abstract": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction...",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6826db50e96adb61ecc437809a361b16ea7546a7.pdf",
    "citation_key": "li2024gue",
    "metadata": {
      "title": "Hybrid-LLM-GNN: Integrating Large Language Models and Graph Neural Networks for Enhanced Materials Property Prediction",
      "authors": [
        "Youjia Li",
        "Vishu Gupta",
        "Muhammed Nur Talha Kilic",
        "Kamal Choudhary",
        "D. Wines",
        "Wei-keng Liao",
        "Alok N. Choudhary",
        "Ankit Agrawal"
      ],
      "published_date": "2024",
      "abstract": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction...",
      "file_path": "paper_data/Graph_Neural_Networks/info/6826db50e96adb61ecc437809a361b16ea7546a7.pdf",
      "venue": "Digital Discovery",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction...",
      "keywords": []
    },
    "file_name": "6826db50e96adb61ecc437809a361b16ea7546a7.pdf"
  },
  {
    "success": true,
    "doc_id": "c6bd2091e502c06c46141079094adfdf",
    "summary": "Configuration synthesis is a fundamental technology in the context of self-driving networks, aimed at mitigating network outages by intelligently and automatically generating configurations that align with network intents. However, existing tools often fall short in meeting the practical requirements of network operators, particularly in terms of generality and scalability. Moreover, these tools disregard manual configuration which remains the primary method employed for daily network management. To address these challenges, this paper introduces ConfigReco, a novel, versatile, and scalable configuration recommendation tool tailored for manual configuration. ConfigReco facilitates the automatic generation of configuration templates based on the network operator’s intent. First, ConfigReco leverages existing configurations as input and models them using a knowledge graph. Second, graph neural networks are employed by ConfigReco to estimate the significance of nodes within the configuration knowledge graph. Lastly, configuration recommendations are made by ConfigReco based on the computed importance scores. A prototype system has been implemented to substantiate the effectiveness of ConfigReco, and its performance has been evaluated using real-world configurations. The experimental results demonstrate that ConfigReco achieves a coverage rate of 93.35% while concurrently maintaining a redundancy rate of 23.07% within a configuration knowledge graph comprising 890,464 edges and 40,885 nodes. Furthermore, ConfigReco exhibits high scalability, enabling its applicability to arbitrary datasets, while simultaneously providing efficient recommendations within a response time of 1 second.",
    "intriguing_abstract": "Configuration synthesis is a fundamental technology in the context of self-driving networks, aimed at mitigating network outages by intelligently and automatically generating configurations that align with network intents. However, existing tools often fall short in meeting the practical requirements of network operators, particularly in terms of generality and scalability. Moreover, these tools disregard manual configuration which remains the primary method employed for daily network management. To address these challenges, this paper introduces ConfigReco, a novel, versatile, and scalable configuration recommendation tool tailored for manual configuration. ConfigReco facilitates the automatic generation of configuration templates based on the network operator’s intent. First, ConfigReco leverages existing configurations as input and models them using a knowledge graph. Second, graph neural networks are employed by ConfigReco to estimate the significance of nodes within the configuration knowledge graph. Lastly, configuration recommendations are made by ConfigReco based on the computed importance scores. A prototype system has been implemented to substantiate the effectiveness of ConfigReco, and its performance has been evaluated using real-world configurations. The experimental results demonstrate that ConfigReco achieves a coverage rate of 93.35% while concurrently maintaining a redundancy rate of 23.07% within a configuration knowledge graph comprising 890,464 edges and 40,885 nodes. Furthermore, ConfigReco exhibits high scalability, enabling its applicability to arbitrary datasets, while simultaneously providing efficient recommendations within a response time of 1 second.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf",
    "citation_key": "guo2024zoe",
    "metadata": {
      "title": "ConfigReco: Network Configuration Recommendation With Graph Neural Networks",
      "authors": [
        "Zhenbei Guo",
        "Fuliang Li",
        "Jiaxing Shen",
        "Tangzheng Xie",
        "Shan Jiang",
        "Xingwei Wang"
      ],
      "published_date": "2024",
      "abstract": "Configuration synthesis is a fundamental technology in the context of self-driving networks, aimed at mitigating network outages by intelligently and automatically generating configurations that align with network intents. However, existing tools often fall short in meeting the practical requirements of network operators, particularly in terms of generality and scalability. Moreover, these tools disregard manual configuration which remains the primary method employed for daily network management. To address these challenges, this paper introduces ConfigReco, a novel, versatile, and scalable configuration recommendation tool tailored for manual configuration. ConfigReco facilitates the automatic generation of configuration templates based on the network operator’s intent. First, ConfigReco leverages existing configurations as input and models them using a knowledge graph. Second, graph neural networks are employed by ConfigReco to estimate the significance of nodes within the configuration knowledge graph. Lastly, configuration recommendations are made by ConfigReco based on the computed importance scores. A prototype system has been implemented to substantiate the effectiveness of ConfigReco, and its performance has been evaluated using real-world configurations. The experimental results demonstrate that ConfigReco achieves a coverage rate of 93.35% while concurrently maintaining a redundancy rate of 23.07% within a configuration knowledge graph comprising 890,464 edges and 40,885 nodes. Furthermore, ConfigReco exhibits high scalability, enabling its applicability to arbitrary datasets, while simultaneously providing efficient recommendations within a response time of 1 second.",
      "file_path": "paper_data/Graph_Neural_Networks/info/9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf",
      "venue": "IEEE Network",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Configuration synthesis is a fundamental technology in the context of self-driving networks, aimed at mitigating network outages by intelligently and automatically generating configurations that align with network intents. However, existing tools often fall short in meeting the practical requirements of network operators, particularly in terms of generality and scalability. Moreover, these tools disregard manual configuration which remains the primary method employed for daily network management. To address these challenges, this paper introduces ConfigReco, a novel, versatile, and scalable configuration recommendation tool tailored for manual configuration. ConfigReco facilitates the automatic generation of configuration templates based on the network operator’s intent. First, ConfigReco leverages existing configurations as input and models them using a knowledge graph. Second, graph neural networks are employed by ConfigReco to estimate the significance of nodes within the configuration knowledge graph. Lastly, configuration recommendations are made by ConfigReco based on the computed importance scores. A prototype system has been implemented to substantiate the effectiveness of ConfigReco, and its performance has been evaluated using real-world configurations. The experimental results demonstrate that ConfigReco achieves a coverage rate of 93.35% while concurrently maintaining a redundancy rate of 23.07% within a configuration knowledge graph comprising 890,464 edges and 40,885 nodes. Furthermore, ConfigReco exhibits high scalability, enabling its applicability to arbitrary datasets, while simultaneously providing efficient recommendations within a response time of 1 second.",
      "keywords": []
    },
    "file_name": "9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf"
  },
  {
    "success": true,
    "doc_id": "e9c4306ff1e9ea9ed3213ac299dee394",
    "summary": "Predicting drug-drug interactions (DDIs) is crucial for medication safety and efficacy. Traditional methods often face challenges in capturing complex interactions within large-scale drug networks. In this study, we propose an enhanced approach leveraging Graph Neural Networks (GNNs) and Support Vector Machines (SVM) to improve DDI prediction accuracy. Our method integrates graph representation learning with SVM-based classification, effectively capturing intricate relationships between drugs and their interactions. Through extensive experimentation on benchmark datasets, we demonstrate superior performance compared to existing methods, achieving higher prediction accuracy of ${9 7 \\%}$ and f1 measure of ${9 8 \\%}$. Moreover, our approach offers interpretability, enabling insights into underlying interaction mechanisms. Overall, our study highlights the potential of combining GNNs and SVM for advancing drug interaction prediction, with implications for enhancing medication safety and clinical decision-making.",
    "intriguing_abstract": "Predicting drug-drug interactions (DDIs) is crucial for medication safety and efficacy. Traditional methods often face challenges in capturing complex interactions within large-scale drug networks. In this study, we propose an enhanced approach leveraging Graph Neural Networks (GNNs) and Support Vector Machines (SVM) to improve DDI prediction accuracy. Our method integrates graph representation learning with SVM-based classification, effectively capturing intricate relationships between drugs and their interactions. Through extensive experimentation on benchmark datasets, we demonstrate superior performance compared to existing methods, achieving higher prediction accuracy of ${9 7 \\%}$ and f1 measure of ${9 8 \\%}$. Moreover, our approach offers interpretability, enabling insights into underlying interaction mechanisms. Overall, our study highlights the potential of combining GNNs and SVM for advancing drug interaction prediction, with implications for enhancing medication safety and clinical decision-making.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf",
    "citation_key": "gnanabaskaran20245dg",
    "metadata": {
      "title": "Enhanced Drug-Drug Interaction Prediction with Graph Neural Networks and SVM",
      "authors": [
        "A. Gnanabaskaran",
        "K. Bharathi",
        "S. P. Nandakumar",
        "B. Sanjay",
        "R. Praveen"
      ],
      "published_date": "2024",
      "abstract": "Predicting drug-drug interactions (DDIs) is crucial for medication safety and efficacy. Traditional methods often face challenges in capturing complex interactions within large-scale drug networks. In this study, we propose an enhanced approach leveraging Graph Neural Networks (GNNs) and Support Vector Machines (SVM) to improve DDI prediction accuracy. Our method integrates graph representation learning with SVM-based classification, effectively capturing intricate relationships between drugs and their interactions. Through extensive experimentation on benchmark datasets, we demonstrate superior performance compared to existing methods, achieving higher prediction accuracy of ${9 7 \\%}$ and f1 measure of ${9 8 \\%}$. Moreover, our approach offers interpretability, enabling insights into underlying interaction mechanisms. Overall, our study highlights the potential of combining GNNs and SVM for advancing drug interaction prediction, with implications for enhancing medication safety and clinical decision-making.",
      "file_path": "paper_data/Graph_Neural_Networks/info/8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf",
      "venue": "2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Predicting drug-drug interactions (DDIs) is crucial for medication safety and efficacy. Traditional methods often face challenges in capturing complex interactions within large-scale drug networks. In this study, we propose an enhanced approach leveraging Graph Neural Networks (GNNs) and Support Vector Machines (SVM) to improve DDI prediction accuracy. Our method integrates graph representation learning with SVM-based classification, effectively capturing intricate relationships between drugs and their interactions. Through extensive experimentation on benchmark datasets, we demonstrate superior performance compared to existing methods, achieving higher prediction accuracy of ${9 7 \\%}$ and f1 measure of ${9 8 \\%}$. Moreover, our approach offers interpretability, enabling insights into underlying interaction mechanisms. Overall, our study highlights the potential of combining GNNs and SVM for advancing drug interaction prediction, with implications for enhancing medication safety and clinical decision-making.",
      "keywords": []
    },
    "file_name": "8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf"
  },
  {
    "success": true,
    "doc_id": "c0bb9eb1bc7a427a59924197fc4c13b8",
    "summary": "Graph Neural Networks (GNNs) have been widely developed and grown rapidly to address representation and learning for attribute graph data <inline-formula><tex-math notation=\"LaTeX\">$G(A,X)$</tex-math></inline-formula>. However, existing studies on GNNs mainly focus on the message passing on graph <inline-formula><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> for layer-wise propagation while pay less attention to the robust learning for the input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>, which thus make existing GNNs often perform susceptibility w.r.t feature noises and adversarial perturbations in <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>. In this article, we propose a novel Feature Learning guided Graph Neural Networks (FL-GNNs) by incorporating robust feature learning into GNNs. The core of FL-GNNs is trying to recover (or learn) a more clean and optimal feature data <inline-formula><tex-math notation=\"LaTeX\">$Z$</tex-math></inline-formula> from input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula> that better serves GNNs learning by jointly conducting feature reconstruction and GNNs' learning simultaneously. FL-GNNs is general and can be incorporated into any specific GNN models to enhance their robustness. An efficient algorithm has been derived to optimize FL-GNNs. Experimental results show that FL-GNNs can obviously enhance the robustness of existing GCN and GAT w.r.t feature noises and adversarial perturbations.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have been widely developed and grown rapidly to address representation and learning for attribute graph data <inline-formula><tex-math notation=\"LaTeX\">$G(A,X)$</tex-math></inline-formula>. However, existing studies on GNNs mainly focus on the message passing on graph <inline-formula><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> for layer-wise propagation while pay less attention to the robust learning for the input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>, which thus make existing GNNs often perform susceptibility w.r.t feature noises and adversarial perturbations in <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>. In this article, we propose a novel Feature Learning guided Graph Neural Networks (FL-GNNs) by incorporating robust feature learning into GNNs. The core of FL-GNNs is trying to recover (or learn) a more clean and optimal feature data <inline-formula><tex-math notation=\"LaTeX\">$Z$</tex-math></inline-formula> from input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula> that better serves GNNs learning by jointly conducting feature reconstruction and GNNs' learning simultaneously. FL-GNNs is general and can be incorporated into any specific GNN models to enhance their robustness. An efficient algorithm has been derived to optimize FL-GNNs. Experimental results show that FL-GNNs can obviously enhance the robustness of existing GCN and GAT w.r.t feature noises and adversarial perturbations.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a6c060ab3b997675075415253e0a6bc81591f32e.pdf",
    "citation_key": "wang20245it",
    "metadata": {
      "title": "FL-GNNs: Robust Network Representation via Feature Learning Guided Graph Neural Networks",
      "authors": [
        "Beibei Wang",
        "Bo Jiang",
        "Chris H. Q. Ding"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have been widely developed and grown rapidly to address representation and learning for attribute graph data <inline-formula><tex-math notation=\"LaTeX\">$G(A,X)$</tex-math></inline-formula>. However, existing studies on GNNs mainly focus on the message passing on graph <inline-formula><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> for layer-wise propagation while pay less attention to the robust learning for the input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>, which thus make existing GNNs often perform susceptibility w.r.t feature noises and adversarial perturbations in <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>. In this article, we propose a novel Feature Learning guided Graph Neural Networks (FL-GNNs) by incorporating robust feature learning into GNNs. The core of FL-GNNs is trying to recover (or learn) a more clean and optimal feature data <inline-formula><tex-math notation=\"LaTeX\">$Z$</tex-math></inline-formula> from input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula> that better serves GNNs learning by jointly conducting feature reconstruction and GNNs' learning simultaneously. FL-GNNs is general and can be incorporated into any specific GNN models to enhance their robustness. An efficient algorithm has been derived to optimize FL-GNNs. Experimental results show that FL-GNNs can obviously enhance the robustness of existing GCN and GAT w.r.t feature noises and adversarial perturbations.",
      "file_path": "paper_data/Graph_Neural_Networks/info/a6c060ab3b997675075415253e0a6bc81591f32e.pdf",
      "venue": "IEEE Transactions on Network Science and Engineering",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Graph Neural Networks (GNNs) have been widely developed and grown rapidly to address representation and learning for attribute graph data <inline-formula><tex-math notation=\"LaTeX\">$G(A,X)$</tex-math></inline-formula>. However, existing studies on GNNs mainly focus on the message passing on graph <inline-formula><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> for layer-wise propagation while pay less attention to the robust learning for the input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>, which thus make existing GNNs often perform susceptibility w.r.t feature noises and adversarial perturbations in <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>. In this article, we propose a novel Feature Learning guided Graph Neural Networks (FL-GNNs) by incorporating robust feature learning into GNNs. The core of FL-GNNs is trying to recover (or learn) a more clean and optimal feature data <inline-formula><tex-math notation=\"LaTeX\">$Z$</tex-math></inline-formula> from input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula> that better serves GNNs learning by jointly conducting feature reconstruction and GNNs' learning simultaneously. FL-GNNs is general and can be incorporated into any specific GNN models to enhance their robustness. An efficient algorithm has been derived to optimize FL-GNNs. Experimental results show that FL-GNNs can obviously enhance the robustness of existing GCN and GAT w.r.t feature noises and adversarial perturbations.",
      "keywords": []
    },
    "file_name": "a6c060ab3b997675075415253e0a6bc81591f32e.pdf"
  },
  {
    "success": true,
    "doc_id": "3a87bee91396caa4ff29abeed81cebbd",
    "summary": "Transmit power control (PC) will become increasingly crucial in alleviating interference as the densification of the wireless networks continues towards 6G. However, the practicality of most PC methods suffers from high complexity, including the sensing and signalling overhead needed to obtain channel state information. In a highly dense scenario such as the deployment of short-range cells installed within production entities, termed in-factory subnetworks (InF-S), sensing and signalling overhead become a major limitation. In this paper, we represent the InF-S as a graph and resort to graph neural networks for solving the power control problem. We present four graph-attribution methods requiring different degrees of channel information corresponding to different levels of sensing and signalling overhead and study the complexity and performance tradeoffs of the resulting power control graph neural network (PCGNN) algorithms. We then propose a PCGNN method with scalable sensing and signalling graph attribution which can meet the stringent outage target while maximizing the global performance by 10% relative to fixed power control. We further verified the size generalizability and robustness of the PCGNN methods to different network settings.",
    "intriguing_abstract": "Transmit power control (PC) will become increasingly crucial in alleviating interference as the densification of the wireless networks continues towards 6G. However, the practicality of most PC methods suffers from high complexity, including the sensing and signalling overhead needed to obtain channel state information. In a highly dense scenario such as the deployment of short-range cells installed within production entities, termed in-factory subnetworks (InF-S), sensing and signalling overhead become a major limitation. In this paper, we represent the InF-S as a graph and resort to graph neural networks for solving the power control problem. We present four graph-attribution methods requiring different degrees of channel information corresponding to different levels of sensing and signalling overhead and study the complexity and performance tradeoffs of the resulting power control graph neural network (PCGNN) algorithms. We then propose a PCGNN method with scalable sensing and signalling graph attribution which can meet the stringent outage target while maximizing the global performance by 10% relative to fixed power control. We further verified the size generalizability and robustness of the PCGNN methods to different network settings.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/73765285b243a53143912b501f7afab98a0c8cb0.pdf",
    "citation_key": "abode2024m4z",
    "metadata": {
      "title": "Power Control for 6G In-Factory Subnetworks With Partial Channel Information Using Graph Neural Networks",
      "authors": [
        "Daniel Abode",
        "Ramoni O. Adeogun",
        "Gilberto Berardinelli"
      ],
      "published_date": "2024",
      "abstract": "Transmit power control (PC) will become increasingly crucial in alleviating interference as the densification of the wireless networks continues towards 6G. However, the practicality of most PC methods suffers from high complexity, including the sensing and signalling overhead needed to obtain channel state information. In a highly dense scenario such as the deployment of short-range cells installed within production entities, termed in-factory subnetworks (InF-S), sensing and signalling overhead become a major limitation. In this paper, we represent the InF-S as a graph and resort to graph neural networks for solving the power control problem. We present four graph-attribution methods requiring different degrees of channel information corresponding to different levels of sensing and signalling overhead and study the complexity and performance tradeoffs of the resulting power control graph neural network (PCGNN) algorithms. We then propose a PCGNN method with scalable sensing and signalling graph attribution which can meet the stringent outage target while maximizing the global performance by 10% relative to fixed power control. We further verified the size generalizability and robustness of the PCGNN methods to different network settings.",
      "file_path": "paper_data/Graph_Neural_Networks/info/73765285b243a53143912b501f7afab98a0c8cb0.pdf",
      "venue": "IEEE Open Journal of the Communications Society",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Transmit power control (PC) will become increasingly crucial in alleviating interference as the densification of the wireless networks continues towards 6G. However, the practicality of most PC methods suffers from high complexity, including the sensing and signalling overhead needed to obtain channel state information. In a highly dense scenario such as the deployment of short-range cells installed within production entities, termed in-factory subnetworks (InF-S), sensing and signalling overhead become a major limitation. In this paper, we represent the InF-S as a graph and resort to graph neural networks for solving the power control problem. We present four graph-attribution methods requiring different degrees of channel information corresponding to different levels of sensing and signalling overhead and study the complexity and performance tradeoffs of the resulting power control graph neural network (PCGNN) algorithms. We then propose a PCGNN method with scalable sensing and signalling graph attribution which can meet the stringent outage target while maximizing the global performance by 10% relative to fixed power control. We further verified the size generalizability and robustness of the PCGNN methods to different network settings.",
      "keywords": []
    },
    "file_name": "73765285b243a53143912b501f7afab98a0c8cb0.pdf"
  },
  {
    "success": true,
    "doc_id": "186391040e689d8e34cecf5340316f9b",
    "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, DisamGCL which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. DisamGCL identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. DisamGCL promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of DisamGCL and highlight its potential to improve GNN performance in underrepresented graph regions.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, DisamGCL which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. DisamGCL identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. DisamGCL promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of DisamGCL and highlight its potential to improve GNN performance in underrepresented graph regions.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf",
    "citation_key": "zhao20244un",
    "metadata": {
      "title": "Disambiguated Node Classification with Graph Neural Networks",
      "authors": [
        "Tianxiang Zhao",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, DisamGCL which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. DisamGCL identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. DisamGCL promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of DisamGCL and highlight its potential to improve GNN performance in underrepresented graph regions.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf",
      "venue": "The Web Conference",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, DisamGCL which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. DisamGCL identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. DisamGCL promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of DisamGCL and highlight its potential to improve GNN performance in underrepresented graph regions.",
      "keywords": []
    },
    "file_name": "f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf"
  },
  {
    "success": true,
    "doc_id": "1ef64aa06ce91fcbae2e5d62ee2f7c58",
    "summary": "The authors introduce a novel framework that integrates federated learning with Graph Neural Networks (GNNs) to classify diseases, incorporating Human-in-the-Loop methodologies. This advanced framework innovatively employs collaborative voting mechanisms on subgraphs within a Protein-Protein Interaction (PPI) network, situated in a federated ensemble-based deep learning context. This methodological approach marks a significant stride in the development of explainable and privacy-aware Artificial Intelligence, significantly contributing to the progression of personalized digital medicine in a responsible and transparent manner.",
    "intriguing_abstract": "The authors introduce a novel framework that integrates federated learning with Graph Neural Networks (GNNs) to classify diseases, incorporating Human-in-the-Loop methodologies. This advanced framework innovatively employs collaborative voting mechanisms on subgraphs within a Protein-Protein Interaction (PPI) network, situated in a federated ensemble-based deep learning context. This methodological approach marks a significant stride in the development of explainable and privacy-aware Artificial Intelligence, significantly contributing to the progression of personalized digital medicine in a responsible and transparent manner.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf",
    "citation_key": "hausleitner2024vw0",
    "metadata": {
      "title": "Collaborative weighting in federated graph neural networks for disease classification with the human-in-the-loop",
      "authors": [
        "Christian Hausleitner",
        "Heimo Mueller",
        "Andreas Holzinger",
        "B. Pfeifer"
      ],
      "published_date": "2024",
      "abstract": "The authors introduce a novel framework that integrates federated learning with Graph Neural Networks (GNNs) to classify diseases, incorporating Human-in-the-Loop methodologies. This advanced framework innovatively employs collaborative voting mechanisms on subgraphs within a Protein-Protein Interaction (PPI) network, situated in a federated ensemble-based deep learning context. This methodological approach marks a significant stride in the development of explainable and privacy-aware Artificial Intelligence, significantly contributing to the progression of personalized digital medicine in a responsible and transparent manner.",
      "file_path": "paper_data/Graph_Neural_Networks/info/aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf",
      "venue": "Scientific Reports",
      "citationCount": 10,
      "score": 10.0,
      "summary": "The authors introduce a novel framework that integrates federated learning with Graph Neural Networks (GNNs) to classify diseases, incorporating Human-in-the-Loop methodologies. This advanced framework innovatively employs collaborative voting mechanisms on subgraphs within a Protein-Protein Interaction (PPI) network, situated in a federated ensemble-based deep learning context. This methodological approach marks a significant stride in the development of explainable and privacy-aware Artificial Intelligence, significantly contributing to the progression of personalized digital medicine in a responsible and transparent manner.",
      "keywords": []
    },
    "file_name": "aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf"
  },
  {
    "success": true,
    "doc_id": "bad8b95f054de9a3e2ca7ce006d652e3",
    "summary": "Earth observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph neural networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs’ applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight the methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the article by comparing them with other popular architectures, like Transformers, and analyzing their potential synergies.",
    "intriguing_abstract": "Earth observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph neural networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs’ applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight the methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the article by comparing them with other popular architectures, like Transformers, and analyzing their potential synergies.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf",
    "citation_key": "zhao2024g7h",
    "metadata": {
      "title": "Beyond Grid Data: Exploring graph neural networks for Earth observation",
      "authors": [
        "Shan Zhao",
        "Zhaiyu Chen",
        "Zhitong Xiong",
        "Yilei Shi",
        "Sudipan Saha",
        "Xiao Xiang Zhu"
      ],
      "published_date": "2024",
      "abstract": "Earth observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph neural networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs’ applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight the methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the article by comparing them with other popular architectures, like Transformers, and analyzing their potential synergies.",
      "file_path": "paper_data/Graph_Neural_Networks/info/dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf",
      "venue": "IEEE Geoscience and Remote Sensing Magazine",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Earth observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph neural networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs’ applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight the methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the article by comparing them with other popular architectures, like Transformers, and analyzing their potential synergies.",
      "keywords": []
    },
    "file_name": "dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf"
  },
  {
    "success": true,
    "doc_id": "056697df4e3f2ace0c53e29b7cc79606",
    "summary": "Significance This article introduces Message-Passing Monte Carlo (MPMC), a machine learning approach for generating low-discrepancy point sets which are essential for efficiently filling space in a uniform manner, and thus play a central role in many problems in science and engineering. To accomplish this, MPMC utilizes tools from Geometric Deep Learning, specifically by employing graph neural networks. MPMC can be extended to a higher-dimensional case which further allows for generating custom-made points. Finally, MPMC point sets significantly outperform previous methods, achieving near-optimal discrepancy in practice for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. This advancement holds promise for enhancing efficiency in fields like scientific computing, computer vision, machine learning, and simulation.",
    "intriguing_abstract": "Significance This article introduces Message-Passing Monte Carlo (MPMC), a machine learning approach for generating low-discrepancy point sets which are essential for efficiently filling space in a uniform manner, and thus play a central role in many problems in science and engineering. To accomplish this, MPMC utilizes tools from Geometric Deep Learning, specifically by employing graph neural networks. MPMC can be extended to a higher-dimensional case which further allows for generating custom-made points. Finally, MPMC point sets significantly outperform previous methods, achieving near-optimal discrepancy in practice for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. This advancement holds promise for enhancing efficiency in fields like scientific computing, computer vision, machine learning, and simulation.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/9fb72c9292bf80f9825e0038d34cef57468a2757.pdf",
    "citation_key": "rusch2024fgp",
    "metadata": {
      "title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks",
      "authors": [
        "T. Konstantin Rusch",
        "Nathan Kirk",
        "M. Bronstein",
        "Christiane Lemieux",
        "Daniela Rus"
      ],
      "published_date": "2024",
      "abstract": "Significance This article introduces Message-Passing Monte Carlo (MPMC), a machine learning approach for generating low-discrepancy point sets which are essential for efficiently filling space in a uniform manner, and thus play a central role in many problems in science and engineering. To accomplish this, MPMC utilizes tools from Geometric Deep Learning, specifically by employing graph neural networks. MPMC can be extended to a higher-dimensional case which further allows for generating custom-made points. Finally, MPMC point sets significantly outperform previous methods, achieving near-optimal discrepancy in practice for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. This advancement holds promise for enhancing efficiency in fields like scientific computing, computer vision, machine learning, and simulation.",
      "file_path": "paper_data/Graph_Neural_Networks/info/9fb72c9292bf80f9825e0038d34cef57468a2757.pdf",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Significance This article introduces Message-Passing Monte Carlo (MPMC), a machine learning approach for generating low-discrepancy point sets which are essential for efficiently filling space in a uniform manner, and thus play a central role in many problems in science and engineering. To accomplish this, MPMC utilizes tools from Geometric Deep Learning, specifically by employing graph neural networks. MPMC can be extended to a higher-dimensional case which further allows for generating custom-made points. Finally, MPMC point sets significantly outperform previous methods, achieving near-optimal discrepancy in practice for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. This advancement holds promise for enhancing efficiency in fields like scientific computing, computer vision, machine learning, and simulation.",
      "keywords": []
    },
    "file_name": "9fb72c9292bf80f9825e0038d34cef57468a2757.pdf"
  },
  {
    "success": true,
    "doc_id": "4e5773617092934606c762a404a6dcd4",
    "summary": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identify pseudo-labeled nodes that both are informative and capable of redeeming the distribution discrepancy and formulate it as a differentiable optimization task. A distribution-shift-aware edge predictor is further adopted to augment the graph and increase the model's generalizability in assigning pseudo labels. We evaluate our proposed method on four publicly available benchmark datasets and extensive experiments demonstrate that our framework consistently outperforms state-of-the-art baselines.",
    "intriguing_abstract": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identify pseudo-labeled nodes that both are informative and capable of redeeming the distribution discrepancy and formulate it as a differentiable optimization task. A distribution-shift-aware edge predictor is further adopted to augment the graph and increase the model's generalizability in assigning pseudo labels. We evaluate our proposed method on four publicly available benchmark datasets and extensive experiments demonstrate that our framework consistently outperforms state-of-the-art baselines.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf",
    "citation_key": "wang2024htw",
    "metadata": {
      "title": "Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels",
      "authors": [
        "Fali Wang",
        "Tianxiang Zhao",
        "Suhang Wang"
      ],
      "published_date": "2024",
      "abstract": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identify pseudo-labeled nodes that both are informative and capable of redeeming the distribution discrepancy and formulate it as a differentiable optimization task. A distribution-shift-aware edge predictor is further adopted to augment the graph and increase the model's generalizability in assigning pseudo labels. We evaluate our proposed method on four publicly available benchmark datasets and extensive experiments demonstrate that our framework consistently outperforms state-of-the-art baselines.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf",
      "venue": "Web Search and Data Mining",
      "citationCount": 10,
      "score": 10.0,
      "summary": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identify pseudo-labeled nodes that both are informative and capable of redeeming the distribution discrepancy and formulate it as a differentiable optimization task. A distribution-shift-aware edge predictor is further adopted to augment the graph and increase the model's generalizability in assigning pseudo labels. We evaluate our proposed method on four publicly available benchmark datasets and extensive experiments demonstrate that our framework consistently outperforms state-of-the-art baselines.",
      "keywords": []
    },
    "file_name": "b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf"
  },
  {
    "success": true,
    "doc_id": "06e9888f4fae265a8f96b1374ba6ba08",
    "summary": "This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good “robustness”.",
    "intriguing_abstract": "This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good “robustness”.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf",
    "citation_key": "liu2024sbb",
    "metadata": {
      "title": "Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment",
      "authors": [
        "Bingyao Liu",
        "Iris Li",
        "Jianhua Yao",
        "Yuan Chen",
        "Guanming Huang",
        "Jiajing Wang"
      ],
      "published_date": "2024",
      "abstract": "This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good “robustness”.",
      "file_path": "paper_data/Graph_Neural_Networks/info/2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf",
      "venue": "2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)",
      "citationCount": 10,
      "score": 10.0,
      "summary": "This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good “robustness”.",
      "keywords": []
    },
    "file_name": "2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf"
  },
  {
    "success": true,
    "doc_id": "e3fa459e4b29b3f82b20c7f87affd15d",
    "summary": "In machine-learning-assisted high-throughput defect studies, a defect-aware latent representation of the supercell structure is crucial for the accurate prediction of defect properties. The performance of current graph neural network (GNN) models is limited due to the fact that defect properties depend strongly on the local atomic configurations near the defect sites and due to the oversmoothing problem of GNN. Herein, we demonstrate that persistent homology features, which encode the topological information on the local chemical environment around each atomic site, can characterize the structural information on defects. Using the dataset containing a wide spectrum of O-based perovskites with all available vacancies as an example, we show that incorporating the persistent homology features, along with proper choices of graph pooling operations, significantly increases the prediction accuracy, with the MAE reduced by 55%. Those features can be easily integrated into the state-of-the-art GNN models, including the graph Transformer network and the equivariant neural network, and universally improve their performance. Besides, our model also overcomes the convergence issue with respect to the supercell size that was present in previous GNN models. Furthermore, using the datasets of defective BaTiO3 with multiple substitutions and multiple vacancies as examples, our GNN model can also predict the defect–defect interactions accurately. These results suggest that persistent homology features can effectively improve the performance of machine learning models and assist the accelerated discovery of functional defects for technological applications.",
    "intriguing_abstract": "In machine-learning-assisted high-throughput defect studies, a defect-aware latent representation of the supercell structure is crucial for the accurate prediction of defect properties. The performance of current graph neural network (GNN) models is limited due to the fact that defect properties depend strongly on the local atomic configurations near the defect sites and due to the oversmoothing problem of GNN. Herein, we demonstrate that persistent homology features, which encode the topological information on the local chemical environment around each atomic site, can characterize the structural information on defects. Using the dataset containing a wide spectrum of O-based perovskites with all available vacancies as an example, we show that incorporating the persistent homology features, along with proper choices of graph pooling operations, significantly increases the prediction accuracy, with the MAE reduced by 55%. Those features can be easily integrated into the state-of-the-art GNN models, including the graph Transformer network and the equivariant neural network, and universally improve their performance. Besides, our model also overcomes the convergence issue with respect to the supercell size that was present in previous GNN models. Furthermore, using the datasets of defective BaTiO3 with multiple substitutions and multiple vacancies as examples, our GNN model can also predict the defect–defect interactions accurately. These results suggest that persistent homology features can effectively improve the performance of machine learning models and assist the accelerated discovery of functional defects for technological applications.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf",
    "citation_key": "fang2024zd6",
    "metadata": {
      "title": "Leveraging Persistent Homology Features for Accurate Defect Formation Energy Predictions via Graph Neural Networks",
      "authors": [
        "Zhenyao Fang",
        "Qimin Yan"
      ],
      "published_date": "2024",
      "abstract": "In machine-learning-assisted high-throughput defect studies, a defect-aware latent representation of the supercell structure is crucial for the accurate prediction of defect properties. The performance of current graph neural network (GNN) models is limited due to the fact that defect properties depend strongly on the local atomic configurations near the defect sites and due to the oversmoothing problem of GNN. Herein, we demonstrate that persistent homology features, which encode the topological information on the local chemical environment around each atomic site, can characterize the structural information on defects. Using the dataset containing a wide spectrum of O-based perovskites with all available vacancies as an example, we show that incorporating the persistent homology features, along with proper choices of graph pooling operations, significantly increases the prediction accuracy, with the MAE reduced by 55%. Those features can be easily integrated into the state-of-the-art GNN models, including the graph Transformer network and the equivariant neural network, and universally improve their performance. Besides, our model also overcomes the convergence issue with respect to the supercell size that was present in previous GNN models. Furthermore, using the datasets of defective BaTiO3 with multiple substitutions and multiple vacancies as examples, our GNN model can also predict the defect–defect interactions accurately. These results suggest that persistent homology features can effectively improve the performance of machine learning models and assist the accelerated discovery of functional defects for technological applications.",
      "file_path": "paper_data/Graph_Neural_Networks/info/5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf",
      "venue": "Chemistry of Materials",
      "citationCount": 10,
      "score": 10.0,
      "summary": "In machine-learning-assisted high-throughput defect studies, a defect-aware latent representation of the supercell structure is crucial for the accurate prediction of defect properties. The performance of current graph neural network (GNN) models is limited due to the fact that defect properties depend strongly on the local atomic configurations near the defect sites and due to the oversmoothing problem of GNN. Herein, we demonstrate that persistent homology features, which encode the topological information on the local chemical environment around each atomic site, can characterize the structural information on defects. Using the dataset containing a wide spectrum of O-based perovskites with all available vacancies as an example, we show that incorporating the persistent homology features, along with proper choices of graph pooling operations, significantly increases the prediction accuracy, with the MAE reduced by 55%. Those features can be easily integrated into the state-of-the-art GNN models, including the graph Transformer network and the equivariant neural network, and universally improve their performance. Besides, our model also overcomes the convergence issue with respect to the supercell size that was present in previous GNN models. Furthermore, using the datasets of defective BaTiO3 with multiple substitutions and multiple vacancies as examples, our GNN model can also predict the defect–defect interactions accurately. These results suggest that persistent homology features can effectively improve the performance of machine learning models and assist the accelerated discovery of functional defects for technological applications.",
      "keywords": []
    },
    "file_name": "5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf"
  },
  {
    "success": true,
    "doc_id": "8d0eb01920d6ff16fa7bad77d9ce5800",
    "summary": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving\"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
    "intriguing_abstract": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving\"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/242425415e28da757bb9c7d24dd0a99654d66027.pdf",
    "citation_key": "benedikt2024153",
    "metadata": {
      "title": "Decidability of Graph Neural Networks via Logical Characterizations",
      "authors": [
        "Michael Benedikt",
        "Chia-Hsuan Lu",
        "Boris Motik",
        "Tony Tan"
      ],
      "published_date": "2024",
      "abstract": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving\"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
      "file_path": "paper_data/Graph_Neural_Networks/info/242425415e28da757bb9c7d24dd0a99654d66027.pdf",
      "venue": "International Colloquium on Automata, Languages and Programming",
      "citationCount": 9,
      "score": 9.0,
      "summary": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving\"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
      "keywords": []
    },
    "file_name": "242425415e28da757bb9c7d24dd0a99654d66027.pdf"
  },
  {
    "success": true,
    "doc_id": "20d3bfaf691022d62a4deb24f25e605c",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/da3c1508794ba0d4f070a9bc47b06575422f456f.pdf",
    "citation_key": "zhang20241k0",
    "metadata": {
      "title": "Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning",
      "authors": [
        "Yuelin Zhang",
        "Jiacheng Cen",
        "Jiaqi Han",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Wenbing Huang"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/da3c1508794ba0d4f070a9bc47b06575422f456f.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 9,
      "score": 9.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "da3c1508794ba0d4f070a9bc47b06575422f456f.pdf"
  },
  {
    "success": true,
    "doc_id": "aa2e1d51bc11150be9ec1e854eeca716",
    "summary": "",
    "intriguing_abstract": "",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf",
    "citation_key": "graziani2024lgd",
    "metadata": {
      "title": "The Expressive Power of Path-Based Graph Neural Networks",
      "authors": [
        "Caterina Graziani",
        "Tamara Drucks",
        "Fabian Jogl",
        "M. Bianchini",
        "F. Scarselli",
        "Thomas Gärtner"
      ],
      "published_date": "2024",
      "abstract": "",
      "file_path": "paper_data/Graph_Neural_Networks/info/0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf",
      "venue": "International Conference on Machine Learning",
      "citationCount": 9,
      "score": 9.0,
      "summary": "",
      "keywords": []
    },
    "file_name": "0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf"
  },
  {
    "success": true,
    "doc_id": "53898816b4af0737d2bd0ddbf1803a4b",
    "summary": "Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.",
    "intriguing_abstract": "Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf",
    "citation_key": "shi2024g4z",
    "metadata": {
      "title": "Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks",
      "authors": [
        "Dai Shi",
        "Andi Han",
        "Lequan Lin",
        "Yi Guo",
        "Zhiyong Wang",
        "Junbin Gao"
      ],
      "published_date": "2024",
      "abstract": "Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.",
      "file_path": "paper_data/Graph_Neural_Networks/info/a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf",
      "venue": "International Journal of Machine Learning and Cybernetics",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.",
      "keywords": []
    },
    "file_name": "a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf"
  },
  {
    "success": true,
    "doc_id": "ee8b892300f86a1ea1cf971383025e02",
    "summary": "Font classification of oracle bone inscriptions serves as a crucial basis for determining the historical period to which they belong and holds significant importance in reconstructing significant historical events. However, conventional methods for font classification in oracle bone inscriptions heavily rely on expert knowledge, resulting in low efficiency and time-consuming procedures. In this paper, we proposed a novel recurrent graph neural network (R-GNN) for the automatic recognition of oracle bone inscription fonts. The proposed method used convolutional neural networks (CNNs) to perform local feature extraction and downsampling on oracle bone inscriptions. Furthermore, it employed graph neural networks (GNNs) to model the complex topologiure and global contextual information of oracle bone inscriptions. Finally, we used recurrent neural networks (RNNs) to effectively combine the extracted local features and global contextual information, thereby enhancing the discriminative power of the R-GNN. Extensive experiments on our benchmark dataset demonstrate that the proposed method achieves a Top-1 accuracy of 88.2%, significantly outperforming the competing approaches. The method presented in this paper further advances the integration of oracle bone inscriptions research and artificial intelligence. The code is publicly available at: https://github.com/yj3214/oracle-font-classification .",
    "intriguing_abstract": "Font classification of oracle bone inscriptions serves as a crucial basis for determining the historical period to which they belong and holds significant importance in reconstructing significant historical events. However, conventional methods for font classification in oracle bone inscriptions heavily rely on expert knowledge, resulting in low efficiency and time-consuming procedures. In this paper, we proposed a novel recurrent graph neural network (R-GNN) for the automatic recognition of oracle bone inscription fonts. The proposed method used convolutional neural networks (CNNs) to perform local feature extraction and downsampling on oracle bone inscriptions. Furthermore, it employed graph neural networks (GNNs) to model the complex topologiure and global contextual information of oracle bone inscriptions. Finally, we used recurrent neural networks (RNNs) to effectively combine the extracted local features and global contextual information, thereby enhancing the discriminative power of the R-GNN. Extensive experiments on our benchmark dataset demonstrate that the proposed method achieves a Top-1 accuracy of 88.2%, significantly outperforming the competing approaches. The method presented in this paper further advances the integration of oracle bone inscriptions research and artificial intelligence. The code is publicly available at: https://github.com/yj3214/oracle-font-classification .",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf",
    "citation_key": "yuan2024b8b",
    "metadata": {
      "title": "R-GNN: recurrent graph neural networks for font classification of oracle bone inscriptions",
      "authors": [
        "Jiang Yuan",
        "Shanxiong Chen",
        "Bofeng Mo",
        "Yuqi Ma",
        "Wenjun Zheng",
        "Chongsheng Zhang"
      ],
      "published_date": "2024",
      "abstract": "Font classification of oracle bone inscriptions serves as a crucial basis for determining the historical period to which they belong and holds significant importance in reconstructing significant historical events. However, conventional methods for font classification in oracle bone inscriptions heavily rely on expert knowledge, resulting in low efficiency and time-consuming procedures. In this paper, we proposed a novel recurrent graph neural network (R-GNN) for the automatic recognition of oracle bone inscription fonts. The proposed method used convolutional neural networks (CNNs) to perform local feature extraction and downsampling on oracle bone inscriptions. Furthermore, it employed graph neural networks (GNNs) to model the complex topologiure and global contextual information of oracle bone inscriptions. Finally, we used recurrent neural networks (RNNs) to effectively combine the extracted local features and global contextual information, thereby enhancing the discriminative power of the R-GNN. Extensive experiments on our benchmark dataset demonstrate that the proposed method achieves a Top-1 accuracy of 88.2%, significantly outperforming the competing approaches. The method presented in this paper further advances the integration of oracle bone inscriptions research and artificial intelligence. The code is publicly available at: https://github.com/yj3214/oracle-font-classification .",
      "file_path": "paper_data/Graph_Neural_Networks/info/c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf",
      "venue": "Heritage Science",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Font classification of oracle bone inscriptions serves as a crucial basis for determining the historical period to which they belong and holds significant importance in reconstructing significant historical events. However, conventional methods for font classification in oracle bone inscriptions heavily rely on expert knowledge, resulting in low efficiency and time-consuming procedures. In this paper, we proposed a novel recurrent graph neural network (R-GNN) for the automatic recognition of oracle bone inscription fonts. The proposed method used convolutional neural networks (CNNs) to perform local feature extraction and downsampling on oracle bone inscriptions. Furthermore, it employed graph neural networks (GNNs) to model the complex topologiure and global contextual information of oracle bone inscriptions. Finally, we used recurrent neural networks (RNNs) to effectively combine the extracted local features and global contextual information, thereby enhancing the discriminative power of the R-GNN. Extensive experiments on our benchmark dataset demonstrate that the proposed method achieves a Top-1 accuracy of 88.2%, significantly outperforming the competing approaches. The method presented in this paper further advances the integration of oracle bone inscriptions research and artificial intelligence. The code is publicly available at: https://github.com/yj3214/oracle-font-classification .",
      "keywords": []
    },
    "file_name": "c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf"
  },
  {
    "success": true,
    "doc_id": "d436c6e4813a98eee98766a94be6a897",
    "summary": "Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.",
    "intriguing_abstract": "Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf",
    "citation_key": "wang2024kx8",
    "metadata": {
      "title": "A Gearbox Fault Diagnosis Method Based on Graph Neural Networks and Markov Transform Fields",
      "authors": [
        "Haitao Wang",
        "Zelin Liu",
        "Mingjun Li",
        "Xiyang Dai",
        "Ruihua Wang",
        "Lichen Shi"
      ],
      "published_date": "2024",
      "abstract": "Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf",
      "venue": "IEEE Sensors Journal",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.",
      "keywords": []
    },
    "file_name": "67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf"
  },
  {
    "success": true,
    "doc_id": "2424c5f671f56e0ced017c835fb646e7",
    "summary": "Alzheimer’s disease (AD), the most prevalent form of dementia, requires early prediction for timely intervention. Current deep learning approaches, particularly those using traditional neural networks, face challenges such as handling high-dimensional data, interpreting complex relationships, and managing data bias. To address these limitations, we propose a framework utilizing graph neural networks (GNNs), which excel in modeling relationships within graph-structured data. Our study employs GNNs on data from the Alzheimer’s Disease Neuroimaging Initiative for binary and multi-class classification across the three stages of AD: cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer’s disease (AD). By incorporating comorbidity data derived from electronic health records, we achieved the most effective multi-classification results. Notably, the GNN model (Chebyshev Convolutional Neural Networks) demonstrated superior performance with a 0.98 accuracy in multi-class classification and 0.99, 0.93, and 0.94 in the AD/CN, AD/MCI, and CN/MCI binary tasks, respectively. The model’s robustness was further validated using the Australian Imaging, Biomarker & Lifestyle dataset as an external validation set. This work contributes to the field by offering a robust, accurate, and cost-effective method for early AD prediction (CN vs. MCI), addressing key challenges in existing deep learning approaches.",
    "intriguing_abstract": "Alzheimer’s disease (AD), the most prevalent form of dementia, requires early prediction for timely intervention. Current deep learning approaches, particularly those using traditional neural networks, face challenges such as handling high-dimensional data, interpreting complex relationships, and managing data bias. To address these limitations, we propose a framework utilizing graph neural networks (GNNs), which excel in modeling relationships within graph-structured data. Our study employs GNNs on data from the Alzheimer’s Disease Neuroimaging Initiative for binary and multi-class classification across the three stages of AD: cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer’s disease (AD). By incorporating comorbidity data derived from electronic health records, we achieved the most effective multi-classification results. Notably, the GNN model (Chebyshev Convolutional Neural Networks) demonstrated superior performance with a 0.98 accuracy in multi-class classification and 0.99, 0.93, and 0.94 in the AD/CN, AD/MCI, and CN/MCI binary tasks, respectively. The model’s robustness was further validated using the Australian Imaging, Biomarker & Lifestyle dataset as an external validation set. This work contributes to the field by offering a robust, accurate, and cost-effective method for early AD prediction (CN vs. MCI), addressing key challenges in existing deep learning approaches.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/12e60b9fd69f18c1c01996d108229051432b6090.pdf",
    "citation_key": "abuhantash202458c",
    "metadata": {
      "title": "Comorbidity-based framework for Alzheimer’s disease classification using graph neural networks",
      "authors": [
        "Ferial Abuhantash",
        "Mohd Khalil Abu Hantash",
        "Aamna AlShehhi"
      ],
      "published_date": "2024",
      "abstract": "Alzheimer’s disease (AD), the most prevalent form of dementia, requires early prediction for timely intervention. Current deep learning approaches, particularly those using traditional neural networks, face challenges such as handling high-dimensional data, interpreting complex relationships, and managing data bias. To address these limitations, we propose a framework utilizing graph neural networks (GNNs), which excel in modeling relationships within graph-structured data. Our study employs GNNs on data from the Alzheimer’s Disease Neuroimaging Initiative for binary and multi-class classification across the three stages of AD: cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer’s disease (AD). By incorporating comorbidity data derived from electronic health records, we achieved the most effective multi-classification results. Notably, the GNN model (Chebyshev Convolutional Neural Networks) demonstrated superior performance with a 0.98 accuracy in multi-class classification and 0.99, 0.93, and 0.94 in the AD/CN, AD/MCI, and CN/MCI binary tasks, respectively. The model’s robustness was further validated using the Australian Imaging, Biomarker & Lifestyle dataset as an external validation set. This work contributes to the field by offering a robust, accurate, and cost-effective method for early AD prediction (CN vs. MCI), addressing key challenges in existing deep learning approaches.",
      "file_path": "paper_data/Graph_Neural_Networks/info/12e60b9fd69f18c1c01996d108229051432b6090.pdf",
      "venue": "Scientific Reports",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Alzheimer’s disease (AD), the most prevalent form of dementia, requires early prediction for timely intervention. Current deep learning approaches, particularly those using traditional neural networks, face challenges such as handling high-dimensional data, interpreting complex relationships, and managing data bias. To address these limitations, we propose a framework utilizing graph neural networks (GNNs), which excel in modeling relationships within graph-structured data. Our study employs GNNs on data from the Alzheimer’s Disease Neuroimaging Initiative for binary and multi-class classification across the three stages of AD: cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer’s disease (AD). By incorporating comorbidity data derived from electronic health records, we achieved the most effective multi-classification results. Notably, the GNN model (Chebyshev Convolutional Neural Networks) demonstrated superior performance with a 0.98 accuracy in multi-class classification and 0.99, 0.93, and 0.94 in the AD/CN, AD/MCI, and CN/MCI binary tasks, respectively. The model’s robustness was further validated using the Australian Imaging, Biomarker & Lifestyle dataset as an external validation set. This work contributes to the field by offering a robust, accurate, and cost-effective method for early AD prediction (CN vs. MCI), addressing key challenges in existing deep learning approaches.",
      "keywords": []
    },
    "file_name": "12e60b9fd69f18c1c01996d108229051432b6090.pdf"
  },
  {
    "success": true,
    "doc_id": "549b233790e628a09e219ed1d8fbee77",
    "summary": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: \\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: \\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf",
    "citation_key": "abbahaddou2024bq2",
    "metadata": {
      "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
      "authors": [
        "Yassine Abbahaddou",
        "Sofiane Ennadir",
        "J. Lutzeyer",
        "M. Vazirgiannis",
        "Henrik Boström"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: \\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
      "file_path": "paper_data/Graph_Neural_Networks/info/c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: \\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
      "keywords": []
    },
    "file_name": "c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf"
  },
  {
    "success": true,
    "doc_id": "53c36db2b94cb9cafb5b6e77bfd18752",
    "summary": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks.",
    "intriguing_abstract": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/7bda10706047e154e22259c4b20d70240296963e.pdf",
    "citation_key": "huang2024tdd",
    "metadata": {
      "title": "Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Networks",
      "authors": [
        "Renhong Huang",
        "Jiarong Xu",
        "Xin Jiang",
        "Chenglu Pan",
        "Zhiming Yang",
        "Chunping Wang",
        "Yang Yang"
      ],
      "published_date": "2024",
      "abstract": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/7bda10706047e154e22259c4b20d70240296963e.pdf",
      "venue": "AAAI Conference on Artificial Intelligence",
      "citationCount": 9,
      "score": 9.0,
      "summary": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks.",
      "keywords": []
    },
    "file_name": "7bda10706047e154e22259c4b20d70240296963e.pdf"
  },
  {
    "success": true,
    "doc_id": "3d26c05f7a6bde0e3cacc35f5af5ef2f",
    "summary": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning’s potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements’ properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model’s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.",
    "intriguing_abstract": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning’s potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements’ properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model’s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/a3340cd6f24e4c83bec616c7bda719737492fe74.pdf",
    "citation_key": "jiang202448s",
    "metadata": {
      "title": "Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces",
      "authors": [
        "Yue Jiang",
        "Changkong Zhou",
        "Vikas Garg",
        "A. Oulasvirta"
      ],
      "published_date": "2024",
      "abstract": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning’s potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements’ properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model’s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.",
      "file_path": "paper_data/Graph_Neural_Networks/info/a3340cd6f24e4c83bec616c7bda719737492fe74.pdf",
      "venue": "International Conference on Human Factors in Computing Systems",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning’s potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements’ properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model’s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.",
      "keywords": []
    },
    "file_name": "a3340cd6f24e4c83bec616c7bda719737492fe74.pdf"
  },
  {
    "success": true,
    "doc_id": "43f5b84c3d2c52ac50f3b4e2c6bd4164",
    "summary": "The gear fault diagnosis technology based on the signal is crucial for maintaining the normal operation of the gear in the motor drive chain. In some cases, it is challenging to add sensors on the unit of the motor transmission chain for collecting vibration signals in practical engineering applications. However, the current signal can be collected. Nonetheless, due to the long distance between the collection point and the fault source, it becomes difficult to extract the features of the weak gear fault from the current signal. In order to solve the aforementioned problems efficiently, an optimized principal neighborhood aggregation (OPNA) graph neural network (GNN) was proposed to diagnose gear faults in the motor drive chain. First, the current signal is reconstructed to obtain the topological data graph sample by the graph sample construction method proposed in this article. Second, OPNA, an architecture that combines multiple message aggregators with a degree scaler, was designed to extract the features of nodes and edges. Subsequently, the embedding and the particular pooling improvement were used to reduce the number of nodes and achieve steady and rapid classification. Finally, the experimental studies, based on the current signal of the gear dataset, were conducted to validate the effectiveness of the proposed method and its superiority over the traditional methods.",
    "intriguing_abstract": "The gear fault diagnosis technology based on the signal is crucial for maintaining the normal operation of the gear in the motor drive chain. In some cases, it is challenging to add sensors on the unit of the motor transmission chain for collecting vibration signals in practical engineering applications. However, the current signal can be collected. Nonetheless, due to the long distance between the collection point and the fault source, it becomes difficult to extract the features of the weak gear fault from the current signal. In order to solve the aforementioned problems efficiently, an optimized principal neighborhood aggregation (OPNA) graph neural network (GNN) was proposed to diagnose gear faults in the motor drive chain. First, the current signal is reconstructed to obtain the topological data graph sample by the graph sample construction method proposed in this article. Second, OPNA, an architecture that combines multiple message aggregators with a degree scaler, was designed to extract the features of nodes and edges. Subsequently, the embedding and the particular pooling improvement were used to reduce the number of nodes and achieve steady and rapid classification. Finally, the experimental studies, based on the current signal of the gear dataset, were conducted to validate the effectiveness of the proposed method and its superiority over the traditional methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/abd2ac274abe25f40f5268324d4774e67b467ef8.pdf",
    "citation_key": "wang20246bq",
    "metadata": {
      "title": "Gear Fault Diagnosis Method Based on the Optimized Graph Neural Networks",
      "authors": [
        "Bin Wang",
        "Yadong Xu",
        "Manyi Wang",
        "Yanze Li"
      ],
      "published_date": "2024",
      "abstract": "The gear fault diagnosis technology based on the signal is crucial for maintaining the normal operation of the gear in the motor drive chain. In some cases, it is challenging to add sensors on the unit of the motor transmission chain for collecting vibration signals in practical engineering applications. However, the current signal can be collected. Nonetheless, due to the long distance between the collection point and the fault source, it becomes difficult to extract the features of the weak gear fault from the current signal. In order to solve the aforementioned problems efficiently, an optimized principal neighborhood aggregation (OPNA) graph neural network (GNN) was proposed to diagnose gear faults in the motor drive chain. First, the current signal is reconstructed to obtain the topological data graph sample by the graph sample construction method proposed in this article. Second, OPNA, an architecture that combines multiple message aggregators with a degree scaler, was designed to extract the features of nodes and edges. Subsequently, the embedding and the particular pooling improvement were used to reduce the number of nodes and achieve steady and rapid classification. Finally, the experimental studies, based on the current signal of the gear dataset, were conducted to validate the effectiveness of the proposed method and its superiority over the traditional methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/abd2ac274abe25f40f5268324d4774e67b467ef8.pdf",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "citationCount": 9,
      "score": 9.0,
      "summary": "The gear fault diagnosis technology based on the signal is crucial for maintaining the normal operation of the gear in the motor drive chain. In some cases, it is challenging to add sensors on the unit of the motor transmission chain for collecting vibration signals in practical engineering applications. However, the current signal can be collected. Nonetheless, due to the long distance between the collection point and the fault source, it becomes difficult to extract the features of the weak gear fault from the current signal. In order to solve the aforementioned problems efficiently, an optimized principal neighborhood aggregation (OPNA) graph neural network (GNN) was proposed to diagnose gear faults in the motor drive chain. First, the current signal is reconstructed to obtain the topological data graph sample by the graph sample construction method proposed in this article. Second, OPNA, an architecture that combines multiple message aggregators with a degree scaler, was designed to extract the features of nodes and edges. Subsequently, the embedding and the particular pooling improvement were used to reduce the number of nodes and achieve steady and rapid classification. Finally, the experimental studies, based on the current signal of the gear dataset, were conducted to validate the effectiveness of the proposed method and its superiority over the traditional methods.",
      "keywords": []
    },
    "file_name": "abd2ac274abe25f40f5268324d4774e67b467ef8.pdf"
  },
  {
    "success": true,
    "doc_id": "f6dc81436dc89e951ec3a8b15c568fd7",
    "summary": "Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.",
    "intriguing_abstract": "Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/3d0911fabeebc22506ac3b006a553448debf03a5.pdf",
    "citation_key": "silva2024trs",
    "metadata": {
      "title": "Using graph neural networks to predict local culture",
      "authors": [
        "Thiago H. Silva",
        "Daniel Silver"
      ],
      "published_date": "2024",
      "abstract": "Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.",
      "file_path": "paper_data/Graph_Neural_Networks/info/3d0911fabeebc22506ac3b006a553448debf03a5.pdf",
      "venue": "Environment and Planning B Urban Analytics and City Science",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.",
      "keywords": []
    },
    "file_name": "3d0911fabeebc22506ac3b006a553448debf03a5.pdf"
  },
  {
    "success": true,
    "doc_id": "7ef00902c41c424f9115ed80709195a2",
    "summary": "In the current context of accelerated globalization and digitalization, the complexity and uncertainty of financial markets are increasing, and the identification and prevention of economic risks have become a key link in maintaining the stability of the financial system. Traditional risk identification methods often have limitations because they are difficult to cope with the multi-level and dynamically changing complex relationships in financial networks. With the rapid development of financial technology, graph neural network (GNN) technology, as an emerging deep learning method, has gradually shown great potential in the field of financial risk management. GNN can map transaction behaviors, financial institutions, individuals and their interactive relationships in financial networks into graph structures, and effectively capture potential patterns and abnormal signals in financial data through embedded representation learning. Using this technology, financial institutions can extract valuable information from complex transaction networks, identify hidden dangers or abnormal behaviors that may cause systemic risks in a timely manner, optimize decision-making processes, and improve the accuracy of risk warnings. This paper explores the economic risk identification algorithm based on the GNN algorithm, aiming to provide financial institutions and regulators with more intelligent technical tools to help maintain the security and stability of the financial market. Improving the efficiency of economic risk identification through innovative technical means is expected to further enhance the risk resistance of the financial system and lay the foundation for building a robust global financial system.",
    "intriguing_abstract": "In the current context of accelerated globalization and digitalization, the complexity and uncertainty of financial markets are increasing, and the identification and prevention of economic risks have become a key link in maintaining the stability of the financial system. Traditional risk identification methods often have limitations because they are difficult to cope with the multi-level and dynamically changing complex relationships in financial networks. With the rapid development of financial technology, graph neural network (GNN) technology, as an emerging deep learning method, has gradually shown great potential in the field of financial risk management. GNN can map transaction behaviors, financial institutions, individuals and their interactive relationships in financial networks into graph structures, and effectively capture potential patterns and abnormal signals in financial data through embedded representation learning. Using this technology, financial institutions can extract valuable information from complex transaction networks, identify hidden dangers or abnormal behaviors that may cause systemic risks in a timely manner, optimize decision-making processes, and improve the accuracy of risk warnings. This paper explores the economic risk identification algorithm based on the GNN algorithm, aiming to provide financial institutions and regulators with more intelligent technical tools to help maintain the security and stability of the financial market. Improving the efficiency of economic risk identification through innovative technical means is expected to further enhance the risk resistance of the financial system and lay the foundation for building a robust global financial system.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf",
    "citation_key": "zhang2024ctj",
    "metadata": {
      "title": "Robust Graph Neural Networks for Stability Analysis in Dynamic Networks",
      "authors": [
        "Xin Zhang",
        "Zhen Xu",
        "Yue Liu",
        "Mengfang Sun",
        "Tong Zhou",
        "Wenying Sun"
      ],
      "published_date": "2024",
      "abstract": "In the current context of accelerated globalization and digitalization, the complexity and uncertainty of financial markets are increasing, and the identification and prevention of economic risks have become a key link in maintaining the stability of the financial system. Traditional risk identification methods often have limitations because they are difficult to cope with the multi-level and dynamically changing complex relationships in financial networks. With the rapid development of financial technology, graph neural network (GNN) technology, as an emerging deep learning method, has gradually shown great potential in the field of financial risk management. GNN can map transaction behaviors, financial institutions, individuals and their interactive relationships in financial networks into graph structures, and effectively capture potential patterns and abnormal signals in financial data through embedded representation learning. Using this technology, financial institutions can extract valuable information from complex transaction networks, identify hidden dangers or abnormal behaviors that may cause systemic risks in a timely manner, optimize decision-making processes, and improve the accuracy of risk warnings. This paper explores the economic risk identification algorithm based on the GNN algorithm, aiming to provide financial institutions and regulators with more intelligent technical tools to help maintain the security and stability of the financial market. Improving the efficiency of economic risk identification through innovative technical means is expected to further enhance the risk resistance of the financial system and lay the foundation for building a robust global financial system.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf",
      "venue": "2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)",
      "citationCount": 9,
      "score": 9.0,
      "summary": "In the current context of accelerated globalization and digitalization, the complexity and uncertainty of financial markets are increasing, and the identification and prevention of economic risks have become a key link in maintaining the stability of the financial system. Traditional risk identification methods often have limitations because they are difficult to cope with the multi-level and dynamically changing complex relationships in financial networks. With the rapid development of financial technology, graph neural network (GNN) technology, as an emerging deep learning method, has gradually shown great potential in the field of financial risk management. GNN can map transaction behaviors, financial institutions, individuals and their interactive relationships in financial networks into graph structures, and effectively capture potential patterns and abnormal signals in financial data through embedded representation learning. Using this technology, financial institutions can extract valuable information from complex transaction networks, identify hidden dangers or abnormal behaviors that may cause systemic risks in a timely manner, optimize decision-making processes, and improve the accuracy of risk warnings. This paper explores the economic risk identification algorithm based on the GNN algorithm, aiming to provide financial institutions and regulators with more intelligent technical tools to help maintain the security and stability of the financial market. Improving the efficiency of economic risk identification through innovative technical means is expected to further enhance the risk resistance of the financial system and lay the foundation for building a robust global financial system.",
      "keywords": []
    },
    "file_name": "1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf"
  },
  {
    "success": true,
    "doc_id": "740915425a4ab4e89178221fdfe2874a",
    "summary": "This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.",
    "intriguing_abstract": "This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf",
    "citation_key": "sun2024ztz",
    "metadata": {
      "title": "Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis",
      "authors": [
        "Mengfang Sun",
        "Wenying Sun",
        "Ying Sun",
        "Shaobo Liu",
        "Mohan Jiang",
        "Zhen Xu"
      ],
      "published_date": "2024",
      "abstract": "This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.",
      "file_path": "paper_data/Graph_Neural_Networks/info/413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf",
      "venue": "2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)",
      "citationCount": 9,
      "score": 9.0,
      "summary": "This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.",
      "keywords": []
    },
    "file_name": "413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf"
  },
  {
    "success": true,
    "doc_id": "33ad7ff6c7d6c0ffd1573e9b27129197",
    "summary": "Most proteins exert their functions by interacting with other proteins, making the identification of protein-protein interactions (PPI) crucial for understanding biological activities, pathological mechanisms, and clinical therapies. Developing effective and reliable computational methods for predicting PPI can significantly reduce the time-consuming and labor-intensive associated traditional biological experiments. However, accurately identifying the specific categories of protein-protein interactions and improving the prediction accuracy of the computational methods remain dual challenges. To tackle these challenges, we proposed a novel graph neural network method called GNNGL-PPI for multi-category prediction of PPI based on global graphs and local subgraphs. GNNGL-PPI consisted of two main components: using Graph Isomorphism Network (GIN) to extract global graph features from PPI network graph, and employing GIN As Kernel (GIN-AK) to extract local subgraph features from the subgraphs of protein vertices. Additionally, considering the imbalanced distribution of samples in each category within the benchmark datasets, we introduced an Asymmetric Loss (ASL) function to further enhance the predictive performance of the method. Through evaluations on six benchmark test sets formed by three different dataset partitioning algorithms (Random, BFS, DFS), GNNGL-PPI outperformed the state-of-the-art multi-category prediction methods of PPI, as measured by the comprehensive performance evaluation metric F1-measure. Furthermore, interpretability analysis confirmed the effectiveness of GNNGL-PPI as a reliable multi-category prediction method for predicting protein-protein interactions.",
    "intriguing_abstract": "Most proteins exert their functions by interacting with other proteins, making the identification of protein-protein interactions (PPI) crucial for understanding biological activities, pathological mechanisms, and clinical therapies. Developing effective and reliable computational methods for predicting PPI can significantly reduce the time-consuming and labor-intensive associated traditional biological experiments. However, accurately identifying the specific categories of protein-protein interactions and improving the prediction accuracy of the computational methods remain dual challenges. To tackle these challenges, we proposed a novel graph neural network method called GNNGL-PPI for multi-category prediction of PPI based on global graphs and local subgraphs. GNNGL-PPI consisted of two main components: using Graph Isomorphism Network (GIN) to extract global graph features from PPI network graph, and employing GIN As Kernel (GIN-AK) to extract local subgraph features from the subgraphs of protein vertices. Additionally, considering the imbalanced distribution of samples in each category within the benchmark datasets, we introduced an Asymmetric Loss (ASL) function to further enhance the predictive performance of the method. Through evaluations on six benchmark test sets formed by three different dataset partitioning algorithms (Random, BFS, DFS), GNNGL-PPI outperformed the state-of-the-art multi-category prediction methods of PPI, as measured by the comprehensive performance evaluation metric F1-measure. Furthermore, interpretability analysis confirmed the effectiveness of GNNGL-PPI as a reliable multi-category prediction method for predicting protein-protein interactions.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf",
    "citation_key": "zeng2024fpp",
    "metadata": {
      "title": "GNNGL-PPI: multi-category prediction of protein-protein interactions using graph neural networks based on global graphs and local subgraphs",
      "authors": [
        "Xin Zeng",
        "Fan-Fang Meng",
        "Meng-Liang Wen",
        "Shu-Juan Li",
        "Yi Li"
      ],
      "published_date": "2024",
      "abstract": "Most proteins exert their functions by interacting with other proteins, making the identification of protein-protein interactions (PPI) crucial for understanding biological activities, pathological mechanisms, and clinical therapies. Developing effective and reliable computational methods for predicting PPI can significantly reduce the time-consuming and labor-intensive associated traditional biological experiments. However, accurately identifying the specific categories of protein-protein interactions and improving the prediction accuracy of the computational methods remain dual challenges. To tackle these challenges, we proposed a novel graph neural network method called GNNGL-PPI for multi-category prediction of PPI based on global graphs and local subgraphs. GNNGL-PPI consisted of two main components: using Graph Isomorphism Network (GIN) to extract global graph features from PPI network graph, and employing GIN As Kernel (GIN-AK) to extract local subgraph features from the subgraphs of protein vertices. Additionally, considering the imbalanced distribution of samples in each category within the benchmark datasets, we introduced an Asymmetric Loss (ASL) function to further enhance the predictive performance of the method. Through evaluations on six benchmark test sets formed by three different dataset partitioning algorithms (Random, BFS, DFS), GNNGL-PPI outperformed the state-of-the-art multi-category prediction methods of PPI, as measured by the comprehensive performance evaluation metric F1-measure. Furthermore, interpretability analysis confirmed the effectiveness of GNNGL-PPI as a reliable multi-category prediction method for predicting protein-protein interactions.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf",
      "venue": "BMC Genomics",
      "citationCount": 9,
      "score": 9.0,
      "summary": "Most proteins exert their functions by interacting with other proteins, making the identification of protein-protein interactions (PPI) crucial for understanding biological activities, pathological mechanisms, and clinical therapies. Developing effective and reliable computational methods for predicting PPI can significantly reduce the time-consuming and labor-intensive associated traditional biological experiments. However, accurately identifying the specific categories of protein-protein interactions and improving the prediction accuracy of the computational methods remain dual challenges. To tackle these challenges, we proposed a novel graph neural network method called GNNGL-PPI for multi-category prediction of PPI based on global graphs and local subgraphs. GNNGL-PPI consisted of two main components: using Graph Isomorphism Network (GIN) to extract global graph features from PPI network graph, and employing GIN As Kernel (GIN-AK) to extract local subgraph features from the subgraphs of protein vertices. Additionally, considering the imbalanced distribution of samples in each category within the benchmark datasets, we introduced an Asymmetric Loss (ASL) function to further enhance the predictive performance of the method. Through evaluations on six benchmark test sets formed by three different dataset partitioning algorithms (Random, BFS, DFS), GNNGL-PPI outperformed the state-of-the-art multi-category prediction methods of PPI, as measured by the comprehensive performance evaluation metric F1-measure. Furthermore, interpretability analysis confirmed the effectiveness of GNNGL-PPI as a reliable multi-category prediction method for predicting protein-protein interactions.",
      "keywords": []
    },
    "file_name": "f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf"
  },
  {
    "success": true,
    "doc_id": "6fe3c88acae32ce4871dbbb20c3028a4",
    "summary": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.",
    "intriguing_abstract": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf",
    "citation_key": "chen20241tu",
    "metadata": {
      "title": "Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs",
      "authors": [
        "Ziang Chen",
        "Xiaohan Chen",
        "Jialin Liu",
        "Xinshang Wang",
        "Wotao Yin"
      ],
      "published_date": "2024",
      "abstract": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.",
      "file_path": "paper_data/Graph_Neural_Networks/info/1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf",
      "venue": "arXiv.org",
      "citationCount": 8,
      "score": 8.0,
      "summary": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.",
      "keywords": []
    },
    "file_name": "1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf"
  },
  {
    "success": true,
    "doc_id": "4b345b139622c4698d750b65b77ccedd",
    "summary": "Hypergraphs extend traditional graphs by allowing edges to connect multiple nodes, while superhypergraphs further generalize this concept to represent even more complex relationships. Neural networks, inspired by biological systems, are widely used for tasks such as pattern recognition, data classification, and prediction. Graph Neural Networks (GNNs), a well-established framework, have recently been extended to Hypergraph Neural Networks (HGNNs), with their properties and applications being actively studied. The Plithogenic Graph framework enhances graph representations by integrating multi-valued attributes, as well as membership and contradiction functions, enabling the detailed modeling of complex relationships. In the context of handling uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained prominence. It is well established that Plithogenic Graphs serve as a generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the Fuzzy Graph Neural Network has been proposed and is an active area of research. This paper establishes the theoretical foundation for the development of SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks, expanding the applicability of neural networks to these advanced graph structures. While mathematical generalizations and proofs are presented, future computational experiments are anticipated.",
    "intriguing_abstract": "Hypergraphs extend traditional graphs by allowing edges to connect multiple nodes, while superhypergraphs further generalize this concept to represent even more complex relationships. Neural networks, inspired by biological systems, are widely used for tasks such as pattern recognition, data classification, and prediction. Graph Neural Networks (GNNs), a well-established framework, have recently been extended to Hypergraph Neural Networks (HGNNs), with their properties and applications being actively studied. The Plithogenic Graph framework enhances graph representations by integrating multi-valued attributes, as well as membership and contradiction functions, enabling the detailed modeling of complex relationships. In the context of handling uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained prominence. It is well established that Plithogenic Graphs serve as a generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the Fuzzy Graph Neural Network has been proposed and is an active area of research. This paper establishes the theoretical foundation for the development of SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks, expanding the applicability of neural networks to these advanced graph structures. While mathematical generalizations and proofs are presented, future computational experiments are anticipated.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf",
    "citation_key": "fujita2024crj",
    "metadata": {
      "title": "Superhypergraph Neural Networks and Plithogenic Graph Neural Networks: Theoretical Foundations",
      "authors": [
        "Takaaki Fujita"
      ],
      "published_date": "2024",
      "abstract": "Hypergraphs extend traditional graphs by allowing edges to connect multiple nodes, while superhypergraphs further generalize this concept to represent even more complex relationships. Neural networks, inspired by biological systems, are widely used for tasks such as pattern recognition, data classification, and prediction. Graph Neural Networks (GNNs), a well-established framework, have recently been extended to Hypergraph Neural Networks (HGNNs), with their properties and applications being actively studied. The Plithogenic Graph framework enhances graph representations by integrating multi-valued attributes, as well as membership and contradiction functions, enabling the detailed modeling of complex relationships. In the context of handling uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained prominence. It is well established that Plithogenic Graphs serve as a generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the Fuzzy Graph Neural Network has been proposed and is an active area of research. This paper establishes the theoretical foundation for the development of SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks, expanding the applicability of neural networks to these advanced graph structures. While mathematical generalizations and proofs are presented, future computational experiments are anticipated.",
      "file_path": "paper_data/Graph_Neural_Networks/info/cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf",
      "venue": "arXiv.org",
      "citationCount": 8,
      "score": 8.0,
      "summary": "Hypergraphs extend traditional graphs by allowing edges to connect multiple nodes, while superhypergraphs further generalize this concept to represent even more complex relationships. Neural networks, inspired by biological systems, are widely used for tasks such as pattern recognition, data classification, and prediction. Graph Neural Networks (GNNs), a well-established framework, have recently been extended to Hypergraph Neural Networks (HGNNs), with their properties and applications being actively studied. The Plithogenic Graph framework enhances graph representations by integrating multi-valued attributes, as well as membership and contradiction functions, enabling the detailed modeling of complex relationships. In the context of handling uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained prominence. It is well established that Plithogenic Graphs serve as a generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the Fuzzy Graph Neural Network has been proposed and is an active area of research. This paper establishes the theoretical foundation for the development of SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks, expanding the applicability of neural networks to these advanced graph structures. While mathematical generalizations and proofs are presented, future computational experiments are anticipated.",
      "keywords": []
    },
    "file_name": "cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf"
  },
  {
    "success": true,
    "doc_id": "377d8d801b7daaa2d498acc67932921d",
    "summary": "In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping.†",
    "intriguing_abstract": "In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping.†",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf",
    "citation_key": "saleh2024d2a",
    "metadata": {
      "title": "Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact",
      "authors": [
        "Mahdi Saleh",
        "Michael Sommersperger",
        "N. Navab",
        "F. Tombari"
      ],
      "published_date": "2024",
      "abstract": "In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping.†",
      "file_path": "paper_data/Graph_Neural_Networks/info/ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf",
      "venue": "IEEE International Conference on Robotics and Automation",
      "citationCount": 8,
      "score": 8.0,
      "summary": "In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping.†",
      "keywords": []
    },
    "file_name": "ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf"
  },
  {
    "success": true,
    "doc_id": "03ecf56c6cdd2fd25b30483c192e95d1",
    "summary": "Graph Neural Networks (GNNs) have demonstrated significant success across diverse domains like social and biological networks. However, their susceptibility to adversarial attacks presents substantial risks in security-sensitive contexts. Even imperceptible perturbations within graphs can lead to considerable performance degradation, highlighting the urgent need for robust GNN models to ensure safety and privacy in critical applications. To address this challenge, we propose training-time optimization-based attacks on GNNs, specifically targeting modifications to graph structures. Our approach revolves around utilizing meta-gradients to tackle the two-level problem inherent in training-time attacks. This involves treating the graph as a hyperparameter to optimize, followed by leveraging convex relaxation and projected momentum optimization techniques to generate the attacks. In our evaluation on node classification tasks, our attacks surpass state-of-the-art methods within the same perturbation budget, underscoring the effectiveness of our approach. Our experiments consistently demonstrate that even minor graph perturbations result in a significant performance decline for graph convolutional networks. Our attacks do not require any prior knowledge of or access to the target classifiers. This research contributes significantly to bolstering the resilience of GNNs against adversarial manipulations in real-world scenarios.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have demonstrated significant success across diverse domains like social and biological networks. However, their susceptibility to adversarial attacks presents substantial risks in security-sensitive contexts. Even imperceptible perturbations within graphs can lead to considerable performance degradation, highlighting the urgent need for robust GNN models to ensure safety and privacy in critical applications. To address this challenge, we propose training-time optimization-based attacks on GNNs, specifically targeting modifications to graph structures. Our approach revolves around utilizing meta-gradients to tackle the two-level problem inherent in training-time attacks. This involves treating the graph as a hyperparameter to optimize, followed by leveraging convex relaxation and projected momentum optimization techniques to generate the attacks. In our evaluation on node classification tasks, our attacks surpass state-of-the-art methods within the same perturbation budget, underscoring the effectiveness of our approach. Our experiments consistently demonstrate that even minor graph perturbations result in a significant performance decline for graph convolutional networks. Our attacks do not require any prior knowledge of or access to the target classifiers. This research contributes significantly to bolstering the resilience of GNNs against adversarial manipulations in real-world scenarios.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/99d50bb7b0155203c908228d086eb232c34ee0a6.pdf",
    "citation_key": "aburidi2024023",
    "metadata": {
      "title": "Topological Adversarial Attacks on Graph Neural Networks Via Projected Meta Learning",
      "authors": [
        "Mohammed Aburidi",
        "Roummel F. Marcia"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success across diverse domains like social and biological networks. However, their susceptibility to adversarial attacks presents substantial risks in security-sensitive contexts. Even imperceptible perturbations within graphs can lead to considerable performance degradation, highlighting the urgent need for robust GNN models to ensure safety and privacy in critical applications. To address this challenge, we propose training-time optimization-based attacks on GNNs, specifically targeting modifications to graph structures. Our approach revolves around utilizing meta-gradients to tackle the two-level problem inherent in training-time attacks. This involves treating the graph as a hyperparameter to optimize, followed by leveraging convex relaxation and projected momentum optimization techniques to generate the attacks. In our evaluation on node classification tasks, our attacks surpass state-of-the-art methods within the same perturbation budget, underscoring the effectiveness of our approach. Our experiments consistently demonstrate that even minor graph perturbations result in a significant performance decline for graph convolutional networks. Our attacks do not require any prior knowledge of or access to the target classifiers. This research contributes significantly to bolstering the resilience of GNNs against adversarial manipulations in real-world scenarios.",
      "file_path": "paper_data/Graph_Neural_Networks/info/99d50bb7b0155203c908228d086eb232c34ee0a6.pdf",
      "venue": "IEEE Conference on Evolving and Adaptive Intelligent Systems",
      "citationCount": 8,
      "score": 8.0,
      "summary": "Graph Neural Networks (GNNs) have demonstrated significant success across diverse domains like social and biological networks. However, their susceptibility to adversarial attacks presents substantial risks in security-sensitive contexts. Even imperceptible perturbations within graphs can lead to considerable performance degradation, highlighting the urgent need for robust GNN models to ensure safety and privacy in critical applications. To address this challenge, we propose training-time optimization-based attacks on GNNs, specifically targeting modifications to graph structures. Our approach revolves around utilizing meta-gradients to tackle the two-level problem inherent in training-time attacks. This involves treating the graph as a hyperparameter to optimize, followed by leveraging convex relaxation and projected momentum optimization techniques to generate the attacks. In our evaluation on node classification tasks, our attacks surpass state-of-the-art methods within the same perturbation budget, underscoring the effectiveness of our approach. Our experiments consistently demonstrate that even minor graph perturbations result in a significant performance decline for graph convolutional networks. Our attacks do not require any prior knowledge of or access to the target classifiers. This research contributes significantly to bolstering the resilience of GNNs against adversarial manipulations in real-world scenarios.",
      "keywords": []
    },
    "file_name": "99d50bb7b0155203c908228d086eb232c34ee0a6.pdf"
  },
  {
    "success": true,
    "doc_id": "c44e093c326b926b800f61c7eff945c9",
    "summary": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf",
    "citation_key": "wang2024481",
    "metadata": {
      "title": "NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise",
      "authors": [
        "Zhonghao Wang",
        "Danyu Sun",
        "Sheng Zhou",
        "Haobo Wang",
        "Jiapei Fan",
        "Longtao Huang",
        "Jiajun Bu"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
      "file_path": "paper_data/Graph_Neural_Networks/info/6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf",
      "venue": "Neural Information Processing Systems",
      "citationCount": 7,
      "score": 7.0,
      "summary": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
      "keywords": []
    },
    "file_name": "6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf"
  },
  {
    "success": true,
    "doc_id": "2cd825751f073f9e470e672eb3c05463",
    "summary": "Graph Neural Networks (GNNs) have become the standard method of choice for learning with structured data, demonstrating particular promise in classical planning. Their inherent invariance under symmetries of the input graphs endows them with superior generalization capabilities, compared to their symmetry-oblivious counterparts. However, this comes at the cost of limited expressive power. Particularly, GNNs cannot distinguish between graphs that satisfy identical sentences of C2 logic.\n \nTo leverage GNNs for learning policies in PDDL domains, one needs to encode the contextual representation of the planning states as graphs. The expressiveness of this encoding, coupled with a specific GNN architecture, then hinges on the absence of indistinguishable states necessitating distinct actions. This paper provides a comprehensive theoretical and statistical exploration of such situations in PDDL domains across diverse natural encoding schemes and GNN models.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have become the standard method of choice for learning with structured data, demonstrating particular promise in classical planning. Their inherent invariance under symmetries of the input graphs endows them with superior generalization capabilities, compared to their symmetry-oblivious counterparts. However, this comes at the cost of limited expressive power. Particularly, GNNs cannot distinguish between graphs that satisfy identical sentences of C2 logic.\n \nTo leverage GNNs for learning policies in PDDL domains, one needs to encode the contextual representation of the planning states as graphs. The expressiveness of this encoding, coupled with a specific GNN architecture, then hinges on the absence of indistinguishable states necessitating distinct actions. This paper provides a comprehensive theoretical and statistical exploration of such situations in PDDL domains across diverse natural encoding schemes and GNN models.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ae683dbd44ec508f63254d864f83d6c1006dd652.pdf",
    "citation_key": "horck2024a8s",
    "metadata": {
      "title": "Expressiveness of Graph Neural Networks in Planning Domains",
      "authors": [
        "Rostislav Horcík",
        "Gustav Sír"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have become the standard method of choice for learning with structured data, demonstrating particular promise in classical planning. Their inherent invariance under symmetries of the input graphs endows them with superior generalization capabilities, compared to their symmetry-oblivious counterparts. However, this comes at the cost of limited expressive power. Particularly, GNNs cannot distinguish between graphs that satisfy identical sentences of C2 logic.\n \nTo leverage GNNs for learning policies in PDDL domains, one needs to encode the contextual representation of the planning states as graphs. The expressiveness of this encoding, coupled with a specific GNN architecture, then hinges on the absence of indistinguishable states necessitating distinct actions. This paper provides a comprehensive theoretical and statistical exploration of such situations in PDDL domains across diverse natural encoding schemes and GNN models.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ae683dbd44ec508f63254d864f83d6c1006dd652.pdf",
      "venue": "International Conference on Automated Planning and Scheduling",
      "citationCount": 7,
      "score": 7.0,
      "summary": "Graph Neural Networks (GNNs) have become the standard method of choice for learning with structured data, demonstrating particular promise in classical planning. Their inherent invariance under symmetries of the input graphs endows them with superior generalization capabilities, compared to their symmetry-oblivious counterparts. However, this comes at the cost of limited expressive power. Particularly, GNNs cannot distinguish between graphs that satisfy identical sentences of C2 logic.\n \nTo leverage GNNs for learning policies in PDDL domains, one needs to encode the contextual representation of the planning states as graphs. The expressiveness of this encoding, coupled with a specific GNN architecture, then hinges on the absence of indistinguishable states necessitating distinct actions. This paper provides a comprehensive theoretical and statistical exploration of such situations in PDDL domains across diverse natural encoding schemes and GNN models.",
      "keywords": []
    },
    "file_name": "ae683dbd44ec508f63254d864f83d6c1006dd652.pdf"
  },
  {
    "success": true,
    "doc_id": "0bf020b73ac908b44ac37cf98d549b75",
    "summary": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
    "intriguing_abstract": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf",
    "citation_key": "sun2024pix",
    "metadata": {
      "title": "Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation",
      "authors": [
        "Jianshan Sun",
        "Suyuan Mei",
        "Kun Yuan",
        "Yuanchun Jiang",
        "Jie Cao"
      ],
      "published_date": "2024",
      "abstract": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
      "file_path": "paper_data/Graph_Neural_Networks/info/6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf",
      "venue": "ACM Transactions on Knowledge Discovery from Data",
      "citationCount": 7,
      "score": 7.0,
      "summary": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
      "keywords": []
    },
    "file_name": "6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf"
  },
  {
    "success": true,
    "doc_id": "5047a9c8022e08f41954a52a066250b6",
    "summary": "This paper explores the escalating complexity of network security threats and the critical need for sophisticated analytical tools to understand and combat these threats effectively. We highlight the efficacy of knowledge graphs in elucidating the complex relationships between various network threats and related entities such as threat actors, attack techniques, and vulnerabilities. Traditional machine learning methods often struggle to grasp the intricate dependencies within these graphs, prompting the adoption of Graph Neural Networks (GNNs). GNNs stand out for their ability to learn representations of graph nodes and edges, capturing and propagating relational information to unearth hidden patterns and facilitate advanced threat analysis. We discuss the advantages of GNNs, including their capacity to integrate diverse data types, handle large-scale knowledge graphs, and reveal critical insights that aid in predicting and mitigating network security threats. Through leveraging GNNs' structural and relational analysis capabilities, this paper demonstrates how organizations can enhance their threat intelligence and develop more robust defense mechanisms against the evolving landscape of network security threats.",
    "intriguing_abstract": "This paper explores the escalating complexity of network security threats and the critical need for sophisticated analytical tools to understand and combat these threats effectively. We highlight the efficacy of knowledge graphs in elucidating the complex relationships between various network threats and related entities such as threat actors, attack techniques, and vulnerabilities. Traditional machine learning methods often struggle to grasp the intricate dependencies within these graphs, prompting the adoption of Graph Neural Networks (GNNs). GNNs stand out for their ability to learn representations of graph nodes and edges, capturing and propagating relational information to unearth hidden patterns and facilitate advanced threat analysis. We discuss the advantages of GNNs, including their capacity to integrate diverse data types, handle large-scale knowledge graphs, and reveal critical insights that aid in predicting and mitigating network security threats. Through leveraging GNNs' structural and relational analysis capabilities, this paper demonstrates how organizations can enhance their threat intelligence and develop more robust defense mechanisms against the evolving landscape of network security threats.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/675a150a89f5c3dd44bf8312b00a896716c7082b.pdf",
    "citation_key": "li2024r82",
    "metadata": {
      "title": "Advancing Cybersecurity: Graph Neural Networks in Threat Intelligence Knowledge Graphs",
      "authors": [
        "Langsha Li",
        "Feng Qiang",
        "Li Ma"
      ],
      "published_date": "2024",
      "abstract": "This paper explores the escalating complexity of network security threats and the critical need for sophisticated analytical tools to understand and combat these threats effectively. We highlight the efficacy of knowledge graphs in elucidating the complex relationships between various network threats and related entities such as threat actors, attack techniques, and vulnerabilities. Traditional machine learning methods often struggle to grasp the intricate dependencies within these graphs, prompting the adoption of Graph Neural Networks (GNNs). GNNs stand out for their ability to learn representations of graph nodes and edges, capturing and propagating relational information to unearth hidden patterns and facilitate advanced threat analysis. We discuss the advantages of GNNs, including their capacity to integrate diverse data types, handle large-scale knowledge graphs, and reveal critical insights that aid in predicting and mitigating network security threats. Through leveraging GNNs' structural and relational analysis capabilities, this paper demonstrates how organizations can enhance their threat intelligence and develop more robust defense mechanisms against the evolving landscape of network security threats.",
      "file_path": "paper_data/Graph_Neural_Networks/info/675a150a89f5c3dd44bf8312b00a896716c7082b.pdf",
      "venue": "International Conference on Algorithms, Software Engineering, and Network Security",
      "citationCount": 7,
      "score": 7.0,
      "summary": "This paper explores the escalating complexity of network security threats and the critical need for sophisticated analytical tools to understand and combat these threats effectively. We highlight the efficacy of knowledge graphs in elucidating the complex relationships between various network threats and related entities such as threat actors, attack techniques, and vulnerabilities. Traditional machine learning methods often struggle to grasp the intricate dependencies within these graphs, prompting the adoption of Graph Neural Networks (GNNs). GNNs stand out for their ability to learn representations of graph nodes and edges, capturing and propagating relational information to unearth hidden patterns and facilitate advanced threat analysis. We discuss the advantages of GNNs, including their capacity to integrate diverse data types, handle large-scale knowledge graphs, and reveal critical insights that aid in predicting and mitigating network security threats. Through leveraging GNNs' structural and relational analysis capabilities, this paper demonstrates how organizations can enhance their threat intelligence and develop more robust defense mechanisms against the evolving landscape of network security threats.",
      "keywords": []
    },
    "file_name": "675a150a89f5c3dd44bf8312b00a896716c7082b.pdf"
  },
  {
    "success": true,
    "doc_id": "124df2b38c0b89879b5142089695ce83",
    "summary": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
    "intriguing_abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/d42ebd3b0673341125e374223e0882e99557cc8c.pdf",
    "citation_key": "luo20240ot",
    "metadata": {
      "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
      "authors": [
        "Renqiang Luo",
        "Huafei Huang",
        "Shuo Yu",
        "Zhuoyang Han",
        "Estrid He",
        "Xiuzhen Zhang",
        "Feng Xia"
      ],
      "published_date": "2024",
      "abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
      "file_path": "paper_data/Graph_Neural_Networks/info/d42ebd3b0673341125e374223e0882e99557cc8c.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 7,
      "score": 7.0,
      "summary": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
      "keywords": []
    },
    "file_name": "d42ebd3b0673341125e374223e0882e99557cc8c.pdf"
  },
  {
    "success": true,
    "doc_id": "e203c92b3463a3f80c3d99bde87f76e0",
    "summary": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data. In this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
    "intriguing_abstract": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data. In this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/13391f9fb2094227ecc567fef76fd95adc57e972.pdf",
    "citation_key": "li202492k",
    "metadata": {
      "title": "Towards Bridging Generalization and Expressivity of Graph Neural Networks",
      "authors": [
        "Shouheng Li",
        "F. Geerts",
        "Dongwoo Kim",
        "Qing Wang"
      ],
      "published_date": "2024",
      "abstract": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data. In this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
      "file_path": "paper_data/Graph_Neural_Networks/info/13391f9fb2094227ecc567fef76fd95adc57e972.pdf",
      "venue": "International Conference on Learning Representations",
      "citationCount": 6,
      "score": 6.0,
      "summary": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data. In this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
      "keywords": []
    },
    "file_name": "13391f9fb2094227ecc567fef76fd95adc57e972.pdf"
  },
  {
    "success": true,
    "doc_id": "95370dfcbb47c45028810b97a8384e3a",
    "summary": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf",
    "citation_key": "liao20249wq",
    "metadata": {
      "title": "Graph Neural Networks on Quantum Computers",
      "authors": [
        "Yidong Liao",
        "Xiao-Ming Zhang",
        "Chris Ferrie"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
      "file_path": "paper_data/Graph_Neural_Networks/info/406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf",
      "venue": "arXiv.org",
      "citationCount": 6,
      "score": 6.0,
      "summary": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
      "keywords": []
    },
    "file_name": "406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf"
  },
  {
    "success": true,
    "doc_id": "bbf117cd6479f9061cef57c526ca87e9",
    "summary": "In recent years, Temporal Graph Neural Networks (TGNNs) have achieved great success in learning tasks for graphs that change over time. These dynamic/temporal graphs represent topology changes as either discrete static graph snapshots (called DTDGs), or a continuous stream of timestamped edges (called CTDGs). Because continuous-time graphs have richer time information, it will be crucial to have abstractions for programming CTDG-based models so that practitioners can easily explore new designs and optimizations in this space. A few recent frameworks have been proposed for programming and accelerating TGNN models, but these either do not support continuous-time graphs, lack easy composability, and/or do not facilitate CTDG-specific optimizations. In this paper, we propose a lightweight framework called TGLite to fill this apparent gap in the status quo. It provides abstractions that serve as composable building blocks for implementing TGNN models for CTDGs. It introduces a novel TBlock representation for capturing message-flow dependencies between nodes, with explicit support for temporal-related attributes, which is well-suited for common TGNN computation patterns. TBlocks serve as a central representation on which many different operators can be defined, such as temporal neighborhood sampling, scatter/segmented computations, as well as optimizations tailored to CTDGs. We use TGLite to implement four existing TGNN models. Compared to the TGL framework, TGLite is able to accelerate runtime performance of training (1.06 -- 3.43×) and inference (1.09 -- 4.65×) of these models on V100 and A100 GPUs across different experimental settings. Notably, when scaling to larger datasets, TGL runs out-of-memory in some cases on the V100 while TGLite is able to run successfully.",
    "intriguing_abstract": "In recent years, Temporal Graph Neural Networks (TGNNs) have achieved great success in learning tasks for graphs that change over time. These dynamic/temporal graphs represent topology changes as either discrete static graph snapshots (called DTDGs), or a continuous stream of timestamped edges (called CTDGs). Because continuous-time graphs have richer time information, it will be crucial to have abstractions for programming CTDG-based models so that practitioners can easily explore new designs and optimizations in this space. A few recent frameworks have been proposed for programming and accelerating TGNN models, but these either do not support continuous-time graphs, lack easy composability, and/or do not facilitate CTDG-specific optimizations. In this paper, we propose a lightweight framework called TGLite to fill this apparent gap in the status quo. It provides abstractions that serve as composable building blocks for implementing TGNN models for CTDGs. It introduces a novel TBlock representation for capturing message-flow dependencies between nodes, with explicit support for temporal-related attributes, which is well-suited for common TGNN computation patterns. TBlocks serve as a central representation on which many different operators can be defined, such as temporal neighborhood sampling, scatter/segmented computations, as well as optimizations tailored to CTDGs. We use TGLite to implement four existing TGNN models. Compared to the TGL framework, TGLite is able to accelerate runtime performance of training (1.06 -- 3.43×) and inference (1.09 -- 4.65×) of these models on V100 and A100 GPUs across different experimental settings. Notably, when scaling to larger datasets, TGL runs out-of-memory in some cases on the V100 while TGLite is able to run successfully.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf",
    "citation_key": "wang2024ged",
    "metadata": {
      "title": "TGLite: A Lightweight Programming Framework for Continuous-Time Temporal Graph Neural Networks",
      "authors": [
        "Yufeng Wang",
        "Charith Mendis"
      ],
      "published_date": "2024",
      "abstract": "In recent years, Temporal Graph Neural Networks (TGNNs) have achieved great success in learning tasks for graphs that change over time. These dynamic/temporal graphs represent topology changes as either discrete static graph snapshots (called DTDGs), or a continuous stream of timestamped edges (called CTDGs). Because continuous-time graphs have richer time information, it will be crucial to have abstractions for programming CTDG-based models so that practitioners can easily explore new designs and optimizations in this space. A few recent frameworks have been proposed for programming and accelerating TGNN models, but these either do not support continuous-time graphs, lack easy composability, and/or do not facilitate CTDG-specific optimizations. In this paper, we propose a lightweight framework called TGLite to fill this apparent gap in the status quo. It provides abstractions that serve as composable building blocks for implementing TGNN models for CTDGs. It introduces a novel TBlock representation for capturing message-flow dependencies between nodes, with explicit support for temporal-related attributes, which is well-suited for common TGNN computation patterns. TBlocks serve as a central representation on which many different operators can be defined, such as temporal neighborhood sampling, scatter/segmented computations, as well as optimizations tailored to CTDGs. We use TGLite to implement four existing TGNN models. Compared to the TGL framework, TGLite is able to accelerate runtime performance of training (1.06 -- 3.43×) and inference (1.09 -- 4.65×) of these models on V100 and A100 GPUs across different experimental settings. Notably, when scaling to larger datasets, TGL runs out-of-memory in some cases on the V100 while TGLite is able to run successfully.",
      "file_path": "paper_data/Graph_Neural_Networks/info/0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf",
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "citationCount": 6,
      "score": 6.0,
      "summary": "In recent years, Temporal Graph Neural Networks (TGNNs) have achieved great success in learning tasks for graphs that change over time. These dynamic/temporal graphs represent topology changes as either discrete static graph snapshots (called DTDGs), or a continuous stream of timestamped edges (called CTDGs). Because continuous-time graphs have richer time information, it will be crucial to have abstractions for programming CTDG-based models so that practitioners can easily explore new designs and optimizations in this space. A few recent frameworks have been proposed for programming and accelerating TGNN models, but these either do not support continuous-time graphs, lack easy composability, and/or do not facilitate CTDG-specific optimizations. In this paper, we propose a lightweight framework called TGLite to fill this apparent gap in the status quo. It provides abstractions that serve as composable building blocks for implementing TGNN models for CTDGs. It introduces a novel TBlock representation for capturing message-flow dependencies between nodes, with explicit support for temporal-related attributes, which is well-suited for common TGNN computation patterns. TBlocks serve as a central representation on which many different operators can be defined, such as temporal neighborhood sampling, scatter/segmented computations, as well as optimizations tailored to CTDGs. We use TGLite to implement four existing TGNN models. Compared to the TGL framework, TGLite is able to accelerate runtime performance of training (1.06 -- 3.43×) and inference (1.09 -- 4.65×) of these models on V100 and A100 GPUs across different experimental settings. Notably, when scaling to larger datasets, TGL runs out-of-memory in some cases on the V100 while TGLite is able to run successfully.",
      "keywords": []
    },
    "file_name": "0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf"
  },
  {
    "success": true,
    "doc_id": "f5407e40a232331c161966d4048def98",
    "summary": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
    "intriguing_abstract": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/f60492aece8e86203ed95303cb809332a11d74b5.pdf",
    "citation_key": "liu20245da",
    "metadata": {
      "title": "LinkSAGE: Optimizing Job Matching Using Graph Neural Networks",
      "authors": [
        "Ping Liu",
        "Haichao Wei",
        "Xiaochen Hou",
        "Jianqiang Shen",
        "Shihai He",
        "Qianqi Shen",
        "Zhujun Chen",
        "Fedor Borisyuk",
        "Daniel Hewlett",
        "Liang Wu",
        "S. Veeraraghavan",
        "Alex Tsun",
        "Chen-Chen Jiang",
        "Wenjing Zhang"
      ],
      "published_date": "2024",
      "abstract": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
      "file_path": "paper_data/Graph_Neural_Networks/info/f60492aece8e86203ed95303cb809332a11d74b5.pdf",
      "venue": "Knowledge Discovery and Data Mining",
      "citationCount": 6,
      "score": 6.0,
      "summary": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
      "keywords": []
    },
    "file_name": "f60492aece8e86203ed95303cb809332a11d74b5.pdf"
  },
  {
    "success": true,
    "doc_id": "84c57f8f4b40cc28d3f0a351874bb846",
    "summary": "Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.",
    "intriguing_abstract": "Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf",
    "citation_key": "varghese2024ygs",
    "metadata": {
      "title": "SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification",
      "authors": [
        "Alan John Varghese",
        "Zhen Zhang",
        "G. Karniadakis"
      ],
      "published_date": "2024",
      "abstract": "Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.",
      "file_path": "paper_data/Graph_Neural_Networks/info/b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf",
      "venue": "Neural Networks",
      "citationCount": 5,
      "score": 5.0,
      "summary": "Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.",
      "keywords": []
    },
    "file_name": "b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf"
  },
  {
    "success": true,
    "doc_id": "c183f8332bbc7ed98e43e0d32b75a102",
    "summary": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion. Based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they were demonstrated to be equivalent (Morris et al., 2019 and Xu et al., 2019). From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability - related to the Vapnik Chervonekis (VC) dimension (Scarselli et al., 2018) - has recently been investigated for GNNs with piecewise polynomial activation functions (Morris et al., 2023). The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as the sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to the architecture parameters (depth, number of neurons, input size) as well as with respect to the number of colors resulting from the 1-WL test applied on the graph domain. The theoretical analysis is supported by a preliminary experimental study.",
    "intriguing_abstract": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion. Based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they were demonstrated to be equivalent (Morris et al., 2019 and Xu et al., 2019). From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability - related to the Vapnik Chervonekis (VC) dimension (Scarselli et al., 2018) - has recently been investigated for GNNs with piecewise polynomial activation functions (Morris et al., 2023). The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as the sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to the architecture parameters (depth, number of neurons, input size) as well as with respect to the number of colors resulting from the 1-WL test applied on the graph domain. The theoretical analysis is supported by a preliminary experimental study.",
    "keywords": [],
    "file_path": "paper_data/Graph_Neural_Networks/ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf",
    "citation_key": "dinverno2024vkw",
    "metadata": {
      "title": "VC dimension of Graph Neural Networks with Pfaffian activation functions",
      "authors": [
        "Giuseppe Alessio D’Inverno",
        "M. Bianchini",
        "F. Scarselli"
      ],
      "published_date": "2024",
      "abstract": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion. Based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they were demonstrated to be equivalent (Morris et al., 2019 and Xu et al., 2019). From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability - related to the Vapnik Chervonekis (VC) dimension (Scarselli et al., 2018) - has recently been investigated for GNNs with piecewise polynomial activation functions (Morris et al., 2023). The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as the sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to the architecture parameters (depth, number of neurons, input size) as well as with respect to the number of colors resulting from the 1-WL test applied on the graph domain. The theoretical analysis is supported by a preliminary experimental study.",
      "file_path": "paper_data/Graph_Neural_Networks/info/ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf",
      "venue": "Neural Networks",
      "citationCount": 3,
      "score": 3.0,
      "summary": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion. Based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they were demonstrated to be equivalent (Morris et al., 2019 and Xu et al., 2019). From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability - related to the Vapnik Chervonekis (VC) dimension (Scarselli et al., 2018) - has recently been investigated for GNNs with piecewise polynomial activation functions (Morris et al., 2023). The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as the sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to the architecture parameters (depth, number of neurons, input size) as well as with respect to the number of colors resulting from the 1-WL test applied on the graph domain. The theoretical analysis is supported by a preliminary experimental study.",
      "keywords": []
    },
    "file_name": "ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf"
  }
]