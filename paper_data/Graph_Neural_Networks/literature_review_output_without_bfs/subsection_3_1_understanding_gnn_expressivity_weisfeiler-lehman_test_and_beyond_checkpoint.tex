\subsection*{Understanding GNN Expressivity: Weisfeiler-Lehman Test and Beyond}

The theoretical expressive power of Graph Neural Networks (GNNs) is a critical area of research, particularly concerning their ability to distinguish non-isomorphic graphs and capture complex structural patterns. A foundational insight in this domain connects the discriminative power of many standard GNNs to the Weisfeiler-Lehman (WL) test for graph isomorphism. Early work by \cite{morris20185sd} rigorously demonstrated that standard message-passing GNNs (1-GNNs) are at most as powerful as the 1-dimensional WL test (1-WL), and under suitable conditions, achieve equivalent power. This equivalence implies inherent limitations, as 1-WL cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or count specific substructures.

These limitations manifest in GNNs' inability to capture higher-order structural information. For instance, \cite{chen2020e6g} formally proved that Message Passing Neural Networks (MPNNs) and even 2-Invariant Graph Networks (2-IGNs) cannot count induced subgraphs for any connected pattern with three or more nodes. Extending this, \cite{garg2020z6o} provided impossibility proofs, showing that a broad class of GNNs, including popular Locally Unordered GNNs (LU-GNNs) and CPNGNNs, cannot compute fundamental graph properties such as girth, circumference, diameter, or k-clique. The review by \cite{reiser2022b08} further acknowledges these expressivity limitations, alongside issues like over-squashing and over-smoothing, as key challenges for GNNs in practical applications like chemistry and materials science.

To overcome the 1-WL bottleneck, researchers have explored several avenues, primarily focusing on higher-order architectures that can capture richer structural information. \cite{morris20185sd} initially proposed k-dimensional GNNs (k-GNNs), which perform message passing on k-element subsets (k-tuples) of nodes, theoretically achieving greater discriminative power than 1-WL. However, these k-GNNs often incur prohibitive computational costs due to the exponential growth in the number of k-tuples. Addressing this efficiency concern, \cite{balcilar20215ga} introduced GNNML3, a spectral approach that leverages non-linear custom functions of eigenvalues to design graph convolution supports. This method experimentally matches the power of 3-WL equivalent models while maintaining linear computational complexity after an initial eigendecomposition, offering a more scalable alternative to direct k-tuple processing.

Other approaches enhance expressivity by explicitly incorporating path or subgraph information. \cite{michel2023hc4} developed Path Neural Networks (PathNNs) that aggregate information from various paths. Their most expressive variant, operating on "annotated sets of paths," was theoretically shown to be strictly more powerful than 1-WL and empirically capable of distinguishing graphs indistinguishable by the 3-WL algorithm. Similarly, \cite{zeng20237gv} proposed Substructure Aware Graph Neural Networks (SAGNN), which inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk encodings, into node features. SAGNNs are provably more powerful than 1-WL and can also distinguish graphs that fool the 3-WL test. For tasks like link prediction, \cite{chamberlain2022fym} introduced ELPH and BUDDY, full-graph GNNs that efficiently approximate subgraph features (or "sketches") to achieve expressivity comparable to subgraph-based GNNs, effectively breaking 1-WL limitations for this specific task.

Another powerful strategy involves augmenting GNNs with positional or structural identifiers. \cite{abboud2020x5e} demonstrated the "surprising power" of standard MPNNs when augmented with Random Node Initialization (RNI), proving that such models are universal approximators for functions on graphs of a fixed order, thereby efficiently breaking the 1-WL barrier. Building on this, \cite{papp20211ac} proposed DropGNN, which uses random node dropouts during both training and testing across multiple runs to systematically explore perturbed graph structures, proving that this mechanism increases expressiveness beyond 1-WL. Furthermore, \cite{dwivedi2021af0} introduced Learnable Structural and Positional Encodings (LSPE), a framework that decouples and learns both structural and positional representations throughout the GNN layers, utilizing a Random Walk Positional Encoding (RWPE) to avoid sign ambiguity issues common in Laplacian-based encodings. This concept was further refined by \cite{wang2022p2r} with Equivariant and Stable Positional Encoding (PEG), which uses separate channels for node and positional features, ensuring O(p) equivariance and stability for positional embeddings, particularly beneficial for node-set tasks. The utility of positional encodings, such as Laplacian eigenvectors, for enhancing GNN expressivity on challenging datasets was also empirically highlighted by \cite{dwivedi20239ab}.

For domains where geometric information is paramount, such as molecular modeling, specialized equivariant GNNs have pushed expressivity further. \cite{klicpera20215fk} introduced GemNet, proving universality for GNNs using spherical (S2) representations and incorporating a novel two-hop geometric message passing scheme that explicitly captures distances, angles, and dihedral angles. Similarly, \cite{satorras2021pzl} developed E(n)-Equivariant Graph Neural Networks (EGNNs), which directly update node coordinates and embeddings while preserving E(n) equivariance, offering a simpler and more scalable alternative to methods relying on complex higher-order representations. Beyond static structural information, \cite{finkelshtein202301z} proposed Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose their communication actions (listen, broadcast, isolate) at each layer. This dynamic and asynchronous message passing paradigm is theoretically shown to be more expressive than 1-WL due to the variance introduced by action sampling.

While enhancing theoretical expressivity, practical limitations like "over-squashing" can hinder the effective utilization of deep GNNs for long-range dependencies, as identified by \cite{alon2020fok}. This phenomenon limits the amount of information that can be propagated across many layers. To address this, \cite{zeng2022jhz} proposed decoupling the depth and scope of GNNs by applying deep GNNs on shallow, localized subgraphs. This approach not only mitigates oversmoothing but also enhances expressivity beyond 1-WL by allowing deeper processing of local information. More recently, \cite{geisler2024wli} introduced Spatio-Spectral Graph Neural Networks (S2GNNs), a hybrid paradigm combining local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to overcome over-squashing and are strictly more expressive than 1-WL, offering tighter approximation bounds and "free" positional encodings. Finally, the expressivity of spectral GNNs has also been re-evaluated, with \cite{wang2022u2l} demonstrating that even linear spectral GNNs can achieve universal approximation under specific conditions, challenging the necessity of non-linearity and connecting their power to the 1-WL test.

In conclusion, understanding the theoretical bounds imposed by the Weisfeiler-Lehman test is crucial for designing more powerful and discriminative GNN models. The field has moved significantly beyond the 1-WL limitations of standard message-passing architectures through various innovations. These include the development of higher-order GNNs, the strategic augmentation of node features with positional or structural encodings, the incorporation of geometric and dynamic message-passing mechanisms, and the exploration of hybrid spatial-spectral architectures. While significant progress has been made in enhancing expressivity, ongoing challenges remain in balancing this power with computational scalability, data efficiency, and generalization to diverse real-world graph structures and tasks.