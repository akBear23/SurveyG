\subsection*{Adversarial Robustness and Defenses}

Graph Neural Networks (GNNs) have demonstrated remarkable capabilities across various domains, yet their vulnerability to adversarial attacks poses significant security and reliability concerns, particularly in critical applications. These attacks involve small, often imperceptible, perturbations to the graph structure or node features that can drastically alter a GNN's predictions. The field has seen a rapid evolution in understanding these vulnerabilities and developing corresponding defense mechanisms.

Early research highlighted the susceptibility of GNNs to poisoning attacks, where an adversary manipulates the training data. \cite{zgner2019bbi} introduced the first global poisoning attack on GNNs, formulating it as a bilevel optimization problem solved via meta-gradients to degrade overall model performance. This demonstrated that GNNs are vulnerable beyond targeted misclassifications, even performing worse than non-relational baselines under attack. Building on this, \cite{xu2019l8n} proposed an optimization-based framework for both topology attacks and defenses. They addressed the challenge of discrete graph structures by relaxing edge perturbation variables to continuous ones, enabling gradient-based Projected Gradient Descent (PGD) attacks and min-max adversarial training for robust GNNs.

In response to these emerging threats, several defense mechanisms were proposed. \cite{zhang2020jrt} introduced GNNGuard, a general, plug-and-play defense algorithm designed to protect against training-time structural attacks. GNNGuard operates by dynamically estimating neighbor importance and employing a layer-wise graph memory to prune suspicious edges and stabilize message passing, notably demonstrating effectiveness even on heterophily graphs, a limitation of prior defenses. Further enhancing robustness, \cite{liu2021ee2} developed Elastic Graph Neural Networks (Elastic GNNs) that incorporate \(\ell_1\)-based graph smoothing. This approach allows for adaptive local smoothness, preserving important discontinuities while providing enhanced robustness against adversarial attacks, outperforming \(\ell_2\)-based smoothing methods.

The landscape of adversarial attacks also expanded beyond simple poisoning. \cite{he2020kz4} revealed a critical privacy vulnerability by demonstrating "link stealing attacks," where an adversary can infer the underlying graph structure from a black-box GNN's outputs. This highlights that GNNs implicitly encode significant structural information, which can be exploited for privacy breaches. Furthermore, \cite{zhang2020b0m} introduced backdoor attacks to GNNs for graph classification tasks. These attacks inject a predefined subgraph trigger into a small fraction of training graphs, causing the trained GNN to consistently misclassify any graph containing this trigger to a target label, posing a stealthy and persistent threat.

A significant challenge in GNN robustness research has been the scalability of both attack and defense methods to real-world, large-scale graphs. Addressing this, \cite{geisler2021dcq} developed sparsity-aware first-order optimization attacks, such as Projected Randomized Block Coordinate Descent (PR-BCD), which overcome the prohibitive memory requirements of prior methods. They also introduced novel surrogate losses (e.g., Masked Cross Entropy) for more effective global attacks and a scalable robust aggregation function (Soft Median) for defenses, enabling the study of GNN robustness on graphs orders of magnitude larger than previously possible.

Despite the proliferation of defense mechanisms, a critical evaluation of their true effectiveness against adaptive adversaries remained largely unexplored. \cite{mujkanovic20238fi} conducted a comprehensive study, rigorously evaluating seven popular GNN defenses against custom-designed adaptive attacks. Their findings were sobering: most existing defenses offered "no or only marginal improvement" compared to an undefended baseline when faced with an adversary aware of the defense mechanism. This work, which introduced a systematic methodology for designing strong adaptive attacks (including a novel Meta-PGD attack), exposed a significant overestimation of GNN robustness in prior literature, akin to similar revelations in the computer vision community.

The ongoing challenge of creating truly robust GNNs against adaptive adversaries, who can circumvent naive protective measures, remains a crucial area for continued research. The findings of \cite{mujkanovic20238fi} underscore the need for a paradigm shift in GNN defense evaluation, advocating for adaptive attacks as the new gold standard. Future work must focus on developing inherently robust GNN architectures and training strategies that can withstand sophisticated, white-box adversaries, moving beyond superficial improvements to achieve genuine security and reliability in real-world deployments.
```