\subsection{Handling Real-World Graph Imperfections: Heterophily and Structure Learning}

Real-world graphs frequently deviate from ideal homophilic assumptions, presenting significant challenges for Graph Neural Networks (GNNs). These imperfections manifest as heterophily, where connected nodes exhibit dissimilar features or labels, or as incomplete, noisy, or entirely absent structural information. Addressing these issues is paramount for extending GNN utility beyond controlled datasets to the complex realities of real-world graph data, demanding more flexible and robust models.

The pervasive nature of heterophily, where nodes connect to dissimilar neighbors, is a primary concern for traditional GNNs that implicitly rely on homophilic message passing \cite{zheng2022qxr}. However, a nuanced understanding reveals that not all heterophily is equally detrimental; GCNs can perform well on certain heterophilous graphs if same-label nodes exhibit consistent neighborhood patterns, even if those neighbors are of different labels \cite{ma2021sim}. This highlights the concept of "structural disparity," where real-world graphs contain a mixture of homophilic and heterophilic patterns, leading to performance disparities where GNNs excel on majority patterns but struggle on minority ones \cite{mao202313j}. To better characterize these complexities, \cite{luan202272y} introduced novel post-aggregation similarity metrics, such as Aggregation Similarity Score ($S_{agg}$) and Graph Aggregation Homophily ($H_{agg}(G)$), which more accurately reflect GNN performance under heterophily than traditional metrics.

To address the limitations of uniform, global filtering in such mixed-pattern graphs, researchers have developed adaptive filtering mechanisms. \cite{luan202272y} proposed the Adaptive Channel Mixing (ACM) framework, which augments baseline GNNs by adaptively exploiting aggregation (low-pass), diversification (high-pass), and identity channels node-wisely. While effective, ACM's channel weights are learned globally per layer, potentially limiting its granularity. Building on this, \cite{han2024rkj} introduced NODE-MOE (Node-wise Filtering via Mixture of Experts), a more granular approach where a gating model dynamically selects and applies appropriate "expert" GNNs, each potentially equipped with a different filter type, to individual nodes based on their local patterns. This theoretically demonstrates the suboptimality of global filters and offers superior adaptability, albeit with increased computational overhead due to the gating mechanism and multiple expert networks. A related strategy for handling heterophily involves transforming the input graph itself to improve its assortativity. \cite{suresh202191q} proposed creating a multi-relational "computation graph" that distinguishes between proximity and structural information, thereby enhancing node-level assortativity and improving GNN performance under diverse mixing patterns. This approach implicitly adapts the graph structure to better suit GNNs.

A distinct strategy for heterophily involves seeking "global homophily" rather than purely local adaptation. \cite{li2022315} proposed GloGNN/GloGNN++, which aggregates information from the entire graph using a learned signed coefficient matrix. This matrix implicitly combines low-pass and high-pass filtering by assigning positive coefficients to homophilous nodes and negative ones to heterophilous nodes, effectively leveraging global correlations. Crucially, GloGNN achieves this with linear time complexity ($O(k^2n)$) by reordering matrix multiplications, making it highly scalable compared to naive global aggregation ($O(n^3)$ or $O(n^2c)$). While powerful for capturing long-range homophilous connections, the effectiveness of GloGNN relies on the assumption that such global patterns exist and are learnable, which might not hold for all graph types.

Beyond heterophily, the absence or imperfection of graph structures presents another critical challenge. When initial graph structures are noisy, incomplete, or unavailable, GNNs require mechanisms to learn or augment these structures. \cite{chen2020bvl} pioneered the Iterative Deep Graph Learning (IDGL) framework, which jointly and iteratively learns optimal graph structures and GNN parameters, explicitly optimizing for downstream tasks. This framework formulates graph learning as a similarity metric learning problem, enabling inductive capabilities and robustness to adversarial graph examples. However, this joint optimization process is computationally intensive ($O(n^2)$ for the full graph) and risks converging to suboptimal structures if not carefully regularized. To mitigate this, IDGL introduced a scalable anchor-based version (IDGL-ANCH) that reduces complexity to $O(ns)$ for large graphs, though it assumes noiseless node features.

Expanding on structure learning, \cite{fatemi2021dmb} introduced SLAPS (Self-Supervision Improves Structure Learning), which addresses the "supervision starvation" problem in latent graph learning by employing a self-supervised denoising autoencoder task. This provides additional supervision for learning a more robust graph structure itself, particularly beneficial when explicit labels for structure learning are scarce. Similarly, \cite{wei20246l2} explored self-supervised GNNs for enhanced feature extraction in Heterogeneous Information Networks (HINs), aiming to flexibly combine diverse information types to mine deep features and improve adaptability to graph diversity and complexity. For augmenting existing graphs, \cite{zhao2020bmj} proposed GAUG, a framework that leverages neural edge predictors to strategically add "missing" intra-class edges and remove "noisy" inter-class edges, thereby promoting class-homophilic structure and improving performance. In scenarios of "extreme weak information," where structure, features, and labels are simultaneously deficient, \cite{liu2023v3e} introduced D2PT (Dual-channel Diffused Propagation then Transformation), a comprehensive dual-channel GNN framework that combines an input graph backbone with a learned global graph and prototype contrastive alignment to effectively propagate information and connect stray nodes. This represents a robust solution for the most challenging structure-deficient environments.

In conclusion, the research landscape for GNNs operating on imperfect real-world data has evolved significantly, moving towards more flexible and robust models. Adaptive filtering mechanisms like ACM and NODE-MOE offer fine-grained control over message passing to handle structural disparities, albeit with varying computational overheads. Concurrently, methods like GloGNN provide scalable global aggregation for heterophily. For structural deficiencies, iterative and self-supervised structure learning frameworks such as IDGL, SLAPS, and D2PT enable GNNs to operate effectively even with noisy or absent graph topologies. While substantial progress has been made, challenges remain in ensuring the computational efficiency and theoretical guarantees of dynamic graph learning mechanisms, especially for extremely large and evolving graphs. Furthermore, designing models that can seamlessly adapt between diverse local and global structural patterns without explicit architectural changes or significant overhead remains an active area of research, particularly in achieving truly universal generalization across all types of complex, unseen graph imperfections.