\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 328 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Graph Data and its Significance}
\label{sec:1\_1\_graph\_data\_\_and\_\_its\_significance}

Graph-structured data, fundamentally characterized by a collection of nodes (entities) and edges (relationships) that may possess associated attributes, constitutes a powerful and ubiquitous paradigm for modeling complex real-world systems. Formally, a graph $G = (V, E, X, A)$ comprises a set of nodes $V$, a set of edges $E$ connecting pairs of nodes, an optional node feature matrix $X \in \mathbb{R}^{|V| \times d\_v}$ where $d\_v$ is the dimension of node features, and an optional edge attribute matrix $A \in \mathbb{R}^{|E| \times d\_e}$ for edge features. Edges can be directed or undirected, weighted or unweighted, and graphs can be homogeneous (nodes and edges of a single type) or heterogeneous (multiple types of nodes and edges) \cite{wu20193b0, zhang2021jqr}. Common representations include the adjacency matrix, which captures connectivity, and feature matrices for nodes and edges.

The pervasive nature of graphs in modeling intricate interdependencies makes them indispensable across diverse domains. In social sciences, graphs represent social networks, mapping individuals and their friendships, collaborations, or communication patterns \cite{wu2022ptq}. In biology and chemistry, they model molecular structures (atoms as nodes, bonds as edges), protein-protein interaction networks, or gene regulatory pathways \cite{wu20193b0}. Knowledge graphs organize factual information by connecting entities with semantic relationships, forming the backbone of many AI systems. Beyond these, graphs are crucial in transportation networks, recommender systems (users and items), citation networks (papers and citations), and communication infrastructures, offering a rich, holistic representation of relational information that is often lost in simpler data structures \cite{wang2023zr0}. This ability to explicitly encode relationships and context is what makes graph data profoundly significant for understanding complex systems.

However, the inherent characteristics of graph data present unique and substantial challenges for traditional machine learning algorithms, which are primarily designed for Euclidean data (e.g., images as grids, text as sequences) \cite{wu20193b0, wang2023zr0}. The most prominent challenge stems from the \textbf{non-Euclidean nature} of graphs. Unlike images or sequences, graphs lack a fixed grid structure, a canonical node ordering, or a global coordinate system. This means that graph data is permutation-invariant; the underlying structure and properties of a graph remain the same regardless of how its nodes are ordered or indexed. Traditional deep learning models like Convolutional Neural Networks (CNNs) rely on local connectivity and translation invariance on grid-like data, while Recurrent Neural Networks (RNNs) assume sequential dependencies. Neither paradigm naturally extends to the irregular, variable-sized, and arbitrarily structured topology of graphs \cite{zhang2021jqr}.

Furthermore, the \textbf{interdependence of elements} within a graph is both its strength and a significant challenge. The features and labels of a node are often strongly influenced by its neighbors and the broader graph topology. This relational context is crucial for accurate predictions, yet traditional machine learning models typically assume independent and identically distributed (i.i.d.) data samples. Applying standard classifiers to individual nodes or edges in isolation would ignore this vital relational information, leading to suboptimal performance \cite{wu2022ptq}. Extracting meaningful features from graph structures for traditional machine learning models often requires extensive, handcrafted feature engineering (e.g., centrality measures, graphlet counts), which is labor-intensive, domain-specific, and may not generalize well across different graphs or tasks. This process is also prone to losing rich topological information.

These challenges highlight a critical gap: the need for specialized machine learning models capable of directly processing graph-structured data, learning rich, distributed representations that simultaneously encode both node features and their structural context. This necessity has driven the development of Graph Neural Networks (GNNs). GNNs are designed to overcome the limitations of traditional methods by enabling end-to-end learning directly on graph topology, capturing intricate relational patterns and structural information through message-passing mechanisms \cite{wu20193b0, wang2023zr0}. Understanding these fundamental characteristics of graph data and the inherent difficulties they pose for conventional machine learning is crucial for appreciating why GNNs have emerged as an indispensable and rapidly evolving paradigm for effective analysis and prediction in graph domains. The subsequent sections of this review will delve into how GNNs address these challenges through various architectural advancements and learning paradigms.
\subsection{Historical Context and Evolution of Graph Neural Networks}
\label{sec:1\_2\_historical\_context\_\_and\_\_evolution\_of\_graph\_neural\_networks}

The advent of Graph Neural Networks (GNNs) marks a pivotal paradigm shift in machine learning, fundamentally transforming how complex relational data is processed and understood. Prior to GNNs, analyzing graph-structured data, ubiquitous in domains from social networks to molecular chemistry, presented significant challenges. Traditional graph analysis algorithms often relied on labor-intensive, handcrafted features, which were inherently rigid, domain-specific, and struggled to scale with the combinatorial complexity of large or dynamic graphs. Concurrently, conventional neural networks, designed for the grid-like structures of Euclidean data (e.g., images, sequences), lacked the intrinsic architectural inductive biases to directly process arbitrary graph topologies. This fundamental mismatch meant they could not effectively capture the rich relational information and structural patterns crucial for tasks like node classification, link prediction, and graph classification. GNNs emerged precisely to bridge this gap, offering an adaptive, data-driven approach to feature extraction by enabling end-to-end learning of rich, distributed representations directly from graph structure and node features. This paradigm shift has been instrumental in unlocking unprecedented capabilities, leading to their widespread adoption across numerous scientific and industrial applications.

The intellectual lineage of GNNs can be traced back to the early 2000s, where initial theoretical conceptualizations laid the groundwork for learning on graph domains. Pioneering efforts sought to extend neural network principles to handle non-Euclidean data by defining a state-transition system that iteratively propagated information across graph nodes. \cite{Gori05} first proposed a "neural network for graphs" that aimed to learn node representations by recursively aggregating information from neighbors until a stable fixed-point was achieved. This foundational idea was further formalized by \cite{Scarselli09}, who introduced the term "Graph Neural Network" (GNN) and rigorously defined it as an extension of recursive neural networks. Their work emphasized the fixed-point iteration as a mechanism for learning node embeddings that inherently encode neighborhood information, demonstrating the universal approximation capabilities of such models for graph functions. While these seminal contributions established the mathematical and conceptual framework for the GNN paradigm, their reliance on computationally intensive fixed-point iterations and inherent scalability limitations hindered their immediate practical widespread adoption. These early models, though theoretically profound, highlighted the need for more efficient and scalable architectures.

A pivotal breakthrough in the evolution of GNNs occurred in the mid-2010s with the emergence of scalable and effective message-passing architectures, which transformed GNNs from a theoretical curiosity into a widely applicable tool. This era was characterized by a shift from complex fixed-point iterations to simpler, layer-wise aggregation functions, often drawing inspiration from convolutional operations in image processing. Early explorations, such as \cite{Duvenaud15}'s work on adapting neighbor feature summation for molecular fingerprinting, hinted at the potential of localized aggregation. However, it was the introduction of models like the Graph Convolutional Network (GCN) by \cite{Kipf17} that marked a true paradigm shift. GCNs simplified spectral graph convolutions into a highly efficient, localized, first-order approximation, making semi-supervised node classification on large graphs practical and establishing a de-facto standard for subsequent research. Building on this, \cite{Hamilton17} introduced GraphSAGE, a framework that addressed the limitations of transductive learning by enabling inductive representation learning through efficient neighbor sampling. This innovation was crucial for applying GNNs to dynamic and unseen graphs. Further enhancing the expressiveness of message passing, \cite{Velickovic18} proposed Graph Attention Networks (GATs), which allowed models to learn adaptive importance weights for different neighbors. This mechanism overcame the fixed-weight aggregation of earlier models, enabling more nuanced and flexible information propagation. Collectively, these architectural innovations democratized GNNs, making them practical, robust, and widely adopted for a broader spectrum of graph-based tasks.

As GNNs gained prominence, the field entered a phase of critical analysis and theoretical maturation, focusing on understanding their fundamental capabilities, limitations, and expanding their applicability to more complex scenarios. A significant line of inquiry focused on the expressive power of GNNs. The seminal work by \cite{xu2018c8q} provided a rigorous theoretical framework, demonstrating that many standard message-passing GNNs are upper-bounded in their discriminative power by the 1-dimensional Weisfeiler-Lehman (1-WL) graph isomorphism test. This insight revealed inherent limitations in GNNs' ability to distinguish certain non-isomorphic graphs or count specific substructures, prompting extensive research into more powerful architectures. Subsequent work explored ways to overcome these bounds, including investigations into higher-order message passing, which aggregates information from $k$-hop neighborhoods or considers more complex substructures. For instance, \cite{feng20225sa} provided a theoretical characterization of K-hop message passing, demonstrating its enhanced expressive power beyond 1-WL while also identifying its own limitations, bounded by the 3-WL test. Similarly, the theoretical understanding of spectral GNNs also deepened, with works like \cite{wang2022u2l} analyzing their expressive power and the role of nonlinearities, even proving universality conditions for linear spectral GNNs under certain assumptions. Beyond expressivity, the evolution also encompassed adapting GNNs to more dynamic and intricate graph structures. While early GNNs primarily focused on static graphs, the need to model evolving relationships led to the development of Temporal GNNs (TGNNs). Surveys like \cite{longa202399q} highlight this crucial expansion, categorizing approaches that handle time-varying nodes, edges, and features, thus extending GNNs' utility to real-world dynamic systems. Furthermore, the pursuit of deeper GNNs and the exploration of recurrent architectures, as discussed by \cite{pflueger2024qi6}, which connect GNNs to concepts like bisimulation and logic, signify a growing emphasis on theoretical robustness and the ability to capture long-range dependencies and complex reasoning patterns beyond fixed-layer computations. This period thus marked a crucial shift towards a more profound understanding of GNNs' theoretical underpinnings and a concerted effort to push their boundaries in terms of expressivity, depth, and adaptability to diverse graph types.

In summary, the historical trajectory of Graph Neural Networks reflects a remarkable evolution from nascent theoretical concepts to a powerful and versatile paradigm in machine learning. This journey began by recognizing the inherent limitations of traditional methods for non-Euclidean data, leading to the pioneering fixed-point iteration models. The field then underwent a transformative shift with the advent of scalable message-passing architectures, which made GNNs practical and widely accessible. Subsequently, a period of critical theoretical analysis and architectural innovation ensued, deepening our understanding of GNN expressivity, pushing the boundaries of model depth, and extending their applicability to dynamic and complex graph structures. This continuous evolution has cemented GNNs as indispensable tools for learning on graph-structured data, enabling unprecedented capabilities in diverse applications. The foundational concepts and architectural breakthroughs discussed here lay the essential groundwork for understanding the more detailed methodologies, advanced models, and specific challenges that will be explored in the subsequent sections of this review.


\label{sec:foundational_concepts_and_early_models}

\section{Foundational Concepts and Early Models}
\label{sec:foundational\_concepts\_\_and\_\_early\_models}

\subsection{Graph Theory Basics and Representation}
\label{sec:2\_1\_graph\_theory\_basics\_\_and\_\_representation}

Graphs are indispensable mathematical structures for modeling complex systems of interacting entities, forming the foundational input upon which Graph Neural Networks (GNNs) operate. A thorough understanding of fundamental graph theory concepts and their various numerical representations is therefore paramount for comprehending the intricate mechanics and capabilities of GNNs \cite{wu2022ptq, zhang2021jqr}. This subsection establishes the essential vocabulary and introduces common methods for structuring and representing graph data, preparing the reader for the subsequent exploration of GNN architectures and their processing of relational information.

Formally, a graph is defined as $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, comprising a set of $N = |\mathcal{V}|$ nodes (or vertices) and a set of $M = |\mathcal{E}|$ edges (or links) connecting pairs of nodes. Nodes typically represent individual entities, such as atoms in a molecule, users in a social network, or keypoints in an image. Edges, on the other hand, signify relationships or interactions between these entities, which can be chemical bonds, friendships, or potential correspondences. Beyond mere connectivity, both nodes and edges can carry rich semantic information in the form of attributes. Node attributes are commonly represented as feature vectors $\mathbf{x}\_v \in \mathbb{R}^{d\_v}$ for each node $v \in \mathcal{V}$, capturing intrinsic properties of the entity (e.g., atomic number, user profile information). Similarly, edge attributes $\mathbf{e}\_{uv} \in \mathbb{R}^{d\_e}$ can quantify characteristics of the relationship between nodes $u$ and $v$, such as strength, type, or direction (e.g., bond type, interaction frequency). The capacity of GNNs to jointly process this topological structure and the associated rich attributes is a primary source of their analytical power \cite{wu2022ptq}.

For computational processing by GNNs, graphs are translated into numerical data structures. The most prevalent structural representation is the \textbf{adjacency matrix} $\mathbf{A} \in \{0, 1\}^{N \times N}$, where $\mathbf{A}\_{ij} = 1$ if an edge exists from node $i$ to node $j$, and $0$ otherwise. For graphs with node attributes, a \textbf{node feature matrix} $\mathbf{X} \in \mathbb{R}^{N \times d\_v}$ is constructed, where each row $\mathbf{X}\_i$ corresponds to the feature vector $\mathbf{x}\_i$ of node $i$. Edge attributes can be stored in an \textbf{edge feature matrix} $\mathbf{E} \in \mathbb{R}^{M \times d\_e}$ or, for weighted graphs, directly populate the adjacency matrix with real-valued weights.

Crucially, for many GNN architectures, particularly those rooted in spectral graph theory, the \textbf{degree matrix} $\mathbf{D} \in \mathbb{R}^{N \times N}$ is also fundamental. $\mathbf{D}$ is a diagonal matrix where $\mathbf{D}\_{ii} = \sum\_{j} \mathbf{A}\_{ij}$ represents the degree of node $i$ (i.e., the sum of weights of its connections for weighted graphs, or simply the number of connections for unweighted graphs). From $\mathbf{A}$ and $\mathbf{D}$, the \textbf{Graph Laplacian} $\mathbf{L} = \mathbf{D} - \mathbf{A}$ is derived. The Laplacian matrix is central to graph signal processing and spectral graph theory, as its eigenvalues and eigenvectors reveal fundamental properties of the graph's structure and connectivity. Normalized variants, such as the symmetric normalized Laplacian $\mathbf{L}\_{sym} = \mathbf{I} - \mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ and the random walk normalized Laplacian $\mathbf{L}\_{rw} = \mathbf{I} - \mathbf{D}^{-1}\mathbf{A}$, are particularly important for spectral GNNs like Graph Convolutional Networks (GCNs). These normalized Laplacians facilitate stable message propagation, mitigate issues like exploding or vanishing gradients, and define how information propagates across the graph spectrum \cite{wang2022u2l}.

While adjacency matrices are conceptually straightforward, they can be memory-inefficient for large, sparse graphs (where $M \ll N^2$). In such cases, \textbf{adjacency lists} are often preferred for their computational efficiency. An adjacency list stores, for each node, a list of its direct neighbors. This representation is crucial for scalability, especially when dealing with real-world graphs that can contain billions of nodes and edges, as it allows GNNs to efficiently sample neighbors or process subgraphs without materializing the full adjacency matrix \cite{jin2021pf0}. The choice of representation significantly impacts the memory footprint and computational complexity of GNN training and inference, a critical consideration for large-scale applications.

Graphs can be categorized based on the nature of their edges, which significantly impacts how GNNs are designed and applied:
\begin{itemize}
    \item \textbf{Undirected Graphs:} Edges represent symmetric relationships, meaning if node $i$ is connected to node $j$, then $j$ is also connected to $i$. Their adjacency matrices are symmetric ($\mathbf{A}\_{ij} = \mathbf{A}\_{ji}$). Many social networks or co-occurrence graphs are naturally undirected.
    \item \textbf{Directed Graphs:} Edges denote asymmetric relationships, indicating a one-way flow of information or interaction from a source node to a target node. Here, $\mathbf{A}\_{ij}$ does not necessarily imply $\mathbf{A}\_{ji}$. Citation networks or knowledge graphs often fall into this category, requiring GNNs to differentiate between incoming and outgoing messages.
    \item \textbf{Weighted Graphs:} Edges are assigned numerical weights, quantifying the strength, distance, or cost of a relationship. The adjacency matrix entries $\mathbf{A}\_{ij}$ then contain these real-valued weights instead of binary values. Road networks with travel times or protein-protein interaction networks with interaction strengths are common examples, where GNNs must incorporate these weights into aggregation functions.
    \item \textbf{Attributed Graphs:} Both nodes and/or edges possess explicit feature vectors, providing rich semantic context beyond mere connectivity. Most real-world graphs processed by GNNs are attributed, as these features are indispensable for learning complex patterns. For instance, in computer vision tasks like feature matching, nodes might represent image keypoints and edges, weighted by feature similarity, represent potential correspondences, with GNNs learning to refine these assignments \cite{sarlin20198a6}.
\end{itemize}
GNN architectures must be tailored to these graph types. For example, directed graphs necessitate separate aggregation of incoming and outgoing messages, while weighted graphs require weight-aware aggregation functions to properly leverage the strength of relationships.

A thorough understanding of graph theory fundamentals—including the definitions of nodes, edges, and attributes, their various numerical representations (adjacency matrices, feature matrices, degree matrices, Laplacians, and adjacency lists), and the distinctions between different graph types—is indispensable for comprehending Graph Neural Networks. These foundational concepts dictate how graph-structured information is encoded, processed, and ultimately leveraged by GNNs to learn complex relational patterns, forming the essential groundwork for all subsequent discussions on GNN architectures and their advanced applications.
\subsection{Early Graph Neural Network Models: Fixed-Point Iteration}
\label{sec:2\_2\_early\_graph\_neural\_network\_models:\_fixed-point\_iteration}

The initial conceptualization of Graph Neural Networks (GNNs) emerged from the challenge of extending neural network capabilities to non-Euclidean graph domains, particularly focusing on models that learn node representations through an iterative information propagation process, aiming to reach a stable fixed-point. These pioneering models laid the crucial theoretical and mathematical groundwork for the entire field, establishing the idea of learning structural information through local aggregation.

One of the earliest and most influential works in this direction was proposed by \cite{Gori05}, which introduced a novel "neural network for graphs." This model conceptualized node states as being updated iteratively based on their previous state and the states of their neighbors, along with the features of the edges connecting them. The core idea was to propagate information across the graph until the node states converged to a stable equilibrium, effectively encoding the structural context of each node into its representation. This foundational work established the paradigm of a state-transition system on graphs, where node embeddings are derived from a recursive update rule.

Building upon this initial framework, \cite{Scarselli09} formalized the Graph Neural Network (GNN) model, defining it as an extension of recursive neural networks capable of processing general graph structures. Their model explicitly defined a node's state (embedding) as the fixed-point of a contraction mapping, ensuring convergence and uniqueness of the solution. The GNN model, as formulated by \cite{Scarselli09}, utilized an iterative update function to compute the state of each node, considering its own features and the features of its neighbors and incident edges, until a stable fixed-point was attained. This theoretical underpinning demonstrated the GNN's ability to approximate universal functions on graphs, meaning it could, in principle, learn any computable function on graph-structured data. The output function then mapped these learned fixed-point states to task-specific predictions, such as node or graph classification.

Despite their theoretical elegance and foundational importance in establishing the GNN paradigm, these early fixed-point models faced significant practical limitations that hindered their widespread adoption. A primary concern was computational complexity; reaching a stable fixed-point often required numerous iterative steps, making training and inference computationally expensive, especially for large graphs. The convergence of these fixed-point iterations was also a practical challenge, as guaranteeing and achieving convergence in real-world scenarios could be difficult. Furthermore, the scalability of these models to large graphs was severely limited due to the global nature of fixed-point computation, which often necessitated processing the entire graph repeatedly. Training these models involved backpropagating gradients through potentially many unrolled fixed-point iterations, which could be cumbersome and prone to vanishing or exploding gradients. These practical constraints, particularly concerning computational efficiency, convergence guarantees, and scalability, spurred subsequent research into more efficient and scalable GNN architectures that moved away from strict fixed-point iteration in favor of finite-layer message-passing schemes.
\subsection{Message-Passing Paradigm: GCNs, GraphSAGE, and GATs}
\label{sec:2\_3\_message-passing\_paradigm:\_gcns,\_graphsage,\_\_and\_\_gats}

The emergence of the message-passing paradigm marked a pivotal moment in the development of Graph Neural Networks (GNNs), significantly simplifying and popularizing these models by abstracting complex spectral operations into intuitive, localized aggregations. This paradigm transformed GNNs from theoretical constructs, which often involved computationally intensive fixed-point iterations \cite{Gori05, Scarselli09}, into widely accessible and effective tools for learning on graph-structured data.

A foundational breakthrough in this paradigm was the introduction of Graph Convolutional Networks (GCNs) by \cite{Kipf17}. GCNs elegantly simplified the computationally demanding spectral graph convolutions into a localized, first-order approximation, enabling efficient semi-supervised learning on node classification tasks. The core idea involves iteratively aggregating feature information from immediate neighbors and transforming it with a learnable weight matrix, followed by a non-linear activation function. While highly effective and widely adopted for its simplicity and performance, the original GCN formulation was inherently transductive, meaning it struggled to generalize to unseen nodes or entirely new graphs, and it aggregated neighbor features with fixed, unweighted averages.

Addressing the critical challenge of inductive learning and scalability, \cite{Hamilton17} proposed GraphSAGE (SAmple and aggreGatE). GraphSAGE introduced a framework to learn node embeddings by sampling a fixed number of neighbors and then aggregating their features using various learnable aggregation functions (e.g., mean, LSTM, pooling). This neighbor sampling mechanism allowed GraphSAGE to operate on large graphs and generalize to unseen nodes, making GNNs applicable to dynamic and evolving graph structures. Despite its advancements in scalability and inductive capabilities, GraphSAGE, similar to GCNs, still employed aggregation functions that assigned uniform importance to all sampled neighbors, potentially overlooking the varying relevance of different connections.

The limitation of fixed-weight aggregation was directly tackled by \cite{Velickovic18} with the introduction of Graph Attention Networks (GATs). GATs empower the model to learn adaptive importance weights for different neighbors by employing a self-attention mechanism. For each node, GATs compute attention coefficients between the node and its neighbors, allowing the model to selectively focus on more relevant neighbors during the aggregation process. This mechanism not only enhances the model's expressiveness by overcoming the isotropic aggregation of GCNs and GraphSAGE but also provides a degree of interpretability by revealing which neighbors are considered more important. Furthermore, GATs leverage multi-head attention to stabilize the learning process and capture diverse relational patterns.

The theoretical underpinnings of these architectural advancements were further explored by \cite{xu2018c8q}, who analyzed the expressive power of GNNs. Their work demonstrated that popular GNN variants like GCNs and GraphSAGE, while powerful, are fundamentally limited in their ability to distinguish certain graph structures, connecting their discriminative power to the Weisfeiler-Lehman graph isomorphism test. This theoretical insight highlighted the inherent limitations of models relying on simple, fixed aggregation schemes and underscored the value of more sophisticated mechanisms, such as the attention mechanism in GATs, for enhancing a GNN's capacity to differentiate complex graph topologies.

In conclusion, the message-passing paradigm, championed by GCNs, GraphSAGE, and GATs, revolutionized GNN research by providing practical, scalable, and increasingly expressive architectures. These models addressed critical limitations in a progressive manner: GCNs simplified convolutions, GraphSAGE enabled inductive learning, and GATs introduced adaptive attention. This progression formed the backbone of many subsequent GNN developments, demonstrating the profound power of localized message aggregation for learning on graph structures. However, challenges remain, particularly in scaling these models to extremely deep architectures without encountering issues like over-smoothing, and in developing more sophisticated aggregation mechanisms that can handle complex relational patterns beyond simple attention, as seen in advanced applications like feature matching \cite{sarlin20198a6}.


\label{sec:enhancing_expressive_power_and_deep_architectures}

\section{Enhancing Expressive Power and Deep Architectures}
\label{sec:enhancing\_expressive\_power\_\_and\_\_deep\_architectures}

\subsection{Understanding GNN Expressivity: Weisfeiler-Lehman Test and Beyond}
\label{sec:3\_1\_underst\_and\_ing\_gnn\_expressivity:\_weisfeiler-lehman\_test\_\_and\_\_beyond}

The theoretical expressive power of Graph Neural Networks (GNNs) is a critical area of research, particularly concerning their ability to distinguish non-isomorphic graphs and capture complex structural patterns. A foundational insight in this domain connects the discriminative power of many standard GNNs to the Weisfeiler-Lehman (WL) test for graph isomorphism. Early work by \cite{morris20185sd} rigorously demonstrated that standard message-passing GNNs (1-GNNs) are at most as powerful as the 1-dimensional WL test (1-WL), and under suitable conditions, achieve equivalent power. This equivalence implies inherent limitations, as 1-WL cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or count specific substructures.

These limitations manifest in GNNs' inability to capture higher-order structural information. For instance, \cite{chen2020e6g} formally proved that Message Passing Neural Networks (MPNNs) and even 2-Invariant Graph Networks (2-IGNs) cannot count induced subgraphs for any connected pattern with three or more nodes. Extending this, \cite{garg2020z6o} provided impossibility proofs, showing that a broad class of GNNs, including popular Locally Unordered GNNs (LU-GNNs) and CPNGNNs, cannot compute fundamental graph properties such as girth, circumference, diameter, or k-clique. The review by \cite{reiser2022b08} further acknowledges these expressivity limitations, alongside issues like over-squashing and over-smoothing, as key challenges for GNNs in practical applications like chemistry and materials science.

To overcome the 1-WL bottleneck, researchers have explored several avenues, primarily focusing on higher-order architectures that can capture richer structural information. \cite{morris20185sd} initially proposed k-dimensional GNNs (k-GNNs), which perform message passing on k-element subsets (k-tuples) of nodes, theoretically achieving greater discriminative power than 1-WL. However, these k-GNNs often incur prohibitive computational costs due to the exponential growth in the number of k-tuples. Addressing this efficiency concern, \cite{balcilar20215ga} introduced GNNML3, a spectral approach that leverages non-linear custom functions of eigenvalues to design graph convolution supports. This method experimentally matches the power of 3-WL equivalent models while maintaining linear computational complexity after an initial eigendecomposition, offering a more scalable alternative to direct k-tuple processing.

Other approaches enhance expressivity by explicitly incorporating path or subgraph information. \cite{michel2023hc4} developed Path Neural Networks (PathNNs) that aggregate information from various paths. Their most expressive variant, operating on "annotated sets of paths," was theoretically shown to be strictly more powerful than 1-WL and empirically capable of distinguishing graphs indistinguishable by the 3-WL algorithm. Similarly, \cite{zeng20237gv} proposed Substructure Aware Graph Neural Networks (SAGNN), which inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk encodings, into node features. SAGNNs are provably more powerful than 1-WL and can also distinguish graphs that fool the 3-WL test. For tasks like link prediction, \cite{chamberlain2022fym} introduced ELPH and BUDDY, full-graph GNNs that efficiently approximate subgraph features (or "sketches") to achieve expressivity comparable to subgraph-based GNNs, effectively breaking 1-WL limitations for this specific task.

Another powerful strategy involves augmenting GNNs with positional or structural identifiers. \cite{abboud2020x5e} demonstrated the "surprising power" of standard MPNNs when augmented with Random Node Initialization (RNI), proving that such models are universal approximators for functions on graphs of a fixed order, thereby efficiently breaking the 1-WL barrier. Building on this, \cite{papp20211ac} proposed DropGNN, which uses random node dropouts during both training and testing across multiple runs to systematically explore perturbed graph structures, proving that this mechanism increases expressiveness beyond 1-WL. Furthermore, \cite{dwivedi2021af0} introduced Learnable Structural and Positional Encodings (LSPE), a framework that decouples and learns both structural and positional representations throughout the GNN layers, utilizing a Random Walk Positional Encoding (RWPE) to avoid sign ambiguity issues common in Laplacian-based encodings. This concept was further refined by \cite{wang2022p2r} with Equivariant and Stable Positional Encoding (PEG), which uses separate channels for node and positional features, ensuring O(p) equivariance and stability for positional embeddings, particularly beneficial for node-set tasks. The utility of positional encodings, such as Laplacian eigenvectors, for enhancing GNN expressivity on challenging datasets was also empirically highlighted by \cite{dwivedi20239ab}.

For domains where geometric information is paramount, such as molecular modeling, specialized equivariant GNNs have pushed expressivity further. \cite{klicpera20215fk} introduced GemNet, proving universality for GNNs using spherical (S2) representations and incorporating a novel two-hop geometric message passing scheme that explicitly captures distances, angles, and dihedral angles. Similarly, \cite{satorras2021pzl} developed E(n)-Equivariant Graph Neural Networks (EGNNs), which directly update node coordinates and embeddings while preserving E(n) equivariance, offering a simpler and more scalable alternative to methods relying on complex higher-order representations. Beyond static structural information, \cite{finkelshtein202301z} proposed Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose their communication actions (listen, broadcast, isolate) at each layer. This dynamic and asynchronous message passing paradigm is theoretically shown to be more expressive than 1-WL due to the variance introduced by action sampling.

While enhancing theoretical expressivity, practical limitations like "over-squashing" can hinder the effective utilization of deep GNNs for long-range dependencies, as identified by \cite{alon2020fok}. This phenomenon limits the amount of information that can be propagated across many layers. To address this, \cite{zeng2022jhz} proposed decoupling the depth and scope of GNNs by applying deep GNNs on shallow, localized subgraphs. This approach not only mitigates oversmoothing but also enhances expressivity beyond 1-WL by allowing deeper processing of local information. More recently, \cite{geisler2024wli} introduced Spatio-Spectral Graph Neural Networks (S2GNNs), a hybrid paradigm combining local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to overcome over-squashing and are strictly more expressive than 1-WL, offering tighter approximation bounds and "free" positional encodings. Finally, the expressivity of spectral GNNs has also been re-evaluated, with \cite{wang2022u2l} demonstrating that even linear spectral GNNs can achieve universal approximation under specific conditions, challenging the necessity of non-linearity and connecting their power to the 1-WL test.

In conclusion, understanding the theoretical bounds imposed by the Weisfeiler-Lehman test is crucial for designing more powerful and discriminative GNN models. The field has moved significantly beyond the 1-WL limitations of standard message-passing architectures through various innovations. These include the development of higher-order GNNs, the strategic augmentation of node features with positional or structural encodings, the incorporation of geometric and dynamic message-passing mechanisms, and the exploration of hybrid spatial-spectral architectures. While significant progress has been made in enhancing expressivity, ongoing challenges remain in balancing this power with computational scalability, data efficiency, and generalization to diverse real-world graph structures and tasks.
\subsection{Overcoming Depth Limitations: Over-smoothing and Over-squashing}
\label{sec:3\_2\_overcoming\_depth\_limitations:\_over-smoothing\_\_and\_\_over-squashing}

The aspiration to develop deeper Graph Neural Networks (GNNs) capable of discerning intricate, long-range dependencies within graph-structured data is fundamentally impeded by two critical and intertwined challenges: over-smoothing and over-squashing. Over-smoothing manifests when iterative message passing causes node representations to converge towards indistinguishable values, leading to a profound loss of discriminative power and an inability to differentiate between nodes \cite{rusch2023xev, cai2020k4b}. Complementary to this, over-squashing describes an information bottleneck where the exponentially expanding receptive field of a node is compressed into a fixed-size vector, resulting in the significant loss of crucial information from distant nodes \cite{alon2020fok}. Effectively addressing these limitations is paramount for extending the practical utility and expressive capabilities of GNNs.

Early investigations into deep GNNs quickly identified the over-smoothing phenomenon. \cite{klicpera20186xu} made a seminal contribution by proposing to decouple the neural network's feature transformation from the graph's propagation mechanism. Their Personalized Propagation of Neural Predictions (PPNP) and its efficient approximation (APPNP) leverage personalized PageRank to facilitate deep propagation steps while preserving locality through a "teleport probability." This strategy effectively mitigates over-smoothing without increasing the neural network's parameter count or explicit depth. Building on this decoupling principle, \cite{liu2020w3t} further re-evaluated over-smoothing, suggesting that for moderate depths, performance degradation primarily stems from the entanglement of representation transformation and propagation. Their Deep Adaptive Graph Neural Network (DAGNN) demonstrated that explicit decoupling enables significantly deeper GNNs without performance loss, with severe over-smoothing becoming critical only at extreme depths.

The theoretical understanding of over-smoothing has been rigorously advanced through the lens of Dirichlet energy. \cite{cai2020k4b} provided a foundational theoretical framework, demonstrating that the Dirichlet energy of node embeddings exponentially converges to zero with increasing layers, directly signifying a loss of discriminative power. This concept was further formalized by \cite{rusch2023xev}, who introduced an axiomatic definition of over-smoothing based on the layer-wise exponential convergence of a node-similarity measure like Dirichlet energy. Their comprehensive survey \cite{rusch2023xev} also critically evaluates various mitigation strategies. Leveraging these theoretical insights, \cite{zhou20213lg} proposed a "Dirichlet energy constrained learning" principle, guiding the training of deep GNNs. Their Energetic Graph Neural Network (EGNN) incorporates orthogonal weight control, lower-bounded residual connections, and Shifted ReLU (SReLU) activation to maintain Dirichlet energy within an optimal range, enabling GNNs to effectively scale up to 64 layers. Similarly, \cite{bianchi20194ea} introduced Graph Neural Networks with Convolutional ARMA Filters, which employ recursive updates with skip connections to capture global dependencies and mitigate over-smoothing by offering a more flexible frequency response than traditional polynomial filters. In a different vein, \cite{eliasof202189g} proposed PDE-GCN, a family of architectures motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, allowing for the control of over-smoothing by design and offering a theoretical explanation for their behavior.

While over-smoothing primarily concerns feature homogenization, \cite{alon2020fok} introduced "over-squashing" as a distinct bottleneck, particularly for tasks necessitating long-range interactions. This phenomenon arises from the inherent limitation of fixed-size message aggregation, which struggles to compress the exponentially growing information from a node's receptive field without significant loss. To address this, \cite{zeng2022jhz} proposed a novel principle to decouple the \textit{depth} and \textit{scope} of GNNs. Their SHADOW-GNN applies deep GNNs on shallow, localized subgraphs, theoretically proving it prevents both over-smoothing and neighbor explosion, thereby enabling more expressive and scalable models. Taking a different approach, \cite{finkelshtein202301z} introduced Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose communication actions (listen, broadcast, isolate) at each layer. This adaptive, node-specific information flow inherently mitigates over-squashing and over-smoothing by allowing for more nuanced and efficient long-range communication. Furthermore, \cite{wu20221la}'s GraphTrans leverages Transformer-based self-attention with a novel readout mechanism to learn long-range pairwise relationships, offering a permutation-invariant approach that implicitly addresses the challenge of capturing distant node interactions often lost to over-squashing.

Beyond architectural modifications, novel non-convolutional designs and alternative learning paradigms offer robust pathways to overcome depth limitations and address both challenges. \cite{wang2024oi8} presented Random Walk with Unifying Memory (RUM), a fundamentally non-convolutional GNN that processes information via random walks and RNNs. RUM is theoretically shown to jointly remedy limited expressiveness, over-smoothing, and over-squashing by maintaining non-diminishing Dirichlet energy and slower inter-node Jacobian decay. Similarly, \cite{kang2024fsk} generalized continuous GNNs with fractional calculus in their FROND framework, introducing memory-dependent dynamics that algebraically mitigate over-smoothing by slowing the convergence rate to stationarity. \cite{geisler2024wli}'s Spatio-Spectral Graph Neural Networks (S2GNNs) synergistically combine local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to vanquish over-squashing and offer superior approximation bounds, demonstrating a powerful way to integrate local and global information. A physics-informed agnostic method proposed by \cite{shi2024g4z} enriches graph structures with additional nodes and rewired connections, theoretically verifying its ability to circumvent both over-smoothing and over-squashing. Moreover, \cite{zhu2021zc3} proposed a unified optimization framework that interprets the propagation mechanisms of various GNNs, including those that alleviate over-smoothing, by designing flexible graph convolutional kernels.

Despite significant architectural and theoretical progress, a critical re-evaluation by \cite{peng2024t2s} suggests that the drastic performance degradation in deep GNNs might not solely be attributable to over-smoothing. Instead, they argue that the inherent trainability challenges of deep Multi-Layer Perceptrons (MLPs) within GNN layers are a dominant problem, and many existing methods that ostensibly tackle over-smoothing actually improve the trainability of these MLPs. This perspective highlights that properly constrained gradient flow can significantly enhance GNN trainability, providing new insights for constructing deep graph models. This nuanced understanding underscores that while preventing feature homogenization is crucial, it is a necessary but not always sufficient condition for building high-performing deep GNNs.

In conclusion, while significant strides have been made in mitigating over-smoothing through decoupling, regularization, and theoretically grounded designs, and in addressing over-squashing via adaptive communication and global information integration, challenges persist. A unified theoretical framework that simultaneously addresses both over-smoothing and over-squashing, rather than treating them as separate phenomena, is still emerging. Developing adaptive mechanisms that dynamically adjust GNN depth, receptive field, or communication strategies based on local graph properties and task requirements remains an active research area. Furthermore, integrating insights from trainability studies to design deep GNNs that are both robust to information loss and efficiently optimizable continues to be a critical direction for future research.
\subsection{Higher-Order and Equivariant GNNs}
\label{sec:3\_3\_higher-order\_\_and\_\_equivariant\_gnns}

The inherent limitations of standard Graph Neural Networks (GNNs), particularly their equivalence to the 1-Weisfeiler-Lehman (1-WL) test, restrict their ability to distinguish complex graph structures and adequately capture intricate motifs \cite{xu2018c8q, morris20185sd, jegelka20222lq}. This section explores advanced GNN architectures designed to overcome these expressivity bottlenecks and to respect inherent symmetries in data, which is crucial for physical and geometric tasks.

To enhance discriminative power beyond the 1-WL test, a class of \textbf{higher-order GNNs} has emerged. Initial efforts, such as k-dimensional GNNs (k-GNNs), directly generalize message passing to operate on k-tuples or k-element subsets of nodes, proving strictly more powerful than 1-GNNs by capturing higher-order structural information like triangle counts \cite{morris20185sd}. While theoretically robust, these models often incur significant computational and memory costs, scaling polynomially or exponentially with the number of nodes, which limits their applicability to large graphs \cite{morris20185sd, garg2020z6o}.

To mitigate the computational overhead of explicit k-tuple processing while retaining enhanced expressiveness, alternative strategies have been developed. Random Node Initialization (RNI) has been shown to enable standard Message Passing Neural Networks (MPNNs) to achieve universal approximation capabilities, effectively breaking the 1-WL barrier by individualizing nodes, though often at the cost of slower convergence \cite{abboud2020x5e}. Similarly, DropGNN leverages random node dropouts across multiple runs during both training and testing to perturb and explore diverse neighborhood patterns, allowing GNNs to distinguish graphs beyond the 1-WL test with relatively low overhead \cite{papp20211ac}.

Another powerful approach involves injecting explicit positional or structural information. Positional Encodings (PEs), often derived from graph Laplacian eigenvectors, provide unique node identities that help GNNs differentiate isomorphic nodes and capture global structural context \cite{dwivedi2021af0}. However, traditional Laplacian PEs suffer from sign ambiguity and instability issues. The Learnable Structural and Positional Encodings (LSPE) framework addresses this by decoupling and concurrently learning both structural and positional representations throughout the GNN layers, often outperforming static PEs \cite{dwivedi2021af0}. Building on this, PEG (Positional Encoding GNN) further improves stability and equivariance for PEs by imposing O(p) equivariance on positional features, making them robust to eigenvalue multiplicities and suitable for large graphs \cite{wang2022p2r}.

Beyond node-level enhancements, models explicitly designed to capture substructures or paths offer increased expressiveness. Substructure Aware Graph Neural Networks (SAGNNs) inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk return probability encodings, into node features, enabling GNNs to perceive higher-order substructures and surpass the 1-WL limit \cite{zeng20237gv}. For tasks like link prediction, the SEAL framework leverages a $\beta$-decaying heuristic theory to justify learning high-order features from local enclosing subgraphs using GNNs, achieving superior performance by dynamically learning structural patterns rather than relying on fixed heuristics \cite{zhang2018kdl}. Path Neural Networks (PathNNs) take a different route, aggregating information from various paths, with its most expressive variant, $\tilde{AP}$, capable of distinguishing graphs indistinguishable by the 3-WL algorithm through recursively annotated path sets \cite{michel2023hc4}.

Furthermore, dynamic and hybrid approaches are pushing the boundaries of expressiveness and long-range interaction. Cooperative Graph Neural Networks (CO-GNNs) allow nodes to dynamically choose their communication actions (listen, broadcast, isolate) at each layer, leading to adaptive information flow that enhances expressive power beyond 1-WL and mitigates over-smoothing \cite{finkelshtein202301z}. Similarly, Spatio-Spectral Graph Neural Networks (S2GNNs) combine local spatial message passing with global, spectrally bounded but spatially unbounded spectral filters. This hybrid design effectively vanquishes the "over-squashing" problem, enabling robust long-range information exchange and achieving state-of-the-art performance on challenging benchmarks \cite{geisler2024wli}.

A distinct but equally critical area focuses on \textbf{equivariant GNNs}, which are designed to respect inherent symmetries in data, particularly for physical and geometric tasks. Many scientific domains, such as molecular modeling, quantum chemistry, and physics, involve 3D structures whose properties must transform predictably under geometric operations like rotation, translation, and reflection \cite{reiser2022b08}. Enforcing this equivariance acts as a powerful inductive bias, leading to more accurate and data-efficient models.

While some GNNs achieve E(n) invariance (e.g., by using relative distances), they often fail to maintain equivariance for vector outputs \cite{satorras2021pzl}. Early equivariant models often relied on computationally expensive higher-order representations like spherical harmonics, limiting their scalability and efficiency. E(n)-Equivariant Graph Neural Networks (EGNNs) offer a simpler and more efficient solution by directly integrating coordinate updates into the message-passing framework. EGNNs update node features and coordinates simultaneously, using squared relative distances for message computation and relative differences for coordinate updates, thereby preserving E(n) equivariance without complex higher-order tensors \cite{satorras2021pzl}. This design scales effectively to arbitrary n-dimensional spaces.

Building on these principles, more specialized equivariant architectures have emerged. GemNet, for instance, provides a universal directional GNN for molecules, leveraging spherical (S2) representations and a novel two-hop message passing scheme that explicitly incorporates interatomic distances, angles, and crucial dihedral angles. This allows GemNet to achieve state-of-the-art accuracy and data efficiency in molecular property prediction \cite{klicpera20215fk}. Another notable example is Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network that employs geometric tensors for interactions, leading to highly data-efficient and accurate interatomic potentials for molecular dynamics simulations \cite{batzner2021t07}.

In conclusion, the development of higher-order and equivariant GNNs represents a significant leap in addressing the fundamental limitations of traditional GNNs. Higher-order models, through various strategies from k-tuples to sophisticated positional and substructural encodings, are pushing the boundaries of structural discriminative power. Concurrently, equivariant GNNs are enabling physically consistent and data-efficient predictions in scientific domains by explicitly respecting geometric symmetries. Despite these advancements, challenges remain in balancing expressiveness with scalability for extremely large graphs, developing more generalizable equivariant models for diverse physical systems, and enhancing the interpretability of complex higher-order features. The continuous development of rigorous benchmarking frameworks remains crucial for fairly evaluating and driving progress in these advanced GNN architectures \cite{dwivedi20239ab}.


\label{sec:robustness,_adaptability,_and_trustworthiness}

\section{Robustness, Adaptability, and Trustworthiness}
\label{sec:robustness,\_adaptability,\_\_and\_\_trustworthiness}

\subsection{Handling Real-World Graph Imperfections: Heterophily and Structure Learning}
\label{sec:4\_1\_h\_and\_ling\_real-world\_graph\_imperfections:\_heterophily\_\_and\_\_structure\_learning}

Real-world graphs frequently deviate from ideal homophilic assumptions, presenting significant challenges for Graph Neural Networks (GNNs). These imperfections manifest as heterophily, where connected nodes exhibit dissimilar features or labels, or as incomplete, noisy, or entirely absent structural information. Addressing these issues is paramount for extending GNN utility beyond controlled datasets to the complex realities of real-world graph data, demanding more flexible and robust models.

The pervasive nature of heterophily, where nodes connect to dissimilar neighbors, is a primary concern for traditional GNNs that implicitly rely on homophilic message passing \cite{zheng2022qxr}. However, a nuanced understanding reveals that not all heterophily is equally detrimental; GCNs can perform well on certain heterophilous graphs if same-label nodes exhibit consistent neighborhood patterns, even if those neighbors are of different labels \cite{ma2021sim}. This highlights the concept of "structural disparity," where real-world graphs contain a mixture of homophilic and heterophilic patterns, leading to performance disparities where GNNs excel on majority patterns but struggle on minority ones \cite{mao202313j}. To better characterize these complexities, \cite{luan202272y} introduced novel post-aggregation similarity metrics, such as Aggregation Similarity Score ($S\_{agg}$) and Graph Aggregation Homophily ($H\_{agg}(G)$), which more accurately reflect GNN performance under heterophily than traditional metrics.

To address the limitations of uniform, global filtering in such mixed-pattern graphs, researchers have developed adaptive filtering mechanisms. \cite{luan202272y} proposed the Adaptive Channel Mixing (ACM) framework, which augments baseline GNNs by adaptively exploiting aggregation (low-pass), diversification (high-pass), and identity channels node-wisely. While effective, ACM's channel weights are learned globally per layer, potentially limiting its granularity. Building on this, \cite{han2024rkj} introduced NODE-MOE (Node-wise Filtering via Mixture of Experts), a more granular approach where a gating model dynamically selects and applies appropriate "expert" GNNs, each potentially equipped with a different filter type, to individual nodes based on their local patterns. This theoretically demonstrates the suboptimality of global filters and offers superior adaptability, albeit with increased computational overhead due to the gating mechanism and multiple expert networks. A related strategy for handling heterophily involves transforming the input graph itself to improve its assortativity. \cite{suresh202191q} proposed creating a multi-relational "computation graph" that distinguishes between proximity and structural information, thereby enhancing node-level assortativity and improving GNN performance under diverse mixing patterns. This approach implicitly adapts the graph structure to better suit GNNs.

A distinct strategy for heterophily involves seeking "global homophily" rather than purely local adaptation. \cite{li2022315} proposed GloGNN/GloGNN++, which aggregates information from the entire graph using a learned signed coefficient matrix. This matrix implicitly combines low-pass and high-pass filtering by assigning positive coefficients to homophilous nodes and negative ones to heterophilous nodes, effectively leveraging global correlations. Crucially, GloGNN achieves this with linear time complexity ($O(k^2n)$) by reordering matrix multiplications, making it highly scalable compared to naive global aggregation ($O(n^3)$ or $O(n^2c)$). While powerful for capturing long-range homophilous connections, the effectiveness of GloGNN relies on the assumption that such global patterns exist and are learnable, which might not hold for all graph types.

Beyond heterophily, the absence or imperfection of graph structures presents another critical challenge. When initial graph structures are noisy, incomplete, or unavailable, GNNs require mechanisms to learn or augment these structures. \cite{chen2020bvl} pioneered the Iterative Deep Graph Learning (IDGL) framework, which jointly and iteratively learns optimal graph structures and GNN parameters, explicitly optimizing for downstream tasks. This framework formulates graph learning as a similarity metric learning problem, enabling inductive capabilities and robustness to adversarial graph examples. However, this joint optimization process is computationally intensive ($O(n^2)$ for the full graph) and risks converging to suboptimal structures if not carefully regularized. To mitigate this, IDGL introduced a scalable anchor-based version (IDGL-ANCH) that reduces complexity to $O(ns)$ for large graphs, though it assumes noiseless node features.

Expanding on structure learning, \cite{fatemi2021dmb} introduced SLAPS (Self-Supervision Improves Structure Learning), which addresses the "supervision starvation" problem in latent graph learning by employing a self-supervised denoising autoencoder task. This provides additional supervision for learning a more robust graph structure itself, particularly beneficial when explicit labels for structure learning are scarce. Similarly, \cite{wei20246l2} explored self-supervised GNNs for enhanced feature extraction in Heterogeneous Information Networks (HINs), aiming to flexibly combine diverse information types to mine deep features and improve adaptability to graph diversity and complexity. For augmenting existing graphs, \cite{zhao2020bmj} proposed GAUG, a framework that leverages neural edge predictors to strategically add "missing" intra-class edges and remove "noisy" inter-class edges, thereby promoting class-homophilic structure and improving performance. In scenarios of "extreme weak information," where structure, features, and labels are simultaneously deficient, \cite{liu2023v3e} introduced D2PT (Dual-channel Diffused Propagation then Transformation), a comprehensive dual-channel GNN framework that combines an input graph backbone with a learned global graph and prototype contrastive alignment to effectively propagate information and connect stray nodes. This represents a robust solution for the most challenging structure-deficient environments.

In conclusion, the research landscape for GNNs operating on imperfect real-world data has evolved significantly, moving towards more flexible and robust models. Adaptive filtering mechanisms like ACM and NODE-MOE offer fine-grained control over message passing to handle structural disparities, albeit with varying computational overheads. Concurrently, methods like GloGNN provide scalable global aggregation for heterophily. For structural deficiencies, iterative and self-supervised structure learning frameworks such as IDGL, SLAPS, and D2PT enable GNNs to operate effectively even with noisy or absent graph topologies. While substantial progress has been made, challenges remain in ensuring the computational efficiency and theoretical guarantees of dynamic graph learning mechanisms, especially for extremely large and evolving graphs. Furthermore, designing models that can seamlessly adapt between diverse local and global structural patterns without explicit architectural changes or significant overhead remains an active area of research, particularly in achieving truly universal generalization across all types of complex, unseen graph imperfections.
\subsection{Adversarial Robustness and Defenses}
\label{sec:4\_2\_adversarial\_robustness\_\_and\_\_defenses}

Graph Neural Networks (GNNs) have demonstrated remarkable capabilities across various domains, yet their vulnerability to adversarial attacks poses significant security and reliability concerns, particularly in critical applications. These attacks involve small, often imperceptible, perturbations to the graph structure or node features that can drastically alter a GNN's predictions. The field has seen a rapid evolution in understanding these vulnerabilities and developing corresponding defense mechanisms.

Early research highlighted the susceptibility of GNNs to poisoning attacks, where an adversary manipulates the training data. \cite{zgner2019bbi} introduced the first global poisoning attack on GNNs, formulating it as a bilevel optimization problem solved via meta-gradients to degrade overall model performance. This demonstrated that GNNs are vulnerable beyond targeted misclassifications, even performing worse than non-relational baselines under attack. Building on this, \cite{xu2019l8n} proposed an optimization-based framework for both topology attacks and defenses. They addressed the challenge of discrete graph structures by relaxing edge perturbation variables to continuous ones, enabling gradient-based Projected Gradient Descent (PGD) attacks and min-max adversarial training for robust GNNs.

In response to these emerging threats, several defense mechanisms were proposed. \cite{zhang2020jrt} introduced GNNGuard, a general, plug-and-play defense algorithm designed to protect against training-time structural attacks. GNNGuard operates by dynamically estimating neighbor importance and employing a layer-wise graph memory to prune suspicious edges and stabilize message passing, notably demonstrating effectiveness even on heterophily graphs, a limitation of prior defenses. Further enhancing robustness, \cite{liu2021ee2} developed Elastic Graph Neural Networks (Elastic GNNs) that incorporate \(\ell\_1\)-based graph smoothing. This approach allows for adaptive local smoothness, preserving important discontinuities while providing enhanced robustness against adversarial attacks, outperforming \(\ell\_2\)-based smoothing methods.

The landscape of adversarial attacks also expanded beyond simple poisoning. \cite{he2020kz4} revealed a critical privacy vulnerability by demonstrating "link stealing attacks," where an adversary can infer the underlying graph structure from a black-box GNN's outputs. This highlights that GNNs implicitly encode significant structural information, which can be exploited for privacy breaches. Furthermore, \cite{zhang2020b0m} introduced backdoor attacks to GNNs for graph classification tasks. These attacks inject a predefined subgraph trigger into a small fraction of training graphs, causing the trained GNN to consistently misclassify any graph containing this trigger to a target label, posing a stealthy and persistent threat.

A significant challenge in GNN robustness research has been the scalability of both attack and defense methods to real-world, large-scale graphs. Addressing this, \cite{geisler2021dcq} developed sparsity-aware first-order optimization attacks, such as Projected Randomized Block Coordinate Descent (PR-BCD), which overcome the prohibitive memory requirements of prior methods. They also introduced novel surrogate losses (e.g., Masked Cross Entropy) for more effective global attacks and a scalable robust aggregation function (Soft Median) for defenses, enabling the study of GNN robustness on graphs orders of magnitude larger than previously possible.

Despite the proliferation of defense mechanisms, a critical evaluation of their true effectiveness against adaptive adversaries remained largely unexplored. \cite{mujkanovic20238fi} conducted a comprehensive study, rigorously evaluating seven popular GNN defenses against custom-designed adaptive attacks. Their findings were sobering: most existing defenses offered "no or only marginal improvement" compared to an undefended baseline when faced with an adversary aware of the defense mechanism. This work, which introduced a systematic methodology for designing strong adaptive attacks (including a novel Meta-PGD attack), exposed a significant overestimation of GNN robustness in prior literature, akin to similar revelations in the computer vision community.

The ongoing challenge of creating truly robust GNNs against adaptive adversaries, who can circumvent naive protective measures, remains a crucial area for continued research. The findings of \cite{mujkanovic20238fi} underscore the need for a paradigm shift in GNN defense evaluation, advocating for adaptive attacks as the new gold standard. Future work must focus on developing inherently robust GNN architectures and training strategies that can withstand sophisticated, white-box adversaries, moving beyond superficial improvements to achieve genuine security and reliability in real-world deployments.
```
\subsection{Fairness and Privacy in GNNs}
\label{sec:4\_3\_fairness\_\_and\_\_privacy\_in\_gnns}

The increasing deployment of Graph Neural Networks (GNNs) in sensitive domains necessitates a rigorous focus on their ethical dimensions, particularly ensuring fair outcomes and protecting sensitive information. The relational nature of graph data presents unique challenges, as information propagation can inadvertently amplify biases or leak sensitive details, making careful design and evaluation paramount for trustworthy GNN systems \cite{zhang20222g3, dai2022hsi}.

Fairness in GNNs is a critical concern, as models can inherit and even exacerbate biases present in graph data and structures. Sources of bias include imbalanced representation of demographic groups, homophily (tendency for similar nodes to connect), and the message-passing mechanism itself, which can propagate and amplify discriminatory patterns \cite{zhang20222g3}. Early efforts to mitigate unfairness, such as FairGNN \cite{dai2020p5t}, addressed the practical challenge of limited sensitive attribute information by employing a GCN-based estimator for missing attributes, combined with adversarial debiasing to ensure predictions are independent of sensitive attributes. This approach demonstrated significant reduction in discrimination while maintaining high accuracy, even with sparse sensitive attribute knowledge. Building on this, \cite{dong2021qcg} shifted the focus to data-centric debiasing by proposing EDITS, a model-agnostic pre-processing framework. EDITS formally defines and mitigates "attribute bias" and "structural bias" directly in the input attributed network using Wasserstein-1 distance-based metrics, optimizing to reduce these biases before GNN training. Further, \cite{wang2022531} identified the critical issue of "sensitive attribute leakage" where GNN feature propagation dynamically alters feature correlations, exacerbating bias. Their FairVGNN framework mitigates this by using a generative adversarial debiasing module to mask sensitive-correlated features and an adaptive weight clamping module to minimize representation differences between sensitive groups.

Beyond group fairness, research has also delved into individual fairness, which ensures that similar individuals receive similar predictions. \cite{dong202183w} addressed this more granular problem by proposing REDRESS, a novel ranking-based framework. This approach refines the notion of individual fairness to overcome the practical difficulties associated with Lipschitz conditions, offering a plug-and-play solution for existing GNN architectures. More recently, \cite{li20245zy} rethought GNN fairness from a re-balancing perspective, introducing FairGB, which combines Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL). CNM generates unbiased ego-networks by interpolating node attributes and neighbor distributions across counterfactual pairs, while CAL is a gradient-based re-weighting method that dynamically balances group contributions during training, achieving state-of-the-art fairness-utility trade-offs. A more fundamental approach to learning unbiased representations is presented by \cite{wu2022vcx}, which proposed Discovering Invariant Rationale (DIR). DIR is an invariant learning strategy for intrinsically interpretable GNNs that identifies causal patterns stable across different data distributions, thereby inherently contributing to fairness by avoiding "shortcut features" and improving out-of-distribution generalization.

In addition to fairness, privacy is a paramount concern in GNNs, especially when dealing with sensitive graph structures or node attributes. The interconnected nature of graphs means that even seemingly innocuous information can be inferred or leaked, making GNNs uniquely vulnerable to various privacy attacks \cite{dai2022hsi}. A significant privacy vulnerability was exposed by \cite{he2020kz4}, which pioneered the study of "link stealing attacks" against GNNs. Their work demonstrated that the underlying graph structure can be inferred from a black-box GNN's outputs, even with minimal adversary knowledge. This research introduced a comprehensive threat model and various attack methodologies (unsupervised, supervised, transferring) that achieved high AUCs, revealing that GNNs inherently encode and implicitly reveal structural information about their training graphs through their message-passing mechanisms. Beyond link stealing, GNNs are also susceptible to "node attribute inference attacks," where sensitive node features (e.g., medical conditions, income) are inferred from a model's outputs or learned representations, and "membership inference attacks," which determine if a specific node was part of the GNN's training dataset \cite{dai2022hsi, zhang20222g3}.

To counter these threats, several privacy-preserving mechanisms have been explored for GNNs. Differential Privacy (DP) is a robust technique that adds carefully calibrated noise to data or model parameters to provide strong privacy guarantees. In GNNs, DP can be applied by perturbing node features, adding noise to gradients during training, or even altering the graph structure (e.g., adding/removing edges) before training \cite{dai2022hsi, zhang20222g3}. However, applying DP to non-Euclidean graph data is challenging, as noise addition can significantly degrade model utility, and the interdependent nature of graph data makes global privacy guarantees difficult to achieve without substantial performance loss. Another promising paradigm is Federated Learning (FL), which enables collaborative training of GNNs across multiple clients without centralizing their raw graph data \cite{liu2022gcg}. In Federated GNNs (FedGNNs), clients train local GNN models on their private subgraphs and only share aggregated model updates with a central server, thus preserving data locality. This approach is particularly relevant for sensitive applications like disease classification, where patient data must remain private \cite{hausleitner2024vw0}. Challenges in FedGNNs include handling graph heterogeneity across clients, managing communication overhead, and ensuring robustness against malicious clients. Traditional graph anonymization techniques (e.g., k-anonymity, l-diversity) can also be applied, but often suffer from significant utility loss or are vulnerable to re-identification attacks in complex graph structures \cite{dai2022hsi}.

The unique challenges posed by the relational nature of graph data mean that fairness and privacy are often intertwined, presenting complex trade-offs. Bias amplification can occur through message passing, and sensitive attribute leakage can be a privacy concern. Critically, privacy-preserving mechanisms like Differential Privacy, while protecting individual data, can sometimes disproportionately affect minority groups, exacerbating existing biases or making them harder to detect and mitigate \cite{dai2022hsi, zhang20222g3}. Conversely, debiasing techniques aimed at achieving fairness might inadvertently reveal more sensitive information about individuals or groups. Building trustworthy GNN systems therefore requires careful consideration of these ethical dimensions, necessitating novel architectural designs and evaluation methodologies that balance predictive performance with robust fairness and privacy guarantees. Future research must continue to explore comprehensive solutions that address multi-modal bias, offer stronger and provable privacy guarantees, and provide certified fairness, especially in the non-IID graph setting. Key open challenges include developing unified frameworks that simultaneously optimize for fairness and privacy without severe trade-offs, designing scalable and efficient privacy-preserving GNNs for massive real-world graphs, and establishing standardized benchmarks and metrics for evaluating the complex interplay between fairness, privacy, and utility in GNNs. This holistic approach is essential for ensuring GNNs are deployed responsibly and ethically.


\label{sec:advanced_learning_paradigms:_pre-training_and_explainability}

\section{Advanced Learning Paradigms: Pre-training and Explainability}
\label{sec:advanced\_learning\_paradigms:\_pre-training\_\_and\_\_explainability}

\subsection{Self-Supervised and Generative Pre-training for GNNs}
\label{sec:5\_1\_self-supervised\_\_and\_\_generative\_pre-training\_for\_gnns}

The pervasive scarcity of labeled graph data presents a significant bottleneck for training powerful Graph Neural Networks (GNNs) in numerous real-world applications. To circumvent this, researchers have increasingly focused on developing robust pre-training strategies that leverage vast amounts of unlabeled graph data to learn transferable representations. This paradigm aims to overcome issues like negative transfer and enhance generalization across diverse downstream tasks, thereby maximizing the utility of GNNs. This section delves into the evolution of self-supervised and generative pre-training methods, highlighting their core principles, key advancements, and remaining challenges.

Early foundational work systematically investigated strategies for pre-training GNNs by designing various self-supervised pretext tasks. \cite{hu2019r47} pioneered a comprehensive approach by proposing tasks at both node and graph levels. Node-level tasks included context prediction (predicting a node's K-hop neighborhood) and attribute masking (reconstructing masked node/edge attributes), while graph-level tasks focused on predicting graph-level properties or generating graph summaries. This combined strategy was crucial for learning meaningful local and global representations and was shown to effectively mitigate negative transfer, a common pitfall when naively applying pre-training. Beyond these, other predictive self-supervised tasks include link prediction, predicting structural roles or properties (e.g., node centrality, community membership proxies), and graph reconstruction via autoencoders \cite{xie2021n52}. These methods train GNNs to capture inherent graph characteristics by generating labels directly from the graph structure itself.

A dominant and highly influential paradigm in self-supervised learning for GNNs is contrastive learning. Inspired by its success in computer vision and natural language processing, contrastive methods aim to learn robust representations by maximizing the mutual information between different augmented "views" of a graph or its components. The core idea is to pull positive pairs (different augmentations of the same graph or node) closer in the embedding space while pushing negative pairs (augmentations of different graphs or nodes) further apart. Key to this approach is the design of effective graph augmentation strategies, which can involve node/edge dropping, feature masking, subgraph sampling, or diffusion-based transformations \cite{xie2021n52}.

Seminal works in contrastive graph learning include Deep Graph Infomax (DGI) \cite{velickovic2019deep}, which maximizes mutual information between node representations and a global graph summary, distinguishing positive local-global pairs from corrupted negative ones. GraphCL \cite{you2020graphcl} further generalized this by exploring various graph augmentation strategies to generate diverse views, demonstrating that the choice of augmentation is critical for effective pre-training. GRACE \cite{zhu2020deep} extended this to node-level contrastive learning, applying both feature and structure corruption to generate two distinct views of a node, then maximizing agreement between their representations. While effective, traditional contrastive methods often rely on carefully constructed negative samples, which can be computationally intensive or challenging to define optimally. Addressing this, \cite{zhang20211dl} proposed a conceptually simple yet effective model that moves away from explicit negative sampling. Inspired by Canonical Correlation Analysis (CCA), their method generates two views of an input graph through augmentation but optimizes a feature-level objective to learn invariant representations and prevent degenerated solutions by decorrelating features in different dimensions. This approach essentially aims to discard augmentation-variant information, offering an alternative to traditional contrastive objectives.

Beyond self-supervised predictive and contrastive tasks, generative pre-training approaches model the underlying graph structure and features directly. Building upon the concept of learning general graph knowledge, \cite{hu2020u8o} introduced GPT-GNN, a generative pre-training framework that models the likelihood of an attributed graph by explicitly factorizing the generation process into coupled attribute and edge generation tasks. This novel dependency-aware factorization allowed the GNN to capture the intricate interplay between node features and graph structure, providing a more holistic understanding of the graph compared to simpler self-supervised pretext tasks. Such generative models aim to learn the data distribution of graphs, enabling them to synthesize new graphs or reconstruct corrupted ones, thereby acquiring a deep understanding of graph topology and feature dependencies.

The utility of self-supervised pre-training extends to complex graph types, such as Heterogeneous Information Networks (HINs). \cite{wei20246l2} demonstrated how self-supervised GNNs can enhance feature extraction in HINs, addressing challenges like heterogeneity and redundancy by flexibly combining different types of additional information. This highlights the adaptability of self-supervision in mining deep features from diverse and complex graph data, improving model adaptability and overall performance. Furthermore, \cite{lu20213kr} recognized the inherent objective divergence between pre-training and downstream fine-tuning. They proposed L2P-GNN, which leverages a meta-learning framework to explicitly optimize the GNN's ability to rapidly adapt to new tasks during pre-training, thereby learning more transferable prior knowledge and bridging the gap between the two phases.

The progression from simple self-supervised pretext tasks to sophisticated contrastive and generative modeling highlights a clear trajectory in GNN research: from learning general graph knowledge to efficiently leveraging that knowledge for specific, often low-resource, tasks. While significant strides have been made in overcoming labeled data scarcity and mitigating negative transfer, persistent challenges remain. These include designing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring extensive task-specific engineering, and developing more principled graph augmentation strategies for contrastive learning that are robust across various graph domains. Future research will likely focus on exploring more theoretically grounded approaches to view generation, developing unified frameworks that combine the strengths of predictive, contrastive, and generative methods, and extending these paradigms to increasingly complex graph structures such as dynamic, temporal, and heterogeneous graphs.
\subsection{Prompt-based Learning and Task Adaptation}
\label{sec:5\_2\_prompt-based\_learning\_\_and\_\_task\_adaptation}

The efficient adaptation of pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks, particularly with limited labeled data, presents a significant challenge. While Section 5.1 detailed various self-supervised and generative pre-training strategies that learn robust representations from unlabeled graph data, the subsequent fine-tuning of these models often incurs high computational costs and can suffer from an "objective gap" between the general pre-training task and specific downstream requirements. This gap can lead to suboptimal performance, especially in few-shot learning scenarios where labeled data is scarce, a problem recognized early in GNN research \cite{satorras20174cv}. To address these limitations, prompt-based learning has emerged as a highly efficient paradigm within the broader context of parameter-efficient fine-tuning (PEFT), drawing inspiration from its success in natural language processing and computer vision. Unlike full fine-tuning or alternative PEFT strategies like self-training which expands labeled data via pseudo-labeling \cite{wang2024htw}, prompting aims to adapt pre-trained GNNs with minimal or no modification to the core model parameters, primarily by reformulating tasks or introducing learnable prompts.

Early explicit prompt-based learning for GNNs, such as \textbf{GPPT} \cite{sun2022d18}, directly tackled the objective gap by reformulating downstream node classification tasks to mimic the pre-training objective of masked edge prediction. GPPT transforms a standalone node and a candidate label class into a "token pair," allowing the pre-trained GNN to evaluate the "linking probability" between them. This approach demonstrated significant improvements in few-shot settings and faster convergence by leveraging the pre-trained knowledge without extensive fine-tuning. However, GPPT's primary limitation was its task-specific nature, requiring careful reformulation of each downstream task to align with a particular pre-training objective, thereby hindering its universality across diverse GNN pre-training strategies and downstream tasks.

To overcome the architectural and task-specific constraints of earlier methods, \textbf{Graph Prompt Feature (GPF)} \cite{fang2022tjj} proposed a more universal prompt tuning method. GPF operates by modifying the input graph's feature space, introducing a learnable vector that is added to all node features. This simple yet effective input-level prompting mechanism allows adaptation to \textit{any} pre-trained GNN and \textit{any} pre-training strategy, irrespective of the underlying pretext task. GPF provides theoretical guarantees for its universality, demonstrating that it can achieve an equivalent effect to various prompting functions. Empirically, it consistently outperforms traditional fine-tuning with significantly fewer tunable parameters, particularly in few-shot settings, showcasing average performance improvements of 1.4\\% in full-shot and 3.2\\% in few-shot scenarios over fine-tuning. This highlights a key design trade-off: GPF's input-level prompting offers broad model-agnosticism and universality.

Further generalizing the concept of prompt-based adaptation, \textbf{GraphPrompt} \cite{liu2023ent} introduced a framework that unifies pre-training (e.g., link prediction) and diverse downstream tasks (node and graph classification) into a common "subgraph similarity" template. In contrast to GPF's input feature modification, GraphPrompt's core innovation lies in designing task-specific learnable prompts that modify the \texttt{ReadOut} operation. These prompts act as adaptive parameters for the aggregation function, enabling the pre-trained GNN to flexibly fuse node representations into task-specific subgraph representations without fine-tuning the backbone model. This readout-level prompting provides more targeted control for graph-level tasks, allowing a single pre-trained model to serve a wider array of downstream objectives by adapting the final aggregation step, albeit potentially being more constrained by the GNN's architecture than input-level prompts.

Most recently, the paradigm has expanded to bridge GNNs with large language models (LLMs), moving towards more semantically aware graph models. \textbf{Morpher (Multi-modal Prompt Learning for Graph Neural Networks)} \cite{li202444f} addresses scenarios with \textit{extremely weak text supervision}. Morpher aligns pre-trained GNN representations with the semantic embedding space of LLMs by simultaneously learning both graph prompts (with an improved, stable design that decouples the prompt from node features to prevent interference) and text prompts, along with a cross-modal projector. By keeping the GNN and LLM frozen, this multi-modal prompting enables CLIP-style zero-shot generalization for GNNs to unseen classes, representing a significant advancement in leveraging rich semantic information for graph understanding.

In conclusion, prompt-based learning for GNNs represents a critical evolution in efficient knowledge transfer, moving from costly full fine-tuning to parameter-efficient adaptation. Initial strategies focused on task reformulation, which then evolved into universal input feature modification and adaptive \texttt{ReadOut} prompting, significantly minimizing the objective gap and facilitating few-shot or zero-shot generalization. The latest advancements in multi-modal prompting, integrating GNNs with LLMs, open exciting avenues for developing semantically richer and more adaptable graph models. However, significant challenges remain. These include designing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring complex prompt engineering or task-specific reformulations. Furthermore, the theoretical understanding of \textit{why} prompts are so effective in GNNs is still nascent, and prompt design often remains heuristic, leading to potential brittleness and sensitivity to hyper-parameters. Scaling prompt learning to extremely large graphs and complex multi-modal scenarios also poses ongoing research questions.
\subsection{Interpreting GNN Decisions: Explainability Methods}
\label{sec:5\_3\_interpreting\_gnn\_decisions:\_explainability\_methods}

The inherent complexity of Graph Neural Networks (GNNs), stemming from their non-linear message-passing mechanisms and intricate graph structures, often renders them "black-box" models. This opacity necessitates robust explainability methods to foster trust, enable debugging, and facilitate scientific discovery in critical applications \cite{zhang20222g3, yuan2020fnk}. GNN explainability methods aim to uncover \textit{why} a GNN makes a specific prediction, typically by identifying influential substructures or features. These methods can be broadly categorized into instance-level explanations, which focus on a single prediction, and model-level explanations, which reveal general patterns learned by the GNN \cite{yuan2020fnk}.

Early efforts in instance-level explanations, such as GNNExplainer \cite{ying2019rza}, pioneered the field by formulating the explanation task as an optimization problem. This method maximizes the mutual information between a GNN's prediction and a compact subgraph structure along with a subset of node features, using a perturbation-based approach to jointly identify their importance. However, GNNExplainer and similar gradient-based methods often implicitly assume linear independence of features, which can be problematic given the non-linear feature integration in GNNs \cite{vu2020zkj}. Addressing this, PGM-Explainer \cite{vu2020zkj} introduced a model-agnostic approach that generates explanations as simpler Bayesian Networks. This allows for explicit modeling of complex, non-linear dependencies among features and graph components, providing richer insights into conditional probabilities rather than just additive attributions. More recently, \cite{bui2024zy9} advanced instance-level explanations by proposing the Myerson-Taylor interaction index and the MAGE explainer. This method uniquely incorporates graph structure into the attribution process, mitigating "out-of-distribution" (OOD) biases common in naive Shapley-based perturbation methods, and is capable of identifying both positively and negatively contributing motifs.

While instance-level explanations provide specific insights, understanding the general behavior of a GNN requires model-level interpretations. XGNN \cite{yuan20208v3} was among the first to tackle this by training a graph generator, guided by reinforcement learning, to produce graph patterns that maximize a specific prediction score of the target GNN. This approach aims to reveal the general graph patterns associated with a particular class, although the generated patterns might not always be directly human-intelligible or perfectly reflect real-world structures \cite{yuan2020fnk}. Building on this, \cite{wang2024j6z} introduced Global Interactive Pattern (GIP) learning, an intrinsically interpretable scheme for graph classification. GIP identifies global interactive patterns by first coarsening the graph and then matching the compressed representation with learnable graph prototypes, moving towards more transparent graph-level reasoning.

Despite these advancements, the faithfulness and generalizability of GNN explainability methods remain critical challenges. A significant critique by \cite{chen2024woq} exposes approximation failures in many existing attention-based interpretable GNNs. Their theoretical framework, based on the Subgraph Multilinear Extension (SubMT), rigorously proves that prevalent attention mechanisms, when combined with non-linear GNNs, cannot accurately approximate the expected prediction over subgraphs. This fundamental limitation leads to degenerated interpretability and poor out-of-distribution (OOD) generalization, underscoring the need for more robust and theoretically grounded explanation mechanisms. In response, \cite{chen2024woq} proposes the Graph Multilinear ne T (GMT) architecture, which uses random subgraph sampling to more faithfully approximate SubMT. Furthermore, to enhance the reliability of explanations, \cite{wu2022vcx} focuses on discovering \textit{invariant rationales} for GNNs. By employing a novel invariant learning strategy with causal interventions, this method aims to identify causal patterns that are stable across different data distributions, thereby filtering out spurious correlations and improving generalization to OOD data, directly addressing the faithfulness concern. The field continues to evolve, with comprehensive surveys like \cite{yuan2020fnk} providing a taxonomic overview and standardized testbeds to guide future research.

The ongoing challenge lies in developing explanation mechanisms that are not only efficient and scalable but also truly faithful to the GNN's decision-making process and generalizable across diverse graph structures and tasks. The critiques regarding approximation failures highlight the need for more rigorous theoretical foundations and novel architectural designs that can inherently provide reliable and trustworthy explanations, rather than relying on potentially misleading proxies.


\label{sec:scalability,_benchmarking,_and_real-world_applications}

\section{Scalability, Benchmarking, and Real-World Applications}
\label{sec:scalability,\_benchmarking,\_\_and\_\_real-world\_applications}

\subsection{Scaling GNNs to Large Graphs}
\label{sec:6\_1\_scaling\_gnns\_to\_large\_graphs}

The application of Graph Neural Networks (GNNs) to massive, real-world graphs, often containing billions of nodes and edges, presents significant computational and memory bottlenecks. Overcoming these limitations is critical for deploying GNNs in web-scale recommender systems, social networks, and other large-scale applications. Researchers have developed a diverse array of techniques, encompassing sophisticated graph sampling strategies, graph condensation, and distributed computing frameworks, to enable efficient training and inference at scale.

A primary approach to address the scalability challenge involves intelligently sampling subgraphs or neighborhoods to reduce the computational burden. A seminal work in this area is PinSage \cite{ying20189jc}, which successfully scaled Graph Convolutional Networks (GCNs) to Pinterest's web-scale recommender system with billions of nodes and edges. PinSage introduced several innovations, including on-the-fly convolutions that dynamically construct computation graphs for mini-batches by sampling node neighborhoods using biased random walks, and an efficient producer-consumer architecture for data loading. For inference, it leveraged a MapReduce pipeline to generate embeddings for billions of nodes in a distributed manner, demonstrating the crucial role of system-level engineering. Building on the concept of efficient propagation, the Approximate Personalized Propagation of Neural Predictions (APPNP) model \cite{klicpera20186xu} offers a scalable solution for deeper GNNs by decoupling feature transformation from propagation. APPNP approximates Personalized PageRank via power iteration, achieving linear time complexity $O(mK)$ (where $m$ is edges, $K$ is iterations) and enabling information flow over large receptive fields without the quadratic complexity of its predecessor. Similarly, for tasks like link prediction, the SEAL framework \cite{zhang2018kdl} demonstrated that high-order heuristics can be accurately approximated from small $h$-hop enclosing subgraphs, providing a theoretical justification for processing localized subgraphs rather than the entire graph, thus reducing the computational scope. Moving beyond traditional convolutional paradigms, the Random Walk with Unifying Memory (RUM) neural network \cite{wang2024oi8} proposes an entirely non-convolutional GNN that processes information via random walks and RNNs. RUM is inherently scalable, compatible with mini-batching, and boasts a runtime complexity agnostic to the number of edges, offering a distinct path to efficiency.

Beyond sampling, another powerful strategy for scaling GNNs is graph condensation, which aims to synthesize smaller, representative graphs. Graph Condensation (GCOND) \cite{jin2021pf0} addresses the immense storage and training time challenges of large graphs by learning a small, synthetic, yet highly informative graph that preserves the GNN's predictive performance. GCOND employs a gradient matching scheme to ensure that GNNs trained on the condensed graph mimic the training trajectory on the original large graph, and innovatively parameterizes the condensed graph structure as a function of its condensed node features, significantly reducing parameter complexity. Complementing this, Iterative Deep Graph Learning with Anchors (IDGL-ANCH) \cite{chen2020bvl} focuses on learning optimal graph structures for GNNs at scale. By introducing an anchor-based approximation, IDGL-ANCH reduces the computational complexity of graph learning from $O(n^2)$ to $O(ns)$ (where $s$ is the number of anchors), making it feasible for large graphs where initial structures might be noisy or incomplete. While not solely focused on scaling, the Dual-channel Diffused Propagation then Transformation (D2PT) framework \cite{liu2023v3e} also contributes to handling large, potentially incomplete graphs by learning a "global graph" to connect stray nodes and enable long-range propagation, thereby enhancing robustness and information flow in challenging large-scale scenarios.

Architectural and system-level innovations are crucial for overcoming these practical limitations. As exemplified by PinSage's distributed MapReduce inference \cite{ying20189jc}, leveraging distributed computing frameworks is essential for processing graphs with billions of nodes. The pervasive nature of scalability challenges is further highlighted by research into adversarial robustness at scale. For instance, methods like Projected Randomized Block Coordinate Descent (PR-BCD) and Greedy R-BCD (GR-BCD) \cite{geisler2021dcq} were developed to enable adversarial attacks on GNNs for graphs with millions of nodes by maintaining sparsity-aware, memory-efficient optimization, underscoring the necessity for scalable operations across all GNN functionalities.

In conclusion, scaling GNNs to large graphs requires a multi-faceted approach. Techniques like neighbor sampling (e.g., PinSage, APPNP, SEAL, RUM) efficiently reduce the effective computational graph, while graph condensation (e.g., GCOND) and scalable structure learning (e.g., IDGL-ANCH) create smaller, representative graphs or optimize connectivity for large-scale processing. These architectural and algorithmic innovations, coupled with robust system-level support and distributed computing, are crucial for enabling GNNs to operate effectively and efficiently in real-world, web-scale applications. However, the trade-off between model expressivity and computational efficiency remains an active area of research, continually pushing the boundaries of what GNNs can achieve on truly massive datasets.
\subsection{Standardized Benchmarking and Evaluation}
\label{sec:6\_2\_st\_and\_ardized\_benchmarking\_\_and\_\_evaluation}

The rapid proliferation of Graph Neural Network (GNN) architectures and applications has underscored a critical need for rigorous and standardized evaluation practices. Historically, the GNN research landscape suffered from a lack of consistent experimental protocols and the prevalence of non-discriminative datasets, which collectively hindered fair comparisons between models and obfuscated truly impactful architectural advancements. Early work, such as the study by \textcite{klicpera20186xu}, already highlighted this issue, observing that many reported performance improvements of GNN models "vanish" under careful statistical scrutiny, emphasizing the necessity for a more stringent evaluation methodology. Similarly, the successful deployment of GNNs in large-scale industrial applications, exemplified by PinSage for web-scale recommender systems \textcite{ying20189jc}, implicitly demanded robust and scalable evaluation frameworks that could reflect real-world complexities.

To address these challenges, the GNN community has witnessed a concerted effort to develop comprehensive benchmarking frameworks and specialized datasets. Early contributions focused on creating rich data resources, such as the large pre-training datasets introduced by \textcite{hu2019r47} to overcome data scarcity and facilitate transfer learning in scientific domains. More recently, specialized datasets have emerged to provide diverse and discriminative testbeds for GNN models in specific application areas. For instance, \textcite{cui2022mjr} introduced BrainGB, a benchmark specifically tailored for brain network analysis, which addresses the unique characteristics of neuroimaging data. In the domain of critical infrastructure, \textcite{varbella20242iz} developed PowerGraph, a power grid benchmark dataset that uniquely offers empirical ground-truth explanations for cascading failure events, enabling the rigorous evaluation of GNN explainability methods.

The development of overarching benchmarking frameworks has been pivotal in standardizing evaluation across a broad spectrum of GNN tasks and models. The Open Graph Benchmark (OGB) \cite{hu2020} (not explicitly in provided summaries but a key framework for context) and related initiatives have played a crucial role. A significant contribution in this area is the comprehensive, open-source benchmarking framework introduced by \textcite{dwivedi20239ab}. This framework directly tackles the "lack of consistent evaluation protocols and discriminative datasets" by proposing a standardized experimental setting. It features a diverse collection of 12 medium-scale datasets, encompassing both real-world graphs (e.g., ZINC, AQSOL, OGB-COLLAB) and mathematically constructed graphs (e.g., PATTERN, CLUSTER, CSL, CYCLES) designed to test specific theoretical properties and differentiate GNN performance. Crucially, it enforces consistent experimental protocols, including fixed parameter budgets (e.g., 100k and 500k parameters) and standardized metrics, to ensure fair comparisons between different GNN architectures. This initiative has already proven instrumental, facilitating the discovery of Graph Positional Encoding (PE) using Laplacian eigenvectors as a crucial component for enhancing GNN performance, particularly on graphs lacking canonical positional information \textcite{dwivedi20239ab}. Complementing this, the GraphGym platform by \textcite{you2020drv} provides a modular infrastructure for systematically studying the GNN design space and promoting standardized, reproducible evaluation. Furthermore, \textcite{li2023o4c} critically examined evaluation practices for link prediction, identifying common pitfalls and proposing a new benchmarking framework that employs a Heuristic Related Sampling Technique (HeaRT) to generate more challenging and realistic negative samples, thereby enabling a more robust assessment of link prediction models.

Beyond predictive accuracy, the maturation of GNN research necessitates rigorous evaluation of trustworthiness aspects, including robustness, fairness, and explainability. \textcite{zhang20222g3} provides a comprehensive survey of trustworthy GNNs, highlighting that each of these dimensions demands specialized and rigorous evaluation protocols. The assessment of GNN robustness, for instance, has evolved significantly. Initial works demonstrated GNN vulnerabilities to adversarial attacks, such as global poisoning attacks via meta-learning \textcite{zgner2019bbi} and topology attacks using optimization perspectives \textcite{xu2019l8n}. While defenses like GNNGuard \textcite{zhang2020jrt} and Elastic GNNs \textcite{liu2021ee2} were proposed, a critical meta-analysis by \textcite{mujkanovic20238fi} revealed that many existing GNN defenses are not robust against \textit{adaptive attacks}. This work underscored the vital importance of evaluating defenses against adversaries who are aware of the defense mechanism, pushing for more sophisticated and realistic evaluation protocols to truly assess GNN security. Efforts to evaluate robustness at scale, as demonstrated by \textcite{geisler2021dcq}, further contribute to this rigorous assessment. Similarly, the evaluation of fairness in GNNs, addressed by works like \textcite{dai2020p5t}, \textcite{dong202183w}, \textcite{wang2022531}, and \textcite{li20245zy}, requires specific metrics and protocols to measure and mitigate bias in node classification and other tasks. The interpretability of GNNs also demands careful evaluation. While early methods like GNNExplainer \textcite{ying2019rza} and XGNN \textcite{yuan20208v3} provided instance-level and model-level explanations, respectively, recent work by \textcite{chen2024woq} critically examined the faithfulness of interpretable GNNs, identifying "Subgraph Multilinear Extension (SubMT) approximation failure" and proposing new fidelity measures. This is complemented by methods like \textcite{bui2024zy9}'s structure-aware interaction index, which aims for more accurate and comprehensive explanations. Furthermore, the need for reliable confidence scores in GNN predictions, particularly given the observed under-confidence in GNNs, has led to the development of calibration methods like CaGCN \textcite{wang20214ku}, which also require rigorous evaluation.

These initiatives are crucial for addressing the historical lack of consistent evaluation practices and the prevalence of non-discriminative datasets. By fostering a culture of rigorous benchmarking, using diverse and discriminative testbeds, and adhering to consistent experimental protocols, the GNN community can accelerate the identification of truly impactful architectural advancements, ensure the trustworthiness of models, and foster reproducible research. However, ongoing challenges remain, including the continuous need for larger, more complex, and dynamic benchmarks that reflect real-world data evolution, and the development of even more sophisticated adaptive evaluation protocols to keep pace with emerging GNN capabilities and potential risks.
\subsection{Diverse Applications of Graph Neural Networks}
\label{sec:6\_3\_diverse\_applications\_of\_graph\_neural\_networks}

Graph Neural Networks (GNNs) have emerged as a pivotal technology, extending far beyond academic research to demonstrate profound practical utility and transformative potential across an increasingly diverse array of real-world domains. Their inherent ability to effectively model intricate relational data and leverage structural information has enabled significant breakthroughs in solving complex problems, driving innovation across various scientific and industrial sectors. The success of GNNs stems from their capacity to learn rich, distributed representations directly from graph topology, making them uniquely suited for tasks where entities and their relationships are paramount.

In the realm of \textbf{recommender systems}, GNNs have proven exceptionally effective due to the inherently graph-structured nature of user-item interactions. These systems often involve bipartite graphs of users and items, or more complex social graphs. Pioneering work like PinSage demonstrated the application of GNNs for web-scale recommendation at Pinterest, showcasing their utility in processing massive graphs to generate high-quality item embeddings \cite{ying20189jc}. Further, GraphRec leveraged GNNs for social recommendation by integrating dual graphs (user-item and user-user) and employing attention mechanisms to capture heterogeneous social relations and opinions, significantly enhancing recommendation quality by modeling the influence of social ties \cite{fan2019k6u}. For sequential recommendation, where dynamic user preferences are crucial, SURGE utilized GNNs to construct item-item interest graphs from interaction sequences, employing attention and dynamic graph pooling to extract activated core preferences \cite{chang2021yyt}. A comprehensive review further highlights the critical challenges and advancements of GNNs in recommender systems, spanning graph construction, network design, and computational efficiency \cite{gao2022f3h}.

Beyond recommendation, GNNs have significantly improved \textbf{link prediction accuracy}, a fundamental task in network analysis critical for inferring missing connections or predicting future interactions. The SEAL framework, for instance, learned high-order features from local enclosing subgraphs using GNNs, providing a theoretical justification for approximating complex heuristics from localized information and outperforming traditional methods \cite{zhang2018kdl}. GNNs are also crucial for \textbf{analyzing complex social networks}, where they can model influence propagation, facilitate community detection, and track information diffusion by learning robust node embeddings that reflect structural roles and relationships.

In \textbf{molecular science and scientific discovery}, GNNs are revolutionizing the prediction of molecular properties, accelerating drug discovery, and advancing physical simulations. GNNs excel here by treating molecules as graphs where atoms are nodes and bonds are edges. Early advancements with k-GNNs demonstrated their ability to capture higher-order graph structures (e.g., triangles, cliques), leading to significant improvements in molecular property prediction tasks \cite{morris20185sd}. Further enhancing this, E(3)-equivariant GNNs like NequIP and EGNNs have emerged, which inherently respect the physical symmetries (rotations, translations) of molecules. This property leads to remarkable data efficiency and state-of-the-art accuracy in learning interatomic potentials and modeling N-body systems, crucial for tasks like quantum chemistry and materials design \cite{batzner2021t07, satorras2021pzl}. GemNet, a universal directional GNN, further pushed boundaries by explicitly incorporating dihedral angles and two-hop message passing for highly accurate molecular property predictions \cite{klicpera20215fk}. Beyond molecules, GNNs are also applied to identify high-dimensional Hamiltonian systems and their dynamics, leveraging symplectic maps combined with permutation equivariance \cite{varghese2024ygs}. A broader review underscores the pervasive use of GNNs in chemistry and materials science, highlighting their role in property prediction, inverse design, and accelerating simulations \cite{reiser2022b08}.

The financial sector benefits immensely from GNNs' ability to model complex transactional and institutional relationships for \textbf{financial fraud detection and risk management}. GNNs can represent financial entities (users, merchants, banks) and their interactions (transactions) as a graph, enabling the detection of anomalous patterns indicative of fraud. Quantum Graph Neural Networks (QGNNs) have been proposed to leverage quantum computing for more efficient fraud detection, outperforming classical GNNs on real-world datasets \cite{innan2023fa7}. More advanced approaches, like Causal Temporal Graph Neural Networks (CaT-GNN), enhance credit card fraud detection by integrating causal invariant learning to reveal inherent correlations and improve robustness against evolving fraud patterns \cite{duan2024que}. GNNs are also vital for identifying economic risks by capturing multi-level and dynamically changing relationships in financial networks, thereby helping institutions and regulators maintain financial system stability \cite{zhang2024ctj}.

In \textbf{critical infrastructure management}, GNNs are instrumental for urban computing, power systems, and communication networks. For \textbf{urban computing}, GNNs model complex spatio-temporal data, such as road networks and traffic flows. Google Maps successfully deployed a GNN-based estimator for Estimated Time of Arrival (ETA) prediction, leveraging road network topology and sophisticated featurization to significantly reduce prediction errors \cite{derrowpinion2021mwn}. For multivariate time series forecasting, GNNs can automatically extract latent spatial dependencies among variables, integrating external knowledge to capture intricate spatio-temporal patterns \cite{wu2020hi3}. A comprehensive survey on Spatio-Temporal GNNs (STGNNs) in urban computing further details their application in traffic prediction, environmental monitoring, and public safety \cite{jin2023e18}. In \textbf{power systems}, GNNs are applied for fault scenario identification, power flow calculation, and stability analysis by modeling the grid as a graph of generators, loads, and transmission lines \cite{liao202120x, zhao2024aer}. The PowerGraph benchmark provides a dedicated dataset for training and evaluating GNNs for these critical tasks, including cascading failure prediction \cite{varbella20242iz}. GNNs are even being explored for power control in 6G in-factory subnetworks, optimizing transmit power by representing the network as a graph \cite{abode2024m4z}. For \textbf{communication networks}, GNNs enhance fault scenario identification by integrating propositional logic rules with graph learning to improve accuracy in diagnosing network issues \cite{zhao2024aer}.

GNNs are also making significant strides in \textbf{information processing} domains like natural language processing (NLP), knowledge graphs, and computer vision. In \textbf{NLP}, GNNs are increasingly applied to tasks like text classification by transforming text into graph structures (e.g., word-document graphs), capturing global and contextual-aware word relations that traditional sequential models often miss \cite{wang2023wrg}. For \textbf{knowledge graphs} (KGs), which represent factual information among entities and relations, GNNs are fundamental for tasks such as link prediction (knowledge graph completion), knowledge graph alignment, and reasoning, by learning robust embeddings for entities and relations \cite{ye20226hn}. In \textbf{computer vision}, GNNs handle irregular data types like point clouds, meshes, or object relationships in images and videos. They are used for tasks such as 3D object detection, semantic segmentation, and classification, offering a powerful way to process non-Euclidean visual data \cite{chen2022mmu, li2024yyl}.

Furthermore, GNNs are advancing \textbf{network neuroscience}, where they analyze brain connectivity data for tasks like disease diagnosis and synthesizing missing data, effectively preserving the non-Euclidean topological properties of brain graphs \cite{bessadok2021bfy}. The BrainGB benchmark provides a standardized platform for developing and evaluating GNNs for brain network analysis \cite{cui2022mjr}. GNNs are also being applied to \textbf{combinatorial optimization (CO)} problems, where they can learn to guide heuristic search, improve exact solvers, or even perform end-to-end algorithmic reasoning, leveraging their permutation invariance and sparsity awareness \cite{cappart2021xrp}.

The widespread adoption of GNNs across these diverse fields underscores their versatility and power. By effectively mapping real-world problems into graph structures and leveraging their unique ability to learn from relational data, GNNs continue to drive innovation and provide sophisticated solutions across an ever-expanding array of scientific and industrial applications.


\label{sec:conclusion_and_future_directions}

\section{Conclusion and Future Directions}
\label{sec:conclusion\_\_and\_\_future\_directions}

\subsection{Summary of Key Advancements}
\label{sec:7\_1\_summary\_of\_key\_advancements}

The trajectory of Graph Neural Network (GNN) research represents a profound evolution, transforming the landscape of machine learning on graph-structured data from nascent theoretical constructs to a powerful, versatile, and increasingly trustworthy paradigm. This remarkable progression, as comprehensively reviewed by \cite{wu2022ptq}, has been driven by a continuous cycle of identifying fundamental challenges, developing innovative solutions, and rigorously evaluating their impact.

The field's intellectual journey began with foundational theoretical models, conceptualizing GNNs as iterative processes that propagate information to learn stable node representations. Pioneering works by \cite{Gori05} and \cite{Scarselli09} established the mathematical groundwork, viewing GNNs through the lens of fixed-point iteration. While groundbreaking, these early models were often computationally intensive and lacked scalability. A pivotal shift occurred with the emergence of the message-passing paradigm, which simplified and popularized GNNs, making them practical for a wider array of applications. Key breakthroughs included the Graph Convolutional Network (GCN) by \cite{Kipf17}, which offered a spectral simplification for efficient semi-supervised learning, and GraphSAGE by \cite{Hamilton17}, enabling inductive learning on large graphs through efficient neighbor sampling. The introduction of Graph Attention Networks (GATs) by \cite{Velickovic18} further enhanced expressivity by allowing models to learn adaptive importance weights for neighboring nodes, addressing the limitations of fixed aggregation schemes. These architectures formed the bedrock for subsequent advancements, demonstrating the power of localized information aggregation.

As GNNs gained prominence, researchers delved deeper into their theoretical capabilities and limitations, particularly concerning their expressive power. The connection to the Weisfeiler-Lehman (WL) graph isomorphism test, notably formalized by \cite{Xu19}, provided a crucial benchmark, revealing that many standard GNNs are upper-bounded by the 1-WL test in their ability to distinguish non-isomorphic graphs. This understanding spurred efforts to enhance expressivity through higher-order GNNs, such as k-GNNs \cite{morris20185sd}, which operate on k-tuples of nodes to capture richer structural information. Concurrently, theoretical analyses extended to spectral GNNs, with works like \cite{wang2022u2l} challenging assumptions about the necessity of nonlinearity and proving conditions for universal approximation in linear spectral models, further deepening the understanding of GNN capabilities. The pursuit of deeper GNN architectures, however, exposed new challenges like over-smoothing, where repeated message passing homogenizes node representations, and over-squashing, which impedes long-range information propagation. Solutions ranged from decoupling propagation from prediction, as seen in PPNP/APPNP \cite{klicpera20186xu}, to incorporating residual connections and developing non-convolutional designs like RUM \cite{wang2024oi8}, all aimed at enabling effective learning in deeper layers. For specialized domains, equivariant GNNs, exemplified by E(n) Equivariant GNNs (EGNNs) \cite{satorras2021pzl}, emerged to respect inherent symmetries in physical and geometric data, crucial for fields like molecular modeling.

The transition to real-world deployment highlighted the critical need for GNNs to be robust, adaptable, and trustworthy. Real-world graphs often deviate from ideal assumptions, exhibiting heterophily (connections between dissimilar nodes) or possessing incomplete and noisy structures. This led to the development of adaptive filtering mechanisms, node-wise expert mixtures \cite{han2024rkj}, and methods for jointly learning graph structures and node embeddings \cite{chen2020bvl}, significantly improving GNN performance on complex, imperfect data. A critical area of focus has been the trustworthiness of GNNs, encompassing robustness against adversarial attacks, fairness in predictions, and privacy protection. Surveys like \cite{dai2022hsi} and \cite{zhang20222g3} underscore the unique vulnerabilities of GNNs due to their relational nature, where small perturbations or biases can propagate widely. Research has addressed various attack types (e.g., poisoning, evasion) and defenses (e.g., robust aggregation, adversarial training), while also tackling fairness issues through debiasing techniques \cite{dai2020p5t} and privacy concerns like link stealing \cite{he2020kz4} through differential privacy and anonymization. The interconnectedness of these trustworthiness dimensions—where explainability can aid in debugging biases or adversarial vulnerabilities—is a growing area of focus \cite{dai2022hsi}.

Recent advancements have also focused on enhancing GNN utility through advanced learning paradigms, particularly pre-training and prompt-based learning. Addressing the common challenge of labeled data scarcity, self-supervised and generative pre-training strategies have emerged, allowing GNNs to learn robust and transferable representations from abundant unlabeled graph data \cite{hu2019r47, hu2020u8o}. This paradigm has been further refined by prompt-based learning, which enables efficient adaptation of pre-trained GNNs to diverse downstream tasks with minimal fine-tuning, often by reformulating tasks or introducing learnable prompts \cite{sun2022d18, liu2023ent}. This approach significantly minimizes the "objective gap" between pre-training and downstream tasks, facilitating few-shot and even zero-shot generalization. Concurrently, the critical need for interpretability has driven the development of methods to explain GNN decisions, fostering trust and enabling debugging. Techniques range from instance-level explanations identifying influential subgraphs \cite{ying2019rza} to model-level approaches uncovering general patterns \cite{yuan20208v3}, though recent critiques highlight ongoing challenges in ensuring the faithfulness of these explanations \cite{chen2024woq}.

Finally, the maturation of the field is evident in dedicated efforts towards scalability, standardized benchmarking, and the proliferation of real-world applications. Techniques like graph sampling \cite{Hamilton17, ying20189jc} and graph condensation \cite{jin2021pf0} have been crucial for scaling GNNs to massive graphs with billions of nodes and edges. The establishment of comprehensive benchmarking frameworks, such as the Open Graph Benchmark (OGB) \cite{Hu20} and other specialized datasets \cite{dwivedi20239ab, varbella20242iz}, has instilled scientific rigor, enabling fair comparisons and accelerating the identification of truly impactful architectural advancements. These collective efforts have solidified GNNs' practical relevance across a diverse array of domains, from recommender systems \cite{fan2019k6u} and molecular science to urban computing and cybersecurity \cite{mitra2024x43}, demonstrating their transformative potential in solving complex relational problems.

In summary, the journey of GNN research has been one of continuous innovation, moving from abstract theoretical models to highly practical, expressive, and increasingly trustworthy architectures. This progression has been characterized by a deep interplay between theoretical insights into expressivity and depth, and practical exigencies arising from real-world data imperfections and ethical considerations. The development of advanced learning paradigms like pre-training and prompt tuning, coupled with robust benchmarking and scalability solutions, has cemented GNNs as an indispensable tool for complex relational reasoning. These collective advancements have not only addressed fundamental challenges but have profoundly transformed the landscape of machine learning on graph-structured data, establishing GNNs as a powerful and versatile paradigm for complex relational reasoning.
\subsection{Open Challenges and Research Gaps}
\label{sec:7\_2\_open\_challenges\_\_and\_\_research\_gaps}

Despite the remarkable advancements in Graph Neural Networks (GNNs), several critical unresolved issues and fundamental research gaps persist, defining the frontiers of the field. These challenges often involve inherent trade-offs between expressive power, scalability, and trustworthiness, continuously driving innovation towards the next generation of GNN research.

One significant challenge lies in developing truly universal pre-training objectives that generalize across vastly different domains. Early efforts, such as those by \cite{hu2019r47}, systematically investigated strategies for pre-training GNNs using self-supervised node- and graph-level tasks like context prediction and attribute masking to mitigate negative transfer. Building on this, \cite{hu2020u8o} introduced GPT-GNN, a generative pre-training framework that explicitly models the dependency between node attributes and graph structure, offering a more robust initialization. Further, \cite{lu20213kr} leveraged meta-learning to optimize GNN pre-training for rapid adaptation, directly addressing the objective divergence between pre-training and fine-tuning. However, these methods often require careful task reformulation or are tailored to specific graph types. More recent work, like \cite{sun2022d18}'s GPPT and \cite{liu2023ent}'s GraphPrompt, explored prompt-based tuning to bridge the pre-training and downstream task gap by reformulating tasks or prompting the readout function. While \cite{li202444f} extended this to multi-modal settings, aligning GNNs with Large Language Models for semantic understanding, the core challenge remains in designing pre-training objectives that are intrinsically universal and adaptable without extensive domain-specific engineering or complex prompt design, especially for out-of-distribution scenarios.

Achieving robust defenses against increasingly sophisticated adaptive attacks is another pressing concern. Initial work by \cite{zhang2020jrt} proposed GNNGuard, a defense mechanism that prunes suspicious edges based on neighbor importance and layer-wise graph memory. Similarly, \cite{xu2019l8n} introduced an optimization-based adversarial training framework, leveraging convex relaxation to generate topology attacks and train robust GNNs. However, a critical meta-analysis by \cite{mujkanovic20238fi} revealed that most existing GNN defenses are not robust against adaptive attacks, often performing no better than undefended baselines when the adversary has knowledge of the defense mechanism. This highlights a significant gap between perceived and actual robustness. Furthermore, \cite{geisler2021dcq} demonstrated that even advanced attacks can be scaled to graphs with millions of nodes, underscoring the need for defenses that are both effective and scalable. The development of truly robust GNNs requires a paradigm shift towards defenses that can withstand adaptive, white-box adversaries, potentially through certified robustness or fundamentally new architectural designs.

Designing intrinsically interpretable GNNs that offer faithful explanations is paramount for their deployment in high-stakes domains. \cite{ying2019rza}'s GNNExplainer pioneered instance-level explanations by identifying influential subgraphs and node features. Moving towards model-level insights, \cite{yuan20208v3}'s XGNN utilized reinforcement learning to generate graph patterns that maximize specific GNN predictions, while \cite{wang2024j6z} explored unveiling global interactive patterns across graphs via coarsening. \cite{bui2024zy9} further refined instance-level explanations using structure-aware interaction indices. Despite these advancements, a significant interpretability gap was exposed by \cite{chen2024woq}, which theoretically proved the "Subgraph Multilinear Extension (SubMT) approximation failure" of many attention-based interpretable GNNs. This implies that their explanations are often unfaithful and do not reliably generalize, highlighting a critical need for GNNs that are not only interpretable but also provably faithful in their explanations.

Handling dynamic and heterogeneous graphs more effectively remains a complex challenge. While \cite{zheng2022qxr} provided a comprehensive survey on GNNs for heterophilous graphs, and \cite{ma2021sim} challenged the strict homophily assumption, the issue of mixed homophilic and heterophilic patterns within a single graph persists \cite{mao202313j}. Solutions like \cite{luan202272y}'s Adaptive Channel Mixing and \cite{han2024rkj}'s NODE-MOE (Node-wise Filtering via Mixture of Experts) adaptively apply filters, but a unified approach for dynamically evolving heterogeneous structures is still elusive. For temporal graphs, \cite{longa202399q} surveyed the state-of-the-art and identified numerous open challenges, while \cite{chang2021yyt}'s SURGE dynamically constructed item-item graphs for sequential recommendation, albeit in a domain-specific manner. Furthermore, real-world graphs often suffer from weak information (missing structure, features, or labels), which \cite{liu2023v3e}'s D2PT framework addressed with dual-channel propagation. The goal is to develop GNNs that can adaptively learn optimal structures and propagation rules for graphs that are simultaneously dynamic, heterogeneous, and incomplete, without relying on extensive manual tuning or domain-specific heuristics.

Finally, scaling to even larger and more complex real-world systems with billions of nodes and edges presents persistent computational hurdles. Pioneering work like \cite{ying20189jc}'s PinSage successfully scaled GCNs to web-scale recommender systems through innovations like on-the-fly convolutions and importance pooling. \cite{klicpera20186xu}'s APPNP addressed oversmoothing and complexity by decoupling prediction and propagation, enabling deeper GNNs without increasing model parameters. \cite{jin2021pf0}'s GCOND introduced graph condensation, drastically reducing graph size while preserving GNN performance. More recently, \cite{wang2024oi8}'s RUM proposed a non-convolutional GNN that is theoretically more expressive and scalable by processing random walk trajectories with RNNs, mitigating over-smoothing and over-squashing. While benchmarks like \cite{dwivedi20239ab}'s BrainGB and PowerGraph \cite{varbella20242iz} provide valuable resources, they often focus on medium-scale datasets, highlighting the gap for truly massive, real-world graphs. The challenge extends beyond merely optimizing existing GNNs; it necessitates designing fundamentally new architectures that inherently scale to unprecedented sizes while maintaining expressive power and efficiency.

These open challenges underscore the inherent trade-offs between expressive power (e.g., \cite{xu2018c8q}, \cite{morris20185sd}), scalability (\cite{ying20189jc}, \cite{jin2021pf0}), and trustworthiness (including robustness, interpretability, fairness, and privacy, as highlighted by \cite{zhang20222g3}, \cite{dai2020p5t}, \cite{dong202183w}, \cite{dong2021qcg}, \cite{he2020kz4}, \cite{li20245zy}, \cite{wu2022vcx}, \cite{zgner2019bbi}). Addressing these intricate relationships and developing GNNs that can navigate these trade-offs effectively continues to drive innovation and define the next generation of GNN research.
\subsection{Ethical Considerations and Societal Impact}
\label{sec:7\_3\_ethical\_considerations\_\_and\_\_societal\_impact}

The rapid advancement of Graph Neural Networks (GNNs) presents a dual narrative of immense societal benefit and profound ethical challenges, necessitating a comprehensive examination of their broader implications. GNNs are uniquely positioned to address grand societal challenges by leveraging the intricate relationships inherent in complex data. In healthcare, GNNs are instrumental in accelerating drug discovery by modeling molecular structures \cite{zhang2021jqr}, enhancing our understanding of brain function and neurological disorders through advanced brain network analysis \cite{bessadok2021bfy, cui2022mjr}, and facilitating disease diagnosis, as supported by benchmarks like \textit{BrainGB} \cite{cui2022mjr}. Beyond medicine, GNNs contribute to climate modeling through sophisticated time series forecasting \cite{jin2023ijy}, enhance the resilience of critical infrastructure via applications in power grid management using datasets like \textit{PowerGraph} \cite{varbella20242iz}, and enable smarter urban environments and health monitoring in Internet of Things (IoT) applications \cite{dong20225aw}. Their utility also extends to web-scale recommender systems, exemplified by \textit{PinSage} \cite{ying20189jc}, and bolstering defensive cyber operations by identifying complex threat patterns \cite{mitra2024x43}.

However, the transformative power of GNNs in these high-stakes domains underscores the paramount importance of developing and deploying them responsibly. A holistic framework for trustworthy GNNs, encompassing robustness, explainability, privacy, fairness, accountability, and environmental well-being, is crucial, acknowledging the unique challenges posed by graph data \cite{zhang20222g3, dai2022hsi}. The ethical pitfalls are not merely technical hurdles but translate directly into societal risks, demanding careful consideration.

In \textbf{healthcare and personalized medicine}, the promise of GNNs is tempered by significant ethical concerns. The highly sensitive nature of patient data makes privacy a critical issue. As discussed in Section 4.3, GNNs are vulnerable to "link stealing attacks" that can infer relationships, or node attribute inference, potentially exposing private medical conditions or social connections \cite{he2020kz4}. Federated GNNs (FedGNNs) offer a promising technical solution to enhance data privacy by enabling collaborative model training without centralizing sensitive data \cite{liu2022gcg}. Furthermore, GNNs trained on biased medical datasets could lead to discriminatory diagnoses or treatment recommendations for certain demographic groups, amplifying existing health disparities. The lack of transparent reasoning (explainability) in GNN predictions, as detailed in Section 5.3, poses a significant barrier to trust in medical applications, where clinicians need to understand \textit{why} a diagnosis or treatment is suggested. While methods like \textit{GNNExplainer} \cite{ying2019rza} and \textit{MAGE} \cite{bui2024zy9} aim to provide instance-level explanations, critical evaluations reveal that many attention-based interpretable GNNs suffer from approximation failures, leading to unfaithful explanations \cite{chen2024woq, agarwal2022xfp}. This highlights the need for methods that discover invariant, causal rationales to ensure explanations are stable and reflect true underlying mechanisms \cite{wu2022vcx}.

For \textbf{critical infrastructure and cybersecurity}, the reliability and robustness of GNNs are paramount. The deployment of GNNs in power grid management or threat detection systems, while beneficial, introduces vulnerabilities. As elaborated in Section 4.2, GNNs are highly susceptible to adversarial attacks that subtly perturb graph structure or features, leading to catastrophic performance degradation \cite{zhang2020jrt, zgner2019bbi, xu2019l8n}. Such attacks could compromise the stability of power grids or allow malicious actors to evade detection, with potentially devastating societal consequences. While defenses like \textit{GNNGuard} \cite{zhang2020jrt} exist, a critical assessment reveals that many proposed GNN defenses are not robust against adaptive attacks, leading to overly optimistic security estimates \cite{mujkanovic20238fi}. This lack of guaranteed robustness raises serious questions about accountability when GNN-driven systems fail due to malicious interference.

In \textbf{social networks and recommender systems}, GNNs offer personalized experiences but also carry risks of bias amplification and social manipulation. As discussed in Section 4.3, GNNs can inherit and magnify historical biases present in training data, leading to discriminatory outcomes in recommendations or content moderation \cite{dai2020p5t, wang2022531}. This can reinforce stereotypes, create "filter bubbles," and limit exposure to diverse perspectives. Beyond technical bias, the sheer power of GNNs to model and predict social interactions raises concerns about their potential for misuse in mass surveillance, social scoring systems, or the targeted spread of misinformation, posing threats to individual liberties and democratic processes.

Beyond these domain-specific challenges, broader ethical considerations demand attention. The \textbf{environmental impact} of training increasingly large and complex GNNs, especially for web-scale applications, is a growing concern. The computational resources and energy consumption required contribute to carbon emissions, linking GNN scalability (Section 6.1) directly to environmental well-being \cite{zhang20222g3}. Furthermore, the \textbf{dual-use nature} of GNN technology means that advancements intended for beneficial purposes (e.g., drug discovery) could potentially be repurposed for harmful ones (e.g., designing toxic molecules). This necessitates robust \textbf{governance and accountability} frameworks to ensure responsible innovation and deployment, especially given the complex, often opaque nature of GNN models.

In conclusion, while GNNs offer immense potential for societal advancement across diverse critical domains, their ethical deployment hinges on addressing fundamental challenges related to bias, privacy, robustness, transparency, and broader societal impacts. The literature demonstrates a growing commitment to developing GNNs that are not only powerful but also fair, secure, reliable, and interpretable. Future research must continue to bridge the gap between predictive performance and these crucial trustworthiness aspects, focusing on rigorous evaluation, developing theoretically grounded methods for interpretability \cite{yuan2020fnk}, and designing inherently robust and privacy-preserving architectures to foster public trust and mitigate unintended negative consequences in an increasingly interconnected world.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{328}

\bibitem{wang2024oi8}
Yuanqing Wang, and Kyunghyun Cho (2024). \textit{Non-convolutional Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{li2022315}
Xiang Li, Renyu Zhu, Yao Cheng, et al. (2022). \textit{Finding Global Homophily in Graph Neural Networks When Meeting Heterophily}. International Conference on Machine Learning.

\bibitem{kang2024fsk}
Qiyu Kang, Kai Zhao, Qinxu Ding, et al. (2024). \textit{Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND}. International Conference on Learning Representations.

\bibitem{gao2022f3h}
Chen Gao, Xiang Wang, Xiangnan He, et al. (2022). \textit{Graph Neural Networks for Recommender System}. Web Search and Data Mining.

\bibitem{li2023o4c}
Juanhui Li, Harry Shomer, Haitao Mao, et al. (2023). \textit{Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking}. Neural Information Processing Systems.

\bibitem{michel2023hc4}
Gaspard Michel, Giannis Nikolentzos, J. Lutzeyer, et al. (2023). \textit{Path Neural Networks: Expressive and Accurate Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{chen2022mmu}
Chaoqi Chen, Yushuang Wu, Qiyuan Dai, et al. (2022). \textit{A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{yuan2021pgd}
Hao Yuan, Haiyang Yu, Jie Wang, et al. (2021). \textit{On Explainability of Graph Neural Networks via Subgraph Explorations}. International Conference on Machine Learning.

\bibitem{dong202183w}
Yushun Dong, Jian Kang, H. Tong, et al. (2021). \textit{Individual Fairness for Graph Neural Networks: A Ranking based Approach}. Knowledge Discovery and Data Mining.

\bibitem{cappart2021xrp}
Quentin Cappart, D. Chételat, Elias Boutros Khalil, et al. (2021). \textit{Combinatorial optimization and reasoning with graph neural networks}. International Joint Conference on Artificial Intelligence.

\bibitem{dong2021qcg}
Yushun Dong, Ninghao Liu, B. Jalaeian, et al. (2021). \textit{EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks}. The Web Conference.

\bibitem{li20245zy}
Zhixun Li, Yushun Dong, Qiang Liu, et al. (2024). \textit{Rethinking Fair Graph Neural Networks from Re-balancing}. Knowledge Discovery and Data Mining.

\bibitem{zhao2020bmj}
Tong Zhao, Yozen Liu, Leonardo Neves, et al. (2020). \textit{Data Augmentation for Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{joshi20239d0}
Chaitanya K. Joshi, and Simon V. Mathis (2023). \textit{On the Expressive Power of Geometric Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{sun2022d18}
Mingchen Sun, Kaixiong Zhou, Xingbo He, et al. (2022). \textit{GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{derrowpinion2021mwn}
Austin Derrow-Pinion, Jennifer She, David Wong, et al. (2021). \textit{ETA Prediction with Graph Neural Networks in Google Maps}. International Conference on Information and Knowledge Management.

\bibitem{chen2020bvl}
Yu Chen, Lingfei Wu, and Mohammed J. Zaki (2020). \textit{Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings}. Neural Information Processing Systems.

\bibitem{zeng2022jhz}
Hanqing Zeng, Muhan Zhang, Yinglong Xia, et al. (2022). \textit{Decoupling the Depth and Scope of Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{yuan20208v3}
Haonan Yuan, Jiliang Tang, Xia Hu, et al. (2020). \textit{XGNN: Towards Model-Level Explanations of Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{xie2021n52}
Yaochen Xie, Zhao Xu, Zhengyang Wang, et al. (2021). \textit{Self-Supervised Learning of Graph Neural Networks: A Unified Review}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mitra2024x43}
Shaswata Mitra, Trisha Chakraborty, Subash Neupane, et al. (2024). \textit{Use of Graph Neural Networks in Aiding Defensive Cyber Operations}. arXiv.org.

\bibitem{zhang2021kc7}
Muhan Zhang, and Pan Li (2021). \textit{Nested Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{wang2022p2r}
Hongya Wang, Haoteng Yin, Muhan Zhang, et al. (2022). \textit{Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{lu20213kr}
Yuanfu Lu, Xunqiang Jiang, Yuan Fang, et al. (2021). \textit{Learning to Pre-train Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{fan2022m67}
Shaohua Fan, Xiao Wang, Yanhu Mo, et al. (2022). \textit{Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure}. Neural Information Processing Systems.

\bibitem{zhang2020b0m}
Zaixi Zhang, Jinyuan Jia, Binghui Wang, et al. (2020). \textit{Backdoor Attacks to Graph Neural Networks}. ACM Symposium on Access Control Models and Technologies.

\bibitem{cui2022mjr}
Hejie Cui, Wei Dai, Yanqiao Zhu, et al. (2022). \textit{BrainGB: A Benchmark for Brain Network Analysis With Graph Neural Networks}. IEEE Transactions on Medical Imaging.

\bibitem{bui2024zy9}
Ngoc Bui, Hieu Trung Nguyen, Viet Anh Nguyen, et al. (2024). \textit{Explaining Graph Neural Networks via Structure-aware Interaction Index}. International Conference on Machine Learning.

\bibitem{liu2022a5y}
Chuang Liu, Yibing Zhan, Chang Li, et al. (2022). \textit{Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities}. International Joint Conference on Artificial Intelligence.

\bibitem{jin2023ijy}
Ming Jin, Huan Yee Koh, Qingsong Wen, et al. (2023). \textit{A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{ying2019rza}
Rex Ying, Dylan Bourgeois, Jiaxuan You, et al. (2019). \textit{GNNExplainer: Generating Explanations for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{liu2020w3t}
Meng Liu, Hongyang Gao, and Shuiwang Ji (2020). \textit{Towards Deeper Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{longa202399q}
Antonio Longa, Veronica Lachi, G. Santin, et al. (2023). \textit{Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities}. Trans. Mach. Learn. Res..

\bibitem{papp20211ac}
P. Papp, Karolis Martinkus, Lukas Faber, et al. (2021). \textit{DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{chang2021yyt}
Jianxin Chang, Chen Gao, Y. Zheng, et al. (2021). \textit{Sequential Recommendation with Graph Neural Networks}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{mujkanovic20238fi}
Felix Mujkanovic, Simon Geisler, Stephan Gunnemann, et al. (2023). \textit{Are Defenses for Graph Neural Networks Robust?}. Neural Information Processing Systems.

\bibitem{you2021uxi}
Jiaxuan You, Jonathan M. Gomes-Selman, Rex Ying, et al. (2021). \textit{Identity-aware Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{luo2024euy}
Dongsheng Luo, Tianxiang Zhao, Wei Cheng, et al. (2024). \textit{Towards Inductive and Efficient Explanations for Graph Neural Networks}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{cui2022pap}
Hejie Cui, Wei Dai, Yanqiao Zhu, et al. (2022). \textit{Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{dai2022xze}
Enyan Dai, Wei-dong Jin, Hui Liu, et al. (2022). \textit{Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels}. Web Search and Data Mining.

\bibitem{wang2023wrg}
Kunze Wang, Yihao Ding, and S. Han (2023). \textit{Graph Neural Networks for Text Classification: A Survey}. Artificial Intelligence Review.

\bibitem{khemani2024i8r}
Bharti Khemani, S. Patil, K. Kotecha, et al. (2024). \textit{A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions}. Journal of Big Data.

\bibitem{agarwal2022xfp}
Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, et al. (2022). \textit{Evaluating explainability for graph neural networks}. Scientific Data.

\bibitem{dwivedi20239ab}
Vijay Prakash Dwivedi, Chaitanya K. Joshi, T. Laurent, et al. (2023). \textit{Benchmarking Graph Neural Networks}. Journal of machine learning research.

\bibitem{abboud2020x5e}
Ralph Abboud, .Ismail .Ilkan Ceylan, Martin Grohe, et al. (2020). \textit{The Surprising Power of Graph Neural Networks with Random Node Initialization}. International Joint Conference on Artificial Intelligence.

\bibitem{liu2023v3e}
Yixin Liu, Kaize Ding, Jianling Wang, et al. (2023). \textit{Learning Strong Graph Neural Networks with Weak Information}. Knowledge Discovery and Data Mining.

\bibitem{liu2021ee2}
Xiaorui Liu, W. Jin, Yao Ma, et al. (2021). \textit{Elastic Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{balcilar20215ga}
M. Balcilar, P. Héroux, Benoit Gaüzère, et al. (2021). \textit{Breaking the Limits of Message Passing Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{hu2019r47}
Weihua Hu, Bowen Liu, Joseph Gomes, et al. (2019). \textit{Strategies for Pre-training Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{chamberlain2022fym}
B. Chamberlain, S. Shirobokov, Emanuele Rossi, et al. (2022). \textit{Graph Neural Networks for Link Prediction with Subgraph Sketching}. International Conference on Learning Representations.

\bibitem{reiser2022b08}
Patrick Reiser, Marlen Neubert, Andr'e Eberhard, et al. (2022). \textit{Graph neural networks for materials science and chemistry}. Communications Materials.

\bibitem{li2021orq}
Guohao Li, Matthias Müller, Bernard Ghanem, et al. (2021). \textit{Training Graph Neural Networks with 1000 Layers}. International Conference on Machine Learning.

\bibitem{wang2022u2l}
Xiyuan Wang, and Muhan Zhang (2022). \textit{How Powerful are Spectral Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{zhang2021wgf}
Zaixin Zhang, Qi Liu, Hao Wang, et al. (2021). \textit{ProtGNN: Towards Self-Explaining Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{garg2020z6o}
Vikas K. Garg, S. Jegelka, and T. Jaakkola (2020). \textit{Generalization and Representational Limits of Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{fatemi2021dmb}
Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi (2021). \textit{SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zhang2021jqr}
Yuyu Zhang, Xinshi Chen, Yuan Yang, et al. (2021). \textit{Graph Neural Networks}. Deep Learning on Graphs.

\bibitem{varbella20242iz}
Anna Varbella, Kenza Amara, B. Gjorgiev, et al. (2024). \textit{PowerGraph: A power grid benchmark dataset for graph neural networks}. Neural Information Processing Systems.

\bibitem{rusch2023xev}
T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra (2023). \textit{A Survey on Oversmoothing in Graph Neural Networks}. arXiv.org.

\bibitem{chen2020e6g}
Zhengdao Chen, Lei Chen, Soledad Villar, et al. (2020). \textit{Can graph neural networks count substructures?}. Neural Information Processing Systems.

\bibitem{zhang20222g3}
He Zhang, Bang Wu, Xingliang Yuan, et al. (2022). \textit{Trustworthy Graph Neural Networks: Aspects, Methods, and Trends}. Proceedings of the IEEE.

\bibitem{han2024rkj}
Haoyu Han, Juanhui Li, Wei Huang, et al. (2024). \textit{Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach}. arXiv.org.

\bibitem{rossi2020otv}
Emanuele Rossi, Fabrizio Frasca, B. Chamberlain, et al. (2020). \textit{SIGN: Scalable Inception Graph Neural Networks}. arXiv.org.

\bibitem{wu2022vcx}
Yingmin Wu, Xiang Wang, An Zhang, et al. (2022). \textit{Discovering Invariant Rationales for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{morris20185sd}
Christopher Morris, Martin Ritzert, Matthias Fey, et al. (2018). \textit{Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{dai2022hsi}
Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, et al. (2022). \textit{A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability}. Machine Intelligence Research.

\bibitem{wang2024j6z}
Yuwen Wang, Shunyu Liu, Tongya Zheng, et al. (2024). \textit{Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{ju2023prm}
Haotian Ju, Dongyue Li, Aneesh Sharma, et al. (2023). \textit{Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion}. International Conference on Artificial Intelligence and Statistics.

\bibitem{liu20242g6}
Zewen Liu, Guancheng Wan, B. A. Prakash, et al. (2024). \textit{A Review of Graph Neural Networks in Epidemic Modeling}. Knowledge Discovery and Data Mining.

\bibitem{zhang2018kdl}
Muhan Zhang, and Yixin Chen (2018). \textit{Link Prediction Based on Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{bianchi20194ea}
F. Bianchi, Daniele Grattarola, L. Livi, et al. (2019). \textit{Graph Neural Networks With Convolutional ARMA Filters}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{ma2021sim}
Yao Ma, Xiaorui Liu, Neil Shah, et al. (2021). \textit{Is Homophily a Necessity for Graph Neural Networks?}. International Conference on Learning Representations.

\bibitem{li202444f}
Li, Lecheng Zheng, Bowen Jin, et al. (2024). \textit{Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{he2020kz4}
Xinlei He, Jinyuan Jia, M. Backes, et al. (2020). \textit{Stealing Links from Graph Neural Networks}. USENIX Security Symposium.

\bibitem{fang2022tjj}
Taoran Fang, Yunchao Zhang, Yang Yang, et al. (2022). \textit{Universal Prompt Tuning for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{chen2024woq}
Yongqiang Chen, Yatao Bian, Bo Han, et al. (2024). \textit{How Interpretable Are Interpretable Graph Neural Networks?}. International Conference on Machine Learning.

\bibitem{liu2023ent}
Zemin Liu, Xingtong Yu, Yuan Fang, et al. (2023). \textit{GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks}. The Web Conference.

\bibitem{dong20225aw}
Guimin Dong, Mingyue Tang, Zhiyuan Wang, et al. (2022). \textit{Graph Neural Networks in IoT: A Survey}. ACM Trans. Sens. Networks.

\bibitem{fan2019k6u}
Wenqi Fan, Yao Ma, Qing Li, et al. (2019). \textit{Graph Neural Networks for Social Recommendation}. The Web Conference.

\bibitem{you2020drv}
Jiaxuan You, Rex Ying, and J. Leskovec (2020). \textit{Design Space for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{cai2020k4b}
Chen Cai, and Yusu Wang (2020). \textit{A Note on Over-Smoothing for Graph Neural Networks}. arXiv.org.

\bibitem{gosch20237yi}
Lukas Gosch, Simon Geisler, Daniel Sturm, et al. (2023). \textit{Adversarial Training for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zhang2020jrt}
Xiang Zhang, and M. Zitnik (2020). \textit{GNNGuard: Defending Graph Neural Networks against Adversarial Attacks}. Neural Information Processing Systems.

\bibitem{alon2020fok}
Uri Alon, and Eran Yahav (2020). \textit{On the Bottleneck of Graph Neural Networks and its Practical Implications}. International Conference on Learning Representations.

\bibitem{zhu2021zc3}
Meiqi Zhu, Xiao Wang, C. Shi, et al. (2021). \textit{Interpreting and Unifying Graph Neural Networks with An Optimization Framework}. The Web Conference.

\bibitem{zou2021qkz}
Xu Zou, Qinkai Zheng, Yuxiao Dong, et al. (2021). \textit{TDGIA: Effective Injection Attacks on Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{xu2019l8n}
Kaidi Xu, Hongge Chen, Sijia Liu, et al. (2019). \textit{Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective}. International Joint Conference on Artificial Intelligence.

\bibitem{xia20247w9}
Donglin Xia, Xiao Wang, Nian Liu, et al. (2024). \textit{Learning Invariant Representations of Graph Neural Networks via Cluster Generalization}. Neural Information Processing Systems.

\bibitem{wu2020dc8}
Shiwen Wu, Fei Sun, Fei Sun, et al. (2020). \textit{Graph Neural Networks in Recommender Systems: A Survey}. ACM Computing Surveys.

\bibitem{bianchi20239ee}
F. Bianchi, and Veronica Lachi (2023). \textit{The expressive power of pooling in Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{vu2020zkj}
Minh N. Vu, and M. Thai (2020). \textit{PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{gao20213kp}
Chen Gao, Yu Zheng, Nian Li, et al. (2021). \textit{A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions}. Trans. Recomm. Syst..

\bibitem{bessadok2021bfy}
Alaa Bessadok, M. Mahjoub, and I. Rekik (2021). \textit{Graph Neural Networks in Network Neuroscience}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{wang20214ku}
Xiao Wang, Hongrui Liu, Chuan Shi, et al. (2021). \textit{Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration}. Neural Information Processing Systems.

\bibitem{geisler2024wli}
Simon Geisler, Arthur Kosmala, Daniel Herbst, et al. (2024). \textit{Spatio-Spectral Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zeng20237gv}
DingYi Zeng, Wanlong Liu, Wenyu Chen, et al. (2023). \textit{Substructure Aware Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{jin2020dh4}
Wei Jin, Yao Ma, Xiaorui Liu, et al. (2020). \textit{Graph Structure Learning for Robust Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{dai2020p5t}
Enyan Dai, and Suhang Wang (2020). \textit{Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information}. Web Search and Data Mining.

\bibitem{klicpera20215fk}
Johannes Klicpera, Florian Becker, and Stephan Gunnemann (2021). \textit{GemNet: Universal Directional Graph Neural Networks for Molecules}. Neural Information Processing Systems.

\bibitem{dwivedi2021af0}
Vijay Prakash Dwivedi, A. Luu, T. Laurent, et al. (2021). \textit{Graph Neural Networks with Learnable Structural and Positional Representations}. International Conference on Learning Representations.

\bibitem{feng20225sa}
Jiarui Feng, Yixin Chen, Fuhai Li, et al. (2022). \textit{How Powerful are K-hop Message Passing Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{satorras2021pzl}
Victor Garcia Satorras, Emiel Hoogeboom, and M. Welling (2021). \textit{E(n) Equivariant Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{mao202313j}
Haitao Mao, Zhikai Chen, Wei Jin, et al. (2023). \textit{Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?}. Neural Information Processing Systems.

\bibitem{zgner2019bbi}
Daniel Zügner, and Stephan Günnemann (2019). \textit{Adversarial Attacks on Graph Neural Networks via Meta Learning}. International Conference on Learning Representations.

\bibitem{yuan2020fnk}
Hao Yuan, Haiyang Yu, Shurui Gui, et al. (2020). \textit{Explainability in Graph Neural Networks: A Taxonomic Survey}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{finkelshtein202301z}
Ben Finkelshtein, Xingyue Huang, Michael M. Bronstein, et al. (2023). \textit{Cooperative Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{lucic2021p70}
Ana Lucic, Maartje ter Hoeve, Gabriele Tolomei, et al. (2021). \textit{CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks}. International Conference on Artificial Intelligence and Statistics.

\bibitem{zheng2022qxr}
Xin Zheng, Yixin Liu, Shirui Pan, et al. (2022). \textit{Graph Neural Networks for Graphs with Heterophily: A Survey}. arXiv.org.

\bibitem{dai2023tuj}
Enyan Dai, M. Lin, Xiang Zhang, et al. (2023). \textit{Unnoticeable Backdoor Attacks on Graph Neural Networks}. The Web Conference.

\bibitem{jin2023e18}
G. Jin, Yuxuan Liang, Yuchen Fang, et al. (2023). \textit{Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ying20189jc}
Rex Ying, Ruining He, Kaifeng Chen, et al. (2018). \textit{Graph Convolutional Neural Networks for Web-Scale Recommender Systems}. Knowledge Discovery and Data Mining.

\bibitem{hu2020u8o}
Ziniu Hu, Yuxiao Dong, Kuansan Wang, et al. (2020). \textit{GPT-GNN: Generative Pre-Training of Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{luan202272y}
Sitao Luan, Chenqing Hua, Qincheng Lu, et al. (2022). \textit{Revisiting Heterophily For Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{klicpera20186xu}
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann (2018). \textit{Predict then Propagate: Graph Neural Networks meet Personalized PageRank}. International Conference on Learning Representations.

\bibitem{chen2019s47}
Deli Chen, Yankai Lin, Wei Li, et al. (2019). \textit{Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2022531}
Yu Wang, Yuying Zhao, Yushun Dong, et al. (2022). \textit{Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage}. Knowledge Discovery and Data Mining.

\bibitem{zhou20213lg}
Kaixiong Zhou, Xiao Huang, D. Zha, et al. (2021). \textit{Dirichlet Energy Constrained Learning for Deep Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{jegelka20222lq}
S. Jegelka (2022). \textit{Theory of Graph Neural Networks: Representation and Learning}. arXiv.org.

\bibitem{jin2021pf0}
Wei Jin, Lingxiao Zhao, Shichang Zhang, et al. (2021). \textit{Graph Condensation for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{geisler2021dcq}
Simon Geisler, Tobias Schmidt, Hakan cSirin, et al. (2021). \textit{Robustness of Graph Neural Networks at Scale}. Neural Information Processing Systems.

\bibitem{wu20193b0}
Zonghan Wu, Shirui Pan, Fengwen Chen, et al. (2019). \textit{A Comprehensive Survey on Graph Neural Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{xu2018c8q}
Keyulu Xu, Weihua Hu, J. Leskovec, et al. (2018). \textit{How Powerful are Graph Neural Networks?}. International Conference on Learning Representations.

\bibitem{zhou20188n6}
Jie Zhou, Ganqu Cui, Zhengyan Zhang, et al. (2018). \textit{Graph Neural Networks: A Review of Methods and Applications}. AI Open.

\bibitem{batzner2021t07}
Simon L. Batzner, Albert Musaelian, Lixin Sun, et al. (2021). \textit{E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials}. Nature Communications.

\bibitem{sarlin20198a6}
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, et al. (2019). \textit{SuperGlue: Learning Feature Matching With Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{wu2020hi3}
Zonghan Wu, Shirui Pan, Guodong Long, et al. (2020). \textit{Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{wu2018t43}
Shu Wu, Yuyuan Tang, Yanqiao Zhu, et al. (2018). \textit{Session-based Recommendation with Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020c3j}
Jiong Zhu, Yujun Yan, Lingxiao Zhao, et al. (2020). \textit{Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs}. Neural Information Processing Systems.

\bibitem{wang2019t4a}
Minjie Wang, Da Zheng, Zihao Ye, et al. (2019). \textit{Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.}. Unpublished manuscript.

\bibitem{li2020fil}
Mengzhang Li, and Zhanxing Zhu (2020). \textit{Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting}. AAAI Conference on Artificial Intelligence.

\bibitem{satorras20174cv}
Victor Garcia Satorras, and Joan Bruna (2017). \textit{Few-Shot Learning with Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{zhou20195xo}
Yaqin Zhou, Shangqing Liu, J. Siow, et al. (2019). \textit{Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{oono2019usb}
Kenta Oono, and Taiji Suzuki (2019). \textit{Graph Neural Networks Exponentially Lose Expressive Power for Node Classification}. International Conference on Learning Representations.

\bibitem{shi2019vl4}
Lei Shi, Yifan Zhang, Jian Cheng, et al. (2019). \textit{Skeleton-Based Action Recognition With Directed Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{wu20221la}
Zhanghao Wu, Paras Jain, Matthew A. Wright, et al. (2022). \textit{Representing Long-Range Context for Graph Neural Networks with Global Attention}. Neural Information Processing Systems.

\bibitem{wang2020khd}
Ziyang Wang, Wei Wei, G. Cong, et al. (2020). \textit{Global Context Enhanced Graph Neural Networks for Session-based Recommendation}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{wang2019vol}
Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, et al. (2019). \textit{Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems}. Knowledge Discovery and Data Mining.

\bibitem{zhong2019kka}
Peixiang Zhong, Di Wang, and C. Miao (2019). \textit{EEG-Based Emotion Recognition Using Regularized Graph Neural Networks}. IEEE Transactions on Affective Computing.

\bibitem{zhao2021po9}
Tianxiang Zhao, Xiang Zhang, and Suhang Wang (2021). \textit{GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks}. Web Search and Data Mining.

\bibitem{lv20219al}
Qingsong Lv, Ming Ding, Qiang Liu, et al. (2021). \textit{Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks}. Knowledge Discovery and Data Mining.

\bibitem{yu201969a}
Yue Yu, Jie Chen, Tian Gao, et al. (2019). \textit{DAG-GNN: DAG Structure Learning with Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{tang2022g66}
Jianheng Tang, Jiajin Li, Zi-Chao Gao, et al. (2022). \textit{Rethinking Graph Neural Networks for Anomaly Detection}. International Conference on Machine Learning.

\bibitem{zhao2021lls}
Jianan Zhao, Xiao Wang, C. Shi, et al. (2021). \textit{Heterogeneous Graph Structure Learning for Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{keisler2022t7p}
R. Keisler (2022). \textit{Forecasting Global Weather with Graph Neural Networks}. arXiv.org.

\bibitem{li2020mk1}
Maosen Li, Siheng Chen, Yangheng Zhao, et al. (2020). \textit{Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction}. Computer Vision and Pattern Recognition.

\bibitem{wu2022ptq}
Lingfei Wu, P. Cui, Jian Pei, et al. (2022). \textit{Graph Neural Networks: Foundation, Frontiers and Applications}. Knowledge Discovery and Data Mining.

\bibitem{liu2021qyl}
Zhenguang Liu, Peng Qian, Xiaoyang Wang, et al. (2021). \textit{Combining Graph Neural Networks With Expert Knowledge for Smart Contract Vulnerability Detection}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang20211dl}
Hengrui Zhang, Qitian Wu, Junchi Yan, et al. (2021). \textit{From Canonical Correlation Analysis to Self-supervised Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{shen202037i}
Yifei Shen, Yuanming Shi, Jun Zhang, et al. (2020). \textit{Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis}. IEEE Journal on Selected Areas in Communications.

\bibitem{zhang2020tdy}
Yufeng Zhang, Xueli Yu, Zeyu Cui, et al. (2020). \textit{Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang20209zd}
Qian Huang, Horace He, Abhay Singh, et al. (2020). \textit{Combining Label Propagation and Simple Models Out-performs Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{schaefer2022rsz}
S. Schaefer, Daniel Gehrig, and D. Scaramuzza (2022). \textit{AEGNN: Asynchronous Event-based Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{chen20201cf}
Tianwen Chen, and R. C. Wong (2020). \textit{Handling Information Loss of Graph Neural Networks for Session-based Recommendation}. Knowledge Discovery and Data Mining.

\bibitem{shen2022gcz}
Yifei Shen, Jun Zhang, Shenghui Song, et al. (2022). \textit{Graph Neural Networks for Wireless Communications: From Theory to Practice}. IEEE Transactions on Wireless Communications.

\bibitem{sharma2022liz}
Kartik Sharma, Yeon-Chang Lee, S. Nambi, et al. (2022). \textit{A Survey of Graph Neural Networks for Social Recommender Systems}. ACM Computing Surveys.

\bibitem{chen2021x8i}
Tianlong Chen, Yongduo Sui, Xuxi Chen, et al. (2021). \textit{A Unified Lottery Ticket Hypothesis for Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{chen2022ifd}
Cen Chen, Kenli Li, Wei Wei, et al. (2022). \textit{Hierarchical Graph Neural Networks for Few-Shot Learning}. IEEE transactions on circuits and systems for video technology (Print).

\bibitem{li2022hw4}
Jiachen Li, Siheng Chen, Xiaoyong Pan, et al. (2022). \textit{Cell clustering for spatial transcriptomics data with graph neural networks}. Nature Computational Science.

\bibitem{yun2022s4i}
Seongjun Yun, Seoyoon Kim, Junhyun Lee, et al. (2022). \textit{Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction}. Neural Information Processing Systems.

\bibitem{wijesinghe20225ms}
Asiri Wijesinghe, and Qing Wang (2022). \textit{A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"}. International Conference on Learning Representations.

\bibitem{cini20213l6}
Andrea Cini, Ivan Marisca, and C. Alippi (2021). \textit{Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{wu2023zm5}
Lingfei Wu, Yu Chen, Kai Shen, et al. (2023). \textit{Graph Neural Networks for Natural Language Processing: A Survey}. Found. Trends Mach. Learn..

\bibitem{li2022a34}
Tianfu Li, Zheng Zhou, Sinan Li, et al. (2022). \textit{The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study}. Mechanical systems and signal processing.

\bibitem{velickovic2023p4r}
Petar Velickovic (2023). \textit{Everything is Connected: Graph Neural Networks}. Current Opinion in Structural Biology.

\bibitem{jiang2020gaq}
Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, et al. (2020). \textit{Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models}. Journal of Cheminformatics.

\bibitem{sun2023vsl}
Xiangguo Sun, Hongtao Cheng, Jia Li, et al. (2023). \textit{All in One: Multi-Task Prompting for Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{zhang2021f18}
Xiao-Meng Zhang, Li Liang, Lin Liu, et al. (2021). \textit{Graph Neural Networks and Their Current Applications in Bioinformatics}. Frontiers in Genetics.

\bibitem{bojchevski2020c51}
Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, et al. (2020). \textit{Scaling Graph Neural Networks with Approximate PageRank}. Knowledge Discovery and Data Mining.

\bibitem{xia2023bpu}
Jun Xia, Chengshuai Zhao, Bozhen Hu, et al. (2023). \textit{Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules}. International Conference on Learning Representations.

\bibitem{rahmani2023kh4}
Saeed Rahmani, Asiye Baghbani, N. Bouguila, et al. (2023). \textit{Graph Neural Networks for Intelligent Transportation Systems: A Survey}. IEEE transactions on intelligent transportation systems (Print).

\bibitem{chen2024gbe}
Hao Chen, Yuan-Qi Bei, Qijie Shen, et al. (2024). \textit{Macro Graph Neural Networks for Online Billion-Scale Recommender Systems}. The Web Conference.

\bibitem{liao202120x}
Wenlong Liao, B. Bak‐Jensen, J. Pillai, et al. (2021). \textit{A Review of Graph Neural Networks and Their Applications in Power Systems}. Journal of Modern Power Systems and Clean Energy.

\bibitem{hin2022g19}
David Hin, Andrey Kan, Huaming Chen, et al. (2022). \textit{LineVD: Statement-level Vulnerability Detection using Graph Neural Networks}. IEEE Working Conference on Mining Software Repositories.

\bibitem{tsitsulin20209pl}
Anton Tsitsulin, John Palowitch, Bryan Perozzi, et al. (2020). \textit{Graph Clustering with Graph Neural Networks}. Journal of machine learning research.

\bibitem{fung20212kw}
Victor Fung, Jiaxin Zhang, Eric Juarez, et al. (2021). \textit{Benchmarking graph neural networks for materials chemistry}. npj Computational Materials.

\bibitem{wang2021mxw}
Yongxin Wang, Kris Kitani, and Xinshuo Weng (2021). \textit{Joint Object Detection and Multi-Object Tracking with Graph Neural Networks}. IEEE International Conference on Robotics and Automation.

\bibitem{wang2020nbg}
Danqing Wang, Pengfei Liu, Y. Zheng, et al. (2020). \textit{Heterogeneous Graph Neural Networks for Extractive Document Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{jha2022cj8}
Kanchan Jha, S. Saha, and Hiteshi Singh (2022). \textit{Prediction of protein–protein interaction using graph neural networks}. Scientific Reports.

\bibitem{schuetz2021cod}
M. Schuetz, J. K. Brubaker, and H. Katzgraber (2021). \textit{Combinatorial optimization with physics-inspired graph neural networks}. Nature Machine Intelligence.

\bibitem{shen2021sbk}
Meng Shen, Jinpeng Zhang, Liehuang Zhu, et al. (2021). \textit{Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks}. IEEE Transactions on Information Forensics and Security.

\bibitem{bo2023rwt}
Deyu Bo, Chuan Shi, Lele Wang, et al. (2023). \textit{Specformer: Spectral Graph Neural Networks Meet Transformers}. International Conference on Learning Representations.

\bibitem{zhang20212ke}
Mengqi Zhang, Shu Wu, Xueli Yu, et al. (2021). \textit{Dynamic Graph Neural Networks for Sequential Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{wei20246l2}
Jianjun Wei, Yue Liu, Xin Huang, et al. (2024). \textit{Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks}. 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA).

\bibitem{yu2020u32}
Feng Yu, Yanqiao Zhu, Q. Liu, et al. (2020). \textit{TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{he2021x8v}
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, et al. (2021). \textit{FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks}. arXiv.org.

\bibitem{wu20210h4}
Yulei Wu, Hongning Dai, and Haina Tang (2021). \textit{Graph Neural Networks for Anomaly Detection in Industrial Internet of Things}. IEEE Internet of Things Journal.

\bibitem{kofinas2024t2b}
Miltiadis Kofinas, Boris Knyazev, Yan Zhang, et al. (2024). \textit{Graph Neural Networks for Learning Equivariant Representations of Neural Networks}. International Conference on Learning Representations.

\bibitem{li2021v1l}
Shuangli Li, Jingbo Zhou, Tong Xu, et al. (2021). \textit{Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity}. Knowledge Discovery and Data Mining.

\bibitem{balcilar2021di1}
M. Balcilar, G. Renton, P. Héroux, et al. (2021). \textit{Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective}. International Conference on Learning Representations.

\bibitem{zhang2020f4l}
Muhan Zhang, Pan Li, Yinglong Xia, et al. (2020). \textit{Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning}. Neural Information Processing Systems.

\bibitem{bilot20234ui}
Tristan Bilot, Nour El Madhoun, K. A. Agha, et al. (2023). \textit{Graph Neural Networks for Intrusion Detection: A Survey}. IEEE Access.

\bibitem{wu2023303}
Qitian Wu, Yiting Chen, Chenxiao Yang, et al. (2023). \textit{Energy-based Out-of-Distribution Detection for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{huang2021lpu}
P. Huang, Han-Hung Lee, Hwann-Tzong Chen, et al. (2021). \textit{Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation}. AAAI Conference on Artificial Intelligence.

\bibitem{suresh202191q}
Susheel Suresh, Vinith Budde, Jennifer Neville, et al. (2021). \textit{Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns}. Knowledge Discovery and Data Mining.

\bibitem{liu2022gcg}
R. Liu, and Han Yu (2022). \textit{Federated Graph Neural Networks: Overview, Techniques, and Challenges}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{wang202201n}
Lijing Wang, A. Adiga, Jiangzhuo Chen, et al. (2022). \textit{CausalGNN: Causal-Based Graph Neural Networks for Spatio-Temporal Epidemic Forecasting}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2021c3l}
Fan Zhou, and Chengtai Cao (2021). \textit{Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay}. AAAI Conference on Artificial Intelligence.

\bibitem{vasimuddin2021x7c}
Vasimuddin, Sanchit Misra, Guixiang Ma, et al. (2021). \textit{DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks}. International Conference for High Performance Computing, Networking, Storage and Analysis.

\bibitem{eliasof202189g}
Moshe Eliasof, E. Haber, and Eran Treister (2021). \textit{PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations}. Neural Information Processing Systems.

\bibitem{huang2023fk1}
Kexin Huang, Ying Jin, E. Candès, et al. (2023). \textit{Uncertainty Quantification over Graph with Conformalized Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{fey2021smn}
Matthias Fey, J. E. Lenssen, F. Weichert, et al. (2021). \textit{GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings}. International Conference on Machine Learning.

\bibitem{nguyen2021g12}
Van-Anh Nguyen, D. Q. Nguyen, Van Nguyen, et al. (2021). \textit{ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection}. 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion).

\bibitem{innan2023fa7}
Nouhaila Innan, Abhishek Sawaika, Ashim Dhor, et al. (2023). \textit{Financial Fraud Detection using Quantum Graph Neural Networks}. Quantum Machine Intelligence.

\bibitem{guo2022hu1}
Jia Guo, and Chenyang Yang (2022). \textit{Learning Power Allocation for Multi-Cell-Multi-User Systems With Heterogeneous Graph Neural Networks}. IEEE Transactions on Wireless Communications.

\bibitem{maurizi202293p}
M. Maurizi, Chao Gao, and F. Berto (2022). \textit{Predicting stress, strain and deformation fields in materials and structures with graph neural networks}. Scientific Reports.

\bibitem{ye20226hn}
Zi Ye, Y. J. Kumar, G. O. Sing, et al. (2022). \textit{A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs}. IEEE Access.

\bibitem{liu2021efj}
Zemin Liu, Trung-Kien Nguyen, and Yuan Fang (2021). \textit{Tail-GNN: Tail-Node Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{du2021kn9}
Lun Du, Xiaozhou Shi, Qiang Fu, et al. (2021). \textit{GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily}. The Web Conference.

\bibitem{xu20226vc}
Weizhi Xu, Jun Wu, Qiang Liu, et al. (2022). \textit{Evidence-aware Fake News Detection with Graph Neural Networks}. The Web Conference.

\bibitem{wang2023a6u}
Shaocong Wang, Yi Li, Dingchen Wang, et al. (2023). \textit{Echo state graph neural networks with analogue random resistive memory arrays}. Nature Machine Intelligence.

\bibitem{bing2022oka}
Rui Bing, Guan Yuan, Mu Zhu, et al. (2022). \textit{Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications}. Artificial Intelligence Review.

\bibitem{lyu2023ao0}
Ziyu Lyu, Yue Wu, Junjie Lai, et al. (2023). \textit{Knowledge Enhanced Graph Neural Networks for Explainable Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{peng2021gbb}
Hao Peng, Ruitong Zhang, Yingtong Dou, et al. (2021). \textit{Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks}. ACM Trans. Inf. Syst..

\bibitem{xia2021s85}
Ying Xia, Chun-Qiu Xia, Xiaoyong Pan, et al. (2021). \textit{GraphBind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues}. Nucleic Acids Research.

\bibitem{feng2022914}
Aosong Feng, Chenyu You, Shiqiang Wang, et al. (2022). \textit{KerGNNs: Interpretable Graph Neural Networks with Graph Kernels}. AAAI Conference on Artificial Intelligence.

\bibitem{paper2022mw4}
Unknown Authors (2022). \textit{Graph Neural Networks: Foundations, Frontiers, and Applications}. Unpublished manuscript.

\bibitem{luan2021g2p}
Sitao Luan, Chenqing Hua, Qincheng Lu, et al. (2021). \textit{Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?}. arXiv.org.

\bibitem{waikhom20226fa}
Lilapati Waikhom, and Ripon Patgiri (2022). \textit{A survey of graph neural networks in various learning paradigms: methods, applications, and challenges}. Artificial Intelligence Review.

\bibitem{tang2021h2z}
Siyi Tang, Jared A. Dunnmon, Khaled Saab, et al. (2021). \textit{Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis}. International Conference on Learning Representations.

\bibitem{thost20211ln}
Veronika Thost, and Jie Chen (2021). \textit{Directed Acyclic Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{chai2022nf9}
Ziwei Chai, Siqi You, Yang Yang, et al. (2022). \textit{Can Abnormality be Detected by Graph Neural Networks?}. International Joint Conference on Artificial Intelligence.

\bibitem{sun20239ly}
Chengcheng Sun, Chenhao Li, Xiang Lin, et al. (2023). \textit{Attention-based graph neural networks: a survey}. Artificial Intelligence Review.

\bibitem{zhang2022atq}
Mengqi Zhang, Shu Wu, Meng Gao, et al. (2022). \textit{Personalized Graph Neural Networks With Attention Mechanism for Session-Aware Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{munikoti2022k7d}
Sai Munikoti, D. Agarwal, L. Das, et al. (2022). \textit{Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{huoh2023i97}
Ting-Li Huoh, Yan Luo, Peilong Li, et al. (2023). \textit{Flow-Based Encrypted Network Traffic Classification With Graph Neural Networks}. IEEE Transactions on Network and Service Management.

\bibitem{han20227gn}
Jiaqi Han, Yu Rong, Tingyang Xu, et al. (2022). \textit{Geometrically Equivariant Graph Neural Networks: A Survey}. arXiv.org.

\bibitem{kim2022yql}
Hwan Kim, Byung Suk Lee, Won-Yong Shin, et al. (2022). \textit{Graph Anomaly Detection With Graph Neural Networks: Current Status and Challenges}. IEEE Access.

\bibitem{zhang2022uih}
Zeyang Zhang, Xin Wang, Ziwei Zhang, et al. (2022). \textit{Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift}. Neural Information Processing Systems.

\bibitem{zhou2022a3h}
Yang Zhou, Jiuhong Xiao, Yuee Zhou, et al. (2022). \textit{Multi-Robot Collaborative Perception With Graph Neural Networks}. IEEE Robotics and Automation Letters.

\bibitem{wu2023aqs}
Xinyi Wu, A. Ajorlou, Zihui Wu, et al. (2023). \textit{Demystifying Oversmoothing in Attention-Based Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{long2022l97}
Yahui Long, Min Wu, Yong Liu, et al. (2022). \textit{Pre-training graph neural networks for link prediction in biomedical networks}. Bioinform..

\bibitem{cini2022pjy}
Andrea Cini, Ivan Marisca, F. Bianchi, et al. (2022). \textit{Scalable Spatiotemporal Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023ann}
Zhen Zhang, Mohammed Haroon Dupty, Fan Wu, et al. (2023). \textit{Factor Graph Neural Networks}. Journal of machine learning research.

\bibitem{chang2023ex5}
Jianxin Chang, Chen Gao, Xiangnan He, et al. (2023). \textit{Bundle Recommendation and Generation With Graph Neural Networks}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{wang2023zr0}
J. Wang (2023). \textit{A survey on graph neural networks}. EAI Endorsed Trans. e Learn..

\bibitem{zhao2022fvg}
Xusheng Zhao, Jia Wu, Hao Peng, et al. (2022). \textit{Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis}. Neural Networks.

\bibitem{sahili2023f2x}
Zahraa Al Sahili, and M. Awad (2023). \textit{Spatio-Temporal Graph Neural Networks: A Survey}. arXiv.org.

\bibitem{levie2023c1s}
R. Levie (2023). \textit{A graphon-signal analysis of graph neural networks}. Neural Information Processing Systems.

\bibitem{wang2024nuq}
Pengcheng Wang, Linping Tao, Mingwei Tang, et al. (2024). \textit{Incorporating syntax and semantics with dual graph neural networks for aspect-level sentiment analysis}. Engineering applications of artificial intelligence.

\bibitem{dong2024dx0}
Hu Dong, Longjie Li, Dongwen Tian, et al. (2024). \textit{Dynamic link prediction by learning the representation of node-pair via graph neural networks}. Expert systems with applications.

\bibitem{zhao2024oyr}
Pengju Zhao, Wenjie Liao, Yuli Huang, et al. (2024). \textit{Beam layout design of shear wall structures based on graph neural networks}. Automation in Construction.

\bibitem{chen2024h2c}
Ming Chen, Yajian Jiang, Xiujuan Lei, et al. (2024). \textit{Drug-Target Interactions Prediction Based on Signed Heterogeneous Graph Neural Networks}. Chinese journal of electronics.

\bibitem{foroutan2024nhg}
P. Foroutan, and Salim Lahmiri (2024). \textit{Deep Learning-Based Spatial-Temporal Graph Neural Networks for Price Movement Classification in Crude Oil and Precious Metal Markets}. Machine Learning with Applications.

\bibitem{wander2024nnn}
Brook Wander, Muhammed Shuaibi, John R. Kitchin, et al. (2024). \textit{CatTSunami: Accelerating Transition State Energy Calculations with Pretrained Graph Neural Networks}. ACS Catalysis.

\bibitem{li20248gg}
Duantengchuan Li, Yuxuan Gao, Zhihao Wang, et al. (2024). \textit{Homogeneous graph neural networks for third-party library recommendation}. Information Processing & Management.

\bibitem{duan2024que}
Yifan Duan, Guibin Zhang, Shilong Wang, et al. (2024). \textit{CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks}. arXiv.org.

\bibitem{praveen202498y}
R. Praveen, Aktalina Torogeldieva, B. Saravanan, et al. (2024). \textit{Enhancing Intellectual Property Rights(IPR) Transparency with Blockchain and Dual Graph Neural Networks}. 2024 First International Conference on Software, Systems and Information Technology (SSITCON).

\bibitem{wang2024p88}
Huiwei Wang, Tianhua Liu, Ziyu Sheng, et al. (2024). \textit{Explanatory subgraph attacks against Graph Neural Networks}. Neural Networks.

\bibitem{jing2024az0}
Baoyu Jing, Dawei Zhou, Kan Ren, et al. (2024). \textit{Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024370}
Zhongjian Zhang, Xiao Wang, Huichi Zhou, et al. (2024). \textit{Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?}. Knowledge Discovery and Data Mining.

\bibitem{kanatsoulis2024l6i}
Charilaos I. Kanatsoulis, and Alejandro Ribeiro (2024). \textit{Counting Graph Substructures with Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{mishra2024v89}
Rajat Mishra, and S. Shridevi (2024). \textit{Knowledge graph driven medicine recommendation system using graph neural networks on longitudinal medical records}. Scientific Reports.

\bibitem{fang2024p34}
Zhenyao Fang, and Qimin Yan (2024). \textit{Towards accurate prediction of configurational disorder properties in materials using graph neural networks}. npj Computational Materials.

\bibitem{zhang202483k}
Jintu Zhang, Luigi Bonati, Enrico Trizio, et al. (2024). \textit{Descriptor-Free Collective Variables from Geometric Graph Neural Networks.}. Journal of Chemical Theory and Computation.

\bibitem{yin20241mx}
Nan Yin, Mengzhu Wang, Li Shen, et al. (2024). \textit{Continuous Spiking Graph Neural Networks}. arXiv.org.

\bibitem{yan20240up}
Liuxi Yan, and Yaoqun Xu (2024). \textit{XGBoost-Enhanced Graph Neural Networks: A New Architecture for Heterogeneous Tabular Data}. Applied Sciences.

\bibitem{shen2024exf}
Xu Shen, P. Lió, Lintao Yang, et al. (2024). \textit{Graph Rewiring and Preprocessing for Graph Neural Networks Based on Effective Resistance}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{manivannan2024830}
S. K. Manivannan, Venkatesh Kavididevi, D. Muthukumaran, et al. (2024). \textit{Graph Neural Networks for Resource Allocation Optimization in Healthcare Industry}. 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS).

\bibitem{he202455s}
Xingyang He (2024). \textit{Graph neural networks in recommender systems}. Applied and Computational Engineering.

\bibitem{zhao2024qw6}
Zhe Zhao, Pengkun Wang, Haibin Wen, et al. (2024). \textit{A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{yan2024ikq}
Yafeng Yan, Shuyao He, Zhou Yu, et al. (2024). \textit{Investigation of Customized Medical Decision Algorithms Utilizing Graph Neural Networks}. 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE).

\bibitem{xia2024xc9}
Zaishuo Xia, Han Yang, Binghui Wang, et al. (2024). \textit{GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations}. International Conference on Learning Representations.

\bibitem{zhou2024t2r}
Yicheng Zhou, P. Wang, Hao Dong, et al. (2024). \textit{Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{lu2024eu9}
Shengyao Lu, Keith G. Mills, Jiao He, et al. (2024). \textit{GOAt: Explaining Graph Neural Networks via Graph Output Attribution}. International Conference on Learning Representations.

\bibitem{wang2024cb8}
Zhiyang Wang, J. Cerviño, and Alejandro Ribeiro (2024). \textit{A Manifold Perspective on the Statistical Generalization of Graph Neural Networks}. arXiv.org.

\bibitem{li2024yyl}
Dilong Li, Chenghui Lu, Zi-xing Chen, et al. (2024). \textit{Graph Neural Networks in Point Clouds: A Survey}. Remote Sensing.

\bibitem{castroospina2024iy2}
A. Castro-Ospina, M. Solarte-Sanchez, L. Vega-Escobar, et al. (2024). \textit{Graph-Based Audio Classification Using Pre-Trained Models and Graph Neural Networks}. Italian National Conference on Sensors.

\bibitem{zhao2024g5p}
Tianyi Zhao, Jian Kang, and Lu Cheng (2024). \textit{Conformalized Link Prediction on Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{duan2024efz}
Wei Duan, Jie Lu, Yu Guang Wang, et al. (2024). \textit{Layer-diverse Negative Sampling for Graph Neural Networks}. Trans. Mach. Learn. Res..

\bibitem{luo2024h2k}
Xuexiong Luo, Jia Wu, Jian Yang, et al. (2024). \textit{Graph Neural Networks for Brain Graph Learning: A Survey}. International Joint Conference on Artificial Intelligence.

\bibitem{carlo2024a3g}
Alessandro De Carlo, D. Ronchi, Marco Piastra, et al. (2024). \textit{Predicting ADMET Properties from Molecule SMILE: A Bottom-Up Approach Using Attention-Based Graph Neural Networks}. Pharmaceutics.

\bibitem{zandi2024dgs}
Sahab Zandi, Kamesh Korangi, Mar'ia 'Oskarsd'ottir, et al. (2024). \textit{Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction}. European Journal of Operational Research.

\bibitem{zhao2024aer}
Haihong Zhao, Bo Yang, Jiaxu Cui, et al. (2024). \textit{Effective Fault Scenario Identification for Communication Networks via Knowledge-Enhanced Graph Neural Networks}. IEEE Transactions on Mobile Computing.

\bibitem{yao2024pyk}
Rufan Yao, Zhenhua Shen, Xinyi Xu, et al. (2024). \textit{Knowledge mapping of graph neural networks for drug discovery: a bibliometric and visualized analysis}. Frontiers in Pharmacology.

\bibitem{vinh20243q3}
Tuan Vinh, Loc Nguyen, Quang H. Trinh, et al. (2024). \textit{Predicting Cardiotoxicity of Molecules Using Attention-Based Graph Neural Networks}. Journal of Chemical Information and Modeling.

\bibitem{ashraf202443e}
Inaam Ashraf, Janine Strotherm, L. Hermes, et al. (2024). \textit{Physics-Informed Graph Neural Networks for Water Distribution Systems}. AAAI Conference on Artificial Intelligence.

\bibitem{smith2024q8n}
Zachary Smith, Michael Strobel, Bodhi P. Vani, et al. (2024). \textit{Graph Attention Site Prediction (GrASP): Identifying Druggable Binding Sites Using Graph Neural Networks with Attention}. Journal of Chemical Information and Modeling.

\bibitem{abadal2024w7e}
S. Abadal, Pablo Galván, Alberto Mármol, et al. (2024). \textit{Graph neural networks for electroencephalogram analysis: Alzheimer's disease and epilepsy use cases}. Neural Networks.

\bibitem{pflueger2024qi6}
Maximilian Pflueger, David J. Tena Cucala, and Egor V. Kostylev (2024). \textit{Recurrent Graph Neural Networks and Their Connections to Bisimulation and Logic}. AAAI Conference on Artificial Intelligence.

\bibitem{mohammadi202476q}
H. Mohammadi, and Waldemar Karwowski (2024). \textit{Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions}. Brain Science.

\bibitem{sui2024xh9}
Yongduo Sui, Xiang Wang, Tianlong Chen, et al. (2024). \textit{Inductive Lottery Ticket Learning for Graph Neural Networks}. Journal of Computational Science and Technology.

\bibitem{peng2024t2s}
Jie Peng, Runlin Lei, and Zhewei Wei (2024). \textit{Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep Graph Neural Networks}. International Conference on Information and Knowledge Management.

\bibitem{zhao2024e2x}
Shan Zhao, Ioannis Prapas, Ilektra Karasante, et al. (2024). \textit{Causal Graph Neural Networks for Wildfire Danger Prediction}. arXiv.org.

\bibitem{nabian2024vto}
M. A. Nabian (2024). \textit{X-MeshGraphNet: Scalable Multi-Scale Graph Neural Networks for Physics Simulation}. arXiv.org.

\bibitem{cen2024md8}
Jiacheng Cen, Anyi Li, Ning Lin, et al. (2024). \textit{Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?}. Neural Information Processing Systems.

\bibitem{yang2024vy7}
Yachao Yang, Yanfeng Sun, Shaofan Wang, et al. (2024). \textit{Graph Neural Networks with Soft Association between Topology and Attribute}. AAAI Conference on Artificial Intelligence.

\bibitem{li2024gue}
Youjia Li, Vishu Gupta, Muhammed Nur Talha Kilic, et al. (2024). \textit{Hybrid-LLM-GNN: Integrating Large Language Models and Graph Neural Networks for Enhanced Materials Property Prediction}. Digital Discovery.

\bibitem{guo2024zoe}
Zhenbei Guo, Fuliang Li, Jiaxing Shen, et al. (2024). \textit{ConfigReco: Network Configuration Recommendation With Graph Neural Networks}. IEEE Network.

\bibitem{gnanabaskaran20245dg}
A. Gnanabaskaran, K. Bharathi, S. P. Nandakumar, et al. (2024). \textit{Enhanced Drug-Drug Interaction Prediction with Graph Neural Networks and SVM}. 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS).

\bibitem{wang20245it}
Beibei Wang, Bo Jiang, and Chris H. Q. Ding (2024). \textit{FL-GNNs: Robust Network Representation via Feature Learning Guided Graph Neural Networks}. IEEE Transactions on Network Science and Engineering.

\bibitem{abode2024m4z}
Daniel Abode, Ramoni O. Adeogun, and Gilberto Berardinelli (2024). \textit{Power Control for 6G In-Factory Subnetworks With Partial Channel Information Using Graph Neural Networks}. IEEE Open Journal of the Communications Society.

\bibitem{zhao20244un}
Tianxiang Zhao, Xiang Zhang, and Suhang Wang (2024). \textit{Disambiguated Node Classification with Graph Neural Networks}. The Web Conference.

\bibitem{hausleitner2024vw0}
Christian Hausleitner, Heimo Mueller, Andreas Holzinger, et al. (2024). \textit{Collaborative weighting in federated graph neural networks for disease classification with the human-in-the-loop}. Scientific Reports.

\bibitem{zhao2024g7h}
Shan Zhao, Zhaiyu Chen, Zhitong Xiong, et al. (2024). \textit{Beyond Grid Data: Exploring graph neural networks for Earth observation}. IEEE Geoscience and Remote Sensing Magazine.

\bibitem{rusch2024fgp}
T. Konstantin Rusch, Nathan Kirk, M. Bronstein, et al. (2024). \textit{Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks}. Proceedings of the National Academy of Sciences of the United States of America.

\bibitem{wang2024htw}
Fali Wang, Tianxiang Zhao, and Suhang Wang (2024). \textit{Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels}. Web Search and Data Mining.

\bibitem{liu2024sbb}
Bingyao Liu, Iris Li, Jianhua Yao, et al. (2024). \textit{Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).

\bibitem{fang2024zd6}
Zhenyao Fang, and Qimin Yan (2024). \textit{Leveraging Persistent Homology Features for Accurate Defect Formation Energy Predictions via Graph Neural Networks}. Chemistry of Materials.

\bibitem{benedikt2024153}
Michael Benedikt, Chia-Hsuan Lu, Boris Motik, et al. (2024). \textit{Decidability of Graph Neural Networks via Logical Characterizations}. International Colloquium on Automata, Languages and Programming.

\bibitem{zhang20241k0}
Yuelin Zhang, Jiacheng Cen, Jiaqi Han, et al. (2024). \textit{Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning}. International Conference on Machine Learning.

\bibitem{graziani2024lgd}
Caterina Graziani, Tamara Drucks, Fabian Jogl, et al. (2024). \textit{The Expressive Power of Path-Based Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{shi2024g4z}
Dai Shi, Andi Han, Lequan Lin, et al. (2024). \textit{Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks}. International Journal of Machine Learning and Cybernetics.

\bibitem{yuan2024b8b}
Jiang Yuan, Shanxiong Chen, Bofeng Mo, et al. (2024). \textit{R-GNN: recurrent graph neural networks for font classification of oracle bone inscriptions}. Heritage Science.

\bibitem{wang2024kx8}
Haitao Wang, Zelin Liu, Mingjun Li, et al. (2024). \textit{A Gearbox Fault Diagnosis Method Based on Graph Neural Networks and Markov Transform Fields}. IEEE Sensors Journal.

\bibitem{abuhantash202458c}
Ferial Abuhantash, Mohd Khalil Abu Hantash, and Aamna AlShehhi (2024). \textit{Comorbidity-based framework for Alzheimer’s disease classification using graph neural networks}. Scientific Reports.

\bibitem{abbahaddou2024bq2}
Yassine Abbahaddou, Sofiane Ennadir, J. Lutzeyer, et al. (2024). \textit{Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks}. International Conference on Learning Representations.

\bibitem{huang2024tdd}
Renhong Huang, Jiarong Xu, Xin Jiang, et al. (2024). \textit{Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{jiang202448s}
Yue Jiang, Changkong Zhou, Vikas Garg, et al. (2024). \textit{Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces}. International Conference on Human Factors in Computing Systems.

\bibitem{wang20246bq}
Bin Wang, Yadong Xu, Manyi Wang, et al. (2024). \textit{Gear Fault Diagnosis Method Based on the Optimized Graph Neural Networks}. IEEE Transactions on Instrumentation and Measurement.

\bibitem{silva2024trs}
Thiago H. Silva, and Daniel Silver (2024). \textit{Using graph neural networks to predict local culture}. Environment and Planning B Urban Analytics and City Science.

\bibitem{zhang2024ctj}
Xin Zhang, Zhen Xu, Yue Liu, et al. (2024). \textit{Robust Graph Neural Networks for Stability Analysis in Dynamic Networks}. 2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE).

\bibitem{sun2024ztz}
Mengfang Sun, Wenying Sun, Ying Sun, et al. (2024). \textit{Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis}. 2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE).

\bibitem{zeng2024fpp}
Xin Zeng, Fan-Fang Meng, Meng-Liang Wen, et al. (2024). \textit{GNNGL-PPI: multi-category prediction of protein-protein interactions using graph neural networks based on global graphs and local subgraphs}. BMC Genomics.

\bibitem{chen20241tu}
Ziang Chen, Xiaohan Chen, Jialin Liu, et al. (2024). \textit{Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs}. arXiv.org.

\bibitem{fujita2024crj}
Takaaki Fujita (2024). \textit{Superhypergraph Neural Networks and Plithogenic Graph Neural Networks: Theoretical Foundations}. arXiv.org.

\bibitem{saleh2024d2a}
Mahdi Saleh, Michael Sommersperger, N. Navab, et al. (2024). \textit{Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact}. IEEE International Conference on Robotics and Automation.

\bibitem{aburidi2024023}
Mohammed Aburidi, and Roummel F. Marcia (2024). \textit{Topological Adversarial Attacks on Graph Neural Networks Via Projected Meta Learning}. IEEE Conference on Evolving and Adaptive Intelligent Systems.

\bibitem{wang2024481}
Zhonghao Wang, Danyu Sun, Sheng Zhou, et al. (2024). \textit{NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise}. Neural Information Processing Systems.

\bibitem{horck2024a8s}
Rostislav Horcík, and Gustav Sír (2024). \textit{Expressiveness of Graph Neural Networks in Planning Domains}. International Conference on Automated Planning and Scheduling.

\bibitem{sun2024pix}
Jianshan Sun, Suyuan Mei, Kun Yuan, et al. (2024). \textit{Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2024r82}
Langsha Li, Feng Qiang, and Li Ma (2024). \textit{Advancing Cybersecurity: Graph Neural Networks in Threat Intelligence Knowledge Graphs}. International Conference on Algorithms, Software Engineering, and Network Security.

\bibitem{luo20240ot}
Renqiang Luo, Huafei Huang, Shuo Yu, et al. (2024). \textit{FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{li202492k}
Shouheng Li, F. Geerts, Dongwoo Kim, et al. (2024). \textit{Towards Bridging Generalization and Expressivity of Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{liao20249wq}
Yidong Liao, Xiao-Ming Zhang, and Chris Ferrie (2024). \textit{Graph Neural Networks on Quantum Computers}. arXiv.org.

\bibitem{wang2024ged}
Yufeng Wang, and Charith Mendis (2024). \textit{TGLite: A Lightweight Programming Framework for Continuous-Time Temporal Graph Neural Networks}. International Conference on Architectural Support for Programming Languages and Operating Systems.

\bibitem{liu20245da}
Ping Liu, Haichao Wei, Xiaochen Hou, et al. (2024). \textit{LinkSAGE: Optimizing Job Matching Using Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{varghese2024ygs}
Alan John Varghese, Zhen Zhang, and G. Karniadakis (2024). \textit{SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification}. Neural Networks.

\bibitem{dinverno2024vkw}
Giuseppe Alessio D’Inverno, M. Bianchini, and F. Scarselli (2024). \textit{VC dimension of Graph Neural Networks with Pfaffian activation functions}. Neural Networks.

\end{thebibliography}

\end{document}