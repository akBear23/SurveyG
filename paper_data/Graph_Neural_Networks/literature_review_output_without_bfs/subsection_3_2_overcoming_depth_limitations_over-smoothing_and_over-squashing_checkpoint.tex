\subsection{Overcoming Depth Limitations: Over-smoothing and Over-squashing}

The aspiration to develop deeper Graph Neural Networks (GNNs) capable of discerning intricate, long-range dependencies within graph-structured data is fundamentally impeded by two critical and intertwined challenges: over-smoothing and over-squashing. Over-smoothing manifests when iterative message passing causes node representations to converge towards indistinguishable values, leading to a profound loss of discriminative power and an inability to differentiate between nodes \cite{rusch2023xev, cai2020k4b}. Complementary to this, over-squashing describes an information bottleneck where the exponentially expanding receptive field of a node is compressed into a fixed-size vector, resulting in the significant loss of crucial information from distant nodes \cite{alon2020fok}. Effectively addressing these limitations is paramount for extending the practical utility and expressive capabilities of GNNs.

Early investigations into deep GNNs quickly identified the over-smoothing phenomenon. \cite{klicpera20186xu} made a seminal contribution by proposing to decouple the neural network's feature transformation from the graph's propagation mechanism. Their Personalized Propagation of Neural Predictions (PPNP) and its efficient approximation (APPNP) leverage personalized PageRank to facilitate deep propagation steps while preserving locality through a "teleport probability." This strategy effectively mitigates over-smoothing without increasing the neural network's parameter count or explicit depth. Building on this decoupling principle, \cite{liu2020w3t} further re-evaluated over-smoothing, suggesting that for moderate depths, performance degradation primarily stems from the entanglement of representation transformation and propagation. Their Deep Adaptive Graph Neural Network (DAGNN) demonstrated that explicit decoupling enables significantly deeper GNNs without performance loss, with severe over-smoothing becoming critical only at extreme depths.

The theoretical understanding of over-smoothing has been rigorously advanced through the lens of Dirichlet energy. \cite{cai2020k4b} provided a foundational theoretical framework, demonstrating that the Dirichlet energy of node embeddings exponentially converges to zero with increasing layers, directly signifying a loss of discriminative power. This concept was further formalized by \cite{rusch2023xev}, who introduced an axiomatic definition of over-smoothing based on the layer-wise exponential convergence of a node-similarity measure like Dirichlet energy. Their comprehensive survey \cite{rusch2023xev} also critically evaluates various mitigation strategies. Leveraging these theoretical insights, \cite{zhou20213lg} proposed a "Dirichlet energy constrained learning" principle, guiding the training of deep GNNs. Their Energetic Graph Neural Network (EGNN) incorporates orthogonal weight control, lower-bounded residual connections, and Shifted ReLU (SReLU) activation to maintain Dirichlet energy within an optimal range, enabling GNNs to effectively scale up to 64 layers. Similarly, \cite{bianchi20194ea} introduced Graph Neural Networks with Convolutional ARMA Filters, which employ recursive updates with skip connections to capture global dependencies and mitigate over-smoothing by offering a more flexible frequency response than traditional polynomial filters. In a different vein, \cite{eliasof202189g} proposed PDE-GCN, a family of architectures motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, allowing for the control of over-smoothing by design and offering a theoretical explanation for their behavior.

While over-smoothing primarily concerns feature homogenization, \cite{alon2020fok} introduced "over-squashing" as a distinct bottleneck, particularly for tasks necessitating long-range interactions. This phenomenon arises from the inherent limitation of fixed-size message aggregation, which struggles to compress the exponentially growing information from a node's receptive field without significant loss. To address this, \cite{zeng2022jhz} proposed a novel principle to decouple the *depth* and *scope* of GNNs. Their SHADOW-GNN applies deep GNNs on shallow, localized subgraphs, theoretically proving it prevents both over-smoothing and neighbor explosion, thereby enabling more expressive and scalable models. Taking a different approach, \cite{finkelshtein202301z} introduced Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose communication actions (listen, broadcast, isolate) at each layer. This adaptive, node-specific information flow inherently mitigates over-squashing and over-smoothing by allowing for more nuanced and efficient long-range communication. Furthermore, \cite{wu20221la}'s GraphTrans leverages Transformer-based self-attention with a novel readout mechanism to learn long-range pairwise relationships, offering a permutation-invariant approach that implicitly addresses the challenge of capturing distant node interactions often lost to over-squashing.

Beyond architectural modifications, novel non-convolutional designs and alternative learning paradigms offer robust pathways to overcome depth limitations and address both challenges. \cite{wang2024oi8} presented Random Walk with Unifying Memory (RUM), a fundamentally non-convolutional GNN that processes information via random walks and RNNs. RUM is theoretically shown to jointly remedy limited expressiveness, over-smoothing, and over-squashing by maintaining non-diminishing Dirichlet energy and slower inter-node Jacobian decay. Similarly, \cite{kang2024fsk} generalized continuous GNNs with fractional calculus in their FROND framework, introducing memory-dependent dynamics that algebraically mitigate over-smoothing by slowing the convergence rate to stationarity. \cite{geisler2024wli}'s Spatio-Spectral Graph Neural Networks (S2GNNs) synergistically combine local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to vanquish over-squashing and offer superior approximation bounds, demonstrating a powerful way to integrate local and global information. A physics-informed agnostic method proposed by \cite{shi2024g4z} enriches graph structures with additional nodes and rewired connections, theoretically verifying its ability to circumvent both over-smoothing and over-squashing. Moreover, \cite{zhu2021zc3} proposed a unified optimization framework that interprets the propagation mechanisms of various GNNs, including those that alleviate over-smoothing, by designing flexible graph convolutional kernels.

Despite significant architectural and theoretical progress, a critical re-evaluation by \cite{peng2024t2s} suggests that the drastic performance degradation in deep GNNs might not solely be attributable to over-smoothing. Instead, they argue that the inherent trainability challenges of deep Multi-Layer Perceptrons (MLPs) within GNN layers are a dominant problem, and many existing methods that ostensibly tackle over-smoothing actually improve the trainability of these MLPs. This perspective highlights that properly constrained gradient flow can significantly enhance GNN trainability, providing new insights for constructing deep graph models. This nuanced understanding underscores that while preventing feature homogenization is crucial, it is a necessary but not always sufficient condition for building high-performing deep GNNs.

In conclusion, while significant strides have been made in mitigating over-smoothing through decoupling, regularization, and theoretically grounded designs, and in addressing over-squashing via adaptive communication and global information integration, challenges persist. A unified theoretical framework that simultaneously addresses both over-smoothing and over-squashing, rather than treating them as separate phenomena, is still emerging. Developing adaptive mechanisms that dynamically adjust GNN depth, receptive field, or communication strategies based on local graph properties and task requirements remains an active research area. Furthermore, integrating insights from trainability studies to design deep GNNs that are both robust to information loss and efficiently optimizable continues to be a critical direction for future research.