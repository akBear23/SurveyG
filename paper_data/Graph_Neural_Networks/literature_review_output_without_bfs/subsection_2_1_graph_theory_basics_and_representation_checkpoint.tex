\subsection*{Graph Theory Basics and Representation}

Graphs are indispensable mathematical structures for modeling complex systems of interacting entities, forming the foundational input upon which Graph Neural Networks (GNNs) operate. A thorough understanding of fundamental graph theory concepts and their various numerical representations is therefore paramount for comprehending the intricate mechanics and capabilities of GNNs \cite{wu2022ptq, zhang2021jqr}. This subsection establishes the essential vocabulary and introduces common methods for structuring and representing graph data, preparing the reader for the subsequent exploration of GNN architectures and their processing of relational information.

Formally, a graph is defined as $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, comprising a set of $N = |\mathcal{V}|$ nodes (or vertices) and a set of $M = |\mathcal{E}|$ edges (or links) connecting pairs of nodes. Nodes typically represent individual entities, such as atoms in a molecule, users in a social network, or keypoints in an image. Edges, on the other hand, signify relationships or interactions between these entities, which can be chemical bonds, friendships, or potential correspondences. Beyond mere connectivity, both nodes and edges can carry rich semantic information in the form of attributes. Node attributes are commonly represented as feature vectors $\mathbf{x}_v \in \mathbb{R}^{d_v}$ for each node $v \in \mathcal{V}$, capturing intrinsic properties of the entity (e.g., atomic number, user profile information). Similarly, edge attributes $\mathbf{e}_{uv} \in \mathbb{R}^{d_e}$ can quantify characteristics of the relationship between nodes $u$ and $v$, such as strength, type, or direction (e.g., bond type, interaction frequency). The capacity of GNNs to jointly process this topological structure and the associated rich attributes is a primary source of their analytical power \cite{wu2022ptq}.

For computational processing by GNNs, graphs are translated into numerical data structures. The most prevalent structural representation is the \textbf{adjacency matrix} $\mathbf{A} \in \{0, 1\}^{N \times N}$, where $\mathbf{A}_{ij} = 1$ if an edge exists from node $i$ to node $j$, and $0$ otherwise. For graphs with node attributes, a \textbf{node feature matrix} $\mathbf{X} \in \mathbb{R}^{N \times d_v}$ is constructed, where each row $\mathbf{X}_i$ corresponds to the feature vector $\mathbf{x}_i$ of node $i$. Edge attributes can be stored in an \textbf{edge feature matrix} $\mathbf{E} \in \mathbb{R}^{M \times d_e}$ or, for weighted graphs, directly populate the adjacency matrix with real-valued weights.

Crucially, for many GNN architectures, particularly those rooted in spectral graph theory, the \textbf{degree matrix} $\mathbf{D} \in \mathbb{R}^{N \times N}$ is also fundamental. $\mathbf{D}$ is a diagonal matrix where $\mathbf{D}_{ii} = \sum_{j} \mathbf{A}_{ij}$ represents the degree of node $i$ (i.e., the sum of weights of its connections for weighted graphs, or simply the number of connections for unweighted graphs). From $\mathbf{A}$ and $\mathbf{D}$, the \textbf{Graph Laplacian} $\mathbf{L} = \mathbf{D} - \mathbf{A}$ is derived. The Laplacian matrix is central to graph signal processing and spectral graph theory, as its eigenvalues and eigenvectors reveal fundamental properties of the graph's structure and connectivity. Normalized variants, such as the symmetric normalized Laplacian $\mathbf{L}_{sym} = \mathbf{I} - \mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ and the random walk normalized Laplacian $\mathbf{L}_{rw} = \mathbf{I} - \mathbf{D}^{-1}\mathbf{A}$, are particularly important for spectral GNNs like Graph Convolutional Networks (GCNs). These normalized Laplacians facilitate stable message propagation, mitigate issues like exploding or vanishing gradients, and define how information propagates across the graph spectrum \cite{wang2022u2l}.

While adjacency matrices are conceptually straightforward, they can be memory-inefficient for large, sparse graphs (where $M \ll N^2$). In such cases, \textbf{adjacency lists} are often preferred for their computational efficiency. An adjacency list stores, for each node, a list of its direct neighbors. This representation is crucial for scalability, especially when dealing with real-world graphs that can contain billions of nodes and edges, as it allows GNNs to efficiently sample neighbors or process subgraphs without materializing the full adjacency matrix \cite{jin2021pf0}. The choice of representation significantly impacts the memory footprint and computational complexity of GNN training and inference, a critical consideration for large-scale applications.

Graphs can be categorized based on the nature of their edges, which significantly impacts how GNNs are designed and applied:
\begin{itemize}
    \item \textbf{Undirected Graphs:} Edges represent symmetric relationships, meaning if node $i$ is connected to node $j$, then $j$ is also connected to $i$. Their adjacency matrices are symmetric ($\mathbf{A}_{ij} = \mathbf{A}_{ji}$). Many social networks or co-occurrence graphs are naturally undirected.
    \item \textbf{Directed Graphs:} Edges denote asymmetric relationships, indicating a one-way flow of information or interaction from a source node to a target node. Here, $\mathbf{A}_{ij}$ does not necessarily imply $\mathbf{A}_{ji}$. Citation networks or knowledge graphs often fall into this category, requiring GNNs to differentiate between incoming and outgoing messages.
    \item \textbf{Weighted Graphs:} Edges are assigned numerical weights, quantifying the strength, distance, or cost of a relationship. The adjacency matrix entries $\mathbf{A}_{ij}$ then contain these real-valued weights instead of binary values. Road networks with travel times or protein-protein interaction networks with interaction strengths are common examples, where GNNs must incorporate these weights into aggregation functions.
    \item \textbf{Attributed Graphs:} Both nodes and/or edges possess explicit feature vectors, providing rich semantic context beyond mere connectivity. Most real-world graphs processed by GNNs are attributed, as these features are indispensable for learning complex patterns. For instance, in computer vision tasks like feature matching, nodes might represent image keypoints and edges, weighted by feature similarity, represent potential correspondences, with GNNs learning to refine these assignments \cite{sarlin20198a6}.
\end{itemize}
GNN architectures must be tailored to these graph types. For example, directed graphs necessitate separate aggregation of incoming and outgoing messages, while weighted graphs require weight-aware aggregation functions to properly leverage the strength of relationships.

A thorough understanding of graph theory fundamentals—including the definitions of nodes, edges, and attributes, their various numerical representations (adjacency matrices, feature matrices, degree matrices, Laplacians, and adjacency lists), and the distinctions between different graph types—is indispensable for comprehending Graph Neural Networks. These foundational concepts dictate how graph-structured information is encoded, processed, and ultimately leveraged by GNNs to learn complex relational patterns, forming the essential groundwork for all subsequent discussions on GNN architectures and their advanced applications.