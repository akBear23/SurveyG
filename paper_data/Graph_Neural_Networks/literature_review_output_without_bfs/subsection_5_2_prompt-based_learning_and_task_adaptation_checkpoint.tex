\subsection{Prompt-based Learning and Task Adaptation}

The efficient adaptation of pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks, particularly with limited labeled data, presents a significant challenge. While Section 5.1 detailed various self-supervised and generative pre-training strategies that learn robust representations from unlabeled graph data, the subsequent fine-tuning of these models often incurs high computational costs and can suffer from an "objective gap" between the general pre-training task and specific downstream requirements. This gap can lead to suboptimal performance, especially in few-shot learning scenarios where labeled data is scarce, a problem recognized early in GNN research \cite{satorras20174cv}. To address these limitations, prompt-based learning has emerged as a highly efficient paradigm within the broader context of parameter-efficient fine-tuning (PEFT), drawing inspiration from its success in natural language processing and computer vision. Unlike full fine-tuning or alternative PEFT strategies like self-training which expands labeled data via pseudo-labeling \cite{wang2024htw}, prompting aims to adapt pre-trained GNNs with minimal or no modification to the core model parameters, primarily by reformulating tasks or introducing learnable prompts.

Early explicit prompt-based learning for GNNs, such as **GPPT** \cite{sun2022d18}, directly tackled the objective gap by reformulating downstream node classification tasks to mimic the pre-training objective of masked edge prediction. GPPT transforms a standalone node and a candidate label class into a "token pair," allowing the pre-trained GNN to evaluate the "linking probability" between them. This approach demonstrated significant improvements in few-shot settings and faster convergence by leveraging the pre-trained knowledge without extensive fine-tuning. However, GPPT's primary limitation was its task-specific nature, requiring careful reformulation of each downstream task to align with a particular pre-training objective, thereby hindering its universality across diverse GNN pre-training strategies and downstream tasks.

To overcome the architectural and task-specific constraints of earlier methods, **Graph Prompt Feature (GPF)** \cite{fang2022tjj} proposed a more universal prompt tuning method. GPF operates by modifying the input graph's feature space, introducing a learnable vector that is added to all node features. This simple yet effective input-level prompting mechanism allows adaptation to *any* pre-trained GNN and *any* pre-training strategy, irrespective of the underlying pretext task. GPF provides theoretical guarantees for its universality, demonstrating that it can achieve an equivalent effect to various prompting functions. Empirically, it consistently outperforms traditional fine-tuning with significantly fewer tunable parameters, particularly in few-shot settings, showcasing average performance improvements of 1.4\% in full-shot and 3.2\% in few-shot scenarios over fine-tuning. This highlights a key design trade-off: GPF's input-level prompting offers broad model-agnosticism and universality.

Further generalizing the concept of prompt-based adaptation, **GraphPrompt** \cite{liu2023ent} introduced a framework that unifies pre-training (e.g., link prediction) and diverse downstream tasks (node and graph classification) into a common "subgraph similarity" template. In contrast to GPF's input feature modification, GraphPrompt's core innovation lies in designing task-specific learnable prompts that modify the `ReadOut` operation. These prompts act as adaptive parameters for the aggregation function, enabling the pre-trained GNN to flexibly fuse node representations into task-specific subgraph representations without fine-tuning the backbone model. This readout-level prompting provides more targeted control for graph-level tasks, allowing a single pre-trained model to serve a wider array of downstream objectives by adapting the final aggregation step, albeit potentially being more constrained by the GNN's architecture than input-level prompts.

Most recently, the paradigm has expanded to bridge GNNs with large language models (LLMs), moving towards more semantically aware graph models. **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)** \cite{li202444f} addresses scenarios with *extremely weak text supervision*. Morpher aligns pre-trained GNN representations with the semantic embedding space of LLMs by simultaneously learning both graph prompts (with an improved, stable design that decouples the prompt from node features to prevent interference) and text prompts, along with a cross-modal projector. By keeping the GNN and LLM frozen, this multi-modal prompting enables CLIP-style zero-shot generalization for GNNs to unseen classes, representing a significant advancement in leveraging rich semantic information for graph understanding.

In conclusion, prompt-based learning for GNNs represents a critical evolution in efficient knowledge transfer, moving from costly full fine-tuning to parameter-efficient adaptation. Initial strategies focused on task reformulation, which then evolved into universal input feature modification and adaptive `ReadOut` prompting, significantly minimizing the objective gap and facilitating few-shot or zero-shot generalization. The latest advancements in multi-modal prompting, integrating GNNs with LLMs, open exciting avenues for developing semantically richer and more adaptable graph models. However, significant challenges remain. These include designing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring complex prompt engineering or task-specific reformulations. Furthermore, the theoretical understanding of *why* prompts are so effective in GNNs is still nascent, and prompt design often remains heuristic, leading to potential brittleness and sensitivity to hyper-parameters. Scaling prompt learning to extremely large graphs and complex multi-modal scenarios also poses ongoing research questions.