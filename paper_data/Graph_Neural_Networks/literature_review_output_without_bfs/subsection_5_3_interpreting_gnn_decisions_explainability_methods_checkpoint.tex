\subsection{Interpreting GNN Decisions: Explainability Methods}
The inherent complexity of Graph Neural Networks (GNNs), stemming from their non-linear message-passing mechanisms and intricate graph structures, often renders them "black-box" models. This opacity necessitates robust explainability methods to foster trust, enable debugging, and facilitate scientific discovery in critical applications \cite{zhang20222g3, yuan2020fnk}. GNN explainability methods aim to uncover *why* a GNN makes a specific prediction, typically by identifying influential substructures or features. These methods can be broadly categorized into instance-level explanations, which focus on a single prediction, and model-level explanations, which reveal general patterns learned by the GNN \cite{yuan2020fnk}.

Early efforts in instance-level explanations, such as GNNExplainer \cite{ying2019rza}, pioneered the field by formulating the explanation task as an optimization problem. This method maximizes the mutual information between a GNN's prediction and a compact subgraph structure along with a subset of node features, using a perturbation-based approach to jointly identify their importance. However, GNNExplainer and similar gradient-based methods often implicitly assume linear independence of features, which can be problematic given the non-linear feature integration in GNNs \cite{vu2020zkj}. Addressing this, PGM-Explainer \cite{vu2020zkj} introduced a model-agnostic approach that generates explanations as simpler Bayesian Networks. This allows for explicit modeling of complex, non-linear dependencies among features and graph components, providing richer insights into conditional probabilities rather than just additive attributions. More recently, \cite{bui2024zy9} advanced instance-level explanations by proposing the Myerson-Taylor interaction index and the MAGE explainer. This method uniquely incorporates graph structure into the attribution process, mitigating "out-of-distribution" (OOD) biases common in naive Shapley-based perturbation methods, and is capable of identifying both positively and negatively contributing motifs.

While instance-level explanations provide specific insights, understanding the general behavior of a GNN requires model-level interpretations. XGNN \cite{yuan20208v3} was among the first to tackle this by training a graph generator, guided by reinforcement learning, to produce graph patterns that maximize a specific prediction score of the target GNN. This approach aims to reveal the general graph patterns associated with a particular class, although the generated patterns might not always be directly human-intelligible or perfectly reflect real-world structures \cite{yuan2020fnk}. Building on this, \cite{wang2024j6z} introduced Global Interactive Pattern (GIP) learning, an intrinsically interpretable scheme for graph classification. GIP identifies global interactive patterns by first coarsening the graph and then matching the compressed representation with learnable graph prototypes, moving towards more transparent graph-level reasoning.

Despite these advancements, the faithfulness and generalizability of GNN explainability methods remain critical challenges. A significant critique by \cite{chen2024woq} exposes approximation failures in many existing attention-based interpretable GNNs. Their theoretical framework, based on the Subgraph Multilinear Extension (SubMT), rigorously proves that prevalent attention mechanisms, when combined with non-linear GNNs, cannot accurately approximate the expected prediction over subgraphs. This fundamental limitation leads to degenerated interpretability and poor out-of-distribution (OOD) generalization, underscoring the need for more robust and theoretically grounded explanation mechanisms. In response, \cite{chen2024woq} proposes the Graph Multilinear ne T (GMT) architecture, which uses random subgraph sampling to more faithfully approximate SubMT. Furthermore, to enhance the reliability of explanations, \cite{wu2022vcx} focuses on discovering *invariant rationales* for GNNs. By employing a novel invariant learning strategy with causal interventions, this method aims to identify causal patterns that are stable across different data distributions, thereby filtering out spurious correlations and improving generalization to OOD data, directly addressing the faithfulness concern. The field continues to evolve, with comprehensive surveys like \cite{yuan2020fnk} providing a taxonomic overview and standardized testbeds to guide future research.

The ongoing challenge lies in developing explanation mechanisms that are not only efficient and scalable but also truly faithful to the GNN's decision-making process and generalizable across diverse graph structures and tasks. The critiques regarding approximation failures highlight the need for more rigorous theoretical foundations and novel architectural designs that can inherently provide reliable and trustworthy explanations, rather than relying on potentially misleading proxies.