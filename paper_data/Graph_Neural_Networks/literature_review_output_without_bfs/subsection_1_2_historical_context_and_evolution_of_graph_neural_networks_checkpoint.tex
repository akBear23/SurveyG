\subsection*{Historical Context and Evolution of Graph Neural Networks}

The advent of Graph Neural Networks (GNNs) marks a pivotal paradigm shift in machine learning, fundamentally transforming how complex relational data is processed and understood. Prior to GNNs, analyzing graph-structured data, ubiquitous in domains from social networks to molecular chemistry, presented significant challenges. Traditional graph analysis algorithms often relied on labor-intensive, handcrafted features, which were inherently rigid, domain-specific, and struggled to scale with the combinatorial complexity of large or dynamic graphs. Concurrently, conventional neural networks, designed for the grid-like structures of Euclidean data (e.g., images, sequences), lacked the intrinsic architectural inductive biases to directly process arbitrary graph topologies. This fundamental mismatch meant they could not effectively capture the rich relational information and structural patterns crucial for tasks like node classification, link prediction, and graph classification. GNNs emerged precisely to bridge this gap, offering an adaptive, data-driven approach to feature extraction by enabling end-to-end learning of rich, distributed representations directly from graph structure and node features. This paradigm shift has been instrumental in unlocking unprecedented capabilities, leading to their widespread adoption across numerous scientific and industrial applications.

The intellectual lineage of GNNs can be traced back to the early 2000s, where initial theoretical conceptualizations laid the groundwork for learning on graph domains. Pioneering efforts sought to extend neural network principles to handle non-Euclidean data by defining a state-transition system that iteratively propagated information across graph nodes. \cite{Gori05} first proposed a "neural network for graphs" that aimed to learn node representations by recursively aggregating information from neighbors until a stable fixed-point was achieved. This foundational idea was further formalized by \cite{Scarselli09}, who introduced the term "Graph Neural Network" (GNN) and rigorously defined it as an extension of recursive neural networks. Their work emphasized the fixed-point iteration as a mechanism for learning node embeddings that inherently encode neighborhood information, demonstrating the universal approximation capabilities of such models for graph functions. While these seminal contributions established the mathematical and conceptual framework for the GNN paradigm, their reliance on computationally intensive fixed-point iterations and inherent scalability limitations hindered their immediate practical widespread adoption. These early models, though theoretically profound, highlighted the need for more efficient and scalable architectures.

A pivotal breakthrough in the evolution of GNNs occurred in the mid-2010s with the emergence of scalable and effective message-passing architectures, which transformed GNNs from a theoretical curiosity into a widely applicable tool. This era was characterized by a shift from complex fixed-point iterations to simpler, layer-wise aggregation functions, often drawing inspiration from convolutional operations in image processing. Early explorations, such as \cite{Duvenaud15}'s work on adapting neighbor feature summation for molecular fingerprinting, hinted at the potential of localized aggregation. However, it was the introduction of models like the Graph Convolutional Network (GCN) by \cite{Kipf17} that marked a true paradigm shift. GCNs simplified spectral graph convolutions into a highly efficient, localized, first-order approximation, making semi-supervised node classification on large graphs practical and establishing a de-facto standard for subsequent research. Building on this, \cite{Hamilton17} introduced GraphSAGE, a framework that addressed the limitations of transductive learning by enabling inductive representation learning through efficient neighbor sampling. This innovation was crucial for applying GNNs to dynamic and unseen graphs. Further enhancing the expressiveness of message passing, \cite{Velickovic18} proposed Graph Attention Networks (GATs), which allowed models to learn adaptive importance weights for different neighbors. This mechanism overcame the fixed-weight aggregation of earlier models, enabling more nuanced and flexible information propagation. Collectively, these architectural innovations democratized GNNs, making them practical, robust, and widely adopted for a broader spectrum of graph-based tasks.

As GNNs gained prominence, the field entered a phase of critical analysis and theoretical maturation, focusing on understanding their fundamental capabilities, limitations, and expanding their applicability to more complex scenarios. A significant line of inquiry focused on the expressive power of GNNs. The seminal work by \cite{xu2018c8q} provided a rigorous theoretical framework, demonstrating that many standard message-passing GNNs are upper-bounded in their discriminative power by the 1-dimensional Weisfeiler-Lehman (1-WL) graph isomorphism test. This insight revealed inherent limitations in GNNs' ability to distinguish certain non-isomorphic graphs or count specific substructures, prompting extensive research into more powerful architectures. Subsequent work explored ways to overcome these bounds, including investigations into higher-order message passing, which aggregates information from $k$-hop neighborhoods or considers more complex substructures. For instance, \cite{feng20225sa} provided a theoretical characterization of K-hop message passing, demonstrating its enhanced expressive power beyond 1-WL while also identifying its own limitations, bounded by the 3-WL test. Similarly, the theoretical understanding of spectral GNNs also deepened, with works like \cite{wang2022u2l} analyzing their expressive power and the role of nonlinearities, even proving universality conditions for linear spectral GNNs under certain assumptions. Beyond expressivity, the evolution also encompassed adapting GNNs to more dynamic and intricate graph structures. While early GNNs primarily focused on static graphs, the need to model evolving relationships led to the development of Temporal GNNs (TGNNs). Surveys like \cite{longa202399q} highlight this crucial expansion, categorizing approaches that handle time-varying nodes, edges, and features, thus extending GNNs' utility to real-world dynamic systems. Furthermore, the pursuit of deeper GNNs and the exploration of recurrent architectures, as discussed by \cite{pflueger2024qi6}, which connect GNNs to concepts like bisimulation and logic, signify a growing emphasis on theoretical robustness and the ability to capture long-range dependencies and complex reasoning patterns beyond fixed-layer computations. This period thus marked a crucial shift towards a more profound understanding of GNNs' theoretical underpinnings and a concerted effort to push their boundaries in terms of expressivity, depth, and adaptability to diverse graph types.

In summary, the historical trajectory of Graph Neural Networks reflects a remarkable evolution from nascent theoretical concepts to a powerful and versatile paradigm in machine learning. This journey began by recognizing the inherent limitations of traditional methods for non-Euclidean data, leading to the pioneering fixed-point iteration models. The field then underwent a transformative shift with the advent of scalable message-passing architectures, which made GNNs practical and widely accessible. Subsequently, a period of critical theoretical analysis and architectural innovation ensued, deepening our understanding of GNN expressivity, pushing the boundaries of model depth, and extending their applicability to dynamic and complex graph structures. This continuous evolution has cemented GNNs as indispensable tools for learning on graph-structured data, enabling unprecedented capabilities in diverse applications. The foundational concepts and architectural breakthroughs discussed here lay the essential groundwork for understanding the more detailed methodologies, advanced models, and specific challenges that will be explored in the subsequent sections of this review.