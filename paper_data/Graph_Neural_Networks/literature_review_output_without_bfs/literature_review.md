# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T21:39:25.678665
**Papers analyzed:** 328

## Papers Included:
1. 9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf [wang2024oi8]
2. b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf [li2022315]
3. 900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf [kang2024fsk]
4. 4fa31616b834c377c4995c346a2b17464f25692a.pdf [gao2022f3h]
5. f442378ead6282024cf5b9046daa10422fe9fc5f.pdf [li2023o4c]
6. 20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf [michel2023hc4]
7. 741a7faf9dbefd418cda878c61c5b839ecc02977.pdf [chen2022mmu]
8. 123139463809b5acf98b95d4c8e958be334a32b5.pdf [yuan2021pgd]
9. fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf [dong202183w]
10. d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf [cappart2021xrp]
11. cd551790992d16148fe2e5ff2cc76861195e2191.pdf [dong2021qcg]
12. 458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf [li20245zy]
13. 27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf [zhao2020bmj]
14. 5e6db511e736f77f844bbeebaa2b177427abada1.pdf [joshi20239d0]
15. e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf [sun2022d18]
16. 5822490cf59df7f7ccb92b8901f244850b867a66.pdf [derrowpinion2021mwn]
17. ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf [chen2020bvl]
18. b4895de425a02af87713bd78ed1a29fe425753af.pdf [zeng2022jhz]
19. 75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf [yuan20208v3]
20. 8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf [xie2021n52]
21. b20589941cd52d199ba381b92e092ba7fb36d689.pdf [mitra2024x43]
22. 775a6e0f9104b282ed867871d743e3afd1e66d96.pdf [zhang2021kc7]
23. facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf [wang2022p2r]
24. 94497472eecb7530a2b75c564548c540ebd61e9b.pdf [lu20213kr]
25. 24b2aed0f130e5278325b5055711de44d247460e.pdf [fan2022m67]
26. bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf [zhang2020b0m]
27. 3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf [cui2022mjr]
28. a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf [bui2024zy9]
29. 3db15a5534050ab2cfc1d09dd772d032395515e1.pdf [liu2022a5y]
30. d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf [jin2023ijy]
31. 00358a3f17821476d93461192b9229fe7d92bb3f.pdf [ying2019rza]
32. 639206a9a32d91386924f1c94e9760dfb43df72e.pdf [liu2020w3t]
33. b88f456daaf29860d2b59c621be3bd878a581a59.pdf [longa202399q]
34. 0d4184cff17f093e0487b27180be515c385feff6.pdf [papp20211ac]
35. fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf [chang2021yyt]
36. 3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf [mujkanovic20238fi]
37. 44b9f16ba417b90e2e7c42f9074378dd06415809.pdf [you2021uxi]
38. 35792d528bd07aed95df46f0ecb87019cb123147.pdf [luo2024euy]
39. a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf [cui2022pap]
40. bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf [dai2022xze]
41. 03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf [wang2023wrg]
42. 1fad14bcfc2b75797c686a5a05779076437a683e.pdf [khemani2024i8r]
43. f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf [agarwal2022xfp]
44. d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf [dwivedi20239ab]
45. aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf [abboud2020x5e]
46. c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf [liu2023v3e]
47. 140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf [liu2021ee2]
48. aafe1338caef4682069e92378f1190785ec24c2c.pdf [balcilar20215ga]
49. 789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf [hu2019r47]
50. 68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf [chamberlain2022fym]
51. 81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf [reiser2022b08]
52. cf30fb61a5943781144c8442563e3ef9c38df871.pdf [li2021orq]
53. 641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf [wang2022u2l]
54. 7de413da6e0a00e14270cfaed2a31666e7c28747.pdf [zhang2021wgf]
55. 3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf [garg2020z6o]
56. 4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf [fatemi2021dmb]
57. 510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf [zhang2021jqr]
58. ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf [varbella20242iz]
59. 90dead8a056b848be164c2e5cdadfa2e191c3265.pdf [rusch2023xev]
60. 536da0e76290aea9cbe75c29bac096aeb45ef875.pdf [chen2020e6g]
61. 21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf [zhang20222g3]
62. f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf [han2024rkj]
63. 993377a3fc8334558463b82053904e3d684f29c0.pdf [rossi2020otv]
64. bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf [wu2022vcx]
65. 6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf [morris20185sd]
66. cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf [dai2022hsi]
67. 6dc0932670a0b5140a426ca310bbb03783ff2240.pdf [wang2024j6z]
68. fcdd4300f937cef11af297329ed4bd2b611871e7.pdf [ju2023prm]
69. 9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf [liu20242g6]
70. e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf [zhang2018kdl]
71. f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf [bianchi20194ea]
72. 8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf [ma2021sim]
73. 14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf [li202444f]
74. e4b1d7553020258d7e537e2cfa53865359389eac.pdf [he2020kz4]
75. 85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf [fang2022tjj]
76. 854342cf063eef4428a5441c8d317dfbabb8117f.pdf [chen2024woq]
77. e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf [liu2023ent]
78. d09608593caa20b79a8aaddfe19df7e31513d711.pdf [dong20225aw]
79. 398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf [fan2019k6u]
80. 0a69c8815536a657668e089e3281ff2e963d947a.pdf [you2020drv]
81. c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf [cai2020k4b]
82. ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf [gosch20237yi]
83. 73366d75289c5e37481639fb54fdee28a664e2b3.pdf [zhang2020jrt]
84. 3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf [alon2020fok]
85. 4b776e7f26464e5b230c1679560f12618730dcc6.pdf [zhu2021zc3]
86. 218223e91f55a1e0186f5b008b55f5e0fe350698.pdf [zou2021qkz]
87. 341880efaef452f631a4a5cd61bef5dae47741d7.pdf [xu2019l8n]
88. c9845a625e2dac5e32db172d353f81d377760a5f.pdf [xia20247w9]
89. 3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf [wu2020dc8]
90. 018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf [bianchi20239ee]
91. a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf [vu2020zkj]
92. 071e053890765ecc2ff8ef9054e9c75ec135e167.pdf [gao20213kp]
93. db5d583782264529456a475ce8e9a90823b3a2b5.pdf [bessadok2021bfy]
94. 3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf [wang20214ku]
95. edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf [geisler2024wli]
96. f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf [zeng20237gv]
97. 011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf [jin2020dh4]
98. 0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf [dai2020p5t]
99. 5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf [klicpera20215fk]
100. 454304628bf10f02aba1c2cfc95891e94d09208e.pdf [dwivedi2021af0]
101. caf8927cf3c872698a0e97591a1205ba577bbba5.pdf [feng20225sa]
102. 8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf [satorras2021pzl]
103. 707142f242ee4e40489062870ca53810cb33d404.pdf [mao202313j]
104. 6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf [zgner2019bbi]
105. 6ae2967bb0a5e57cc545176120a4845576e068a3.pdf [yuan2020fnk]
106. 46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf [finkelshtein202301z]
107. 11b9f4729c8e355dec7122993076f6e2788c03c4.pdf [lucic2021p70]
108. 02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf [zheng2022qxr]
109. 530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf [dai2023tuj]
110. 252351936bd6fabf4b6cd2962fa0ee613772278d.pdf [jin2023e18]
111. 6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf [ying20189jc]
112. 04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf [hu2020u8o]
113. 4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf [luan202272y]
114. ac225094aab9e7b629bc5b3343e026dea0200c70.pdf [klicpera20186xu]
115. 94194703e83b5447f519fd8bcbb903916e05aaf9.pdf [chen2019s47]
116. 0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf [wang2022531]
117. faa6fce9a16925eb3091271281f923bc95291ebb.pdf [zhou20213lg]
118. 2a85846fd827a157b624ee012e75cbe37344281c.pdf [jegelka20222lq]
119. 5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf [jin2021pf0]
120. 3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf [geisler2021dcq]
121. 81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf [wu20193b0]
122. 62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf [xu2018c8q]
123. ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf [zhou20188n6]
124. 7456dea3a3646f2df6392773a196a5abd0d53b11.pdf [batzner2021t07]
125. 347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf [sarlin20198a6]
126. 75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf [wu2020hi3]
127. 0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf [wu2018t43]
128. 21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf [zhu2020c3j]
129. 381411d740562de1e766dc8cc833844eb99dde01.pdf [wang2019t4a]
130. 24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf [li2020fil]
131. 572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf [satorras20174cv]
132. 00549af4bc3270e0f688acbf694f912d7ee39cad.pdf [zhou20195xo]
133. 4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf [oono2019usb]
134. 68a024d7b70ef3989a6751678f635cbe754440fc.pdf [shi2019vl4]
135. 2b8a207189bc02d73d1dce850bcde24dbd984483.pdf [wu20221la]
136. b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf [wang2020khd]
137. 594dc362b4332ae661e3d71da17d097bb4a357dd.pdf [wang2019vol]
138. 0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf [zhong2019kka]
139. 2b1eae2cceb377cb9267b2c96294228d5e583136.pdf [zhao2021po9]
140. cda969fd7362bdf21aa1f3398078982dcb350d76.pdf [lv20219al]
141. 1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf [yu201969a]
142. 53e80869c6582d7f95ef0a351170736afd1742d0.pdf [tang2022g66]
143. 6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf [zhao2021lls]
144. 80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf [keisler2022t7p]
145. 1478c3c0225368419f68aabc6b67033531d3b4c1.pdf [li2020mk1]
146. b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf [wu2022ptq]
147. 030046515a20a4b4f86c290361881923694e458a.pdf [liu2021qyl]
148. e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf [zhang20211dl]
149. 16351ff232f2e475c8d8347809ef905d67998fa5.pdf [shen202037i]
150. 23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf [zhang2020tdy]
151. f1e5e65941617604923225cc4bf464e370fcae67.pdf [huang20209zd]
152. 2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf [schaefer2022rsz]
153. b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf [chen20201cf]
154. 1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf [shen2022gcz]
155. 250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf [sharma2022liz]
156. 2028710190373ef893e3055c9113e04274a152d7.pdf [chen2021x8i]
157. 18b2c7dd5f37818f74407a69985322f8a109f75f.pdf [chen2022ifd]
158. 5ab6888c67d2877f15c2b065da4216538835d141.pdf [li2022hw4]
159. 38e320f860d54e4071d68955c774b3e4a091bfe0.pdf [yun2022s4i]
160. 6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf [wijesinghe20225ms]
161. 2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf [cini20213l6]
162. da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf [wu2023zm5]
163. 91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf [li2022a34]
164. 4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf [velickovic2023p4r]
165. 08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf [jiang2020gaq]
166. 80c698688bb4488beaceaab5c64f701a946cb7ae.pdf [sun2023vsl]
167. 19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf [zhang2021f18]
168. 3da4626411d83c19c9919bb41dba94fff88da90e.pdf [bojchevski2020c51]
169. f470ac3537339514bb9d88fcad9c075441906d45.pdf [xia2023bpu]
170. 116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf [rahmani2023kh4]
171. 6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf [chen2024gbe]
172. 2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf [liao202120x]
173. 94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf [hin2022g19]
174. 81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf [tsitsulin20209pl]
175. d08167fd8583b0f70ba8a26821c29ea8af420826.pdf [fung20212kw]
176. e243c89ac61aae7330792c6c3f8791f07f40d031.pdf [wang2021mxw]
177. 3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf [wang2020nbg]
178. fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf [jha2022cj8]
179. 84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf [schuetz2021cod]
180. 94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf [shen2021sbk]
181. c193011099906126fe7b6cfcb04062cf4591ccf9.pdf [bo2023rwt]
182. 286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf [zhang20212ke]
183. c00673042d8cc539d903c4f30b55a71487f5c701.pdf [wei20246l2]
184. aac77c36a9a5c24aa135538c32950096e59ba442.pdf [yu2020u32]
185. 86ad9d1dd6626921297a8456b048f4bccafe967c.pdf [he2021x8v]
186. d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf [wu20210h4]
187. fc580c211689663a64f42e2ba92c864cb134ba9b.pdf [kofinas2024t2b]
188. ef41b29312860bc284640e35ab499053f4966bbf.pdf [li2021v1l]
189. 064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf [balcilar2021di1]
190. 99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf [zhang2020f4l]
191. dc5e841197165c3c38940cc9f607fce7b09116cf.pdf [bilot20234ui]
192. 0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf [wu2023303]
193. 5f1913828e30c3070f32c154d2d142ec17e91189.pdf [huang2021lpu]
194. 4bc7d63595d194a6e0930019764216e6b42da0d4.pdf [suresh202191q]
195. bda290f54791e4719917e44a7e6441d000c43ab3.pdf [liu2022gcg]
196. 5b1978e8284c8514165938bff6e3276977088f94.pdf [wang202201n]
197. 8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf [zhou2021c3l]
198. b0d62f38592dbae23628d9700490cb11ac873182.pdf [vasimuddin2021x7c]
199. 54ff6c9ad037792e938e05985720d313512539b7.pdf [eliasof202189g]
200. 569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf [huang2023fk1]
201. e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf [fey2021smn]
202. 2efc9d5bb114f7114b041d621e002b1562366903.pdf [nguyen2021g12]
203. 84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf [innan2023fa7]
204. 91b9fa72da566afc77a07ec856c3d8da23714367.pdf [guo2022hu1]
205. 066dd731b5aaeede92d129344776783590c338d2.pdf [maurizi202293p]
206. 60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf [ye20226hn]
207. ac44bbb4c62033d558aad57712438e5571069d9c.pdf [liu2021efj]
208. cc827043e6c5be8337df4edb155096f9d0006020.pdf [du2021kn9]
209. 741bf9081fe341c173f36739a50606bf2a159610.pdf [xu20226vc]
210. f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf [wang2023a6u]
211. e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf [bing2022oka]
212. e46aa831aac38520249dff35916937f0d094f32e.pdf [lyu2023ao0]
213. a52ae33c11309a98887405db21e930a1f298d865.pdf [peng2021gbb]
214. a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf [xia2021s85]
215. 5fb4947831352af6d6231a830a943f0f2069ee8b.pdf [feng2022914]
216. cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf [paper2022mw4]
217. 7544db2ae3140081b1581a99eee88960cc31415a.pdf [luan2021g2p]
218. cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf [waikhom20226fa]
219. 0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf [tang2021h2z]
220. c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf [thost20211ln]
221. b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf [chai2022nf9]
222. 9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf [sun20239ly]
223. cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf [zhang2022atq]
224. 6b72135bf31e78ccee78478228b635201326d217.pdf [munikoti2022k7d]
225. 77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf [huoh2023i97]
226. cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf [han20227gn]
227. 20309e3990cd612a13e389e1572786e55100f03d.pdf [kim2022yql]
228. bd4b8cad70faa48605163eaede13d62fb671f5de.pdf [zhang2022uih]
229. e925e38c5bade594237439c1d4a77e1376535697.pdf [zhou2022a3h]
230. 8b9f01585a679dffe92261ecdec56425db9ef97f.pdf [wu2023aqs]
231. ab27a370d87617255455a05cb2d98c268b5fa06b.pdf [long2022l97]
232. 5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf [cini2022pjy]
233. 5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf [zhang2023ann]
234. 985a47671c30e2d059c568ba8eb8e2813bab9423.pdf [chang2023ex5]
235. 9208290fd7948ed14ebe55718118c401e8396159.pdf [wang2023zr0]
236. 3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf [zhao2022fvg]
237. ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf [sahili2023f2x]
238. f42898181e56cb6fee860143c96663ed361449e0.pdf [levie2023c1s]
239. df519a15af1e83824340212477d9d356f86f15ec.pdf [wang2024nuq]
240. 181ff84051e375be829ec230c1e65439a199171c.pdf [dong2024dx0]
241. cfc041534d57719d893ec5af01a7065621f7c410.pdf [zhao2024oyr]
242. 613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf [chen2024h2c]
243. a55c59163cf138d31994afc875d46997d3ef5c4b.pdf [foroutan2024nhg]
244. 3492576dae538ad34a6ecae5b631651e8ddebf92.pdf [wander2024nnn]
245. 4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf [li20248gg]
246. ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf [duan2024que]
247. 5ca285d36255114938751e1787681fa17073a313.pdf [praveen202498y]
248. adf1318ee484fe32d227a5084ed981eedb828c72.pdf [wang2024p88]
249. 7a42822cb3102041bad5ff7058451e35e48fd15f.pdf [jing2024az0]
250. cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf [zhang2024370]
251. 3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf [kanatsoulis2024l6i]
252. c1fbf79a695352b906d8c980608fccb99d3366ee.pdf [mishra2024v89]
253. cd90ab144ca439fad38ad952d254ef2036da6d96.pdf [fang2024p34]
254. 1697df4909875d593e1f82aeba49f2861640017a.pdf [zhang202483k]
255. cb2a45084f0c7bdc38271e94205603d1237945d8.pdf [yin20241mx]
256. 21dce0407d0ee3bec185b0361593d73bb26a532e.pdf [yan20240up]
257. bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf [shen2024exf]
258. 105191ce014da7d36d93d405c920a261dba3e937.pdf [manivannan2024830]
259. 10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf [he202455s]
260. 7779b880700e9e3495557e076d60594d18d69277.pdf [zhao2024qw6]
261. 599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf [yan2024ikq]
262. 687abbb274492f95b2c0fe82137c009754456d4c.pdf [xia2024xc9]
263. 3911024df853ccf11138d35835572ce863df51bf.pdf [zhou2024t2r]
264. cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf [lu2024eu9]
265. 774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf [wang2024cb8]
266. e49178ea82233947837c135ec303852dc776dbde.pdf [li2024yyl]
267. 0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf [castroospina2024iy2]
268. 046f6abdbf63fbb80d831102e7889c6801ad3545.pdf [zhao2024g5p]
269. fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf [duan2024efz]
270. cfff81fc166668790f4099cebd785cdd20f25b6d.pdf [luo2024h2k]
271. 01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf [carlo2024a3g]
272. 0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf [zandi2024dgs]
273. df2701c0fabf50b511182a287d112dfcc84c59b3.pdf [zhao2024aer]
274. 1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf [yao2024pyk]
275. 7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf [vinh20243q3]
276. 326430bd401c2ac820fc08a0a198ceacf1cde506.pdf [ashraf202443e]
277. 2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf [smith2024q8n]
278. c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf [abadal2024w7e]
279. 478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf [pflueger2024qi6]
280. 817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf [mohammadi202476q]
281. 0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf [sui2024xh9]
282. 93ad698088aa72fcbd5004bd59ff38c25054f319.pdf [peng2024t2s]
283. c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf [zhao2024e2x]
284. 9391738dff06189f64ced951df6c1848311731dc.pdf [nabian2024vto]
285. d0cd5ede6535f617e40b58517fe593b648b737b0.pdf [cen2024md8]
286. 819d9ef75975c78c5ce12e54af93737f4b698f55.pdf [yang2024vy7]
287. 6826db50e96adb61ecc437809a361b16ea7546a7.pdf [li2024gue]
288. 9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf [guo2024zoe]
289. 8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf [gnanabaskaran20245dg]
290. a6c060ab3b997675075415253e0a6bc81591f32e.pdf [wang20245it]
291. 73765285b243a53143912b501f7afab98a0c8cb0.pdf [abode2024m4z]
292. f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf [zhao20244un]
293. aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf [hausleitner2024vw0]
294. dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf [zhao2024g7h]
295. 9fb72c9292bf80f9825e0038d34cef57468a2757.pdf [rusch2024fgp]
296. b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf [wang2024htw]
297. 2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf [liu2024sbb]
298. 5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf [fang2024zd6]
299. 242425415e28da757bb9c7d24dd0a99654d66027.pdf [benedikt2024153]
300. da3c1508794ba0d4f070a9bc47b06575422f456f.pdf [zhang20241k0]
301. 0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf [graziani2024lgd]
302. a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf [shi2024g4z]
303. c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf [yuan2024b8b]
304. 67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf [wang2024kx8]
305. 12e60b9fd69f18c1c01996d108229051432b6090.pdf [abuhantash202458c]
306. c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf [abbahaddou2024bq2]
307. 7bda10706047e154e22259c4b20d70240296963e.pdf [huang2024tdd]
308. a3340cd6f24e4c83bec616c7bda719737492fe74.pdf [jiang202448s]
309. abd2ac274abe25f40f5268324d4774e67b467ef8.pdf [wang20246bq]
310. 3d0911fabeebc22506ac3b006a553448debf03a5.pdf [silva2024trs]
311. 1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf [zhang2024ctj]
312. 413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf [sun2024ztz]
313. f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf [zeng2024fpp]
314. 1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf [chen20241tu]
315. cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf [fujita2024crj]
316. ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf [saleh2024d2a]
317. 99d50bb7b0155203c908228d086eb232c34ee0a6.pdf [aburidi2024023]
318. 6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf [wang2024481]
319. ae683dbd44ec508f63254d864f83d6c1006dd652.pdf [horck2024a8s]
320. 6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf [sun2024pix]
321. 675a150a89f5c3dd44bf8312b00a896716c7082b.pdf [li2024r82]
322. d42ebd3b0673341125e374223e0882e99557cc8c.pdf [luo20240ot]
323. 13391f9fb2094227ecc567fef76fd95adc57e972.pdf [li202492k]
324. 406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf [liao20249wq]
325. 0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf [wang2024ged]
326. f60492aece8e86203ed95303cb809332a11d74b5.pdf [liu20245da]
327. b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf [varghese2024ygs]
328. ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf [dinverno2024vkw]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}



\subsection{Graph Data and its Significance}
\label{sec:1_1_graph_data__and__its_significance}


Graph-structured data, fundamentally characterized by a collection of nodes (entities) and edges (relationships) that may possess associated attributes, constitutes a powerful and ubiquitous paradigm for modeling complex real-world systems. Formally, a graph $G = (V, E, X, A)$ comprises a set of nodes $V$, a set of edges $E$ connecting pairs of nodes, an optional node feature matrix $X \in \mathbb{R}^{|V| \times d_v}$ where $d_v$ is the dimension of node features, and an optional edge attribute matrix $A \in \mathbb{R}^{|E| \times d_e}$ for edge features. Edges can be directed or undirected, weighted or unweighted, and graphs can be homogeneous (nodes and edges of a single type) or heterogeneous (multiple types of nodes and edges) [wu20193b0, zhang2021jqr]. Common representations include the adjacency matrix, which captures connectivity, and feature matrices for nodes and edges.

The pervasive nature of graphs in modeling intricate interdependencies makes them indispensable across diverse domains. In social sciences, graphs represent social networks, mapping individuals and their friendships, collaborations, or communication patterns [wu2022ptq]. In biology and chemistry, they model molecular structures (atoms as nodes, bonds as edges), protein-protein interaction networks, or gene regulatory pathways [wu20193b0]. Knowledge graphs organize factual information by connecting entities with semantic relationships, forming the backbone of many AI systems. Beyond these, graphs are crucial in transportation networks, recommender systems (users and items), citation networks (papers and citations), and communication infrastructures, offering a rich, holistic representation of relational information that is often lost in simpler data structures [wang2023zr0]. This ability to explicitly encode relationships and context is what makes graph data profoundly significant for understanding complex systems.

However, the inherent characteristics of graph data present unique and substantial challenges for traditional machine learning algorithms, which are primarily designed for Euclidean data (e.g., images as grids, text as sequences) [wu20193b0, wang2023zr0]. The most prominent challenge stems from the **non-Euclidean nature** of graphs. Unlike images or sequences, graphs lack a fixed grid structure, a canonical node ordering, or a global coordinate system. This means that graph data is permutation-invariant; the underlying structure and properties of a graph remain the same regardless of how its nodes are ordered or indexed. Traditional deep learning models like Convolutional Neural Networks (CNNs) rely on local connectivity and translation invariance on grid-like data, while Recurrent Neural Networks (RNNs) assume sequential dependencies. Neither paradigm naturally extends to the irregular, variable-sized, and arbitrarily structured topology of graphs [zhang2021jqr].

Furthermore, the **interdependence of elements** within a graph is both its strength and a significant challenge. The features and labels of a node are often strongly influenced by its neighbors and the broader graph topology. This relational context is crucial for accurate predictions, yet traditional machine learning models typically assume independent and identically distributed (i.i.d.) data samples. Applying standard classifiers to individual nodes or edges in isolation would ignore this vital relational information, leading to suboptimal performance [wu2022ptq]. Extracting meaningful features from graph structures for traditional machine learning models often requires extensive, handcrafted feature engineering (e.g., centrality measures, graphlet counts), which is labor-intensive, domain-specific, and may not generalize well across different graphs or tasks. This process is also prone to losing rich topological information.

These challenges highlight a critical gap: the need for specialized machine learning models capable of directly processing graph-structured data, learning rich, distributed representations that simultaneously encode both node features and their structural context. This necessity has driven the development of Graph Neural Networks (GNNs). GNNs are designed to overcome the limitations of traditional methods by enabling end-to-end learning directly on graph topology, capturing intricate relational patterns and structural information through message-passing mechanisms [wu20193b0, wang2023zr0]. Understanding these fundamental characteristics of graph data and the inherent difficulties they pose for conventional machine learning is crucial for appreciating why GNNs have emerged as an indispensable and rapidly evolving paradigm for effective analysis and prediction in graph domains. The subsequent sections of this review will delve into how GNNs address these challenges through various architectural advancements and learning paradigms.
\subsection{Historical Context and Evolution of Graph Neural Networks}
\label{sec:1_2_historical_context__and__evolution_of_graph_neural_networks}


The advent of Graph Neural Networks (GNNs) marks a pivotal paradigm shift in machine learning, fundamentally transforming how complex relational data is processed and understood. Prior to GNNs, analyzing graph-structured data, ubiquitous in domains from social networks to molecular chemistry, presented significant challenges. Traditional graph analysis algorithms often relied on labor-intensive, handcrafted features, which were inherently rigid, domain-specific, and struggled to scale with the combinatorial complexity of large or dynamic graphs. Concurrently, conventional neural networks, designed for the grid-like structures of Euclidean data (e.g., images, sequences), lacked the intrinsic architectural inductive biases to directly process arbitrary graph topologies. This fundamental mismatch meant they could not effectively capture the rich relational information and structural patterns crucial for tasks like node classification, link prediction, and graph classification. GNNs emerged precisely to bridge this gap, offering an adaptive, data-driven approach to feature extraction by enabling end-to-end learning of rich, distributed representations directly from graph structure and node features. This paradigm shift has been instrumental in unlocking unprecedented capabilities, leading to their widespread adoption across numerous scientific and industrial applications.

The intellectual lineage of GNNs can be traced back to the early 2000s, where initial theoretical conceptualizations laid the groundwork for learning on graph domains. Pioneering efforts sought to extend neural network principles to handle non-Euclidean data by defining a state-transition system that iteratively propagated information across graph nodes. [Gori05] first proposed a "neural network for graphs" that aimed to learn node representations by recursively aggregating information from neighbors until a stable fixed-point was achieved. This foundational idea was further formalized by [Scarselli09], who introduced the term "Graph Neural Network" (GNN) and rigorously defined it as an extension of recursive neural networks. Their work emphasized the fixed-point iteration as a mechanism for learning node embeddings that inherently encode neighborhood information, demonstrating the universal approximation capabilities of such models for graph functions. While these seminal contributions established the mathematical and conceptual framework for the GNN paradigm, their reliance on computationally intensive fixed-point iterations and inherent scalability limitations hindered their immediate practical widespread adoption. These early models, though theoretically profound, highlighted the need for more efficient and scalable architectures.

A pivotal breakthrough in the evolution of GNNs occurred in the mid-2010s with the emergence of scalable and effective message-passing architectures, which transformed GNNs from a theoretical curiosity into a widely applicable tool. This era was characterized by a shift from complex fixed-point iterations to simpler, layer-wise aggregation functions, often drawing inspiration from convolutional operations in image processing. Early explorations, such as [Duvenaud15]'s work on adapting neighbor feature summation for molecular fingerprinting, hinted at the potential of localized aggregation. However, it was the introduction of models like the Graph Convolutional Network (GCN) by [Kipf17] that marked a true paradigm shift. GCNs simplified spectral graph convolutions into a highly efficient, localized, first-order approximation, making semi-supervised node classification on large graphs practical and establishing a de-facto standard for subsequent research. Building on this, [Hamilton17] introduced GraphSAGE, a framework that addressed the limitations of transductive learning by enabling inductive representation learning through efficient neighbor sampling. This innovation was crucial for applying GNNs to dynamic and unseen graphs. Further enhancing the expressiveness of message passing, [Velickovic18] proposed Graph Attention Networks (GATs), which allowed models to learn adaptive importance weights for different neighbors. This mechanism overcame the fixed-weight aggregation of earlier models, enabling more nuanced and flexible information propagation. Collectively, these architectural innovations democratized GNNs, making them practical, robust, and widely adopted for a broader spectrum of graph-based tasks.

As GNNs gained prominence, the field entered a phase of critical analysis and theoretical maturation, focusing on understanding their fundamental capabilities, limitations, and expanding their applicability to more complex scenarios. A significant line of inquiry focused on the expressive power of GNNs. The seminal work by [xu2018c8q] provided a rigorous theoretical framework, demonstrating that many standard message-passing GNNs are upper-bounded in their discriminative power by the 1-dimensional Weisfeiler-Lehman (1-WL) graph isomorphism test. This insight revealed inherent limitations in GNNs' ability to distinguish certain non-isomorphic graphs or count specific substructures, prompting extensive research into more powerful architectures. Subsequent work explored ways to overcome these bounds, including investigations into higher-order message passing, which aggregates information from $k$-hop neighborhoods or considers more complex substructures. For instance, [feng20225sa] provided a theoretical characterization of K-hop message passing, demonstrating its enhanced expressive power beyond 1-WL while also identifying its own limitations, bounded by the 3-WL test. Similarly, the theoretical understanding of spectral GNNs also deepened, with works like [wang2022u2l] analyzing their expressive power and the role of nonlinearities, even proving universality conditions for linear spectral GNNs under certain assumptions. Beyond expressivity, the evolution also encompassed adapting GNNs to more dynamic and intricate graph structures. While early GNNs primarily focused on static graphs, the need to model evolving relationships led to the development of Temporal GNNs (TGNNs). Surveys like [longa202399q] highlight this crucial expansion, categorizing approaches that handle time-varying nodes, edges, and features, thus extending GNNs' utility to real-world dynamic systems. Furthermore, the pursuit of deeper GNNs and the exploration of recurrent architectures, as discussed by [pflueger2024qi6], which connect GNNs to concepts like bisimulation and logic, signify a growing emphasis on theoretical robustness and the ability to capture long-range dependencies and complex reasoning patterns beyond fixed-layer computations. This period thus marked a crucial shift towards a more profound understanding of GNNs' theoretical underpinnings and a concerted effort to push their boundaries in terms of expressivity, depth, and adaptability to diverse graph types.

In summary, the historical trajectory of Graph Neural Networks reflects a remarkable evolution from nascent theoretical concepts to a powerful and versatile paradigm in machine learning. This journey began by recognizing the inherent limitations of traditional methods for non-Euclidean data, leading to the pioneering fixed-point iteration models. The field then underwent a transformative shift with the advent of scalable message-passing architectures, which made GNNs practical and widely accessible. Subsequently, a period of critical theoretical analysis and architectural innovation ensued, deepening our understanding of GNN expressivity, pushing the boundaries of model depth, and extending their applicability to dynamic and complex graph structures. This continuous evolution has cemented GNNs as indispensable tools for learning on graph-structured data, enabling unprecedented capabilities in diverse applications. The foundational concepts and architectural breakthroughs discussed here lay the essential groundwork for understanding the more detailed methodologies, advanced models, and specific challenges that will be explored in the subsequent sections of this review.


### Foundational Concepts and Early Models

\section{Foundational Concepts and Early Models}
\label{sec:foundational_concepts__and__early_models}



\subsection{Graph Theory Basics and Representation}
\label{sec:2_1_graph_theory_basics__and__representation}


Graphs are indispensable mathematical structures for modeling complex systems of interacting entities, forming the foundational input upon which Graph Neural Networks (GNNs) operate. A thorough understanding of fundamental graph theory concepts and their various numerical representations is therefore paramount for comprehending the intricate mechanics and capabilities of GNNs [wu2022ptq, zhang2021jqr]. This subsection establishes the essential vocabulary and introduces common methods for structuring and representing graph data, preparing the reader for the subsequent exploration of GNN architectures and their processing of relational information.

Formally, a graph is defined as $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, comprising a set of $N = |\mathcal{V}|$ nodes (or vertices) and a set of $M = |\mathcal{E}|$ edges (or links) connecting pairs of nodes. Nodes typically represent individual entities, such as atoms in a molecule, users in a social network, or keypoints in an image. Edges, on the other hand, signify relationships or interactions between these entities, which can be chemical bonds, friendships, or potential correspondences. Beyond mere connectivity, both nodes and edges can carry rich semantic information in the form of attributes. Node attributes are commonly represented as feature vectors $\mathbf{x}_v \in \mathbb{R}^{d_v}$ for each node $v \in \mathcal{V}$, capturing intrinsic properties of the entity (e.g., atomic number, user profile information). Similarly, edge attributes $\mathbf{e}_{uv} \in \mathbb{R}^{d_e}$ can quantify characteristics of the relationship between nodes $u$ and $v$, such as strength, type, or direction (e.g., bond type, interaction frequency). The capacity of GNNs to jointly process this topological structure and the associated rich attributes is a primary source of their analytical power [wu2022ptq].

For computational processing by GNNs, graphs are translated into numerical data structures. The most prevalent structural representation is the \textbf{adjacency matrix} $\mathbf{A} \in \{0, 1\}^{N \times N}$, where $\mathbf{A}_{ij} = 1$ if an edge exists from node $i$ to node $j$, and $0$ otherwise. For graphs with node attributes, a \textbf{node feature matrix} $\mathbf{X} \in \mathbb{R}^{N \times d_v}$ is constructed, where each row $\mathbf{X}_i$ corresponds to the feature vector $\mathbf{x}_i$ of node $i$. Edge attributes can be stored in an \textbf{edge feature matrix} $\mathbf{E} \in \mathbb{R}^{M \times d_e}$ or, for weighted graphs, directly populate the adjacency matrix with real-valued weights.

Crucially, for many GNN architectures, particularly those rooted in spectral graph theory, the \textbf{degree matrix} $\mathbf{D} \in \mathbb{R}^{N \times N}$ is also fundamental. $\mathbf{D}$ is a diagonal matrix where $\mathbf{D}_{ii} = \sum_{j} \mathbf{A}_{ij}$ represents the degree of node $i$ (i.e., the sum of weights of its connections for weighted graphs, or simply the number of connections for unweighted graphs). From $\mathbf{A}$ and $\mathbf{D}$, the \textbf{Graph Laplacian} $\mathbf{L} = \mathbf{D} - \mathbf{A}$ is derived. The Laplacian matrix is central to graph signal processing and spectral graph theory, as its eigenvalues and eigenvectors reveal fundamental properties of the graph's structure and connectivity. Normalized variants, such as the symmetric normalized Laplacian $\mathbf{L}_{sym} = \mathbf{I} - \mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ and the random walk normalized Laplacian $\mathbf{L}_{rw} = \mathbf{I} - \mathbf{D}^{-1}\mathbf{A}$, are particularly important for spectral GNNs like Graph Convolutional Networks (GCNs). These normalized Laplacians facilitate stable message propagation, mitigate issues like exploding or vanishing gradients, and define how information propagates across the graph spectrum [wang2022u2l].

While adjacency matrices are conceptually straightforward, they can be memory-inefficient for large, sparse graphs (where $M \ll N^2$). In such cases, \textbf{adjacency lists} are often preferred for their computational efficiency. An adjacency list stores, for each node, a list of its direct neighbors. This representation is crucial for scalability, especially when dealing with real-world graphs that can contain billions of nodes and edges, as it allows GNNs to efficiently sample neighbors or process subgraphs without materializing the full adjacency matrix [jin2021pf0]. The choice of representation significantly impacts the memory footprint and computational complexity of GNN training and inference, a critical consideration for large-scale applications.

Graphs can be categorized based on the nature of their edges, which significantly impacts how GNNs are designed and applied:
\begin{itemize}
    \item \textbf{Undirected Graphs:} Edges represent symmetric relationships, meaning if node $i$ is connected to node $j$, then $j$ is also connected to $i$. Their adjacency matrices are symmetric ($\mathbf{A}_{ij} = \mathbf{A}_{ji}$). Many social networks or co-occurrence graphs are naturally undirected.
    \item \textbf{Directed Graphs:} Edges denote asymmetric relationships, indicating a one-way flow of information or interaction from a source node to a target node. Here, $\mathbf{A}_{ij}$ does not necessarily imply $\mathbf{A}_{ji}$. Citation networks or knowledge graphs often fall into this category, requiring GNNs to differentiate between incoming and outgoing messages.
    \item \textbf{Weighted Graphs:} Edges are assigned numerical weights, quantifying the strength, distance, or cost of a relationship. The adjacency matrix entries $\mathbf{A}_{ij}$ then contain these real-valued weights instead of binary values. Road networks with travel times or protein-protein interaction networks with interaction strengths are common examples, where GNNs must incorporate these weights into aggregation functions.
    \item \textbf{Attributed Graphs:} Both nodes and/or edges possess explicit feature vectors, providing rich semantic context beyond mere connectivity. Most real-world graphs processed by GNNs are attributed, as these features are indispensable for learning complex patterns. For instance, in computer vision tasks like feature matching, nodes might represent image keypoints and edges, weighted by feature similarity, represent potential correspondences, with GNNs learning to refine these assignments [sarlin20198a6].
\end{itemize}
GNN architectures must be tailored to these graph types. For example, directed graphs necessitate separate aggregation of incoming and outgoing messages, while weighted graphs require weight-aware aggregation functions to properly leverage the strength of relationships.

A thorough understanding of graph theory fundamentals—including the definitions of nodes, edges, and attributes, their various numerical representations (adjacency matrices, feature matrices, degree matrices, Laplacians, and adjacency lists), and the distinctions between different graph types—is indispensable for comprehending Graph Neural Networks. These foundational concepts dictate how graph-structured information is encoded, processed, and ultimately leveraged by GNNs to learn complex relational patterns, forming the essential groundwork for all subsequent discussions on GNN architectures and their advanced applications.
\subsection{Early Graph Neural Network Models: Fixed-Point Iteration}
\label{sec:2_2_early_graph_neural_network_models:_fixed-point_iteration}


The initial conceptualization of Graph Neural Networks (GNNs) emerged from the challenge of extending neural network capabilities to non-Euclidean graph domains, particularly focusing on models that learn node representations through an iterative information propagation process, aiming to reach a stable fixed-point. These pioneering models laid the crucial theoretical and mathematical groundwork for the entire field, establishing the idea of learning structural information through local aggregation.

One of the earliest and most influential works in this direction was proposed by [Gori05], which introduced a novel "neural network for graphs." This model conceptualized node states as being updated iteratively based on their previous state and the states of their neighbors, along with the features of the edges connecting them. The core idea was to propagate information across the graph until the node states converged to a stable equilibrium, effectively encoding the structural context of each node into its representation. This foundational work established the paradigm of a state-transition system on graphs, where node embeddings are derived from a recursive update rule.

Building upon this initial framework, [Scarselli09] formalized the Graph Neural Network (GNN) model, defining it as an extension of recursive neural networks capable of processing general graph structures. Their model explicitly defined a node's state (embedding) as the fixed-point of a contraction mapping, ensuring convergence and uniqueness of the solution. The GNN model, as formulated by [Scarselli09], utilized an iterative update function to compute the state of each node, considering its own features and the features of its neighbors and incident edges, until a stable fixed-point was attained. This theoretical underpinning demonstrated the GNN's ability to approximate universal functions on graphs, meaning it could, in principle, learn any computable function on graph-structured data. The output function then mapped these learned fixed-point states to task-specific predictions, such as node or graph classification.

Despite their theoretical elegance and foundational importance in establishing the GNN paradigm, these early fixed-point models faced significant practical limitations that hindered their widespread adoption. A primary concern was computational complexity; reaching a stable fixed-point often required numerous iterative steps, making training and inference computationally expensive, especially for large graphs. The convergence of these fixed-point iterations was also a practical challenge, as guaranteeing and achieving convergence in real-world scenarios could be difficult. Furthermore, the scalability of these models to large graphs was severely limited due to the global nature of fixed-point computation, which often necessitated processing the entire graph repeatedly. Training these models involved backpropagating gradients through potentially many unrolled fixed-point iterations, which could be cumbersome and prone to vanishing or exploding gradients. These practical constraints, particularly concerning computational efficiency, convergence guarantees, and scalability, spurred subsequent research into more efficient and scalable GNN architectures that moved away from strict fixed-point iteration in favor of finite-layer message-passing schemes.
\subsection{Message-Passing Paradigm: GCNs, GraphSAGE, and GATs}
\label{sec:2_3_message-passing_paradigm:_gcns,_graphsage,__and__gats}


The emergence of the message-passing paradigm marked a pivotal moment in the development of Graph Neural Networks (GNNs), significantly simplifying and popularizing these models by abstracting complex spectral operations into intuitive, localized aggregations. This paradigm transformed GNNs from theoretical constructs, which often involved computationally intensive fixed-point iterations [Gori05, Scarselli09], into widely accessible and effective tools for learning on graph-structured data.

A foundational breakthrough in this paradigm was the introduction of Graph Convolutional Networks (GCNs) by [Kipf17]. GCNs elegantly simplified the computationally demanding spectral graph convolutions into a localized, first-order approximation, enabling efficient semi-supervised learning on node classification tasks. The core idea involves iteratively aggregating feature information from immediate neighbors and transforming it with a learnable weight matrix, followed by a non-linear activation function. While highly effective and widely adopted for its simplicity and performance, the original GCN formulation was inherently transductive, meaning it struggled to generalize to unseen nodes or entirely new graphs, and it aggregated neighbor features with fixed, unweighted averages.

Addressing the critical challenge of inductive learning and scalability, [Hamilton17] proposed GraphSAGE (SAmple and aggreGatE). GraphSAGE introduced a framework to learn node embeddings by sampling a fixed number of neighbors and then aggregating their features using various learnable aggregation functions (e.g., mean, LSTM, pooling). This neighbor sampling mechanism allowed GraphSAGE to operate on large graphs and generalize to unseen nodes, making GNNs applicable to dynamic and evolving graph structures. Despite its advancements in scalability and inductive capabilities, GraphSAGE, similar to GCNs, still employed aggregation functions that assigned uniform importance to all sampled neighbors, potentially overlooking the varying relevance of different connections.

The limitation of fixed-weight aggregation was directly tackled by [Velickovic18] with the introduction of Graph Attention Networks (GATs). GATs empower the model to learn adaptive importance weights for different neighbors by employing a self-attention mechanism. For each node, GATs compute attention coefficients between the node and its neighbors, allowing the model to selectively focus on more relevant neighbors during the aggregation process. This mechanism not only enhances the model's expressiveness by overcoming the isotropic aggregation of GCNs and GraphSAGE but also provides a degree of interpretability by revealing which neighbors are considered more important. Furthermore, GATs leverage multi-head attention to stabilize the learning process and capture diverse relational patterns.

The theoretical underpinnings of these architectural advancements were further explored by [xu2018c8q], who analyzed the expressive power of GNNs. Their work demonstrated that popular GNN variants like GCNs and GraphSAGE, while powerful, are fundamentally limited in their ability to distinguish certain graph structures, connecting their discriminative power to the Weisfeiler-Lehman graph isomorphism test. This theoretical insight highlighted the inherent limitations of models relying on simple, fixed aggregation schemes and underscored the value of more sophisticated mechanisms, such as the attention mechanism in GATs, for enhancing a GNN's capacity to differentiate complex graph topologies.

In conclusion, the message-passing paradigm, championed by GCNs, GraphSAGE, and GATs, revolutionized GNN research by providing practical, scalable, and increasingly expressive architectures. These models addressed critical limitations in a progressive manner: GCNs simplified convolutions, GraphSAGE enabled inductive learning, and GATs introduced adaptive attention. This progression formed the backbone of many subsequent GNN developments, demonstrating the profound power of localized message aggregation for learning on graph structures. However, challenges remain, particularly in scaling these models to extremely deep architectures without encountering issues like over-smoothing, and in developing more sophisticated aggregation mechanisms that can handle complex relational patterns beyond simple attention, as seen in advanced applications like feature matching [sarlin20198a6].


### Enhancing Expressive Power and Deep Architectures

\section{Enhancing Expressive Power and Deep Architectures}
\label{sec:enhancing_expressive_power__and__deep_architectures}



\subsection{Understanding GNN Expressivity: Weisfeiler-Lehman Test and Beyond}
\label{sec:3_1_underst_and_ing_gnn_expressivity:_weisfeiler-lehman_test__and__beyond}


The theoretical expressive power of Graph Neural Networks (GNNs) is a critical area of research, particularly concerning their ability to distinguish non-isomorphic graphs and capture complex structural patterns. A foundational insight in this domain connects the discriminative power of many standard GNNs to the Weisfeiler-Lehman (WL) test for graph isomorphism. Early work by [morris20185sd] rigorously demonstrated that standard message-passing GNNs (1-GNNs) are at most as powerful as the 1-dimensional WL test (1-WL), and under suitable conditions, achieve equivalent power. This equivalence implies inherent limitations, as 1-WL cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or count specific substructures.

These limitations manifest in GNNs' inability to capture higher-order structural information. For instance, [chen2020e6g] formally proved that Message Passing Neural Networks (MPNNs) and even 2-Invariant Graph Networks (2-IGNs) cannot count induced subgraphs for any connected pattern with three or more nodes. Extending this, [garg2020z6o] provided impossibility proofs, showing that a broad class of GNNs, including popular Locally Unordered GNNs (LU-GNNs) and CPNGNNs, cannot compute fundamental graph properties such as girth, circumference, diameter, or k-clique. The review by [reiser2022b08] further acknowledges these expressivity limitations, alongside issues like over-squashing and over-smoothing, as key challenges for GNNs in practical applications like chemistry and materials science.

To overcome the 1-WL bottleneck, researchers have explored several avenues, primarily focusing on higher-order architectures that can capture richer structural information. [morris20185sd] initially proposed k-dimensional GNNs (k-GNNs), which perform message passing on k-element subsets (k-tuples) of nodes, theoretically achieving greater discriminative power than 1-WL. However, these k-GNNs often incur prohibitive computational costs due to the exponential growth in the number of k-tuples. Addressing this efficiency concern, [balcilar20215ga] introduced GNNML3, a spectral approach that leverages non-linear custom functions of eigenvalues to design graph convolution supports. This method experimentally matches the power of 3-WL equivalent models while maintaining linear computational complexity after an initial eigendecomposition, offering a more scalable alternative to direct k-tuple processing.

Other approaches enhance expressivity by explicitly incorporating path or subgraph information. [michel2023hc4] developed Path Neural Networks (PathNNs) that aggregate information from various paths. Their most expressive variant, operating on "annotated sets of paths," was theoretically shown to be strictly more powerful than 1-WL and empirically capable of distinguishing graphs indistinguishable by the 3-WL algorithm. Similarly, [zeng20237gv] proposed Substructure Aware Graph Neural Networks (SAGNN), which inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk encodings, into node features. SAGNNs are provably more powerful than 1-WL and can also distinguish graphs that fool the 3-WL test. For tasks like link prediction, [chamberlain2022fym] introduced ELPH and BUDDY, full-graph GNNs that efficiently approximate subgraph features (or "sketches") to achieve expressivity comparable to subgraph-based GNNs, effectively breaking 1-WL limitations for this specific task.

Another powerful strategy involves augmenting GNNs with positional or structural identifiers. [abboud2020x5e] demonstrated the "surprising power" of standard MPNNs when augmented with Random Node Initialization (RNI), proving that such models are universal approximators for functions on graphs of a fixed order, thereby efficiently breaking the 1-WL barrier. Building on this, [papp20211ac] proposed DropGNN, which uses random node dropouts during both training and testing across multiple runs to systematically explore perturbed graph structures, proving that this mechanism increases expressiveness beyond 1-WL. Furthermore, [dwivedi2021af0] introduced Learnable Structural and Positional Encodings (LSPE), a framework that decouples and learns both structural and positional representations throughout the GNN layers, utilizing a Random Walk Positional Encoding (RWPE) to avoid sign ambiguity issues common in Laplacian-based encodings. This concept was further refined by [wang2022p2r] with Equivariant and Stable Positional Encoding (PEG), which uses separate channels for node and positional features, ensuring O(p) equivariance and stability for positional embeddings, particularly beneficial for node-set tasks. The utility of positional encodings, such as Laplacian eigenvectors, for enhancing GNN expressivity on challenging datasets was also empirically highlighted by [dwivedi20239ab].

For domains where geometric information is paramount, such as molecular modeling, specialized equivariant GNNs have pushed expressivity further. [klicpera20215fk] introduced GemNet, proving universality for GNNs using spherical (S2) representations and incorporating a novel two-hop geometric message passing scheme that explicitly captures distances, angles, and dihedral angles. Similarly, [satorras2021pzl] developed E(n)-Equivariant Graph Neural Networks (EGNNs), which directly update node coordinates and embeddings while preserving E(n) equivariance, offering a simpler and more scalable alternative to methods relying on complex higher-order representations. Beyond static structural information, [finkelshtein202301z] proposed Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose their communication actions (listen, broadcast, isolate) at each layer. This dynamic and asynchronous message passing paradigm is theoretically shown to be more expressive than 1-WL due to the variance introduced by action sampling.

While enhancing theoretical expressivity, practical limitations like "over-squashing" can hinder the effective utilization of deep GNNs for long-range dependencies, as identified by [alon2020fok]. This phenomenon limits the amount of information that can be propagated across many layers. To address this, [zeng2022jhz] proposed decoupling the depth and scope of GNNs by applying deep GNNs on shallow, localized subgraphs. This approach not only mitigates oversmoothing but also enhances expressivity beyond 1-WL by allowing deeper processing of local information. More recently, [geisler2024wli] introduced Spatio-Spectral Graph Neural Networks (S2GNNs), a hybrid paradigm combining local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to overcome over-squashing and are strictly more expressive than 1-WL, offering tighter approximation bounds and "free" positional encodings. Finally, the expressivity of spectral GNNs has also been re-evaluated, with [wang2022u2l] demonstrating that even linear spectral GNNs can achieve universal approximation under specific conditions, challenging the necessity of non-linearity and connecting their power to the 1-WL test.

In conclusion, understanding the theoretical bounds imposed by the Weisfeiler-Lehman test is crucial for designing more powerful and discriminative GNN models. The field has moved significantly beyond the 1-WL limitations of standard message-passing architectures through various innovations. These include the development of higher-order GNNs, the strategic augmentation of node features with positional or structural encodings, the incorporation of geometric and dynamic message-passing mechanisms, and the exploration of hybrid spatial-spectral architectures. While significant progress has been made in enhancing expressivity, ongoing challenges remain in balancing this power with computational scalability, data efficiency, and generalization to diverse real-world graph structures and tasks.
\subsection{Overcoming Depth Limitations: Over-smoothing and Over-squashing}
\label{sec:3_2_overcoming_depth_limitations:_over-smoothing__and__over-squashing}


The aspiration to develop deeper Graph Neural Networks (GNNs) capable of discerning intricate, long-range dependencies within graph-structured data is fundamentally impeded by two critical and intertwined challenges: over-smoothing and over-squashing. Over-smoothing manifests when iterative message passing causes node representations to converge towards indistinguishable values, leading to a profound loss of discriminative power and an inability to differentiate between nodes [rusch2023xev, cai2020k4b]. Complementary to this, over-squashing describes an information bottleneck where the exponentially expanding receptive field of a node is compressed into a fixed-size vector, resulting in the significant loss of crucial information from distant nodes [alon2020fok]. Effectively addressing these limitations is paramount for extending the practical utility and expressive capabilities of GNNs.

Early investigations into deep GNNs quickly identified the over-smoothing phenomenon. [klicpera20186xu] made a seminal contribution by proposing to decouple the neural network's feature transformation from the graph's propagation mechanism. Their Personalized Propagation of Neural Predictions (PPNP) and its efficient approximation (APPNP) leverage personalized PageRank to facilitate deep propagation steps while preserving locality through a "teleport probability." This strategy effectively mitigates over-smoothing without increasing the neural network's parameter count or explicit depth. Building on this decoupling principle, [liu2020w3t] further re-evaluated over-smoothing, suggesting that for moderate depths, performance degradation primarily stems from the entanglement of representation transformation and propagation. Their Deep Adaptive Graph Neural Network (DAGNN) demonstrated that explicit decoupling enables significantly deeper GNNs without performance loss, with severe over-smoothing becoming critical only at extreme depths.

The theoretical understanding of over-smoothing has been rigorously advanced through the lens of Dirichlet energy. [cai2020k4b] provided a foundational theoretical framework, demonstrating that the Dirichlet energy of node embeddings exponentially converges to zero with increasing layers, directly signifying a loss of discriminative power. This concept was further formalized by [rusch2023xev], who introduced an axiomatic definition of over-smoothing based on the layer-wise exponential convergence of a node-similarity measure like Dirichlet energy. Their comprehensive survey [rusch2023xev] also critically evaluates various mitigation strategies. Leveraging these theoretical insights, [zhou20213lg] proposed a "Dirichlet energy constrained learning" principle, guiding the training of deep GNNs. Their Energetic Graph Neural Network (EGNN) incorporates orthogonal weight control, lower-bounded residual connections, and Shifted ReLU (SReLU) activation to maintain Dirichlet energy within an optimal range, enabling GNNs to effectively scale up to 64 layers. Similarly, [bianchi20194ea] introduced Graph Neural Networks with Convolutional ARMA Filters, which employ recursive updates with skip connections to capture global dependencies and mitigate over-smoothing by offering a more flexible frequency response than traditional polynomial filters. In a different vein, [eliasof202189g] proposed PDE-GCN, a family of architectures motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, allowing for the control of over-smoothing by design and offering a theoretical explanation for their behavior.

While over-smoothing primarily concerns feature homogenization, [alon2020fok] introduced "over-squashing" as a distinct bottleneck, particularly for tasks necessitating long-range interactions. This phenomenon arises from the inherent limitation of fixed-size message aggregation, which struggles to compress the exponentially growing information from a node's receptive field without significant loss. To address this, [zeng2022jhz] proposed a novel principle to decouple the *depth* and *scope* of GNNs. Their SHADOW-GNN applies deep GNNs on shallow, localized subgraphs, theoretically proving it prevents both over-smoothing and neighbor explosion, thereby enabling more expressive and scalable models. Taking a different approach, [finkelshtein202301z] introduced Cooperative Graph Neural Networks (CO-GNNs), where nodes dynamically choose communication actions (listen, broadcast, isolate) at each layer. This adaptive, node-specific information flow inherently mitigates over-squashing and over-smoothing by allowing for more nuanced and efficient long-range communication. Furthermore, [wu20221la]'s GraphTrans leverages Transformer-based self-attention with a novel readout mechanism to learn long-range pairwise relationships, offering a permutation-invariant approach that implicitly addresses the challenge of capturing distant node interactions often lost to over-squashing.

Beyond architectural modifications, novel non-convolutional designs and alternative learning paradigms offer robust pathways to overcome depth limitations and address both challenges. [wang2024oi8] presented Random Walk with Unifying Memory (RUM), a fundamentally non-convolutional GNN that processes information via random walks and RNNs. RUM is theoretically shown to jointly remedy limited expressiveness, over-smoothing, and over-squashing by maintaining non-diminishing Dirichlet energy and slower inter-node Jacobian decay. Similarly, [kang2024fsk] generalized continuous GNNs with fractional calculus in their FROND framework, introducing memory-dependent dynamics that algebraically mitigate over-smoothing by slowing the convergence rate to stationarity. [geisler2024wli]'s Spatio-Spectral Graph Neural Networks (S2GNNs) synergistically combine local spatial message passing with global spectral filtering. S2GNNs are theoretically proven to vanquish over-squashing and offer superior approximation bounds, demonstrating a powerful way to integrate local and global information. A physics-informed agnostic method proposed by [shi2024g4z] enriches graph structures with additional nodes and rewired connections, theoretically verifying its ability to circumvent both over-smoothing and over-squashing. Moreover, [zhu2021zc3] proposed a unified optimization framework that interprets the propagation mechanisms of various GNNs, including those that alleviate over-smoothing, by designing flexible graph convolutional kernels.

Despite significant architectural and theoretical progress, a critical re-evaluation by [peng2024t2s] suggests that the drastic performance degradation in deep GNNs might not solely be attributable to over-smoothing. Instead, they argue that the inherent trainability challenges of deep Multi-Layer Perceptrons (MLPs) within GNN layers are a dominant problem, and many existing methods that ostensibly tackle over-smoothing actually improve the trainability of these MLPs. This perspective highlights that properly constrained gradient flow can significantly enhance GNN trainability, providing new insights for constructing deep graph models. This nuanced understanding underscores that while preventing feature homogenization is crucial, it is a necessary but not always sufficient condition for building high-performing deep GNNs.

In conclusion, while significant strides have been made in mitigating over-smoothing through decoupling, regularization, and theoretically grounded designs, and in addressing over-squashing via adaptive communication and global information integration, challenges persist. A unified theoretical framework that simultaneously addresses both over-smoothing and over-squashing, rather than treating them as separate phenomena, is still emerging. Developing adaptive mechanisms that dynamically adjust GNN depth, receptive field, or communication strategies based on local graph properties and task requirements remains an active research area. Furthermore, integrating insights from trainability studies to design deep GNNs that are both robust to information loss and efficiently optimizable continues to be a critical direction for future research.
\subsection{Higher-Order and Equivariant GNNs}
\label{sec:3_3_higher-order__and__equivariant_gnns}


The inherent limitations of standard Graph Neural Networks (GNNs), particularly their equivalence to the 1-Weisfeiler-Lehman (1-WL) test, restrict their ability to distinguish complex graph structures and adequately capture intricate motifs [xu2018c8q, morris20185sd, jegelka20222lq]. This section explores advanced GNN architectures designed to overcome these expressivity bottlenecks and to respect inherent symmetries in data, which is crucial for physical and geometric tasks.

To enhance discriminative power beyond the 1-WL test, a class of **higher-order GNNs** has emerged. Initial efforts, such as k-dimensional GNNs (k-GNNs), directly generalize message passing to operate on k-tuples or k-element subsets of nodes, proving strictly more powerful than 1-GNNs by capturing higher-order structural information like triangle counts [morris20185sd]. While theoretically robust, these models often incur significant computational and memory costs, scaling polynomially or exponentially with the number of nodes, which limits their applicability to large graphs [morris20185sd, garg2020z6o].

To mitigate the computational overhead of explicit k-tuple processing while retaining enhanced expressiveness, alternative strategies have been developed. Random Node Initialization (RNI) has been shown to enable standard Message Passing Neural Networks (MPNNs) to achieve universal approximation capabilities, effectively breaking the 1-WL barrier by individualizing nodes, though often at the cost of slower convergence [abboud2020x5e]. Similarly, DropGNN leverages random node dropouts across multiple runs during both training and testing to perturb and explore diverse neighborhood patterns, allowing GNNs to distinguish graphs beyond the 1-WL test with relatively low overhead [papp20211ac].

Another powerful approach involves injecting explicit positional or structural information. Positional Encodings (PEs), often derived from graph Laplacian eigenvectors, provide unique node identities that help GNNs differentiate isomorphic nodes and capture global structural context [dwivedi2021af0]. However, traditional Laplacian PEs suffer from sign ambiguity and instability issues. The Learnable Structural and Positional Encodings (LSPE) framework addresses this by decoupling and concurrently learning both structural and positional representations throughout the GNN layers, often outperforming static PEs [dwivedi2021af0]. Building on this, PEG (Positional Encoding GNN) further improves stability and equivariance for PEs by imposing O(p) equivariance on positional features, making them robust to eigenvalue multiplicities and suitable for large graphs [wang2022p2r].

Beyond node-level enhancements, models explicitly designed to capture substructures or paths offer increased expressiveness. Substructure Aware Graph Neural Networks (SAGNNs) inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk return probability encodings, into node features, enabling GNNs to perceive higher-order substructures and surpass the 1-WL limit [zeng20237gv]. For tasks like link prediction, the SEAL framework leverages a $\beta$-decaying heuristic theory to justify learning high-order features from local enclosing subgraphs using GNNs, achieving superior performance by dynamically learning structural patterns rather than relying on fixed heuristics [zhang2018kdl]. Path Neural Networks (PathNNs) take a different route, aggregating information from various paths, with its most expressive variant, $\tilde{AP}$, capable of distinguishing graphs indistinguishable by the 3-WL algorithm through recursively annotated path sets [michel2023hc4].

Furthermore, dynamic and hybrid approaches are pushing the boundaries of expressiveness and long-range interaction. Cooperative Graph Neural Networks (CO-GNNs) allow nodes to dynamically choose their communication actions (listen, broadcast, isolate) at each layer, leading to adaptive information flow that enhances expressive power beyond 1-WL and mitigates over-smoothing [finkelshtein202301z]. Similarly, Spatio-Spectral Graph Neural Networks (S2GNNs) combine local spatial message passing with global, spectrally bounded but spatially unbounded spectral filters. This hybrid design effectively vanquishes the "over-squashing" problem, enabling robust long-range information exchange and achieving state-of-the-art performance on challenging benchmarks [geisler2024wli].

A distinct but equally critical area focuses on **equivariant GNNs**, which are designed to respect inherent symmetries in data, particularly for physical and geometric tasks. Many scientific domains, such as molecular modeling, quantum chemistry, and physics, involve 3D structures whose properties must transform predictably under geometric operations like rotation, translation, and reflection [reiser2022b08]. Enforcing this equivariance acts as a powerful inductive bias, leading to more accurate and data-efficient models.

While some GNNs achieve E(n) invariance (e.g., by using relative distances), they often fail to maintain equivariance for vector outputs [satorras2021pzl]. Early equivariant models often relied on computationally expensive higher-order representations like spherical harmonics, limiting their scalability and efficiency. E(n)-Equivariant Graph Neural Networks (EGNNs) offer a simpler and more efficient solution by directly integrating coordinate updates into the message-passing framework. EGNNs update node features and coordinates simultaneously, using squared relative distances for message computation and relative differences for coordinate updates, thereby preserving E(n) equivariance without complex higher-order tensors [satorras2021pzl]. This design scales effectively to arbitrary n-dimensional spaces.

Building on these principles, more specialized equivariant architectures have emerged. GemNet, for instance, provides a universal directional GNN for molecules, leveraging spherical (S2) representations and a novel two-hop message passing scheme that explicitly incorporates interatomic distances, angles, and crucial dihedral angles. This allows GemNet to achieve state-of-the-art accuracy and data efficiency in molecular property prediction [klicpera20215fk]. Another notable example is Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network that employs geometric tensors for interactions, leading to highly data-efficient and accurate interatomic potentials for molecular dynamics simulations [batzner2021t07].

In conclusion, the development of higher-order and equivariant GNNs represents a significant leap in addressing the fundamental limitations of traditional GNNs. Higher-order models, through various strategies from k-tuples to sophisticated positional and substructural encodings, are pushing the boundaries of structural discriminative power. Concurrently, equivariant GNNs are enabling physically consistent and data-efficient predictions in scientific domains by explicitly respecting geometric symmetries. Despite these advancements, challenges remain in balancing expressiveness with scalability for extremely large graphs, developing more generalizable equivariant models for diverse physical systems, and enhancing the interpretability of complex higher-order features. The continuous development of rigorous benchmarking frameworks remains crucial for fairly evaluating and driving progress in these advanced GNN architectures [dwivedi20239ab].


### Robustness, Adaptability, and Trustworthiness

\section{Robustness, Adaptability, and Trustworthiness}
\label{sec:robustness,_adaptability,__and__trustworthiness}



\subsection{Handling Real-World Graph Imperfections: Heterophily and Structure Learning}
\label{sec:4_1_h_and_ling_real-world_graph_imperfections:_heterophily__and__structure_learning}


Real-world graphs frequently deviate from ideal homophilic assumptions, presenting significant challenges for Graph Neural Networks (GNNs). These imperfections manifest as heterophily, where connected nodes exhibit dissimilar features or labels, or as incomplete, noisy, or entirely absent structural information. Addressing these issues is paramount for extending GNN utility beyond controlled datasets to the complex realities of real-world graph data, demanding more flexible and robust models.

The pervasive nature of heterophily, where nodes connect to dissimilar neighbors, is a primary concern for traditional GNNs that implicitly rely on homophilic message passing [zheng2022qxr]. However, a nuanced understanding reveals that not all heterophily is equally detrimental; GCNs can perform well on certain heterophilous graphs if same-label nodes exhibit consistent neighborhood patterns, even if those neighbors are of different labels [ma2021sim]. This highlights the concept of "structural disparity," where real-world graphs contain a mixture of homophilic and heterophilic patterns, leading to performance disparities where GNNs excel on majority patterns but struggle on minority ones [mao202313j]. To better characterize these complexities, [luan202272y] introduced novel post-aggregation similarity metrics, such as Aggregation Similarity Score ($S_{agg}$) and Graph Aggregation Homophily ($H_{agg}(G)$), which more accurately reflect GNN performance under heterophily than traditional metrics.

To address the limitations of uniform, global filtering in such mixed-pattern graphs, researchers have developed adaptive filtering mechanisms. [luan202272y] proposed the Adaptive Channel Mixing (ACM) framework, which augments baseline GNNs by adaptively exploiting aggregation (low-pass), diversification (high-pass), and identity channels node-wisely. While effective, ACM's channel weights are learned globally per layer, potentially limiting its granularity. Building on this, [han2024rkj] introduced NODE-MOE (Node-wise Filtering via Mixture of Experts), a more granular approach where a gating model dynamically selects and applies appropriate "expert" GNNs, each potentially equipped with a different filter type, to individual nodes based on their local patterns. This theoretically demonstrates the suboptimality of global filters and offers superior adaptability, albeit with increased computational overhead due to the gating mechanism and multiple expert networks. A related strategy for handling heterophily involves transforming the input graph itself to improve its assortativity. [suresh202191q] proposed creating a multi-relational "computation graph" that distinguishes between proximity and structural information, thereby enhancing node-level assortativity and improving GNN performance under diverse mixing patterns. This approach implicitly adapts the graph structure to better suit GNNs.

A distinct strategy for heterophily involves seeking "global homophily" rather than purely local adaptation. [li2022315] proposed GloGNN/GloGNN++, which aggregates information from the entire graph using a learned signed coefficient matrix. This matrix implicitly combines low-pass and high-pass filtering by assigning positive coefficients to homophilous nodes and negative ones to heterophilous nodes, effectively leveraging global correlations. Crucially, GloGNN achieves this with linear time complexity ($O(k^2n)$) by reordering matrix multiplications, making it highly scalable compared to naive global aggregation ($O(n^3)$ or $O(n^2c)$). While powerful for capturing long-range homophilous connections, the effectiveness of GloGNN relies on the assumption that such global patterns exist and are learnable, which might not hold for all graph types.

Beyond heterophily, the absence or imperfection of graph structures presents another critical challenge. When initial graph structures are noisy, incomplete, or unavailable, GNNs require mechanisms to learn or augment these structures. [chen2020bvl] pioneered the Iterative Deep Graph Learning (IDGL) framework, which jointly and iteratively learns optimal graph structures and GNN parameters, explicitly optimizing for downstream tasks. This framework formulates graph learning as a similarity metric learning problem, enabling inductive capabilities and robustness to adversarial graph examples. However, this joint optimization process is computationally intensive ($O(n^2)$ for the full graph) and risks converging to suboptimal structures if not carefully regularized. To mitigate this, IDGL introduced a scalable anchor-based version (IDGL-ANCH) that reduces complexity to $O(ns)$ for large graphs, though it assumes noiseless node features.

Expanding on structure learning, [fatemi2021dmb] introduced SLAPS (Self-Supervision Improves Structure Learning), which addresses the "supervision starvation" problem in latent graph learning by employing a self-supervised denoising autoencoder task. This provides additional supervision for learning a more robust graph structure itself, particularly beneficial when explicit labels for structure learning are scarce. Similarly, [wei20246l2] explored self-supervised GNNs for enhanced feature extraction in Heterogeneous Information Networks (HINs), aiming to flexibly combine diverse information types to mine deep features and improve adaptability to graph diversity and complexity. For augmenting existing graphs, [zhao2020bmj] proposed GAUG, a framework that leverages neural edge predictors to strategically add "missing" intra-class edges and remove "noisy" inter-class edges, thereby promoting class-homophilic structure and improving performance. In scenarios of "extreme weak information," where structure, features, and labels are simultaneously deficient, [liu2023v3e] introduced D2PT (Dual-channel Diffused Propagation then Transformation), a comprehensive dual-channel GNN framework that combines an input graph backbone with a learned global graph and prototype contrastive alignment to effectively propagate information and connect stray nodes. This represents a robust solution for the most challenging structure-deficient environments.

In conclusion, the research landscape for GNNs operating on imperfect real-world data has evolved significantly, moving towards more flexible and robust models. Adaptive filtering mechanisms like ACM and NODE-MOE offer fine-grained control over message passing to handle structural disparities, albeit with varying computational overheads. Concurrently, methods like GloGNN provide scalable global aggregation for heterophily. For structural deficiencies, iterative and self-supervised structure learning frameworks such as IDGL, SLAPS, and D2PT enable GNNs to operate effectively even with noisy or absent graph topologies. While substantial progress has been made, challenges remain in ensuring the computational efficiency and theoretical guarantees of dynamic graph learning mechanisms, especially for extremely large and evolving graphs. Furthermore, designing models that can seamlessly adapt between diverse local and global structural patterns without explicit architectural changes or significant overhead remains an active area of research, particularly in achieving truly universal generalization across all types of complex, unseen graph imperfections.
\subsection{Adversarial Robustness and Defenses}
\label{sec:4_2_adversarial_robustness__and__defenses}


Graph Neural Networks (GNNs) have demonstrated remarkable capabilities across various domains, yet their vulnerability to adversarial attacks poses significant security and reliability concerns, particularly in critical applications. These attacks involve small, often imperceptible, perturbations to the graph structure or node features that can drastically alter a GNN's predictions. The field has seen a rapid evolution in understanding these vulnerabilities and developing corresponding defense mechanisms.

Early research highlighted the susceptibility of GNNs to poisoning attacks, where an adversary manipulates the training data. [zgner2019bbi] introduced the first global poisoning attack on GNNs, formulating it as a bilevel optimization problem solved via meta-gradients to degrade overall model performance. This demonstrated that GNNs are vulnerable beyond targeted misclassifications, even performing worse than non-relational baselines under attack. Building on this, [xu2019l8n] proposed an optimization-based framework for both topology attacks and defenses. They addressed the challenge of discrete graph structures by relaxing edge perturbation variables to continuous ones, enabling gradient-based Projected Gradient Descent (PGD) attacks and min-max adversarial training for robust GNNs.

In response to these emerging threats, several defense mechanisms were proposed. [zhang2020jrt] introduced GNNGuard, a general, plug-and-play defense algorithm designed to protect against training-time structural attacks. GNNGuard operates by dynamically estimating neighbor importance and employing a layer-wise graph memory to prune suspicious edges and stabilize message passing, notably demonstrating effectiveness even on heterophily graphs, a limitation of prior defenses. Further enhancing robustness, [liu2021ee2] developed Elastic Graph Neural Networks (Elastic GNNs) that incorporate \(\ell_1\)-based graph smoothing. This approach allows for adaptive local smoothness, preserving important discontinuities while providing enhanced robustness against adversarial attacks, outperforming \(\ell_2\)-based smoothing methods.

The landscape of adversarial attacks also expanded beyond simple poisoning. [he2020kz4] revealed a critical privacy vulnerability by demonstrating "link stealing attacks," where an adversary can infer the underlying graph structure from a black-box GNN's outputs. This highlights that GNNs implicitly encode significant structural information, which can be exploited for privacy breaches. Furthermore, [zhang2020b0m] introduced backdoor attacks to GNNs for graph classification tasks. These attacks inject a predefined subgraph trigger into a small fraction of training graphs, causing the trained GNN to consistently misclassify any graph containing this trigger to a target label, posing a stealthy and persistent threat.

A significant challenge in GNN robustness research has been the scalability of both attack and defense methods to real-world, large-scale graphs. Addressing this, [geisler2021dcq] developed sparsity-aware first-order optimization attacks, such as Projected Randomized Block Coordinate Descent (PR-BCD), which overcome the prohibitive memory requirements of prior methods. They also introduced novel surrogate losses (e.g., Masked Cross Entropy) for more effective global attacks and a scalable robust aggregation function (Soft Median) for defenses, enabling the study of GNN robustness on graphs orders of magnitude larger than previously possible.

Despite the proliferation of defense mechanisms, a critical evaluation of their true effectiveness against adaptive adversaries remained largely unexplored. [mujkanovic20238fi] conducted a comprehensive study, rigorously evaluating seven popular GNN defenses against custom-designed adaptive attacks. Their findings were sobering: most existing defenses offered "no or only marginal improvement" compared to an undefended baseline when faced with an adversary aware of the defense mechanism. This work, which introduced a systematic methodology for designing strong adaptive attacks (including a novel Meta-PGD attack), exposed a significant overestimation of GNN robustness in prior literature, akin to similar revelations in the computer vision community.

The ongoing challenge of creating truly robust GNNs against adaptive adversaries, who can circumvent naive protective measures, remains a crucial area for continued research. The findings of [mujkanovic20238fi] underscore the need for a paradigm shift in GNN defense evaluation, advocating for adaptive attacks as the new gold standard. Future work must focus on developing inherently robust GNN architectures and training strategies that can withstand sophisticated, white-box adversaries, moving beyond superficial improvements to achieve genuine security and reliability in real-world deployments.
```
\subsection{Fairness and Privacy in GNNs}
\label{sec:4_3_fairness__and__privacy_in_gnns}

The increasing deployment of Graph Neural Networks (GNNs) in sensitive domains necessitates a rigorous focus on their ethical dimensions, particularly ensuring fair outcomes and protecting sensitive information. The relational nature of graph data presents unique challenges, as information propagation can inadvertently amplify biases or leak sensitive details, making careful design and evaluation paramount for trustworthy GNN systems [zhang20222g3, dai2022hsi].

Fairness in GNNs is a critical concern, as models can inherit and even exacerbate biases present in graph data and structures. Sources of bias include imbalanced representation of demographic groups, homophily (tendency for similar nodes to connect), and the message-passing mechanism itself, which can propagate and amplify discriminatory patterns [zhang20222g3]. Early efforts to mitigate unfairness, such as FairGNN [dai2020p5t], addressed the practical challenge of limited sensitive attribute information by employing a GCN-based estimator for missing attributes, combined with adversarial debiasing to ensure predictions are independent of sensitive attributes. This approach demonstrated significant reduction in discrimination while maintaining high accuracy, even with sparse sensitive attribute knowledge. Building on this, [dong2021qcg] shifted the focus to data-centric debiasing by proposing EDITS, a model-agnostic pre-processing framework. EDITS formally defines and mitigates "attribute bias" and "structural bias" directly in the input attributed network using Wasserstein-1 distance-based metrics, optimizing to reduce these biases before GNN training. Further, [wang2022531] identified the critical issue of "sensitive attribute leakage" where GNN feature propagation dynamically alters feature correlations, exacerbating bias. Their FairVGNN framework mitigates this by using a generative adversarial debiasing module to mask sensitive-correlated features and an adaptive weight clamping module to minimize representation differences between sensitive groups.

Beyond group fairness, research has also delved into individual fairness, which ensures that similar individuals receive similar predictions. [dong202183w] addressed this more granular problem by proposing REDRESS, a novel ranking-based framework. This approach refines the notion of individual fairness to overcome the practical difficulties associated with Lipschitz conditions, offering a plug-and-play solution for existing GNN architectures. More recently, [li20245zy] rethought GNN fairness from a re-balancing perspective, introducing FairGB, which combines Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL). CNM generates unbiased ego-networks by interpolating node attributes and neighbor distributions across counterfactual pairs, while CAL is a gradient-based re-weighting method that dynamically balances group contributions during training, achieving state-of-the-art fairness-utility trade-offs. A more fundamental approach to learning unbiased representations is presented by [wu2022vcx], which proposed Discovering Invariant Rationale (DIR). DIR is an invariant learning strategy for intrinsically interpretable GNNs that identifies causal patterns stable across different data distributions, thereby inherently contributing to fairness by avoiding "shortcut features" and improving out-of-distribution generalization.

In addition to fairness, privacy is a paramount concern in GNNs, especially when dealing with sensitive graph structures or node attributes. The interconnected nature of graphs means that even seemingly innocuous information can be inferred or leaked, making GNNs uniquely vulnerable to various privacy attacks [dai2022hsi]. A significant privacy vulnerability was exposed by [he2020kz4], which pioneered the study of "link stealing attacks" against GNNs. Their work demonstrated that the underlying graph structure can be inferred from a black-box GNN's outputs, even with minimal adversary knowledge. This research introduced a comprehensive threat model and various attack methodologies (unsupervised, supervised, transferring) that achieved high AUCs, revealing that GNNs inherently encode and implicitly reveal structural information about their training graphs through their message-passing mechanisms. Beyond link stealing, GNNs are also susceptible to "node attribute inference attacks," where sensitive node features (e.g., medical conditions, income) are inferred from a model's outputs or learned representations, and "membership inference attacks," which determine if a specific node was part of the GNN's training dataset [dai2022hsi, zhang20222g3].

To counter these threats, several privacy-preserving mechanisms have been explored for GNNs. Differential Privacy (DP) is a robust technique that adds carefully calibrated noise to data or model parameters to provide strong privacy guarantees. In GNNs, DP can be applied by perturbing node features, adding noise to gradients during training, or even altering the graph structure (e.g., adding/removing edges) before training [dai2022hsi, zhang20222g3]. However, applying DP to non-Euclidean graph data is challenging, as noise addition can significantly degrade model utility, and the interdependent nature of graph data makes global privacy guarantees difficult to achieve without substantial performance loss. Another promising paradigm is Federated Learning (FL), which enables collaborative training of GNNs across multiple clients without centralizing their raw graph data [liu2022gcg]. In Federated GNNs (FedGNNs), clients train local GNN models on their private subgraphs and only share aggregated model updates with a central server, thus preserving data locality. This approach is particularly relevant for sensitive applications like disease classification, where patient data must remain private [hausleitner2024vw0]. Challenges in FedGNNs include handling graph heterogeneity across clients, managing communication overhead, and ensuring robustness against malicious clients. Traditional graph anonymization techniques (e.g., k-anonymity, l-diversity) can also be applied, but often suffer from significant utility loss or are vulnerable to re-identification attacks in complex graph structures [dai2022hsi].

The unique challenges posed by the relational nature of graph data mean that fairness and privacy are often intertwined, presenting complex trade-offs. Bias amplification can occur through message passing, and sensitive attribute leakage can be a privacy concern. Critically, privacy-preserving mechanisms like Differential Privacy, while protecting individual data, can sometimes disproportionately affect minority groups, exacerbating existing biases or making them harder to detect and mitigate [dai2022hsi, zhang20222g3]. Conversely, debiasing techniques aimed at achieving fairness might inadvertently reveal more sensitive information about individuals or groups. Building trustworthy GNN systems therefore requires careful consideration of these ethical dimensions, necessitating novel architectural designs and evaluation methodologies that balance predictive performance with robust fairness and privacy guarantees. Future research must continue to explore comprehensive solutions that address multi-modal bias, offer stronger and provable privacy guarantees, and provide certified fairness, especially in the non-IID graph setting. Key open challenges include developing unified frameworks that simultaneously optimize for fairness and privacy without severe trade-offs, designing scalable and efficient privacy-preserving GNNs for massive real-world graphs, and establishing standardized benchmarks and metrics for evaluating the complex interplay between fairness, privacy, and utility in GNNs. This holistic approach is essential for ensuring GNNs are deployed responsibly and ethically.


### Advanced Learning Paradigms: Pre-training and Explainability

\section{Advanced Learning Paradigms: Pre-training and Explainability}
\label{sec:advanced_learning_paradigms:_pre-training__and__explainability}



\subsection{Self-Supervised and Generative Pre-training for GNNs}
\label{sec:5_1_self-supervised__and__generative_pre-training_for_gnns}


The pervasive scarcity of labeled graph data presents a significant bottleneck for training powerful Graph Neural Networks (GNNs) in numerous real-world applications. To circumvent this, researchers have increasingly focused on developing robust pre-training strategies that leverage vast amounts of unlabeled graph data to learn transferable representations. This paradigm aims to overcome issues like negative transfer and enhance generalization across diverse downstream tasks, thereby maximizing the utility of GNNs. This section delves into the evolution of self-supervised and generative pre-training methods, highlighting their core principles, key advancements, and remaining challenges.

Early foundational work systematically investigated strategies for pre-training GNNs by designing various self-supervised pretext tasks. [hu2019r47] pioneered a comprehensive approach by proposing tasks at both node and graph levels. Node-level tasks included context prediction (predicting a node's K-hop neighborhood) and attribute masking (reconstructing masked node/edge attributes), while graph-level tasks focused on predicting graph-level properties or generating graph summaries. This combined strategy was crucial for learning meaningful local and global representations and was shown to effectively mitigate negative transfer, a common pitfall when naively applying pre-training. Beyond these, other predictive self-supervised tasks include link prediction, predicting structural roles or properties (e.g., node centrality, community membership proxies), and graph reconstruction via autoencoders [xie2021n52]. These methods train GNNs to capture inherent graph characteristics by generating labels directly from the graph structure itself.

A dominant and highly influential paradigm in self-supervised learning for GNNs is contrastive learning. Inspired by its success in computer vision and natural language processing, contrastive methods aim to learn robust representations by maximizing the mutual information between different augmented "views" of a graph or its components. The core idea is to pull positive pairs (different augmentations of the same graph or node) closer in the embedding space while pushing negative pairs (augmentations of different graphs or nodes) further apart. Key to this approach is the design of effective graph augmentation strategies, which can involve node/edge dropping, feature masking, subgraph sampling, or diffusion-based transformations [xie2021n52].

Seminal works in contrastive graph learning include Deep Graph Infomax (DGI) [velickovic2019deep], which maximizes mutual information between node representations and a global graph summary, distinguishing positive local-global pairs from corrupted negative ones. GraphCL [you2020graphcl] further generalized this by exploring various graph augmentation strategies to generate diverse views, demonstrating that the choice of augmentation is critical for effective pre-training. GRACE [zhu2020deep] extended this to node-level contrastive learning, applying both feature and structure corruption to generate two distinct views of a node, then maximizing agreement between their representations. While effective, traditional contrastive methods often rely on carefully constructed negative samples, which can be computationally intensive or challenging to define optimally. Addressing this, [zhang20211dl] proposed a conceptually simple yet effective model that moves away from explicit negative sampling. Inspired by Canonical Correlation Analysis (CCA), their method generates two views of an input graph through augmentation but optimizes a feature-level objective to learn invariant representations and prevent degenerated solutions by decorrelating features in different dimensions. This approach essentially aims to discard augmentation-variant information, offering an alternative to traditional contrastive objectives.

Beyond self-supervised predictive and contrastive tasks, generative pre-training approaches model the underlying graph structure and features directly. Building upon the concept of learning general graph knowledge, [hu2020u8o] introduced GPT-GNN, a generative pre-training framework that models the likelihood of an attributed graph by explicitly factorizing the generation process into coupled attribute and edge generation tasks. This novel dependency-aware factorization allowed the GNN to capture the intricate interplay between node features and graph structure, providing a more holistic understanding of the graph compared to simpler self-supervised pretext tasks. Such generative models aim to learn the data distribution of graphs, enabling them to synthesize new graphs or reconstruct corrupted ones, thereby acquiring a deep understanding of graph topology and feature dependencies.

The utility of self-supervised pre-training extends to complex graph types, such as Heterogeneous Information Networks (HINs). [wei20246l2] demonstrated how self-supervised GNNs can enhance feature extraction in HINs, addressing challenges like heterogeneity and redundancy by flexibly combining different types of additional information. This highlights the adaptability of self-supervision in mining deep features from diverse and complex graph data, improving model adaptability and overall performance. Furthermore, [lu20213kr] recognized the inherent objective divergence between pre-training and downstream fine-tuning. They proposed L2P-GNN, which leverages a meta-learning framework to explicitly optimize the GNN's ability to rapidly adapt to new tasks during pre-training, thereby learning more transferable prior knowledge and bridging the gap between the two phases.

The progression from simple self-supervised pretext tasks to sophisticated contrastive and generative modeling highlights a clear trajectory in GNN research: from learning general graph knowledge to efficiently leveraging that knowledge for specific, often low-resource, tasks. While significant strides have been made in overcoming labeled data scarcity and mitigating negative transfer, persistent challenges remain. These include designing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring extensive task-specific engineering, and developing more principled graph augmentation strategies for contrastive learning that are robust across various graph domains. Future research will likely focus on exploring more theoretically grounded approaches to view generation, developing unified frameworks that combine the strengths of predictive, contrastive, and generative methods, and extending these paradigms to increasingly complex graph structures such as dynamic, temporal, and heterogeneous graphs.
\subsection{Prompt-based Learning and Task Adaptation}
\label{sec:5_2_prompt-based_learning__and__task_adaptation}


The efficient adaptation of pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks, particularly with limited labeled data, presents a significant challenge. While Section 5.1 detailed various self-supervised and generative pre-training strategies that learn robust representations from unlabeled graph data, the subsequent fine-tuning of these models often incurs high computational costs and can suffer from an "objective gap" between the general pre-training task and specific downstream requirements. This gap can lead to suboptimal performance, especially in few-shot learning scenarios where labeled data is scarce, a problem recognized early in GNN research [satorras20174cv]. To address these limitations, prompt-based learning has emerged as a highly efficient paradigm within the broader context of parameter-efficient fine-tuning (PEFT), drawing inspiration from its success in natural language processing and computer vision. Unlike full fine-tuning or alternative PEFT strategies like self-training which expands labeled data via pseudo-labeling [wang2024htw], prompting aims to adapt pre-trained GNNs with minimal or no modification to the core model parameters, primarily by reformulating tasks or introducing learnable prompts.

Early explicit prompt-based learning for GNNs, such as **GPPT** [sun2022d18], directly tackled the objective gap by reformulating downstream node classification tasks to mimic the pre-training objective of masked edge prediction. GPPT transforms a standalone node and a candidate label class into a "token pair," allowing the pre-trained GNN to evaluate the "linking probability" between them. This approach demonstrated significant improvements in few-shot settings and faster convergence by leveraging the pre-trained knowledge without extensive fine-tuning. However, GPPT's primary limitation was its task-specific nature, requiring careful reformulation of each downstream task to align with a particular pre-training objective, thereby hindering its universality across diverse GNN pre-training strategies and downstream tasks.

To overcome the architectural and task-specific constraints of earlier methods, **Graph Prompt Feature (GPF)** [fang2022tjj] proposed a more universal prompt tuning method. GPF operates by modifying the input graph's feature space, introducing a learnable vector that is added to all node features. This simple yet effective input-level prompting mechanism allows adaptation to *any* pre-trained GNN and *any* pre-training strategy, irrespective of the underlying pretext task. GPF provides theoretical guarantees for its universality, demonstrating that it can achieve an equivalent effect to various prompting functions. Empirically, it consistently outperforms traditional fine-tuning with significantly fewer tunable parameters, particularly in few-shot settings, showcasing average performance improvements of 1.4\% in full-shot and 3.2\% in few-shot scenarios over fine-tuning. This highlights a key design trade-off: GPF's input-level prompting offers broad model-agnosticism and universality.

Further generalizing the concept of prompt-based adaptation, **GraphPrompt** [liu2023ent] introduced a framework that unifies pre-training (e.g., link prediction) and diverse downstream tasks (node and graph classification) into a common "subgraph similarity" template. In contrast to GPF's input feature modification, GraphPrompt's core innovation lies in designing task-specific learnable prompts that modify the `ReadOut` operation. These prompts act as adaptive parameters for the aggregation function, enabling the pre-trained GNN to flexibly fuse node representations into task-specific subgraph representations without fine-tuning the backbone model. This readout-level prompting provides more targeted control for graph-level tasks, allowing a single pre-trained model to serve a wider array of downstream objectives by adapting the final aggregation step, albeit potentially being more constrained by the GNN's architecture than input-level prompts.

Most recently, the paradigm has expanded to bridge GNNs with large language models (LLMs), moving towards more semantically aware graph models. **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)** [li202444f] addresses scenarios with *extremely weak text supervision*. Morpher aligns pre-trained GNN representations with the semantic embedding space of LLMs by simultaneously learning both graph prompts (with an improved, stable design that decouples the prompt from node features to prevent interference) and text prompts, along with a cross-modal projector. By keeping the GNN and LLM frozen, this multi-modal prompting enables CLIP-style zero-shot generalization for GNNs to unseen classes, representing a significant advancement in leveraging rich semantic information for graph understanding.

In conclusion, prompt-based learning for GNNs represents a critical evolution in efficient knowledge transfer, moving from costly full fine-tuning to parameter-efficient adaptation. Initial strategies focused on task reformulation, which then evolved into universal input feature modification and adaptive `ReadOut` prompting, significantly minimizing the objective gap and facilitating few-shot or zero-shot generalization. The latest advancements in multi-modal prompting, integrating GNNs with LLMs, open exciting avenues for developing semantically richer and more adaptable graph models. However, significant challenges remain. These include designing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring complex prompt engineering or task-specific reformulations. Furthermore, the theoretical understanding of *why* prompts are so effective in GNNs is still nascent, and prompt design often remains heuristic, leading to potential brittleness and sensitivity to hyper-parameters. Scaling prompt learning to extremely large graphs and complex multi-modal scenarios also poses ongoing research questions.
\subsection{Interpreting GNN Decisions: Explainability Methods}
\label{sec:5_3_interpreting_gnn_decisions:_explainability_methods}

The inherent complexity of Graph Neural Networks (GNNs), stemming from their non-linear message-passing mechanisms and intricate graph structures, often renders them "black-box" models. This opacity necessitates robust explainability methods to foster trust, enable debugging, and facilitate scientific discovery in critical applications [zhang20222g3, yuan2020fnk]. GNN explainability methods aim to uncover *why* a GNN makes a specific prediction, typically by identifying influential substructures or features. These methods can be broadly categorized into instance-level explanations, which focus on a single prediction, and model-level explanations, which reveal general patterns learned by the GNN [yuan2020fnk].

Early efforts in instance-level explanations, such as GNNExplainer [ying2019rza], pioneered the field by formulating the explanation task as an optimization problem. This method maximizes the mutual information between a GNN's prediction and a compact subgraph structure along with a subset of node features, using a perturbation-based approach to jointly identify their importance. However, GNNExplainer and similar gradient-based methods often implicitly assume linear independence of features, which can be problematic given the non-linear feature integration in GNNs [vu2020zkj]. Addressing this, PGM-Explainer [vu2020zkj] introduced a model-agnostic approach that generates explanations as simpler Bayesian Networks. This allows for explicit modeling of complex, non-linear dependencies among features and graph components, providing richer insights into conditional probabilities rather than just additive attributions. More recently, [bui2024zy9] advanced instance-level explanations by proposing the Myerson-Taylor interaction index and the MAGE explainer. This method uniquely incorporates graph structure into the attribution process, mitigating "out-of-distribution" (OOD) biases common in naive Shapley-based perturbation methods, and is capable of identifying both positively and negatively contributing motifs.

While instance-level explanations provide specific insights, understanding the general behavior of a GNN requires model-level interpretations. XGNN [yuan20208v3] was among the first to tackle this by training a graph generator, guided by reinforcement learning, to produce graph patterns that maximize a specific prediction score of the target GNN. This approach aims to reveal the general graph patterns associated with a particular class, although the generated patterns might not always be directly human-intelligible or perfectly reflect real-world structures [yuan2020fnk]. Building on this, [wang2024j6z] introduced Global Interactive Pattern (GIP) learning, an intrinsically interpretable scheme for graph classification. GIP identifies global interactive patterns by first coarsening the graph and then matching the compressed representation with learnable graph prototypes, moving towards more transparent graph-level reasoning.

Despite these advancements, the faithfulness and generalizability of GNN explainability methods remain critical challenges. A significant critique by [chen2024woq] exposes approximation failures in many existing attention-based interpretable GNNs. Their theoretical framework, based on the Subgraph Multilinear Extension (SubMT), rigorously proves that prevalent attention mechanisms, when combined with non-linear GNNs, cannot accurately approximate the expected prediction over subgraphs. This fundamental limitation leads to degenerated interpretability and poor out-of-distribution (OOD) generalization, underscoring the need for more robust and theoretically grounded explanation mechanisms. In response, [chen2024woq] proposes the Graph Multilinear ne T (GMT) architecture, which uses random subgraph sampling to more faithfully approximate SubMT. Furthermore, to enhance the reliability of explanations, [wu2022vcx] focuses on discovering *invariant rationales* for GNNs. By employing a novel invariant learning strategy with causal interventions, this method aims to identify causal patterns that are stable across different data distributions, thereby filtering out spurious correlations and improving generalization to OOD data, directly addressing the faithfulness concern. The field continues to evolve, with comprehensive surveys like [yuan2020fnk] providing a taxonomic overview and standardized testbeds to guide future research.

The ongoing challenge lies in developing explanation mechanisms that are not only efficient and scalable but also truly faithful to the GNN's decision-making process and generalizable across diverse graph structures and tasks. The critiques regarding approximation failures highlight the need for more rigorous theoretical foundations and novel architectural designs that can inherently provide reliable and trustworthy explanations, rather than relying on potentially misleading proxies.


### Scalability, Benchmarking, and Real-World Applications

\section{Scalability, Benchmarking, and Real-World Applications}
\label{sec:scalability,_benchmarking,__and__real-world_applications}



\subsection{Scaling GNNs to Large Graphs}
\label{sec:6_1_scaling_gnns_to_large_graphs}

The application of Graph Neural Networks (GNNs) to massive, real-world graphs, often containing billions of nodes and edges, presents significant computational and memory bottlenecks. Overcoming these limitations is critical for deploying GNNs in web-scale recommender systems, social networks, and other large-scale applications. Researchers have developed a diverse array of techniques, encompassing sophisticated graph sampling strategies, graph condensation, and distributed computing frameworks, to enable efficient training and inference at scale.

A primary approach to address the scalability challenge involves intelligently sampling subgraphs or neighborhoods to reduce the computational burden. A seminal work in this area is PinSage [ying20189jc], which successfully scaled Graph Convolutional Networks (GCNs) to Pinterest's web-scale recommender system with billions of nodes and edges. PinSage introduced several innovations, including on-the-fly convolutions that dynamically construct computation graphs for mini-batches by sampling node neighborhoods using biased random walks, and an efficient producer-consumer architecture for data loading. For inference, it leveraged a MapReduce pipeline to generate embeddings for billions of nodes in a distributed manner, demonstrating the crucial role of system-level engineering. Building on the concept of efficient propagation, the Approximate Personalized Propagation of Neural Predictions (APPNP) model [klicpera20186xu] offers a scalable solution for deeper GNNs by decoupling feature transformation from propagation. APPNP approximates Personalized PageRank via power iteration, achieving linear time complexity $O(mK)$ (where $m$ is edges, $K$ is iterations) and enabling information flow over large receptive fields without the quadratic complexity of its predecessor. Similarly, for tasks like link prediction, the SEAL framework [zhang2018kdl] demonstrated that high-order heuristics can be accurately approximated from small $h$-hop enclosing subgraphs, providing a theoretical justification for processing localized subgraphs rather than the entire graph, thus reducing the computational scope. Moving beyond traditional convolutional paradigms, the Random Walk with Unifying Memory (RUM) neural network [wang2024oi8] proposes an entirely non-convolutional GNN that processes information via random walks and RNNs. RUM is inherently scalable, compatible with mini-batching, and boasts a runtime complexity agnostic to the number of edges, offering a distinct path to efficiency.

Beyond sampling, another powerful strategy for scaling GNNs is graph condensation, which aims to synthesize smaller, representative graphs. Graph Condensation (GCOND) [jin2021pf0] addresses the immense storage and training time challenges of large graphs by learning a small, synthetic, yet highly informative graph that preserves the GNN's predictive performance. GCOND employs a gradient matching scheme to ensure that GNNs trained on the condensed graph mimic the training trajectory on the original large graph, and innovatively parameterizes the condensed graph structure as a function of its condensed node features, significantly reducing parameter complexity. Complementing this, Iterative Deep Graph Learning with Anchors (IDGL-ANCH) [chen2020bvl] focuses on learning optimal graph structures for GNNs at scale. By introducing an anchor-based approximation, IDGL-ANCH reduces the computational complexity of graph learning from $O(n^2)$ to $O(ns)$ (where $s$ is the number of anchors), making it feasible for large graphs where initial structures might be noisy or incomplete. While not solely focused on scaling, the Dual-channel Diffused Propagation then Transformation (D2PT) framework [liu2023v3e] also contributes to handling large, potentially incomplete graphs by learning a "global graph" to connect stray nodes and enable long-range propagation, thereby enhancing robustness and information flow in challenging large-scale scenarios.

Architectural and system-level innovations are crucial for overcoming these practical limitations. As exemplified by PinSage's distributed MapReduce inference [ying20189jc], leveraging distributed computing frameworks is essential for processing graphs with billions of nodes. The pervasive nature of scalability challenges is further highlighted by research into adversarial robustness at scale. For instance, methods like Projected Randomized Block Coordinate Descent (PR-BCD) and Greedy R-BCD (GR-BCD) [geisler2021dcq] were developed to enable adversarial attacks on GNNs for graphs with millions of nodes by maintaining sparsity-aware, memory-efficient optimization, underscoring the necessity for scalable operations across all GNN functionalities.

In conclusion, scaling GNNs to large graphs requires a multi-faceted approach. Techniques like neighbor sampling (e.g., PinSage, APPNP, SEAL, RUM) efficiently reduce the effective computational graph, while graph condensation (e.g., GCOND) and scalable structure learning (e.g., IDGL-ANCH) create smaller, representative graphs or optimize connectivity for large-scale processing. These architectural and algorithmic innovations, coupled with robust system-level support and distributed computing, are crucial for enabling GNNs to operate effectively and efficiently in real-world, web-scale applications. However, the trade-off between model expressivity and computational efficiency remains an active area of research, continually pushing the boundaries of what GNNs can achieve on truly massive datasets.
\subsection{Standardized Benchmarking and Evaluation}
\label{sec:6_2_st_and_ardized_benchmarking__and__evaluation}


The rapid proliferation of Graph Neural Network (GNN) architectures and applications has underscored a critical need for rigorous and standardized evaluation practices. Historically, the GNN research landscape suffered from a lack of consistent experimental protocols and the prevalence of non-discriminative datasets, which collectively hindered fair comparisons between models and obfuscated truly impactful architectural advancements. Early work, such as the study by \textcite{klicpera20186xu}, already highlighted this issue, observing that many reported performance improvements of GNN models "vanish" under careful statistical scrutiny, emphasizing the necessity for a more stringent evaluation methodology. Similarly, the successful deployment of GNNs in large-scale industrial applications, exemplified by PinSage for web-scale recommender systems \textcite{ying20189jc}, implicitly demanded robust and scalable evaluation frameworks that could reflect real-world complexities.

To address these challenges, the GNN community has witnessed a concerted effort to develop comprehensive benchmarking frameworks and specialized datasets. Early contributions focused on creating rich data resources, such as the large pre-training datasets introduced by \textcite{hu2019r47} to overcome data scarcity and facilitate transfer learning in scientific domains. More recently, specialized datasets have emerged to provide diverse and discriminative testbeds for GNN models in specific application areas. For instance, \textcite{cui2022mjr} introduced BrainGB, a benchmark specifically tailored for brain network analysis, which addresses the unique characteristics of neuroimaging data. In the domain of critical infrastructure, \textcite{varbella20242iz} developed PowerGraph, a power grid benchmark dataset that uniquely offers empirical ground-truth explanations for cascading failure events, enabling the rigorous evaluation of GNN explainability methods.

The development of overarching benchmarking frameworks has been pivotal in standardizing evaluation across a broad spectrum of GNN tasks and models. The Open Graph Benchmark (OGB) [hu2020] (not explicitly in provided summaries but a key framework for context) and related initiatives have played a crucial role. A significant contribution in this area is the comprehensive, open-source benchmarking framework introduced by \textcite{dwivedi20239ab}. This framework directly tackles the "lack of consistent evaluation protocols and discriminative datasets" by proposing a standardized experimental setting. It features a diverse collection of 12 medium-scale datasets, encompassing both real-world graphs (e.g., ZINC, AQSOL, OGB-COLLAB) and mathematically constructed graphs (e.g., PATTERN, CLUSTER, CSL, CYCLES) designed to test specific theoretical properties and differentiate GNN performance. Crucially, it enforces consistent experimental protocols, including fixed parameter budgets (e.g., 100k and 500k parameters) and standardized metrics, to ensure fair comparisons between different GNN architectures. This initiative has already proven instrumental, facilitating the discovery of Graph Positional Encoding (PE) using Laplacian eigenvectors as a crucial component for enhancing GNN performance, particularly on graphs lacking canonical positional information \textcite{dwivedi20239ab}. Complementing this, the GraphGym platform by \textcite{you2020drv} provides a modular infrastructure for systematically studying the GNN design space and promoting standardized, reproducible evaluation. Furthermore, \textcite{li2023o4c} critically examined evaluation practices for link prediction, identifying common pitfalls and proposing a new benchmarking framework that employs a Heuristic Related Sampling Technique (HeaRT) to generate more challenging and realistic negative samples, thereby enabling a more robust assessment of link prediction models.

Beyond predictive accuracy, the maturation of GNN research necessitates rigorous evaluation of trustworthiness aspects, including robustness, fairness, and explainability. \textcite{zhang20222g3} provides a comprehensive survey of trustworthy GNNs, highlighting that each of these dimensions demands specialized and rigorous evaluation protocols. The assessment of GNN robustness, for instance, has evolved significantly. Initial works demonstrated GNN vulnerabilities to adversarial attacks, such as global poisoning attacks via meta-learning \textcite{zgner2019bbi} and topology attacks using optimization perspectives \textcite{xu2019l8n}. While defenses like GNNGuard \textcite{zhang2020jrt} and Elastic GNNs \textcite{liu2021ee2} were proposed, a critical meta-analysis by \textcite{mujkanovic20238fi} revealed that many existing GNN defenses are not robust against *adaptive attacks*. This work underscored the vital importance of evaluating defenses against adversaries who are aware of the defense mechanism, pushing for more sophisticated and realistic evaluation protocols to truly assess GNN security. Efforts to evaluate robustness at scale, as demonstrated by \textcite{geisler2021dcq}, further contribute to this rigorous assessment. Similarly, the evaluation of fairness in GNNs, addressed by works like \textcite{dai2020p5t}, \textcite{dong202183w}, \textcite{wang2022531}, and \textcite{li20245zy}, requires specific metrics and protocols to measure and mitigate bias in node classification and other tasks. The interpretability of GNNs also demands careful evaluation. While early methods like GNNExplainer \textcite{ying2019rza} and XGNN \textcite{yuan20208v3} provided instance-level and model-level explanations, respectively, recent work by \textcite{chen2024woq} critically examined the faithfulness of interpretable GNNs, identifying "Subgraph Multilinear Extension (SubMT) approximation failure" and proposing new fidelity measures. This is complemented by methods like \textcite{bui2024zy9}'s structure-aware interaction index, which aims for more accurate and comprehensive explanations. Furthermore, the need for reliable confidence scores in GNN predictions, particularly given the observed under-confidence in GNNs, has led to the development of calibration methods like CaGCN \textcite{wang20214ku}, which also require rigorous evaluation.

These initiatives are crucial for addressing the historical lack of consistent evaluation practices and the prevalence of non-discriminative datasets. By fostering a culture of rigorous benchmarking, using diverse and discriminative testbeds, and adhering to consistent experimental protocols, the GNN community can accelerate the identification of truly impactful architectural advancements, ensure the trustworthiness of models, and foster reproducible research. However, ongoing challenges remain, including the continuous need for larger, more complex, and dynamic benchmarks that reflect real-world data evolution, and the development of even more sophisticated adaptive evaluation protocols to keep pace with emerging GNN capabilities and potential risks.
\subsection{Diverse Applications of Graph Neural Networks}
\label{sec:6_3_diverse_applications_of_graph_neural_networks}

Graph Neural Networks (GNNs) have emerged as a pivotal technology, extending far beyond academic research to demonstrate profound practical utility and transformative potential across an increasingly diverse array of real-world domains. Their inherent ability to effectively model intricate relational data and leverage structural information has enabled significant breakthroughs in solving complex problems, driving innovation across various scientific and industrial sectors. The success of GNNs stems from their capacity to learn rich, distributed representations directly from graph topology, making them uniquely suited for tasks where entities and their relationships are paramount.

In the realm of \textbf{recommender systems}, GNNs have proven exceptionally effective due to the inherently graph-structured nature of user-item interactions. These systems often involve bipartite graphs of users and items, or more complex social graphs. Pioneering work like PinSage demonstrated the application of GNNs for web-scale recommendation at Pinterest, showcasing their utility in processing massive graphs to generate high-quality item embeddings [ying20189jc]. Further, GraphRec leveraged GNNs for social recommendation by integrating dual graphs (user-item and user-user) and employing attention mechanisms to capture heterogeneous social relations and opinions, significantly enhancing recommendation quality by modeling the influence of social ties [fan2019k6u]. For sequential recommendation, where dynamic user preferences are crucial, SURGE utilized GNNs to construct item-item interest graphs from interaction sequences, employing attention and dynamic graph pooling to extract activated core preferences [chang2021yyt]. A comprehensive review further highlights the critical challenges and advancements of GNNs in recommender systems, spanning graph construction, network design, and computational efficiency [gao2022f3h].

Beyond recommendation, GNNs have significantly improved \textbf{link prediction accuracy}, a fundamental task in network analysis critical for inferring missing connections or predicting future interactions. The SEAL framework, for instance, learned high-order features from local enclosing subgraphs using GNNs, providing a theoretical justification for approximating complex heuristics from localized information and outperforming traditional methods [zhang2018kdl]. GNNs are also crucial for \textbf{analyzing complex social networks}, where they can model influence propagation, facilitate community detection, and track information diffusion by learning robust node embeddings that reflect structural roles and relationships.

In \textbf{molecular science and scientific discovery}, GNNs are revolutionizing the prediction of molecular properties, accelerating drug discovery, and advancing physical simulations. GNNs excel here by treating molecules as graphs where atoms are nodes and bonds are edges. Early advancements with k-GNNs demonstrated their ability to capture higher-order graph structures (e.g., triangles, cliques), leading to significant improvements in molecular property prediction tasks [morris20185sd]. Further enhancing this, E(3)-equivariant GNNs like NequIP and EGNNs have emerged, which inherently respect the physical symmetries (rotations, translations) of molecules. This property leads to remarkable data efficiency and state-of-the-art accuracy in learning interatomic potentials and modeling N-body systems, crucial for tasks like quantum chemistry and materials design [batzner2021t07, satorras2021pzl]. GemNet, a universal directional GNN, further pushed boundaries by explicitly incorporating dihedral angles and two-hop message passing for highly accurate molecular property predictions [klicpera20215fk]. Beyond molecules, GNNs are also applied to identify high-dimensional Hamiltonian systems and their dynamics, leveraging symplectic maps combined with permutation equivariance [varghese2024ygs]. A broader review underscores the pervasive use of GNNs in chemistry and materials science, highlighting their role in property prediction, inverse design, and accelerating simulations [reiser2022b08].

The financial sector benefits immensely from GNNs' ability to model complex transactional and institutional relationships for \textbf{financial fraud detection and risk management}. GNNs can represent financial entities (users, merchants, banks) and their interactions (transactions) as a graph, enabling the detection of anomalous patterns indicative of fraud. Quantum Graph Neural Networks (QGNNs) have been proposed to leverage quantum computing for more efficient fraud detection, outperforming classical GNNs on real-world datasets [innan2023fa7]. More advanced approaches, like Causal Temporal Graph Neural Networks (CaT-GNN), enhance credit card fraud detection by integrating causal invariant learning to reveal inherent correlations and improve robustness against evolving fraud patterns [duan2024que]. GNNs are also vital for identifying economic risks by capturing multi-level and dynamically changing relationships in financial networks, thereby helping institutions and regulators maintain financial system stability [zhang2024ctj].

In \textbf{critical infrastructure management}, GNNs are instrumental for urban computing, power systems, and communication networks. For \textbf{urban computing}, GNNs model complex spatio-temporal data, such as road networks and traffic flows. Google Maps successfully deployed a GNN-based estimator for Estimated Time of Arrival (ETA) prediction, leveraging road network topology and sophisticated featurization to significantly reduce prediction errors [derrowpinion2021mwn]. For multivariate time series forecasting, GNNs can automatically extract latent spatial dependencies among variables, integrating external knowledge to capture intricate spatio-temporal patterns [wu2020hi3]. A comprehensive survey on Spatio-Temporal GNNs (STGNNs) in urban computing further details their application in traffic prediction, environmental monitoring, and public safety [jin2023e18]. In \textbf{power systems}, GNNs are applied for fault scenario identification, power flow calculation, and stability analysis by modeling the grid as a graph of generators, loads, and transmission lines [liao202120x, zhao2024aer]. The PowerGraph benchmark provides a dedicated dataset for training and evaluating GNNs for these critical tasks, including cascading failure prediction [varbella20242iz]. GNNs are even being explored for power control in 6G in-factory subnetworks, optimizing transmit power by representing the network as a graph [abode2024m4z]. For \textbf{communication networks}, GNNs enhance fault scenario identification by integrating propositional logic rules with graph learning to improve accuracy in diagnosing network issues [zhao2024aer].

GNNs are also making significant strides in \textbf{information processing} domains like natural language processing (NLP), knowledge graphs, and computer vision. In \textbf{NLP}, GNNs are increasingly applied to tasks like text classification by transforming text into graph structures (e.g., word-document graphs), capturing global and contextual-aware word relations that traditional sequential models often miss [wang2023wrg]. For \textbf{knowledge graphs} (KGs), which represent factual information among entities and relations, GNNs are fundamental for tasks such as link prediction (knowledge graph completion), knowledge graph alignment, and reasoning, by learning robust embeddings for entities and relations [ye20226hn]. In \textbf{computer vision}, GNNs handle irregular data types like point clouds, meshes, or object relationships in images and videos. They are used for tasks such as 3D object detection, semantic segmentation, and classification, offering a powerful way to process non-Euclidean visual data [chen2022mmu, li2024yyl].

Furthermore, GNNs are advancing \textbf{network neuroscience}, where they analyze brain connectivity data for tasks like disease diagnosis and synthesizing missing data, effectively preserving the non-Euclidean topological properties of brain graphs [bessadok2021bfy]. The BrainGB benchmark provides a standardized platform for developing and evaluating GNNs for brain network analysis [cui2022mjr]. GNNs are also being applied to \textbf{combinatorial optimization (CO)} problems, where they can learn to guide heuristic search, improve exact solvers, or even perform end-to-end algorithmic reasoning, leveraging their permutation invariance and sparsity awareness [cappart2021xrp].

The widespread adoption of GNNs across these diverse fields underscores their versatility and power. By effectively mapping real-world problems into graph structures and leveraging their unique ability to learn from relational data, GNNs continue to drive innovation and provide sophisticated solutions across an ever-expanding array of scientific and industrial applications.


### Conclusion and Future Directions

\section{Conclusion and Future Directions}
\label{sec:conclusion__and__future_directions}



\subsection{Summary of Key Advancements}
\label{sec:7_1_summary_of_key_advancements}


The trajectory of Graph Neural Network (GNN) research represents a profound evolution, transforming the landscape of machine learning on graph-structured data from nascent theoretical constructs to a powerful, versatile, and increasingly trustworthy paradigm. This remarkable progression, as comprehensively reviewed by [wu2022ptq], has been driven by a continuous cycle of identifying fundamental challenges, developing innovative solutions, and rigorously evaluating their impact.

The field's intellectual journey began with foundational theoretical models, conceptualizing GNNs as iterative processes that propagate information to learn stable node representations. Pioneering works by [Gori05] and [Scarselli09] established the mathematical groundwork, viewing GNNs through the lens of fixed-point iteration. While groundbreaking, these early models were often computationally intensive and lacked scalability. A pivotal shift occurred with the emergence of the message-passing paradigm, which simplified and popularized GNNs, making them practical for a wider array of applications. Key breakthroughs included the Graph Convolutional Network (GCN) by [Kipf17], which offered a spectral simplification for efficient semi-supervised learning, and GraphSAGE by [Hamilton17], enabling inductive learning on large graphs through efficient neighbor sampling. The introduction of Graph Attention Networks (GATs) by [Velickovic18] further enhanced expressivity by allowing models to learn adaptive importance weights for neighboring nodes, addressing the limitations of fixed aggregation schemes. These architectures formed the bedrock for subsequent advancements, demonstrating the power of localized information aggregation.

As GNNs gained prominence, researchers delved deeper into their theoretical capabilities and limitations, particularly concerning their expressive power. The connection to the Weisfeiler-Lehman (WL) graph isomorphism test, notably formalized by [Xu19], provided a crucial benchmark, revealing that many standard GNNs are upper-bounded by the 1-WL test in their ability to distinguish non-isomorphic graphs. This understanding spurred efforts to enhance expressivity through higher-order GNNs, such as k-GNNs [morris20185sd], which operate on k-tuples of nodes to capture richer structural information. Concurrently, theoretical analyses extended to spectral GNNs, with works like [wang2022u2l] challenging assumptions about the necessity of nonlinearity and proving conditions for universal approximation in linear spectral models, further deepening the understanding of GNN capabilities. The pursuit of deeper GNN architectures, however, exposed new challenges like over-smoothing, where repeated message passing homogenizes node representations, and over-squashing, which impedes long-range information propagation. Solutions ranged from decoupling propagation from prediction, as seen in PPNP/APPNP [klicpera20186xu], to incorporating residual connections and developing non-convolutional designs like RUM [wang2024oi8], all aimed at enabling effective learning in deeper layers. For specialized domains, equivariant GNNs, exemplified by E(n) Equivariant GNNs (EGNNs) [satorras2021pzl], emerged to respect inherent symmetries in physical and geometric data, crucial for fields like molecular modeling.

The transition to real-world deployment highlighted the critical need for GNNs to be robust, adaptable, and trustworthy. Real-world graphs often deviate from ideal assumptions, exhibiting heterophily (connections between dissimilar nodes) or possessing incomplete and noisy structures. This led to the development of adaptive filtering mechanisms, node-wise expert mixtures [han2024rkj], and methods for jointly learning graph structures and node embeddings [chen2020bvl], significantly improving GNN performance on complex, imperfect data. A critical area of focus has been the trustworthiness of GNNs, encompassing robustness against adversarial attacks, fairness in predictions, and privacy protection. Surveys like [dai2022hsi] and [zhang20222g3] underscore the unique vulnerabilities of GNNs due to their relational nature, where small perturbations or biases can propagate widely. Research has addressed various attack types (e.g., poisoning, evasion) and defenses (e.g., robust aggregation, adversarial training), while also tackling fairness issues through debiasing techniques [dai2020p5t] and privacy concerns like link stealing [he2020kz4] through differential privacy and anonymization. The interconnectedness of these trustworthiness dimensions—where explainability can aid in debugging biases or adversarial vulnerabilities—is a growing area of focus [dai2022hsi].

Recent advancements have also focused on enhancing GNN utility through advanced learning paradigms, particularly pre-training and prompt-based learning. Addressing the common challenge of labeled data scarcity, self-supervised and generative pre-training strategies have emerged, allowing GNNs to learn robust and transferable representations from abundant unlabeled graph data [hu2019r47, hu2020u8o]. This paradigm has been further refined by prompt-based learning, which enables efficient adaptation of pre-trained GNNs to diverse downstream tasks with minimal fine-tuning, often by reformulating tasks or introducing learnable prompts [sun2022d18, liu2023ent]. This approach significantly minimizes the "objective gap" between pre-training and downstream tasks, facilitating few-shot and even zero-shot generalization. Concurrently, the critical need for interpretability has driven the development of methods to explain GNN decisions, fostering trust and enabling debugging. Techniques range from instance-level explanations identifying influential subgraphs [ying2019rza] to model-level approaches uncovering general patterns [yuan20208v3], though recent critiques highlight ongoing challenges in ensuring the faithfulness of these explanations [chen2024woq].

Finally, the maturation of the field is evident in dedicated efforts towards scalability, standardized benchmarking, and the proliferation of real-world applications. Techniques like graph sampling [Hamilton17, ying20189jc] and graph condensation [jin2021pf0] have been crucial for scaling GNNs to massive graphs with billions of nodes and edges. The establishment of comprehensive benchmarking frameworks, such as the Open Graph Benchmark (OGB) [Hu20] and other specialized datasets [dwivedi20239ab, varbella20242iz], has instilled scientific rigor, enabling fair comparisons and accelerating the identification of truly impactful architectural advancements. These collective efforts have solidified GNNs' practical relevance across a diverse array of domains, from recommender systems [fan2019k6u] and molecular science to urban computing and cybersecurity [mitra2024x43], demonstrating their transformative potential in solving complex relational problems.

In summary, the journey of GNN research has been one of continuous innovation, moving from abstract theoretical models to highly practical, expressive, and increasingly trustworthy architectures. This progression has been characterized by a deep interplay between theoretical insights into expressivity and depth, and practical exigencies arising from real-world data imperfections and ethical considerations. The development of advanced learning paradigms like pre-training and prompt tuning, coupled with robust benchmarking and scalability solutions, has cemented GNNs as an indispensable tool for complex relational reasoning. These collective advancements have not only addressed fundamental challenges but have profoundly transformed the landscape of machine learning on graph-structured data, establishing GNNs as a powerful and versatile paradigm for complex relational reasoning.
\subsection{Open Challenges and Research Gaps}
\label{sec:7_2_open_challenges__and__research_gaps}

Despite the remarkable advancements in Graph Neural Networks (GNNs), several critical unresolved issues and fundamental research gaps persist, defining the frontiers of the field. These challenges often involve inherent trade-offs between expressive power, scalability, and trustworthiness, continuously driving innovation towards the next generation of GNN research.

One significant challenge lies in developing truly universal pre-training objectives that generalize across vastly different domains. Early efforts, such as those by [hu2019r47], systematically investigated strategies for pre-training GNNs using self-supervised node- and graph-level tasks like context prediction and attribute masking to mitigate negative transfer. Building on this, [hu2020u8o] introduced GPT-GNN, a generative pre-training framework that explicitly models the dependency between node attributes and graph structure, offering a more robust initialization. Further, [lu20213kr] leveraged meta-learning to optimize GNN pre-training for rapid adaptation, directly addressing the objective divergence between pre-training and fine-tuning. However, these methods often require careful task reformulation or are tailored to specific graph types. More recent work, like [sun2022d18]'s GPPT and [liu2023ent]'s GraphPrompt, explored prompt-based tuning to bridge the pre-training and downstream task gap by reformulating tasks or prompting the readout function. While [li202444f] extended this to multi-modal settings, aligning GNNs with Large Language Models for semantic understanding, the core challenge remains in designing pre-training objectives that are intrinsically universal and adaptable without extensive domain-specific engineering or complex prompt design, especially for out-of-distribution scenarios.

Achieving robust defenses against increasingly sophisticated adaptive attacks is another pressing concern. Initial work by [zhang2020jrt] proposed GNNGuard, a defense mechanism that prunes suspicious edges based on neighbor importance and layer-wise graph memory. Similarly, [xu2019l8n] introduced an optimization-based adversarial training framework, leveraging convex relaxation to generate topology attacks and train robust GNNs. However, a critical meta-analysis by [mujkanovic20238fi] revealed that most existing GNN defenses are not robust against adaptive attacks, often performing no better than undefended baselines when the adversary has knowledge of the defense mechanism. This highlights a significant gap between perceived and actual robustness. Furthermore, [geisler2021dcq] demonstrated that even advanced attacks can be scaled to graphs with millions of nodes, underscoring the need for defenses that are both effective and scalable. The development of truly robust GNNs requires a paradigm shift towards defenses that can withstand adaptive, white-box adversaries, potentially through certified robustness or fundamentally new architectural designs.

Designing intrinsically interpretable GNNs that offer faithful explanations is paramount for their deployment in high-stakes domains. [ying2019rza]'s GNNExplainer pioneered instance-level explanations by identifying influential subgraphs and node features. Moving towards model-level insights, [yuan20208v3]'s XGNN utilized reinforcement learning to generate graph patterns that maximize specific GNN predictions, while [wang2024j6z] explored unveiling global interactive patterns across graphs via coarsening. [bui2024zy9] further refined instance-level explanations using structure-aware interaction indices. Despite these advancements, a significant interpretability gap was exposed by [chen2024woq], which theoretically proved the "Subgraph Multilinear Extension (SubMT) approximation failure" of many attention-based interpretable GNNs. This implies that their explanations are often unfaithful and do not reliably generalize, highlighting a critical need for GNNs that are not only interpretable but also provably faithful in their explanations.

Handling dynamic and heterogeneous graphs more effectively remains a complex challenge. While [zheng2022qxr] provided a comprehensive survey on GNNs for heterophilous graphs, and [ma2021sim] challenged the strict homophily assumption, the issue of mixed homophilic and heterophilic patterns within a single graph persists [mao202313j]. Solutions like [luan202272y]'s Adaptive Channel Mixing and [han2024rkj]'s NODE-MOE (Node-wise Filtering via Mixture of Experts) adaptively apply filters, but a unified approach for dynamically evolving heterogeneous structures is still elusive. For temporal graphs, [longa202399q] surveyed the state-of-the-art and identified numerous open challenges, while [chang2021yyt]'s SURGE dynamically constructed item-item graphs for sequential recommendation, albeit in a domain-specific manner. Furthermore, real-world graphs often suffer from weak information (missing structure, features, or labels), which [liu2023v3e]'s D2PT framework addressed with dual-channel propagation. The goal is to develop GNNs that can adaptively learn optimal structures and propagation rules for graphs that are simultaneously dynamic, heterogeneous, and incomplete, without relying on extensive manual tuning or domain-specific heuristics.

Finally, scaling to even larger and more complex real-world systems with billions of nodes and edges presents persistent computational hurdles. Pioneering work like [ying20189jc]'s PinSage successfully scaled GCNs to web-scale recommender systems through innovations like on-the-fly convolutions and importance pooling. [klicpera20186xu]'s APPNP addressed oversmoothing and complexity by decoupling prediction and propagation, enabling deeper GNNs without increasing model parameters. [jin2021pf0]'s GCOND introduced graph condensation, drastically reducing graph size while preserving GNN performance. More recently, [wang2024oi8]'s RUM proposed a non-convolutional GNN that is theoretically more expressive and scalable by processing random walk trajectories with RNNs, mitigating over-smoothing and over-squashing. While benchmarks like [dwivedi20239ab]'s BrainGB and PowerGraph [varbella20242iz] provide valuable resources, they often focus on medium-scale datasets, highlighting the gap for truly massive, real-world graphs. The challenge extends beyond merely optimizing existing GNNs; it necessitates designing fundamentally new architectures that inherently scale to unprecedented sizes while maintaining expressive power and efficiency.

These open challenges underscore the inherent trade-offs between expressive power (e.g., [xu2018c8q], [morris20185sd]), scalability ([ying20189jc], [jin2021pf0]), and trustworthiness (including robustness, interpretability, fairness, and privacy, as highlighted by [zhang20222g3], [dai2020p5t], [dong202183w], [dong2021qcg], [he2020kz4], [li20245zy], [wu2022vcx], [zgner2019bbi]). Addressing these intricate relationships and developing GNNs that can navigate these trade-offs effectively continues to drive innovation and define the next generation of GNN research.
\subsection{Ethical Considerations and Societal Impact}
\label{sec:7_3_ethical_considerations__and__societal_impact}


The rapid advancement of Graph Neural Networks (GNNs) presents a dual narrative of immense societal benefit and profound ethical challenges, necessitating a comprehensive examination of their broader implications. GNNs are uniquely positioned to address grand societal challenges by leveraging the intricate relationships inherent in complex data. In healthcare, GNNs are instrumental in accelerating drug discovery by modeling molecular structures [zhang2021jqr], enhancing our understanding of brain function and neurological disorders through advanced brain network analysis [bessadok2021bfy, cui2022mjr], and facilitating disease diagnosis, as supported by benchmarks like \textit{BrainGB} [cui2022mjr]. Beyond medicine, GNNs contribute to climate modeling through sophisticated time series forecasting [jin2023ijy], enhance the resilience of critical infrastructure via applications in power grid management using datasets like \textit{PowerGraph} [varbella20242iz], and enable smarter urban environments and health monitoring in Internet of Things (IoT) applications [dong20225aw]. Their utility also extends to web-scale recommender systems, exemplified by \textit{PinSage} [ying20189jc], and bolstering defensive cyber operations by identifying complex threat patterns [mitra2024x43].

However, the transformative power of GNNs in these high-stakes domains underscores the paramount importance of developing and deploying them responsibly. A holistic framework for trustworthy GNNs, encompassing robustness, explainability, privacy, fairness, accountability, and environmental well-being, is crucial, acknowledging the unique challenges posed by graph data [zhang20222g3, dai2022hsi]. The ethical pitfalls are not merely technical hurdles but translate directly into societal risks, demanding careful consideration.

In **healthcare and personalized medicine**, the promise of GNNs is tempered by significant ethical concerns. The highly sensitive nature of patient data makes privacy a critical issue. As discussed in Section 4.3, GNNs are vulnerable to "link stealing attacks" that can infer relationships, or node attribute inference, potentially exposing private medical conditions or social connections [he2020kz4]. Federated GNNs (FedGNNs) offer a promising technical solution to enhance data privacy by enabling collaborative model training without centralizing sensitive data [liu2022gcg]. Furthermore, GNNs trained on biased medical datasets could lead to discriminatory diagnoses or treatment recommendations for certain demographic groups, amplifying existing health disparities. The lack of transparent reasoning (explainability) in GNN predictions, as detailed in Section 5.3, poses a significant barrier to trust in medical applications, where clinicians need to understand *why* a diagnosis or treatment is suggested. While methods like \textit{GNNExplainer} [ying2019rza] and \textit{MAGE} [bui2024zy9] aim to provide instance-level explanations, critical evaluations reveal that many attention-based interpretable GNNs suffer from approximation failures, leading to unfaithful explanations [chen2024woq, agarwal2022xfp]. This highlights the need for methods that discover invariant, causal rationales to ensure explanations are stable and reflect true underlying mechanisms [wu2022vcx].

For **critical infrastructure and cybersecurity**, the reliability and robustness of GNNs are paramount. The deployment of GNNs in power grid management or threat detection systems, while beneficial, introduces vulnerabilities. As elaborated in Section 4.2, GNNs are highly susceptible to adversarial attacks that subtly perturb graph structure or features, leading to catastrophic performance degradation [zhang2020jrt, zgner2019bbi, xu2019l8n]. Such attacks could compromise the stability of power grids or allow malicious actors to evade detection, with potentially devastating societal consequences. While defenses like \textit{GNNGuard} [zhang2020jrt] exist, a critical assessment reveals that many proposed GNN defenses are not robust against adaptive attacks, leading to overly optimistic security estimates [mujkanovic20238fi]. This lack of guaranteed robustness raises serious questions about accountability when GNN-driven systems fail due to malicious interference.

In **social networks and recommender systems**, GNNs offer personalized experiences but also carry risks of bias amplification and social manipulation. As discussed in Section 4.3, GNNs can inherit and magnify historical biases present in training data, leading to discriminatory outcomes in recommendations or content moderation [dai2020p5t, wang2022531]. This can reinforce stereotypes, create "filter bubbles," and limit exposure to diverse perspectives. Beyond technical bias, the sheer power of GNNs to model and predict social interactions raises concerns about their potential for misuse in mass surveillance, social scoring systems, or the targeted spread of misinformation, posing threats to individual liberties and democratic processes.

Beyond these domain-specific challenges, broader ethical considerations demand attention. The **environmental impact** of training increasingly large and complex GNNs, especially for web-scale applications, is a growing concern. The computational resources and energy consumption required contribute to carbon emissions, linking GNN scalability (Section 6.1) directly to environmental well-being [zhang20222g3]. Furthermore, the **dual-use nature** of GNN technology means that advancements intended for beneficial purposes (e.g., drug discovery) could potentially be repurposed for harmful ones (e.g., designing toxic molecules). This necessitates robust **governance and accountability** frameworks to ensure responsible innovation and deployment, especially given the complex, often opaque nature of GNN models.

In conclusion, while GNNs offer immense potential for societal advancement across diverse critical domains, their ethical deployment hinges on addressing fundamental challenges related to bias, privacy, robustness, transparency, and broader societal impacts. The literature demonstrates a growing commitment to developing GNNs that are not only powerful but also fair, secure, reliable, and interpretable. Future research must continue to bridge the gap between predictive performance and these crucial trustworthiness aspects, focusing on rigorous evaluation, developing theoretically grounded methods for interpretability [yuan2020fnk], and designing inherently robust and privacy-preserving architectures to foster public trust and mitigate unintended negative consequences in an increasingly interconnected world.


