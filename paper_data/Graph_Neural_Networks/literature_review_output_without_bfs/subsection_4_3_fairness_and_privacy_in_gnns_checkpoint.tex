\subsection{Fairness and Privacy in GNNs}
The increasing deployment of Graph Neural Networks (GNNs) in sensitive domains necessitates a rigorous focus on their ethical dimensions, particularly ensuring fair outcomes and protecting sensitive information. The relational nature of graph data presents unique challenges, as information propagation can inadvertently amplify biases or leak sensitive details, making careful design and evaluation paramount for trustworthy GNN systems \cite{zhang20222g3, dai2022hsi}.

Fairness in GNNs is a critical concern, as models can inherit and even exacerbate biases present in graph data and structures. Sources of bias include imbalanced representation of demographic groups, homophily (tendency for similar nodes to connect), and the message-passing mechanism itself, which can propagate and amplify discriminatory patterns \cite{zhang20222g3}. Early efforts to mitigate unfairness, such as FairGNN \cite{dai2020p5t}, addressed the practical challenge of limited sensitive attribute information by employing a GCN-based estimator for missing attributes, combined with adversarial debiasing to ensure predictions are independent of sensitive attributes. This approach demonstrated significant reduction in discrimination while maintaining high accuracy, even with sparse sensitive attribute knowledge. Building on this, \cite{dong2021qcg} shifted the focus to data-centric debiasing by proposing EDITS, a model-agnostic pre-processing framework. EDITS formally defines and mitigates "attribute bias" and "structural bias" directly in the input attributed network using Wasserstein-1 distance-based metrics, optimizing to reduce these biases before GNN training. Further, \cite{wang2022531} identified the critical issue of "sensitive attribute leakage" where GNN feature propagation dynamically alters feature correlations, exacerbating bias. Their FairVGNN framework mitigates this by using a generative adversarial debiasing module to mask sensitive-correlated features and an adaptive weight clamping module to minimize representation differences between sensitive groups.

Beyond group fairness, research has also delved into individual fairness, which ensures that similar individuals receive similar predictions. \cite{dong202183w} addressed this more granular problem by proposing REDRESS, a novel ranking-based framework. This approach refines the notion of individual fairness to overcome the practical difficulties associated with Lipschitz conditions, offering a plug-and-play solution for existing GNN architectures. More recently, \cite{li20245zy} rethought GNN fairness from a re-balancing perspective, introducing FairGB, which combines Counterfactual Node Mixup (CNM) and Contribution Alignment Loss (CAL). CNM generates unbiased ego-networks by interpolating node attributes and neighbor distributions across counterfactual pairs, while CAL is a gradient-based re-weighting method that dynamically balances group contributions during training, achieving state-of-the-art fairness-utility trade-offs. A more fundamental approach to learning unbiased representations is presented by \cite{wu2022vcx}, which proposed Discovering Invariant Rationale (DIR). DIR is an invariant learning strategy for intrinsically interpretable GNNs that identifies causal patterns stable across different data distributions, thereby inherently contributing to fairness by avoiding "shortcut features" and improving out-of-distribution generalization.

In addition to fairness, privacy is a paramount concern in GNNs, especially when dealing with sensitive graph structures or node attributes. The interconnected nature of graphs means that even seemingly innocuous information can be inferred or leaked, making GNNs uniquely vulnerable to various privacy attacks \cite{dai2022hsi}. A significant privacy vulnerability was exposed by \cite{he2020kz4}, which pioneered the study of "link stealing attacks" against GNNs. Their work demonstrated that the underlying graph structure can be inferred from a black-box GNN's outputs, even with minimal adversary knowledge. This research introduced a comprehensive threat model and various attack methodologies (unsupervised, supervised, transferring) that achieved high AUCs, revealing that GNNs inherently encode and implicitly reveal structural information about their training graphs through their message-passing mechanisms. Beyond link stealing, GNNs are also susceptible to "node attribute inference attacks," where sensitive node features (e.g., medical conditions, income) are inferred from a model's outputs or learned representations, and "membership inference attacks," which determine if a specific node was part of the GNN's training dataset \cite{dai2022hsi, zhang20222g3}.

To counter these threats, several privacy-preserving mechanisms have been explored for GNNs. Differential Privacy (DP) is a robust technique that adds carefully calibrated noise to data or model parameters to provide strong privacy guarantees. In GNNs, DP can be applied by perturbing node features, adding noise to gradients during training, or even altering the graph structure (e.g., adding/removing edges) before training \cite{dai2022hsi, zhang20222g3}. However, applying DP to non-Euclidean graph data is challenging, as noise addition can significantly degrade model utility, and the interdependent nature of graph data makes global privacy guarantees difficult to achieve without substantial performance loss. Another promising paradigm is Federated Learning (FL), which enables collaborative training of GNNs across multiple clients without centralizing their raw graph data \cite{liu2022gcg}. In Federated GNNs (FedGNNs), clients train local GNN models on their private subgraphs and only share aggregated model updates with a central server, thus preserving data locality. This approach is particularly relevant for sensitive applications like disease classification, where patient data must remain private \cite{hausleitner2024vw0}. Challenges in FedGNNs include handling graph heterogeneity across clients, managing communication overhead, and ensuring robustness against malicious clients. Traditional graph anonymization techniques (e.g., k-anonymity, l-diversity) can also be applied, but often suffer from significant utility loss or are vulnerable to re-identification attacks in complex graph structures \cite{dai2022hsi}.

The unique challenges posed by the relational nature of graph data mean that fairness and privacy are often intertwined, presenting complex trade-offs. Bias amplification can occur through message passing, and sensitive attribute leakage can be a privacy concern. Critically, privacy-preserving mechanisms like Differential Privacy, while protecting individual data, can sometimes disproportionately affect minority groups, exacerbating existing biases or making them harder to detect and mitigate \cite{dai2022hsi, zhang20222g3}. Conversely, debiasing techniques aimed at achieving fairness might inadvertently reveal more sensitive information about individuals or groups. Building trustworthy GNN systems therefore requires careful consideration of these ethical dimensions, necessitating novel architectural designs and evaluation methodologies that balance predictive performance with robust fairness and privacy guarantees. Future research must continue to explore comprehensive solutions that address multi-modal bias, offer stronger and provable privacy guarantees, and provide certified fairness, especially in the non-IID graph setting. Key open challenges include developing unified frameworks that simultaneously optimize for fairness and privacy without severe trade-offs, designing scalable and efficient privacy-preserving GNNs for massive real-world graphs, and establishing standardized benchmarks and metrics for evaluating the complex interplay between fairness, privacy, and utility in GNNs. This holistic approach is essential for ensuring GNNs are deployed responsibly and ethically.