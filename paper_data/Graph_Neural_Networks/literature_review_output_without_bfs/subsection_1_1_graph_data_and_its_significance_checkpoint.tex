\subsection{Graph Data and its Significance}

Graph-structured data, fundamentally characterized by a collection of nodes (entities) and edges (relationships) that may possess associated attributes, constitutes a powerful and ubiquitous paradigm for modeling complex real-world systems. Formally, a graph $G = (V, E, X, A)$ comprises a set of nodes $V$, a set of edges $E$ connecting pairs of nodes, an optional node feature matrix $X \in \mathbb{R}^{|V| \times d_v}$ where $d_v$ is the dimension of node features, and an optional edge attribute matrix $A \in \mathbb{R}^{|E| \times d_e}$ for edge features. Edges can be directed or undirected, weighted or unweighted, and graphs can be homogeneous (nodes and edges of a single type) or heterogeneous (multiple types of nodes and edges) \cite{wu20193b0, zhang2021jqr}. Common representations include the adjacency matrix, which captures connectivity, and feature matrices for nodes and edges.

The pervasive nature of graphs in modeling intricate interdependencies makes them indispensable across diverse domains. In social sciences, graphs represent social networks, mapping individuals and their friendships, collaborations, or communication patterns \cite{wu2022ptq}. In biology and chemistry, they model molecular structures (atoms as nodes, bonds as edges), protein-protein interaction networks, or gene regulatory pathways \cite{wu20193b0}. Knowledge graphs organize factual information by connecting entities with semantic relationships, forming the backbone of many AI systems. Beyond these, graphs are crucial in transportation networks, recommender systems (users and items), citation networks (papers and citations), and communication infrastructures, offering a rich, holistic representation of relational information that is often lost in simpler data structures \cite{wang2023zr0}. This ability to explicitly encode relationships and context is what makes graph data profoundly significant for understanding complex systems.

However, the inherent characteristics of graph data present unique and substantial challenges for traditional machine learning algorithms, which are primarily designed for Euclidean data (e.g., images as grids, text as sequences) \cite{wu20193b0, wang2023zr0}. The most prominent challenge stems from the **non-Euclidean nature** of graphs. Unlike images or sequences, graphs lack a fixed grid structure, a canonical node ordering, or a global coordinate system. This means that graph data is permutation-invariant; the underlying structure and properties of a graph remain the same regardless of how its nodes are ordered or indexed. Traditional deep learning models like Convolutional Neural Networks (CNNs) rely on local connectivity and translation invariance on grid-like data, while Recurrent Neural Networks (RNNs) assume sequential dependencies. Neither paradigm naturally extends to the irregular, variable-sized, and arbitrarily structured topology of graphs \cite{zhang2021jqr}.

Furthermore, the **interdependence of elements** within a graph is both its strength and a significant challenge. The features and labels of a node are often strongly influenced by its neighbors and the broader graph topology. This relational context is crucial for accurate predictions, yet traditional machine learning models typically assume independent and identically distributed (i.i.d.) data samples. Applying standard classifiers to individual nodes or edges in isolation would ignore this vital relational information, leading to suboptimal performance \cite{wu2022ptq}. Extracting meaningful features from graph structures for traditional machine learning models often requires extensive, handcrafted feature engineering (e.g., centrality measures, graphlet counts), which is labor-intensive, domain-specific, and may not generalize well across different graphs or tasks. This process is also prone to losing rich topological information.

These challenges highlight a critical gap: the need for specialized machine learning models capable of directly processing graph-structured data, learning rich, distributed representations that simultaneously encode both node features and their structural context. This necessity has driven the development of Graph Neural Networks (GNNs). GNNs are designed to overcome the limitations of traditional methods by enabling end-to-end learning directly on graph topology, capturing intricate relational patterns and structural information through message-passing mechanisms \cite{wu20193b0, wang2023zr0}. Understanding these fundamental characteristics of graph data and the inherent difficulties they pose for conventional machine learning is crucial for appreciating why GNNs have emerged as an indispensable and rapidly evolving paradigm for effective analysis and prediction in graph domains. The subsequent sections of this review will delve into how GNNs address these challenges through various architectural advancements and learning paradigms.