{
  "6c96c2d4a3fbd572fef2d59cb856521ee1746789": {
    "seed_title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
    "summary": "\n\n2. *Evolution Analysis:*\n\n**Trend 1: Scaling Graph Neural Networks for Industrial-Strength Applications**\n\n*Methodological progression*: The journey of Graph Neural Networks (GNNs) has been significantly marked by the challenge of scaling these powerful models from academic benchmarks to real-world, web-scale production environments. Early GNNs, while conceptually groundbreaking, were computationally prohibitive, limiting their application to small graphs. Subsequent advancements, particularly in Graph Convolutional Networks (GCNs) and algorithms like GraphSAGE, improved efficiency and introduced inductive capabilities, allowing models to generalize to unseen nodes. However, these methods still operated under the implicit assumption that the entire graph could be stored in memory or accessed with relative ease, a constraint that became a critical bottleneck for graphs with billions of nodes and edges.\n\nThe paper \"[ying20189jc] Graph Convolutional Neural Networks for Web-Scale Recommender Systems (2018)\" represents a pivotal methodological leap in this progression. It introduces PinSage, a novel GCN algorithm that fundamentally re-architects how GNNs interact with massive graphs. Instead of relying on the full graph Laplacian, PinSage performs efficient, localized convolutions by dynamically sampling node neighborhoods and constructing computation graphs on-the-fly for each minibatch. This \"on-the-fly\" approach is a radical departure from previous static, full-graph processing methods. Furthermore, PinSage integrates random walks not merely for sampling, but to derive \"importance scores\" for neighbors, leading to a more sophisticated aggregation strategy called \"Importance Pooling.\" The entire system is designed with production in mind, featuring a producer-consumer architecture for efficient minibatch generation and a MapReduce pipeline for scalable inference, demonstrating a holistic methodological progression towards industrial-grade GNN deployment.\n\n*Problem evolution*: The primary problem addressed by \"[ying20189jc] Graph Convolutional Neural Networks for Web-Scale Recommender Systems (2018)\" is the critical scalability gap that prevented GCNs from being deployed in real-world, web-scale recommender systems. While GCNs showed state-of-the-art performance on smaller datasets, their requirement to operate on the full graph Laplacian or to store the entire graph in memory rendered them infeasible for platforms like Pinterest, which operate on graphs with billions of nodes and edges. Previous solutions, including GraphSAGE, while more inductive, still struggled with the sheer magnitude of such graphs. Unsupervised graph embedding methods like node2vec and DeepWalk were also inadequate, as they could not incorporate rich node features and their parameter counts scaled linearly with graph size, making them prohibitive. PinSage directly tackles these limitations by providing a framework capable of training and inferring embeddings on graphs orders of magnitude larger than previously possible. It also addresses the problem of suboptimal information aggregation in GCNs by introducing importance-based weighting and improves training robustness for massive models through a curriculum training strategy.\n\n*Key innovations*: The breakthrough contributions of \"[ying20189jc] Graph Convolutional Neural Networks for Web-Scale Recommender Systems (2018)\" are numerous and impactful. The core innovation is the PinSage framework itself, which successfully scales GCNs to an unprecedented degree. Key technical innovations include:\n1.  **On-the-fly, Localized Convolutions:** This dynamic approach to constructing computation graphs for minibatches eliminates the need for the full graph Laplacian, making web-scale training feasible.\n2.  **Random Walk-based Neighborhood Sampling:** This method efficiently defines and samples computation graphs while also yielding importance scores for neighbors, a more nuanced approach than simple random sampling.\n3.  **Importance Pooling:** A novel, weighted aggregation function that leverages L1-normalized visit counts from random walks to prioritize more relevant neighbors, leading to significant performance gains (e.g., 46% improvement in offline metrics).\n4.  **Producer-Consumer Minibatch Construction:** An architectural innovation that optimizes CPU-GPU utilization by decoupling CPU-bound tasks (sampling, feature fetching) from GPU-bound tasks (model consumption, gradient computation).\n5.  **Efficient MapReduce Inference Pipeline:** A scalable system designed to generate billions of node embeddings post-training, crucial for practical, large-scale deployment.\n6.  **Curriculum Training:** A strategy that feeds progressively \"harder\" examples during training, enhancing model robustness and convergence (contributing a 12% performance gain).\n\nThese innovations collectively enabled the largest-ever application of deep graph embeddings at the time, demonstrating that GCNs could overcome previous scalability barriers and achieve state-of-the-art performance in real-world recommendation tasks, significantly outperforming existing deep learning and graph-based alternatives. This work provided a blueprint for designing scalable GCNs for massive graphs, paving the way for broader industrial applications.\n\n3. *Synthesis*\nThe unified intellectual trajectory connecting this work is the relentless pursuit of scalability and practical applicability for Graph Neural Networks. \"[ying20189jc] Graph Convolutional Neural Networks for Web-Scale Recommender Systems (2018)\" collectively contributes to advancing GNNs by providing a comprehensive blueprint and a proven methodology for deploying deep graph embeddings in real-world, web-scale production environments, thereby bridging the gap between academic research and industrial demands.",
    "path": [
      "6c96c2d4a3fbd572fef2d59cb856521ee1746789"
    ],
    "layer1_papers": [
      {
        "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
        "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Scaling Graph Convolutional Networks (GCNs) to web-scale recommender systems, which involve billions of items and hundreds of millions of users.\n    *   **Importance and Challenge:** While GCNs achieve state-of-the-art performance on recommender system benchmarks, their deployment in real-world production environments is challenging. Existing GCNs typically require operating on the full graph Laplacian during training, which is infeasible for massive, constantly evolving graphs with billions of nodes and edges due to memory and computational constraints.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon recent advancements in deep learning for graph-structured data, particularly Graph Convolutional Networks (GCNs) and is most closely related to the GraphSAGE algorithm \\cite{ying20189jc}.\n    *   **Limitations of Previous Solutions:**\n        *   Early graph neural networks (e.g., Gori et al., Scarselli et al.) were computationally expensive, requiring message-passing to convergence, limiting them to small graphs (<10,000 nodes).\n        *   Traditional GCNs (spectral and approximation-based) require operating on the *entire graph Laplacian* during training, making them impractical for graphs with billions of nodes and edges.\n        *   GraphSAGE \\cite{ying20189jc} improved inductive capabilities but still implicitly assumed the entire graph could be stored in GPU memory or accessed efficiently, which is not scalable to Pinterest's graph size.\n        *   Graph embedding methods like node2vec and DeepWalk are unsupervised, cannot incorporate node feature information, and have a number of parameters linear with graph size, making them prohibitive for web-scale applications.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces PinSage \\cite{ying20189jc}, a data-efficient Graph Convolutional Network (GCN) algorithm. PinSage combines efficient random walks and graph convolutions to generate embeddings of nodes (items) that incorporate both graph structure and node feature information.\n    *   **Novelty and Differentiation:**\n        *   **On-the-fly Convolutions:** Unlike traditional GCNs that use the full graph Laplacian, PinSage performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph for each minibatch.\n        *   **Producer-Consumer Minibatch Construction:** A CPU-bound producer efficiently samples node neighborhoods and fetches features, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs for stochastic gradient descent, maximizing GPU utilization.\n        *   **Efficient MapReduce Inference:** A designed MapReduce pipeline distributes the trained model to generate embeddings for billions of nodes, minimizing repeated computations.\n        *   **Constructing Convolutions via Random Walks:** Instead of random sampling, PinSage uses short random walks to sample the computation graph, which also provides importance scores for neighbors.\n        *   **Importance Pooling:** A novel aggregation method where neighbor features are weighted based on their L1-normalized visit counts from random walks, improving the quality of aggregated information.\n        *   **Curriculum Training:** A training strategy where the algorithm is fed progressively \"harder\" examples during training to improve model robustness and convergence.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   PinSage \\cite{ying20189jc}: A highly scalable GCN framework specifically designed for web-scale recommender systems.\n        *   Importance-based neighborhood definition and sampling using random walks.\n        *   Importance pooling: A weighted aggregation function for GCNs based on random walk visit counts.\n        *   Curriculum training scheme for GCNs.\n    *   **System Design or Architectural Innovations:**\n        *   Dynamic, on-the-fly computation graph construction for localized convolutions, eliminating the need for the full graph Laplacian.\n        *   Producer-consumer architecture for efficient minibatch generation and GPU utilization.\n        *   MapReduce pipeline for scalable inference of billions of node embeddings.\n    *   **Theoretical Insights or Analysis:** The paper primarily focuses on practical engineering innovations for scalability and empirical performance gains rather than deep theoretical analysis.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   PinSage \\cite{ying20189jc} was developed and deployed at Pinterest, operating on a graph with 3 billion nodes (pins and boards) and 18 billion edges, trained on 7.5 billion examples.\n        *   Evaluated for item-item recommendation (related-pin recommendation) and \"homefeed\" recommendation tasks.\n        *   Validation included extensive offline metrics, controlled user studies, and A/B tests.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Offline Ranking Metrics:** PinSage improved over the best performing baseline by more than 40%.\n        *   **Human Evaluations:** In head-to-head comparisons, PinSage recommendations were preferred about 60% of the time.\n        *   **A/B Tests:** Showed 30% to 100% improvements in user engagement across various settings.\n        *   **Impact of Innovations:** Importance pooling led to a 46% performance gain in offline evaluation metrics, and curriculum training resulted in a 12% performance gain.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The paper primarily addresses the limitations of *prior* GCN approaches rather than explicitly detailing limitations of PinSage itself. It assumes the availability of rich node features (e.g., visual, textual for pins). The approach is tailored for bipartite graphs (pins and boards) but is presented as generalizable to item-context graphs.\n    *   **Scope of Applicability:** Applicable to web-scale recommender systems, content discovery platforms, and any domain requiring scalable deep graph embeddings for massive graph-structured data.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** PinSage \\cite{ying20189jc} represents the largest-ever application of deep graph embeddings to date, successfully scaling GCNs to a production environment with billions of nodes and edges. It demonstrates that GCNs can overcome previous scalability barriers and achieve state-of-the-art performance in real-world recommendation tasks, significantly outperforming existing deep learning and graph-based alternatives.\n    *   **Potential Impact on Future Research:** This work paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures, opening up GCNs for broader industrial applications beyond academic benchmarks. It provides a blueprint for designing scalable GCNs for massive graphs.",
        "year": 2018,
        "citation_key": "ying20189jc"
      }
    ],
    "layer2_papers": [],
    "layer3_papers": [],
    "layer2_summary": null
  },
  "d08a0eb7024dff5c4fabd58144a38031633d4e1a": {
    "seed_title": "Benchmarking Graph Neural Networks",
    "summary": "**Integration Analysis:**\n\nThe new papers from 2024 significantly extend and refine both previously identified trends, demonstrating a rapid evolution towards more sophisticated and robust GNNs.\n\n*   **Trend 1: Breaking the Weisfeiler-Leman (WL) Barrier: Enhancing GNN Expressive Power**\n    *   **Extension/New Branch:** [geisler2024wli] \"Spatio-Spectral Graph Neural Networks (2024)\" introduces a fundamentally new architectural paradigm (spatio-spectral filters) to overcome over-squashing and limited receptive fields, which are direct impediments to expressive power. This offers a distinct approach to long-range interactions compared to previous methods focusing on K-hop aggregation, paths, or subgraphs, and provides a novel way to break the 1-WL barrier.\n    *   **Indirect Relation:** [kang2024fsk] \"Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND (2024)\" introduces a novel mathematical framework (fractional calculus) for continuous GNNs. While primarily addressing robustness (oversmoothing), its ability to model non-local, memory-dependent dynamics inherently enhances the modeling capacity and potentially the expressive power of GNNs in capturing complex temporal or hierarchical graph structures.\n\n*   **Trend 2: Maturation and Robustness: Towards Practical, Reliable, and Interpretable GNNs**\n    *   **Significant Strengthening and Diversification:** This trend is profoundly enriched by the new papers, pushing the boundaries of GNN reliability, interpretability, and transferability.\n        *   **Interpretability (Deeper Theoretical Grounding):** [chen2024woq] \"How Interpretable Are Interpretable Graph Neural Networks? (2024)\" and [bui2024zy9] \"Explaining Graph Neural Networks via Structure-aware Interaction Index (2024)\" provide critical theoretical and methodological advancements. They move beyond merely *providing* explanations to *rigorously ensuring* their faithfulness, structure-awareness, and ability to capture high-order interactions, directly critiquing and refining existing XGNN approaches.\n        *   **Robustness & Generalization (OOD):** [xia20247w9] \"Learning Invariant Representations of Graph Neural Networks via Cluster Generalization (2024)\" offers a novel mechanism for learning invariant representations against \"structure shift,\" a crucial aspect of Out-of-Distribution (OOD) generalization. This complements previous work on causal invariance. [kang2024fsk] also contributes by mitigating oversmoothing, a key robustness challenge.\n        *   **Transferability & Data Efficiency (LLM Integration):** [li202444f] \"Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)\" introduces a groundbreaking paradigm for integrating GNNs with Large Language Models (LLMs) using multi-modal prompt learning, enabling CLIP-style zero-shot generalization with *extremely weak text supervision*. This is a major leap for GNN adaptability and semantic understanding in low-resource settings.\n        *   **Maturity Validation (Review):** [liu20242g6] \"A Review of Graph Neural Networks in Epidemic Modeling (2024)\" serves as a meta-analysis, consolidating the state-of-the-art in a critical application domain. It validates the field's maturity and practical impact, identifying both successes and future challenges.\n\n*   **New Methodological/Conceptual Shifts:**\n    *   **Hybrid Spatial-Spectral Architectures:** The introduction of S2GNNs by [geisler2024wli] marks a significant architectural shift, combining local and global information propagation in a principled way.\n    *   **Fractional Calculus for GNN Dynamics:** [kang2024fsk] introduces a novel mathematical foundation for modeling non-Markovian, memory-dependent graph dynamics.\n    *   **Theoretical Frameworks for XGNNs:** [chen2024woq]'s SubMT framework and [bui2024zy9]'s axiomatic Myerson-Taylor index establish deeper theoretical rigor for interpretability.\n    *   **Multi-modal Prompt Learning for GNN-LLM Alignment:** [li202444f] pioneers a new approach to leverage LLMs for GNNs, specifically designed for data scarcity and weak supervision.\n    *   **Embedding Space Manipulation for Invariance:** [xia20247w9]'s CIT mechanism demonstrates a novel way to achieve OOD robustness by manipulating representations in the embedding space.\n\n*   **Gaps Filled/New Directions:**\n    *   **Over-squashing:** [geisler2024wli] directly addresses this long-standing limitation with a novel architectural solution.\n    *   **Faithful and Structure-Aware Interpretability:** [chen2024woq] and [bui2024zy9] fill the gap of theoretically robust and contextually accurate explanations for GNNs.\n    *   **GNN-LLM Synergy under Weak Supervision:** [li202444f] opens a major new research direction for GNNs to gain semantic understanding and zero-shot capabilities.\n    *   **Non-Markovian Dynamics:** [kang2024fsk] provides a tool to model more complex, memory-dependent processes in graphs.\n\n*   **Connections to Earlier Works:**\n    *   [geisler2024wli] builds on spectral GNNs and the concept of positional encodings ([dwivedi2021af0]) but offers a distinct architectural solution.\n    *   [chen2024woq] and [bui2024zy9] directly advance the interpretability efforts, refining the understanding of \"rationales\" and causal patterns introduced by works like [wu2022vcx].\n    *   [xia20247w9] provides an alternative or complementary approach to invariant learning strategies ([wu2022vcx]) and pre-training for transferability ([lu20213kr]).\n    *   [li202444f] extends the idea of pre-training and transferability ([lu20213kr]) by integrating LLMs, pushing the boundaries of data efficiency.\n\n*   **Overall Narrative Change:** The addition of these papers significantly strengthens the narrative that GNN research is maturing rapidly. It shows a field moving beyond initial theoretical breakthroughs to a phase of deep refinement, addressing fundamental practical challenges with sophisticated theoretical and architectural innovations. The focus is now on making GNNs not just powerful, but also truly reliable, interpretable, adaptable, and capable of integrating with other advanced AI paradigms like LLMs.\n\n**Temporal Positioning:**\nAll new papers are from 2024, representing the absolute latest developments in the field, building directly upon and extending the research trajectory established by the 2020-2023 papers in the previous synthesis. They push the boundaries of GNN capabilities and address emerging challenges.\n\n---\n\n2. *Updated Evolution Analysis:*\n\nThe evolution of Graph Neural Networks (GNNs) continues to be driven by two dominant, interconnected trends: a relentless pursuit of **enhanced expressive power beyond the Weisfeiler-Leman (WL) barrier** and a growing focus on **GNN maturity and robustness for practical, reliable, and interpretable applications**. The latest contributions from 2024 demonstrate a significant deepening and diversification within these trends, introducing novel architectural paradigms, mathematical foundations, and sophisticated interpretability and generalization mechanisms.\n\n*Trend 1: Breaking the Weisfeiler-Leman (WL) Barrier: Enhancing GNN Expressive Power*\n\nThe fundamental limitation of standard Message Passing Neural Networks (MPNNs)—their equivalence to at most the 1-Weisfeiler-Leman (1-WL) test—remains a central problem. Recent work continues to explore novel ways to overcome this, particularly by addressing the challenge of long-range information propagation and over-squashing.\n\n*   **Methodological progression**: Early work, such as [abboud2020x5e] \"The Surprising Power of Graph Neural Networks with Random Node Initialization (2020),\" provided a surprising theoretical breakthrough by proving that MPNNs with Random Node Initialization (RNI) are universal. This was followed by more sophisticated approaches like [balcilar20215ga] \"Breaking the Limits of Message Passing Graph Neural Networks (2021)\" (GNNML3 using spectral methods) and [dwivedi2021af0] \"Graph Neural Networks with Learnable Structural and Positional Representations (2021)\" (LSPE, RWPE). The focus then broadened to understanding and enhancing specific architectural components, with [feng20225sa] \"How Powerful are K-hop Message Passing Graph Neural Networks (2022)\" characterizing K-hop message passing and [bianchi20239ee] \"The expressive power of pooling in Graph Neural Networks (2023)\" extending WL analysis to hierarchical GNNs. Specialized expressivity for different graph types emerged with [joshi20239d0] \"On the Expressive Power of Geometric Graph Neural Networks (2023)\" introducing the Geometric Weisfeiler-Leman (GWL) test. More recently, [michel2023hc4] \"Path Neural Networks: Expressive and Accurate Graph Neural Networks (2023)\" and [zeng20237gv] \"Substructure Aware Graph Neural Networks (2023)\" explored leveraging richer structural information like paths and subgraphs.\n    The latest advancement, [geisler2024wli] \"Spatio-Spectral Graph Neural Networks (2024),\" introduces a novel paradigm by synergistically combining spatially and spectrally parametrized graph filters. This allows for *spatially unbounded* information propagation, directly addressing the over-squashing problem and limited receptive fields in MPNNs. This approach offers a distinct architectural solution to long-range interactions, complementing previous methods that focused on K-hop or explicit subgraph/path encoding.\n*   **Problem evolution**: The initial problem of the 1-WL bottleneck evolved into finding *efficient* ways to surpass it. Subsequent work addressed the expressivity of specific GNN components and extended the framework to specialized graph types. The latest papers tackled encoding richer structural primitives. With [geisler2024wli], the problem has evolved to fundamentally redesign GNN architectures to overcome inherent limitations like over-squashing, which directly impacts the ability to capture global patterns and thus expressivity.\n*   **Key innovations**: The universality proof for RNI, spectral filter design for efficient 3-WL equivalence, learnable and adaptive positional encodings (LSPE, RWPE), theoretical bounds for K-hop GNNs, the Geometric Weisfeiler-Leman (GWL) test, \"annotated sets of paths,\" and novel \"Cut subgraph\" extraction. A significant new innovation is the **Spatio-Spectral Graph Neural Network (S2GNN)** architecture [geisler2024wli], which proposes the first neural network designed to operate directly in the spectral domain, offering a principled way to mitigate over-squashing and enhance expressivity beyond 1-WL with novel, \"free\" positional encodings.\n*   **Integration points**: [geisler2024wli] builds upon the understanding of spectral GNNs and the importance of positional encodings ([dwivedi2021af0]) but introduces a new architectural solution to the expressivity challenge, particularly for long-range interactions, offering an alternative to path-based or K-hop methods.\n\n*Trend 2: Maturation and Robustness: Towards Practical, Reliable, and Interpretable GNNs*\n\nAs GNNs demonstrated increasing expressive power, research simultaneously shifted towards addressing practical challenges, ensuring scalability, reliability, and interpretability, and establishing rigorous evaluation standards. This trend has seen substantial deepening with the latest contributions.\n\n*   **Methodological progression**: Early concerns about GNN reliability emerged with [he2020kz4] \"Stealing Links from Graph Neural Networks (2020).\" To enable deeper models, [li2021orq] \"Training Graph Neural Networks with 1000 Layers (2021)\" introduced Grouped Reversible GNNs. Addressing data efficiency, [lu20213kr] \"Learning to Pre-train Graph Neural Networks (2021)\" proposed L2P-GNN. The need for trustworthy GNNs led to advancements in interpretability, with [wu2022vcx] \"Discovering Invariant Rationales for Graph Neural Networks (2022)\" introducing DIR for causal patterns. The community also recognized the need for standardized evaluation, leading to [dwivedi20239ab] \"Benchmarking Graph Neural Networks (2023)\" and [li2023o4c] \"Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking (2023).\" Review papers like [bessadok2021bfy] \"Graph Neural Networks in Network Neuroscience (2021)\" and [cappart2021xrp] \"Combinatorial optimization and reasoning with graph neural networks (2021)\" reflected GNNs' increasing adoption.\n    The latest papers significantly advance this trend. In interpretability, [chen2024woq] \"How Interpretable Are Interpretable Graph Neural Networks? (2024)\" introduces the **Subgraph Multilinear Extension (SubMT)** framework, proving limitations of existing attention-based XGNNs and proposing **GMT** with random subgraph sampling for more faithful interpretations. Complementing this, [bui2024zy9] \"Explaining Graph Neural Networks via Structure-aware Interaction Index (2024)\" proposes the **Myerson-Taylor interaction index** and **MAGE** explainer, which axiomatically account for graph structure and high-order interactions, identifying both positive and negative influential motifs. For robustness and generalization, [xia20247w9] \"Learning Invariant Representations of Graph Neural Networks via Cluster Generalization (2024)\" introduces the **Cluster Information Transfer (CIT)** mechanism, a plug-in method to learn invariant representations against \"structure shift\" by manipulating node embeddings. Furthermore, [kang2024fsk] \"Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND (2024)\" introduces the **FROND** framework, which uses fractional derivatives in continuous GNNs to capture non-local, memory-dependent dynamics, inherently mitigating oversmoothing. Finally, [li202444f] \"Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)\" presents **Morpher**, a multi-modal prompt learning paradigm that aligns GNNs with LLMs using *extremely weak text supervision*, enabling CLIP-style zero-shot generalization for GNNs. This pushes the frontier of GNN transferability and semantic understanding. The increasing maturity and practical impact of GNNs are further validated by [liu20242g6] \"A Review of Graph Neural Networks in Epidemic Modeling (2024),\" a comprehensive survey of GNN applications in a critical domain.\n*   **Problem evolution**: The initial problem of GNN vulnerabilities expanded to practical bottlenecks like memory for deep models, effective pre-training, and trustworthy explanations. The community then recognized the need for rigorous evaluation. The latest papers tackle deeper theoretical issues in interpretability (faithfulness, structure-awareness, high-order interactions), novel mechanisms for OOD generalization (structure shift, invariant representations), and groundbreaking methods for leveraging external knowledge (LLMs) to overcome data scarcity and enhance semantic understanding. The problem of oversmoothing is addressed with a novel mathematical approach.\n*   **Key innovations**: Link stealing attacks, reversible GNNs, meta-learning for GNN pre-training, causal intervention for invariant rationales, and community-standard benchmarking frameworks. New key innovations include the **SubMT framework** and **GMT architecture** for faithful interpretability [chen2024woq], the **Myerson-Taylor interaction index** and **MAGE** for structure-aware, high-order motif explanations [bui2024zy9], the **Cluster Information Transfer (CIT) mechanism** for invariant representation learning against structure shift [xia20247w9], the **FROND framework** using fractional calculus to mitigate oversmoothing and model memory-dependent dynamics [kang2024fsk], and the **Morpher paradigm** for multi-modal prompt learning to enable zero-shot GNNs with weak text supervision [li202444f]. The comprehensive review by [liu20242g6] itself is a key contribution to organizing and validating the field's progress.\n*   **Integration points**: [chen2024woq] and [bui2024zy9] refine and provide deeper theoretical grounding for the interpretability efforts initiated by works like [wu2022vcx]. [xia20247w9] offers a novel approach to invariant representation learning, complementing previous causal intervention methods ([wu2022vcx]) and pre-training for transferability ([lu20213kr]). [kang2024fsk] provides a foundational mathematical enhancement to continuous GNNs, which can be seen as an underlying improvement for models like those in [li2021orq]. [li202444f] extends the concept of pre-training and transferability ([lu20213kr]) by integrating LLMs, marking a significant step towards more semantically aware and adaptable GNNs. [liu20242g6] synthesizes the practical applications, validating the overall maturation trend.\n\n3. *Refined Synthesis:*\n\nThese works collectively chart an intellectual trajectory from understanding and overcoming fundamental expressive power limitations of GNNs to developing profoundly robust, scalable, and interpretable models for real-world applications. The field has evolved from foundational theoretical breakthroughs to a phase of deep refinement, addressing practical challenges with sophisticated architectural, mathematical, and multi-modal innovations. This expanded view confirms GNNs' advancement from a promising but constrained paradigm to a mature, versatile, and rigorously evaluated tool, increasingly capable of integrating with other advanced AI systems and tackling complex graph-structured data across diverse scientific and engineering domains.",
    "path": [
      "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "aafe1338caef4682069e92378f1190785ec24c2c",
      "e4b1d7553020258d7e537e2cfa53865359389eac",
      "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "454304628bf10f02aba1c2cfc95891e94d09208e",
      "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "5e6db511e736f77f844bbeebaa2b177427abada1",
      "94497472eecb7530a2b75c564548c540ebd61e9b",
      "cf30fb61a5943781144c8442563e3ef9c38df871",
      "db5d583782264529456a475ce8e9a90823b3a2b5",
      "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "854342cf063eef4428a5441c8d317dfbabb8117f",
      "c9845a625e2dac5e32db172d353f81d377760a5f",
      "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f"
    ],
    "layer1_papers": [
      {
        "title": "Benchmarking Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{dwivedi20239ab}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: The rapid growth of Graph Neural Networks (GNNs) has led to numerous promising techniques, but a lack of community-standard benchmarks makes it challenging to track progress, fairly compare models, and identify truly impactful architectural advancements.\n*   **Importance & Challenge**:\n    *   Existing evaluations often use small, non-discriminative datasets (e.g., Cora, Citeseer, TU) that fail to differentiate complex from simple or graph-agnostic architectures.\n    *   Inconsistent experimental settings and varying parameter budgets across studies make fair comparisons difficult, hindering the identification of universal, generalizable, and scalable GNN principles.\n    *   Designing effective benchmarks is challenging, requiring a rigorous experimental setting, reproducibility, and appropriate datasets that can statistically separate model performance.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**: While GNNs have been extensively developed across various domains (chemistry, physics, social sciences, etc.), the paper positions itself as addressing a critical gap in the *evaluation* and *benchmarking* of these models. Previous GNN research focused on developing new architectures (message-passing, attention-based, theoretically expressive GNNs) but lacked a standardized framework for their rigorous comparison.\n*   **Limitations of Previous Solutions**:\n    *   Evaluation on traditionally-used small datasets (e.g., Cora, Citeseer, TU) is insufficient to differentiate the expressive power of various GNN architectures \\cite{dwivedi20239ab}.\n    *   Inconsistent experimental comparisons and a lack of consensus on unifying experimental settings make it difficult to draw reliable conclusions about model performance gains \\cite{dwivedi20239ab}.\n    *   Absence of fixed parameter budgets means performance improvements could be due to increased model capacity rather than architectural innovation.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method/Algorithm**: The paper introduces an open-source, modular benchmarking framework for GNNs built upon PyTorch and DGL. It provides a standardized environment for evaluating GNNs across diverse tasks and datasets.\n*   **Novelty/Difference**:\n    *   **Diverse Dataset Collection**: Comprises 12 medium-scale datasets, including both real-world (e.g., ZINC, AQSOL, OGB-COLLAB, WikiCS, MNIST, CIFAR10) and essential mathematical graphs (e.g., PATTERN, CLUSTER, TSP, CSL, CYCLES, GraphTheoryProp) designed to test specific theoretical graph properties and discriminate GNN performance. AQSOL is a new addition with real-world measured chemical targets.\n    *   **Fair Comparison Protocol**: Enforces fixed parameter budgets (100k and 500k) for all GNN models, ensuring that performance differences are attributable to architectural design rather than varying model capacity.\n    *   **Modular & Reproducible Infrastructure**: Provides an easy-to-use, open-source code infrastructure with independent components for data pipelines, GNN layers/models, training/evaluation functions, and configurations, enabling researchers to experiment with new ideas at any stage.\n    *   **Focus on Medium-Scale Datasets**: Prioritizes medium-scale datasets to enable swift yet reliable prototyping and achieve statistical differences in GNN performance within reasonable computational times (e.g., 12 hours for a single experiment run).\n\n### 4. Key Technical Contributions\n\n*   **System Design/Architectural Innovations**:\n    *   Development of a comprehensive, open-source GNN benchmarking framework with a modular architecture (data, configs, layers, nets, train modules) that promotes ease-of-use, extensibility, and reproducibility \\cite{dwivedi20239ab}.\n    *   Introduction of a standardized experimental protocol, including fixed parameter budgets, to ensure fair and rigorous comparison of GNN architectures.\n*   **Novel Methods/Techniques (demonstrated via the benchmark)**:\n    *   The benchmark itself facilitated the introduction and validation of **Graph Positional Encoding (PE) using Laplacian eigenvectors** as a crucial component for GNNs, especially for graphs lacking canonical positional information or node features \\cite{dwivedi20239ab}. This technique significantly improved MP-GCN performance on synthetic and real-world datasets.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted (using the benchmark)**: The paper demonstrates the utility of its framework by studying the case of **Graph Positional Encoding (PE)**. It used the benchmark to validate and quantify the improvement provided by Laplacian eigenvectors as node positional encodings.\n*   **Key Performance Metrics & Comparison Results**:\n    *   Laplacian PE was shown to effectively improve Message-Passing GCNs (MP-GCNs) on synthetic datasets (CSL, CYCLES, GraphTheoryProp) where nodes are anonymous and traditional GCNs perform poorly.\n    *   Improvements were also observed on other real-world datasets, including the newly added AQSOL dataset.\n    *   While specific numerical results are detailed in the appendix (not provided in the excerpt), the paper emphasizes that the benchmark allowed for robust experimental settings to quantify these gains.\n*   **Community Adoption as Validation**: The framework's wide usage (2,000+ GitHub stars, 380+ forks, 470+ citations) since its initial release serves as strong empirical validation of its utility and impact on the GNN research community \\cite{dwivedi20239ab}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus on **medium-scale datasets** is motivated by enabling swift prototyping and academic-scale research, which might not fully capture the computational challenges or specific properties of very large-scale graphs.\n    *   The use of **fixed parameter budgets** for fair comparison, while crucial, means the benchmark does not aim to find the *optimal* hyperparameters for every specific model, which is computationally expensive.\n*   **Scope of Applicability**: The benchmark is designed for evaluating and prototyping GNNs across graph-level, node-level, and edge-level tasks, and is particularly useful for exploring fundamental GNN design choices (e.g., aggregation functions, expressive power, pooling, normalization, robustness, positional encodings). It supports both Message Passing GCNs and Weisfeiler Lehman GNNs.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: The framework significantly advances the technical state-of-the-art by providing a much-needed standardized, fair, and reproducible environment for GNN research. It moves beyond inconsistent evaluations on small datasets, enabling rigorous comparison and identification of fundamental GNN principles.\n*   **Potential Impact on Future Research**:\n    *   It serves as a critical tool for accelerating GNN development by allowing researchers to quickly and robustly test new ideas and explore insights.\n    *   The benchmark has already demonstrated its ability to steer research directions, notably by introducing Graph Positional Encoding (PE) with Laplacian eigenvectors, which subsequently spurred extensive follow-up research on improving PE for GNNs and Transformers \\cite{dwivedi20239ab}.\n    *   It fosters a more scientific approach to GNN development by enabling the community to identify universal, generalizable, and scalable architectures.",
        "year": 2023,
        "citation_key": "dwivedi20239ab"
      }
    ],
    "layer2_papers": [
      {
        "title": "Substructure Aware Graph Neural Networks",
        "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Conventional Graph Neural Networks (GNNs) are fundamentally limited by the expressive power of the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \\cite{zeng20237gv}.\n    *   This limitation prevents GNNs from perceiving crucial higher-order substructures, which are vital for many downstream tasks (e.g., functional groups in organic chemistry) \\cite{zeng20237gv}.\n    *   The problem is challenging because directly constructing higher-order GNNs leads to scalability and complexity issues, while using predefined hand-crafted substructures compromises generalization ability \\cite{zeng20237gv}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to overcome 1-WL limitations include: higher-order GNNs (suffering from high computational cost), predefined hand-crafted substructures (lacking generalization), inductive coloring methods (often task-specific), and positional encoding methods (facing issues like global sign ambiguity or limited inductive generalization) \\cite{zeng20237gv}.\n    *   This work positions itself by introducing a novel framework, Substructure Aware Graph Neural Networks (SAGNN), that leverages subgraphs to enhance GNN expressiveness. It aims to overcome the 1-WL barrier without incurring the high complexity of higher-order GNNs or the generalization issues of fixed substructures, by focusing on flexible subgraph extraction and encoding \\cite{zeng20237gv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the Substructure Aware Graph Neural Network (SAGNN) framework, which injects subgraph-level structural information into node features during message passing \\cite{zeng20237gv}.\n    *   **Novel Subgraph Extraction:** Introduces the \"Cut subgraph,\" which is obtained from the original graph by continuously and selectively removing edges with the highest Edge Betweenness Centrality (EBC) until the graph is split into a specified number of connected blocks \\cite{zeng20237gv}. This allows for capturing structural information at various granularities.\n    *   **Novel Subgraph Encoding:** Extends the random walk encoding paradigm to compute \"return probabilities\" of a rooted node within a subgraph for a given number of steps \\cite{zeng20237gv}. This method efficiently captures rich structural information and is used as a node feature.\n    *   **Subgraph Information Injection:** The encoded subgraph features (from both Ego-networks and Cut subgraphs) are concatenated with initial node features and original graph structural representations. These enhanced features are then propagated through two parallel message passing channels (Ego channel and Cut channel) \\cite{zeng20237gv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Subgraph Definition:** The \"Cut subgraph\" extraction strategy, which dynamically partitions the graph based on edge centrality to reveal meaningful substructures \\cite{zeng20237gv}.\n    *   **Novel Subgraph Encoding Method:** An efficient random walk return probability encoding for subgraphs, designed to capture structural information with reduced time complexity and improved expressiveness \\cite{zeng20237gv}.\n    *   **Framework Design:** The Substructure Aware Graph Neural Network (SAGNN) framework, which seamlessly integrates these novel subgraph features into standard GNN message passing architectures \\cite{zeng20237gv}.\n    *   **Theoretical Insight:** Provides theoretical proof that any 1-WL GNN equipped with components of the SAGNN framework is strictly more powerful than 1-WL \\cite{zeng20237gv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various well-proven graph tasks, including graph classification on TUDatasets (MUTAG, PTC, PROTEINS, NCI1, IMDB-B) and graph regression for drug constrained solubility prediction (ZINC-FULL) \\cite{zeng20237gv}.\n    *   **Key Performance Metrics:** Accuracy for classification tasks and Mean Absolute Error (MAE) for regression tasks \\cite{zeng20237gv}.\n    *   **Comparison Results:**\n        *   Achieved state-of-the-art (SOTA) performance across a variety of datasets and when integrated with different base GNN models (e.g., GIN, PNA) \\cite{zeng20237gv}.\n        *   Demonstrated a maximum performance improvement of 83% compared to base models and 32% compared to previous SOTA methods \\cite{zeng20237gv}.\n        *   GNNs equipped with SAGNN performed flawlessly even on graphs that are known to fail the 3-WL test, indicating significantly enhanced structure perception \\cite{zeng20237gv}.\n\n*   **Limitations & Scope**\n    *   The provided abstract and introduction do not explicitly detail specific technical limitations or assumptions inherent to the SAGNN framework itself.\n    *   The scope of applicability is broad, covering general graph learning tasks such as graph classification and regression, particularly in domains where higher-order structural information is crucial, like chemo-informatics \\cite{zeng20237gv}. The method is designed to enhance the expressiveness of existing 1-WL GNNs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** Significantly pushes the expressive power of GNNs beyond the 1-WL limit by leveraging subgraph information, offering a more scalable alternative to complex higher-order GNNs \\cite{zeng20237gv}.\n    *   **Improved Structure Perception:** Enables GNNs to perceive and utilize higher-order substructures that were previously indistinguishable, leading to superior performance on challenging graph tasks and even on graphs that fail the 3-WL test \\cite{zeng20237gv}.\n    *   **Practical Impact:** Provides a general and flexible framework that can be integrated with various existing GNN architectures, leading to substantial performance gains on real-world graph learning tasks \\cite{zeng20237gv}.\n    *   **Future Research:** The novel subgraph extraction and encoding methods open new avenues for exploring dynamic and expressive ways to incorporate substructural information into graph learning models \\cite{zeng20237gv}.",
        "year": 2023,
        "citation_key": "zeng20237gv"
      },
      {
        "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.",
        "summary": "Here's a focused summary of the paper \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" \\cite{michel2023hc4} for a literature review:\n\n*   **CITATION**: \\cite{michel2023hc4}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressive power, being no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in distinguishing non-isomorphic graphs \\cite{michel2023hc4}.\n    *   This limitation restricts their ability to capture complex structural information, hindering performance on various graph learning tasks.\n    *   The problem is challenging because finding all paths in a graph is NP-hard, requiring efficient methods to leverage path information without incurring prohibitive computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Path-based GNNs**: Previous works incorporated shortest path distances as features (e.g., Graphormer, PEGN, SP-MPNN, Geodesic GNN) or aggregated nodes at specific shortest path distances.\n    *   **Limitations of Previous Path-based GNNs**: Many either use path information indirectly or focus on specific path types/lengths. The most related work, PathNet (Sun et al., 2022), samples paths, is only evaluated on node classification, and lacks an extensive study of expressive power, which PathNNs \\cite{michel2023hc4} explicitly address.\n    *   **Expressive GNNs**: Other approaches to enhance GNN expressive power include higher-order WL variants, k-order graph networks (some achieving 3-WL power), and methods using subgraphs or vertex identifiers. PathNNs \\cite{michel2023hc4} offer a distinct path-centric approach to achieve higher expressive power.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea**: Path Neural Networks (PathNNs) \\cite{michel2023hc4} update node representations by aggregating information from various paths emanating from each node.\n    *   **Path Encoding**: For each path length, a recurrent layer is used to encode paths into vectors.\n    *   **Aggregation**: The representations of all relevant paths emanating from a node are aggregated to produce the node's new representation.\n    *   **Three Variants**:\n        *   **Single Shortest Paths (SP)**: Aggregates a single shortest path for all possible node pair combinations.\n        *   **All Shortest Paths (SP+)**: Aggregates all possible shortest paths between every node pair combination.\n        *   **All Simple Paths (AP)**: Aggregates all simple paths (not necessarily shortest) up to a fixed length K.\n    *   **Key Innovation: Annotated Sets of Paths**: While initial `Path-Trees` (analogous to `WL-Trees`) were shown to be insufficient for `SP` and `SP+` to surpass 1-WL, the core innovation lies in operating on \"annotated sets of paths\" (`˜SP`, `˜SP+`, `˜AP`). Nodes within these paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, creating a richer, hierarchical structural encoding.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of Path Neural Networks (PathNNs) \\cite{michel2023hc4} as a novel GNN architecture that explicitly leverages path information for node representation learning.\n        *   Development of three distinct PathNN variants (SP, SP+, AP) based on different path collection strategies.\n        *   The concept and implementation of \"annotated sets of paths\" for recursively enriching path information, which is crucial for achieving higher expressive power.\n    *   **Theoretical Insights/Analysis**:\n        *   Formal definition of `Path-Trees` as a path-based analogue to `WL-Trees`.\n        *   Proof that `AP-Trees` are strictly more powerful than `WL-Trees` in distinguishing non-isomorphic graphs (Theorem 3.3).\n        *   Crucially, the theoretical demonstration that by operating on **annotated sets of paths**, two of the PathNN variants (`˜SP`, `˜SP+`) are strictly more powerful than the 1-WL algorithm, and the most expressive variant (`˜AP`) can even distinguish graphs indistinguishable by the 3-WL algorithm (Theorem 3.5, as implied by abstract/intro).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation on synthetic datasets specifically designed to measure expressive power, testing the ability to distinguish non-isomorphic graphs.\n        *   Performance evaluation on real-world graph classification and graph regression datasets.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Expressive Power**: PathNNs \\cite{michel2023hc4} successfully distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL. The most expressive PathNN variant (`˜AP`) demonstrated the ability to distinguish graphs that are even 3-WL indistinguishable, empirically validating the theoretical claims.\n        *   **Real-world Tasks**: On graph classification and regression datasets, the different PathNN variants achieved high levels of performance, outperforming baseline methods in most cases.\n\n6.  **Limitations & Scope**\n    *   **Computational Complexity**: Finding all simple paths (`AP` variant) in a graph is NP-hard. The model addresses this by considering paths only up to a fixed length `K`.\n    *   **Scalability of AP-Trees**: `AP-Trees` (and by extension, the `˜AP` variant) can grow exponentially with height `K` and graph density, potentially limiting the practical maximum path length `K` that can be used.\n    *   **Scope of Applicability**: The paper focuses on undirected graphs and does not explicitly discuss applicability to directed graphs or other graph types (e.g., hypergraphs).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PathNNs \\cite{michel2023hc4} significantly advance the technical state-of-the-art by providing GNN architectures that provably surpass the expressive power of the 1-WL algorithm, a known bottleneck for many standard GNNs. The ability to distinguish 3-WL indistinguishable graphs with a path-based approach is a notable achievement.\n    *   **Potential Impact on Future Research**: This work opens new avenues for designing more powerful GNNs by effectively leveraging path information. It highlights the importance of recursive, hierarchical structural encoding (via annotated paths) for enhancing expressive power. Future research could explore more efficient path enumeration/sampling strategies, adaptive path length selection, or the integration of path information with other structural elements (e.g., subgraphs) to further push the boundaries of GNN expressiveness and performance.",
        "year": 2023,
        "citation_key": "michel2023hc4"
      },
      {
        "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
        "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT",
        "summary": "This paper, `Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking` by Li et al. \\cite{li2023o4c}, critically examines the evaluation practices of Graph Neural Networks (GNNs) for link prediction and proposes a new, more realistic benchmarking framework.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses significant pitfalls in the current evaluation of GNN-based link prediction methods, which lead to unreliable comparisons and hinder progress.\n    *   **Importance and Challenge**: These pitfalls include: (1) underreported performance of existing baselines due to poor hyperparameter tuning or non-standard settings; (2) a lack of unified data splits and evaluation metrics across different studies and datasets; and (3) an unrealistic evaluation setting that uses \"easy\" negative samples, which do not reflect real-world scenarios (e.g., negative samples often lack common neighbors, making them trivial to classify). This problem is crucial because it obscures the true capabilities of GNN models and makes it difficult to identify genuinely superior approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself not by proposing a new GNN architecture, but by scrutinizing and improving the *evaluation methodology* for existing and future GNN-based link prediction models. It acknowledges the proliferation of GNN methods for link prediction but highlights the absence of a comprehensive, critical examination of their evaluation.\n    *   **Limitations of Previous Solutions**: The paper directly addresses the limitations of previous *evaluation practices*, which are the three pitfalls mentioned above. It argues that current benchmarking efforts are inconsistent and often misleading, preventing fair comparisons and accurate assessment of model performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        1.  **Reproducible and Fair Comparison**: The authors first conduct a rigorous re-evaluation of 17 prominent link prediction methods (including heuristics, embedding methods, GNNs, and GNN+Pairwise Info models) across 7 diverse datasets (Cora, Citeseer, Pubmed, ogbl-collab, ogbl-ddi, ogbl-ppa, ogbl-citation2). This involves unified data splits, consistent evaluation metrics (AUC, MRR, Hits@K), and a comprehensive hyperparameter search for all models.\n        2.  **New Evaluation Setting with Heuristic Related Sampling Technique (HeaRT)**: To address the unrealistic negative sampling, the paper proposes a novel evaluation strategy. This strategy tailors negative samples to each positive sample by restricting them to be \"corruptions\" (i.e., sharing one node with the positive sample). Within this more realistic pool, a **Heuristic Related Sampling Technique (HeaRT)** is introduced to select *hard* negative samples based on a combination of multiple structural heuristics.\n    *   **Novelty**: The primary novelty lies in two aspects: (1) the systematic and comprehensive re-benchmarking effort itself, which corrects widespread misrepresentations of model performance and establishes a unified evaluation framework; and (2) the introduction of HeaRT, a novel negative sampling technique that generates challenging, realistic negative samples, thereby creating a more robust and real-world aligned evaluation task for link prediction.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A standardized and reproducible benchmarking methodology for GNN-based link prediction, including unified data splits, metrics, and rigorous hyperparameter tuning.\n        *   HeaRT: A novel negative sampling technique that generates hard, heuristic-related negative samples for link prediction evaluation, making the task more challenging and realistic.\n    *   **System Design/Architectural Innovations**: While not proposing a new GNN architecture, the paper significantly innovates the *evaluation framework* for GNNs in link prediction.\n    *   **Theoretical Insights/Analysis**: Provides empirical insights into the actual performance of GNNs and the profound impact of evaluation settings on reported results.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A \"fair comparison\" of 17 methods on 7 datasets under existing, but unified, evaluation settings.\n        *   Evaluation of these methods under the proposed new evaluation setting utilizing HeaRT.\n    *   **Key Performance Metrics**: AUC, MRR, and Hits@K (with K varying based on dataset).\n    *   **Comparison Results (from fair comparison under existing settings)**:\n        *   **Better than Reported Performance**: Many models, including GCN, GAE, and Neo-GNN, achieved significantly higher performance than previously reported when properly tuned and evaluated under consistent settings. For example, Neo-GNN's Hits@50 on `ogbl-collab` increased from 57.52 to 66.13 \\cite{li2023o4c}. Heuristic methods on `ogbl-citation2` also saw substantial improvements (e.g., MRR around 75% vs. 50%) by treating the graph as undirected, consistent with GNN evaluations.\n        *   **Divergence from Reported Results**: Some methods, like BUDDY on `ogbl-ddi`, showed lower performance than previously reported, attributed to differences in training negative sampling strategies, further highlighting inconsistencies in prior work.\n        *   The new HeaRT evaluation setting is designed to expose new challenges and opportunities by aligning evaluation with real-world situations, though specific results from this setting are not detailed in the provided abstract and introduction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on homogeneous graphs. For large OGB datasets, hyperparameter search was constrained due to computational cost. The HeaRT method, while more realistic, still samples *K* negatives due to the prohibitive cost of considering all possible corruptions.\n    *   **Scope of Applicability**: The findings and proposed benchmark are most directly applicable to GNN-based link prediction on homogeneous graphs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art by establishing a robust, reproducible, and more realistic benchmarking framework for GNN-based link prediction. It corrects previous misrepresentations of model performance, providing a clearer picture of current capabilities.\n    *   **Potential Impact**: It provides a reliable foundation for future research, enabling fair comparisons and accelerating the development of more robust and generalizable link prediction models. By exposing the limitations of current evaluation practices and offering a more challenging setting, it encourages the community to develop models that perform better in real-world applications, thereby fostering more impactful research.",
        "year": 2023,
        "citation_key": "li2023o4c"
      },
      {
        "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
        "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing -- aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the fundamental limitation of most popular Graph Neural Networks (GNNs), which rely on 1-hop message passing and are bounded in expressive power by the 1-dimensional Weisfeiler-Lehman (1-WL) test \\cite{feng20225sa}. This means they cannot distinguish many non-isomorphic graph structures.\n    *   While K-hop message passing (aggregating information from K-hop neighbors) has been proposed, there was no theoretical work characterizing its expressive power, leaving a gap in understanding its capabilities and limitations \\cite{feng20225sa}.\n\n*   **Related Work & Positioning**\n    *   Existing GNNs primarily use 1-hop message passing, which is known to be limited by the 1-WL test \\cite{feng20225sa}.\n    *   Previous works extended to K-hop message passing (e.g., GPR-GNN, MixHop, GINE+, Graphormer) but often misused or interchanged different definitions of K-hop kernels (shortest path distance vs. graph diffusion) without theoretical analysis of their distinct expressive powers \\cite{feng20225sa}.\n    *   The paper positions itself as the first to theoretically characterize the expressive power of K-hop message passing, differentiate its kernels, identify its limitations, and propose an enhancement.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach involves a rigorous theoretical characterization of K-hop message passing GNNs.\n    *   **Differentiation of K-hop Kernels**: Formally distinguishes two kernels for K-hop neighbors: shortest path distance (spd) and graph diffusion (gd) \\cite{feng20225sa}.\n    *   **Expressive Power Analysis**: Proves that K-hop message passing is strictly more powerful than 1-WL and can distinguish almost all regular graphs, but also shows it's bounded by the 3-WL test and fails on some simple regular graphs \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Introduces the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework. This novel approach enhances K-hop message passing by incorporating information from the \"peripheral subgraph\" (subgraph induced by neighbors in a specific hop) in addition to individual neighbor features \\cite{feng20225sa}. This allows the model to learn more expressive local structural features.\n\n*   **Key Technical Contributions**\n    *   **Formal Kernel Definitions**: Provides formal definitions and differentiation of shortest path distance (spd) and graph diffusion (gd) kernels for K-hop neighbors, clarifying their distinct impacts on expressive power \\cite{feng20225sa}.\n    *   **Theoretical Expressive Power Bounds**:\n        *   Proves that K-hop message passing (with K > 1) is strictly more powerful than 1-hop message passing and the 1-WL test \\cite{feng20225sa}.\n        *   Demonstrates that K-hop GNNs can distinguish almost all regular graphs with a modest K \\cite{feng20225sa}.\n        *   Establishes that the expressive power of K-hop message passing, regardless of the kernel, is bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Proposes KP-GNN, which enhances K-hop message passing by integrating peripheral subgraph information (edges within a hop's neighbors). This framework significantly improves expressive power, enabling it to distinguish many distance regular graphs that even 3-WL or previous distance encoding methods could not \\cite{feng20225sa}.\n    *   **Flexibility and Efficiency**: KP-GNN can be applied to most existing K-hop GNNs with minor modifications and adds only little computational complexity \\cite{feng20225sa}.\n\n*   **Experimental Validation**\n    *   The paper states that experimental results verify the expressive power and effectiveness of KP-GNN \\cite{feng20225sa}.\n    *   KP-GNN achieves competitive results across all benchmark datasets, demonstrating its practical utility \\cite{feng20225sa}. (Specific datasets or metrics are not detailed in the provided abstract/introduction).\n\n*   **Limitations & Scope**\n    *   **K-hop Limitations**: The choice of K-hop kernel (spd vs. gd) affects expressive power, and neither can distinguish all simple non-isomorphic structures \\cite{feng20225sa}. The expressive power of K-hop message passing is fundamentally bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **Scope**: The analysis primarily focuses on distinguishing non-isomorphic graphs and node configurations based on local structural information, assuming uniform node features for theoretical clarity.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing the first comprehensive theoretical characterization of K-hop message passing GNNs \\cite{feng20225sa}.\n    *   It clarifies the impact of different K-hop kernels and establishes precise bounds on their expressive power, offering crucial insights for GNN design \\cite{feng20225sa}.\n    *   The introduction of KP-GNN provides a practical and theoretically grounded method to overcome the limitations of standard K-hop GNNs, paving the way for more powerful and discriminative GNN architectures in future research \\cite{feng20225sa}.",
        "year": 2022,
        "citation_key": "feng20225sa"
      },
      {
        "title": "Breaking the Limits of Message Passing Graph Neural Networks",
        "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.",
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements and bullet format:\n\n*   **CITATION**: \\cite{balcilar20215ga}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Message Passing Neural Networks (MPNNs) are computationally efficient (linear complexity for sparse graphs) but suffer from limited expressive power, theoretically equivalent to at most the 1-Weisfeiler-Lehman (1-WL) test. This limitation prevents them from distinguishing many non-isomorphic graphs and counting simple graph substructures (e.g., triangles, 4-cycles).\n*   **Importance and Challenge**:\n    *   The inability to distinguish non-isomorphic graphs or count basic graphlets hinders MPNNs' performance on various graph-related tasks.\n    *   Existing provably more powerful Graph Neural Networks (GNNs), such as those equivalent to the 3-WL test (e.g., PPGN), achieve higher expressive power but at a prohibitive computational cost (O(n^3) complexity, O(n^2) memory) and often rely on non-local update mechanisms, breaking the locality principle crucial for Euclidean learning.\n    *   Other attempts to enhance MPNNs (e.g., randomization, unique labels, handcrafted features) have their own drawbacks, such as slow convergence, massive training data requirements, or reliance on domain expertise.\n    *   Many MPNNs also act as low-pass filters, further reducing their spectral expressive power.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   The work directly addresses the limitations of standard MPNNs (GCN, GAT, GraphSage, GIN) which are shown to be no more powerful than the 1-WL test \\cite{balcilar20215ga}.\n    *   It positions itself against higher-order GNNs (e.g., PPGN \\cite{balcilar20215ga}, Morris et al., 2019) that achieve 3-WL equivalence but with significantly higher computational and memory costs.\n*   **Limitations of Previous Solutions**:\n    *   **1-WL MPNNs**: Cannot distinguish 1-WL equivalent non-isomorphic graphs (e.g., Decalin and Bicyclopentyl) and fail to count graphlets like triangles or 4-cycles \\cite{balcilar20215ga}.\n    *   **Higher-Order GNNs (e.g., 3-WL equivalent)**: Suffer from O(n^3) computational complexity and O(n^2) memory usage, making them impractical for large graphs. They also typically employ non-local update mechanisms and do not inherently provide spectral richness in their output profiles \\cite{balcilar20215ga}.\n    *   **Other MPNN enhancements**: Approaches like node feature randomization or unique labels require extensive training and slow convergence. Handcrafted features demand domain expertise and feature selection.\n    *   **Spectral limitations**: Most MPNNs function as low-pass filters, limiting their ability to capture diverse frequency components in graph signals \\cite{balcilar20215ga}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes designing graph convolution supports in the spectral domain using **non-linear custom functions of eigenvalues** and masking these supports with an **arbitrarily large receptive field** while maintaining spatial locality \\cite{balcilar20215ga}.\n    *   The general MPNN formulation `H(l+1)= \\sigma(\\sum_s C(s)H(l)W(l;s))` is used, where `C(s)` are convolution supports \\cite{balcilar20215ga}.\n    *   For spectral GNNs, `C(s) = U diag(\\phi_s(\\lambda)) U^T`, where `U` and `\\lambda` are eigenvectors and eigenvalues of the graph Laplacian, and `\\phi_s(\\lambda)` is the custom filter function \\cite{balcilar20215ga}.\n    *   The paper introduces two models:\n        *   **GNNML1**: An MPNN with a richer node update schema including element-wise feature multiplication, shown to be exactly as powerful as the 1-WL test \\cite{balcilar20215ga}.\n        *   **GNNML3**: The primary innovation, which leverages the spectral design. By precomputing the eigendecomposition of the graph Laplacian and designing `\\phi_s(\\lambda)` as custom non-linear functions of eigenvalues, it implicitly incorporates operations like `trace` and `element-wise multiplication` (identified by MATLANG as crucial for exceeding 1-WL) into the convolution process. The convolution supports `C(s)` are then masked to ensure sparsity and spatial locality.\n*   **Novelty/Differentiation**:\n    *   **Spectral Design for Expressivity**: Unlike previous MPNNs that implicitly act as low-pass filters, this approach explicitly designs spectral filters (`\\phi_s(\\lambda)`) to capture various frequency components and implicitly perform higher-order operations.\n    *   **Breaking 1-WL with Efficiency**: It theoretically surpasses the 1-WL expressive power and experimentally matches 3-WL equivalent models, *while retaining linear computational complexity* (after an initial eigendecomposition preprocessing step) and spatially localized updates \\cite{balcilar20215ga}. This is a significant departure from O(n^3) 3-WL models.\n    *   **Rich Output Profile**: The custom filter functions enable the output to have diverse frequency components, offering richer representations than most existing GNNs \\cite{balcilar20215ga}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms/Methods**:\n    *   A novel MPNN architecture (GNNML3) that designs graph convolution supports in the spectral domain using non-linear custom functions of eigenvalues, masked to ensure spatial locality and linear complexity \\cite{balcilar20215ga}.\n    *   Introduction of GNNML1, an MPNN with feature-wise multiplication, provably 1-WL equivalent but with richer node representations \\cite{balcilar20215ga}.\n*   **Theoretical Insights/Analysis**:\n    *   Demonstrates that by leveraging spectral domain design with custom non-linear filters and masked receptive fields, MPNNs can theoretically exceed the 1-WL expressive power \\cite{balcilar20215ga}.\n    *   Utilizes the MATLANG framework to formally characterize the expressive power of various MPNNs and identify the operations (trace, element-wise multiplication) required to surpass 1-WL and reach 3-WL equivalence \\cite{balcilar20215ga}.\n    *   Proves that Chebnet can be more powerful than 1-WL under specific conditions (different maximum eigenvalues) \\cite{balcilar20215ga}.\n    *   Characterizes the MATLANG operations needed to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle) \\cite{balcilar20215ga}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**:\n    *   **Graph Isomorphism Tests**: Evaluated the ability to distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL tests.\n    *   **Substructure Counting**: Tested the models' capacity to count specific graphlets (3-star, triangle, 4-cycle, tailed triangle).\n    *   **Downstream Tasks**: Performance on various benchmark graph learning tasks (though specific datasets/tasks are not detailed in the provided excerpt, the abstract claims state-of-the-art results).\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Distinguishing 1-WL Equivalent Graphs**: GNNML3 successfully distinguishes Decalin and Bicyclopentyl graphs (Figure 1), which are 1-WL equivalent but have different maximum eigenvalues or can be distinguished by `tr(A^5)` \\cite{balcilar20215ga}.\n    *   **Distinguishing Cospectral Graphs**: GNNML3 can distinguish cospectral graphs (Figure 3), which are `L1` and `L2` equivalent, by implicitly leveraging element-wise multiplication operations \\cite{balcilar20215ga}.\n    *   **Substructure Counting**: Unlike 1-WL equivalent MPNNs, GNNML3 is shown to be capable of counting triangles, 4-cycles, and tailed triangles, demonstrating its enhanced expressive power for structural features \\cite{balcilar20215ga}.\n    *   **Overall Performance**: The proposed method achieves state-of-the-art results on many downstream tasks, outperforming existing MPNNs and matching the performance of computationally expensive 3-WL equivalent models \\cite{balcilar20215ga}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The eigendecomposition of the graph Laplacian is a preprocessing step, which itself has a complexity of O(n^3) in the general case. While the subsequent MPNN operations are linear, this initial cost can be significant for very large graphs where even O(n^3) preprocessing is prohibitive \\cite{balcilar20215ga}.\n    *   The \"3-WL equivalent\" claim for GNNML3 is stated as \"experimentally as powerful as a 3-WL existing models,\" implying empirical validation rather than a formal proof of equivalence to the 3-WL test in the same theoretical sense as some O(n^3) models \\cite{balcilar20215ga}.\n*   **Scope of Applicability**:\n    *   The theoretical analysis using MATLANG primarily focuses on undirected graphs with monochromatic edges and nodes for simplicity in proofs \\cite{balcilar20215ga}.\n    *   The method is particularly beneficial for sparse graphs, where the linear complexity of the MPNN operations (after preprocessing) is maintained.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: This paper significantly advances the state-of-the-art by demonstrating a practical method to break the 1-WL expressive power limit of MPNNs without incurring the prohibitive computational costs of existing higher-order GNNs \\cite{balcilar20215ga}. It offers a compelling alternative to O(n^3) 3-WL models.\n*   **Impact on Future Research**:\n    *   **Efficient Powerful GNNs**: It opens new avenues for designing highly expressive yet computationally efficient GNNs by leveraging spectral domain insights and custom filter functions.\n    *   **Spectral Design Importance**: Highlights the critical role of spectral filter design in enhancing GNN expressive power and generating rich output representations.\n    *   **Bridging Theory and Practice**: Provides a practical framework that bridges theoretical insights from WL tests and matrix languages with efficient, localized MPNN architectures.\n    *   **Substructure Learning**: Improves the ability of MPNNs to learn and count complex graph substructures, which is crucial for many real-world applications in chemistry, biology, and social networks.",
        "year": 2021,
        "citation_key": "balcilar20215ga"
      },
      {
        "title": "Stealing Links from Graph Neural Networks",
        "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the problem of inferring the underlying graph structure (specifically, the existence of links between any pair of nodes) from the black-box outputs of a Graph Neural Network (GNN) model that was trained on that graph \\cite{he2020kz4}.\n    *   **Motivation:** Graph data, such as chemical networks or social networks, is often considered confidential or private. This is either because data owners invest significant resources in collecting it (making it intellectual property) or because it contains sensitive information (e.g., private social relationships). GNNs are increasingly used in various applications (healthcare, recommender systems, fraud detection) due to their superior performance, but the security and privacy implications of training GNNs on such sensitive graphs are largely unexplored \\cite{he2020kz4}.\n\n*   **Related Work & Positioning**\n    *   This work is positioned as the *first* to propose and systematically study \"link stealing attacks\" against GNNs, specifically focusing on extracting the training graph's structure from a black-box GNN model's outputs \\cite{he2020kz4}.\n    *   It differentiates itself from conventional link prediction methods, which typically aim to predict missing links within a *known partial graph*, whereas this work aims to infer the existence of links in the *entire original graph* based solely on the GNN's predictions \\cite{he2020kz4}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** The fundamental intuition behind the attacks is that if two nodes are linked in a graph, their attributes and/or their prediction posteriors (outputs) from a GNN model trained on that graph should be more similar due to the GNN's message-passing and aggregation mechanisms \\cite{he2020kz4}.\n    *   **Threat Model Taxonomy:** A novel and comprehensive threat model is introduced, characterizing an adversary's background knowledge along three dimensions: access to the target dataset's nodes' attributes (F), access to a partial graph of the target dataset (A'), and access to an auxiliary \"shadow dataset\" (D'). This taxonomy systematically defines 8 different types of adversaries and their corresponding attack strategies \\cite{he2020kz4}.\n    *   **Attack Methodologies:**\n        *   **Unsupervised Attacks (e.g., Attack-0, Attack-2):** For adversaries with minimal knowledge, these attacks calculate distances (using 8 common metrics like Cosine, Euclidean) between node posteriors (from the target GNN) or node attributes to infer link existence \\cite{he2020kz4}.\n        *   **Supervised Attacks (e.g., Attack-4, Attack-6):** When a partial graph (A') is available, a binary classifier (Multi-Layer Perceptron - MLP) is trained. Features for this classifier are derived from node posteriors (distances, pairwise operations on entropies) and/or node attributes (distances, pairwise vector operations) \\cite{he2020kz4}.\n        *   **Transferring Attacks (e.g., Attack-1, Attack-3, Attack-5, Attack-7):** For adversaries with a shadow dataset (D'), a shadow GNN model is trained. Features are then extracted from the shadow model's posteriors (using distances and entropy-based operations) and transferred to train an attack model for the target GNN. This approach addresses the \"dimension mismatch\" problem between different datasets' posteriors by using dimension-agnostic feature representations \\cite{he2020kz4}.\n    *   **Novelty:** The paper's primary innovation lies in proposing the first systematic framework and concrete methodologies for link stealing against GNNs, demonstrating that GNN outputs inherently encode significant structural information about their training graphs, even under black-box access \\cite{he2020kz4}.\n\n*   **Key Technical Contributions**\n    *   **First Link Stealing Attacks:** Introduction of the concept and practical realization of link stealing attacks against GNNs, revealing a critical privacy vulnerability \\cite{he2020kz4}.\n    *   **Comprehensive Threat Model:** A novel, 3-dimensional threat model leading to an 8-attack taxonomy, providing a structured way to analyze adversary capabilities and attack effectiveness \\cite{he2020kz4}.\n    *   **Advanced Feature Engineering:** Development of effective feature sets for attack models, including various distance metrics on posteriors/attributes and pairwise operations on posterior entropies, designed to handle challenges like node pair order and dimension mismatch \\cite{he2020kz4}.\n    *   **Transfer Learning for Graph Inference:** Demonstration of successful transferring attacks that leverage knowledge from auxiliary datasets to infer links in a target graph, even across different domains \\cite{he2020kz4}.\n\n*   **Experimental Validation**\n    *   **Experiments:** Extensive experiments were conducted to evaluate the effectiveness of all 8 proposed link stealing attacks \\cite{he2020kz4}.\n    *   **Datasets:** The attacks were validated on 8 diverse real-world datasets, showcasing their broad applicability \\cite{he2020kz4}.\n    *   **Key Performance Metrics:** Attack performance was primarily measured using AUC (Area Under the ROC Curve) \\cite{he2020kz4}.\n    *   **Comparison Results:**\n        *   **High Effectiveness:** The attacks proved highly effective, achieving AUCs above 0.95 in multiple scenarios, indicating a strong ability to infer links \\cite{he2020kz4}.\n        *   **Impact of Background Knowledge:** Performance generally improved with more background knowledge (e.g., on Citeseer, AUC increased from 0.878 with only attributes to 0.977 with all available knowledge) \\cite{he2020kz4}.\n        *   **Knowledge Dimension Hierarchy:** The target dataset's partial graph (A') had the strongest impact on attack success, followed by nodes' attributes (F), with the shadow dataset (D') having the weakest but still beneficial impact \\cite{he2020kz4}.\n        *   **Transferring Attack Efficacy:** Transferring attacks achieved high AUCs, performing better when the shadow dataset originated from the same domain as the target dataset, suggesting that structural similarities aid knowledge transfer \\cite{he2020kz4}.\n        *   **Superiority over Baselines:** The proposed attacks consistently outperformed conventional link prediction methods \\cite{he2020kz4}.\n        *   **Generalizability:** The attacks were shown to be effective not only against GCNs but also against other popular GNN architectures like GraphSAGE and GAT \\cite{he2020kz4}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The paper primarily focuses on undirected graphs. While the attacks generalize to different GNN architectures, the specific GNN model's complexity or regularization might influence the degree of information leakage, which is not deeply explored as a limitation \\cite{he2020kz4}.\n    *   **Assumptions:** The attacks assume black-box access to the GNN model, allowing queries for node posteriors. It also assumes the adversary knows the set of nodes in the target graph \\cite{he2020kz4}.\n    *   **Scope of Applicability:** The findings are directly applicable to GNNs used for node classification tasks. The demonstrated vulnerability highlights a general concern for any application where the confidentiality or intellectual property of the underlying graph structure is paramount \\cite{he2020kz4}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the state-of-the-art by being the first to systematically identify, formalize, and demonstrate practical link stealing attacks against GNNs, thereby establishing a new frontier in GNN security and privacy research \\cite{he2020kz4}.\n    *   **Potential Impact on Future Research:**\n        *   **Privacy-Preserving GNNs:** The findings create an urgent need for research into privacy-preserving GNN training mechanisms, architectures, and differential privacy techniques to protect graph structure \\cite{he2020kz4}.\n        *   **Adversarial Machine Learning:** It contributes a novel attack vector to the broader field of adversarial machine learning, specifically tailored to the unique properties of graph data and GNNs \\cite{he2020kz4}.\n        *   **GNN Interpretability and Information Flow:** The results offer crucial insights into how GNNs encode and implicitly reveal graph structural information through their outputs, which can inform future GNN design and theoretical understanding \\cite{he2020kz4}.\n        *   **Ethical and Practical Implications:** It raises critical awareness for practitioners and organizations deploying GNNs on sensitive graph data, highlighting the potential for intellectual property theft and privacy breaches, and urging the consideration of these risks in deployment strategies \\cite{he2020kz4}.",
        "year": 2020,
        "citation_key": "he2020kz4"
      },
      {
        "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization",
        "abstract": "Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs), particularly Message Passing Neural Networks (MPNNs), are limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism heuristic. This means they cannot distinguish between many non-isomorphic graphs.\n    *   Existing higher-order GNNs (e.g., k-GNNs, invariant/equivariant graph networks) overcome this limitation but are computationally very demanding, requiring excessive memory (e.g., O(|V|^k) or O(|V|^2) for universality), making them impractical for many applications.\n    *   The problem is to enhance GNNs' expressive power beyond 1-WL without incurring the prohibitive computational costs of higher-order models.\n\n*   **Related Work & Positioning**\n    *   This work builds upon the empirical observation that Random Node Initialization (RNI) can improve MPNN performance and detect fixed substructures \\cite{abboud2020x5e}.\n    *   It positions itself against higher-order GNNs (k-GNNs, invariant/equivariant graph networks, PPGNs) which achieve higher expressiveness but suffer from severe computational and memory requirements (e.g., O(|V|^k) or O(|V|^2) memory for universality), or require exponentially many samples to learn necessary functions.\n    *   The paper provides the first theoretical justification for the effectiveness of RNI, proving its universality for memory-efficient GNNs, which was previously lacking.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to enhance standard MPNNs with Random Node Initialization (RNI), where initial node embeddings are randomized.\n    *   The primary innovation is the *proof* that MPNNs with RNI are **universal**, meaning they can approximate any function defined on graphs of any fixed order. This is a first such universality result for GNNs that does not rely on computationally demanding higher-order properties.\n    *   The approach also demonstrates that RNI preserves the permutation-invariance of GNNs *in expectation*, addressing a potential concern about randomization.\n    *   The proof leverages a logical characterization of MPNN expressiveness (C2 logic) and shows that RNI individualizes input graphs with high probability, allowing MPNNs to capture arbitrary Boolean functions and, by extension, real-valued functions.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Insight**: Proof of universality for MPNNs with RNI, establishing that they can approximate any invariant function on graphs, while maintaining permutation-invariance in expectation. This breaks the 1-WL barrier for memory-efficient GNNs.\n    *   **Logical Characterization**: Provides a logical characterization for MPNNs with RNI, substantiating how randomization improves expressiveness by individualizing nodes.\n    *   **Partial RNI**: Demonstrates that universality holds even with partially randomized initial node features (e.g., only one randomized dimension), offering flexibility.\n    *   **Novel Datasets**: Introduction of two carefully designed synthetic datasets, EXP and CEXP, specifically constructed to evaluate GNN expressiveness beyond 1-WL (requiring 2-WL or higher), which are crucial for rigorous empirical validation.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on two synthetic datasets:\n        *   **EXP**: Designed to require 2-WL expressive power, containing 1-WL indistinguishable but 2-WL distinguishable non-isomorphic graph pairs with different SAT outcomes.\n        *   **CEXP**: A modification of EXP with 50% 1-WL distinguishable data and 50% data requiring >1-WL expressiveness, making the learning task more challenging.\n    *   **Models Compared**:\n        *   **1-WL GCN (1-GCN)**: A standard MPNN, expected to perform at chance (50%) on EXP.\n        *   **GCN-RNI**: 1-GCN enhanced with RNI, tested with various initialization distributions (Normal, Uniform, Xavier Normal, Xavier Uniform).\n        *   **GCN-x%RNI**: GCN-RNI with partial randomization (e.g., 12.5%, 50%, 87.5% of dimensions randomized).\n        *   **Higher-order GNNs**: PPGN (2-WL power), 1-2-3-GCN-L (emulating 2-WL on 3-node tuples), and 3-GCN (full 2-WL procedure over 3-node tuples).\n    *   **Key Performance Metrics & Results**:\n        *   **Expressiveness on EXP**: GCN-RNI models achieved near-perfect test accuracy (e.g., 97.3% to 98.0%), substantially surpassing the 50% baseline of 1-GCN and PPGN, and closely matching the performance of the computationally intensive 3-GCN (99.7%).\n        *   **Convergence**: GCN-RNI models showed slower convergence compared to deterministic GNNs.\n        *   **Partial RNI**: Partially randomizing initial node features (e.g., GCN-12.5%RNI) significantly improved model convergence and often led to higher accuracy compared to full RNI, demonstrating a practical trade-off.\n        *   **CEXP**: GCN-RNI models also performed well on CEXP, demonstrating robustness in mixed-expressiveness settings.\n        *   **Sparser Datasets**: Similar behavior was observed with sparser datasets and longer training, though with more volatility.\n\n*   **Limitations & Scope**\n    *   **Theoretical vs. Practical Construction Size**: While universal, the theoretical constructions for approximating arbitrary functions can be very large, a common limitation of universality results.\n    *   **Slower Convergence**: Empirically, GNNs with RNI exhibit slower convergence compared to standard GNNs, which might require longer training times.\n    *   **Volatility**: Experiments on sparser datasets showed more volatile behavior, suggesting potential sensitivity to graph density.\n    *   **Scope**: The universality result is for functions defined on graphs of *any fixed order* (i.e., fixed number of nodes `n`).\n\n*   **Technical Significance**\n    *   **Breaks Expressiveness Barrier**: Provides a theoretically sound and empirically validated method to overcome the 1-WL expressiveness limitation of standard MPNNs without resorting to computationally prohibitive higher-order GNNs.\n    *   **Justifies RNI**: Offers a strong theoretical justification for the widespread empirical success of random initialization in GNNs.\n    *   **Practical Viability**: Demonstrates that MPNNs with RNI combine high expressiveness with practical efficiency, making them a viable alternative to complex higher-order models.\n    *   **Future Research**: Opens new avenues for logically grounded theoretical studies of randomized MPNN models and for developing more expressive and memory-efficient GNN architectures.",
        "year": 2020,
        "citation_key": "abboud2020x5e"
      },
      {
        "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
        "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.",
        "summary": "Here's a focused summary of the paper \"GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS\" by Dwivedi et al. for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses a major limitation of standard Graph Neural Networks (GNNs), particularly message-passing GNNs (MP-GNNs): their inability to distinguish isomorphic nodes and other graph symmetries due to the absence of canonical positional information \\cite{dwivedi2021af0}. This limits their representation power, as node representations primarily depend on local structure, making it difficult to differentiate nodes with identical local neighborhoods but distinct global positions or roles.\n    *   This problem is critical because it restricts the theoretical expressivity of MP-GNNs, bounding them by the 1-Weisfeiler-Leman (WL) test, leading to poor performance on graphs with symmetries \\cite{dwivedi2021af0}. Existing solutions like stacking layers (prone to over-squashing) or higher-order GNNs (computationally expensive) have their own drawbacks.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to introduce positional encoding (PE) in GNNs often concatenate PEs with input node features, similar to Transformers \\cite{dwivedi2021af0}.\n    *   **Limitations of previous solutions**:\n        *   **Laplacian Eigenvectors (LapPE)**: While forming a meaningful coordinate system, they suffer from sign ambiguity (2^k possible sign values for k eigenvectors) and potential instability due to eigenvalue multiplicities, requiring the network to learn invariance \\cite{dwivedi2021af0}.\n        *   **Learnable position-aware embeddings**: Some rely on random anchor sets, which can limit generalizability on inductive tasks \\cite{dwivedi2021af0}.\n        *   **Pre-computed substructure information**: Methods encoding prior information (e.g., rings for molecules) require pre-computation and can have high computational complexity (O(n^k) for k-tuple substructures) \\cite{dwivedi2021af0}.\n        *   **Transformer-based GNNs**: While addressing long-range interactions, many use non-learnable PEs or inject learned PEs based on Laplacian eigenvectors, inheriting their limitations \\cite{dwivedi2021af0}.\n    *   This work positions itself by proposing a novel framework that *decouples* structural and positional representations, allowing both to be learned simultaneously and adaptively, addressing the limitations of fixed or ambiguously defined PEs \\cite{dwivedi2021af0}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **LSPE (Learnable Structural and Positional Encodings)**, a generic architecture that decouples structural and positional representations. Instead of merely injecting PEs at the input, LSPE allows both structural node features (`h`) and positional features (`p`) to be updated independently through message-passing layers \\cite{dwivedi2021af0}.\n    *   The update equations for LSPE are:\n        *   `h^(l+1)_i = f_h([h^l_i; p^l_i], {[h^l_j; p^l_j] | j ∈ N(i)}, e^l_ij)`\n        *   `e^(l+1)_ij = f_e(h^l_i, h^l_j, e^l_ij)`\n        *   `p^(l+1)_i = f_p(p^l_i, {p^l_j | j ∈ N(i)}, e^l_ij)`\n        where `f_h`, `f_e`, `f_p` are learnable functions, and `[;]` denotes concatenation \\cite{dwivedi2021af0}.\n    *   **Novelty**:\n        *   **Decoupled Learning**: The key innovation is the explicit separation and simultaneous learning of structural and positional representations throughout the GNN layers, rather than just injecting static PEs at the input \\cite{dwivedi2021af0}. This allows the network to adaptively adjust positional information to the task.\n        *   **Random Walk Positional Encoding (RWPE)**: A novel initialization scheme for PEs based on the k-step random walk diffusion process, specifically using the landing probability of a node to itself (`RW^k_ii`). This avoids the sign ambiguity issues of LapPE and provides unique node representations for many real-world graphs \\cite{dwivedi2021af0}.\n        *   **Positional Loss**: The framework incorporates an optional `Loss_LapEig` term alongside the task loss, encouraging the learned PEs to form a coordinate system constrained by graph topology, similar to Laplacian eigenvectors \\cite{dwivedi2021af0}.\n\n*   **Key Technical Contributions**\n    *   **Novel Architecture (LSPE)**: A generic framework that enables GNNs to learn and update both structural and positional representations concurrently, enhancing expressivity while maintaining linear complexity \\cite{dwivedi2021af0}.\n    *   **Random Walk Positional Encoding (RWPE)**: A new, sign-ambiguity-free method for initializing positional embeddings, shown to be effective and outperform LapPE in experiments \\cite{dwivedi2021af0}.\n    *   **Positional Loss Integration**: The ability to incorporate a `Loss_LapEig` to guide the learning of positional embeddings, ensuring they capture meaningful graph topology \\cite{dwivedi2021af0}.\n    *   **Generalizability**: The LSPE framework is demonstrated to be applicable to various GNN architectures, including sparse MP-GNNs (GatedGCN, PNA) and fully-connected Transformer-based GNNs (SAN, GraphiT) \\cite{dwivedi2021af0}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed LSPE architecture was evaluated by instantiating it with both sparse MP-GNNs (GatedGCN-LSPE, PNA-LSPE) and Transformer-based GNNs (SAN-LSPE, GraphiT-LSPE) \\cite{dwivedi2021af0}.\n    *   **Datasets**: Experiments were performed on three standard molecular benchmarks (ZINC, QM9, PCQM4Mv2) and three non-molecular benchmarks (CSL, Pattern, Cluster) \\cite{dwivedi2021af0}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   LSPE consistently improved performance across various GNN classes, showing performance increases from 1.79% up to 64.14% on molecular datasets \\cite{dwivedi2021af0}.\n        *   On molecular benchmarks, LSPE instances surpassed previous state-of-the-art (SOTA) on one dataset (PCQM4Mv2) by a considerable margin (26.23%) and achieved SOTA-comparable scores on the other two \\cite{dwivedi2021af0}.\n        *   RWPE consistently outperformed LapPE, suggesting that learning sign invariance is more challenging than dealing with potential non-uniqueness for some nodes \\cite{dwivedi2021af0}.\n        *   Sparse MP-GNNs with LSPE were found to outperform fully-connected Transformer-based GNNs with LSPE, suggesting a path towards efficient yet powerful graph architectures \\cite{dwivedi2021af0}.\n\n*   **Limitations & Scope**\n    *   **RWPE Uniqueness Assumption**: While RWPE generally provides unique node representations for real-world graphs (e.g., ZINC molecules), it may not be unique for all nodes in synthetic strongly regular graphs (e.g., CSL graphs, where all nodes are isomorphic) \\cite{dwivedi2021af0}. However, it can still distinguish isomorphic graph classes.\n    *   **LapPE Sign Ambiguity**: The paper acknowledges that LapPE's inherent sign ambiguity (2^k possibilities) makes it harder for networks to learn, which is a limitation of using LapPE as an initial PE \\cite{dwivedi2021af0}.\n    *   **Scope of Applicability**: The framework is generic and applicable to any MP-GNN architecture, including sparse and fully-connected variants, making it broadly useful for various graph learning tasks \\cite{dwivedi2021af0}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: LSPE significantly enhances the expressivity and performance of GNNs by providing a principled way to learn and adapt positional information, addressing a fundamental limitation of MP-GNNs \\cite{dwivedi2021af0}.\n    *   **Improved Discriminative Power**: By decoupling and learning positional representations, GNNs can better differentiate between structurally similar but positionally distinct nodes, overcoming the 1-WL test limitations without resorting to computationally expensive higher-order methods \\cite{dwivedi2021af0}.\n    *   **Potential Impact**: This work opens new avenues for designing more powerful and efficient GNNs, particularly for tasks requiring fine-grained understanding of node roles and global graph structure (e.g., molecular property prediction, drug discovery). The finding that sparse MP-GNNs with LSPE can outperform fully-connected Transformer-GNNs suggests a promising direction for developing highly efficient yet expressive models \\cite{dwivedi2021af0}.",
        "year": 2021,
        "citation_key": "dwivedi2021af0"
      },
      {
        "title": "Discovering Invariant Rationales for Graph Neural Networks",
        "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
        "summary": "Here's a focused summary of the paper \\cite{wu2022vcx} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Existing intrinsically interpretable Graph Neural Networks (GNNs) for rationalization often rely on \"shortcut features\" or data biases to compose rationales and make predictions. These shortcuts are unstable and change in out-of-distribution (OOD) data, leading to a significant drop in both interpretability (rationales don't reflect true causal patterns) and predictive performance on OOD data.\n    *   **Importance & Challenge**: Understanding \"what knowledge drives the model\" is crucial for auditing GNNs, justifying predictions, and enabling real-world applications (e.g., drug discovery). The challenge lies in identifying the *causal* patterns that are stable across different data distributions, rather than spurious correlations, especially when environments are not explicitly observable in the training data.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: GNN rationalization falls into post-hoc explainability (separate explanation method) and intrinsic interpretability (rationalization module within the model). This work focuses on intrinsic interpretability, where methods like graph attention and pooling operators generate masks on input graphs to identify rationales.\n    *   **Limitations of Previous Solutions**: Current intrinsic rationalization methods, despite their appeal, are shown to be prone to exploiting data biases and shortcuts. They minimize empirical risk, which can lead to learning non-causal statistical associations rather than the true underlying causal mechanisms, resulting in poor generalization to OOD data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{wu2022vcx} proposes Discovering Invariant Rationale (DIR), a novel invariant learning strategy for intrinsically interpretable GNNs. DIR aims to identify causal patterns that are stable (invariant) across different data distributions while filtering out unstable, spurious patterns.\n    *   **Novelty**:\n        *   **Causal Intervention for Environment Generation**: Unlike prior invariant learning (IL) methods that assume observable environments, DIR generates \"interventional distributions\" by performing causal interventions (`do(S=s)`) on the non-causal part (`S`) of the input graph. This allows the model to simulate multiple environments from a standard training set, enabling the distinction between causal and non-causal patterns.\n        *   **DIR Principle**: The learning strategy minimizes both the average s-interventional risk and the variance of these risks across different interventional distributions. This encourages the model to find rationales whose relationship with the label is stable.\n        *   **Modular Architecture**: DIR consists of four key modules:\n            1.  **Rationale Generator**: Splits the input graph into a potential causal part (`~c`) and a non-causal part (`~s`) by masking edges based on importance scores.\n            2.  **Distribution Intervener**: Creates interventional distributions by sampling non-causal parts from a memory bank and replacing the original non-causal part of an instance, forming an intervened pair `(~c_j, ~s_i)`.\n            3.  **Graph Encoder**: A shared GNN encoder processes both `~c` and `~s` into graph representations.\n            4.  **Two Classifiers**: One classifier (`phi_c`) predicts from the causal part (`^y_~c`), and another (`phi_s`) predicts from the non-causal part (`^y_~s`). The joint prediction `^y` under intervention is formulated as `^y_~c` masked by `sigmoid(^y_~s)`, where the sigmoid function helps compensate for spurious biases.\n        *   **Optimization Strategy**: The overall objective combines the DIR principle's invariant risk minimization with an additional loss for the non-causal classifier (`phi_s`) that is *not* backpropagated to other components. This encourages `phi_s` to learn spurious biases from non-causal features without interfering with the causal representation learning. During inference, only the causal rationale `~c` and its prediction `^y_~c` are used.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the Discovering Invariant Rationale (DIR) algorithm, a new invariant learning approach specifically tailored for intrinsically interpretable GNNs.\n    *   **Causal Intervention Mechanism**: A method to generate interventional distributions via `do(S=s)` operations on the non-causal graph components, addressing the challenge of unobservable environments in standard IL settings.\n    *   **Theoretical Justification**: Provides causality-theoretic analysis to guarantee that the DIR principle can discover invariant rationales and that the oracle model respects this principle.\n    *   **System Design**: A modular framework integrating a rationale generator, distribution intervener, shared encoder, and dual classifiers with a specialized optimization strategy for invariant learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate DIR's effectiveness in discovering causal features and improving generalization, as well as to understand its learning patterns.\n    *   **Datasets**:\n        *   **Spurious-Motif (synthetic)**: A controlled dataset where graph labels are *solely* determined by a \"motif\" (causal part), while \"bases\" (non-causal part) are introduced with varying degrees of spurious correlation to the label in the training set. Testing data has random motif-base attachments to assess OOD generalization.\n        *   Three real-world graph classification datasets (details in Appendix D).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   DIR consistently demonstrated superior generalization ability compared to state-of-the-art invariant learning methods (e.g., IRM \\cite{arjovsky2019invariant}, V-REx \\cite{krueger2021out}, GroupDRO \\cite{sagawa2019distributionally}).\n        *   It also outperformed attention- and pooling-based rationalization methods (e.g., GAT \\cite{velickovic2018graph}, GNNExplainer \\cite{ying2019gnnexplainer}) in terms of interpretability (discovering true causal features) and predictive performance on OOD data.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper assumes a causal view of the data-generating process with a clear distinction between causal and non-causal parts, and that the causal part is the sole determinant of the ground-truth label. The effectiveness of the rationale generator in accurately splitting the graph into `~c` and `~s` is crucial. The `do(S=s)` intervention relies on the ability to isolate and manipulate the non-causal part.\n    *   **Scope of Applicability**: The current implementation is demonstrated for graph classification tasks. While the paper states the invariant learning algorithm is \"suitable for any deep models,\" its specific implementation details (e.g., rationale generator for graphs) are tailored to GNNs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wu2022vcx} significantly advances the field of interpretable GNNs by moving beyond statistical correlations to identify truly causal and invariant rationales. It addresses a critical weakness of existing methods: their vulnerability to data biases and poor OOD generalization.\n    *   **Potential Impact**: This work provides a robust framework for building more trustworthy and generalizable GNNs. By ensuring that models learn from causal patterns, it can lead to more reliable insights in sensitive applications (e.g., drug discovery, material science) where understanding the *why* behind predictions is paramount. The novel use of causal interventions to generate environments opens new avenues for invariant learning research in settings where explicit environment labels are unavailable.",
        "year": 2022,
        "citation_key": "wu2022vcx"
      },
      {
        "title": "The expressive power of pooling in Graph Neural Networks",
        "abstract": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{bianchi20239ee}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of understanding regarding how graph pooling operators impact the expressive power of Graph Neural Networks (GNNs) \\cite{bianchi20239ee}. Specifically, there is no principled, theoretically grounded criterion to compare or design these operators.\n    *   **Importance & Challenge**: GNNs' ability to distinguish non-isomorphic graphs (their expressive power) is crucial for their performance. Hierarchical pooling is vital for building deep GNNs and learning abstract graph representations. However, current pooling evaluation relies heavily on empirical downstream task performance, which is indirect, influenced by many external factors, and doesn't directly assess expressiveness \\cite{bianchi20239ee}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has extensively characterized the expressive power of message-passing (MP) layers in GNNs, often linking it to the Weisfeiler-Lehman (WL) isomorphism test \\cite{bianchi20239ee}.\n    *   **Limitations of Previous Solutions**: These expressiveness studies are largely confined to \"flat GNNs\" (stacks of MP layers followed by a readout), overlooking the role and impact of hierarchical pooling operators \\cite{bianchi20239ee}. Existing empirical pooling evaluation methods are indirect and lack theoretical grounding, while other proposed criteria (e.g., spectral similarity) can be inconsistent \\cite{bianchi20239ee}. This work positions itself to fill this theoretical gap for hierarchical GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to derive sufficient conditions under which a graph pooling operator can fully preserve the expressive power of the preceding MP layers \\cite{bianchi20239ee}. This ensures that the combined GNN architecture remains injective, capable of distinguishing non-isomorphic graphs.\n    *   **The conditions (Theorem 1) are**:\n        1.  The preceding MP layers must produce distinct feature sums for WL-distinguishable graphs (i.e., `sum(XL_i)` != `sum(YL_i)` for non-isomorphic G1, G2) \\cite{bianchi20239ee}.\n        2.  The `SEL` (selection) function of the pooling operator must ensure that the sum of membership scores for each original node across all supernodes is a positive constant (`sum_j(s_ij) = lambda > 0`). This implies all original nodes contribute to the coarsened graph \\cite{bianchi20239ee}.\n        3.  The `RED` (reduction) function must compute pooled node features as a linear transformation of the original node features using the cluster assignment matrix (`XP = S^T XL`), effectively a convex combination \\cite{bianchi20239ee}.\n    *   **Novelty**: This work introduces the first principled, theoretically grounded criterion for evaluating and designing graph pooling operators based on their expressiveness, moving beyond indirect empirical performance metrics \\cite{bianchi20239ee}. It provides clear design principles and a diagnostic tool for existing methods.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Framework**: Formal derivation of sufficient conditions (Theorem 1) for graph pooling operators to preserve the expressive power of preceding MP layers, ensuring the GNN's ability to distinguish non-isomorphic graphs \\cite{bianchi20239ee}.\n    *   **Universal Criterion**: Introduction of a theoretically grounded and universal criterion for comparing and designing graph pooling operators, focusing on their expressiveness rather than indirect downstream task performance \\cite{bianchi20239ee}.\n    *   **Analysis of Existing Operators**: Identification of commonly used pooling operators (e.g., Top-k, ASAPool, SAGPool, PanPool) that fail to meet these expressiveness conditions, explaining their potential limitations. Conversely, dense pooling operators (e.g., DiffPool, MinCutPool, DMoN) and certain sparse ones (e.g., ECPool, k-MISPool) are shown to satisfy the conditions \\cite{bianchi20239ee}.\n    *   **Empirical Validation Setup**: Proposal of a simple yet effective experimental setup to empirically measure the expressive power of any GNN (including those with pooling layers) by directly testing its capability to perform a graph isomorphism test \\cite{bianchi20239ee}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper introduces a novel experimental setup designed to empirically verify the expressive power of GNNs equipped with pooling layers \\cite{bianchi20239ee}.\n    *   **Key Performance Metrics and Comparison Results**: The primary objective of this setup is to measure the GNN's capability to perform a graph isomorphism test \\cite{bianchi20239ee}. While the paper details the *design* of this validation method, specific experimental results, datasets, or comparative performance metrics are not provided in the given text. The focus is on establishing a direct method for expressiveness assessment.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The derived conditions are *sufficient* but not *necessary*, implying that other mechanisms for preserving expressiveness might exist outside this framework \\cite{bianchi20239ee}. The analysis primarily focuses on standard MP-GNNs, which are at most as powerful as the WL test. The `CON` (connection) function of the pooling operator, while not affecting expressiveness under these conditions, can still compromise the effectiveness of subsequent MP layers \\cite{bianchi20239ee}.\n    *   **Scope of Applicability**: The framework is applicable to hierarchical GNNs that interleave MP layers with pooling operators, providing guidance for both selecting existing pooling operators and designing new ones \\cite{bianchi20239ee}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the theoretical understanding of GNN expressiveness by extending the analysis to hierarchical architectures that incorporate pooling layers, a previously underexplored area \\cite{bianchi20239ee}. It provides a much-needed theoretical foundation for evaluating pooling operators, moving beyond purely empirical assessments.\n    *   **Potential Impact on Future Research**:\n        *   **Informed Design**: Offers a principled criterion for designing new, expressive graph pooling operators and for selecting appropriate existing ones, leading to more powerful and reliable GNN architectures \\cite{bianchi20239ee}.\n        *   **Debunking Misconceptions**: Provides a theoretical basis to understand why certain popular pooling operators might fail in specific scenarios, potentially clarifying common criticisms or misconceptions about graph pooling \\cite{bianchi20239ee}.\n        *   **Future Research Directions**: Opens avenues for exploring necessary conditions for pooling expressiveness and for developing pooling operators that explicitly satisfy these conditions while optimizing other factors like computational efficiency or specific task performance \\cite{bianchi20239ee}.",
        "year": 2023,
        "citation_key": "bianchi20239ee"
      },
      {
        "title": "On the Expressive Power of Geometric Graph Neural Networks",
        "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
        "summary": "Here's a focused summary of the paper by \\cite{joshi20239d0} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: On the Expressive Power of Geometric Graph Neural Networks \\cite{joshi20239d0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Characterizing and understanding the expressive power of Graph Neural Networks (GNNs) for *geometric graphs* embedded in Euclidean space. Standard GNNs and the Weisfeiler-Leman (WL) test, widely used for non-geometric graphs, are inapplicable to these systems.\n    *   **Importance & Challenge**: Geometric graphs, such as biomolecules, materials, and physical simulations, inherently possess both relational structure and geometric attributes (e.g., 3D coordinates, velocities). These geometric attributes must respect physical symmetries (permutations, rotation, reflection, translation). Standard GNNs fail to account for these symmetries, leading to a loss of physical meaning and transformation behavior. The challenge lies in developing a theoretical framework that can assess how well geometric GNNs can distinguish between geometrically non-isomorphic graphs while respecting these symmetries.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the established Weisfeiler-Leman (WL) graph isomorphism test, which has been a powerful tool for analyzing the expressive power of non-geometric GNNs.\n    *   **Limitations of Previous Solutions**: The standard WL framework and non-geometric GNNs do not directly apply to geometric graphs because they lack the ability to account for spatial symmetries. Geometric graphs exhibit a stronger notion of \"geometric isomorphism\" that requires invariance or equivariance to Euclidean transformations, which the WL test does not capture. Existing geometric GNNs (both G-equivariant and G-invariant) have shown empirical success, but their theoretical expressive power remained largely uncharacterized.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Geometric Weisfeiler-Leman (GWL) test**, a novel generalization of the WL test designed for discriminating geometric graphs while respecting physical symmetries.\n        *   GWL iteratively updates node \"colors\" and auxiliary \"geometric objects\" (`g(t)_i`) that aggregate geometric information from progressively larger t-hop neighborhoods.\n        *   It employs a **G-orbit injective and G-invariant function (I-HASH)** to compute scalar node colors, ensuring that neighborhoods identical up to a group action receive the same color.\n        *   Crucially, the aggregation of geometric objects (`g(t)_i`) is designed to be **injective and G-equivariant**, preserving local geometric orientation and information.\n    *   **Novelty/Difference**:\n        *   **Symmetry-Aware Isomorphism Test**: GWL is the first theoretical framework to extend the WL test to geometric graphs, explicitly incorporating invariance/equivariance to permutations, rotations, reflections, and translations.\n        *   **Granular Expressivity Analysis**: Unlike binary universal approximation, GWL provides a discrimination-based perspective, offering a more granular and practically insightful lens into geometric GNN expressivity by linking it to the ability to distinguish geometric (sub-)graphs.\n        *   **Unified Framework**: It provides a theoretical upper bound for the expressive power of both G-equivariant and G-invariant geometric GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Geometric Weisfeiler-Leman (GWL) test** as a theoretical upper bound for the expressive power of geometric GNNs.\n    *   **Theoretical Insights/Analysis**:\n        *   **Characterization of Expressivity**: GWL formally characterizes how key design choices influence geometric GNN expressivity:\n            *   **Invariant Layers' Limitations**: Invariant GNNs (and the proposed Invariant GWL, IGWL) have limited expressivity; they cannot distinguish \"1-hop identical\" geometric graphs and fail to compute non-local geometric properties (e.g., volume, centroid).\n            *   **Equivariant Layers' Power**: Equivariant GNNs (and GWL) distinguish a larger class of graphs by propagating geometric information beyond local neighborhoods through stacking equivariant layers.\n            *   **Role of Higher-Order Tensors/Scalarization**: Higher-order tensors and scalarization (e.g., `IGWL(k)` with higher body order `k`) are shown to enable maximally powerful geometric GNNs by providing more complete descriptors of local geometry (e.g., beyond just distances and angles).\n            *   **Depth**: Increasing depth (number of layers) allows GWL and equivariant GNNs to aggregate information from larger k-hop neighborhoods, distinguishing k-hop distinct graphs.\n        *   **Equivalence to Universal Approximation**: The paper proves an equivalence between a model's ability to discriminate geometric graphs (via GWL) and its universal approximation capabilities for G-invariant functions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Synthetic experiments were performed to demonstrate the practical implications of the theoretical findings. These experiments are available at `https://github.com/chaitjo/geometric-gnn-dojo`.\n    *   **Key Performance Metrics & Comparison Results**: The experiments focused on illustrating:\n        *   **Geometric Oversquashing with Increased Depth**: Demonstrating how increasing depth in geometric GNNs can lead to oversquashing, a phenomenon where distinct geometric information becomes indistinguishable.\n        *   **Utility of Higher Order Spherical Tensors**: Providing counterexamples that highlight the necessity and utility of higher-order spherical tensors for distinguishing complex local geometries that lower-order invariants (like distances and angles) cannot resolve. This empirically supports the theoretical claim about the importance of scalarization body order.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   GWL assumes finite-sized geometric graphs and features drawn from countable datasets.\n        *   The `HASH` and `I-HASH` functions are idealized injective and G-orbit injective maps, respectively, which are not necessarily continuous in practical implementations.\n        *   The theoretical analysis focuses on the *upper bound* of expressivity, and achieving this bound in practical GNNs requires specific conditions on aggregation, update, and readout functions (e.g., injectivity, equivariance).\n    *   **Scope of Applicability**: The framework is applicable to geometric graphs embedded in Euclidean space, particularly relevant for systems in biochemistry, material science, and physical simulations where physical symmetries are crucial. It primarily focuses on distinguishing geometric graphs and characterizing the expressive power of GNNs in this context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the theoretical understanding of geometric GNNs by providing the first principled framework (GWL) to analyze their expressive power in a symmetry-aware manner. It extends the foundational WL test to a critical domain where standard GNNs fall short.\n    *   **Potential Impact on Future Research**:\n        *   **Guiding Architecture Design**: The insights derived from GWL (e.g., the importance of equivariant layers, higher-order tensors, and understanding depth limitations) can directly guide the design of more powerful and expressive geometric GNN architectures.\n        *   **Benchmarking and Evaluation**: GWL provides a theoretical benchmark against which the expressive power of new geometric GNN models can be rigorously evaluated.\n        *   **Deeper Theoretical Understanding**: It offers a more granular and practically relevant perspective on expressivity compared to universal approximation, fostering deeper theoretical investigations into the capabilities and limitations of geometric deep learning models.",
        "year": 2023,
        "citation_key": "joshi20239d0"
      },
      {
        "title": "Learning to Pre-train Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have become the defacto standard for representation learning on graphs, which derive effective node representations by recursively aggregating information from graph neighborhoods. \nWhile GNNs can be trained from scratch, pre-training GNNs to learn transferable knowledge for downstream tasks has recently been demonstrated to improve the state of the art. \nHowever, conventional GNN pre-training methods follow a two-step paradigm: 1) pre-training on abundant unlabeled data and 2) fine-tuning on downstream labeled data, between which there exists a significant gap due to the divergence of optimization objectives in the two steps. \nIn this paper, we conduct an analysis to show the divergence between pre-training and fine-tuning, and to alleviate such divergence, we propose L2P-GNN, a self-supervised pre-training strategy for GNNs. \nThe key insight is that L2P-GNN attempts to learn how to fine-tune during the pre-training process in the form of transferable prior knowledge. To encode both local and global information into the prior, L2P-GNN is further designed with a dual adaptation mechanism at both node and graph levels. \nFinally, we conduct a systematic empirical study on the pre-training of various GNN models, using both a public collection of protein graphs and a new compilation of bibliographic graphs for pre-training. Experimental results show that L2P-GNN is capable of learning effective and transferable prior knowledge that yields powerful representations for downstream tasks. \n(Code and datasets are available at https://github.com/rootlu/L2P-GNN.)",
        "summary": "Here's a focused summary of the technical paper \\cite{lu20213kr} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses a \"significant gap\" in conventional Graph Neural Network (GNN) pre-training, which follows a two-step paradigm: 1) pre-training on abundant unlabeled data, and 2) fine-tuning on downstream labeled data. This gap arises from the divergence of optimization objectives between the two steps.\n    *   **Importance and Challenge**: Training GNNs typically requires extensive labeled data, which is often scarce and costly. While pre-training aims to learn transferable knowledge from unlabeled data, the objective mismatch between pre-training (optimizing for intrinsic graph properties) and fine-tuning (optimizing for downstream task performance) significantly hurts the generalization ability and quick adaptability of pre-trained GNN models. A key challenge is to develop a pre-training strategy that can simultaneously capture both local (node-level) and global (graph-level) information using *completely unlabeled* data, as existing methods either focus on node-level or require supervised information for graph-level pre-training.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{lu20213kr} positions itself against conventional GNN pre-training methods and draws inspiration from meta-learning, particularly Model-Agnostic Meta-Learning (MAML).\n    *   **Limitations of Previous Solutions**:\n        *   Prior GNN pre-training strategies (e.g., \\cite{hu2019graph}, \\cite{navarin2018graph}) are decoupled from the fine-tuning process, leading to a suboptimal initialization that is not inherently optimized for rapid adaptation to new tasks.\n        *   Existing methods for learning graph-level representations often require supervised information (e.g., \\cite{hu2020strategies}), failing to provide a fully self-supervised approach for both node and graph levels simultaneously.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{lu20213kr} proposes L2P-GNN (Learning to Pre-train GNNs), a self-supervised pre-training strategy that explicitly learns *how to fine-tune* during the pre-training process. This is achieved by framing the pre-training as an optimization-based meta-learning problem (similar to MAML). The pre-training objective is to find initial GNN parameters that, after a few gradient updates on a *simulated* support set (sampled from the pre-training graph), perform optimally on a *simulated* query set (also from the same pre-training graph).\n    *   **Novelty**:\n        *   The core innovation is the application of a meta-learning paradigm to GNN pre-training, directly optimizing for the model's ability to quickly adapt to new tasks, thereby alleviating the objective divergence.\n        *   It introduces a novel *dual adaptation mechanism* for completely self-supervised learning of both node-level and graph-level representations:\n            *   **Node-level adaptation**: Utilizes a self-supervised link prediction task, where the GNN learns to distinguish existing edges from randomly sampled non-edges.\n            *   **Graph-level adaptation**: Employs a contrastive learning approach, where the GNN is trained to maximize the mutual information between the representation of a whole graph and its sampled sub-structures, ensuring global context is preserved.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The first exploration of \"learning to pre-train GNNs\" to explicitly bridge the gap between pre-training and fine-tuning objectives \\cite{lu20213kr}.\n        *   A novel, completely self-supervised GNN pre-training strategy that simultaneously learns effective representations at both node and graph levels.\n        *   The design of a dual adaptation mechanism combining node-level link prediction and graph-level contrastive learning within a meta-learning framework.\n    *   **System Design or Architectural Innovations**: Integration of the MAML framework with self-supervised tasks specifically designed for graph-structured data.\n    *   **Theoretical Insights or Analysis**: Provides an analysis demonstrating the divergence in optimization objectives of conventional GNN pre-training and formally introduces a meta-learning objective to address it.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A systematic empirical study was performed to evaluate L2P-GNN's effectiveness across various GNN models.\n    *   **Key Performance Metrics and Comparison Results**: The evaluation involved pre-training on a public collection of protein graphs and a newly compiled large-scale bibliographic graph dataset. Experimental results demonstrate that L2P-GNN learns \"effective and transferable prior knowledge\" that yields \"powerful representations for downstream tasks,\" consistently and significantly outperforming state-of-the-art baselines.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: While not explicitly detailed in the provided text, meta-learning approaches like MAML can be computationally intensive due to the need for second-order gradients. The effectiveness of L2P-GNN relies on the quality and diversity of the simulated tasks constructed during pre-training.\n    *   **Scope of Applicability**: L2P-GNN is broadly applicable to various GNN architectures and is particularly beneficial in scenarios where labeled data for downstream tasks is scarce, but abundant unlabeled graph data is available for pre-training. It supports both node-level and graph-level downstream tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of the Technical State-of-the-Art**: \\cite{lu20213kr} significantly advances GNN pre-training by shifting the paradigm from decoupled pre-training to a meta-learning approach that directly optimizes for transferability and rapid adaptation. This addresses a fundamental limitation of previous two-step pre-training strategies.\n    *   **Potential Impact on Future Research**: This work opens new research directions for GNNs, encouraging further exploration of meta-learning techniques for graph representation learning, the development of more sophisticated self-supervised tasks for multi-level graph understanding, and the creation of more robust and generalizable GNN models for real-world applications with limited supervision.",
        "year": 2021,
        "citation_key": "lu20213kr"
      },
      {
        "title": "Training Graph Neural Networks with 1000 Layers",
        "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.24 \\pm 0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude. Please visit our project website https://www.deepgcns.org/arch/gnn1000 for more information.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Training Graph Neural Networks with 1000 Layers \\cite{li2021orq}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The primary challenge is the immense memory complexity (specifically, for intermediate activations) when training deep Graph Neural Networks (GNNs) on increasingly large graph datasets with millions of nodes and edges. This memory bottleneck prevents the development and deployment of deeper and wider GNNs.\n    *   **Importance & Challenge:** Deep and over-parameterized models have shown superior generalization capabilities in other domains (e.g., NLP), suggesting similar benefits for GNNs. However, existing GNN architectures' memory consumption scales linearly with the number of layers (O(LND)), making deep GNNs impractical on current hardware. Prior solutions like graph sampling or partitioning only partially address the issue, as memory still scales with depth, and they can introduce structural compromises or non-trainable propagation schemes.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Deep GNNs with Skip Connections:** Previous works successfully trained GNNs up to 56 or 112 layers using residual and dense connections to mitigate over-smoothing and vanishing gradients.\n        *   **Normalization and Regularization:** Various techniques like DropEdge, PairNorm, GraphNorm.\n        *   **Efficient Propagation Schemes:** Methods using K-power adjacency matrices or graph diffusion matrices to incorporate multi-hop information in a single layer.\n        *   **Mini-batch Training/Sampling:** GraphSAGE, VR-GCN, FastGCN, Cluster-GCN, GraphSAINT reduce the number of nodes or subgraphs processed per batch.\n    *   **Limitations of Previous Solutions:**\n        *   **Memory Scaling with Depth:** Even with skip connections, the memory complexity for activations remains O(LND), limiting depth.\n        *   **Non-trainable Propagation:** Efficient propagation schemes are often non-trainable, which can lead to sub-optimality and limited capacity for learning hierarchical features.\n        *   **Sampling Limitations:** Sampling methods, while reducing the N or M dimension, still incur memory costs that scale with the number of layers (e.g., O(RLBD) or O(LBD)), and can introduce hyperparameters or break important graph structures if not carefully tuned.\n    *   **Positioning:** This work directly addresses the *L* (depth) dimension of memory complexity, proposing orthogonal approaches that eliminate the dependency of memory consumption on network depth, allowing for unprecedentedly deep GNNs. These techniques can also be combined with existing sampling methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper investigates and combines several techniques:\n        *   **Grouped Reversible GNNs (RevGNNs):** This is the primary innovation. Inspired by reversible networks and grouped convolutions, the input vertex features are partitioned into groups. The forward pass is designed such that intermediate activations can be reconstructed during the backward pass, eliminating the need to store them.\n        *   **Parameter-Efficient GNNs:**\n            *   **Weight-tied GNNs:** Shares weights across all layers, significantly reducing parameter count.\n            *   **Deep Equilibrium GNNs (DEQ-GNNs):** Formulates the GNN as finding an equilibrium point of an infinite-depth, weight-tied network. The forward pass uses a root-finding algorithm, and gradients are computed via implicit differentiation, avoiding storage of intermediate states.\n    *   **Novelty/Difference:**\n        *   **O(1) Memory for Activations:** The Grouped Reversible GNNs reduce the memory complexity for activations from O(LND) to O(ND), making it independent of network depth. This is a significant departure from prior deep GNNs.\n        *   **Modified Dropout for Reversibility:** A novel approach to handle dropout in reversible GNNs by sharing dropout patterns across layers, preventing the memory cost from reverting to O(LND).\n        *   **First to 1000+ Layers:** This approach enables the training of GNNs with over 1000 layers, an order of magnitude deeper than previously reported GNNs.\n        *   **Adaptation of Advanced Architectures:** Successfully adapts reversible connections, weight-tying, and deep equilibrium models (previously successful in CV/NLP) to the GNN domain.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Grouped Reversible GNNs:** A new GNN architecture that achieves O(ND) memory complexity for activations, independent of network depth, by reconstructing intermediate states during backpropagation \\cite{li2021orq}.\n        *   **Layer-Shared Dropout:** A technique to apply dropout within reversible GNNs without incurring O(LND) memory overhead for dropout masks, by sharing a single dropout pattern across all layers \\cite{li2021orq}.\n        *   **Adaptation of Weight-Tied and Deep Equilibrium Networks for GNNs:** Demonstrates how these parameter-efficient paradigms can be applied to GNNs, offering constant parameter cost and O(ND) memory for DEQ-GNNs \\cite{li2021orq}.\n    *   **System Design/Architectural Innovations:** The specific design of the reversible GNN block, including how groups are processed and how information is exchanged (e.g., X'0 = sum(Xi)), is a key architectural innovation.\n    *   **Theoretical Insights/Analysis:** The paper provides an analysis of the memory and parameter complexities of the proposed methods, highlighting the O(ND) memory for reversible and equilibrium models and the constant parameter cost for weight-tied and equilibrium models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Extensive evaluation of reversible GNNs (RevGNN-x), weight-tied GNNs (WT-x), and deep equilibrium GNNs (DEQ-x) against residual GNNs (ResGNN-x).\n        *   Comparison across different depths (up to 112 layers for initial comparisons) and widths (channels).\n        *   Training of extremely deep models (RevGNN-Deep with 1001 layers, RevGNN-Wide with 448 layers).\n        *   Demonstration of generality by applying techniques to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN).\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Dataset:** Primarily evaluated on the large-scale `ogbn-proteins` dataset, also mentioned \"several datasets.\"\n        *   **Metrics:** ROC-AUC score for performance, and GPU memory consumption (GB).\n        *   **RevGNN-Deep (1001 layers, 80 channels):** Achieved 87.74 ±0.13 ROC-AUC on `ogbn-proteins` with only 2.86 GB GPU memory, outperforming the previous state-of-the-art while consuming an order of magnitude less memory \\cite{li2021orq}.\n        *   **RevGNN-Wide (448 layers, 224 channels):** Achieved 88.24 ±0.15 ROC-AUC on `ogbn-proteins`, ranking first on the leaderboard by a significant margin \\cite{li2021orq}.\n        *   **Memory Efficiency:** Reversible models consistently achieved comparable or better performance than baselines using a fraction of the memory (e.g., Figure 1 shows RevGNN-224 using ~11GB vs. ResGNN-224 running out of memory at 32GB) \\cite{li2021orq}.\n        *   **Parameter Efficiency:** Weight-tied and equilibrium models offered a good performance-to-parameter efficiency trade-off.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   Weight-tied models, while parameter-efficient, may suffer performance on large datasets due to fewer parameters, requiring increased width to compensate \\cite{li2021orq}.\n        *   Graph equilibrium models' training time vs. performance trade-off can be adjusted by tuning the number of iterations in each optimization step \\cite{li2021orq}.\n        *   The analysis primarily focuses on memory consumption induced by activations, assuming network parameters' memory footprint is negligible.\n    *   **Scope of Applicability:** The proposed techniques are general and can be applied to various GNN operators (GCN, GraphSAGE, GAT, DeepGCN) and can be combined with existing sampling-based methods for further memory reduction or performance boosts \\cite{li2021orq}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** Pushes the technical state-of-the-art in GNN depth by successfully training models with over 1000 layers, a feat previously considered impractical due to memory constraints \\cite{li2021orq}.\n    *   **Memory Bottleneck Resolution:** Provides a fundamental solution to the memory bottleneck for deep GNNs by achieving O(1) memory complexity with respect to depth, making deep and over-parameterized GNNs feasible on commodity hardware \\cite{li2021orq}.\n    *   **Enables Over-parameterization for GNNs:** By freeing up memory, the work demonstrates that deeper and wider GNNs can significantly outperform existing methods, suggesting that over-parameterization is beneficial for GNNs as it is for other deep learning models \\cite{li2021orq}.\n    *   **New Architectural Paradigms for GNNs:** Introduces and validates reversible connections, weight-tying, and deep equilibrium models as powerful architectural choices for designing efficient GNNs, opening new avenues for future research in GNN architecture design \\cite{li2021orq}.\n    *   **Practical Impact:** The ability to train highly performant, extremely deep GNNs on single commodity GPUs democratizes access to advanced GNN models for large-scale graph problems.",
        "year": 2021,
        "citation_key": "li2021orq"
      },
      {
        "title": "Graph Neural Networks in Network Neuroscience",
        "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain –namely brain graph. Relying on its non-euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of effectively analyzing and learning from brain connectivity data, which is inherently non-Euclidean (graph-structured), for tasks like understanding brain function, disease diagnosis, and synthesizing missing data. Existing connectomic datasets often suffer from missing observations (e.g., modalities, resolutions, timepoints) due to high acquisition costs and complex preprocessing \\cite{bessadok2021bfy}.\n    *   **Importance and Challenge:** Understanding brain connectivity is crucial for mapping neuronal activity, tracking brain changes over time, and early diagnosis of neurological disorders (e.g., Alzheimer's, autism) \\cite{bessadok2021bfy}. Traditional deep learning methods struggle with the non-Euclidean nature of brain graphs, leading to a significant loss of inherent topological properties and limiting their effectiveness \\cite{bessadok2021bfy}. There is a need for methods that can generate comprehensive brain mappings from limited data and robustly learn from complex graph structures.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions Graph Neural Networks (GNNs) as a superior alternative to traditional machine learning and deep learning (e.g., CNNs) for analyzing brain graphs \\cite{bessadok2021bfy}. It reviews existing approaches such as graph generative models for synthesizing missing brain data, graph integration models for creating population templates, and brain graph embedding techniques (e.g., in hyperbolic space) for visualizing topology and detecting pathological changes \\cite{bessadok2021bfy}.\n    *   **Limitations of Previous Solutions:** Traditional deep learning methods (like CNNs) do not generalize well to non-Euclidean graph data, overlooking relationships between nodes and local connectedness patterns, which results in a loss of crucial topological information \\cite{bessadok2021bfy}. Existing review papers on GNNs or neuroscience typically do not specifically discuss GNN-based methods for solving neuroscience problems, highlighting a gap this review aims to fill \\cite{bessadok2021bfy}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a review that highlights GNNs as the core technical approach for network neuroscience. GNNs are presented as a solution that preserves graph topological properties while learning to perform tasks \\cite{bessadok2021bfy}. The review details the propagation rule for Graph Convolutional Networks (GCNs), a prominent GNN model, as `Z = φ(eD^(-1/2) * eA * eD^(-1/2) * F * Θ)`, where `eA = A + I` and `eD` is a diagonal matrix \\cite{bessadok2021bfy}. It categorizes GNNs into GCNs, Graph Attention Networks (GATs), and message-passing mechanisms \\cite{bessadok2021bfy}. It also provides an overview of how brain graphs (morphological, functional, structural) are constructed \\cite{bessadok2021bfy}.\n    *   **Novelty/Difference (of the review itself):** The paper's innovation lies in providing a comprehensive, focused review and systematic taxonomy of GNN-based methods specifically applied to network neuroscience, distinguishing between \"graph-based\" (nodes are ROIs) and \"population-based\" (nodes are subjects) models \\cite{bessadok2021bfy}. This addresses a gap in existing literature by outlining current and future applications of GNNs in connectomics \\cite{bessadok2021bfy}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (reviewed):** The paper highlights the utility of GNNs (e.g., GCNs, GATs) for handling non-Euclidean brain graph data, preserving topological properties, and enabling tasks such as:\n        *   Brain graph prediction (e.g., synthesizing missing modalities, super-resolution, temporal evolution) \\cite{bessadok2021bfy}.\n        *   Brain graph integration (e.g., generating population-level brain templates) \\cite{bessadok2021bfy}.\n        *   Disease classification based on brain graphs \\cite{bessadok2021bfy}.\n    *   **Theoretical Insights or Analysis (of the review):** The paper provides a systematic categorization of GNN-based methods in network neuroscience and a detailed explanation of different brain graph types and their construction, emphasizing why GNNs are a natural fit for this domain \\cite{bessadok2021bfy}.\n\n*   **Experimental Validation**\n    *   As a review paper, it does not present new experimental results. However, it notes that the GNN-based frameworks discussed have been evaluated using existing connectomic datasets such as the Human Connectome Project (HCP), Baby Connectome Project (BCP), and Connectome Related to Human Disease (CRHD) \\cite{bessadok2021bfy}. The abstract mentions that GNNs lead to \"enhanced performance in various network neuroscience tasks\" \\cite{bessadok2021bfy}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of GNNs in the field, as discussed):** Only a limited number of GNN architectures have been applied to brain connectomes so far, with GCN being the most frequently used \\cite{bessadok2021bfy}.\n    *   **Scope of Applicability:** The review focuses on papers published between January 2017 and December 2020, specifically excluding non-GNN based methods (e.g., CNNs, traditional ML) for brain graph analysis \\cite{bessadok2021bfy}. The authors suggest the insights could generalize to broader \"omics\" fields like genomics \\cite{bessadok2021bfy}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** The paper advances the technical state-of-the-art by establishing GNNs as the leading approach for learning deep graph structures in network neuroscience, effectively overcoming the limitations of traditional deep learning on non-Euclidean data \\cite{bessadok2021bfy}. It provides a crucial structured overview and taxonomy, making GNN applications more accessible to researchers in the field \\cite{bessadok2021bfy}.\n    *   **Potential Impact on Future Research:** It charts a path for future research, guiding the better application of GNN models for neurological disorder diagnosis and population graph integration \\cite{bessadok2021bfy}. The insights can lead to novel diagnostic tools, reduce the need for extensive clinical scans and processing by enabling prediction of missing brain graph data, and promote the adoption of GNNs in network neuroscience and related \"omics\" fields \\cite{bessadok2021bfy}.",
        "year": 2021,
        "citation_key": "bessadok2021bfy"
      },
      {
        "title": "Combinatorial optimization and reasoning with graph neural networks",
        "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have mostly focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing the former. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at researchers in both optimization and machine learning.",
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n**CITATION REQUIREMENTS**: Always use \"\\cite{cappart2021xrp}\" when referencing this paper.\n\n---\n\n### Technical Paper Analysis: Combinatorial Optimization and Reasoning with Graph Neural Networks \\cite{cappart2021xrp}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the challenge of effectively applying machine learning (ML), particularly Graph Neural Networks (GNNs), to Combinatorial Optimization (CO) problems. Traditionally, CO methods solve problem instances in isolation, despite many practical instances stemming from related data distributions.\n    *   **Why is this problem important and challenging?** CO problems are often NP-hard and computationally intensive. Exploiting common patterns across instances using ML could lead to faster and more efficient algorithms for real-world applications (e.g., vehicle routing, scheduling). Key challenges for ML in CO include:\n        *   **Permutation invariance:** Graphs have no unique representation, requiring methods invariant to node reordering.\n        *   **Scalability and sparsity:** Real-world CO instances are often large and sparse, demanding scalable and sparsity-aware ML models.\n        *   **Expressivity:** Models must be expressive enough to detect and exploit critical structural patterns.\n        *   **Handling side information:** Incorporating auxiliary information like objective functions and user-defined constraints.\n        *   **Data efficiency:** Reducing the reliance on large amounts of labeled training data (which often means solving many hard CO instances).\n        *   **Generalization:** The ability to transfer learned knowledge to unseen instances, including those of different sizes or distributions.\n\n2.  **Related Work & Positioning**\n    *   **How does this work relate to existing approaches?** This paper is a conceptual review that synthesizes recent advancements in using GNNs for CO. It positions itself against:\n        *   **Traditional CO solvers:** Which focus on individual instances without leveraging data distributions.\n        *   **Early ML for CO:** Such as Hopfield networks and self-organizing maps, which were often applied to single instances rather than being trained over datasets.\n        *   **Other contemporary surveys:** It differentiates itself by providing a comprehensive, structured overview specifically on GNNs in CO, covering both heuristic and exact algorithms, and end-to-end algorithmic reasoning. Other surveys often have a broader ML focus, concentrate on specific ML paradigms (e.g., reinforcement learning), or miss recent GNN developments \\cite{cappart2021xrp}.\n    *   **Limitations of previous solutions:**\n        *   Classical CO algorithms heavily rely on human-made pre-processing and feature engineering, which can be costly, error-prone, and introduce biases that may not align with real-world data \\cite{cappart2021xrp}.\n        *   Earlier neural network applications for CO had limited success due to their instance-specific nature, lacking generalization across problem distributions \\cite{cappart2021xrp}.\n        *   Existing surveys, while valuable, often lack the specific focus and depth on GNNs' unique advantages and challenges within the CO domain, particularly regarding algorithmic reasoning \\cite{cappart2021xrp}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper reviews the application of Graph Neural Networks (GNNs) as the core technical method for addressing CO problems. GNNs compute vectorial representations for each node by iteratively aggregating features from neighboring nodes. This process is trained end-to-end to adapt to specific data distributions.\n    *   **What makes this approach novel or different?** The paper itself is a *review*, and its innovation lies in providing a novel, structured, and comprehensive overview of how GNNs are being used in CO. It highlights GNNs' inherent properties that make them suitable:\n        *   **Permutation invariance and equivariance:** GNNs are designed to automatically exploit symmetries in graph-structured data.\n        *   **Sparsity awareness:** Their local aggregation mechanism naturally handles sparse inputs, leading to more scalable models.\n        *   **Feature integration:** GNNs can incorporate multi-dimensional node and edge features, naturally exploiting cost and objective function information \\cite{cappart2021xrp}.\n        *   **End-to-end reasoning:** The review explores GNNs' potential to go beyond imitating classical algorithms, facilitating end-to-end algorithmic reasoning from raw input processing to solving abstracted CO problems \\cite{cappart2021xrp}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:** The paper *surveys* novel GNN-based algorithms and methods developed by others, categorizing their applications in CO. It does not propose new algorithms itself.\n    *   **System design or architectural innovations:** It reviews how GNNs are integrated into CO pipelines, either as direct solvers (e.g., predicting solutions) or as components enhancing existing exact solvers (e.g., guiding branch-and-bound).\n    *   **Theoretical insights or analysis:** The paper provides a conceptual analysis of GNNs' inductive bias, explaining how their permutation invariance and sparsity awareness effectively encode combinatorial and relational input. It also discusses the inherent trade-offs between scalability, expressivity, and generalization in GNN applications for CO \\cite{cappart2021xrp}.\n    *   **The paper's *own* contributions as a review are:**\n        1.  A complete, structured overview of GNN applications to CO, covering both heuristic and exact algorithms \\cite{cappart2021xrp}.\n        2.  A survey of recent progress in using GNN-based end-to-end algorithmic reasoners \\cite{cappart2021xrp}.\n        3.  Highlighting the shortcomings of GNNs in CO and offering guidelines and recommendations for addressing them \\cite{cappart2021xrp}.\n        4.  Providing a list of open research directions to stimulate future research in the field \\cite{cappart2021xrp}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted?** As a review paper, \\cite{cappart2021xrp} does not present new experimental validation. Instead, it references successful applications from the literature to illustrate the efficacy of GNNs in CO.\n    *   **Key performance metrics and comparison results:** The paper highlights examples like the work by Mirhoseini et al. (2021) on chip placement, where GNNs were used with reinforcement learning to generate optimized placements for Google's TPU accelerators. This approach demonstrated the ability to quickly generalize to unseen netlists and optimize power, performance, and area, showcasing the practical impact of GNNs in CO \\cite{cappart2021xrp}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The review identifies several limitations of current GNN approaches in CO:\n        *   **Data efficiency:** GNNs often require large amounts of labeled training data, which can be prohibitive in CO as it means solving many potentially hard instances \\cite{cappart2021xrp}.\n        *   **Scalability:** While GNNs scale linearly with the number of edges and parameters, scalability remains a challenge for extremely large real-world instances.\n        *   **Trade-offs:** There is an inherent trade-off between scalability, expressivity, and generalization that current GNN models must navigate \\cite{cappart2021xrp}.\n    *   **Scope of applicability:** The review focuses on the application of GNNs within the CO context, covering their use for finding primal solutions (heuristics), enhancing dual methods (exact solvers), and facilitating end-to-end algorithmic reasoning. It targets both optimization and machine learning researchers \\cite{cappart2021xrp}.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art?** By providing a comprehensive and structured overview, \\cite{cappart2021xrp} synthesizes the rapidly evolving field of GNNs for CO. It clarifies how GNNs' inherent properties (permutation invariance, sparsity awareness) make them uniquely suited for graph-structured combinatorial problems. The paper's systematic categorization of applications (heuristic, exact, algorithmic reasoning) helps to organize existing knowledge and identify key trends.\n    *   **Potential impact on future research:** The paper explicitly outlines shortcomings of current GNN approaches in CO and provides concrete guidelines and recommendations for tackling them. Crucially, it lists open research directions, aiming to stimulate and guide future research at the intersection of GNNs and CO, thereby facilitating further innovation in this emerging area \\cite{cappart2021xrp}.",
        "year": 2021,
        "citation_key": "cappart2021xrp"
      }
    ],
    "layer3_papers": [
      {
        "title": "Spatio-Spectral Graph Neural Networks",
        "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{geisler2024wli}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Spatial Message Passing Graph Neural Networks (MPGNNs) suffer from a limited \"receptive field\" (typically ℓ-hop neighborhood) and \"over-squashing,\" which severely restricts information exchange between distant nodes. This limits their expressivity and ability to model long-range interactions.\n    *   **Importance and Challenge**: Modeling long-range interactions is crucial for many graph-based tasks, as evidenced by the success of global models like transformers. Over-squashing makes MPGNNs ineffective for problems requiring global information, posing a significant challenge to their applicability in complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of MPGNNs, which have achieved breakthroughs in various domains. It draws inspiration from similar synergistic compositions in molecular point clouds (Kosmala et al., 2023) and sequence models like Mamba (Gu & Dao, 2023) or Hyena (Poli et al., 2023), which offer transformer-like properties with superior scalability.\n    *   **Limitations of Previous Solutions**: MPGNNs are inherently limited by their local message-passing scheme and the over-squashing phenomenon. The design space for spectral GNNs, in contrast to spatial MPGNNs, has been largely unexplored, leaving potential for novel architectural advancements.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Spatio-Spectral Graph Neural Networks (S2GNNs) \\cite{geisler2024wli}, a new modeling paradigm that synergistically combines spatially and spectrally parametrized graph filters.\n        *   S2GNNs utilize a partial eigendecomposition to enable spectral filters that are *spectrally bounded* (operating on a truncated frequency spectrum) but *spatially unbounded*, allowing for global information propagation.\n        *   The architecture can combine spatial and spectral filters additively or sequentially.\n        *   The spectral filter is defined as `Spectral(l)(H(l−1);V,λ) =V [ˆg(l)ϑ(λ)⊙ [V⊤f(l)θ(H(l−1))]]` (Eq. 3), ensuring permutation equivariance.\n    *   **Novelty/Difference**:\n        *   **Synergistic Combination**: The core innovation is the principled combination of local spatial message passing with global spectral filtering, addressing the limitations of each individually.\n        *   **Spectral Domain Neural Network**: The paper proposes the first neural network designed to operate directly in the spectral domain, allowing for data-dependent filtering and channel mixing with negligible computational cost for truncated spectra \\cite{geisler2024wli}.\n        *   **Directed Graph Filters**: S2GNNs generalize spectrally parametrized filters to directed graphs, expanding their applicability.\n        *   **Expressive Positional Encodings**: They introduce stable positional encodings (PEs) derived almost \"for free\" from the partial eigendecomposition, making S2GNNs strictly more expressive than the 1-Weisfeiler-Lehman (WL) test \\cite{geisler2024wli}.\n        *   **Over-squashing Mitigation**: The global nature of spectral filters inherently vanquishes over-squashing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The S2GNN architecture itself, which integrates spatial and spectral filtering for enhanced graph learning \\cite{geisler2024wli}.\n        *   A method for parametrizing spectral filters using Gaussian smearing and linear transformations, designed for stability and expressivity.\n        *   The concept and implementation of a neural network operating directly within the spectral domain.\n        *   Adaptation of spectral filters for directed graphs.\n        *   Novel, cost-effective positional encodings derived from the graph's eigendecomposition.\n    *   **Theoretical Insights or Analysis**:\n        *   **Over-squashing Vanquished**: Theorem 2 formally proves that S2GNNs overcome over-squashing by demonstrating a uniformly lower-bounded Jacobian sensitivity, ensuring effective long-range information exchange.\n        *   **Superior Approximation Bounds**: Theorems 3 and 4 establish strictly tighter approximation-theoretic error bounds for S2GNNs compared to MPGNNs. This is particularly significant for approximating discontinuous or unsmooth ground truth filters, where MPGNNs converge exceedingly slowly.\n        *   **Locality and Spectral Smoothness**: The analysis connects the locality of a filter to the smoothness of its Fourier transform, providing a complementary perspective on MPGNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Empirical verification of MPGNN shortcomings (e.g., over-squashing on \"Clique Path\" graphs) and how S2GNNs overcome them.\n        *   Approximation quality comparison of S2GNNs against purely spatial and spectral filters for a discontinuous target filter.\n        *   Performance evaluation on the challenging peptides-func long-range benchmark tasks (Dwivedi et al., 2022).\n        *   Scalability tests on large graphs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   S2GNNs achieved a new state-of-the-art on the peptides-func benchmark, outperforming MPGNNs and graph transformers with approximately 40% fewer parameters \\cite{geisler2024wli}.\n        *   They demonstrated competitive performance with state-of-the-art sequence models.\n        *   Empirical results on \"Clique Path\" graphs confirmed that spectral filters (and thus S2GNNs) do not exhibit over-squashing (Fig. 5).\n        *   S2GNNs scaled efficiently to millions of nodes on a 40 GB GPU with vanilla full-graph training, exhibiting runtime and space complexity equivalent to MPGNNs \\cite{geisler2024wli}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The efficiency of spectral filters relies on truncating the frequency spectrum (parameter `k` or `λcut`).\n        *   Theoretical guarantees for approximation assume the spectral filter is bandlimited and a universal approximator on a specific interval.\n        *   The default choice for the spectral filter's frequency band is low frequencies, though the method is adaptable.\n    *   **Scope of Applicability**: S2GNNs are designed as general-purpose GNNs, particularly well-suited for tasks requiring effective long-range interactions and where over-squashing is a concern. They are applicable to both undirected and directed graphs.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: S2GNNs introduce a novel and theoretically grounded GNN paradigm that fundamentally addresses the long-standing issues of over-squashing and limited receptive fields in MPGNNs \\cite{geisler2024wli}. By synergistically combining spatial and spectral approaches, they achieve superior expressivity and approximation capabilities, validated by state-of-the-art empirical performance on challenging long-range tasks.\n    *   **Potential Impact on Future Research**: This work opens up a significant new design space for GNN architectures, encouraging further exploration into hybrid spatial-spectral models. The proposed spectral domain neural network, directed graph filters, and \"free\" positional encodings offer concrete avenues for future research, potentially leading to more powerful, efficient, and expressive GNNs for a wider range of applications.",
        "year": 2024,
        "citation_key": "geisler2024wli"
      },
      {
        "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
        "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
        "summary": "Here's a focused summary of the paper \"UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND\" \\cite{kang2024fsk} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Traditional continuous Graph Neural Networks (GNNs) rely on integer-order differential equations, which model instantaneous, local changes (Markovian updates) in node features. This approach struggles to capture long-term dependencies and memory effects inherent in many real-world graph dynamics.\n    *   **Importance & Challenge:** Many real-world graphs (e.g., social, biological, internet networks) exhibit non-local, memory-dependent behaviors and scale-free hierarchical (fractal) structures. Integer-order models are insufficient to accurately describe these complex dynamics, potentially leading to limitations like oversmoothing and suboptimal graph representation learning.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior continuous GNNs (e.g., GRAND, GRAND++, GraphCON, CDE, GREAD) leverage integer-order Ordinary Differential Equations (ODEs) for information propagation, typically using first or second-order derivatives.\n    *   **Limitations of Previous Solutions:** These models are restricted to integer-order derivatives, implying Markovian update mechanisms where feature evolution depends only on the present state. This prevents them from inherently capturing the non-local properties and memory-dependent dynamics crucial for systems with self-similarity or anomalous transport. Other works using fractional calculus either apply it to graph shift operators with integer-order ODEs or to gradient propagation during training, not to the core node feature updating process itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the FRactional-Order graph Neural Dynamical network (FROND) framework \\cite{kang2024fsk}, which replaces the integer-order differential operator in continuous GNNs with the Caputo fractional derivative (DβtX(t) = F(W,X(t)), β > 0).\n    *   **Novelty:**\n        *   **Generalization of Continuous GNNs:** FROND generalizes existing integer-order continuous GNNs by allowing the derivative order β to be any positive real number, effectively subsuming them as special cases when β is an integer.\n        *   **Memory-Dependent Dynamics:** By employing the Caputo fractional derivative, FROND inherently integrates the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics.\n        *   **Non-Markovian Random Walk Interpretation:** For the fractional linear diffusion model (F-GRAND-l), the paper provides an interpretation from a non-Markovian random walk perspective, where the walker's complete path history influences future steps, contrasting with the Markovian walks of traditional models.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Proposed a novel, generalized continuous GNN framework (FROND) that incorporates non-local fractional derivatives, laying the groundwork for a new class of GNNs with learnable memory-dependent feature-updating processes \\cite{kang2024fsk}.\n    *   **System Design/Architectural Innovations:** Demonstrated the seamless compatibility of FROND, showing how it can be integrated to augment the performance of existing integer-order continuous GNNs (e.g., F-GRAND, F-GRAND++, F-GREAD, F-CDE, F-GraphCON).\n    *   **Theoretical Insights/Analysis:**\n        *   Analytically established that the non-Markovian random walk in FROND leads to a slow algebraic rate of convergence to stationarity, which inherently mitigates oversmoothing, unlike the exponentially swift convergence in Markovian integer-order models.\n        *   Suggested a connection between the optimal fractional order β and the inherent \"fractality\" of graph datasets, offering a potential avenue for deeper structural insights.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The FROND framework was validated through extensive experiments comparing fractional adaptations of various established integer-order continuous GNNs (GRAND, GRAND++, GraphCON, CDE, GREAD) on diverse datasets \\cite{kang2024fsk}.\n    *   **Key Performance Metrics & Comparison Results:** The fractional adaptations consistently demonstrated improved performance compared to their integer-order counterparts. Detailed ablation studies were performed to provide insights into the choice of numerical schemes and parameters, underscoring the framework’s potential as an effective extension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on β ∈ (0,1] for initial conditions, though the broader definition for β > 0 is mentioned. The framework relies on numerical FDE solvers, which may introduce computational considerations.\n    *   **Scope of Applicability:** FROND is designed to enhance continuous GNNs for graph representation learning, particularly beneficial for datasets exhibiting non-local, memory-dependent behaviors, or fractal structures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FROND significantly advances the technical state-of-the-art by introducing a principled and generalized approach to incorporate memory-dependent dynamics and non-local interactions into continuous GNNs, overcoming the limitations of purely Markovian integer-order models \\cite{kang2024fsk}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new research directions for designing GNNs that can model more complex, memory-dependent feature-updating processes.\n        *   Provides a robust mechanism for mitigating oversmoothing in GNNs through its algebraic convergence properties.\n        *   Offers a novel tool for exploring the underlying \"fractality\" and self-similarity of graph datasets by optimizing the fractional order β.\n        *   Serves as a powerful and compatible extension to enhance the performance of existing continuous GNN architectures.",
        "year": 2024,
        "citation_key": "kang2024fsk"
      },
      {
        "title": "How Interpretable Are Interpretable Graph Neural Networks?",
        "abstract": "Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the inadequate exploration of representational properties and limitations of existing Interpretable Graph Neural Networks (XGNNs), particularly their ability to provide faithful interpretations and generalize reliably. It identifies a \"huge gap\" in how current attention-based XGNNs approximate the underlying subgraph distribution, leading to degenerated interpretability.\n    *   **Importance and Challenge**: XGNNs are widely adopted in scientific applications (e.g., Physics, Chemistry, Materials), where faithful interpretation of predictions and generalization to unseen or Out-of-Distribution (OOD) graphs are crucial for scientific discovery and practice. The challenge lies in developing XGNNs that can reliably extract causal subgraphs and make robust predictions, while also providing theoretically grounded and empirically verifiable interpretations.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the paradigm of intrinsic XGNNs, which aim to provide reliable explanations and OOD generalizable predictions, contrasting with post-hoc explanation methods that are often suboptimal and sensitive to pre-trained GNN performance. Existing XGNNs typically use an information bottleneck or causal invariance to extract subgraphs, often employing attention mechanisms to learn edge/node sampling probabilities.\n    *   **Limitations of Previous Solutions**: Previous XGNNs, despite their success, lack a strong theoretical understanding of their representational properties and faithfulness. Specifically, the paper argues that the prevalent attention-based paradigm, which approximates the expected prediction over subgraphs (`E[f_c(G_c)]`) by applying the classifier to an expected \"soft\" subgraph (`f_c(E[G_c])`), is fundamentally flawed. This approximation only holds for linear classifiers, which is not the case for multi-layer GNNs, leading to a significant \"Subgraph Multilinear Extension (SubMT) approximation failure.\"\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a theoretical framework that formulates interpretable subgraph learning as approximating the **Subgraph Multilinear Extension (SubMT)**. SubMT is defined as the expected value of the subgraph classifier's output over a subgraph distribution, where edge sampling probabilities are derived from attention scores.\n    *   **Novelty**:\n        *   **SubMT Framework**: It's the first to propose SubMT as a theoretical lens to analyze the expressivity and faithfulness of XGNNs.\n        *   **Identification of Approximation Failure**: It rigorously proves (Proposition 3.3) that existing attention-based XGNNs, particularly those with non-linear GNNs (k > 1 layers), cannot accurately approximate SubMT due to the non-linearity of the classifier, leading to a gap between `f_c(E[G_c])` and `E[f_c(G_c)]`.\n        *   **Graph Multilinear ne T (GMT) Architecture**: To mitigate this, \\cite{chen2024woq} proposes GMT, a new XGNN architecture. GMT's core innovation is a new paradigm to effectively approximate SubMT by first performing **random subgraph sampling** onto the subgraph distribution. This directly estimates `E[f_c(G_c)]` more accurately than `f_c(E[G_c])`. A new classifier is then trained on these sampled subgraphs.\n        *   **Counterfactual Fidelity**: Introduces a novel faithfulness measure, \"(δ, ϵ)-counterfactual fidelity,\" which quantifies how sensitive predictions are to perturbations in the extracted interpretable subgraphs, directly linking interpretability to prediction robustness.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Framework**: The introduction of SubMT as the first theoretical framework for characterizing the expressivity of XGNNs.\n    *   **Formal Proof of Limitations**: A formal proof demonstrating the inherent approximation failure of existing attention-based XGNNs in fitting SubMT.\n    *   **Novel Architecture (GMT)**: The design of Graph Multilinear ne T (GMT), an XGNN architecture that is provably more powerful in approximating SubMT through random subgraph sampling.\n    *   **New Interpretability Metric**: The proposal of (δ, ϵ)-counterfactual fidelity, a novel measure for the causal faithfulness of XGNNs, addressing limitations of existing metrics for post-hoc explanations.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on a number of graph classification benchmarks, including 12 regular and geometric graph datasets. The validation aimed to empirically confirm the theoretical findings regarding SubMT approximation failure and demonstrate the superior performance of GMT.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Interpretability and Generalizability**: GMT significantly outperforms state-of-the-art XGNNs, achieving up to a 10% improvement in both interpretability and generalizability.\n        *   **Counterfactual Fidelity**: Experiments (e.g., on BA-2Motifs and Mutag, Fig. 2b, 2c) show that existing XGNNs (like GSAT) exhibit low counterfactual fidelity, while GMT (with different sampling configurations, e.g., GMT-sam-10, GMT-sam-100) achieves higher fidelity, validating the proposed interpretability measure and GMT's effectiveness.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on edge-centric subgraph sampling, though it states the method can be generalized to node-centric approaches. The SubMT formulation assumes a factorized Bernoulli distribution for subgraph sampling, which implicitly aligns with random graph data models. The computational cost of random subgraph sampling, while more accurate, might be higher than direct \"soft\" subgraph processing, though the paper doesn't explicitly detail this trade-off in the provided abstract/introduction.\n    *   **Scope of Applicability**: The framework and proposed GMT architecture are primarily applicable to intrinsic interpretable GNNs for graph classification tasks, with potential generalization to node-level tasks.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chen2024woq} significantly advances the theoretical understanding of XGNNs by introducing the SubMT framework, formally identifying a critical limitation in existing attention-based methods, and proposing a provably more powerful architecture (GMT) to overcome it. This leads to substantial empirical improvements in both interpretability and OOD generalization.\n    *   **Potential Impact on Future Research**: The SubMT framework provides a new theoretical foundation for analyzing and designing XGNNs, potentially guiding the development of more expressive and faithful interpretable models. The concept of counterfactual fidelity offers a robust metric for evaluating XGNN interpretability. GMT's approach of using random subgraph sampling to approximate expectations could inspire new architectural designs for GNNs dealing with discrete graph structures and probabilistic interpretations.",
        "year": 2024,
        "citation_key": "chen2024woq"
      },
      {
        "title": "Learning Invariant Representations of Graph Neural Networks via Cluster Generalization",
        "abstract": "Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are highly dependent on local graph structures, leading to significant performance drops when the test graph structure differs from the training graph structure, a phenomenon termed \"structure shift\" \\cite{xia20247w9}.\n    *   This structure shift causes GNNs to be biased towards specific training structure patterns, compromising their generalization ability and reliability in real-world dynamic graph environments (e.g., evolving citation or social networks) \\cite{xia20247w9}.\n    *   The problem is challenging because the underlying graph generation mechanism is often unknown and complex, making it difficult to simulate diverse structural environments or learn invariant representations through traditional methods \\cite{xia20247w9}.\n\n*   **Related Work & Positioning**\n    *   Existing graph Out-Of-Distribution (OOD) methods for node classification typically either assume knowledge of the graph generation process to derive regularization terms (e.g., \\cite{xia20247w9} citing [31,9]) or require sampling unbiased test data to match training distributions (e.g., \\cite{xia20247w9} citing [40]).\n    *   Limitations of previous solutions include heavy reliance on unknown graph generation processes or the inability to apply to whole graph-level structure shifts (inductive learning scenarios) due to the need for pre-sampling test data \\cite{xia20247w9}.\n    *   This work positions itself by proposing a novel mechanism that learns invariant representations without these restrictive assumptions, directly addressing the challenge of structure shift in the embedding space \\cite{xia20247w9}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Cluster Information Transfer (CIT) mechanism to learn invariant representations for GNNs by manipulating node representations in the embedding space \\cite{xia20247w9}.\n    *   **Clustering Process**: GNNs first learn initial node representations. Spectral clustering is then applied to these representations to obtain cluster assignments, optimized using a combination of cut loss (to group strongly connected nodes) and orthogonality loss (to prevent collapse and ensure balanced cluster sizes) \\cite{xia20247w9}.\n    *   **Cluster Information Transfer (CIT)**:\n        *   Cluster information is characterized by its mean (center) and standard deviation (aggregating scope) \\cite{xia20247w9}.\n        *   A node's representation is transferred from its original cluster's statistics to a randomly selected new cluster's statistics, while preserving its \"cluster-independent information\" \\cite{xia20247w9}. The transformation is $Z'^{(l)}_i = \\sigma(H_c^j) \\frac{Z^{(l)}_i - H_c^k}{\\sigma(H_c^k)} + H_c^j$ \\cite{xia20247w9}.\n        *   **Innovation**: Gaussian perturbations are added to the cluster statistics during transfer to increase uncertainty and diversity, generating more varied \"domains\" and enhancing robustness to unknown structure shifts \\cite{xia20247w9}.\n    *   **Objective Function**: The overall objective combines the standard cross-entropy loss on the newly generated representations with the clustering loss \\cite{xia20247w9}.\n    *   **Plug-in Design**: The CIT mechanism is designed as a plug-in, implemented before the GNN's final classification layer, making it backbone-agnostic and easily integrable with most existing GNN architectures \\cite{xia20247w9}.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism**: Introduction of the Cluster Information Transfer (CIT) mechanism for learning invariant representations in GNNs to address graph structure shift \\cite{xia20247w9}.\n    *   **Invariant Representation Learning**: A method to generate diverse local environments in the embedding space by transferring cluster information while preserving cluster-independent node features, without requiring explicit graph structure modification \\cite{xia20247w9}.\n    *   **Theoretical Analysis**: Provides theoretical insights and analysis demonstrating that the CIT mechanism mitigates the impact of changing clusters during structure shift, thereby enhancing model robustness \\cite{xia20247w9}.\n    *   **Architectural Flexibility**: The proposed mechanism is a \"friendly plug-in\" that can be easily integrated into and improve the generalization ability of most current GNNs \\cite{xia20247w9}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive evaluation across three typical structure shift scenarios:\n        1.  **Perturbation on graph structures**: Cora, Citeseer, Pubmed datasets, with randomly added or deleted edges \\cite{xia20247w9}.\n        2.  **Multiplex networks**: ACM, IMDB datasets, using different relation structures \\cite{xia20247w9}.\n        3.  **Multigraph**: Twitch-Explicit dataset, training on one network and testing on others \\cite{xia20247w9}.\n    *   **GNN Backbones & Baselines**: The CIT mechanism was plugged into GCN, GAT, APPNP, and GCNII. Comparisons were made against original GNNs and GNNs augmented with existing graph OOD methods (SR-GNN, EERM) \\cite{xia20247w9}. A variant without Gaussian perturbations (CIT-GNN(w/o)) was also tested \\cite{xia20247w9}.\n    *   **Key Performance Metrics**: Node classification accuracy and Macro-f1 score \\cite{xia20247w9}.\n    *   **Comparison Results**: The proposed CIT mechanism consistently and significantly improved the generalization ability of GNNs across all tested structure shift scenarios and GNN backbones \\cite{xia20247w9}. Paired t-tests confirmed statistical significance of improvements (e.g., * for 0.05 level and ** for 0.01 level) \\cite{xia20247w9}. The inclusion of Gaussian perturbations further enhanced performance, demonstrating their effectiveness in increasing diversity and robustness \\cite{xia20247w9}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis primarily focuses on the core transfer equation (Eq. 8), while the full mechanism includes Gaussian perturbations (Eq. 9) \\cite{xia20247w9}. The initial exploration of structure shift assumes a relatively simple scenario (e.g., two community structures) \\cite{xia20247w9}.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks and is applicable to GNNs that rely on message-passing and are sensitive to local structure \\cite{xia20247w9}. Its plug-in nature suggests broad compatibility with various GNN architectures.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a novel and effective solution to the critical and challenging problem of graph structure shift in GNNs, significantly enhancing their robustness and generalization capabilities in dynamic environments \\cite{xia20247w9}.\n    *   **New Paradigm for OOD on Graphs**: Offers an innovative approach to graph Out-Of-Distribution problems by generating diverse environments in the embedding space, circumventing the need for explicit graph structure modifications or knowledge of complex graph generation processes \\cite{xia20247w9}.\n    *   **Potential Impact on Future Research**: The plug-in design encourages widespread adoption and further research into invariant representation learning on graphs. The concept of cluster information transfer in embedding space could be extended to other graph learning tasks or types of distribution shifts, fostering more robust and reliable graph AI systems \\cite{xia20247w9}.",
        "year": 2024,
        "citation_key": "xia20247w9"
      },
      {
        "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
        "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Analysis of \"Explaining Graph Neural Networks via Structure-aware Interaction Index\" \\cite{bui2024zy9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Shapley-based explainability methods for Graph Neural Networks (GNNs) fail to adequately account for the inherent graph structure and high-order interactions among nodes.\n    *   **Importance & Challenge**:\n        *   GNNs are widely used in high-stakes domains, making their explainability crucial for trust and understanding.\n        *   Shapley values, while theoretically sound, create \"pathological\" or Out-Of-Distribution (OOD) perturbed graphs when applied naively to GNNs, leading to biased attribution scores.\n        *   Most methods focus solely on individual node/edge importance, neglecting complex interactions (motifs) and failing to identify groups of nodes that collectively influence predictions.\n        *   Existing explainers primarily identify positive contributions, overlooking structures that negatively affect predictions (crucial for counterfactual reasoning).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Positions itself within black-box, post-hoc, perturbation-based GNN explainers, similar to methods like SubgraphX and SAME that use Shapley values.\n        *   Generalizes both the Myerson value (which incorporates graph structure for node-wise importance) and the Shapley-Taylor index (which captures high-order interactions but is structure-agnostic).\n    *   **Limitations of Previous Solutions**:\n        *   **Shapley-based methods (e.g., Duval & Malliaros, Yuan et al., Ye et al.)**: Neglect graph structure during perturbation, potentially evaluating GNNs on disconnected or OOD inputs, leading to biased attributions.\n        *   **Node-wise importance focus (e.g., Zhang et al., Duval & Malliaros)**: Primarily attribute importance to individual nodes/edges, failing to capture complex interactions or \"motifs\" where groups of nodes act together. Greedy aggregation of node-wise scores often yields disconnected or unintuitive explanations.\n        *   **Lack of negative motif identification**: Most methods only identify substructures that positively affect predictions, missing those that hinder confidence or provide counterfactual insights.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   Introduces the **Myerson-Taylor interaction index**, a novel allocation rule that generalizes both the Myerson value and the Shapley-Taylor index. It incorporates graph structure by using an \"interaction-restricted function\" `f|E(T)` which evaluates disconnected subgraphs as the sum of their connected components, preventing OOD evaluations.\n        *   Proposes **Myerson-Taylor Structure-Aware Graph Explainer (MAGE)**, a two-phase explainer:\n            1.  **Interaction Matrix Computation**: Uses the second-order Myerson-Taylor index (Ψ²) to compute pairwise interaction scores between all nodes, forming an interaction matrix `B`.\n            2.  **Motif Search Optimization**: Solves an optimization problem to identify `m` non-overlapping, connected subgraphs (motifs) of total size at most `M`. This optimization maximizes the sum of *absolute* group attribution scores (GrAttr(S)), allowing for the discovery of both positive and negative influential motifs.\n    *   **Novelty/Differentiation**:\n        *   **Structure-Awareness**: Explicitly integrates graph connectivity into the attribution process via `f|E`, mitigating OOD bias inherent in structure-agnostic Shapley perturbations.\n        *   **High-Order Interactions**: Captures interactions beyond individual nodes, enabling the identification of meaningful motifs.\n        *   **Axiomatic Foundation**: Proves that the Myerson-Taylor index is the unique one satisfying five natural axioms accounting for graph structure and high-order interactions.\n        *   **Dual Motif Identification**: Capable of identifying both positively and negatively contributing motifs, providing a more comprehensive understanding of GNN predictions.\n        *   **Decoupled Computation**: Separates attribution calculation from subgraph search, allowing for parallel computation of the interaction matrix and potentially more tractable motif extraction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Myerson-Taylor interaction index** (Ψᵏ) for graph inputs, which simultaneously accounts for graph structure and high-order node interactions.\n    *   **Theoretical Insights**: Proof of the uniqueness of the Myerson-Taylor index based on a system of five axioms.\n    *   **System Design**: Development of **MAGE**, a two-phase explainer that leverages the second-order Myerson-Taylor index to construct an interaction matrix, followed by an optimization model for identifying multiple, non-overlapping, connected explanatory motifs (both positive and negative).\n    *   **Problem Formulation**: Formulation of the motif search as a quadratic multiple knapsack problem variant, which can be solved via linear relaxations and MILP solvers.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to evaluate MAGE's effectiveness in explaining GNN predictions.\n    *   **Datasets & Models**: Tested on ten diverse graph datasets (molecular prediction, image, sentiment classification) and three different GNN models.\n    *   **Performance Metrics & Comparison**:\n        *   Compared against seven popular, state-of-the-art baselines.\n        *   MAGE consistently provided superior subgraph explanations.\n        *   Achieved up to a **27.55% increase in explanation accuracy** compared to the best baseline.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   The number of connected subgraphs is still exponential in `|V|`, though significantly reduced for sparse graphs compared to all possible subgraphs.\n        *   Relies on Monte Carlo sampling for approximating the Myerson-Taylor index, similar to other game-based explainers, which introduces approximation errors.\n        *   Requires computing connected components for evaluated subsets, adding `O(|V|)` complexity per evaluation.\n    *   **Scope of Applicability**: Primarily designed for black-box GNN models and graph-structured inputs where identifying influential subgraphs (motifs) is desired. The framework does not explicitly exploit node or edge feature vectors, focusing on structural importance.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{bui2024zy9} significantly advances GNN explainability by providing a theoretically grounded, structure-aware, and interaction-capturing attribution method. It overcomes key limitations of prior Shapley-based approaches by explicitly integrating graph topology and enabling the discovery of complex, multi-node motifs.\n    *   **Potential Impact**:\n        *   Enables more accurate and intuitive explanations for GNN predictions, particularly in domains where structural patterns are critical (e.g., chemistry, biology).\n        *   Facilitates counterfactual reasoning by identifying both positive and negative influential motifs.\n        *   The axiomatic foundation provides strong theoretical guarantees for the proposed index.\n        *   The decoupled approach for attribution and motif search could inspire more efficient explainability frameworks for complex structured data.",
        "year": 2024,
        "citation_key": "bui2024zy9"
      },
      {
        "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
        "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
        "year": 2024,
        "citation_key": "li202444f"
      },
      {
        "title": "A Review of Graph Neural Networks in Epidemic Modeling",
        "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
        "summary": "This paper, `A Review of Graph Neural Networks in Epidemic Modeling` by Liu et al. \\cite{liu20242g6}, provides a comprehensive survey of Graph Neural Networks (GNNs) in the context of epidemic modeling.\n\nHere's a focused summary for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional mechanistic epidemic models suffer from oversimplified or fixed assumptions, leading to sub-optimal predictive power and an inability to efficiently capture complex relational information. Existing data-driven approaches like CNNs and RNNs also fall short in incorporating crucial relational data (e.g., human mobility, geographic connections, contact tracing) essential for accurate epidemic forecasting.\n    *   **Importance & Challenge**: Accurate and timely epidemic modeling is critical for public health decision-making, resource allocation, and effective intervention strategies. The challenge lies in developing models that can effectively integrate and leverage the complex, dynamic relational data inherent in disease transmission networks to provide more precise and generalizable predictions.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Mechanistic Models**: Such as SIR, SEIR, and SIRD, mathematically describe disease transmission but are limited by their reliance on fixed and often oversimplified assumptions.\n        *   **Data-driven Models (CNNs, RNNs)**: While successful in some epidemiological predictive tasks (e.g., forecasting case counts), they often lack the capability to incorporate relational information effectively.\n    *   **Limitations of Previous Solutions**: Both mechanistic and general deep learning models struggle to capture the intricate relational dynamics crucial for understanding and predicting disease spread, leading to biases and compromised accuracy.\n    *   **Positioning of this Work**: This paper \\cite{liu20242g6} distinguishes itself as a *comprehensive and pioneering review* specifically focused on the application of GNNs in epidemic modeling. Unlike prior surveys that are often narrow in scope (e.g., specific viruses, single tasks, or general deep learning without deep GNN integration), this work offers a broader and more detailed overview of GNN-based approaches across a spectrum of epidemic tasks, aiming to bridge existing literature gaps.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic and exhaustive review of GNNs in epidemic modeling. This involves:\n        *   Developing hierarchical taxonomies for both epidemic tasks (categorized into Detection, Surveillance, Prediction, and Projection) and GNN methodologies (categorized into Neural Models and Hybrid Models).\n        *   Providing detailed explanations and definitions for each task category.\n        *   Systematically examining existing GNN-based methodologies, including their technical details, data resources, and graph construction techniques.\n    *   **Novelty**: The innovation lies in providing the *first comprehensive and structured review* dedicated solely to GNNs in epidemiology. It offers novel taxonomies for organizing the field, a meticulous examination of existing methods, and a systematic identification of limitations and future research directions, thereby fostering interdisciplinary synergy.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Taxonomies**: Introduction of hierarchical taxonomies for epidemic tasks (Detection, Surveillance, Prediction, Projection) and GNN methodologies (Neural Models, Hybrid Models), providing a structured framework for understanding the field.\n    *   **Systematic Review Framework**: Establishment of a comprehensive framework for analyzing and categorizing GNN applications, encompassing task objectives, data types, graph construction techniques, and technical details of various GNN models.\n    *   **Identification of Limitations and Future Directions**: Systematically discusses the limitations of current GNN methods in epidemiology and proposes concrete, prospective research directions to guide future advancements.\n    *   **Resource Compilation**: Provides a curated list of relevant papers (via a GitHub repository) to serve as a valuable resource for researchers in this interdisciplinary domain.\n\n*   **5. Experimental Validation**\n    *   As a review paper, \\cite{liu20242g6} does not present new experimental results. Instead, its \"validation\" comes from the comprehensive synthesis of empirical evidence reported in the *reviewed literature*. The paper highlights that GNNs have demonstrated significant success in various epidemiological tasks, such as infection prediction, outbreak source detection, and intervention modeling, by effectively capturing relational dynamics and yielding more precise predictions compared to traditional methods. The thoroughness and systematic organization of the reviewed works implicitly validate the claims made about the utility and potential of GNNs in this field.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations (of reviewed methods, as identified by the paper)**: The paper points out that traditional mechanistic models suffer from oversimplified assumptions, while general deep learning models (CNNs, RNNs) often fail to incorporate crucial relational information. While GNNs offer advantages, the paper's unprovided Section 5 is dedicated to discussing the limitations of existing GNN methods from diverse perspectives.\n    *   **Scope of Applicability**: The review focuses exclusively on the application of Graph Neural Networks within the domain of *epidemic modeling*. It covers a broad range of epidemic tasks and GNN methodologies, aiming for a comprehensive overview within this specific interdisciplinary intersection.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper \\cite{liu20242g6} significantly advances the technical state-of-the-art by providing the first comprehensive and structured overview of GNN applications in epidemiology. It consolidates fragmented knowledge, clarifies the landscape of existing methods, and underscores the unique advantages of GNNs in handling complex relational data for epidemic tasks.\n    *   **Potential Impact**:\n        *   **Guidance for Researchers**: Offers a clear roadmap and foundational understanding for researchers entering or working in this interdisciplinary field, facilitating model selection and task comprehension.\n        *   **Promotion of Interdisciplinary Synergy**: Aims to bridge the GNN and epidemiology communities, fostering collaborative research and accelerating advancements in both fields.\n        *   **Identification of Future Research**: Systematically outlines promising future research directions, stimulating innovation and guiding efforts to address current limitations in GNN-based epidemic modeling.",
        "year": 2024,
        "citation_key": "liu20242g6"
      }
    ],
    "layer2_summary": "2. *Evolution Analysis:*\n\nThe evolution of Graph Neural Networks (GNNs) through these 16 papers reveals two dominant, interconnected trends: a relentless pursuit of **enhanced expressive power beyond the Weisfeiler-Leman (WL) barrier** and a growing focus on **GNN maturity and robustness for practical, reliable, and interpretable applications**.\n\n*Trend 1: Breaking the Weisfeiler-Leman (WL) Barrier: Enhancing GNN Expressive Power*\n\nThe fundamental limitation of standard Message Passing Neural Networks (MPNNs)—their equivalence to at most the 1-Weisfeiler-Leman (1-WL) test—has been a central problem driving GNN research. This limitation restricts their ability to distinguish non-isomorphic graphs and capture complex structural patterns.\n\n*   **Methodological progression**: Early work, such as [abboud2020x5e] \"The Surprising Power of Graph Neural Networks with Random Node Initialization (2020),\" provided a surprising theoretical breakthrough by proving that MPNNs with Random Node Initialization (RNI) are universal, effectively breaking the 1-WL barrier with a simple, efficient modification. This was followed by more sophisticated approaches. [balcilar20215ga] \"Breaking the Limits of Message Passing Graph Neural Networks (2021)\" introduced GNNML3, which leveraged non-linear custom functions of eigenvalues in the spectral domain, masked for spatial locality, to achieve expressive power experimentally equivalent to the 3-WL test, but with linear computational complexity. [dwivedi2021af0] \"Graph Neural Networks with Learnable Structural and Positional Representations (2021)\" shifted from static positional encodings (PEs) to learnable, adaptively updated PEs (LSPE), introducing Random Walk Positional Encoding (RWPE) to overcome issues like sign ambiguity in Laplacian PEs. This allowed GNNs to better distinguish isomorphic nodes and graph symmetries.\n\n    The focus then broadened to understanding and enhancing specific architectural components. [feng20225sa] \"How Powerful are K-hop Message Passing Graph Neural Networks (2022)\" provided the first theoretical characterization of K-hop message passing, proving it's more powerful than 1-WL but bounded by 3-WL, and introduced KP-GNN to integrate peripheral subgraph information for further gains. [bianchi20239ee] \"The expressive power of pooling in Graph Neural Networks (2023)\" extended WL-based expressivity analysis to hierarchical GNNs, deriving sufficient conditions for pooling operators to preserve expressivity. More recently, the field has specialized for different graph types and structural primitives. [joshi20239d0] \"On the Expressive Power of Geometric Graph Neural Networks (2023)\" introduced the Geometric Weisfeiler-Leman (GWL) test, extending expressivity analysis to geometric graphs with inherent spatial symmetries. Finally, [michel2023hc4] \"Path Neural Networks: Expressive and Accurate Graph Neural Networks (2023)\" and [zeng20237gv] \"Substructure Aware Graph Neural Networks (2023)\" explored leveraging richer structural information: PathNNs with \"annotated sets of paths\" and SAGNNs with novel \"Cut subgraph\" extraction and random walk encoding, both demonstrating the ability to surpass 3-WL limitations on challenging graphs.\n\n*   **Problem evolution**: The initial problem was the fundamental 1-WL bottleneck. This evolved into finding *efficient* ways to surpass 1-WL without prohibitive computational costs. Subsequent work addressed the expressivity of specific GNN components (e.g., K-hop aggregation, pooling layers) and extended the expressivity framework to specialized graph types (e.g., geometric graphs). The latest papers tackle the problem of encoding and leveraging richer structural primitives like paths and subgraphs to achieve even higher discriminative power.\n\n*   **Key innovations**: The universality proof for RNI, spectral filter design for efficient 3-WL equivalence, learnable and adaptive positional encodings (LSPE, RWPE), theoretical bounds for K-hop GNNs, the Geometric Weisfeiler-Leman (GWL) test, \"annotated sets of paths,\" and novel \"Cut subgraph\" extraction and encoding methods.\n\n*Trend 2: Maturation and Robustness: Towards Practical, Reliable, and Interpretable GNNs*\n\nAs GNNs demonstrated increasing expressive power, research simultaneously shifted towards addressing practical challenges, ensuring scalability, reliability, and interpretability, and establishing rigorous evaluation standards.\n\n*   **Methodological progression**: Early concerns about GNN reliability emerged with [he2020kz4] \"Stealing Links from Graph Neural Networks (2020),\" which introduced link stealing attacks, highlighting critical privacy vulnerabilities. To enable deeper and more powerful models, [li2021orq] \"Training Graph Neural Networks with 1000 Layers (2021)\" introduced Grouped Reversible GNNs (RevGNNs) and adapted Deep Equilibrium GNNs, reducing memory complexity for activations to O(ND) and enabling the training of GNNs with over 1000 layers. Addressing the challenge of data efficiency and transferability, [lu20213kr] \"Learning to Pre-train Graph Neural Networks (2021)\" proposed L2P-GNN, a meta-learning approach for self-supervised pre-training that explicitly optimizes for rapid adaptation to downstream tasks.\n\n    The need for trustworthy GNNs led to advancements in interpretability. [wu2022vcx] \"Discovering Invariant Rationales for Graph Neural Networks (2022)\" introduced DIR, an invariant learning strategy that uses causal interventions to identify stable, causal patterns, improving out-of-distribution generalization. Finally, the community recognized the critical need for standardized evaluation. [dwivedi20239ab] \"Benchmarking Graph Neural Networks (2023)\" provided a comprehensive, open-source benchmarking framework with diverse datasets and fair comparison protocols. Complementing this, [li2023o4c] \"Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking (2023)\" focused on rigorous evaluation for link prediction, introducing the Heuristic Related Sampling Technique (HeaRT) to generate hard, realistic negative samples. The increasing adoption of GNNs in diverse fields was also reflected in review papers like [bessadok2021bfy] \"Graph Neural Networks in Network Neuroscience (2021)\" and [cappart2021xrp] \"Combinatorial optimization and reasoning with graph neural networks (2021),\" which surveyed GNN applications in neuroscience and combinatorial optimization, respectively.\n\n*   **Problem evolution**: The initial problem was the lack of understanding of GNN vulnerabilities. This expanded to addressing practical bottlenecks like memory for deep models, the challenge of effective pre-training for transfer learning, and the need for trustworthy (causal, invariant) explanations. Finally, the community recognized the need for standardized, rigorous evaluation to ensure reliable progress and fair comparison of the proliferating GNN architectures.\n\n*   **Key innovations**: Link stealing attacks, reversible GNNs for O(1) memory with respect to depth, meta-learning for GNN pre-training, causal intervention for invariant rationales, and community-standard benchmarking frameworks (general and task-specific).\n\n3. *Synthesis:*\n\nThese works collectively chart an intellectual trajectory from understanding and overcoming fundamental expressive power limitations of GNNs to developing robust, scalable, and interpretable models for real-world applications. Their collective contribution is the advancement of GNNs from a promising but theoretically constrained paradigm to a mature, versatile, and rigorously evaluated tool capable of tackling complex graph-structured data across scientific and engineering domains."
  },
  "ac225094aab9e7b629bc5b3343e026dea0200c70": {
    "seed_title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
    "summary": "*Evolution Analysis:*\n\nThe evolution of Graph Neural Networks (GNNs) through this chain of papers reveals two overarching trends: a persistent drive towards **adaptive and localized information processing** to handle diverse graph structures, and a continuous effort to **expand GNN capabilities for real-world data imperfections and dynamics**. These trends reflect a maturation of the field, moving beyond idealized graph assumptions to tackle the complexities of real-world applications.\n\n**Trend 1: The Quest for Adaptive and Localized Information Processing**\n\n*   *Methodological progression*: Early GNNs, like the GCN, employed uniform message passing across all nodes. The first significant methodological shift came with **[klicpera20186xu] Predict then Propagate: Graph Neural Networks meet Personalized PageRank (2018)**, which decoupled feature transformation from graph propagation. While its propagation (Personalized PageRank) was still largely global, it introduced a mechanism (teleport probability) to control locality, a precursor to more fine-grained control. The concept of *adaptive filtering* emerged explicitly with **[luan202272y] Revisiting Heterophily For Graph Neural Networks (2022)**, which proposed the Adaptive Channel Mixing (ACM) framework. ACM adaptively combined low-pass, high-pass, and identity filters *node-wisely and locally*. This idea was further refined and theoretically grounded in **[han2024rkj] Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach (2024)**, which introduced NODE-MOE. NODE-MOE employs a sophisticated gating mechanism to dynamically select and apply different \"expert\" GNN filters (low-pass, high-pass, constant) to *individual nodes*, representing a significant leap in node-wise adaptability.\n\n*   *Problem evolution*: The initial problem addressed by **[klicpera20186xu] (2018)** was the oversmoothing and limited neighborhood range caused by coupling GNN depth with propagation steps. While solving this, it highlighted the need for flexible propagation. **[luan202272y] (2022)** then directly confronted the problem of GNNs' poor performance on heterophilic graphs, where connected nodes are dissimilar, a limitation stemming from the homophily assumption. This paper also pointed out the inadequacy of existing homophily metrics. Building on this, **[mao202313j] Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All? (2023)** delved deeper, identifying \"structural disparity\" – the pervasive existence of mixed homophilic and heterophilic patterns within *a single graph*. This paper rigorously demonstrated that GNNs suffer a \"performance disparity\" on minority structural patterns, proving that a \"one-size-fits-all\" approach is fundamentally flawed. **[han2024rkj] (2024)** directly tackled this \"one-size-fits-all\" problem, aiming to adaptively apply appropriate filters to individual nodes without explicit ground truth on node patterns.\n\n*   *Key innovations*: **[klicpera20186xu] (2018)**'s PPNP/APPNP decoupled architecture allowed for arbitrary propagation depth, a foundational innovation. **[luan202272y] (2022)** introduced novel post-aggregation homophily metrics and the ACM framework, enabling local, adaptive filtering. **[mao202313j] (2023)** provided a novel theoretical framework (non-i.i.d PAC-Bayesian generalization bound) and a new graph generation model (CSBM-Structure) to precisely analyze structural disparity, offering deep insights into GNN limitations. Finally, **[han2024rkj] (2024)**'s NODE-MOE framework, with its gating model and expert GNNs, represents a breakthrough in achieving fine-grained, node-wise adaptive filtering, significantly enhancing GNN robustness to mixed structural patterns.\n\n**Trend 2: Expanding GNN Capabilities for Real-world Data Imperfections and Dynamics**\n\n*   *Methodological progression*: The initial step towards robustness was **[klicpera20186xu] (2018)**'s solution to oversmoothing, a common issue in real-world graphs. The field then broadened its scope to dynamic data with **[longa202399q] Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities (2023)**, a survey that systematized the emerging field of Temporal GNNs (TGNNs). This conceptual shift paved the way for models designed for dynamic environments. **[liu2023v3e] Learning Strong Graph Neural Networks with Weak Information (2023)** introduced the Dual-channel Diffused Propagation then Transformation (D2PT) framework, a multi-faceted approach to handle simultaneous data deficiencies. Most recently, **[kang2024fsk] Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND (2024)** proposed FROND, a fundamental redefinition of the propagation mechanism in continuous GNNs, replacing integer-order derivatives with fractional derivatives to model non-local, memory-dependent dynamics.\n\n*   *Problem evolution*: Beyond oversmoothing, **[longa202399q] (2023)** highlighted the critical need for GNNs to operate on *temporal graphs*, as real-world systems are inherently dynamic. This survey identified the lack of a unified taxonomy and formalization for TGNNs as a major hurdle. **[liu2023v3e] (2023)** addressed the pervasive problem of \"weak information\" – incomplete structure, features, and labels – especially when these deficiencies occur *simultaneously* (extreme GLWI), a common scenario in real-world data collection. This paper also specifically tackled the \"stray node problem\" in sparse graphs. **[kang2024fsk] (2024)** identified a limitation in continuous GNNs: their reliance on integer-order differential equations, which model instantaneous, local changes (Markovian updates) and fail to capture long-term dependencies and memory effects prevalent in complex, real-world graph dynamics, contributing to oversmoothing.\n\n*   *Key innovations*: **[longa202399q] (2023)** provided the first comprehensive taxonomy and formalization for TGNNs, establishing a foundational reference for dynamic graph learning. **[liu2023v3e] (2023)**'s D2PT framework, with its dual-channel architecture (input graph and learned global graph) and Prototype Contrastive Alignment, offered a unified and robust solution for handling multiple, simultaneous data deficiencies. **[kang2024fsk] (2024)**'s FROND framework introduced fractional calculus to GNNs, enabling the modeling of non-local, memory-dependent dynamics and inherently mitigating oversmoothing through algebraic convergence, significantly enhancing the expressivity and robustness of continuous GNNs.\n\n3. *Synthesis*:\nThese works collectively trace a trajectory from foundational architectural improvements to increasingly sophisticated adaptations for real-world complexities. They demonstrate a unified intellectual pursuit to make Graph Neural Networks more robust, adaptive, and expressive, capable of handling diverse structural patterns, dynamic changes, and pervasive data imperfections, thereby advancing their applicability across a wider spectrum of real-world graph learning tasks.",
    "path": [
      "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "707142f242ee4e40489062870ca53810cb33d404",
      "b88f456daaf29860d2b59c621be3bd878a581a59",
      "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "900fc1f1d2b9ceeacbc92d74491b0a19c823af20"
    ],
    "layer1_papers": [
      {
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
        "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "summary": "Here's a focused summary of the paper \"PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK\" \\cite{klicpera20186xu} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   Existing neural message passing algorithms (e.g., GCNs) for semi-supervised graph classification are limited to considering only a small, local neighborhood (a few propagation steps) for node classification.\n    *   Extending this neighborhood size is challenging due to two main issues:\n        *   **Oversmoothing:** Too many propagation layers lead to Laplacian smoothing, causing node representations to become indistinguishable and losing local information. This is linked to GCNs converging to a global, root-node-independent limit distribution.\n        *   **Increased Complexity:** Most message passing schemes use learnable weight matrices in each layer, meaning a larger neighborhood requires a deeper neural network with more learnable parameters, leading to computational and memory overhead.\n    *   This fixed relationship between neighborhood size and neural network depth is a strong limitation, forcing compromises in model design.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself against various deep learning approaches on graphs, including node embedding methods (e.g., Node2Vec, DeepWalk) and supervised methods using both graph structure and node features.\n    *   It specifically focuses on message passing (or neighbor aggregation) algorithms (e.g., GCN, GAT, GraphSAGE) which have gained significant attention.\n    *   Previous attempts to improve these algorithms include attention mechanisms, random walks, edge features, and scalability improvements, but these still suffer from limited neighborhood range.\n    *   Methods using skip connections (e.g., residual connections) or batch normalization in deep GNNs have been proposed to mitigate oversmoothing, but these often complicate the model and introduce additional hyperparameters.\n    *   This work differentiates itself by addressing the range limitation through a fundamental architectural change rather than ad-hoc techniques or simply adding more layers/connections.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea:** The paper proposes to decouple the neural network's feature transformation (prediction) from the graph's propagation scheme. It introduces Personalized Propagation of Neural Predictions (PPNP) and its fast approximation, APPNP.\n    *   **Connection to PageRank:** It highlights the connection between GCN's limit distribution and PageRank, then leverages personalized PageRank to overcome the oversmoothing issue.\n    *   **Personalized PageRank for Propagation:** The propagation scheme is derived from personalized PageRank, which includes a \"teleport probability\" ($\\alpha$) to return to the root node. This ensures that the propagation process retains locality and avoids converging to a global, root-node-independent distribution, even with \"infinitely many\" propagation steps.\n    *   **PPNP Model:**\n        *   First, a neural network $f$ (e.g., a simple MLP) generates initial predictions $H$ for each node independently based on its features $X$.\n        *   These predictions $H$ are then propagated using a personalized PageRank matrix: $Z_{PPNP}=\\text{softmax} \\left( \\alpha \\left( I_n - (1-\\alpha)\\tilde{A} \\right)^{-1} H \\right)$ \\cite{klicpera20186xu}.\n        *   The model is trained end-to-end, allowing gradients to flow through the propagation scheme.\n    *   **APPNP (Approximate PPNP):** To address the $O(n^2)$ computational complexity of PPNP for large graphs, APPNP approximates the personalized PageRank via power iteration (a random walk with restarts). This iterative scheme maintains sparsity and achieves linear complexity $O(mK)$ where $m$ is the number of edges and $K$ is the number of power iterations.\n        *   $Z^{(0)}=H=f(X)$; $Z^{(k+1)}=(1-\\alpha)\\tilde{A}Z^{(k)}+\\alpha H$; $Z^{(K)}=\\text{softmax} \\left( (1-\\alpha)\\tilde{A}Z^{(K-1)}+\\alpha H \\right)$ \\cite{klicpera20186xu}.\n    *   **Adjustable Neighborhood:** The teleport probability $\\alpha$ allows for fine-tuning the balance between preserving locality and leveraging information from a large neighborhood, adapting to different graph types.\n\n4.  **Key Technical Contributions**\n    *   **Novel Propagation Scheme:** Derivation and application of a propagation scheme based on personalized PageRank, directly addressing the oversmoothing problem in GCNs.\n    *   **Decoupled Architecture:** Introduction of PPNP and APPNP, which explicitly separate the neural network's feature transformation from the graph's propagation. This allows for arbitrary propagation depth without increasing the neural network's parameter count or depth.\n    *   **Scalable Approximation:** APPNP provides an efficient, linear-time approximation of PPNP, making the approach practical for large graphs by avoiding dense matrix inversions.\n    *   **Adjustable Range:** The teleport probability $\\alpha$ offers a hyperparameter to control the effective range of propagation, enabling the model to leverage large neighborhoods while preserving local information.\n    *   **Flexibility:** The decoupled design allows PPNP/APPNP to be combined with any state-of-the-art neural network for initial feature prediction.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated PPNP and APPNP against several state-of-the-art GCN-like models (V.GCN, GCN, N-GCN, GAT, JK, Bt.FP) for semi-supervised node classification.\n        *   Tested on four standard text-classification graph datasets: Citeseer, Cora-ML, Pubmed, and MS Academic.\n        *   Employed a rigorous evaluation protocol: 100 runs on multiple random splits and initializations, a fixed visible/test set split, hyperparameter optimization only on Citeseer and Cora-ML (then applied across datasets), early stopping, and statistical significance testing (bootstrapping for confidence intervals, paired t-tests for p-values).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Accuracy:** PPNP and APPNP consistently and significantly outperformed all compared models across all datasets (e.g., on Cora-ML, APPNP achieved 85.09% vs. GAT's 84.37% and GCN's 83.41%).\n        *   **Efficiency:** APPNP demonstrated training times on par with or faster than previous models, with a comparable or lower number of parameters. PPNP, due to its $O(n^2)$ complexity, ran out of memory on larger datasets (Pubmed, MS Academic), validating the necessity of APPNP.\n        *   The rigorous evaluation highlighted that many previously reported improvements of other models \"vanish\" under careful statistical scrutiny, underscoring the robustness of PPNP/APPNP's gains.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The original PPNP formulation is computationally intensive ($O(n^2)$) and memory-demanding, making it impractical for large graphs without approximation. This is mitigated by APPNP.\n        *   The choice of teleport probability $\\alpha$ is a hyperparameter that needs tuning, although the paper shows it allows for flexibility.\n    *   **Scope of Applicability:**\n        *   Primarily focused on semi-supervised node classification on graphs.\n        *   Applicable to various graph types, especially those where long-range dependencies are important.\n        *   The initial feature transformation can be performed by \"any neural network,\" offering broad compatibility.\n\n7.  **Technical Significance**\n    *   **State-of-the-Art Advancement:** PPNP/APPNP achieves new state-of-the-art results in semi-supervised node classification, demonstrating superior accuracy and efficiency compared to existing GCN-like models.\n    *   **Fundamental Problem Resolution:** It provides a principled solution to the long-standing \"oversmoothing\" and limited range problems in graph neural networks by leveraging personalized PageRank.\n    *   **Architectural Paradigm Shift:** The decoupling of prediction and propagation offers a novel architectural design principle for GNNs, allowing for independent optimization of feature learning and graph diffusion.\n    *   **Improved Efficiency and Scalability:** APPNP's linear complexity and parameter efficiency make it highly scalable and practical for real-world large-scale graph applications.\n    *   **Enhanced Evaluation Standards:** The paper's emphasis on a rigorous experimental protocol sets a higher bar for future research in graph neural networks, promoting more reliable and reproducible results.",
        "year": 2018,
        "citation_key": "klicpera20186xu"
      }
    ],
    "layer2_papers": [
      {
        "title": "Learning Strong Graph Neural Networks with Weak Information",
        "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{liu2023v3e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) suffer significant performance degradation when input graph data contains \"weak information,\" which encompasses incomplete structure (missing edges), incomplete features (missing node attributes), and insufficient labels.\n    *   **Importance and Challenge**:\n        *   Most GNNs assume ideal, complete data, which is often invalid in real-world scenarios due to privacy concerns, data collection errors, or high annotation costs.\n        *   Existing solutions primarily address only *one type* of weak information (e.g., structure learning, feature imputation, label-efficient learning).\n        *   The critical challenge is to develop a universal and effective GNN framework that can handle *simultaneously occurring and mutually affecting* diverse data deficiencies, particularly in \"extreme GLWI\" scenarios where structure, features, and labels are all weak.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges prior work in graph structure learning, attribute completion, and label-efficient graph learning, which are specialized GNN designs to handle specific data deficiencies.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-aspect focus**: A major limitation is that most existing methods only consider data deficiency from a single perspective (e.g., only weak structure or only weak labels). They fail to address scenarios where multiple types of weak information coexist and interact.\n        *   **Computational Cost**: Many specialized methods require complex, carefully-crafted learning procedures, leading to high computational costs and reduced efficiency on large-scale graphs.\n        *   The paper positions itself as the *first attempt* to investigate graph learning with *extremely weak information* where all three aspects (structure, features, labels) are simultaneously incomplete.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **D2PT (Dual-channel Diffused Propagation then Transformation)**, a dual-channel GNN framework designed to enable effective information propagation on graphs with weak information.\n    *   **Novelty/Differentiation**:\n        *   **Empirically-driven Design Principles**: Based on empirical analysis, the authors identify two key design focal points: 1) enabling **long-range propagation** in GNNs, and 2) allowing information propagation to **stray nodes** (isolated from the largest connected component).\n        *   **Dual-Channel Architecture**:\n            *   **Channel 1 (DPT Backbone)**: Performs efficient long-range information propagation on the *input graph* (even with incomplete structure) using a graph diffusion-based backbone.\n            *   **Channel 2 (Global Graph)**: Learns a *global graph* by connecting nodes sharing similar semantics, derived from the propagated features. This channel specifically addresses the \"stray node problem\" by providing connections for isolated nodes.\n        *   **Prototype Contrastive Alignment**: A novel algorithm that aligns class-level prototypes learned from both channels. This mechanism allows the two different information propagation processes to mutually benefit, enhancing the model's ability to handle GLWI.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **D2PT Framework**: A principled dual-channel GNN architecture for comprehensive GLWI, integrating long-range propagation and global semantic connections.\n        *   **Graph Diffusion-based DPT Backbone**: An efficient mechanism for long-range message passing, crucial for recovering missing information and spreading supervision signals.\n        *   **Global Graph Construction**: A method to dynamically learn a global graph based on node semantic similarities, specifically designed to connect \"stray nodes\" and facilitate information flow to them.\n        *   **Prototype Contrastive Alignment**: A novel contrastive learning objective that aligns class-level prototypes between the two channels, ensuring consistency and mutual enhancement of learned representations.\n    *   **Problem Formulation**: The paper formally defines and investigates the \"extreme GLWI scenario,\" where structure, features, and labels are simultaneously deficient, advancing the research scope beyond single-aspect deficiencies.\n    *   **Empirical Analysis**: Provides a comprehensive analysis of the impact of data deficiency on GNNs, identifying long-range propagation and the stray node problem as critical factors, which directly guides the D2PT design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Performance evaluation of D2PT against various baseline GNNs (GCN, GAT, SGC, PPNP, APPNP) and GLWI-specific methods across different GLWI scenarios.\n        *   Ablation studies to validate the effectiveness of D2PT's components (e.g., dual-channel design, prototype alignment, global graph).\n        *   Efficiency analysis (training time, inference time).\n        *   Generalization capability across different datasets and GLWI settings.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   D2PT consistently demonstrates **superior performance** over baseline GNNs and state-of-the-art GLWI methods in various weak information scenarios, including weak structure, weak features, weak labels, and especially the challenging \"extreme GLWI\" scenario.\n        *   The empirical analysis (Figure 2) shows that graph diffusion-based models with long-range propagation (like DPT) generally outperform shallow GNNs in basic GLWI scenarios, motivating D2PT's design.\n        *   Ablation studies confirm the individual contributions of the dual-channel design, global graph, and prototype contrastive alignment to D2PT's overall effectiveness.\n        *   D2PT also exhibits **high efficiency**, demonstrating competitive or superior performance with reasonable computational costs.\n    *   **Datasets**: Experiments were conducted on eight real-world benchmark datasets (e.g., Cora, CiteSeer, PubMed, Coauthor-CS, Amazon-Photo, etc.).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While D2PT addresses a broad range of weak information, the specific mechanisms for constructing the global graph and performing prototype alignment might have dependencies on the quality of initial feature embeddings or the number of classes. The paper does not explicitly detail theoretical guarantees for the prototype alignment's convergence or optimality.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks. Its direct applicability to other graph learning tasks (e.g., link prediction, graph classification) or different types of graph data (e.g., heterogeneous graphs, dynamic graphs) is not explicitly explored in this paper.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023v3e} significantly advances the state-of-the-art by providing a unified, effective, and efficient solution for the complex problem of graph learning with *simultaneously occurring* weak information (structure, features, and labels). This moves beyond the single-aspect focus of prior research.\n    *   **Potential Impact on Future Research**:\n        *   **Robust GNN Design**: The D2PT framework offers a blueprint for designing more robust GNNs that can operate reliably in real-world, imperfect data environments.\n        *   **Multi-modal/Multi-source Weakness**: The dual-channel approach and prototype alignment could inspire future research into handling other forms of multi-modal or multi-source data deficiencies in graph learning.\n        *   **Diffusion-based GNNs**: Reinforces the importance of long-range propagation and graph diffusion mechanisms for handling data sparsity and incompleteness.\n        *   **Addressing Stray Nodes**: The explicit strategy for connecting stray nodes via semantic similarity provides a valuable direction for improving connectivity in sparse or fragmented graphs.",
        "year": 2023,
        "citation_key": "liu2023v3e"
      },
      {
        "title": "Revisiting Heterophily For Graph Neural Networks",
        "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
        "summary": "Here's a focused summary of the paper \"Revisiting Heterophily For Graph Neural Networks\" by Luan et al. \\cite{luan202272y} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) are typically built on the homophily assumption (connected nodes share similar attributes), which is a relational inductive bias.\n    *   However, GNNs often perform unsatisfactorily, sometimes even worse than simple Multi-Layer Perceptrons (MLPs), on a non-trivial set of real-world datasets.\n    *   Heterophily, where connected nodes have dissimilar attributes, is widely considered the main cause of this performance degradation.\n    *   Existing homophily metrics (edge, node, class homophily) are limited as they only consider graph-label consistency and fail to capture the true impact of heterophily on aggregation-based GNNs.\n\n*   **Related Work & Positioning**\n    *   The paper acknowledges numerous existing works addressing heterophily in GNNs \\cite{luan202272y}.\n    *   **Limitations of previous solutions**:\n        *   Existing homophily metrics are shown to be insufficient, as they can misleadingly indicate strong heterophily even when nodes remain distinguishable after aggregation (e.g., the bipartite graph example in Fig. 1).\n        *   Traditional adaptive filterbanks use scalar weights shared by all nodes, failing to account for diverse *local* heterophily.\n        *   Many existing heterophily-addressing methods rely on high-order filters or global properties of high-frequency signals, which can be computationally expensive.\n        *   Other methods focus on learning filters with high expressive power, which can be less flexible.\n\n*   **Technical Approach & Innovation**\n    *   **Revisiting Homophily Metrics**: The paper re-examines heterophily from the perspective of **post-aggregation node similarity**. It defines a **post-aggregation node similarity matrix** $S(\\hat{A}, X) = \\hat{A}X(\\hat{A}X)^T$ and, based on this, introduces new homophily metrics: **Aggregation Similarity Score** ($S_{agg}$) and **Graph Aggregation Homophily** ($H_{agg}(G)$ and $H^M_{agg}(G)$). These new metrics are shown to be more informative and better reflect GNN performance than existing ones.\n    *   **Diversification Operation**: It theoretically proves that a **local diversification operation** (implemented as a high-pass filter, $I - \\hat{A}$) can effectively address certain harmful cases of heterophily. This is quantified by a new metric called **Diversification Distinguishability (DD)**.\n    *   **Adaptive Channel Mixing (ACM) Framework**: Based on the insights from aggregation and diversification, the paper proposes ACM, a novel framework that augments baseline GNNs. ACM adaptively exploits three distinct channels—**aggregation (low-pass), diversification (high-pass), and identity**—**node-wisely and locally** in each layer. This allows GNNs to extract richer localized information tailored to diverse node heterophily situations.\n\n*   **Key Technical Contributions**\n    *   **Novel Homophily Metrics**: Introduction of post-aggregation node similarity-based metrics ($H_{agg}(G)$, $H^M_{agg}(G)$) that provide a more accurate indication of how graph structure affects GNN performance under heterophily \\cite{luan202272y}.\n    *   **Theoretical Justification for Diversification**: A theoretical proof (Theorem 1) demonstrating the effectiveness of local diversification (high-pass filtering) in addressing harmful heterophily under specific conditions.\n    *   **Adaptive Channel Mixing (ACM) Framework**: A novel, flexible, and efficient GNN architectural framework that adaptively combines aggregation, diversification, and identity channels node-wisely and locally, enhancing GNN robustness to heterophily.\n    *   **Ease of Implementation**: ACM is designed to be easily integrated into existing baseline GNN layers.\n\n*   **Experimental Validation**\n    *   **Synthetic Graphs**: Experiments on synthetic graphs with varying homophily levels demonstrated that the proposed $H^M_{agg}(G)$ metric exhibited a nearly monotonic relationship with GNN performance (SGC and GCN), unlike the U-shaped curves observed with existing homophily metrics \\cite{luan202272y}.\n    *   **Real-world Node Classification**: ACM-augmented baselines were evaluated on 10 benchmark node classification tasks (7 heterophilic, 3 homophilic).\n        *   On heterophilic graphs, ACM consistently achieved significant performance gains (2.04% to 27.5%) over uni-channel baselines and surpassed state-of-the-art GNNs on most tasks.\n        *   On homophilic graphs, ACM-augmented GNNs performed comparably to or better than baselines and were competitive with SOTA models.\n        *   The improvements were achieved without incurring significant computational burden.\n\n*   **Limitations & Scope**\n    *   The theoretical proof for diversification's effectiveness (Theorem 1) is demonstrated under specific conditions (e.g., 2 classes, features equal to labels, specific aggregation operator), suggesting its generalizability might require further investigation.\n    *   The primary experimental validation focuses on node classification tasks, and its applicability to other graph learning tasks (e.g., graph classification, link prediction) is not explicitly detailed.\n    *   ACM is presented as an augmentation framework for existing uni-channel GNNs rather than a standalone GNN architecture.\n\n*   **Technical Significance**\n    *   The paper significantly advances the understanding of heterophily in GNNs by introducing a more nuanced perspective based on post-aggregation node similarity, moving beyond simple graph-label consistency.\n    *   ACM provides a highly effective and computationally efficient method to improve GNN performance on challenging heterophilic graphs, addressing a critical limitation of traditional GNNs.\n    *   The adaptive, node-wise channel mixing approach represents a novel architectural paradigm for GNNs, offering greater flexibility and local adaptability to diverse graph structures and homophily levels.\n    *   This work lays a foundation for designing more robust and versatile GNNs that can perform well across a wider spectrum of real-world graph data.",
        "year": 2022,
        "citation_key": "luan202272y"
      },
      {
        "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
        "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit a \"performance disparity\" on real-world graphs, performing well on nodes that align with the graph's dominant structural pattern (e.g., homophilic nodes in a homophilic graph) but struggling on nodes exhibiting the opposite pattern (e.g., heterophilic nodes in a homophilic graph) \\cite{mao202313j}. This disparity arises because real-world graphs inherently possess \"structural disparity,\" meaning they are a mixture of both homophilic and heterophilic node patterns.\n    *   **Importance & Challenge**: Existing GNN research often focuses on overall graph performance, overlooking the nuanced behavior of GNNs on these diverse node subgroups. Understanding this performance disparity is critical for developing GNNs that are robust and effective across all node types in complex, real-world graph structures \\cite{mao202313j}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior studies on GNN effectiveness primarily analyze performance on the entire graph and typically focus on graphs that are predominantly either homophilic or heterophilic \\cite{mao202313j}.\n    *   **Limitations of previous solutions**: These approaches fail to account for the common real-world scenario where graphs contain a mixture of homophilic and heterophilic patterns. They do not provide insights into GNN performance on specific node subgroups, potentially masking scenarios where GNNs perform poorly on minority patterns despite achieving good overall results \\cite{mao202313j}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Empirical Disparity Analysis**: Systematically investigates GNN performance by categorizing testing nodes based on their local homophily ratios, comparing a vanilla GNN (GCN) against MLP-based models (vanilla MLP and GLNN) \\cite{mao202313j}.\n        *   **Theoretical Analysis of Aggregation**: Examines how the GNN aggregation mechanism differentially impacts nodes with varying structural patterns. This involves introducing a novel graph generation model, CSBM-Structure (CSBM-S), and deriving theoretical insights into aggregated feature distances and class probabilities \\cite{mao202313j}.\n        *   **Generalization Bound**: Develops a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs to formally explain the underlying causes of the observed performance disparity \\cite{mao202313j}.\n        *   **Discriminative Ability Quantification**: Empirically analyzes the discriminative ability of GNNs on majority versus minority nodes using a \"relative discriminative ratio\" based on class prototypes \\cite{mao202313j}.\n    *   **Novelty/Difference**:\n        *   **First Rigorous Disparity Analysis**: This work is novel in its focused and systematic investigation of GNN performance on *subgroups* of nodes defined by their local structural patterns (homophilic vs. heterophilic), moving beyond aggregate performance metrics \\cite{mao202313j}.\n        *   **Novel CSBM-Structure (CSBM-S) Model**: Introduces a new variant of the Contextual Stochastic Block Model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes within a single graph, providing a more realistic testbed for structural disparity \\cite{mao202313j}.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound**: Presents a novel theoretical framework that precisely identifies aggregated feature distance and homophily ratio differences between training and testing nodes as critical factors driving performance disparity \\cite{mao202313j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Framework**: A non-i.i.d PAC-Bayesian generalization bound for GNNs that rigorously explains performance disparity by identifying aggregated feature distance and homophily ratio differences between training and testing nodes as key factors \\cite{mao202313j}.\n    *   **Novel Graph Generation Model**: Introduction of CSBM-Structure (CSBM-S), a variant of CSBM, which enables controlled experimental studies of graphs exhibiting mixed homophilic and heterophilic patterns \\cite{mao202313j}.\n    *   **Theoretical Insights into Aggregation**: Demonstrates that GNN aggregation creates a significant feature distance between homophilic and heterophilic node subgroups *within the same class*, and that differences in homophily ratio directly influence class prediction probabilities \\cite{mao202313j}.\n    *   **Practical Implications**: Provides insights into the effectiveness of deeper GNNs and identifies an overlooked distribution shift factor, leading to a new scenario for the graph out-of-distribution problem \\cite{mao202313j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Structural Disparity Verification**: Analyzed node homophily ratio distributions on real-world homophilic (Ogbn-arxiv, Pubmed) and heterophilic (Chameleon, Squirrel) datasets, confirming the pervasive existence of mixed structural patterns \\cite{mao202313j}.\n        *   **Subgroup Performance Comparison**: Conducted experiments comparing the accuracy of GCN against vanilla MLP and GLNN on test nodes, specifically grouped by their homophily ratio ranges \\cite{mao202313j}.\n        *   **Discriminative Ratio Analysis**: Empirically calculated a \"relative discriminative ratio\" to quantify the ease of prediction for majority versus minority nodes, observing its variation with the number of aggregation layers \\cite{mao202313j}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Accuracy Differences**: Demonstrated that in homophilic graphs, MLP-based models often outperform GCN on heterophilic (minority) nodes, while GCN performs better on homophilic (majority) nodes. Conversely, in heterophilic graphs, MLP models frequently outperform GCN on homophilic (minority) nodes, with GCN excelling on heterophilic (majority) nodes \\cite{mao202313j}.\n        *   **Discriminative Ratio**: The analysis aimed to illustrate how aggregation influences the discriminative ability, with a lower ratio indicating that majority nodes are easier to predict than minority nodes \\cite{mao202313j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical analysis (e.g., Lemma 1) employs simplifying assumptions like balanced class distribution and shared aggregated feature variance for mathematical elegance \\cite{mao202313j}. The CSBM-S model assumes consistent degree distributions and specific probability relationships \\cite{mao202313j}.\n    *   **Scope of Applicability**: The empirical validation primarily focuses on GCN and MLP-based models, and the findings are most directly applicable to semi-supervised node classification tasks \\cite{mao202313j}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of GNN behavior by providing a detailed, subgroup-level analysis of performance, directly addressing the challenges posed by structural disparity in real-world graphs \\cite{mao202313j}.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The identified factors (aggregated feature distance, homophily ratio differences) and the proposed PAC-Bayesian bound offer a theoretical foundation to guide the development of new GNN architectures and training strategies that are more robust and equitable across diverse node patterns \\cite{mao202313j}.\n        *   **Graph Out-of-Distribution (OOD) Problem**: By highlighting an overlooked distribution shift factor related to structural patterns, the work introduces a new dimension to the graph OOD problem, opening new avenues for research in OOD generalization for graphs \\cite{mao202313j}.\n        *   **Deeper GNNs**: Provides novel insights into the mechanisms behind the effectiveness of deeper GNNs in the context of structural disparity \\cite{mao202313j}.",
        "year": 2023,
        "citation_key": "mao202313j"
      },
      {
        "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
        "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
        "summary": "Here's a focused summary of the paper \\cite{longa202399q} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** While Graph Neural Networks (GNNs) excel at learning on static graph-structured data, many real-world systems are dynamic, with graphs and their attributes changing over time. Traditional graph models and static GNNs are ill-equipped to capture this temporal evolution \\cite{longa202399q}.\n    *   **Importance and Challenge:** Extending GNN capabilities to temporal graphs (Temporal GNNs or TGNNs) is a promising research area. However, the field lacked a comprehensive, systematized overview, a rigorous formalization of learning settings and tasks, and a unified taxonomy for existing TGNN approaches \\cite{longa202399q}. Existing surveys were either too general, too specific, or did not provide in-depth coverage of TGNNs \\cite{longa202399q}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a survey that specifically focuses on GNN-based models for temporal graphs, acknowledging other approaches like matrix factorization, temporal motif-based methods, random walks (e.g., DyANE \\cite{longa202399q}), temporal point processes (e.g., DyRep \\cite{longa202399q}), NMF, and other deep learning techniques (e.g., DynGem \\cite{longa202399q}, TRRN \\cite{longa202399q}, STAR \\cite{longa202399q}, TSNet \\cite{longa202399q}) that also address temporal graphs \\cite{longa202399q}.\n    *   **Limitations of Previous Solutions (Surveys):** Previous surveys either discussed general temporal graph learning techniques with only brief mentions of GNNs, focused on narrow topics like temporal link prediction or graph generation, or provided GNN overviews without deep coverage of temporal aspects \\cite{longa202399q}. This work aims to fill the gap by providing the first comprehensive systematization of GNN-based methods for temporal graphs \\cite{longa202399q}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive survey that systematically organizes and formalizes the field of Temporal Graph Neural Networks (TGNNs). It does not propose a new TGNN model but rather provides a foundational framework for understanding existing ones \\cite{longa202399q}.\n    *   **Novelty/Difference:** The core innovation lies in its structured approach to the literature:\n        *   It introduces a rigorous formalization of learning settings and tasks specific to temporal graphs \\cite{longa202399q}.\n        *   It proposes a novel taxonomy that categorizes existing TGNN approaches based on how the temporal aspect is represented (Snapshot-based Temporal Graphs - STG, or Event-based Temporal Graphs - ETG) and processed \\cite{longa202399q}.\n        *   It defines distinct learning settings: transductive vs. inductive, and past vs. future prediction, for temporal graph tasks \\cite{longa202399q}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A coherent formalization of different learning settings (transductive, inductive, past prediction, future prediction) and tasks (e.g., temporal node classification, link prediction) for temporal graphs, unifying disparate definitions found in the literature \\cite{longa202399q}.\n        *   A comprehensive taxonomy that groups existing TGNN methods based on their temporal representation strategy (snapshot-based vs. event-based) and the mechanism used to incorporate time \\cite{longa202399q}.\n        *   Formal definitions for various types of temporal graphs, including Static Graphs (SG), Temporal Graphs (TG), Discrete Time Temporal Graphs (DTTG), Snapshot-based Temporal Graphs (STG), and Event-based Temporal Graphs (ETG) \\cite{longa202399q}.\n    *   **Theoretical Insights or Analysis:** The paper provides a structured theoretical framework for the field, clarifying fundamental concepts, learning objectives, and evaluation contexts for TGNNs \\cite{longa202399q}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{longa202399q} does not present new experimental results from its own proposed models. Instead, it reviews and categorizes the experimental validations performed by the individual TGNN papers it surveys.\n    *   **Key Performance Metrics and Comparison Results:** The paper notes that TGNNs have achieved state-of-the-art results on tasks such as temporal link prediction, node classification, and edge classification \\cite{longa202399q}. It sets the context for understanding how these models are typically evaluated, but does not provide specific metrics or comparative results within the survey itself.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):** The survey explicitly focuses on methods that leverage and adapt the GNN framework to temporal graphs, acknowledging but not deeply exploring other non-GNN approaches for temporal graph learning \\cite{longa202399q}.\n    *   **Limitations of Current TGNN Methods (as identified by the survey):** The paper concludes with a discussion of the most relevant open challenges and limitations of current TGNN methods, from both research and application perspectives, though these specific challenges are not detailed in the provided abstract/introduction \\cite{longa202399q}.\n    *   **Scope of Applicability:** TGNNs are applicable to a wide range of dynamic systems, including recommendation systems, social network analysis, transportation systems, face-to-face interactions, human mobility, and epidemic modeling \\cite{longa202399q}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematized overview of Temporal Graph Neural Networks, filling a critical gap in the existing literature \\cite{longa202399q}.\n    *   **Potential Impact on Future Research:**\n        *   It offers a unified formalization and common language, which is crucial for facilitating comparison, understanding, and the development of new TGNN models \\cite{longa202399q}.\n        *   By highlighting limitations and open challenges, it provides a clear roadmap for future research directions in the field \\cite{longa202399q}.\n        *   The proposed taxonomy and formal definitions will serve as a foundational reference for researchers and practitioners working with dynamic graph data \\cite{longa202399q}.",
        "year": 2023,
        "citation_key": "longa202399q"
      }
    ],
    "layer3_papers": [
      {
        "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
        "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Traditional Graph Neural Networks (GNNs) employ a uniform global filter (e.g., low-pass for homophilic graphs, high-pass for heterophilic graphs) across all nodes \\cite{han2024rkj}. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, and these patterns can vary significantly even among different communities within the same graph \\cite{han2024rkj}.\n    *   **Why important and challenging:** A single global filter is suboptimal and can adversely affect performance on nodes with differing patterns, leading to misclassification \\cite{han2024rkj}. The challenge lies in adaptively applying appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns \\cite{han2024rkj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Most GNNs, including those with learnable graph convolutions, apply a uniform global filter across all nodes \\cite{han2024rkj}.\n    *   **Limitations of previous solutions:** The \"one-size-fits-all\" filtering strategy is ineffective for graphs with mixed homophilic and heterophilic patterns. A global filter optimized for one pattern (e.g., low-pass for homophily) can incur significant losses for nodes exhibiting other patterns (e.g., heterophily) \\cite{han2024rkj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that leverages a Mixture of Experts (MoE) approach to adaptively select and apply appropriate filters for different nodes \\cite{han2024rkj}.\n    *   **What makes this approach novel:** NODE-MOE moves beyond uniform global filtering by dynamically applying distinct filters to individual nodes based on their specific structural patterns. It integrates a gating model to assign weights to different \"expert\" GNNs, each potentially equipped with a different filter type, allowing for node-specific processing \\cite{han2024rkj}.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical insights:** The paper theoretically demonstrates, using a Contextual Stochastic Block Model (CSBM), that a global filter optimized for one pattern can incur significant losses for nodes with other patterns, while node-wise filtering can achieve linear separability for all nodes under mild conditions (Theorem 1) \\cite{han2024rkj}.\n    *   **Novel framework:** Introduction of NODE-MOE, a flexible and efficient Mixture of Experts framework for node-wise filtering in GNNs \\cite{han2024rkj}.\n    *   **Gating Model design:** A novel gating model that estimates node patterns by incorporating contextual features `[X, |AX-X|, |A^2X-X|]` and uses a GNN (e.g., GIN) with low-pass filters to ensure neighboring nodes receive similar expert selections, leveraging community detection capabilities \\cite{han2024rkj}.\n    *   **Expert Model strategy:** Utilizes GNNs with learnable graph convolutions as experts, initialized with diverse filter types (e.g., low-pass, constant, high-pass) to encourage specialization and handle different structural patterns \\cite{han2024rkj}.\n    *   **Filter Smoothing Loss:** Introduction of a `L_s = sum(|f_o(x_i) - f_o(x_{i-1})|^2)` loss to ensure learned filters exhibit smooth behavior in the spectral domain, mitigating training challenges and improving interpretability \\cite{han2024rkj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on both homophilic (Cora, CiteSeer) and heterophilic (Chameleon, Squirrel) graph datasets \\cite{han2024rkj}.\n    *   **Key performance metrics and comparison results:** The experiments demonstrate the effectiveness of NODE-MOE, illustrating significant performance improvement on both types of graphs \\cite{han2024rkj}. The gating model is shown to efficiently assign different nodes to their suitable filters (Section 4.3) \\cite{han2024rkj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The framework addresses the challenge of incorporating various filters and selecting appropriate ones without ground truth on node patterns. The filter smoothing loss is introduced to mitigate issues like filter oscillations and improve interpretability when training multiple filters simultaneously \\cite{han2024rkj}.\n    *   **Scope of applicability:** NODE-MOE is primarily designed for node classification tasks on graphs that exhibit a complex mixture of homophilic and heterophilic structural patterns \\cite{han2024rkj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** NODE-MOE represents a significant advancement by moving beyond the limitations of uniform global filtering in GNNs. It provides a principled and adaptive approach to handle the diverse structural patterns prevalent in real-world graphs, which is a common challenge for existing GNN models \\cite{han2024rkj}.\n    *   **Potential impact on future research:** This work opens avenues for future research into more sophisticated gating mechanisms, dynamic expert selection, and the application of node-wise filtering to other graph learning tasks beyond node classification. The theoretical foundation also encourages further analysis of GNN behavior on mixed-pattern graphs \\cite{han2024rkj}.",
        "year": 2024,
        "citation_key": "han2024rkj"
      },
      {
        "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
        "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
        "summary": "Here's a focused summary of the paper \"UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND\" \\cite{kang2024fsk} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Traditional continuous Graph Neural Networks (GNNs) rely on integer-order differential equations, which model instantaneous, local changes (Markovian updates) in node features. This approach struggles to capture long-term dependencies and memory effects inherent in many real-world graph dynamics.\n    *   **Importance & Challenge:** Many real-world graphs (e.g., social, biological, internet networks) exhibit non-local, memory-dependent behaviors and scale-free hierarchical (fractal) structures. Integer-order models are insufficient to accurately describe these complex dynamics, potentially leading to limitations like oversmoothing and suboptimal graph representation learning.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior continuous GNNs (e.g., GRAND, GRAND++, GraphCON, CDE, GREAD) leverage integer-order Ordinary Differential Equations (ODEs) for information propagation, typically using first or second-order derivatives.\n    *   **Limitations of Previous Solutions:** These models are restricted to integer-order derivatives, implying Markovian update mechanisms where feature evolution depends only on the present state. This prevents them from inherently capturing the non-local properties and memory-dependent dynamics crucial for systems with self-similarity or anomalous transport. Other works using fractional calculus either apply it to graph shift operators with integer-order ODEs or to gradient propagation during training, not to the core node feature updating process itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the FRactional-Order graph Neural Dynamical network (FROND) framework \\cite{kang2024fsk}, which replaces the integer-order differential operator in continuous GNNs with the Caputo fractional derivative (DβtX(t) = F(W,X(t)), β > 0).\n    *   **Novelty:**\n        *   **Generalization of Continuous GNNs:** FROND generalizes existing integer-order continuous GNNs by allowing the derivative order β to be any positive real number, effectively subsuming them as special cases when β is an integer.\n        *   **Memory-Dependent Dynamics:** By employing the Caputo fractional derivative, FROND inherently integrates the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics.\n        *   **Non-Markovian Random Walk Interpretation:** For the fractional linear diffusion model (F-GRAND-l), the paper provides an interpretation from a non-Markovian random walk perspective, where the walker's complete path history influences future steps, contrasting with the Markovian walks of traditional models.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Proposed a novel, generalized continuous GNN framework (FROND) that incorporates non-local fractional derivatives, laying the groundwork for a new class of GNNs with learnable memory-dependent feature-updating processes \\cite{kang2024fsk}.\n    *   **System Design/Architectural Innovations:** Demonstrated the seamless compatibility of FROND, showing how it can be integrated to augment the performance of existing integer-order continuous GNNs (e.g., F-GRAND, F-GRAND++, F-GREAD, F-CDE, F-GraphCON).\n    *   **Theoretical Insights/Analysis:**\n        *   Analytically established that the non-Markovian random walk in FROND leads to a slow algebraic rate of convergence to stationarity, which inherently mitigates oversmoothing, unlike the exponentially swift convergence in Markovian integer-order models.\n        *   Suggested a connection between the optimal fractional order β and the inherent \"fractality\" of graph datasets, offering a potential avenue for deeper structural insights.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The FROND framework was validated through extensive experiments comparing fractional adaptations of various established integer-order continuous GNNs (GRAND, GRAND++, GraphCON, CDE, GREAD) on diverse datasets \\cite{kang2024fsk}.\n    *   **Key Performance Metrics & Comparison Results:** The fractional adaptations consistently demonstrated improved performance compared to their integer-order counterparts. Detailed ablation studies were performed to provide insights into the choice of numerical schemes and parameters, underscoring the framework’s potential as an effective extension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on β ∈ (0,1] for initial conditions, though the broader definition for β > 0 is mentioned. The framework relies on numerical FDE solvers, which may introduce computational considerations.\n    *   **Scope of Applicability:** FROND is designed to enhance continuous GNNs for graph representation learning, particularly beneficial for datasets exhibiting non-local, memory-dependent behaviors, or fractal structures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FROND significantly advances the technical state-of-the-art by introducing a principled and generalized approach to incorporate memory-dependent dynamics and non-local interactions into continuous GNNs, overcoming the limitations of purely Markovian integer-order models \\cite{kang2024fsk}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new research directions for designing GNNs that can model more complex, memory-dependent feature-updating processes.\n        *   Provides a robust mechanism for mitigating oversmoothing in GNNs through its algebraic convergence properties.\n        *   Offers a novel tool for exploring the underlying \"fractality\" and self-similarity of graph datasets by optimizing the fractional order β.\n        *   Serves as a powerful and compatible extension to enhance the performance of existing continuous GNN architectures.",
        "year": 2024,
        "citation_key": "kang2024fsk"
      }
    ],
    "layer2_summary": null
  },
  "75e924bd79d27a23f3f93d9b1ab62a779505c8d2": {
    "seed_title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks",
    "summary": "1. *Evolution Analysis (Chronological List/Table)*\n\n*   **The Field of Graph Neural Networks for Time Series (GNN4TS) (as analyzed by [jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023))**\n\n    *   **Methodological/Conceptual Shifts**:\n        *   A fundamental shift from traditional time series analysis methods (e.g., SVR, GBDT, VAR, ARIMA) and early deep learning models (e.g., CNN, RNN, Transformers) that primarily focus on temporal dependencies or implicitly handle spatial relations, to Graph Neural Networks (GNNs) that *explicitly* model non-Euclidean spatial relationships among time series variables. This allows for a more direct and effective capture of inter-variable dependencies.\n        *   An evolution in the approach to graph construction: initially, GNNs might have relied on readily available or simple heuristic-based graphs (e.g., based on geographical proximity or basic similarity metrics). The field has progressed to sophisticated learning-based methods that infer optimal graph structures directly from data, often end-to-end with the downstream task, enhancing adaptability and performance.\n    *   **Problems Addressed**:\n        *   Traditional and early deep learning methods struggle to effectively capture complex inter-variable (spatial) dependencies in multivariate time series, particularly in non-Euclidean data, leading to suboptimal performance in critical applications (e.g., cloud computing, transportation, IoT).\n        *   The need for a unified framework to analyze and categorize the rapidly growing body of GNN4TS research across diverse tasks (forecasting, classification, imputation, anomaly detection) was a significant gap, as existing surveys were often limited in scope.\n    *   **Innovations/Capabilities Introduced**:\n        *   GNNs introduce the capability to explicitly encode and propagate information across interconnected time series, leveraging graph structures to model complex spatial dependencies. This enables a deeper understanding and more accurate analysis of multivariate time series.\n        *   The development of various graph construction techniques (both heuristic-based like Gaussian kernel, Pearson correlation, DTW, Granger causality, and learning-based via embedding comparisons or attention mechanisms) allows GNNs to be applied even when explicit graph structures are not readily available, making them versatile.\n    *   **Temporal Gaps/Clusters**: The publication of this comprehensive survey in 2023 indicates a recent and significant surge in research applying GNNs to time series. This clustering of research, necessitating a holistic review, is likely driven by advancements in GNN architectures, increased availability of complex multivariate time series datasets, and growing computational power, highlighting a rapidly maturing sub-field.\n\n*   **Contribution of [jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023) itself**:\n\n    *   **Methodological/Conceptual Shifts**: This paper introduces a novel, comprehensive taxonomy for GNN4TS, categorizing existing works by both task-oriented (forecasting, classification, anomaly detection, imputation) and methodology-oriented (spatial/temporal dependency modeling, model architecture) perspectives. This structures a previously fragmented research landscape. It also provides a systematic and detailed review of graph construction methods, a critical component for effective GNN4TS application.\n    *   **Problems Addressed**: It directly addresses the lack of a comprehensive, up-to-date, and domain-agnostic review of GNNs for time series analysis, which was a significant gap in the literature and hindered systematic understanding and progress.\n    *   **Innovations/Capabilities Introduced**: The survey itself is an innovation in synthesizing and structuring knowledge. It provides the first holistic overview of GNN4TS, consolidating fragmented research, clarifying key concepts (like spatial-temporal graphs and GNN operations), and identifying crucial future research directions, thereby serving as a foundational resource for the field.\n\n2. *Evolution Analysis:*\n\n*Trend 1: The Emergence and Formalization of Graph Neural Networks for Explicit Spatial-Temporal Modeling in Time Series*\n\n*   *Methodological progression*: The evolution of time series analysis has seen a significant methodological shift, moving beyond traditional statistical models (e.g., ARIMA, VAR) and early deep learning architectures (e.g., CNNs, RNNs, Transformers) towards Graph Neural Networks (GNNs). As comprehensively detailed by \"[jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023)\", traditional methods often struggle to explicitly model the complex inter-variable, non-Euclidean spatial relationships inherent in many multivariate time series. GNNs, by contrast, introduce a powerful mechanism to explicitly encode and propagate information across interconnected time series variables, leveraging graph structures and operations like AGGREGATE and COMBINE to capture these intricate dependencies. A crucial part of this methodological progression, meticulously outlined in the survey, is the evolution of graph construction techniques. Initially, GNN applications might have relied on readily available or simple heuristic-based graphs, such as those derived from geographical distance, Gaussian kernels, Pearson correlation, Dynamic Time Warping (DTW), or Granger causality. However, the field has advanced significantly to sophisticated learning-based methods that infer optimal graph structures directly from the data, often in an end-to-end fashion with the downstream task, using techniques like embedding comparisons or attention mechanisms with sparsification, allowing for greater adaptability and performance in diverse scenarios.\n\n*   *Problem evolution*: The core problem in time series analysis has evolved from merely predicting future values or classifying patterns to doing so while explicitly understanding and leveraging the intricate spatial-temporal dependencies. Traditional methods often treat variables independently or model their interactions implicitly, leading to less accurate results when strong, non-Euclidean spatial relationships exist, as is common in domains like transportation, climate, and IoT. The challenge lies in capturing these \"diverse and intricate relationships\" that are critical for accurate analysis. GNNs address this by providing a robust framework to explicitly model these \"spatial\" connections (e.g., traffic flow between cities, sensor readings in a network). Furthermore, the rapid growth of research applying GNNs to time series data created a new problem: a fragmented and unorganized understanding of the field. \"[jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023)\" directly addresses this by providing the first comprehensive and up-to-date review, unifying the diverse applications of GNNs across the four fundamental tasks of forecasting, classification, anomaly detection, and imputation, without restricting to specific domains.\n\n*   *Key innovations*: The primary innovation driving this trend is the successful application of GNNs themselves to time series data, enabling the explicit modeling of complex spatial-temporal dependencies that were previously difficult to capture effectively. This capability has led to GNN-based approaches demonstrating \"promising results\" and \"significant advantages\" over traditional methods across various tasks. The survey \"[jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023)\" introduces its own key innovations: a novel, unified, and structured taxonomy that categorizes GNN4TS works from both task-oriented and methodology-oriented perspectives. It also provides a comprehensive discussion and categorization of graph construction methods, which is a foundational and often challenging aspect of applying GNNs to real-world time series data. By consolidating this vast array of knowledge, clarifying key concepts, and outlining future research directions, the survey itself represents a significant intellectual contribution, formalizing and advancing the understanding of the burgeoning GNN4TS field.\n\n3. *Synthesis*\n\nThe unified intellectual trajectory connecting these works is the increasing recognition and sophisticated modeling of complex, non-Euclidean spatial-temporal dependencies in time series data. Their collective contribution to advancing \"Graph Neural Networks\" lies in demonstrating their profound utility beyond static graph data, establishing them as a powerful paradigm for dynamic, interconnected time series analysis, and providing a foundational framework for future research in this burgeoning area.",
    "path": [
      "75e924bd79d27a23f3f93d9b1ab62a779505c8d2",
      "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9"
    ],
    "layer1_papers": [
      {
        "title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks",
        "abstract": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
        "summary": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
        "year": 2020,
        "citation_key": "wu2020hi3"
      }
    ],
    "layer2_papers": [
      {
        "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
        "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Traditional time series analysis methods (e.g., SVR, GBDT, VAR, ARIMA, CNN, RNN, Transformers) struggle to explicitly model complex inter-temporal and inter-variable relationships, particularly spatial relations in non-Euclidean data, which are prevalent in multivariate time series.\n    *   **Importance/Challenge**: Time series data is ubiquitous and critical across various domains (e.g., cloud computing, transportation, IoT). Accurately analyzing this data requires capturing intricate spatial-temporal dependencies, which existing methods often fail to do effectively, leading to less accurate results. Graph Neural Networks (GNNs) offer a promising avenue to address this by explicitly modeling these non-Euclidean relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper is a comprehensive survey that reviews the burgeoning field of GNNs for time series analysis (GNN4TS).\n    *   **Limitations of previous solutions**: Prior time series analysis methods (traditional and deep learning) lack explicit modeling of non-Euclidean spatial relationships. Existing surveys on GNNs or spatial-temporal data are often limited in scope, focusing on specific domains (e.g., traffic, urban computing) or tasks (e.g., forecasting), and do not provide a holistic view of GNNs across the full spectrum of time series analysis tasks.\n    *   **Positioning**: This survey \\cite{jin2023ijy} aims to fill this gap by providing the *first comprehensive and up-to-date review* of GNN4TS, encompassing four fundamental tasks: forecasting, classification, anomaly detection, and imputation, without restricting to specific domains or tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper systematically reviews and categorizes GNN-based approaches for time series analysis. It defines key concepts like spatial-temporal graphs and GNN operations (AGGREGATE, COMBINE). A significant part of its technical approach is detailing how graph structures are generated for time series data.\n    *   **Innovation (of the survey)**:\n        *   **Unified and Structured Taxonomy**: Presents a novel framework to categorize existing GNN4TS works from both task-oriented (forecasting, classification, anomaly detection, imputation) and methodology-oriented perspectives (spatial/temporal dependency modeling, model architecture).\n        *   **Detailed Graph Construction Methods**: Provides a comprehensive discussion and categorization of strategies for generating graph structures when not readily available, including:\n            *   **Heuristic-based Graphs**: Spatial Proximity (e.g., geographical distance, Gaussian kernel), Pairwise Connectivity (e.g., transportation networks), Pairwise Similarity (e.g., cosine similarity, Pearson correlation, DTW), and Functional Dependence (e.g., Granger causality, transfer entropy).\n            *   **Learning-based Graphs**: Approaches that learn graph structures directly from data end-to-end with the downstream task, often using embedding comparisons or attention mechanisms, with sparsification techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Taxonomy**: Introduces a comprehensive, task-oriented taxonomy for GNN4TS, covering forecasting, classification, anomaly detection, and imputation, and a methodology-oriented classification based on spatial/temporal dependency modeling and model architecture.\n    *   **Comprehensive Review of Graph Construction**: Systematically outlines and categorizes various heuristic-based and learning-based methods for constructing graph structures from time series data, which is crucial for applying GNNs.\n    *   **Consolidation of Knowledge**: For the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities.\n    *   **Future Research Agenda**: Identifies and discusses potential future research directions in the field of GNN4TS.\n\n*   **Experimental Validation**\n    *   As a survey, this paper does not present new experimental results. Instead, it synthesizes findings from the reviewed literature.\n    *   **Key Performance Metrics and Comparison Results (from reviewed papers)**: The survey highlights that GNN-based approaches \"demonstrate promising results\" and \"significant advantages\" in modeling real-world time series data compared to traditional methods. It notes that GNNs enable the capture of \"diverse and intricate relationships\" and have shown \"promising outcomes\" across forecasting, classification, anomaly detection, and imputation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The survey \\cite{jin2023ijy} focuses specifically on GNNs for time series analysis (GNN4TS) across four core tasks: forecasting, classification, anomaly detection, and imputation. It primarily discusses spatial GNNs.\n    *   **Technical Limitations (of the field, as identified by the survey)**: The field still faces challenges in graph structure generation (especially for complex, dynamic relationships), scalability for very large time series, and interpretability of GNN models.\n\n*   **Technical Significance**\n    *   **Advances the technical state-of-the-art**: By providing the first comprehensive and structured overview of GNN4TS, the survey consolidates fragmented knowledge, making the field more accessible and understandable for researchers and practitioners. It clarifies the landscape of GNN applications in time series.\n    *   **Potential impact on future research**: The survey \\cite{jin2023ijy} serves as a foundational resource, guiding future research by identifying open questions, emerging trends, and promising directions in GNN4TS. It underscores the power of GNNs in explicitly modeling complex spatial-temporal dependencies, thereby encouraging further innovation in this area.",
        "year": 2023,
        "citation_key": "jin2023ijy"
      }
    ],
    "layer3_papers": [],
    "layer2_summary": null
  },
  "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da": {
    "seed_title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
    "summary": "**Integration Analysis:**\n\nThe new papers significantly extend and refine the previously identified evolutionary trends in Graph Neural Networks, adding new methodological paradigms and deepening the understanding of GNN capabilities and limitations.\n\n*   **Extension of \"Quest for Expressive Power\":**\n    *   **[geisler2024wli] Spatio-Spectral Graph Neural Networks (2024)** directly extends this trend by introducing a novel hybrid spatio-spectral architecture. It addresses the long-standing problem of limited receptive fields and over-squashing in message-passing GNNs, which was previously acknowledged as a limitation (e.g., in [finkelshtein202301z] \"Cooperative Graph Neural Networks\" (2023)). It builds upon the understanding of spectral GNNs ([wang2022u2l] \"How Powerful are Spectral Graph Neural Networks\" (2022)) but innovates by combining them with spatial methods and operating directly in the spectral domain.\n\n*   **Deepening of \"Towards Adaptive, Robust, and Scalable GNNs for Real-World Applications\":**\n    *   **[kang2024fsk] Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND (2024)** introduces a fundamental conceptual shift by applying fractional calculus to GNN dynamics. This offers a novel mathematical framework to model non-local, memory-dependent interactions, inherently mitigating oversmoothing – a robustness issue previously discussed (e.g., [reiser2022b08] \"Graph neural networks for materials science and chemistry\" (2022), [finkelshtein202301z] \"Cooperative Graph Neural Networks\" (2023)). It provides a new avenue for improving generalization and robustness beyond statistical bounds ([ju2023prm] \"Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion\" (2023)).\n    *   **[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)** significantly advances the adaptability aspect by introducing multi-modal prompt learning for GNNs. It directly builds upon the concept of prompt tuning ([fang2022tjj] \"Universal Prompt Tuning for Graph Neural Networks\" (2022)) but extends it to enable CLIP-style zero-shot generalization for general graph data, even with *extremely weak text supervision*. This opens up new possibilities for GNNs to leverage semantic understanding from Large Language Models (LLMs), addressing a critical challenge in data scarcity and transferability.\n    *   **[liu20242g6] A Review of Graph Neural Networks in Epidemic Modeling (2024)** reinforces the real-world application focus by providing a comprehensive review of GNNs in epidemic modeling. Similar to earlier domain-specific reviews ([reiser2022b08] for materials science, [longa202399q] for temporal graphs), it systematizes knowledge in a high-impact area, highlighting the practical utility and challenges of GNNs.\n\n*   **New Methodological/Conceptual Shifts:**\n    *   The introduction of **Spatio-Spectral GNNs** ([geisler2024wli]) as a hybrid modeling paradigm.\n    *   The application of **Fractional Calculus** to GNN dynamics ([kang2024fsk]) for non-local and memory-dependent feature evolution.\n    *   **Multi-modal Prompt Learning** for cross-domain semantic alignment and zero-shot generalization ([li202444f]).\n\n*   **Gaps Filled and New Directions:**\n    *   [geisler2024wli] offers a robust solution to the over-squashing problem, a known limitation.\n    *   [kang2024fsk] provides a novel, mathematically grounded approach to mitigate oversmoothing and model complex graph dynamics, filling a gap in the robustness literature.\n    *   [li202444f] addresses the crucial challenge of integrating semantic knowledge from LLMs into GNNs under weak supervision, enabling zero-shot capabilities.\n    *   [liu20242g6] fills the gap of a comprehensive review for GNNs in epidemic modeling, a critical application area.\n\n*   **Overall Narrative Change:** The addition of these papers strengthens the narrative that GNN research is moving towards increasingly sophisticated, *integrative*, and *mathematically diverse* solutions. It highlights a trend towards hybrid architectures, novel mathematical frameworks for dynamics, and powerful multi-modal adaptation strategies, all aimed at enhancing expressivity, robustness, and real-world applicability, even under challenging data conditions (e.g., weak supervision, long-range dependencies).\n\n**Temporal Positioning:**\nAll four new papers ([geisler2024wli], [kang2024fsk], [li202444f], [liu20242g6]) are from 2024, representing the very latest developments in the field and extending the chronological progression beyond the 2023 papers in the previous synthesis.\n\n---\n\n### Updated Evolution Analysis: Chronological Progression of Graph Neural Network Research\n\nThe evolution of Graph Neural Networks (GNNs) continues to be driven by two deeply intertwined major trends: a relentless \"Quest for Expressive Power\" to overcome fundamental limitations, and a parallel drive \"Towards Adaptive, Robust, and Scalable GNNs for Real-World Applications.\" The latest contributions in 2024 further exemplify and advance these trajectories, introducing novel paradigms and mathematical frameworks.\n\n**Trend 1: The Quest for Expressive Power: Beyond 1-WL and Towards Richer Structural Understanding**\n\nThe foundational understanding of GNN expressivity began with [morris20185sd] \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" (2018), which established the 1-Weisfeiler-Leman (1-WL) equivalence for standard GNNs and proposed k-GNNs for higher expressivity. This theoretical bottleneck spurred extensive research, summarized by [jegelka20222lq] \"Theory of Graph Neural Networks: Representation and Learning\" (2022), which outlined various methods to surpass 1-WL. [wang2022u2l] \"How Powerful are Spectral Graph Neural Networks\" (2022) explored spectral GNNs, connecting their universality to 1-WL.\n\nSubsequent work focused on enhancing information propagation and structural encoding. [feng20225sa] \"How Powerful are K-hop Message Passing Graph Neural Networks\" (2022) characterized K-hop message passing, showing it to be more powerful than 1-WL but bounded by 3-WL, and introduced KP-GNN to exceed this. [bianchi20239ee] \"The expressive power of pooling in Graph Neural Networks\" (2023) extended expressivity analysis to hierarchical GNNs with pooling. More recently, [michel2023hc4] \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" (2023) and [zeng20237gv] \"Substructure Aware Graph Neural Networks\" (2023) introduced PathNNs and SAGNNs, respectively, to leverage explicit path and subgraph information, pushing expressivity beyond 3-WL while maintaining scalability. [finkelshtein202301z] \"Cooperative Graph Neural Networks\" (2023) proposed dynamic, asynchronous message passing to overcome 1-WL limits and mitigate over-smoothing.\n\nA significant advancement in this quest comes from **[geisler2024wli] \"Spatio-Spectral Graph Neural Networks\" (2024)**. This paper introduces S2GNNs, a novel paradigm that synergistically combines local spatial message passing with global spectral filtering. It directly addresses the critical limitations of \"over-squashing\" and restricted receptive fields in traditional GNNs, which were implicit challenges in earlier works seeking long-range interactions or higher-order information. By proposing the first neural network designed to operate directly in the spectral domain and leveraging partial eigendecomposition, S2GNNs are proven to overcome over-squashing and achieve strictly tighter approximation bounds, demonstrating superior expressivity for long-range tasks. This work represents a powerful integration of spatial and spectral approaches, building on earlier spectral GNN research ([wang2022u2l]) to offer a robust solution for capturing global graph properties.\n\n**Trend 2: Towards Adaptive, Robust, and Scalable GNNs for Real-World Applications**\n\nBeyond theoretical expressivity, the field has continuously focused on making GNNs practical and robust. Early applications, such as those reviewed in [reiser2022b08] \"Graph neural networks for materials science and chemistry\" (2022), highlighted the need for incorporating domain-specific constraints like physical symmetries, which was later formalized by [joshi20239d0] \"On the Expressive Power of Geometric Graph Neural Networks\" (2023) with the Geometric Weisfeiler-Leman test. Scalability for tasks like link prediction was addressed by [chamberlain2022fym] \"Graph Neural Networks for Link Prediction with Subgraph Sketching\" (2022) through efficient subgraph sketching.\n\nAdaptability and generalization have also been key concerns. [fang2022tjj] \"Universal Prompt Tuning for Graph Neural Networks\" (2022) introduced a novel prompt-tuning method for adapting pre-trained GNNs to new tasks efficiently. [ju2023prm] \"Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion\" (2023) provided tighter theoretical generalization bounds and a Hessian-based regularization for robustness. The increasing complexity of real-world data also led to the systematization of GNNs for dynamic graphs by [longa202399q] \"Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities\" (2023).\n\nThe latest papers further push the boundaries of adaptability and robustness. **[kang2024fsk] \"Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND\" (2024)** introduces a groundbreaking mathematical framework by replacing integer-order differential equations in continuous GNNs with Caputo fractional derivatives. This allows FROND to inherently model non-local, memory-dependent dynamics, which are crucial for many real-world graphs exhibiting fractal structures or anomalous transport. Crucially, this approach analytically mitigates oversmoothing, a persistent challenge in GNNs, by leading to a slower, algebraic rate of convergence, offering a new, fundamental way to enhance GNN robustness and generalization.\n\nFurther enhancing adaptability, **[li202444f] \"Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?\" (2024)** proposes Morpher, a multi-modal prompt learning paradigm. Building upon the concept of prompt tuning ([fang2022tjj]), Morpher enables pre-trained GNNs to align with the semantic space of Large Language Models (LLMs) using *extremely weak text supervision* and independently pre-trained backbones. This innovation allows for CLIP-style zero-shot generalization for GNNs, a significant leap in making GNNs more transferable and semantically aware, especially in data-scarce environments.\n\nFinally, **[liu20242g6] \"A Review of Graph Neural Networks in Epidemic Modeling\" (2024)** provides a comprehensive overview of GNN applications in a critical real-world domain. Similar to earlier application-focused reviews ([reiser2022b08], [longa202399q]), this work systematizes the use of GNNs for tasks like epidemic detection, surveillance, and prediction, highlighting their unique ability to capture complex relational data and offering a roadmap for future interdisciplinary research.\n\n### Refined Synthesis\n\nThese works collectively trace a powerful intellectual trajectory in Graph Neural Networks, moving from a foundational understanding of their expressive limits to sophisticated architectural innovations, novel mathematical frameworks, and multi-modal adaptation strategies. The field has evolved to embrace hybrid spatial-spectral approaches for enhanced expressivity, leverage fractional calculus for modeling complex, memory-dependent dynamics and mitigating oversmoothing, and integrate multi-modal prompt learning for zero-shot generalization with weak supervision. This expanded view reveals a field increasingly focused on developing GNNs that are not only theoretically powerful but also inherently robust, highly adaptable, and capable of integrating diverse information sources to tackle the most complex and critical real-world applications.",
    "path": [
      "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "2a85846fd827a157b624ee012e75cbe37344281c",
      "5e6db511e736f77f844bbeebaa2b177427abada1",
      "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "b88f456daaf29860d2b59c621be3bd878a581a59",
      "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f"
    ],
    "layer1_papers": [
      {
        "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
        "abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.",
        "summary": "Here's a focused summary of the paper \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" \\cite{morris20185sd} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Standard Graph Neural Networks (GNNs) have shown empirical success but lack a strong theoretical understanding of their expressive power in distinguishing graph structures. It is unclear how they encode graph structure information and whether they have theoretical advantages over traditional graph kernels.\n    *   **Importance & Challenge**: Understanding the theoretical limits of GNNs is crucial for principled design and improvement. Many real-world graphs (e.g., social networks, molecules) possess complex higher-order structures (e.g., triangles, cliques, communities) that simple node-level message passing might not effectively capture, leading to limitations in tasks like graph classification and regression.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work theoretically connects GNNs to the 1-dimensional Weisfeiler-Leman (1-WL) graph isomorphism heuristic, which is a foundational algorithm for many successful graph kernels (e.g., WL subtree kernel).\n    *   **Limitations of Previous Solutions**:\n        *   **Graph Kernels**: Typically rely on fixed feature construction schemes (e.g., indicator features over subgraphs), limiting their adaptability to specific data distributions. Most focus only on graph structure and cannot interpret continuous node/edge labels.\n        *   **Standard GNNs (1-GNNs)**: While addressing some kernel limitations (adaptability, continuous features), this paper theoretically demonstrates that 1-GNNs have the same expressive power as the 1-WL algorithm. Consequently, they inherit the same shortcomings, such as inability to distinguish certain non-isomorphic graphs or capture simple higher-order properties like triangle counts.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **k-dimensional GNNs (k-GNNs)**, a generalization of 1-GNNs based on the k-dimensional Weisfeiler-Leman (k-WL) algorithm. Instead of performing message passing between individual nodes, k-GNNs perform message passing directly between *k-element subsets* (or k-tuples) of nodes.\n    *   **Novelty/Difference**:\n        *   **Higher-Order Message Passing**: This is the core innovation, allowing the model to capture structural information that is not visible at the node-level. The update rule for a k-set `s` aggregates features from its \"neighborhood\" of other k-sets `t` that share `k-1` elements with `s`.\n        *   **Hierarchical k-GNNs (1-k-GNNs)**: A further innovation is the design of hierarchical variants. These models combine graph representations learned at different granularities. Specifically, the initial messages (features) for k-sets in a k-GNN are derived from the output of a lower-dimensional k'-GNN (e.g., a 1-GNN). This allows the model to effectively capture both fine-grained (node-level) and coarse-grained (subgraph-level) structures in an end-to-end trainable framework.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight**: Proved that standard GNNs (1-GNNs) are not more powerful than the 1-WL algorithm in distinguishing non-isomorphic (sub-)graphs. Furthermore, with suitable parameter initialization, 1-GNNs achieve the same expressive power as 1-WL. This establishes a fundamental theoretical upper bound on the expressive power of current GNN architectures.\n    *   **Novel Algorithms/Methods**: Introduction of **k-GNNs**, which are strictly more powerful than 1-GNNs by operating on k-tuples/subsets.\n    *   **System Design/Architectural Innovation**: Proposal of **hierarchical 1-k-GNNs**, which leverage the outputs of lower-dimensional GNNs to initialize higher-dimensional GNNs, enabling multi-scale structural learning.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments on both graph classification and graph regression tasks to validate their theoretical findings and the utility of higher-order information.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Graph Regression (QM9 benchmark)**: On 12 molecular property prediction tasks, hierarchical models (1-2-GNN, 1-3-GNN) consistently and significantly outperformed traditional 1-GNNs. The 1-2-GNN reduced the Mean Absolute Error (MAE) by **54.45% on average** compared to the 1-GNN.\n        *   **Graph Classification (MUTAG, PTC_MR, NCI1, PROTEINS datasets)**: Hierarchical models (e.g., 1-2-GNN) also led to performance gains, though these were described as \"slight\" compared to the dramatic improvements seen in regression.\n    *   **Conclusion**: The experimental results confirm that higher-order graph properties are important for successful graph classification and regression, especially for tasks requiring a deeper understanding of molecular structures.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper highlights that 1-GNNs, like 1-WL, cannot capture simple graph-theoretic properties such as triangle counts or distinguish certain non-isomorphic graphs (e.g., a triangle vs. a 4-cycle). The proposed k-GNNs address these limitations by increasing `k`, but higher `k` values inherently lead to increased computational complexity and memory requirements due to the exponential growth in the number of k-sets. The specific set-based k-WL variant used for k-GNNs is noted to be different from the folklore k-WL but is shown to be as powerful as the folklore (k-1)-WL.\n    *   **Scope of Applicability**: k-GNNs are particularly relevant for applications where higher-order structural motifs (e.g., cycles, cliques, specific substructures) are crucial for understanding graph properties, such as in chemoinformatics (molecular graphs) and social network analysis.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper provides the first rigorous theoretical characterization of the expressive power of standard GNNs, linking them directly to the well-understood 1-WL algorithm. It then proposes a principled and theoretically grounded method (k-GNNs) to overcome these identified limitations, significantly advancing the expressive capabilities of GNNs.\n    *   **Potential Impact on Future Research**: The work opens new avenues for designing more powerful GNN architectures by explicitly incorporating higher-order structural information. It encourages further theoretical analysis of GNNs and the development of scalable k-GNN variants, potentially leading to breakthroughs in tasks requiring fine-grained structural understanding. The concept of hierarchical GNNs also suggests future research into multi-scale graph representation learning.",
        "year": 2018,
        "citation_key": "morris20185sd"
      }
    ],
    "layer2_papers": [
      {
        "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.",
        "summary": "Here's a focused summary of the paper \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" \\cite{michel2023hc4} for a literature review:\n\n*   **CITATION**: \\cite{michel2023hc4}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   Standard Graph Neural Networks (GNNs) are limited in their expressive power, being no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in distinguishing non-isomorphic graphs \\cite{michel2023hc4}.\n    *   This limitation restricts their ability to capture complex structural information, hindering performance on various graph learning tasks.\n    *   The problem is challenging because finding all paths in a graph is NP-hard, requiring efficient methods to leverage path information without incurring prohibitive computational costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Path-based GNNs**: Previous works incorporated shortest path distances as features (e.g., Graphormer, PEGN, SP-MPNN, Geodesic GNN) or aggregated nodes at specific shortest path distances.\n    *   **Limitations of Previous Path-based GNNs**: Many either use path information indirectly or focus on specific path types/lengths. The most related work, PathNet (Sun et al., 2022), samples paths, is only evaluated on node classification, and lacks an extensive study of expressive power, which PathNNs \\cite{michel2023hc4} explicitly address.\n    *   **Expressive GNNs**: Other approaches to enhance GNN expressive power include higher-order WL variants, k-order graph networks (some achieving 3-WL power), and methods using subgraphs or vertex identifiers. PathNNs \\cite{michel2023hc4} offer a distinct path-centric approach to achieve higher expressive power.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Idea**: Path Neural Networks (PathNNs) \\cite{michel2023hc4} update node representations by aggregating information from various paths emanating from each node.\n    *   **Path Encoding**: For each path length, a recurrent layer is used to encode paths into vectors.\n    *   **Aggregation**: The representations of all relevant paths emanating from a node are aggregated to produce the node's new representation.\n    *   **Three Variants**:\n        *   **Single Shortest Paths (SP)**: Aggregates a single shortest path for all possible node pair combinations.\n        *   **All Shortest Paths (SP+)**: Aggregates all possible shortest paths between every node pair combination.\n        *   **All Simple Paths (AP)**: Aggregates all simple paths (not necessarily shortest) up to a fixed length K.\n    *   **Key Innovation: Annotated Sets of Paths**: While initial `Path-Trees` (analogous to `WL-Trees`) were shown to be insufficient for `SP` and `SP+` to surpass 1-WL, the core innovation lies in operating on \"annotated sets of paths\" (`˜SP`, `˜SP+`, `˜AP`). Nodes within these paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, creating a richer, hierarchical structural encoding.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of Path Neural Networks (PathNNs) \\cite{michel2023hc4} as a novel GNN architecture that explicitly leverages path information for node representation learning.\n        *   Development of three distinct PathNN variants (SP, SP+, AP) based on different path collection strategies.\n        *   The concept and implementation of \"annotated sets of paths\" for recursively enriching path information, which is crucial for achieving higher expressive power.\n    *   **Theoretical Insights/Analysis**:\n        *   Formal definition of `Path-Trees` as a path-based analogue to `WL-Trees`.\n        *   Proof that `AP-Trees` are strictly more powerful than `WL-Trees` in distinguishing non-isomorphic graphs (Theorem 3.3).\n        *   Crucially, the theoretical demonstration that by operating on **annotated sets of paths**, two of the PathNN variants (`˜SP`, `˜SP+`) are strictly more powerful than the 1-WL algorithm, and the most expressive variant (`˜AP`) can even distinguish graphs indistinguishable by the 3-WL algorithm (Theorem 3.5, as implied by abstract/intro).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation on synthetic datasets specifically designed to measure expressive power, testing the ability to distinguish non-isomorphic graphs.\n        *   Performance evaluation on real-world graph classification and graph regression datasets.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Expressive Power**: PathNNs \\cite{michel2023hc4} successfully distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL. The most expressive PathNN variant (`˜AP`) demonstrated the ability to distinguish graphs that are even 3-WL indistinguishable, empirically validating the theoretical claims.\n        *   **Real-world Tasks**: On graph classification and regression datasets, the different PathNN variants achieved high levels of performance, outperforming baseline methods in most cases.\n\n6.  **Limitations & Scope**\n    *   **Computational Complexity**: Finding all simple paths (`AP` variant) in a graph is NP-hard. The model addresses this by considering paths only up to a fixed length `K`.\n    *   **Scalability of AP-Trees**: `AP-Trees` (and by extension, the `˜AP` variant) can grow exponentially with height `K` and graph density, potentially limiting the practical maximum path length `K` that can be used.\n    *   **Scope of Applicability**: The paper focuses on undirected graphs and does not explicitly discuss applicability to directed graphs or other graph types (e.g., hypergraphs).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PathNNs \\cite{michel2023hc4} significantly advance the technical state-of-the-art by providing GNN architectures that provably surpass the expressive power of the 1-WL algorithm, a known bottleneck for many standard GNNs. The ability to distinguish 3-WL indistinguishable graphs with a path-based approach is a notable achievement.\n    *   **Potential Impact on Future Research**: This work opens new avenues for designing more powerful GNNs by effectively leveraging path information. It highlights the importance of recursive, hierarchical structural encoding (via annotated paths) for enhancing expressive power. Future research could explore more efficient path enumeration/sampling strategies, adaptive path length selection, or the integration of path information with other structural elements (e.g., subgraphs) to further push the boundaries of GNN expressiveness and performance.",
        "year": 2023,
        "citation_key": "michel2023hc4"
      },
      {
        "title": "Substructure Aware Graph Neural Networks",
        "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Conventional Graph Neural Networks (GNNs) are fundamentally limited by the expressive power of the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \\cite{zeng20237gv}.\n    *   This limitation prevents GNNs from perceiving crucial higher-order substructures, which are vital for many downstream tasks (e.g., functional groups in organic chemistry) \\cite{zeng20237gv}.\n    *   The problem is challenging because directly constructing higher-order GNNs leads to scalability and complexity issues, while using predefined hand-crafted substructures compromises generalization ability \\cite{zeng20237gv}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to overcome 1-WL limitations include: higher-order GNNs (suffering from high computational cost), predefined hand-crafted substructures (lacking generalization), inductive coloring methods (often task-specific), and positional encoding methods (facing issues like global sign ambiguity or limited inductive generalization) \\cite{zeng20237gv}.\n    *   This work positions itself by introducing a novel framework, Substructure Aware Graph Neural Networks (SAGNN), that leverages subgraphs to enhance GNN expressiveness. It aims to overcome the 1-WL barrier without incurring the high complexity of higher-order GNNs or the generalization issues of fixed substructures, by focusing on flexible subgraph extraction and encoding \\cite{zeng20237gv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the Substructure Aware Graph Neural Network (SAGNN) framework, which injects subgraph-level structural information into node features during message passing \\cite{zeng20237gv}.\n    *   **Novel Subgraph Extraction:** Introduces the \"Cut subgraph,\" which is obtained from the original graph by continuously and selectively removing edges with the highest Edge Betweenness Centrality (EBC) until the graph is split into a specified number of connected blocks \\cite{zeng20237gv}. This allows for capturing structural information at various granularities.\n    *   **Novel Subgraph Encoding:** Extends the random walk encoding paradigm to compute \"return probabilities\" of a rooted node within a subgraph for a given number of steps \\cite{zeng20237gv}. This method efficiently captures rich structural information and is used as a node feature.\n    *   **Subgraph Information Injection:** The encoded subgraph features (from both Ego-networks and Cut subgraphs) are concatenated with initial node features and original graph structural representations. These enhanced features are then propagated through two parallel message passing channels (Ego channel and Cut channel) \\cite{zeng20237gv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Subgraph Definition:** The \"Cut subgraph\" extraction strategy, which dynamically partitions the graph based on edge centrality to reveal meaningful substructures \\cite{zeng20237gv}.\n    *   **Novel Subgraph Encoding Method:** An efficient random walk return probability encoding for subgraphs, designed to capture structural information with reduced time complexity and improved expressiveness \\cite{zeng20237gv}.\n    *   **Framework Design:** The Substructure Aware Graph Neural Network (SAGNN) framework, which seamlessly integrates these novel subgraph features into standard GNN message passing architectures \\cite{zeng20237gv}.\n    *   **Theoretical Insight:** Provides theoretical proof that any 1-WL GNN equipped with components of the SAGNN framework is strictly more powerful than 1-WL \\cite{zeng20237gv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various well-proven graph tasks, including graph classification on TUDatasets (MUTAG, PTC, PROTEINS, NCI1, IMDB-B) and graph regression for drug constrained solubility prediction (ZINC-FULL) \\cite{zeng20237gv}.\n    *   **Key Performance Metrics:** Accuracy for classification tasks and Mean Absolute Error (MAE) for regression tasks \\cite{zeng20237gv}.\n    *   **Comparison Results:**\n        *   Achieved state-of-the-art (SOTA) performance across a variety of datasets and when integrated with different base GNN models (e.g., GIN, PNA) \\cite{zeng20237gv}.\n        *   Demonstrated a maximum performance improvement of 83% compared to base models and 32% compared to previous SOTA methods \\cite{zeng20237gv}.\n        *   GNNs equipped with SAGNN performed flawlessly even on graphs that are known to fail the 3-WL test, indicating significantly enhanced structure perception \\cite{zeng20237gv}.\n\n*   **Limitations & Scope**\n    *   The provided abstract and introduction do not explicitly detail specific technical limitations or assumptions inherent to the SAGNN framework itself.\n    *   The scope of applicability is broad, covering general graph learning tasks such as graph classification and regression, particularly in domains where higher-order structural information is crucial, like chemo-informatics \\cite{zeng20237gv}. The method is designed to enhance the expressiveness of existing 1-WL GNNs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** Significantly pushes the expressive power of GNNs beyond the 1-WL limit by leveraging subgraph information, offering a more scalable alternative to complex higher-order GNNs \\cite{zeng20237gv}.\n    *   **Improved Structure Perception:** Enables GNNs to perceive and utilize higher-order substructures that were previously indistinguishable, leading to superior performance on challenging graph tasks and even on graphs that fail the 3-WL test \\cite{zeng20237gv}.\n    *   **Practical Impact:** Provides a general and flexible framework that can be integrated with various existing GNN architectures, leading to substantial performance gains on real-world graph learning tasks \\cite{zeng20237gv}.\n    *   **Future Research:** The novel subgraph extraction and encoding methods open new avenues for exploring dynamic and expressive ways to incorporate substructural information into graph learning models \\cite{zeng20237gv}.",
        "year": 2023,
        "citation_key": "zeng20237gv"
      },
      {
        "title": "Theory of Graph Neural Networks: Representation and Learning",
        "abstract": "Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections.",
        "summary": "This paper \\cite{jegelka20222lq} provides a theoretical survey of Graph Neural Networks (GNNs), focusing on their representational power, generalization capabilities, and extrapolation under distribution shifts. It primarily analyzes Message Passing GNNs (MPNNs) and higher-order GNNs, summarizing key mathematical connections and theoretical results from the literature.\n\nHere's a focused summary for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the theoretical understanding of Graph Neural Networks (GNNs), particularly their ability to learn rich representations of graph-structured data for various prediction tasks (node, graph, or point configurations). It investigates fundamental questions regarding what GNNs can represent, how well they generalize, and their capacity to extrapolate to unseen data distributions.\n    *   **Importance and Challenge**: GNNs have achieved widespread empirical success in diverse applications (social networks, recommender systems, molecular property prediction, combinatorial optimization). However, a deep theoretical understanding of *why* they work, their inherent limitations, and how to design more powerful architectures is crucial. This understanding is challenging due to the complex, non-Euclidean nature of graph data and the permutation invariance/equivariance requirements.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions GNNs as a powerful, adaptable alternative to traditional graph embeddings (e.g., spectral embeddings, graph kernels) because they can adapt embeddings to specific tasks, generalize across graphs, and incorporate attributes. It specifically focuses on Message Passing GNNs (MPNNs) as a popular spatial GNN architecture.\n    *   **Limitations of Previous Solutions (and standard MPNNs)**:\n        *   Traditional graph embeddings often lack adaptability to specific tasks or the ability to incorporate rich node/edge attributes.\n        *   Standard MPNNs, while powerful, are limited in their discriminative power, often being no more powerful than the 1-Weisfeiler-Leman (1-WL) graph isomorphism test. This means they cannot distinguish certain non-isomorphic graphs (e.g., regular graphs) or decide complex structural properties like girth, diameter, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   These limitations motivate the exploration of higher-order GNNs and augmentations like node IDs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to survey and synthesize theoretical results, primarily by connecting GNN capabilities to established computational models and mathematical theories.\n        *   **MPNN Framework**: It formally defines MPNNs as an iterative message-passing scheme (Equations 1.3-1.9 \\cite{jegelka20222lq}), emphasizing the aggregation and update functions, and the requirement for permutation invariance/equivariance.\n        *   **WL Test Analogy**: A central approach is to relate the discriminative power of MPNNs to the 1-Weisfeiler-Leman (1-WL) algorithm for graph isomorphism testing (Theorem 1 \\cite{jegelka20222lq}).\n        *   **Multiset Function Approximation**: It leverages universal approximation theorems for multiset functions (Theorem 2 \\cite{jegelka20222lq}) to explain how GNNs can represent permutation-invariant aggregations.\n        *   **Local Algorithms Analogy**: It draws parallels between GNNs (especially with node IDs) and local algorithms in distributed computing (e.g., CONGEST, LOCAL models) to derive computational power and lower bounds.\n    *   **Novelty/Difference (as discussed in the paper)**:\n        *   **Graph Isomorphism Network (GIN)** \\cite{jegelka20222lq}: Highlighted as an MPNN that explicitly implements the sum decomposition for injective multiset functions, achieving the maximum discriminative power of 1-WL.\n        *   **Node ID Augmentations**: Discusses augmenting MPNNs with unique or random node identifiers to overcome 1-WL limitations, leading to increased representational power and even Turing completeness (Theorem 4 \\cite{jegelka20222lq}).\n        *   **Higher-Order GNNs**: Explores architectures that operate on k-tuples of nodes (instead of single nodes), providing a unified tensor-based framework to increase expressive power beyond 1-WL.\n        *   **CPNGNN**: A specific MPNN model that incorporates port numbering from local algorithms to improve approximation capabilities for certain combinatorial problems (Theorem 3 \\cite{jegelka20222lq}).\n\n4.  **Key Technical Contributions (as summarized/presented by the paper)**\n    *   **Theoretical Insights & Analysis**:\n        *   Formal connection between MPNN discriminative power and the 1-Weisfeiler-Leman algorithm (Theorem 1 \\cite{jegelka20222lq}).\n        *   Universal approximation theorems for multiset functions, explaining the design principles for powerful aggregation functions (Theorem 2 \\cite{jegelka20222lq}).\n        *   Identification of specific graph properties that MPNNs *cannot* decide (Proposition 2 \\cite{jegelka20222lq}).\n        *   Proof of Turing completeness for MPNNs augmented with unique node IDs, establishing equivalence to the LOCAL model of distributed computing (Theorem 4, Corollary 1 \\cite{jegelka20222lq}).\n        *   Derivation of lower bounds on GNN depth and width for solving various combinatorial problems, based on analogies to the CONGEST model (Theorem 5 \\cite{jegelka20222lq}).\n        *   Probabilistic universal approximation results for MPNNs with random node IDs (Theorem 6 \\cite{jegelka20222lq}).\n    *   **Novel Algorithms/Methods (discussed as advancements in the field)**:\n        *   **Graph Isomorphism Network (GIN)**: An MPNN designed for maximal 1-WL discriminative power.\n        *   **CPNGNN**: A GNN model incorporating port numbering for improved approximation algorithms.\n        *   **Higher-order GNNs**: Architectures that process k-tuples of nodes to enhance expressive power.\n\n5.  **Experimental Validation**\n    *   This paper is a *theoretical survey* and *does not present new experimental validation*. Instead, it synthesizes and summarizes theoretical results and mathematical connections from existing literature. The \"key performance metrics and comparison results\" discussed are theoretical bounds and equivalences, rather than empirical benchmarks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations of MPNNs**:\n        *   Cannot distinguish regular graphs or graphs with identical computation trees (Figure 1 \\cite{jegelka20222lq}).\n        *   Inability to decide properties like girth, diameter, circumference, or count induced subgraphs (Proposition 2 \\cite{jegelka20222lq}).\n        *   Limited approximation capabilities for certain combinatorial problems (e.g., maximum matching for CPNGNNs, Theorem 3 \\cite{jegelka20222lq}).\n        *   Computational power loss for bounded size GNNs without unique node IDs (Theorem 5 \\cite{jegelka20222lq}).\n    *   **Assumptions**: Universal approximation results for multiset functions often assume continuous functions and sufficient latent dimension (at least maximum degree). Turing completeness requires Turing complete aggregation/update functions and unbounded width.\n    *   **Scope of Applicability**: The survey primarily focuses on Message Passing GNNs and higher-order GNNs, with a brief mention of spectral GNNs but not in detail. It covers representation, generalization, and extrapolation, acknowledging that GNNs are a rapidly evolving field and not all works can be included.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing a structured and comprehensive theoretical framework for understanding GNNs. It clarifies the fundamental limits of standard MPNNs and highlights architectural innovations (node IDs, higher-order processing) that overcome these limitations.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The theoretical insights (e.g., WL equivalence, multiset approximation) provide guiding principles for designing more expressive and powerful GNN architectures.\n        *   **Understanding Generalization**: The discussion on generalization and extrapolation under distribution shifts is critical for developing robust and reliable GNNs for real-world applications.\n        *   **Combinatorial Optimization**: The connections to local algorithms and lower bounds offer a roadmap for applying GNNs to hard combinatorial problems and understanding their inherent computational limits.\n        *   **Foundation for Further Theory**: By synthesizing existing theoretical results, the paper establishes a strong foundation for future theoretical investigations into GNNs, including their connections to logic, distributed computing, and graph theory.",
        "year": 2022,
        "citation_key": "jegelka20222lq"
      },
      {
        "title": "On the Expressive Power of Geometric Graph Neural Networks",
        "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
        "summary": "Here's a focused summary of the paper by \\cite{joshi20239d0} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: On the Expressive Power of Geometric Graph Neural Networks \\cite{joshi20239d0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Characterizing and understanding the expressive power of Graph Neural Networks (GNNs) for *geometric graphs* embedded in Euclidean space. Standard GNNs and the Weisfeiler-Leman (WL) test, widely used for non-geometric graphs, are inapplicable to these systems.\n    *   **Importance & Challenge**: Geometric graphs, such as biomolecules, materials, and physical simulations, inherently possess both relational structure and geometric attributes (e.g., 3D coordinates, velocities). These geometric attributes must respect physical symmetries (permutations, rotation, reflection, translation). Standard GNNs fail to account for these symmetries, leading to a loss of physical meaning and transformation behavior. The challenge lies in developing a theoretical framework that can assess how well geometric GNNs can distinguish between geometrically non-isomorphic graphs while respecting these symmetries.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the established Weisfeiler-Leman (WL) graph isomorphism test, which has been a powerful tool for analyzing the expressive power of non-geometric GNNs.\n    *   **Limitations of Previous Solutions**: The standard WL framework and non-geometric GNNs do not directly apply to geometric graphs because they lack the ability to account for spatial symmetries. Geometric graphs exhibit a stronger notion of \"geometric isomorphism\" that requires invariance or equivariance to Euclidean transformations, which the WL test does not capture. Existing geometric GNNs (both G-equivariant and G-invariant) have shown empirical success, but their theoretical expressive power remained largely uncharacterized.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Geometric Weisfeiler-Leman (GWL) test**, a novel generalization of the WL test designed for discriminating geometric graphs while respecting physical symmetries.\n        *   GWL iteratively updates node \"colors\" and auxiliary \"geometric objects\" (`g(t)_i`) that aggregate geometric information from progressively larger t-hop neighborhoods.\n        *   It employs a **G-orbit injective and G-invariant function (I-HASH)** to compute scalar node colors, ensuring that neighborhoods identical up to a group action receive the same color.\n        *   Crucially, the aggregation of geometric objects (`g(t)_i`) is designed to be **injective and G-equivariant**, preserving local geometric orientation and information.\n    *   **Novelty/Difference**:\n        *   **Symmetry-Aware Isomorphism Test**: GWL is the first theoretical framework to extend the WL test to geometric graphs, explicitly incorporating invariance/equivariance to permutations, rotations, reflections, and translations.\n        *   **Granular Expressivity Analysis**: Unlike binary universal approximation, GWL provides a discrimination-based perspective, offering a more granular and practically insightful lens into geometric GNN expressivity by linking it to the ability to distinguish geometric (sub-)graphs.\n        *   **Unified Framework**: It provides a theoretical upper bound for the expressive power of both G-equivariant and G-invariant geometric GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Geometric Weisfeiler-Leman (GWL) test** as a theoretical upper bound for the expressive power of geometric GNNs.\n    *   **Theoretical Insights/Analysis**:\n        *   **Characterization of Expressivity**: GWL formally characterizes how key design choices influence geometric GNN expressivity:\n            *   **Invariant Layers' Limitations**: Invariant GNNs (and the proposed Invariant GWL, IGWL) have limited expressivity; they cannot distinguish \"1-hop identical\" geometric graphs and fail to compute non-local geometric properties (e.g., volume, centroid).\n            *   **Equivariant Layers' Power**: Equivariant GNNs (and GWL) distinguish a larger class of graphs by propagating geometric information beyond local neighborhoods through stacking equivariant layers.\n            *   **Role of Higher-Order Tensors/Scalarization**: Higher-order tensors and scalarization (e.g., `IGWL(k)` with higher body order `k`) are shown to enable maximally powerful geometric GNNs by providing more complete descriptors of local geometry (e.g., beyond just distances and angles).\n            *   **Depth**: Increasing depth (number of layers) allows GWL and equivariant GNNs to aggregate information from larger k-hop neighborhoods, distinguishing k-hop distinct graphs.\n        *   **Equivalence to Universal Approximation**: The paper proves an equivalence between a model's ability to discriminate geometric graphs (via GWL) and its universal approximation capabilities for G-invariant functions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Synthetic experiments were performed to demonstrate the practical implications of the theoretical findings. These experiments are available at `https://github.com/chaitjo/geometric-gnn-dojo`.\n    *   **Key Performance Metrics & Comparison Results**: The experiments focused on illustrating:\n        *   **Geometric Oversquashing with Increased Depth**: Demonstrating how increasing depth in geometric GNNs can lead to oversquashing, a phenomenon where distinct geometric information becomes indistinguishable.\n        *   **Utility of Higher Order Spherical Tensors**: Providing counterexamples that highlight the necessity and utility of higher-order spherical tensors for distinguishing complex local geometries that lower-order invariants (like distances and angles) cannot resolve. This empirically supports the theoretical claim about the importance of scalarization body order.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   GWL assumes finite-sized geometric graphs and features drawn from countable datasets.\n        *   The `HASH` and `I-HASH` functions are idealized injective and G-orbit injective maps, respectively, which are not necessarily continuous in practical implementations.\n        *   The theoretical analysis focuses on the *upper bound* of expressivity, and achieving this bound in practical GNNs requires specific conditions on aggregation, update, and readout functions (e.g., injectivity, equivariance).\n    *   **Scope of Applicability**: The framework is applicable to geometric graphs embedded in Euclidean space, particularly relevant for systems in biochemistry, material science, and physical simulations where physical symmetries are crucial. It primarily focuses on distinguishing geometric graphs and characterizing the expressive power of GNNs in this context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the theoretical understanding of geometric GNNs by providing the first principled framework (GWL) to analyze their expressive power in a symmetry-aware manner. It extends the foundational WL test to a critical domain where standard GNNs fall short.\n    *   **Potential Impact on Future Research**:\n        *   **Guiding Architecture Design**: The insights derived from GWL (e.g., the importance of equivariant layers, higher-order tensors, and understanding depth limitations) can directly guide the design of more powerful and expressive geometric GNN architectures.\n        *   **Benchmarking and Evaluation**: GWL provides a theoretical benchmark against which the expressive power of new geometric GNN models can be rigorously evaluated.\n        *   **Deeper Theoretical Understanding**: It offers a more granular and practically relevant perspective on expressivity compared to universal approximation, fostering deeper theoretical investigations into the capabilities and limitations of geometric deep learning models.",
        "year": 2023,
        "citation_key": "joshi20239d0"
      },
      {
        "title": "Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion",
        "abstract": "Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately. Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.",
        "summary": "Here is a focused summary of the technical paper \\cite{ju2023prm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Formally understanding and quantifying the generalization performance (the gap between training and test performance) of Graph Neural Networks (GNNs), especially for deep and overparameterized models and in fine-tuning scenarios.\n    *   **Importance & Challenge**: Generalization is crucial for machine learning model success. For deep models like GNNs, classical complexity measures (e.g., VC dimension) often lead to vacuous bounds that do not explain empirical performance. Prior GNN generalization bounds scale with the graph's maximum degree, `d^(l-1)`, which can be numerically very large for real-world graphs, rendering them uninformative. Fine-tuning large GNNs often leads to overfitting without proper intervention, necessitating a better understanding of generalization.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on PAC-Bayesian analysis and stability properties, similar to prior generalization bounds for fully-connected networks. It extends previous GNN generalization analyses.\n    *   **Limitations of Previous Solutions**:\n        *   Naive application of bounds from feedforward networks to GNNs results in vacuous terms scaling with `n^(l-1)` (number of nodes).\n        *   VC dimension for GNNs scales with `n`, ignoring graph structure.\n        *   Verma and Zhang (2019) provided bounds scaling with the largest singular value of the graph diffusion matrix but only for single-layer GNNs and node prediction tasks.\n        *   Garg et al. (2020) and Liao et al. (2021) analyzed multi-layer message-passing GNNs for graph prediction, but their bounds scaled with `d^(l-1)` (maximum degree), which is often numerically much larger than observed generalization gaps and the spectral norm of diffusion matrices.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs PAC-Bayesian analysis to derive generalization bounds by measuring the stability of GNNs against noise perturbations. The key insight is to quantify this stability using Hessians of the loss function.\n    *   **Novelty/Difference**:\n        *   **Spectral Norm Dependence**: Instead of the maximum degree, the bounds scale with the largest singular value (spectral norm) of the graph diffusion matrix `P_G^(l-1)`. This provides significantly tighter and non-vacuous bounds.\n        *   **Unified Model Analysis**: The analysis applies to a unified GNN model that subsumes various architectures, including Message-Passing Neural Networks (MPNNs), Graph Convolutional Networks (GCNs), and Graph Isomorphism Networks (GINs), without requiring weight tying across layers.\n        *   **Hessian-based Stability Measure**: Introduces a novel approach to measure noise stability via the trace of the loss Hessian matrix, which is shown to correlate accurately with observed generalization gaps. This technique involves a uniform convergence analysis of the Hessian matrix.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Sharp Generalization Bounds (Theorem 3.1)**: Proves PAC-Bayesian generalization bounds for `l`-layer GNNs (MPNN, GCN, GIN) that scale with the spectral norm of `P_G^(l-1)`, where `P_G` is the graph diffusion matrix. These bounds are numerically orders of magnitude smaller than prior `d^(l-1)` bounds.\n        *   **Matching Lower Bound (Theorem 3.2)**: Constructs a lower bound instance where the generalization gap asymptotically matches their upper bound, demonstrating the tightness of their results.\n        *   **Hessian-based Generalization Measure (Lemma 4.3)**: Shows that the trace of the loss Hessian matrix can accurately measure the noise stability and, consequently, the generalization of GNNs. This applies to twice-differentiable and Lipschitz-continuous activations.\n        *   **Hessian Regularization Algorithm**: Proposes an algorithm that performs gradient updates on perturbed weight matrices, effectively minimizing the average loss of multiple perturbed models, which is equivalent to regularizing the GNN's Hessian in expectation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        1.  **Numerical Comparison of Bounds**: Evaluated the numerical values of their spectral norm-based bounds against prior maximum degree-based bounds for GCNs and MPNNs with varying layers.\n        2.  **Correlation of Hessian Measure**: Measured the Hessian-based generalization measure and compared it with empirically observed generalization gaps of GNNs.\n        3.  **Fine-tuning Algorithm Performance**: Applied their Hessian regularization algorithm for fine-tuning pretrained GNNs on graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Bounds Comparison**: On real-world graphs (IMDB-B, IMDB-M, COLLAB, REDDIT-B, REDDIT-M), the spectral norm bounds were found to be orders of magnitude smaller (e.g., `10^3` to `10^27` times smaller) than prior `d^(l-1)` bounds, even when accounting for weight norms (Figure 1a, Figure 2). For GCNs using normalized adjacency matrices, the graph dependence reduces to 1, providing exponential improvement.\n        *   **Hessian Measure Correlation**: The Hessian-based generalization measure accurately correlated with the empirically observed generalization gaps of GNNs (Figure 1b), demonstrating its practical utility as a diagnostic tool.\n        *   **Fine-tuning Performance**: The proposed Hessian regularization algorithm improved test performance on several graph-level classification tasks using Molecular graphs, demonstrating its practical benefit in robust fine-tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis for the Hessian-based measure specifically applies to GNNs with twice-differentiable and Lipschitz-continuous activation functions (e.g., tanh, sigmoid).\n    *   **Scope of Applicability**: Primarily focuses on graph-level prediction tasks, though the authors note that the results permit extension to node prediction tasks. The unified model covers a broad range of common GNN architectures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ju2023prm} significantly advances the theoretical understanding of generalization in GNNs by providing the first sharp, non-vacuous generalization bounds that scale with the spectral norm of the graph diffusion matrix, rather than the often-large maximum degree. This offers a more realistic and tighter quantification of GNN complexity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a practical, Hessian-based tool for measuring GNN generalization, which can guide model design and hyperparameter tuning.\n        *   Inspires the development of robust fine-tuning methods for GNNs by leveraging stability properties, as demonstrated by their proposed algorithm.\n        *   The developed tools and analysis techniques (e.g., uniform convergence of Hessians) may be useful for studying other aspects of GNNs, such as extrapolation to different graph sizes.",
        "year": 2023,
        "citation_key": "ju2023prm"
      },
      {
        "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
        "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing -- aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the fundamental limitation of most popular Graph Neural Networks (GNNs), which rely on 1-hop message passing and are bounded in expressive power by the 1-dimensional Weisfeiler-Lehman (1-WL) test \\cite{feng20225sa}. This means they cannot distinguish many non-isomorphic graph structures.\n    *   While K-hop message passing (aggregating information from K-hop neighbors) has been proposed, there was no theoretical work characterizing its expressive power, leaving a gap in understanding its capabilities and limitations \\cite{feng20225sa}.\n\n*   **Related Work & Positioning**\n    *   Existing GNNs primarily use 1-hop message passing, which is known to be limited by the 1-WL test \\cite{feng20225sa}.\n    *   Previous works extended to K-hop message passing (e.g., GPR-GNN, MixHop, GINE+, Graphormer) but often misused or interchanged different definitions of K-hop kernels (shortest path distance vs. graph diffusion) without theoretical analysis of their distinct expressive powers \\cite{feng20225sa}.\n    *   The paper positions itself as the first to theoretically characterize the expressive power of K-hop message passing, differentiate its kernels, identify its limitations, and propose an enhancement.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach involves a rigorous theoretical characterization of K-hop message passing GNNs.\n    *   **Differentiation of K-hop Kernels**: Formally distinguishes two kernels for K-hop neighbors: shortest path distance (spd) and graph diffusion (gd) \\cite{feng20225sa}.\n    *   **Expressive Power Analysis**: Proves that K-hop message passing is strictly more powerful than 1-WL and can distinguish almost all regular graphs, but also shows it's bounded by the 3-WL test and fails on some simple regular graphs \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Introduces the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework. This novel approach enhances K-hop message passing by incorporating information from the \"peripheral subgraph\" (subgraph induced by neighbors in a specific hop) in addition to individual neighbor features \\cite{feng20225sa}. This allows the model to learn more expressive local structural features.\n\n*   **Key Technical Contributions**\n    *   **Formal Kernel Definitions**: Provides formal definitions and differentiation of shortest path distance (spd) and graph diffusion (gd) kernels for K-hop neighbors, clarifying their distinct impacts on expressive power \\cite{feng20225sa}.\n    *   **Theoretical Expressive Power Bounds**:\n        *   Proves that K-hop message passing (with K > 1) is strictly more powerful than 1-hop message passing and the 1-WL test \\cite{feng20225sa}.\n        *   Demonstrates that K-hop GNNs can distinguish almost all regular graphs with a modest K \\cite{feng20225sa}.\n        *   Establishes that the expressive power of K-hop message passing, regardless of the kernel, is bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **KP-GNN Framework**: Proposes KP-GNN, which enhances K-hop message passing by integrating peripheral subgraph information (edges within a hop's neighbors). This framework significantly improves expressive power, enabling it to distinguish many distance regular graphs that even 3-WL or previous distance encoding methods could not \\cite{feng20225sa}.\n    *   **Flexibility and Efficiency**: KP-GNN can be applied to most existing K-hop GNNs with minor modifications and adds only little computational complexity \\cite{feng20225sa}.\n\n*   **Experimental Validation**\n    *   The paper states that experimental results verify the expressive power and effectiveness of KP-GNN \\cite{feng20225sa}.\n    *   KP-GNN achieves competitive results across all benchmark datasets, demonstrating its practical utility \\cite{feng20225sa}. (Specific datasets or metrics are not detailed in the provided abstract/introduction).\n\n*   **Limitations & Scope**\n    *   **K-hop Limitations**: The choice of K-hop kernel (spd vs. gd) affects expressive power, and neither can distinguish all simple non-isomorphic structures \\cite{feng20225sa}. The expressive power of K-hop message passing is fundamentally bounded by the 3-WL test \\cite{feng20225sa}.\n    *   **Scope**: The analysis primarily focuses on distinguishing non-isomorphic graphs and node configurations based on local structural information, assuming uniform node features for theoretical clarity.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing the first comprehensive theoretical characterization of K-hop message passing GNNs \\cite{feng20225sa}.\n    *   It clarifies the impact of different K-hop kernels and establishes precise bounds on their expressive power, offering crucial insights for GNN design \\cite{feng20225sa}.\n    *   The introduction of KP-GNN provides a practical and theoretically grounded method to overcome the limitations of standard K-hop GNNs, paving the way for more powerful and discriminative GNN architectures in future research \\cite{feng20225sa}.",
        "year": 2022,
        "citation_key": "feng20225sa"
      },
      {
        "title": "The expressive power of pooling in Graph Neural Networks",
        "abstract": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{bianchi20239ee}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of understanding regarding how graph pooling operators impact the expressive power of Graph Neural Networks (GNNs) \\cite{bianchi20239ee}. Specifically, there is no principled, theoretically grounded criterion to compare or design these operators.\n    *   **Importance & Challenge**: GNNs' ability to distinguish non-isomorphic graphs (their expressive power) is crucial for their performance. Hierarchical pooling is vital for building deep GNNs and learning abstract graph representations. However, current pooling evaluation relies heavily on empirical downstream task performance, which is indirect, influenced by many external factors, and doesn't directly assess expressiveness \\cite{bianchi20239ee}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has extensively characterized the expressive power of message-passing (MP) layers in GNNs, often linking it to the Weisfeiler-Lehman (WL) isomorphism test \\cite{bianchi20239ee}.\n    *   **Limitations of Previous Solutions**: These expressiveness studies are largely confined to \"flat GNNs\" (stacks of MP layers followed by a readout), overlooking the role and impact of hierarchical pooling operators \\cite{bianchi20239ee}. Existing empirical pooling evaluation methods are indirect and lack theoretical grounding, while other proposed criteria (e.g., spectral similarity) can be inconsistent \\cite{bianchi20239ee}. This work positions itself to fill this theoretical gap for hierarchical GNNs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is to derive sufficient conditions under which a graph pooling operator can fully preserve the expressive power of the preceding MP layers \\cite{bianchi20239ee}. This ensures that the combined GNN architecture remains injective, capable of distinguishing non-isomorphic graphs.\n    *   **The conditions (Theorem 1) are**:\n        1.  The preceding MP layers must produce distinct feature sums for WL-distinguishable graphs (i.e., `sum(XL_i)` != `sum(YL_i)` for non-isomorphic G1, G2) \\cite{bianchi20239ee}.\n        2.  The `SEL` (selection) function of the pooling operator must ensure that the sum of membership scores for each original node across all supernodes is a positive constant (`sum_j(s_ij) = lambda > 0`). This implies all original nodes contribute to the coarsened graph \\cite{bianchi20239ee}.\n        3.  The `RED` (reduction) function must compute pooled node features as a linear transformation of the original node features using the cluster assignment matrix (`XP = S^T XL`), effectively a convex combination \\cite{bianchi20239ee}.\n    *   **Novelty**: This work introduces the first principled, theoretically grounded criterion for evaluating and designing graph pooling operators based on their expressiveness, moving beyond indirect empirical performance metrics \\cite{bianchi20239ee}. It provides clear design principles and a diagnostic tool for existing methods.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Framework**: Formal derivation of sufficient conditions (Theorem 1) for graph pooling operators to preserve the expressive power of preceding MP layers, ensuring the GNN's ability to distinguish non-isomorphic graphs \\cite{bianchi20239ee}.\n    *   **Universal Criterion**: Introduction of a theoretically grounded and universal criterion for comparing and designing graph pooling operators, focusing on their expressiveness rather than indirect downstream task performance \\cite{bianchi20239ee}.\n    *   **Analysis of Existing Operators**: Identification of commonly used pooling operators (e.g., Top-k, ASAPool, SAGPool, PanPool) that fail to meet these expressiveness conditions, explaining their potential limitations. Conversely, dense pooling operators (e.g., DiffPool, MinCutPool, DMoN) and certain sparse ones (e.g., ECPool, k-MISPool) are shown to satisfy the conditions \\cite{bianchi20239ee}.\n    *   **Empirical Validation Setup**: Proposal of a simple yet effective experimental setup to empirically measure the expressive power of any GNN (including those with pooling layers) by directly testing its capability to perform a graph isomorphism test \\cite{bianchi20239ee}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper introduces a novel experimental setup designed to empirically verify the expressive power of GNNs equipped with pooling layers \\cite{bianchi20239ee}.\n    *   **Key Performance Metrics and Comparison Results**: The primary objective of this setup is to measure the GNN's capability to perform a graph isomorphism test \\cite{bianchi20239ee}. While the paper details the *design* of this validation method, specific experimental results, datasets, or comparative performance metrics are not provided in the given text. The focus is on establishing a direct method for expressiveness assessment.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The derived conditions are *sufficient* but not *necessary*, implying that other mechanisms for preserving expressiveness might exist outside this framework \\cite{bianchi20239ee}. The analysis primarily focuses on standard MP-GNNs, which are at most as powerful as the WL test. The `CON` (connection) function of the pooling operator, while not affecting expressiveness under these conditions, can still compromise the effectiveness of subsequent MP layers \\cite{bianchi20239ee}.\n    *   **Scope of Applicability**: The framework is applicable to hierarchical GNNs that interleave MP layers with pooling operators, providing guidance for both selecting existing pooling operators and designing new ones \\cite{bianchi20239ee}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the theoretical understanding of GNN expressiveness by extending the analysis to hierarchical architectures that incorporate pooling layers, a previously underexplored area \\cite{bianchi20239ee}. It provides a much-needed theoretical foundation for evaluating pooling operators, moving beyond purely empirical assessments.\n    *   **Potential Impact on Future Research**:\n        *   **Informed Design**: Offers a principled criterion for designing new, expressive graph pooling operators and for selecting appropriate existing ones, leading to more powerful and reliable GNN architectures \\cite{bianchi20239ee}.\n        *   **Debunking Misconceptions**: Provides a theoretical basis to understand why certain popular pooling operators might fail in specific scenarios, potentially clarifying common criticisms or misconceptions about graph pooling \\cite{bianchi20239ee}.\n        *   **Future Research Directions**: Opens avenues for exploring necessary conditions for pooling expressiveness and for developing pooling operators that explicitly satisfy these conditions while optimizing other factors like computational efficiency or specific task performance \\cite{bianchi20239ee}.",
        "year": 2023,
        "citation_key": "bianchi20239ee"
      },
      {
        "title": "How Powerful are Spectral Graph Neural Networks",
        "abstract": "Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.",
        "summary": "Here's a focused summary of the paper \\cite{wang2022u2l} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The expressive power of Spectral Graph Neural Networks (GNNs) is largely under-researched, despite their widespread use. Specifically, it's unclear whether nonlinearities or Multi-Layer Perceptrons (MLPs) are truly necessary for spectral GNNs to achieve high expressiveness. Additionally, existing spectral GNNs differ mainly in their choice of polynomial bases for spectral filters, but a systematic analysis of these differences and their impact on performance is lacking.\n    *   **Importance & Challenge**: GNNs achieve state-of-the-art performance in graph representation learning. Understanding the theoretical limits and conditions for universality of spectral GNNs is crucial for designing more powerful and efficient models. The challenge lies in formally proving universality conditions and connecting them to established concepts like Graph Isomorphism (GI) testing, which is typically used for spatial GNNs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions itself against existing spectral GNNs (e.g., ChebyNet, GPRGNN, BernNet) by focusing on their underlying polynomial filter mechanisms and questioning the necessity of nonlinearity. It also contrasts with prior work on removing nonlinearity from GNNs (e.g., APPNP, GBP), which primarily focused on scalability with restricted filters, whereas \\cite{wang2022u2l} analyzes expressive power with arbitrary polynomial filters.\n    *   **Limitations of Previous Solutions**: Previous works on GNN expressivity often focused on spatial GNNs using the Weisfeiler-Lehman (WL) test. There was a gap in connecting spectral GNN expressivity (in terms of universal approximation) to GI testing. Furthermore, while many spectral GNNs use polynomial filters, the theoretical implications of different polynomial bases on optimization and empirical performance, despite having the same expressive power, were not systematically explained.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper theoretically analyzes the expressive power of *linear* spectral GNNs (i.e., without nonlinearity or MLPs, formulated as `Z = g(^L)XW`). It proves conditions under which these linear models can achieve universal approximation. It then connects these universality conditions to Graph Isomorphism (GI) testing, bridging the analysis of spectral and spatial GNNs. Finally, it investigates the optimization properties of different polynomial bases for spectral filters.\n    *   **Novelty**:\n        1.  **Universality of Linear Spectral GNNs**: Proves that linear spectral GNNs can produce arbitrary graph signals under specific, mild conditions, demonstrating that nonlinearity is not strictly necessary for high expressiveness.\n        2.  **Connection to Graph Isomorphism**: Establishes a novel link between the universality conditions of spectral GNNs and the discriminative power of the 1-WL test, showing how these conditions enable 1-WL to differentiate non-isomorphic nodes and constrain graph symmetry.\n        3.  **Optimization Analysis of Filter Bases**: Provides an optimization perspective on why different polynomial bases, despite having the same expressive power, lead to varying empirical performance, suggesting that orthogonal bases weighted by graph signal density can maximize convergence speed.\n        4.  **JacobiConv**: Introduces a novel spectral GNN, JacobiConv, which leverages these insights by using Jacobi polynomials as an orthogonal and flexible basis, and explicitly deserts nonlinearity.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insights**:\n        *   Proof that linear spectral GNNs are universal for one-dimensional predictions under two conditions: 1) no multiple eigenvalues of the graph Laplacian, and 2) no missing frequency components in node features (Theorem 4.1).\n        *   Formal connection between these universality conditions and the 1-WL test, showing that these conditions enable 1-WL to differentiate all non-isomorphic nodes and imply a lack of graph automorphisms (Corollary 4.4, Theorem 4.5, Theorem 4.6).\n        *   Analysis demonstrating that using an orthogonal basis with a weight function corresponding to the graph signal density can maximize convergence speed during optimization.\n    *   **Novel Algorithm/System Design**:\n        *   **JacobiConv**: A new spectral GNN model that foregoes nonlinearity, utilizes Jacobi basis for its spectral filters, and is designed to adapt to a wide range of graph signal densities.\n        *   **Polynomial Coefficient Decomposition (PCD)**: A novel technique proposed to improve the optimization of filter coefficients within JacobiConv.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of JacobiConv's expressive power to approximate filter functions on synthetic datasets.\n        *   Performance comparison of JacobiConv against state-of-the-art spectral GNN baselines on ten real-world datasets for node classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   On synthetic datasets, JacobiConv achieved the lowest loss in learning target filter functions, demonstrating its superior approximation capability.\n        *   On ten real-world datasets, JacobiConv consistently outperformed all baselines, achieving performance improvements of up to 12%.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The universality proofs for linear GNNs rely on specific conditions: one-dimensional prediction, no multiple eigenvalues of the graph Laplacian, and no missing frequency components in node features. While the paper empirically shows these conditions are largely met in real-world datasets (e.g., less than 1% multiple eigenvalues, no missing components), they are theoretical constraints. The analysis of linear GNNs with limited polynomial degrees is discussed in the appendix.\n    *   **Scope of Applicability**: The analysis primarily focuses on node property prediction tasks on fixed graphs with fixed node features. While the insights on filter design and optimization are general, the direct universality proofs are specific to these settings.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wang2022u2l} significantly advances the theoretical understanding of spectral GNNs by formally proving their expressive power without nonlinearity and establishing a crucial link to graph isomorphism testing. This challenges the common assumption that nonlinearity is indispensable for GNN expressiveness.\n    *   **Potential Impact on Future Research**: The findings motivate the design of more efficient and powerful spectral GNNs by focusing on optimal filter basis choices and potentially simplifying architectures by removing unnecessary nonlinearities. The connection between spectral properties and GI testing opens new avenues for unified expressivity analysis across different GNN paradigms. The proposed JacobiConv and PCD technique offer practical improvements for spectral GNN design.",
        "year": 2022,
        "citation_key": "wang2022u2l"
      },
      {
        "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
        "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.",
        "summary": "Here's a focused summary of the paper \"Graph Neural Networks for Link Prediction with Subgraph Sketching\" \\cite{chamberlain2022fym} for a literature review:\n\n---\n\n*   **CITATION**: \\cite{chamberlain2022fym}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs), particularly Message Passing GNNs (MPNNs), perform poorly on Link Prediction (LP) tasks compared to simple heuristics. State-of-the-art subgraph-based GNNs (SGNNs) achieve high performance but suffer from severe inefficiency due to expensive subgraph construction, irregular subgraph batching, and high inference costs.\n    *   **Importance and Challenge**: LP is crucial for applications like recommender systems, drug discovery, and knowledge graph construction. The challenge lies in developing GNNs that are both expressive enough to capture complex link patterns (like triangle counts and distinguishing automorphic nodes) and computationally efficient and scalable for large graphs, overcoming the limitations of both MPNNs and SGNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and critically analyzes Subgraph GNNs (SGNNs) \\cite{chamberlain2022fym}, which convert LP into binary subgraph classification and augment node features with structural information (e.g., Zero-One, Double Radius Node Labeling (DRNL), Distance Encoding (DE)). It also contrasts with traditional MPNNs, which are limited by their equivalence to the Weisfeiler-Leman (WL) test.\n    *   **Limitations of Previous Solutions**:\n        *   **MPNNs**: Provably incapable of counting triangles or distinguishing automorphic nodes (nodes with identical structural roles), leading to poor LP performance \\cite{chamberlain2022fym}.\n        *   **SGNNs**: While achieving state-of-the-art accuracy, they are highly inefficient. Subgraph construction is expensive (O(deg^k) or O(|E|)), batching irregular subgraphs is inefficient on GPUs, and inference costs are nearly as high as training, precluding scalability \\cite{chamberlain2022fym}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes two novel full-graph GNN models: ELPH (Efficient Link Prediction with Hashing) and BUDDY.\n        *   **ELPH**: A full-graph MPNN that approximates the key components of SGNNs (specifically, structure feature counts like DRNL/DE labels) by passing \"subgraph sketches\" as messages between nodes. These sketches are compact representations of node neighborhoods, estimated using HyperLogLog and MinHashing techniques to approximate set intersections and cardinalities \\cite{chamberlain2022fym}. This avoids explicit subgraph construction.\n        *   **BUDDY**: An extension of ELPH designed for scalability when data exceeds GPU memory. It precomputes the subgraph sketches and node features, allowing for highly scalable training and inference without sacrificing predictive performance \\cite{chamberlain2022fym}.\n    *   **Novelty/Difference**:\n        *   **Subgraph Sketching**: Instead of constructing explicit subgraphs for each link, \\cite{chamberlain2022fym} introduces the novel idea of summarizing crucial subgraph properties (like counts of specific distance-based node labels) into compact, node-wise \"sketches.\" These sketches are then propagated via message passing, effectively embedding subgraph-level information into node representations.\n        *   **Full-Graph GNN for SGNN Expressivity**: ELPH achieves the expressive power of SGNNs (solving the automorphic node problem and enabling triangle counting) within a full-graph GNN framework, making it significantly faster and more efficient than SGNNs \\cite{chamberlain2022fym}.\n        *   **Scalability for Large Graphs**: BUDDY addresses the common GNN limitation of requiring data to fit in GPU memory by precomputing features, enabling application to very large datasets.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **ELPH (Efficient Link Prediction with Hashing)**: A full-graph MPNN that uses node-wise subgraph sketches (based on HyperLogLog and MinHashing) to approximate structure feature counts, enabling SGNN-level expressivity without explicit subgraph construction \\cite{chamberlain2022fym}.\n        *   **BUDDY**: A highly scalable model that precomputes these subgraph sketches and node features, addressing memory limitations for large datasets \\cite{chamberlain2022fym}.\n    *   **System Design/Architectural Innovations**: The concept of \"subgraph sketching\" as a message-passing mechanism to encode complex structural information (like DE/DRNL counts) into node features, effectively bridging the gap between the expressivity of SGNNs and the efficiency of full-graph MPNNs \\cite{chamberlain2022fym}.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of SGNN components revealing that structure features are crucial, their propagation via GNNs is often sub-optimal, and edge-level readout functions are superior \\cite{chamberlain2022fym}.\n        *   Proof that ELPH is strictly more expressive than standard MPNNs for LP and effectively solves the automorphic node problem \\cite{chamberlain2022fym}.\n    *   **Open-Source Library**: Provision of an open-source PyTorch library for (sub)graph sketching, facilitating further research and application \\cite{chamberlain2022fym}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The models (ELPH and BUDDY) were evaluated on standard LP benchmarks, including Cora, Citeseer, and Pubmed datasets. Ablation studies were performed on SGNN components to understand the importance of structure features, propagation mechanisms, and readout functions \\cite{chamberlain2022fym}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: ELPH and BUDDY consistently outperform existing SGNN models on many standard LP benchmarks \\cite{chamberlain2022fym}.\n        *   **Speed/Efficiency**: ELPH is orders of magnitude faster than SGNNs. BUDDY is even faster than ELPH and highly scalable, demonstrating superior efficiency while maintaining or improving predictive performance \\cite{chamberlain2022fym}.\n        *   **Ablation Findings**: The analysis showed that structure features significantly improve performance, that an MLP on structure feature counts can outperform SGNN propagation, and that edge-level readout functions are more effective than graph pooling \\cite{chamberlain2022fym}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **ELPH**: Shares the common GNN limitation that it is only efficient when the entire dataset fits into GPU memory \\cite{chamberlain2022fym}.\n        *   **Sketching Accuracy**: The accuracy of structure feature approximation depends on the parameters of HyperLogLog and MinHashing, introducing a trade-off between speed and precision \\cite{chamberlain2022fym}.\n    *   **Scope of Applicability**: The methods are primarily designed for link prediction tasks on static graphs. While BUDDY addresses scalability for large graphs, the core sketching mechanism is tailored to approximating specific structural features relevant to LP.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chamberlain2022fym} significantly advances the state-of-the-art in GNN-based link prediction by resolving the long-standing trade-off between expressivity (achieved by SGNNs) and efficiency/scalability (lacking in SGNNs). It demonstrates that complex structural information can be efficiently encoded and propagated without explicit subgraph construction.\n    *   **Potential Impact on Future Research**:\n        *   **Efficient Expressive GNNs**: The subgraph sketching paradigm could inspire new designs for expressive GNNs that overcome WL limitations without incurring high computational costs, potentially extending to other graph tasks beyond LP.\n        *   **Scalable GNN Architectures**: BUDDY's precomputation strategy offers a blueprint for developing highly scalable GNNs for very large graphs, which is critical for real-world applications.\n        *   **Theoretical Understanding**: The analysis of SGNN components provides valuable insights into what makes GNNs effective for LP, guiding future research in designing more targeted and efficient models.",
        "year": 2022,
        "citation_key": "chamberlain2022fym"
      },
      {
        "title": "Universal Prompt Tuning for Graph Neural Networks",
        "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n---\n\n### Focused Summary for Literature Review: Universal Prompt Tuning for Graph Neural Networks\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: Adapting pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks is challenging due to the wide variety of GNN pre-training strategies. Existing prompt-based tuning methods for GNNs are specialized for particular pre-training tasks (e.g., edge prediction) and lack universality.\n*   **Importance & Challenge**:\n    *   GNNs face challenges like scarcity of labeled data and low out-of-distribution generalization, which pre-trained GNNs aim to mitigate.\n    *   The standard \"pre-train, fine-tune\" paradigm for GNNs suffers from objective misalignment between pre-training and downstream tasks, and catastrophic forgetting, especially in few-shot scenarios.\n    *   Prompt tuning, successful in NLP and CV, offers an alternative by modifying input data instead of model parameters. However, applying it to GNNs is difficult because there's no unified pre-training task (unlike masked language modeling in NLP), making it hard to design a universal prompting function.\n    *   Prior GNN prompt tuning efforts are intuition-based, specialized, and lack theoretical guarantees for effectiveness.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches**:\n    *   `\\cite{fang2022tjj}` builds upon the concept of pre-trained GNN models and the prompt tuning paradigm from NLP and CV.\n    *   It contrasts with traditional fine-tuning, which updates the entire pre-trained GNN model.\n    *   It relates to existing specialized prompt-based tuning methods for GNNs (e.g., \\cite{sun2022graphprompt, liu2023graphprompt}) that introduce virtual nodes or links for models pre-trained with edge prediction.\n*   **Limitations of previous solutions**:\n    *   **Fine-tuning**: Prone to catastrophic forgetting and sub-optimal performance due to objective misalignment, particularly with limited downstream data.\n    *   **Specialized GNN prompt tuning**: These methods are highly specific to certain pre-training strategies (e.g., edge prediction) and cannot be applied to GNNs pre-trained with other common strategies like attribute masking or contrastive learning. They are also often intuitively designed without theoretical guarantees.\n*   **Positioning**: `\\cite{fang2022tjj}` introduces the *first* universal prompt-based tuning method for pre-trained GNN models, designed to be applicable irrespective of the underlying pre-training strategy, thereby addressing the limitations of specialized approaches.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: `\\cite{fang2022tjj}` proposes **Graph Prompt Feature (GPF)**, a universal prompt-based tuning method that operates by modifying the input graph's *feature space*.\n    *   **GPF**: A learnable vector `p` (of dimension `F`, matching node feature dimensionality) is *added to all node features* `xi` in the input graph `X` to generate prompted features `X* = {x1+p, ..., xN+p}`. The pre-trained GNN model parameters are frozen, and only `p` and a learnable projection head `θ` are optimized for the downstream task.\n    *   **GPF-plus**: A theoretically stronger variant of GPF that incorporates *different* learnable prompted features for *different nodes* in the graph, allowing for more nuanced and adaptive input modifications.\n*   **Novelty/Difference**:\n    *   **Universality**: GPF is designed to be compatible with *any* pre-trained GNN model and *any* pre-training strategy, eliminating the need for strategy-specific manual prompting function design.\n    *   **Feature Space Manipulation**: Unlike methods that modify graph structure (e.g., adding virtual nodes/edges), GPF directly adapts the input node features, drawing inspiration from pixel-level visual prompts in computer vision.\n    *   **Theoretical Foundation**: The paper provides rigorous derivations to demonstrate that GPF can theoretically achieve an equivalent effect to *any* form of prompting function, offering a strong theoretical basis for its universality and effectiveness, which was lacking in prior intuitive designs.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The introduction of **Graph Prompt Feature (GPF)** and its variant **GPF-plus**, representing the first universal prompt-based tuning methods for pre-trained GNNs.\n    *   A novel paradigm for GNN prompt tuning by operating on the *input graph's feature space* rather than altering graph structure or fine-tuning model parameters.\n*   **Theoretical Insights/Analysis**:\n    *   Rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to *any* prompting function, thereby guaranteeing its broad applicability.\n    *   Theoretical proofs that GPF and GPF-plus are not weaker than full fine-tuning and can, in some cases, yield *superior* theoretical tuning results.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**:\n    *   Extensive experiments were conducted across various pre-training strategies (e.g., those involving edge prediction and attribute masking) and GNN architectures.\n    *   Evaluations were performed in both **full-shot** (sufficient labeled data) and **few-shot** (limited labeled data) scenarios.\n    *   Performance was compared against traditional **fine-tuning** and existing **specialized prompt-based tuning methods** (when applicable to their specialized pre-training strategies).\n*   **Key Performance Metrics & Comparison Results**:\n    *   GPF and GPF-plus consistently outperformed fine-tuning, despite utilizing significantly fewer tunable parameters.\n    *   **Average performance improvement over fine-tuning**: Approximately **1.4%** in full-shot scenarios and a more substantial **3.2%** in few-shot scenarios, highlighting its effectiveness in data-scarce environments.\n    *   **Superiority over specialized prompts**: GPF and GPF-plus significantly outperformed existing specialized prompt-based tuning methods even when applied to models pre-trained with the specific strategies those specialized methods were designed for (e.g., edge prediction).\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus and detailed validation in the main paper are on graph classification tasks, with extensions to node-wise tasks (node classification, link prediction) mentioned as being in the appendix (not provided in the excerpt).\n    *   While GPF-plus is described as theoretically stronger, the specific implementation details of how \"different prompted features for different nodes\" are learned and applied are not fully elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   Universally applicable to *any* pre-trained GNN model, regardless of its specific pre-training strategy.\n    *   Applicable across various GNN architectures.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   `\\cite{fang2022tjj}` makes a significant advancement by introducing the first *universal* prompt-based tuning method for GNNs, effectively addressing the challenge posed by diverse pre-training strategies.\n    *   It provides a more efficient, effective, and robust alternative to fine-tuning, particularly beneficial in data-limited settings, by adapting the input space while keeping the pre-trained model frozen.\n    *   The work establishes a strong theoretical foundation for GNN prompt tuning, moving beyond heuristic designs.\n*   **Potential Impact on Future Research**:\n    *   This work opens new research directions for developing universal adaptation strategies for GNNs and potentially other complex data structures with varied pre-training paradigms.\n    *   It could lead to the development of more generalizable and easily adaptable pre-trained GNN models, accelerating their deployment in real-world applications.\n    *   It encourages further theoretical and empirical investigation into the mechanisms and optimal designs of prompt tuning for graph-structured data.",
        "year": 2022,
        "citation_key": "fang2022tjj"
      },
      {
        "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
        "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
        "summary": "Here's a focused summary of the paper \\cite{longa202399q} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** While Graph Neural Networks (GNNs) excel at learning on static graph-structured data, many real-world systems are dynamic, with graphs and their attributes changing over time. Traditional graph models and static GNNs are ill-equipped to capture this temporal evolution \\cite{longa202399q}.\n    *   **Importance and Challenge:** Extending GNN capabilities to temporal graphs (Temporal GNNs or TGNNs) is a promising research area. However, the field lacked a comprehensive, systematized overview, a rigorous formalization of learning settings and tasks, and a unified taxonomy for existing TGNN approaches \\cite{longa202399q}. Existing surveys were either too general, too specific, or did not provide in-depth coverage of TGNNs \\cite{longa202399q}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a survey that specifically focuses on GNN-based models for temporal graphs, acknowledging other approaches like matrix factorization, temporal motif-based methods, random walks (e.g., DyANE \\cite{longa202399q}), temporal point processes (e.g., DyRep \\cite{longa202399q}), NMF, and other deep learning techniques (e.g., DynGem \\cite{longa202399q}, TRRN \\cite{longa202399q}, STAR \\cite{longa202399q}, TSNet \\cite{longa202399q}) that also address temporal graphs \\cite{longa202399q}.\n    *   **Limitations of Previous Solutions (Surveys):** Previous surveys either discussed general temporal graph learning techniques with only brief mentions of GNNs, focused on narrow topics like temporal link prediction or graph generation, or provided GNN overviews without deep coverage of temporal aspects \\cite{longa202399q}. This work aims to fill the gap by providing the first comprehensive systematization of GNN-based methods for temporal graphs \\cite{longa202399q}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive survey that systematically organizes and formalizes the field of Temporal Graph Neural Networks (TGNNs). It does not propose a new TGNN model but rather provides a foundational framework for understanding existing ones \\cite{longa202399q}.\n    *   **Novelty/Difference:** The core innovation lies in its structured approach to the literature:\n        *   It introduces a rigorous formalization of learning settings and tasks specific to temporal graphs \\cite{longa202399q}.\n        *   It proposes a novel taxonomy that categorizes existing TGNN approaches based on how the temporal aspect is represented (Snapshot-based Temporal Graphs - STG, or Event-based Temporal Graphs - ETG) and processed \\cite{longa202399q}.\n        *   It defines distinct learning settings: transductive vs. inductive, and past vs. future prediction, for temporal graph tasks \\cite{longa202399q}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A coherent formalization of different learning settings (transductive, inductive, past prediction, future prediction) and tasks (e.g., temporal node classification, link prediction) for temporal graphs, unifying disparate definitions found in the literature \\cite{longa202399q}.\n        *   A comprehensive taxonomy that groups existing TGNN methods based on their temporal representation strategy (snapshot-based vs. event-based) and the mechanism used to incorporate time \\cite{longa202399q}.\n        *   Formal definitions for various types of temporal graphs, including Static Graphs (SG), Temporal Graphs (TG), Discrete Time Temporal Graphs (DTTG), Snapshot-based Temporal Graphs (STG), and Event-based Temporal Graphs (ETG) \\cite{longa202399q}.\n    *   **Theoretical Insights or Analysis:** The paper provides a structured theoretical framework for the field, clarifying fundamental concepts, learning objectives, and evaluation contexts for TGNNs \\cite{longa202399q}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{longa202399q} does not present new experimental results from its own proposed models. Instead, it reviews and categorizes the experimental validations performed by the individual TGNN papers it surveys.\n    *   **Key Performance Metrics and Comparison Results:** The paper notes that TGNNs have achieved state-of-the-art results on tasks such as temporal link prediction, node classification, and edge classification \\cite{longa202399q}. It sets the context for understanding how these models are typically evaluated, but does not provide specific metrics or comparative results within the survey itself.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):** The survey explicitly focuses on methods that leverage and adapt the GNN framework to temporal graphs, acknowledging but not deeply exploring other non-GNN approaches for temporal graph learning \\cite{longa202399q}.\n    *   **Limitations of Current TGNN Methods (as identified by the survey):** The paper concludes with a discussion of the most relevant open challenges and limitations of current TGNN methods, from both research and application perspectives, though these specific challenges are not detailed in the provided abstract/introduction \\cite{longa202399q}.\n    *   **Scope of Applicability:** TGNNs are applicable to a wide range of dynamic systems, including recommendation systems, social network analysis, transportation systems, face-to-face interactions, human mobility, and epidemic modeling \\cite{longa202399q}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematized overview of Temporal Graph Neural Networks, filling a critical gap in the existing literature \\cite{longa202399q}.\n    *   **Potential Impact on Future Research:**\n        *   It offers a unified formalization and common language, which is crucial for facilitating comparison, understanding, and the development of new TGNN models \\cite{longa202399q}.\n        *   By highlighting limitations and open challenges, it provides a clear roadmap for future research directions in the field \\cite{longa202399q}.\n        *   The proposed taxonomy and formal definitions will serve as a foundational reference for researchers and practitioners working with dynamic graph data \\cite{longa202399q}.",
        "year": 2023,
        "citation_key": "longa202399q"
      },
      {
        "title": "Graph neural networks for materials science and chemistry",
        "abstract": "Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs. Graph neural networks are machine learning models that directly access the structural representation of molecules and materials. This Review discusses state-of-the-art architectures and applications of graph neural networks in materials science and chemistry, indicating a possible road-map for their further development.",
        "summary": "This paper, a review article by \\cite{reiser2022b08}, provides a comprehensive overview of Graph Neural Networks (GNNs) in chemistry and materials science.\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively leveraging machine learning for tasks in chemistry and materials science, such as predicting material properties, accelerating simulations, designing new materials, and predicting synthesis routes. Traditional machine learning methods often rely on hand-crafted feature representations, which can be labor-intensive and may not fully capture the complex structural and geometric information inherent in molecules and materials.\n*   **Importance and Challenge:** This problem is crucial because it can significantly accelerate the materials development cycle, from discovery to synthesis. It is challenging due to the irregular, graph-structured nature of molecular and material data (atoms and bonds), which traditional deep learning models (like CNNs for grid-like data) are not inherently designed to handle. Capturing complete atomic-level representations, incorporating physical laws, and dealing with various scales (from atomic to larger phenomena like doping) are key difficulties.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** This work positions GNNs as a powerful alternative and advancement over conventional machine learning models (e.g., decision tree ensembles, densely connected neural networks, random forests, Gaussian process regression) that typically require predefined feature representations (e.g., compositional or fixed-sized vectors, or descriptors like ACSF, SOAP, MBTR). It also relates GNNs to Convolutional Neural Networks (CNNs) by interpreting them as a generalization of CNNs to irregular graph structures.\n*   **Limitations of Previous Solutions:**\n    *   **Hand-crafted Features:** Previous methods often rely on hand-crafted feature representations, which can be incomplete, less flexible, and may not capture all relevant information, especially geometric dependencies.\n    *   **Lack of End-to-End Learning:** Many traditional approaches separate feature engineering from model training, limiting end-to-end optimization.\n    *   **Inability to Directly Process Graph Data:** Conventional deep learning models are not naturally suited for graph-structured data, requiring conversion or flattening that can lose topological information.\n    *   **Data Requirements:** While GNNs can require significant data, the paper notes that traditional methods with hand-crafted features often struggle to learn complex relationships without extensive feature engineering.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper focuses on **Message Passing Graph Neural Networks (MPNNs)** as the unifying framework for most GNNs in chemistry and materials science \\cite{reiser2022b08}.\n    *   MPNNs operate by iteratively propagating information (messages) between neighboring nodes (atoms) and updating node embeddings based on these messages.\n    *   This process is repeated for a fixed number of steps (K), allowing information to travel within the K-hop neighborhood.\n    *   Finally, a graph-level embedding is obtained by pooling all node embeddings, which is then used for prediction tasks (regression or classification).\n    *   The core operations involve learnable functions for message generation ($M_t$) and node updates ($U_t$), and a permutation-invariant readout function ($R$).\n*   **Novelty/Difference:**\n    *   **Direct Graph Processing:** GNNs directly operate on the natural graph representation of molecules and materials (atoms as nodes, bonds as edges), eliminating the need for extensive manual feature engineering.\n    *   **Representation Learning:** They learn informative molecular/material representations end-to-end, allowing the model to discover relevant features from the raw structural data.\n    *   **Incorporation of Geometric Information:** GNNs can flexibly incorporate geometric information (distances, angles) as additional edge or node features, or through specialized architectures, which is crucial for quantum-mechanical properties.\n    *   **Symmetry Awareness:** The approach emphasizes the importance of incorporating physical symmetries (rotational, translational, permutation, periodicity for crystals) into the model's representation or architecture, leading to more data-efficient and robust models.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   **Formalization of MPNNs:** The paper highlights the MPNN framework \\cite{reiser2022b08} as a foundational and widely applicable scheme for GNNs in this domain, detailing its message passing, node update, and readout phases.\n    *   **Integration of Geometric Data:** Discussion of how GNNs integrate geometric information (distances, bond/dihedral angles) as features, often expanded into Gaussian-like, radial, or spherical Fourier-Bessel functions, moving beyond purely topological graphs.\n    *   **Symmetry-Equivariant Representations:** Emphasis on the development of equivariant representations \\cite{reiser2022b08} that are invariant or equivariant under translation, rotation, and permutation operations, crucial for predicting tensorial properties and reducing data requirements.\n    *   **Periodic Extensions:** Introduction of periodic extensions of crystal graphs \\cite{reiser2022b08} to handle solid crystals and periodic structures, incorporating periodicity and space group symmetries.\n*   **System Design or Architectural Innovations (as discussed in the review):**\n    *   **Edge Updates and Skip Connections:** Mention of extensions like D-MPNN \\cite{reiser2022b08} (directed edge embeddings, message passing between edges) and the use of skip connections \\cite{reiser2022b08} to alleviate issues like over-smoothing.\n    *   **Attention Mechanisms:** Integration of masked self-attention layers, as seen in Graph Attention Networks (GATs) \\cite{reiser2022b08} and AttentiveFingerprint models \\cite{reiser2022b08}, to weigh the importance of neighboring nodes.\n    *   **Graph Pooling/Coarsening:** Discussion of algorithms \\cite{reiser2022b08} to reduce input representation and condense structural information for larger molecules.\n*   **Theoretical Insights or Analysis:**\n    *   **Generalization of CNNs:** GNNs are presented as a generalization of CNNs to irregular graph structures, providing a theoretical link to established deep learning paradigms.\n    *   **Addressing GNN Limitations:** Acknowledgment of open research questions regarding GNNs' limited expressive performance \\cite{reiser2022b08} (e.g., comparison to Weisfeiler-Lehman hierarchy \\cite{reiser2022b08}), over-squashing \\cite{reiser2022b08}, and over-smoothing \\cite{reiser2022b08}, and discussion of proposed solutions like hypergraph representations \\cite{reiser2022b08} and higher-order graph networks \\cite{reiser2022b08}.\n\n### 5. Experimental Validation\n*   **Experiments Conducted (as reported in the review):** The paper itself is a review, so it doesn't present new experimental results. However, it summarizes the empirical validation of GNNs by referencing:\n    *   **Benchmark Datasets:** A wide range of benchmark datasets are listed for both molecules (e.g., QM7, QM9, PDBBind, Tox21) and crystals (e.g., Materials Project (MP), Open Quantum Materials Database (OQMD), Open Catalyst Project (OC20)) \\cite{reiser2022b08}. These datasets are used for supervised tasks like regression (e.g., quantum calculations, protein binding affinity, formation energy) and classification (e.g., toxicity, blood-brain barrier penetration).\n    *   **Performance Comparison:** The review highlights that GNNs have \"outperformed conventional machine learning models in predicting molecular properties throughout the last years\" \\cite{reiser2022b08}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **QM9 Benchmark:** Figure 1c in \\cite{reiser2022b08} specifically shows the Mean Absolute Error (MAE) for the prediction of internal, HOMO, and LUMO energies on the QM9 dataset for various GNN models since 2017. This figure visually demonstrates the continuous improvement in prediction accuracy achieved by different GNN architectures over time for quantum properties.\n    *   **General Superiority:** The text generally states that GNNs show \"a systematic advantage over traditional feature-based methods\" \\cite{reiser2022b08} and have \"outperformed conventional machine learning models in predicting molecular properties\" \\cite{reiser2022b08}.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions:**\n    *   **Data Requirements:** GNNs often come at the cost of higher data requirements, potentially limiting their applicability to scenarios where large amounts of data are available \\cite{reiser2022b08}.\n    *   **Expressive Power:** A main open research question revolves around the limited expressive performance of GNNs for specific tasks and how they compare with the Weisfeiler-Lehman hierarchy for graph isomorphism testing \\cite{reiser2022b08}.\n    *   **Over-smoothing and Over-squashing:** Challenges like over-smoothing (indistinguishable representations of neighboring nodes) and over-squashing (distortion of information from long-range dependencies) limit the depth and effectiveness of message passing \\cite{reiser2022b08}.\n    *   **Training Instability:** Training instability \\cite{reiser2022b08} is also an ongoing research subject.\n*   **Scope of Applicability:** The review focuses specifically on applications in chemistry and materials science, primarily for predicting molecular and material properties, accelerating simulations, and aiding in material design. While GNNs have broader applications, this paper's scope is confined to these domains.\n\n### 7. Technical Significance\n*   **Advancement of State-of-the-Art:** This review highlights how GNNs significantly advance the technical state-of-the-art by providing an end-to-end, representation-learning approach that directly processes graph-structured data. This capability allows for more accurate and flexible modeling of complex chemical and material systems compared to traditional methods relying on hand-crafted features. The continuous improvement shown on benchmarks like QM9 \\cite{reiser2022b08} underscores their superior predictive power.\n*   **Potential Impact on Future Research:**\n    *   **Accelerated Discovery:** GNNs have the potential to revolutionize virtual materials design and accelerate the discovery of new materials and drugs by enabling more efficient property prediction and inverse design.\n    *   **Integration of Physics:** The flexibility to incorporate physical laws and symmetries into GNN architectures opens avenues for developing more physically informed and robust models.\n    *   **Addressing Current Limitations:** Ongoing research into improving expressive power, mitigating over-smoothing/over-squashing, and enhancing data efficiency will further broaden GNN applicability.\n    *   **New Architectures:** The field is ripe for developing novel GNN architectures that can better handle complex geometries, periodic boundary conditions, and multi-scale phenomena in materials science.",
        "year": 2022,
        "citation_key": "reiser2022b08"
      },
      {
        "title": "Cooperative Graph Neural Networks",
        "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.",
        "summary": "Here's a focused summary of the paper \"Cooperative Graph Neural Networks\" by Finkelshtein et al. for a literature review:\n\n### Cooperative Graph Neural Networks \\cite{finkelshtein202301z}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Graph Neural Networks (GNNs) rely on a fixed, synchronous message-passing paradigm where every node \"listens and broadcasts\" to all its neighbors at every layer. This rigid information flow leads to several well-known limitations.\n    *   **Importance and Challenge**:\n        *   **Information Bottlenecks**: Standard message passing struggles with long-range dependencies, requiring many layers which can lead to exponential growth of receptive fields and subsequent information loss (over-squashing) \\cite{finkelshtein202301z}.\n        *   **Over-smoothing**: Node features can become increasingly similar with more layers, hindering discriminative power \\cite{finkelshtein202301z}.\n        *   **Fixed & Synchronous Updates**: The static and synchronous nature of message passing prevents nodes from reacting individually to messages or adapting information flow based on their state or the task, which is not always optimal \\cite{finkelshtein202301z}.\n        *   **Limited Expressive Power**: Most GNNs are upper-bounded by the 1-Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs \\cite{finkelshtein202301z}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work generalizes the widely adopted message-passing paradigm (e.g., GCN, GIN, GAT, GraphSAGE) \\cite{finkelshtein202301z}.\n    *   **Limitations of Previous Solutions**:\n        *   Existing approaches to improve expressivity often involve higher-order structures, subgraph counting, or unique identifiers, which can increase complexity \\cite{finkelshtein202301z}.\n        *   Solutions for long-range dependencies typically involve graph rewiring or direct connections between distant nodes, which modify the graph topology globally or based on fixed rules \\cite{finkelshtein202301z}.\n        *   Some works explore dynamic layer counts per node or learned topologies, but these are often fixed across layers or focus on different objectives than dynamic, node-specific communication strategies \\cite{finkelshtein202301z}.\n        *   Attention mechanisms (e.g., GAT) weigh neighbor contributions but do not fundamentally alter the communication strategy or address limitations like counting node degrees \\cite{finkelshtein202301z}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Cooperative Graph Neural Networks (CO-GNNs), a novel framework where each node is viewed as a \"player\" that can dynamically choose one of four actions at every layer:\n        *   **STANDARD (S)**: Broadcast to neighbors that listen and listen to neighbors that broadcast (recovers standard message passing).\n        *   **LISTEN (L)**: Listen to neighbors that broadcast.\n        *   **BROADCAST (B)**: Broadcast to neighbors that listen.\n        *   **ISOLATE (I)**: Neither listen nor broadcast.\n    *   **Novelty/Difference**:\n        *   **Two-Stage Process**: CO-GNNs comprise two jointly trained \"cooperating\" message-passing neural networks:\n            *   An **action network (π)**: Predicts a probability distribution over the four actions for each node based on its current state and neighbors.\n            *   An **environment network (η)**: Updates node representations based on the sampled actions, effectively operating on a dynamically induced computational graph \\cite{finkelshtein202301z}.\n        *   **Dynamic & Asynchronous Message Passing**: Unlike standard GNNs, CO-GNNs allow nodes to determine their own communication strategy (listen, broadcast, or both) based on their state, leading to a flexible, dynamic, and asynchronous information flow \\cite{finkelshtein202301z}.\n        *   **Directed Information Flow**: The interplay of actions can induce directed edges in the computational graph at each layer, allowing for more nuanced information propagation than fixed undirected edges \\cite{finkelshtein202301z}.\n        *   **Differentiable Action Sampling**: Utilizes the Straight-through Gumbel-softmax estimator to enable gradient-based optimization for the categorical action choices \\cite{finkelshtein202301z}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of the CO-GNN framework with its unique two-network (action and environment) architecture and dynamic, node-specific action selection mechanism \\cite{finkelshtein202301z}.\n    *   **System Design/Architectural Innovations**: A message-passing scheme that decouples the input graph from the computational graph, allowing for layer-wise, node-specific \"rewiring\" of information flow \\cite{finkelshtein202301z}.\n    *   **Theoretical Insights/Analysis**:\n        *   **Enhanced Expressive Power**: Theoretically shown to be more expressive than the 1-WL algorithm (Proposition 5.1), capable of distinguishing non-isomorphic graphs that 1-WL cannot, due to the variance introduced by action sampling \\cite{finkelshtein202301z}.\n        *   **Suitability for Long-Range Tasks**: Adaptive nature makes them better suited for long-range tasks by allowing nodes to selectively propagate information \\cite{finkelshtein202301z}.\n        *   **Conceptual Properties**: Identified as task-specific, directed, dynamic, feature and structure-based, asynchronous, and employing conditional aggregation. It is also orthogonal to attention mechanisms and mitigates over-smoothing \\cite{finkelshtein202301z}.\n        *   **Efficiency**: Demonstrated to be efficient in runtime and parameter-efficient by sharing the action network across layers \\cite{finkelshtein202301z}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Validation on a synthetic task to demonstrate the ability to learn specific communication patterns (e.g., listening only to neighbors with a certain degree) \\cite{finkelshtein202301z}.\n        *   Extensive experiments on real-world datasets for various graph machine learning tasks (details in Section 6.2 and Appendix C.3) \\cite{finkelshtein202301z}.\n        *   Analysis of action trends on homophilic and heterophilic graphs \\cite{finkelshtein202301z}.\n        *   Visualization of actions on a heterophilic graph \\cite{finkelshtein202301z}.\n        *   Ablation studies on the choices of action and environment networks \\cite{finkelshtein202301z}.\n        *   Additional experiments on expressive power (Appendix C.1), long-range tasks (Appendix C.2), and over-smoothing (Appendix C.5) \\cite{finkelshtein202301z}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   CO-GNNs consistently improve performance compared to their baseline models (e.g., SUMGNNs, MEANGNNs, GCN, GIN, GAT) \\cite{finkelshtein202301z}.\n        *   Achieved multiple state-of-the-art results on real-world datasets \\cite{finkelshtein202301z}.\n        *   Empirically validated mitigation of over-smoothing \\cite{finkelshtein202301z}.\n        *   Demonstrated ability to solve tasks that GAT alone cannot, when GAT is used as the environment network within a CO-GNN \\cite{finkelshtein202301z}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   While CO-GNNs can mitigate over-smoothing, the optimization becomes increasingly difficult as the number of layers grows very large \\cite{finkelshtein202301z}.\n        *   The framework introduces additional complexity by requiring the training of two networks (action and environment), although it is parameter-efficient \\cite{finkelshtein202301z}.\n    *   **Scope of Applicability**: Applicable to various graph machine learning tasks (node-level, graph-level) and can integrate different base GNN architectures for its action and environment networks \\cite{finkelshtein202301z}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: Introduces a fundamentally new and more flexible message-passing paradigm for GNNs, moving beyond the static and synchronous limitations of traditional approaches \\cite{finkelshtein202301z}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for designing GNNs that can dynamically adapt their computational graph and information flow based on node states and task requirements.\n        *   Provides a robust framework to address long-standing challenges in GNNs such as limited expressive power, over-squashing, and over-smoothing through adaptive communication strategies.\n        *   The concept of \"cooperating\" networks for GNNs could inspire further research into multi-agent or game-theoretic approaches for graph learning \\cite{finkelshtein202301z}.",
        "year": 2023,
        "citation_key": "finkelshtein202301z"
      }
    ],
    "layer3_papers": [
      {
        "title": "Spatio-Spectral Graph Neural Networks",
        "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{geisler2024wli}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Spatial Message Passing Graph Neural Networks (MPGNNs) suffer from a limited \"receptive field\" (typically ℓ-hop neighborhood) and \"over-squashing,\" which severely restricts information exchange between distant nodes. This limits their expressivity and ability to model long-range interactions.\n    *   **Importance and Challenge**: Modeling long-range interactions is crucial for many graph-based tasks, as evidenced by the success of global models like transformers. Over-squashing makes MPGNNs ineffective for problems requiring global information, posing a significant challenge to their applicability in complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of MPGNNs, which have achieved breakthroughs in various domains. It draws inspiration from similar synergistic compositions in molecular point clouds (Kosmala et al., 2023) and sequence models like Mamba (Gu & Dao, 2023) or Hyena (Poli et al., 2023), which offer transformer-like properties with superior scalability.\n    *   **Limitations of Previous Solutions**: MPGNNs are inherently limited by their local message-passing scheme and the over-squashing phenomenon. The design space for spectral GNNs, in contrast to spatial MPGNNs, has been largely unexplored, leaving potential for novel architectural advancements.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Spatio-Spectral Graph Neural Networks (S2GNNs) \\cite{geisler2024wli}, a new modeling paradigm that synergistically combines spatially and spectrally parametrized graph filters.\n        *   S2GNNs utilize a partial eigendecomposition to enable spectral filters that are *spectrally bounded* (operating on a truncated frequency spectrum) but *spatially unbounded*, allowing for global information propagation.\n        *   The architecture can combine spatial and spectral filters additively or sequentially.\n        *   The spectral filter is defined as `Spectral(l)(H(l−1);V,λ) =V [ˆg(l)ϑ(λ)⊙ [V⊤f(l)θ(H(l−1))]]` (Eq. 3), ensuring permutation equivariance.\n    *   **Novelty/Difference**:\n        *   **Synergistic Combination**: The core innovation is the principled combination of local spatial message passing with global spectral filtering, addressing the limitations of each individually.\n        *   **Spectral Domain Neural Network**: The paper proposes the first neural network designed to operate directly in the spectral domain, allowing for data-dependent filtering and channel mixing with negligible computational cost for truncated spectra \\cite{geisler2024wli}.\n        *   **Directed Graph Filters**: S2GNNs generalize spectrally parametrized filters to directed graphs, expanding their applicability.\n        *   **Expressive Positional Encodings**: They introduce stable positional encodings (PEs) derived almost \"for free\" from the partial eigendecomposition, making S2GNNs strictly more expressive than the 1-Weisfeiler-Lehman (WL) test \\cite{geisler2024wli}.\n        *   **Over-squashing Mitigation**: The global nature of spectral filters inherently vanquishes over-squashing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The S2GNN architecture itself, which integrates spatial and spectral filtering for enhanced graph learning \\cite{geisler2024wli}.\n        *   A method for parametrizing spectral filters using Gaussian smearing and linear transformations, designed for stability and expressivity.\n        *   The concept and implementation of a neural network operating directly within the spectral domain.\n        *   Adaptation of spectral filters for directed graphs.\n        *   Novel, cost-effective positional encodings derived from the graph's eigendecomposition.\n    *   **Theoretical Insights or Analysis**:\n        *   **Over-squashing Vanquished**: Theorem 2 formally proves that S2GNNs overcome over-squashing by demonstrating a uniformly lower-bounded Jacobian sensitivity, ensuring effective long-range information exchange.\n        *   **Superior Approximation Bounds**: Theorems 3 and 4 establish strictly tighter approximation-theoretic error bounds for S2GNNs compared to MPGNNs. This is particularly significant for approximating discontinuous or unsmooth ground truth filters, where MPGNNs converge exceedingly slowly.\n        *   **Locality and Spectral Smoothness**: The analysis connects the locality of a filter to the smoothness of its Fourier transform, providing a complementary perspective on MPGNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Empirical verification of MPGNN shortcomings (e.g., over-squashing on \"Clique Path\" graphs) and how S2GNNs overcome them.\n        *   Approximation quality comparison of S2GNNs against purely spatial and spectral filters for a discontinuous target filter.\n        *   Performance evaluation on the challenging peptides-func long-range benchmark tasks (Dwivedi et al., 2022).\n        *   Scalability tests on large graphs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   S2GNNs achieved a new state-of-the-art on the peptides-func benchmark, outperforming MPGNNs and graph transformers with approximately 40% fewer parameters \\cite{geisler2024wli}.\n        *   They demonstrated competitive performance with state-of-the-art sequence models.\n        *   Empirical results on \"Clique Path\" graphs confirmed that spectral filters (and thus S2GNNs) do not exhibit over-squashing (Fig. 5).\n        *   S2GNNs scaled efficiently to millions of nodes on a 40 GB GPU with vanilla full-graph training, exhibiting runtime and space complexity equivalent to MPGNNs \\cite{geisler2024wli}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The efficiency of spectral filters relies on truncating the frequency spectrum (parameter `k` or `λcut`).\n        *   Theoretical guarantees for approximation assume the spectral filter is bandlimited and a universal approximator on a specific interval.\n        *   The default choice for the spectral filter's frequency band is low frequencies, though the method is adaptable.\n    *   **Scope of Applicability**: S2GNNs are designed as general-purpose GNNs, particularly well-suited for tasks requiring effective long-range interactions and where over-squashing is a concern. They are applicable to both undirected and directed graphs.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: S2GNNs introduce a novel and theoretically grounded GNN paradigm that fundamentally addresses the long-standing issues of over-squashing and limited receptive fields in MPGNNs \\cite{geisler2024wli}. By synergistically combining spatial and spectral approaches, they achieve superior expressivity and approximation capabilities, validated by state-of-the-art empirical performance on challenging long-range tasks.\n    *   **Potential Impact on Future Research**: This work opens up a significant new design space for GNN architectures, encouraging further exploration into hybrid spatial-spectral models. The proposed spectral domain neural network, directed graph filters, and \"free\" positional encodings offer concrete avenues for future research, potentially leading to more powerful, efficient, and expressive GNNs for a wider range of applications.",
        "year": 2024,
        "citation_key": "geisler2024wli"
      },
      {
        "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
        "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
        "summary": "Here's a focused summary of the paper \"UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND\" \\cite{kang2024fsk} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Traditional continuous Graph Neural Networks (GNNs) rely on integer-order differential equations, which model instantaneous, local changes (Markovian updates) in node features. This approach struggles to capture long-term dependencies and memory effects inherent in many real-world graph dynamics.\n    *   **Importance & Challenge:** Many real-world graphs (e.g., social, biological, internet networks) exhibit non-local, memory-dependent behaviors and scale-free hierarchical (fractal) structures. Integer-order models are insufficient to accurately describe these complex dynamics, potentially leading to limitations like oversmoothing and suboptimal graph representation learning.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior continuous GNNs (e.g., GRAND, GRAND++, GraphCON, CDE, GREAD) leverage integer-order Ordinary Differential Equations (ODEs) for information propagation, typically using first or second-order derivatives.\n    *   **Limitations of Previous Solutions:** These models are restricted to integer-order derivatives, implying Markovian update mechanisms where feature evolution depends only on the present state. This prevents them from inherently capturing the non-local properties and memory-dependent dynamics crucial for systems with self-similarity or anomalous transport. Other works using fractional calculus either apply it to graph shift operators with integer-order ODEs or to gradient propagation during training, not to the core node feature updating process itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the FRactional-Order graph Neural Dynamical network (FROND) framework \\cite{kang2024fsk}, which replaces the integer-order differential operator in continuous GNNs with the Caputo fractional derivative (DβtX(t) = F(W,X(t)), β > 0).\n    *   **Novelty:**\n        *   **Generalization of Continuous GNNs:** FROND generalizes existing integer-order continuous GNNs by allowing the derivative order β to be any positive real number, effectively subsuming them as special cases when β is an integer.\n        *   **Memory-Dependent Dynamics:** By employing the Caputo fractional derivative, FROND inherently integrates the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics.\n        *   **Non-Markovian Random Walk Interpretation:** For the fractional linear diffusion model (F-GRAND-l), the paper provides an interpretation from a non-Markovian random walk perspective, where the walker's complete path history influences future steps, contrasting with the Markovian walks of traditional models.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Proposed a novel, generalized continuous GNN framework (FROND) that incorporates non-local fractional derivatives, laying the groundwork for a new class of GNNs with learnable memory-dependent feature-updating processes \\cite{kang2024fsk}.\n    *   **System Design/Architectural Innovations:** Demonstrated the seamless compatibility of FROND, showing how it can be integrated to augment the performance of existing integer-order continuous GNNs (e.g., F-GRAND, F-GRAND++, F-GREAD, F-CDE, F-GraphCON).\n    *   **Theoretical Insights/Analysis:**\n        *   Analytically established that the non-Markovian random walk in FROND leads to a slow algebraic rate of convergence to stationarity, which inherently mitigates oversmoothing, unlike the exponentially swift convergence in Markovian integer-order models.\n        *   Suggested a connection between the optimal fractional order β and the inherent \"fractality\" of graph datasets, offering a potential avenue for deeper structural insights.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The FROND framework was validated through extensive experiments comparing fractional adaptations of various established integer-order continuous GNNs (GRAND, GRAND++, GraphCON, CDE, GREAD) on diverse datasets \\cite{kang2024fsk}.\n    *   **Key Performance Metrics & Comparison Results:** The fractional adaptations consistently demonstrated improved performance compared to their integer-order counterparts. Detailed ablation studies were performed to provide insights into the choice of numerical schemes and parameters, underscoring the framework’s potential as an effective extension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on β ∈ (0,1] for initial conditions, though the broader definition for β > 0 is mentioned. The framework relies on numerical FDE solvers, which may introduce computational considerations.\n    *   **Scope of Applicability:** FROND is designed to enhance continuous GNNs for graph representation learning, particularly beneficial for datasets exhibiting non-local, memory-dependent behaviors, or fractal structures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FROND significantly advances the technical state-of-the-art by introducing a principled and generalized approach to incorporate memory-dependent dynamics and non-local interactions into continuous GNNs, overcoming the limitations of purely Markovian integer-order models \\cite{kang2024fsk}.\n    *   **Potential Impact on Future Research:**\n        *   Opens new research directions for designing GNNs that can model more complex, memory-dependent feature-updating processes.\n        *   Provides a robust mechanism for mitigating oversmoothing in GNNs through its algebraic convergence properties.\n        *   Offers a novel tool for exploring the underlying \"fractality\" and self-similarity of graph datasets by optimizing the fractional order β.\n        *   Serves as a powerful and compatible extension to enhance the performance of existing continuous GNN architectures.",
        "year": 2024,
        "citation_key": "kang2024fsk"
      },
      {
        "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
        "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
        "year": 2024,
        "citation_key": "li202444f"
      },
      {
        "title": "A Review of Graph Neural Networks in Epidemic Modeling",
        "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
        "summary": "This paper, `A Review of Graph Neural Networks in Epidemic Modeling` by Liu et al. \\cite{liu20242g6}, provides a comprehensive survey of Graph Neural Networks (GNNs) in the context of epidemic modeling.\n\nHere's a focused summary for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional mechanistic epidemic models suffer from oversimplified or fixed assumptions, leading to sub-optimal predictive power and an inability to efficiently capture complex relational information. Existing data-driven approaches like CNNs and RNNs also fall short in incorporating crucial relational data (e.g., human mobility, geographic connections, contact tracing) essential for accurate epidemic forecasting.\n    *   **Importance & Challenge**: Accurate and timely epidemic modeling is critical for public health decision-making, resource allocation, and effective intervention strategies. The challenge lies in developing models that can effectively integrate and leverage the complex, dynamic relational data inherent in disease transmission networks to provide more precise and generalizable predictions.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Mechanistic Models**: Such as SIR, SEIR, and SIRD, mathematically describe disease transmission but are limited by their reliance on fixed and often oversimplified assumptions.\n        *   **Data-driven Models (CNNs, RNNs)**: While successful in some epidemiological predictive tasks (e.g., forecasting case counts), they often lack the capability to incorporate relational information effectively.\n    *   **Limitations of Previous Solutions**: Both mechanistic and general deep learning models struggle to capture the intricate relational dynamics crucial for understanding and predicting disease spread, leading to biases and compromised accuracy.\n    *   **Positioning of this Work**: This paper \\cite{liu20242g6} distinguishes itself as a *comprehensive and pioneering review* specifically focused on the application of GNNs in epidemic modeling. Unlike prior surveys that are often narrow in scope (e.g., specific viruses, single tasks, or general deep learning without deep GNN integration), this work offers a broader and more detailed overview of GNN-based approaches across a spectrum of epidemic tasks, aiming to bridge existing literature gaps.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic and exhaustive review of GNNs in epidemic modeling. This involves:\n        *   Developing hierarchical taxonomies for both epidemic tasks (categorized into Detection, Surveillance, Prediction, and Projection) and GNN methodologies (categorized into Neural Models and Hybrid Models).\n        *   Providing detailed explanations and definitions for each task category.\n        *   Systematically examining existing GNN-based methodologies, including their technical details, data resources, and graph construction techniques.\n    *   **Novelty**: The innovation lies in providing the *first comprehensive and structured review* dedicated solely to GNNs in epidemiology. It offers novel taxonomies for organizing the field, a meticulous examination of existing methods, and a systematic identification of limitations and future research directions, thereby fostering interdisciplinary synergy.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Taxonomies**: Introduction of hierarchical taxonomies for epidemic tasks (Detection, Surveillance, Prediction, Projection) and GNN methodologies (Neural Models, Hybrid Models), providing a structured framework for understanding the field.\n    *   **Systematic Review Framework**: Establishment of a comprehensive framework for analyzing and categorizing GNN applications, encompassing task objectives, data types, graph construction techniques, and technical details of various GNN models.\n    *   **Identification of Limitations and Future Directions**: Systematically discusses the limitations of current GNN methods in epidemiology and proposes concrete, prospective research directions to guide future advancements.\n    *   **Resource Compilation**: Provides a curated list of relevant papers (via a GitHub repository) to serve as a valuable resource for researchers in this interdisciplinary domain.\n\n*   **5. Experimental Validation**\n    *   As a review paper, \\cite{liu20242g6} does not present new experimental results. Instead, its \"validation\" comes from the comprehensive synthesis of empirical evidence reported in the *reviewed literature*. The paper highlights that GNNs have demonstrated significant success in various epidemiological tasks, such as infection prediction, outbreak source detection, and intervention modeling, by effectively capturing relational dynamics and yielding more precise predictions compared to traditional methods. The thoroughness and systematic organization of the reviewed works implicitly validate the claims made about the utility and potential of GNNs in this field.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations (of reviewed methods, as identified by the paper)**: The paper points out that traditional mechanistic models suffer from oversimplified assumptions, while general deep learning models (CNNs, RNNs) often fail to incorporate crucial relational information. While GNNs offer advantages, the paper's unprovided Section 5 is dedicated to discussing the limitations of existing GNN methods from diverse perspectives.\n    *   **Scope of Applicability**: The review focuses exclusively on the application of Graph Neural Networks within the domain of *epidemic modeling*. It covers a broad range of epidemic tasks and GNN methodologies, aiming for a comprehensive overview within this specific interdisciplinary intersection.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper \\cite{liu20242g6} significantly advances the technical state-of-the-art by providing the first comprehensive and structured overview of GNN applications in epidemiology. It consolidates fragmented knowledge, clarifies the landscape of existing methods, and underscores the unique advantages of GNNs in handling complex relational data for epidemic tasks.\n    *   **Potential Impact**:\n        *   **Guidance for Researchers**: Offers a clear roadmap and foundational understanding for researchers entering or working in this interdisciplinary field, facilitating model selection and task comprehension.\n        *   **Promotion of Interdisciplinary Synergy**: Aims to bridge the GNN and epidemiology communities, fostering collaborative research and accelerating advancements in both fields.\n        *   **Identification of Future Research**: Systematically outlines promising future research directions, stimulating innovation and guiding efforts to address current limitations in GNN-based epidemic modeling.",
        "year": 2024,
        "citation_key": "liu20242g6"
      }
    ],
    "layer2_summary": "### 1. Evolution Analysis: Chronological Progression of Graph Neural Network Research\n\n1.  **[morris20185sd] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks (2018)**\n    *   **Methodological/Conceptual Shift:** Established the foundational theoretical link between standard GNNs (1-GNNs) and the 1-dimensional Weisfeiler-Leman (1-WL) algorithm, setting a theoretical upper bound on their expressive power. Introduced a principled method to increase expressivity by generalizing to k-dimensional GNNs (k-GNNs) operating on k-tuples, inspired by k-WL.\n    *   **Problems Addressed:** The lack of theoretical understanding of standard GNNs' expressive power; the inability of 1-GNNs to distinguish certain non-isomorphic graphs or capture simple higher-order properties (e.g., triangle counts).\n    *   **Innovations/Capabilities:** Theoretical proof of 1-GNN equivalence to 1-WL; k-GNNs for higher-order message passing on k-element subsets; hierarchical 1-k-GNNs for multi-scale structural learning.\n\n2.  **[jegelka20222lq] Theory of Graph Neural Networks: Representation and Learning (2022)**\n    *   **Methodological/Conceptual Shift:** A comprehensive theoretical *survey* that synthesized existing knowledge, formalizing Message Passing Neural Networks (MPNNs) and their connection to 1-WL, and exploring methods to overcome these limits. It broadened the focus to a deeper theoretical understanding of representation, generalization, and extrapolation.\n    *   **Problems Addressed:** The need for a structured theoretical framework to understand GNN capabilities and limitations; lack of clarity on how GNNs generalize and extrapolate; limitations of standard MPNNs (1-WL equivalence, inability to decide complex structural properties).\n    *   **Innovations/Capabilities:** Formal connection between MPNNs and 1-WL; universal approximation theorems for multiset functions; discussion of node ID augmentations and higher-order GNNs as solutions to 1-WL limits; connections to local algorithms in distributed computing.\n    *   **Temporal Gaps/Clusters:** Published 4 years after [morris20185sd], this paper reflects a significant maturation of the field's theoretical understanding, summarizing advancements and open questions up to 2022.\n\n3.  **[reiser2022b08] Graph neural networks for materials science and chemistry (2022)**\n    *   **Methodological/Conceptual Shift:** A *review* paper that shifted focus from abstract theoretical expressivity to the practical application of GNNs in a specific, high-impact domain (chemistry and materials science), emphasizing the integration of physical and geometric constraints.\n    *   **Problems Addressed:** The inefficiency of hand-crafted features for molecular/material property prediction; the need for end-to-end learning and direct processing of graph-structured chemical data; the importance of incorporating physical symmetries (rotation, translation) and periodicity.\n    *   **Innovations/Capabilities:** Highlighted MPNNs as a unifying framework for this domain; emphasized the integration of geometric information and symmetry-equivariant representations; discussed extensions like edge updates, skip connections, and attention in an applied context.\n    *   **Temporal Gaps/Clusters:** Published in the same year as several theoretical papers, this review demonstrates the rapid adoption and practical impact of GNNs, while also acknowledging their theoretical limitations (e.g., WL hierarchy, over-smoothing) as ongoing research problems.\n\n4.  **[wang2022u2l] How Powerful are Spectral Graph Neural Networks (2022)**\n    *   **Methodological/Conceptual Shift:** Focused specifically on *spectral* GNNs, challenging the common assumption that nonlinearity is necessary for their expressiveness and establishing a novel link between spectral universality and the 1-WL test.\n    *   **Problems Addressed:** Lack of theoretical understanding of spectral GNN expressive power; uncertainty about the necessity of nonlinearities/MLPs in spectral GNNs; lack of systematic analysis of different polynomial bases for spectral filters.\n    *   **Innovations/Capabilities:** Proof that *linear* spectral GNNs can be universal under mild conditions; formal connection between spectral GNN universality and the 1-WL test; optimization analysis of filter bases; introduction of JacobiConv, a linear spectral GNN.\n\n5.  **[feng20225sa] How Powerful are K-hop Message Passing Graph Neural Networks (2022)**\n    *   **Methodological/Conceptual Shift:** Provided the first theoretical characterization of K-hop message passing, formally differentiating between various K-hop kernels (shortest path distance vs. graph diffusion) and proposing an enhancement.\n    *   **Problems Addressed:** Lack of theoretical understanding of K-hop message passing GNNs; confusion/interchangeability of different K-hop kernel definitions; the 1-WL bound of 1-hop GNNs.\n    *   **Innovations/Capabilities:** Formal definitions and differentiation of shortest path distance (spd) and graph diffusion (gd) kernels; proof that K-hop message passing (K>1) is strictly more powerful than 1-WL but bounded by 3-WL; introduction of KP-GNN, enhancing K-hop MP with \"peripheral subgraph\" information to surpass 3-WL limitations.\n\n6.  **[chamberlain2022fym] Graph Neural Networks for Link Prediction with Subgraph Sketching (2022)**\n    *   **Methodological/Conceptual Shift:** Addressed the trade-off between expressivity and efficiency for link prediction by introducing \"subgraph sketching\" to approximate complex subgraph features efficiently within a full-graph GNN framework.\n    *   **Problems Addressed:** Poor performance of standard MPNNs for link prediction (due to 1-WL limitations, inability to count triangles/distinguish automorphic nodes); inefficiency and scalability issues of state-of-the-art subgraph-based GNNs (SGNNs) due to explicit subgraph construction.\n    *   **Innovations/Capabilities:** ELPH (Efficient Link Prediction with Hashing) using HyperLogLog and MinHashing for subgraph sketches; BUDDY for scalable precomputation; proof that ELPH is strictly more expressive than standard MPNNs for link prediction.\n\n7.  **[fang2022tjj] Universal Prompt Tuning for Graph Neural Networks (2022)**\n    *   **Methodological/Conceptual Shift:** Introduced a novel paradigm for adapting pre-trained GNNs: \"universal prompt tuning\" by modifying input *features* rather than model parameters or graph structure, drawing inspiration from NLP/CV.\n    *   **Problems Addressed:** Challenges in adapting pre-trained GNNs to diverse downstream tasks due to varied pre-training strategies; limitations of traditional fine-tuning (catastrophic forgetting, objective misalignment); lack of universality and theoretical guarantees in prior GNN prompt tuning methods.\n    *   **Innovations/Capabilities:** Graph Prompt Feature (GPF) and GPF-plus for universal, feature-space-based prompt tuning; theoretical derivations proving GPF's universality and potential superiority over fine-tuning.\n\n8.  **[bianchi20239ee] The expressive power of pooling in Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Extended the theoretical analysis of GNN expressive power (rooted in WL) to hierarchical GNNs that incorporate *pooling* operators, providing principled conditions for pooling to preserve expressivity.\n    *   **Problems Addressed:** Lack of understanding of how pooling operators impact GNN expressive power; absence of a principled, theoretically grounded criterion to compare or design pooling operators; limitations of existing expressivity studies to \"flat GNNs.\"\n    *   **Innovations/Capabilities:** Derivation of sufficient conditions for pooling operators to preserve the expressive power of preceding MP layers; a universal criterion for evaluating and designing pooling operators; analysis of common pooling operators; proposal of an experimental setup for empirical expressivity measurement.\n\n9.  **[finkelshtein202301z] Cooperative Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Proposed a fundamental shift from fixed, synchronous message passing to *dynamic, asynchronous, node-specific communication* via \"cooperative\" networks, where nodes dynamically choose their communication actions.\n    *   **Problems Addressed:** Limitations of fixed, synchronous message passing (information bottlenecks, over-squashing, over-smoothing, 1-WL bound); inability of nodes to react individually or adapt information flow.\n    *   **Innovations/Capabilities:** CO-GNN framework with an action network (π) and an environment network (η); dynamic, asynchronous, and directed information flow; theoretically shown to be more expressive than 1-WL; mitigation of over-smoothing.\n\n10. **[joshi20239d0] On the Expressive Power of Geometric Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Extended the WL framework to *geometric graphs* embedded in Euclidean space, introducing the Geometric Weisfeiler-Leman (GWL) test to account for physical symmetries (rotation, translation, reflection).\n    *   **Problems Addressed:** Inapplicability of standard WL test and non-geometric GNNs to geometric graphs; the need to account for physical symmetries in geometric GNNs to preserve physical meaning; lack of theoretical characterization of geometric GNN expressive power.\n    *   **Innovations/Capabilities:** Geometric Weisfeiler-Leman (GWL) test for symmetry-aware isomorphism; characterization of how invariant vs. equivariant layers, higher-order tensors, and depth influence geometric GNN expressivity; proof of equivalence between GWL discrimination and universal approximation for G-invariant functions.\n\n11. **[ju2023prm] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion (2023)**\n    *   **Methodological/Conceptual Shift:** Shifted focus from expressive power to *generalization performance* and robustness, providing tighter theoretical bounds using PAC-Bayesian analysis and Hessian-based stability.\n    *   **Problems Addressed:** Vacuous generalization bounds for deep/overparameterized GNNs (scaling with maximum degree); lack of formal understanding and quantification of GNN generalization; overfitting in fine-tuning scenarios.\n    *   **Innovations/Capabilities:** Sharp PAC-Bayesian generalization bounds scaling with the spectral norm of the graph diffusion matrix; matching lower bound demonstrating tightness; Hessian-based measure of noise stability correlating with generalization gaps; Hessian regularization algorithm for robust fine-tuning.\n\n12. **[longa202399q] Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities (2023)**\n    *   **Methodological/Conceptual Shift:** A *survey* paper that systematized the emerging field of GNNs for *temporal graphs*, formalizing learning settings, tasks, and proposing a unified taxonomy.\n    *   **Problems Addressed:** Lack of a comprehensive, systematized overview of Temporal GNNs (TGNNs); absence of rigorous formalization for learning settings and tasks in temporal graphs; the need for a unified taxonomy for existing TGNN approaches; limitations of static GNNs for dynamic real-world systems.\n    *   **Innovations/Capabilities:** Coherent formalization of learning settings and tasks for temporal graphs; comprehensive taxonomy for TGNNs based on temporal representation (snapshot-based vs. event-based) and processing; formal definitions for various temporal graph types.\n\n13. **[michel2023hc4] Path Neural Networks: Expressive and Accurate Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Focused on leveraging *path information* explicitly for node representation learning, moving beyond local neighborhood aggregation to capture non-local structural patterns.\n    *   **Problems Addressed:** Limited expressive power of standard GNNs (1-WL bound); difficulty in capturing complex structural information beyond immediate neighbors; computational challenge of efficiently leveraging path information.\n    *   **Innovations/Capabilities:** Path Neural Networks (PathNNs) with variants for different path types; \"annotated sets of paths\" for recursive, hierarchical structural encoding; theoretical proof that `AP-Trees` are strictly more powerful than `WL-Trees`, and `˜AP` can distinguish graphs 3-WL indistinguishable.\n\n14. **[zeng20237gv] Substructure Aware Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Integrated *subgraph-level structural information* into node features during message passing, aiming for high expressivity without the high computational cost of full higher-order GNNs.\n    *   **Problems Addressed:** The 1-WL limitation of conventional GNNs, preventing perception of crucial higher-order substructures; scalability and complexity issues of direct higher-order GNNs; lack of generalization in predefined hand-crafted substructures.\n    *   **Innovations/Capabilities:** Substructure Aware Graph Neural Network (SAGNN) framework; \"Cut subgraph\" extraction strategy using Edge Betweenness Centrality; efficient random walk return probability encoding for subgraphs; theoretical proof that SAGNN-equipped 1-WL GNNs are strictly more powerful than 1-WL, even distinguishing 3-WL failures.\n\n---\n\n### 2. Evolution Analysis: Cohesive Narrative\n\nThe evolution of Graph Neural Networks (GNNs) over these papers reveals two intertwined major trends: a relentless \"Quest for Expressive Power\" to overcome fundamental limitations, and a parallel drive \"Towards Adaptive, Robust, and Scalable GNNs for Real-World Applications.\" These trends reflect the field's progression from foundational theoretical understanding to sophisticated, practical deployments.\n\n**Trend 1: The Quest for Expressive Power: Beyond 1-WL and Towards Richer Structural Understanding**\n\nThe journey begins with the seminal work by [morris20185sd] \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" (2018), which formally established that standard GNNs are no more powerful than the 1-Weisfeiler-Leman (1-WL) algorithm. This theoretical bottleneck meant GNNs struggled to distinguish many non-isomorphic graphs and capture higher-order structural motifs like triangles. To address this, [morris20185sd] introduced k-GNNs, operating on k-element node subsets, a principled way to increase expressivity, albeit with increased computational cost.\n\nThis foundational insight spurred a wave of research aimed at surpassing the 1-WL limit. [jegelka20222lq] \"Theory of Graph Neural Networks: Representation and Learning\" (2022) provided a comprehensive theoretical survey, solidifying the 1-WL connection and outlining various approaches, including higher-order GNNs and node ID augmentations, to enhance discriminative power. Concurrently, [wang2022u2l] \"How Powerful are Spectral Graph Neural Networks\" (2022) delved into spectral GNNs, surprisingly proving that even *linear* spectral GNNs could achieve universal approximation under certain conditions, and connecting their power to the 1-WL test. This challenged the assumption that nonlinearity was always necessary for expressivity.\n\nThe focus then broadened to different forms of structural information. [feng20225sa] \"How Powerful are K-hop Message Passing Graph Neural Networks\" (2022) characterized K-hop message passing, proving it was strictly more powerful than 1-WL but bounded by 3-WL. Their innovation, KP-GNN, by incorporating \"peripheral subgraph\" information, pushed expressivity even beyond 3-WL for certain graphs. [bianchi20239ee] \"The expressive power of pooling in Graph Neural Networks\" (2023) extended this theoretical rigor to hierarchical GNNs, deriving conditions for pooling operators to preserve expressivity, a crucial aspect for deep GNN architectures.\n\nMore recent works have explored increasingly sophisticated ways to encode structural information. [michel2023hc4] \"Path Neural Networks: Expressive and Accurate Graph Neural Networks\" (2023) introduced PathNNs, leveraging explicit path information and \"annotated sets of paths\" to surpass 1-WL and even distinguish graphs indistinguishable by 3-WL. Similarly, [zeng20237gv] \"Substructure Aware Graph Neural Networks\" (2023) proposed SAGNN, which dynamically extracts \"Cut subgraphs\" and encodes them using random walk probabilities, achieving expressivity beyond 3-WL while maintaining scalability. This represents a significant leap, demonstrating that rich subgraph-level information can be integrated efficiently. Finally, [finkelshtein202301z] \"Cooperative Graph Neural Networks\" (2023) introduced a radical shift, allowing nodes to dynamically choose their communication actions, leading to a more flexible, asynchronous message passing that is theoretically more expressive than 1-WL and mitigates issues like over-smoothing.\n\n**Trend 2: Towards Adaptive, Robust, and Scalable GNNs for Real-World Applications**\n\nAs GNNs gained expressive power, the field simultaneously grappled with making them practical, robust, and adaptable for diverse real-world scenarios. [reiser2022b08] \"Graph neural networks for materials science and chemistry\" (2022), a review, highlighted the successful application of GNNs in chemistry and materials science, emphasizing the need for models that incorporate geometric information and physical symmetries. This domain-specific application underscored the importance of adapting GNNs to data with inherent physical constraints. This was further formalized by [joshi20239d0] \"On the Expressive Power of Geometric Graph Neural Networks\" (2023), which introduced the Geometric Weisfeiler-Leman (GWL) test, extending expressivity analysis to geometric graphs and explicitly accounting for symmetries.\n\nScalability and efficiency became paramount. [chamberlain2022fym] \"Graph Neural Networks for Link Prediction with Subgraph Sketching\" (2022) addressed the inefficiency of expressive subgraph-based GNNs for link prediction. Their ELPH and BUDDY models introduced \"subgraph sketching\" to approximate complex structural features efficiently, achieving state-of-the-art performance with significantly improved speed and scalability. This demonstrated a crucial balance between expressivity and practical deployment.\n\nThe challenge of adapting pre-trained GNNs to new tasks efficiently was tackled by [fang2022tjj] \"Universal Prompt Tuning for Graph Neural Networks\" (2022). They introduced Graph Prompt Feature (GPF), a novel, universal prompt-tuning method that modifies input features rather than model parameters, proving its theoretical universality and superior performance over fine-tuning, especially in few-shot settings. This marked a significant step towards more flexible and resource-efficient GNN deployment.\n\nFinally, understanding and quantifying generalization became critical for model reliability. [ju2023prm] \"Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion\" (2023) provided significantly tighter PAC-Bayesian generalization bounds for GNNs, scaling with the spectral norm of the graph diffusion matrix rather than the often-vacuous maximum degree. Their Hessian-based stability measure and regularization algorithm offered practical tools for improving GNN robustness. The increasing complexity of real-world data also led to the systematization of GNNs for dynamic graphs by [longa202399q] \"Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities\" (2023), providing a much-needed framework for understanding and developing Temporal GNNs.\n\n### 3. Synthesis\n\nThese works collectively trace a powerful intellectual trajectory in Graph Neural Networks, moving from a foundational understanding of their expressive limits to sophisticated architectural innovations and theoretical frameworks that overcome these limitations. Their collective contribution lies in pushing GNNs beyond simple local message passing to capture richer, higher-order, and domain-specific structural information, while simultaneously enhancing their adaptability, robustness, and scalability for diverse and complex real-world applications."
  },
  "7456dea3a3646f2df6392773a196a5abd0d53b11": {
    "seed_title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
    "summary": "\n\n2. *Evolution Analysis:*\nI am unable to provide an evolution analysis as the list of papers to reference is empty. Please provide the papers and their summaries.\n\n3. *Synthesis*\nI am unable to provide a synthesis as the list of papers to reference is empty. Please provide the papers and their summaries.",
    "path": [
      "7456dea3a3646f2df6392773a196a5abd0d53b11"
    ],
    "layer1_papers": [
      {
        "title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
        "abstract": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
        "summary": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
        "year": 2021,
        "citation_key": "batzner2021t07"
      }
    ],
    "layer2_papers": [],
    "layer3_papers": [],
    "layer2_summary": null
  },
  "789a7069d1a2d02d784e4821685b216cc63e6ec8": {
    "seed_title": "Strategies for Pre-training Graph Neural Networks",
    "summary": "**1. Evolution Analysis:**\n\n*   **[hu2019r47] Strategies for Pre-training Graph Neural Networks (2019)**\n    *   **Methodological/Conceptual Shift:** Established the foundational paradigm of GNN pre-training for transfer learning. Shifted from ad-hoc or naive supervised approaches to a systematic, multi-level (node and graph) self-supervised pre-training strategy.\n    *   **Problems Addressed:** Addressed the scarcity of labeled graph data, poor out-of-distribution generalization, and the \"negative transfer\" issue in early GNN transfer learning attempts. It tackled the lack of systematic investigation into GNN pre-training strategies.\n    *   **Innovations/Capabilities:** Introduced a combined node- and graph-level pre-training strategy, novel self-supervised node-level tasks (Context Prediction, Attribute Masking), and regularized graph-level pre-training. Enabled significant generalization improvements and state-of-the-art performance in molecular and protein prediction.\n    *   **Temporal Gaps/Clusters:** This paper acts as a starting point, setting the stage for subsequent work on GNN transfer learning.\n\n*   **[sun2022d18] GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks (2022)**\n    *   **Methodological/Conceptual Shift:** Introduced the \"prompt tuning\" paradigm, inspired by NLP, to GNNs. Shifted from the traditional \"pre-train, fine-tune\" to \"pre-train, prompt-tune\" to reduce adaptation costs.\n    *   **Problems Addressed:** Identified the \"inherent training objective gap\" between pre-training pretext tasks and downstream tasks, which made fine-tuning costly and inefficient, even after pre-training established by [hu2019r47].\n    *   **Innovations/Capabilities:** Proposed GPPT, which reformulates downstream node classification to mimic the pre-training task (masked edge prediction) using a novel \"graph prompting function\" and \"token pairs.\" Enabled efficient knowledge transfer *without tedious fine-tuning* and accelerated convergence.\n    *   **Temporal Gaps/Clusters:** A 3-year gap from [hu2019r47] suggests the maturation of pre-training methods and the emergence of prompt learning as a powerful adaptation technique from other domains (NLP).\n\n*   **[fang2022tjj] Universal Prompt Tuning for Graph Neural Networks (2022)**\n    *   **Methodological/Conceptual Shift:** Moved towards a more universal and theoretically grounded approach to GNN prompt tuning. Shifted the prompt application from structural modification/task reformulation to direct manipulation of the *input graph's feature space*.\n    *   **Problems Addressed:** Highlighted the limitation of prior prompt tuning methods (like [sun2022d18]) being specialized for specific pre-training tasks and lacking universality across diverse GNN pre-training strategies. Also addressed the lack of theoretical guarantees for prompt tuning.\n    *   **Innovations/Capabilities:** Introduced Graph Prompt Feature (GPF) and GPF-plus, the first universal prompt-based tuning methods for GNNs, compatible with *any* pre-trained GNN and *any* pre-training strategy. Provided rigorous theoretical derivations for GPF's universality and effectiveness, demonstrating it can be superior to full fine-tuning.\n    *   **Temporal Gaps/Clusters:** Published in the same year as [sun2022d18], indicating rapid, parallel exploration and refinement of prompt tuning concepts in the GNN community.\n\n*   **[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Aimed for broader universality across *task types* (node vs. graph classification), not just pre-training strategies. Introduced a novel prompting mechanism that modifies the `ReadOut` aggregation function.\n    *   **Problems Addressed:** Addressed the limitation of existing graph prompting methods (e.g., [sun2022d18]) being restricted to specific downstream tasks (e.g., node classification), lacking a unified approach for diverse tasks like both node and graph classification. Sought to further bridge the objective inconsistency between pre-training and downstream tasks.\n    *   **Innovations/Capabilities:** Proposed GraphPrompt, a framework that unifies pre-training (link prediction) and diverse downstream tasks (node/graph classification) into a common \"subgraph similarity\" template. Introduced task-specific learnable prompts that guide the `ReadOut` operation, enabling a single pre-trained model to serve multiple tasks effectively in few-shot settings.\n    *   **Temporal Gaps/Clusters:** Continued rapid development in prompt tuning, focusing on expanding its applicability to a wider range of downstream tasks.\n\n*   **[ju2023prm] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion (2023)**\n    *   **Methodological/Conceptual Shift:** Represented a significant conceptual shift from purely empirical performance improvement to a *rigorous theoretical understanding and quantification of GNN generalization*. Introduced a new way to measure GNN complexity and stability.\n    *   **Problems Addressed:** Tackled the fundamental problem of vacuous generalization bounds for deep GNNs (scaling with `d^(l-1)`), which failed to explain empirical performance. Sought to provide a more accurate and tighter theoretical understanding of why GNNs generalize.\n    *   **Innovations/Capabilities:** Derived sharp, non-vacuous PAC-Bayesian generalization bounds that scale with the *spectral norm of the graph diffusion matrix* (`P_G^(l-1)`), offering orders of magnitude tighter bounds. Introduced a novel *Hessian-based stability measure* that accurately correlates with empirical generalization gaps and proposed a Hessian regularization algorithm for robust fine-tuning.\n    *   **Temporal Gaps/Clusters:** Published in the same year as [liu2023ent] and [dai2022hsi], but represents an orthogonal research direction focusing on theoretical foundations rather than direct application/adaptation. This indicates a maturing field where both practical performance and theoretical guarantees are being pursued.\n\n*   **[dai2022hsi] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability (2022)**\n    *   **Methodological/Conceptual Shift:** A meta-level shift, signaling the field's maturation and a growing focus on the *responsible deployment and ethical implications* of GNNs. It moved beyond just performance to address societal impact.\n    *   **Problems Addressed:** Identified the critical trustworthiness issues (privacy leakage, adversarial attacks, inherent biases, lack of interpretability) that limit GNN adoption in high-stakes real-world applications. Highlighted the inadequacy of i.i.d. data solutions for graph data.\n    *   **Innovations/Capabilities:** Provided the *first comprehensive and up-to-date survey* covering all four critical trustworthiness dimensions specifically for GNNs. Systematically categorized existing methods, formulated general frameworks, and discussed interconnections and future directions.\n    *   **Temporal Gaps/Clusters:** Published in 2022, alongside the emergence of prompt tuning, suggesting that as GNNs become more powerful and adaptable, their trustworthiness becomes a pressing concern.\n\n*   **[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)**\n    *   **Methodological/Conceptual Shift:** A major shift towards *multi-modal learning* and *semantic understanding* for GNNs, leveraging the power of Large Language Models (LLMs). Pushed prompt tuning beyond task alignment to *semantic space alignment*.\n    *   **Problems Addressed:** Addressed the lack of real-world semantic understanding in GNNs, the challenge of aligning independently pre-trained GNNs and LLMs for general graph data with *extremely weak text supervision*, and the instability of prior graph prompt designs (e.g., AIO).\n    *   **Innovations/Capabilities:** Proposed **Morpher**, the first paradigm for graph-text multi-modal prompt learning under extremely weak text supervision. Introduced an *improved, stable graph prompt design* and a *cross-modal projector* with contrastive learning to align graph and text embeddings. Enabled *CLIP-style zero-shot generalization* for GNNs to unseen classes.\n    *   **Temporal Gaps/Clusters:** A 2-year gap from the initial prompt tuning papers, indicating a new wave of innovation in multi-modal GNNs, likely influenced by the rapid advancements in LLMs and multi-modal AI in general.\n\n---\n\n**2. *Evolution Analysis:***\n\nThe evolution of Graph Neural Networks (GNNs) through this chain of papers reveals two dominant, interconnected trends: first, a sustained drive towards **efficient and universal knowledge transfer and adaptation** for GNNs, culminating in multi-modal capabilities; and second, a growing emphasis on **theoretical understanding and trustworthiness** as GNNs mature and become more widely deployed.\n\n**Trend 1: Efficient and Universal Knowledge Transfer and Adaptation**\n*   *Methodological progression*: This trend begins with the foundational \"pre-train, fine-tune\" paradigm established by \"[hu2019r47] Strategies for Pre-training Graph Neural Networks (2019)\". This paper systematically explored pre-training strategies, focusing on learning robust representations through combined node- and graph-level self-supervised tasks. The methodology then evolved rapidly with the introduction of \"prompt tuning\" in \"[sun2022d18] GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks (2022)\", shifting from full model fine-tuning to input-level modifications. This was further refined by \"[fang2022tjj] Universal Prompt Tuning for Graph Neural Networks (2022)\", which moved from task-specific structural prompts to universal feature-space prompts (GPF). \"[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)\" then extended prompt tuning to modify the `ReadOut` aggregation function, unifying diverse downstream tasks. Finally, \"[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)\" introduced multi-modal prompt learning, aligning GNNs with LLMs via cross-modal projectors and contrastive learning.\n*   *Problem evolution*: The initial problem, addressed by \"[hu2019r47] Strategies for Pre-training Graph Neural Networks (2019)\", was the scarcity of labeled graph data and the \"negative transfer\" issue in GNNs. While pre-training helped, \"[sun2022d18] GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks (2022)\" identified the \"inherent training objective gap\" that made fine-tuning costly. This led to the problem of prompt tuning's lack of universality across pre-training strategies, which \"[fang2022tjj] Universal Prompt Tuning for Graph Neural Networks (2022)\" solved. The next challenge, tackled by \"[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)\", was the inability of existing prompt methods to universally handle *diverse downstream task types* (node vs. graph classification). The most recent paper, \"[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)\", addressed the fundamental lack of real-world semantic understanding in GNNs and the difficulty of aligning them with LLMs under extremely weak text supervision.\n*   *Key innovations*: \"[hu2019r47] Strategies for Pre-training Graph Neural Networks (2019)\" innovated with combined node- and graph-level self-supervised pre-training. \"[sun2022d18] GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks (2022)\" introduced the \"graph prompting function\" for task reformulation. \"[fang2022tjj] Universal Prompt Tuning for Graph Neural Networks (2022)\" contributed the universal Graph Prompt Feature (GPF) with theoretical guarantees. \"[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)\" developed a unified \"subgraph similarity\" template and learnable prompts for the `ReadOut` operation. \"[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)\" introduced the Morpher paradigm for multi-modal prompt learning, an improved stable graph prompt design, and enabled CLIP-style zero-shot generalization for GNNs.\n\n**Trend 2: Theoretical Understanding and Trustworthiness**\n*   *Methodological progression*: This trend represents a shift from purely empirical performance to rigorous analysis and responsible deployment. \"[ju2023prm] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion (2023)\" employed PAC-Bayesian analysis and Hessian-based stability measures to derive tighter generalization bounds. This theoretical work is complemented by the meta-level methodological approach of \"[dai2022hsi] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability (2022)\", which systematically categorized and synthesized existing literature on GNN trustworthiness.\n*   *Problem evolution*: As GNNs became more powerful and widely adopted (enabled by Trend 1), the problem of their fundamental generalization capabilities became critical. \"[ju2023prm] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion (2023)\" addressed the issue of vacuous generalization bounds that failed to explain deep GNN performance. Concurrently, \"[dai2022hsi] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability (2022)\" tackled the growing concerns about GNNs' trustworthiness (privacy, robustness, fairness, explainability), which limited their adoption in high-stakes applications.\n*   *Key innovations*: \"[ju2023prm] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion (2023)\" innovated by providing sharp, non-vacuous PAC-Bayesian generalization bounds scaling with the *spectral norm of the graph diffusion matrix*, and a novel Hessian-based stability measure. \"[dai2022hsi] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability (2022)\" provided the *first comprehensive and up-to-date survey* covering all four critical trustworthiness dimensions for GNNs, systematically organizing fragmented knowledge and highlighting interconnections.\n\n**3. *Synthesis***\n\nThese works collectively trace an intellectual trajectory from establishing basic GNN transfer learning to developing highly efficient, universal, and multi-modal adaptation strategies, while simultaneously building a robust theoretical and ethical framework for their deployment. Their collective contribution is to advance GNNs from specialized, data-hungry models to versatile, semantically aware, and trustworthy systems capable of operating effectively across diverse tasks and data scarcity levels, thereby broadening their real-world applicability and impact.",
    "path": [
      "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "14c59d6dab548ef023b8a49df4a26b966fe9d00a"
    ],
    "layer1_papers": [
      {
        "title": "Strategies for Pre-training Graph Neural Networks",
        "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Strategies for Pre-training Graph Neural Networks \\cite{hu2019r47}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: How to effectively pre-train Graph Neural Networks (GNNs) for transfer learning, especially when downstream tasks have scarce labeled data and involve out-of-distribution test examples.\n    *   **Importance & Challenge**:\n        *   Labeled graph data, particularly in scientific domains (chemistry, biology), is often extremely scarce and expensive to obtain.\n        *   Real-world graph applications frequently involve out-of-distribution samples, where test graphs are structurally different from training graphs.\n        *   Naive pre-training strategies can lead to \"negative transfer,\" harming generalization performance on downstream tasks, which limits the reliability and applicability of pre-trained GNNs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While deep transfer learning and pre-training have been highly successful in computer vision and natural language processing, few studies have effectively generalized these techniques to graph data.\n    *   **Limitations of Previous Solutions**:\n        *   Existing GNN pre-training efforts are limited, and a systematic large-scale investigation of strategies was lacking.\n        *   Naive pre-training strategies, such as solely focusing on graph-level or node-level properties, often yield only marginal improvements and can even cause negative transfer on many downstream tasks. For instance, extensive graph-level multi-task supervised pre-training, despite using state-of-the-art GNN architectures, showed limited gains and negative transfer on a significant number of tasks \\cite{hu2019r47}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel pre-training strategy that simultaneously pre-trains an expressive GNN at both the individual node level and the entire graph level. This approach aims to learn useful local (node-level semantics) and global (graph-level properties) representations concurrently.\n    *   **Novelty/Difference**:\n        *   **Combined Node- and Graph-level Pre-training**: Unlike naive strategies that focus on one level, this work emphasizes the synergy between learning meaningful node embeddings and composable graph embeddings (Figure 1 (a.iii) in \\cite{hu2019r47}).\n        *   **Self-supervised Node-level Methods**: Introduces two novel self-supervised methods for node-level pre-training:\n            *   **Context Prediction**: Trains the GNN to predict the surrounding graph structure (context graph) of a node's K-hop neighborhood. It uses an auxiliary GNN to encode context graphs into fixed-length vectors and employs negative sampling for learning.\n            *   **Attribute Masking**: Randomly masks node and/or edge attributes (e.g., atom types) and trains the GNN to predict these masked attributes based on the neighboring structure. This helps capture domain-specific regularities like chemical valency or interaction types.\n        *   **Regularized Graph-level Pre-training**: Integrates multi-task supervised graph-level property prediction, but crucially, it regularizes the GNN first with the proposed node-level pre-training methods. This prevents negative transfer often seen with standalone graph-level pre-training by ensuring the underlying node embeddings are meaningful.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Context Prediction**: A self-supervised method for node-level pre-training that learns to map nodes in similar structural contexts to nearby embeddings by predicting surrounding graph structures.\n        *   **Attribute Masking**: A self-supervised method for node/edge attribute prediction on masked inputs, enabling GNNs to learn domain-specific regularities from attribute distributions.\n    *   **System Design/Architectural Innovations**: A holistic pre-training strategy that combines these self-supervised node-level tasks with supervised graph-level tasks, ensuring robust and transferable representations at both local and global scales.\n    *   **Empirical Insights**: First systematic large-scale investigation of GNN pre-training strategies, demonstrating that naive approaches can lead to negative transfer and that a combined strategy is essential.\n    *   **Dataset Contribution**: Creation and release of two large new pre-training datasets (2 million chemistry graphs, 395K biology graphs) to facilitate further research.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Systematic study on multiple graph classification datasets, focusing on molecular property prediction and protein function prediction.\n    *   **Key Performance Metrics**: ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n    *   **Comparison Results**:\n        *   **Significant Generalization Improvement**: Achieved up to 9.4% absolute improvements in ROC-AUC over non-pre-trained GNN models.\n        *   **Superior to Naive Strategies**: Outperformed GNNs with extensive graph-level multi-task supervised pre-training by up to 5.2% higher average ROC-AUC.\n        *   **State-of-the-Art (SOTA)**: Achieved SOTA performance for molecular property prediction and protein function prediction.\n        *   **Negative Transfer Avoidance**: The proposed strategy successfully avoided negative transfer across all tested downstream tasks, unlike naive strategies which showed negative transfer on many tasks (e.g., 2 out of 8 molecular datasets and 13 out of 40 protein prediction tasks).\n        *   **Architecture Benefits**: More expressive GNN architectures (e.g., GIN) benefited more from pre-training.\n        *   **Efficiency**: Pre-training led to orders-of-magnitude faster training and convergence during the fine-tuning stage.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The paper notes that structural similarity prediction as a graph-level pre-training task (e.g., modeling graph edit distance) was considered but left for future work due to the difficulty in obtaining ground truth distances and the computational complexity for large datasets \\cite{hu2019r47}.\n    *   **Scope of Applicability**: The methods are primarily validated on molecular and protein graphs, which are richly annotated and benefit from the attribute masking approach. While the general strategy is applicable to other graph types, its effectiveness might vary depending on the availability and richness of node/edge attributes and structural contexts.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work provides the first systematic and large-scale investigation into GNN pre-training strategies, establishing a robust and effective approach that significantly advances the state-of-the-art in graph-level property prediction.\n    *   **Addressing Negative Transfer**: It offers a crucial solution to the pervasive problem of negative transfer in GNN transfer learning, making pre-trained GNNs more reliable and broadly applicable.\n    *   **Impact on Future Research**: The proposed combined node- and graph-level pre-training strategy, along with the self-supervised methods (Context Prediction, Attribute Masking), provides a strong foundation and benchmark for future research in GNN pre-training, especially for domains with scarce labeled data and out-of-distribution challenges. The released datasets also enable further exploration.",
        "year": 2019,
        "citation_key": "hu2019r47"
      }
    ],
    "layer2_papers": [
      {
        "title": "GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks",
        "abstract": "Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis and accelerating the model convergence up to 4.32X. The code is available in: https://github.com/MingChen-Sun/GPPT.",
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of supervised Graph Neural Network (GNN) training, which notoriously requires large amounts of labeled data. While transfer learning (pre-training GNNs and fine-tuning) is a promising solution, existing self-supervised pre-training methods suffer from a significant \"inherent training objective gap\" between the pretext task and the downstream task.\n    *   **Importance & Challenge**: This task gap necessitates costly and tedious fine-tuning, which prevents efficient elicitation of pre-trained knowledge, often leading to poor downstream performance. In some cases, naive pre-training can even deteriorate downstream tasks, undermining the reliability of transfer learning for graph data \\cite{sun2022d18}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the concept of self-supervised pre-training for GNNs, which aims to encode universal graph knowledge.\n    *   **Limitations of Previous Solutions**: Existing self-supervised pre-training methods for GNNs largely overlook the critical training objective gap between the pre-training pretext task and the downstream task. This oversight leads to inefficient knowledge transfer, requiring extensive fine-tuning, and can even result in performance degradation on downstream tasks \\cite{sun2022d18}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Graph Pre-training and Prompt Tuning (GPPT) \\cite{sun2022d18}.\n        *   **Pre-training Phase**: GNNs are initially pre-trained using the masked edge prediction task, chosen for its simplicity and popularity.\n        *   **Prompt Tuning Phase**: A novel \"graph prompting function\" is introduced. This function modifies a standalone node (for downstream node classification) into a \"token pair.\"\n        *   **Token Pair Composition**: Each token pair consists of a \"candidate label class\" and the \"node entity.\"\n        *   **Task Reformulation**: The downstream node classification task is ingeniously reformulated to mimic the structure of the edge prediction task used during pre-training.\n    *   **Novelty**: The core innovation lies in bridging the inherent training objective gap by reformulating the downstream task to align with the pre-training task. This allows the pre-trained GNNs to be applied *without tedious fine-tuning* to evaluate the \"linking probability\" of the token pair, directly yielding node classification decisions \\cite{sun2022d18}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The Graph Pre-training and Prompt Tuning (GPPT) paradigm itself, which explicitly addresses the task gap in GNN transfer learning \\cite{sun2022d18}.\n        *   The \"graph prompting function\" that transforms individual nodes into token pairs (candidate label class + node entity).\n        *   The reformulation of downstream node classification into an \"edge prediction-like\" task, enabling direct application of pre-trained models without fine-tuning.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on eight benchmark datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   GPPT demonstrated superior performance, delivering an average improvement of **4.29%** in few-shot graph analysis.\n        *   It significantly accelerated model convergence, achieving up to **4.32X** faster convergence \\cite{sun2022d18}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided text does not explicitly state technical limitations or assumptions beyond the focus on node classification and few-shot scenarios. The method is designed for scenarios where the downstream task can be reformulated to resemble the pre-training task (e.g., node classification as a form of \"linking\" between a node and a label).\n    *   **Scope of Applicability**: The method is primarily applicable to transfer learning for GNNs, particularly in few-shot graph analysis and node classification tasks, where the goal is to leverage pre-trained knowledge efficiently without extensive fine-tuning \\cite{sun2022d18}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: GPPT significantly advances the technical state-of-the-art in GNN transfer learning by effectively bridging the critical training objective gap between pre-training and downstream tasks. This leads to more efficient and effective knowledge transfer \\cite{sun2022d18}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into prompt-based learning for GNNs, potentially inspiring similar task reformulation strategies for other graph-related problems (e.g., link prediction, graph classification) to minimize fine-tuning costs and improve few-shot performance. It highlights the importance of aligning pre-training and downstream task objectives.",
        "year": 2022,
        "citation_key": "sun2022d18"
      },
      {
        "title": "Universal Prompt Tuning for Graph Neural Networks",
        "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n---\n\n### Focused Summary for Literature Review: Universal Prompt Tuning for Graph Neural Networks\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: Adapting pre-trained Graph Neural Networks (GNNs) to diverse downstream tasks is challenging due to the wide variety of GNN pre-training strategies. Existing prompt-based tuning methods for GNNs are specialized for particular pre-training tasks (e.g., edge prediction) and lack universality.\n*   **Importance & Challenge**:\n    *   GNNs face challenges like scarcity of labeled data and low out-of-distribution generalization, which pre-trained GNNs aim to mitigate.\n    *   The standard \"pre-train, fine-tune\" paradigm for GNNs suffers from objective misalignment between pre-training and downstream tasks, and catastrophic forgetting, especially in few-shot scenarios.\n    *   Prompt tuning, successful in NLP and CV, offers an alternative by modifying input data instead of model parameters. However, applying it to GNNs is difficult because there's no unified pre-training task (unlike masked language modeling in NLP), making it hard to design a universal prompting function.\n    *   Prior GNN prompt tuning efforts are intuition-based, specialized, and lack theoretical guarantees for effectiveness.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches**:\n    *   `\\cite{fang2022tjj}` builds upon the concept of pre-trained GNN models and the prompt tuning paradigm from NLP and CV.\n    *   It contrasts with traditional fine-tuning, which updates the entire pre-trained GNN model.\n    *   It relates to existing specialized prompt-based tuning methods for GNNs (e.g., \\cite{sun2022graphprompt, liu2023graphprompt}) that introduce virtual nodes or links for models pre-trained with edge prediction.\n*   **Limitations of previous solutions**:\n    *   **Fine-tuning**: Prone to catastrophic forgetting and sub-optimal performance due to objective misalignment, particularly with limited downstream data.\n    *   **Specialized GNN prompt tuning**: These methods are highly specific to certain pre-training strategies (e.g., edge prediction) and cannot be applied to GNNs pre-trained with other common strategies like attribute masking or contrastive learning. They are also often intuitively designed without theoretical guarantees.\n*   **Positioning**: `\\cite{fang2022tjj}` introduces the *first* universal prompt-based tuning method for pre-trained GNN models, designed to be applicable irrespective of the underlying pre-training strategy, thereby addressing the limitations of specialized approaches.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: `\\cite{fang2022tjj}` proposes **Graph Prompt Feature (GPF)**, a universal prompt-based tuning method that operates by modifying the input graph's *feature space*.\n    *   **GPF**: A learnable vector `p` (of dimension `F`, matching node feature dimensionality) is *added to all node features* `xi` in the input graph `X` to generate prompted features `X* = {x1+p, ..., xN+p}`. The pre-trained GNN model parameters are frozen, and only `p` and a learnable projection head `θ` are optimized for the downstream task.\n    *   **GPF-plus**: A theoretically stronger variant of GPF that incorporates *different* learnable prompted features for *different nodes* in the graph, allowing for more nuanced and adaptive input modifications.\n*   **Novelty/Difference**:\n    *   **Universality**: GPF is designed to be compatible with *any* pre-trained GNN model and *any* pre-training strategy, eliminating the need for strategy-specific manual prompting function design.\n    *   **Feature Space Manipulation**: Unlike methods that modify graph structure (e.g., adding virtual nodes/edges), GPF directly adapts the input node features, drawing inspiration from pixel-level visual prompts in computer vision.\n    *   **Theoretical Foundation**: The paper provides rigorous derivations to demonstrate that GPF can theoretically achieve an equivalent effect to *any* form of prompting function, offering a strong theoretical basis for its universality and effectiveness, which was lacking in prior intuitive designs.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The introduction of **Graph Prompt Feature (GPF)** and its variant **GPF-plus**, representing the first universal prompt-based tuning methods for pre-trained GNNs.\n    *   A novel paradigm for GNN prompt tuning by operating on the *input graph's feature space* rather than altering graph structure or fine-tuning model parameters.\n*   **Theoretical Insights/Analysis**:\n    *   Rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to *any* prompting function, thereby guaranteeing its broad applicability.\n    *   Theoretical proofs that GPF and GPF-plus are not weaker than full fine-tuning and can, in some cases, yield *superior* theoretical tuning results.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**:\n    *   Extensive experiments were conducted across various pre-training strategies (e.g., those involving edge prediction and attribute masking) and GNN architectures.\n    *   Evaluations were performed in both **full-shot** (sufficient labeled data) and **few-shot** (limited labeled data) scenarios.\n    *   Performance was compared against traditional **fine-tuning** and existing **specialized prompt-based tuning methods** (when applicable to their specialized pre-training strategies).\n*   **Key Performance Metrics & Comparison Results**:\n    *   GPF and GPF-plus consistently outperformed fine-tuning, despite utilizing significantly fewer tunable parameters.\n    *   **Average performance improvement over fine-tuning**: Approximately **1.4%** in full-shot scenarios and a more substantial **3.2%** in few-shot scenarios, highlighting its effectiveness in data-scarce environments.\n    *   **Superiority over specialized prompts**: GPF and GPF-plus significantly outperformed existing specialized prompt-based tuning methods even when applied to models pre-trained with the specific strategies those specialized methods were designed for (e.g., edge prediction).\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The primary focus and detailed validation in the main paper are on graph classification tasks, with extensions to node-wise tasks (node classification, link prediction) mentioned as being in the appendix (not provided in the excerpt).\n    *   While GPF-plus is described as theoretically stronger, the specific implementation details of how \"different prompted features for different nodes\" are learned and applied are not fully elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   Universally applicable to *any* pre-trained GNN model, regardless of its specific pre-training strategy.\n    *   Applicable across various GNN architectures.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   `\\cite{fang2022tjj}` makes a significant advancement by introducing the first *universal* prompt-based tuning method for GNNs, effectively addressing the challenge posed by diverse pre-training strategies.\n    *   It provides a more efficient, effective, and robust alternative to fine-tuning, particularly beneficial in data-limited settings, by adapting the input space while keeping the pre-trained model frozen.\n    *   The work establishes a strong theoretical foundation for GNN prompt tuning, moving beyond heuristic designs.\n*   **Potential Impact on Future Research**:\n    *   This work opens new research directions for developing universal adaptation strategies for GNNs and potentially other complex data structures with varied pre-training paradigms.\n    *   It could lead to the development of more generalizable and easily adaptable pre-trained GNN models, accelerating their deployment in real-world applications.\n    *   It encourages further theoretical and empirical investigation into the mechanisms and optimal designs of prompt tuning for graph-structured data.",
        "year": 2022,
        "citation_key": "fang2022tjj"
      },
      {
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
        "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
        "summary": "Here's a focused summary of the paper \"GraphPrompt : Unifying Pre-Training and Downstream Tasks for Graph Neural Networks\" \\cite{liu2023ent} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) typically require large amounts of task-specific labeled data for supervised learning. The \"pre-train, fine-tune\" paradigm, while reducing labeling, suffers from inconsistent objectives between pre-training and downstream tasks, leading to suboptimal performance. Existing graph prompting methods are limited, often only supporting specific tasks like node classification, lacking a universal approach for diverse downstream tasks (e.g., both node and graph classification).\n    *   **Importance & Challenge**: Reducing the reliance on extensive labeled data is crucial for GNN applicability. The challenge lies in: 1) unifying pre-training and various downstream tasks into a common framework to enable effective knowledge transfer, and 2) designing effective, task-specific prompts for graphs that can guide the pre-trained model without fine-tuning, similar to advancements in NLP.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon graph representation learning (graph embedding, GNNs) and graph pre-training techniques. It draws inspiration from prompting in natural language processing (NLP) to bridge the pre-training/downstream task gap.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional GNNs: Heavily dependent on large, task-specific labeled data.\n        *   \"Pre-train, fine-tune\" GNNs: Suffer from objective inconsistency between pre-training (e.g., link prediction) and downstream tasks (e.g., node classification), limiting generalization.\n        *   Meta-learning (e.g., L2P-GNN): Simulates fine-tuning but doesn't fundamentally address the objective discrepancy if downstream tasks differ from simulation.\n        *   Prior graph prompting (e.g., GPPT): Limited to specific tasks (e.g., node classification), lacking a universal design for different downstream tasks like graph classification.\n        *   Other \"GraphPrompt\" models: Some exist but focus on NLP tasks with auxiliary graphs, not general GNN pre-training and prompting.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liu2023ent} proposes **GraphPrompt**, a novel framework that unifies pre-training and downstream tasks (node and graph classification) into a common \"subgraph similarity\" template, and employs task-specific learnable prompts to guide the `ReadOut` operation.\n    *   **Novelty/Difference**:\n        *   **Unified Task Template**: Both pre-training (link prediction) and downstream tasks (node/graph classification) are reformulated as calculating the similarity between (sub)graph representations. This is achieved by representing all instances (nodes, graphs) as subgraphs: a node's contextual subgraph for node-level tasks, and the entire graph as its maximum subgraph for graph-level tasks.\n        *   **Learnable Prompts for `ReadOut`**: Instead of fine-tuning the entire GNN, \\cite{liu2023ent} introduces a novel task-specific learnable prompt that modifies the `ReadOut` operation. This prompt acts as parameters for the aggregation function, allowing different downstream tasks to adaptively fuse node representations into a subgraph representation, thereby \"pulling\" relevant knowledge from the frozen pre-trained model in a task-specific manner.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unification framework, **GraphPrompt**, that casts both pre-training (link prediction) and diverse downstream tasks (node and graph classification) as instances of learning subgraph similarity.\n        *   A novel prompting strategy that utilizes a learnable prompt to guide the `ReadOut` operation for task-specific aggregation, enabling effective exploitation of the pre-trained model without fine-tuning.\n    *   **System Design/Architectural Innovations**: The framework integrates a pre-trained GNN with a flexible prompting mechanism that adapts the `ReadOut` function based on the downstream task, allowing a single pre-trained model to serve multiple tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on five public datasets. The evaluation focuses on few-shot learning settings for both node and graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**: The paper demonstrates that **GraphPrompt** achieves superior performance compared to state-of-the-art approaches across the evaluated datasets, particularly in few-shot scenarios. This indicates its effectiveness in transferring knowledge and adapting to new tasks with limited labels.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations of **GraphPrompt** itself. The framework assumes the availability of label-free graphs for pre-training and focuses on the few-shot learning setting for downstream tasks.\n    *   **Scope of Applicability**: Primarily applicable to graph representation learning using GNNs, specifically for pre-training and subsequent adaptation to node classification and graph classification tasks, especially under limited supervision (few-shot learning). The unification is based on graph topology and subgraph similarity.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023ent} significantly advances the state-of-the-art by providing a universal and unified framework for GNN pre-training and prompting that can flexibly support diverse downstream tasks (node and graph classification). It effectively bridges the objective gap between pre-training and downstream tasks, a long-standing challenge in GNNs.\n    *   **Potential Impact**: This work has the potential to make GNNs more efficient and broadly applicable by reducing the need for extensive task-specific labeled data. It opens new avenues for research into more sophisticated graph prompting mechanisms and unified pre-training objectives, potentially leading to more robust and generalizable GNN models.",
        "year": 2023,
        "citation_key": "liu2023ent"
      },
      {
        "title": "Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion",
        "abstract": "Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately. Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.",
        "summary": "Here is a focused summary of the technical paper \\cite{ju2023prm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Formally understanding and quantifying the generalization performance (the gap between training and test performance) of Graph Neural Networks (GNNs), especially for deep and overparameterized models and in fine-tuning scenarios.\n    *   **Importance & Challenge**: Generalization is crucial for machine learning model success. For deep models like GNNs, classical complexity measures (e.g., VC dimension) often lead to vacuous bounds that do not explain empirical performance. Prior GNN generalization bounds scale with the graph's maximum degree, `d^(l-1)`, which can be numerically very large for real-world graphs, rendering them uninformative. Fine-tuning large GNNs often leads to overfitting without proper intervention, necessitating a better understanding of generalization.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on PAC-Bayesian analysis and stability properties, similar to prior generalization bounds for fully-connected networks. It extends previous GNN generalization analyses.\n    *   **Limitations of Previous Solutions**:\n        *   Naive application of bounds from feedforward networks to GNNs results in vacuous terms scaling with `n^(l-1)` (number of nodes).\n        *   VC dimension for GNNs scales with `n`, ignoring graph structure.\n        *   Verma and Zhang (2019) provided bounds scaling with the largest singular value of the graph diffusion matrix but only for single-layer GNNs and node prediction tasks.\n        *   Garg et al. (2020) and Liao et al. (2021) analyzed multi-layer message-passing GNNs for graph prediction, but their bounds scaled with `d^(l-1)` (maximum degree), which is often numerically much larger than observed generalization gaps and the spectral norm of diffusion matrices.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs PAC-Bayesian analysis to derive generalization bounds by measuring the stability of GNNs against noise perturbations. The key insight is to quantify this stability using Hessians of the loss function.\n    *   **Novelty/Difference**:\n        *   **Spectral Norm Dependence**: Instead of the maximum degree, the bounds scale with the largest singular value (spectral norm) of the graph diffusion matrix `P_G^(l-1)`. This provides significantly tighter and non-vacuous bounds.\n        *   **Unified Model Analysis**: The analysis applies to a unified GNN model that subsumes various architectures, including Message-Passing Neural Networks (MPNNs), Graph Convolutional Networks (GCNs), and Graph Isomorphism Networks (GINs), without requiring weight tying across layers.\n        *   **Hessian-based Stability Measure**: Introduces a novel approach to measure noise stability via the trace of the loss Hessian matrix, which is shown to correlate accurately with observed generalization gaps. This technique involves a uniform convergence analysis of the Hessian matrix.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Sharp Generalization Bounds (Theorem 3.1)**: Proves PAC-Bayesian generalization bounds for `l`-layer GNNs (MPNN, GCN, GIN) that scale with the spectral norm of `P_G^(l-1)`, where `P_G` is the graph diffusion matrix. These bounds are numerically orders of magnitude smaller than prior `d^(l-1)` bounds.\n        *   **Matching Lower Bound (Theorem 3.2)**: Constructs a lower bound instance where the generalization gap asymptotically matches their upper bound, demonstrating the tightness of their results.\n        *   **Hessian-based Generalization Measure (Lemma 4.3)**: Shows that the trace of the loss Hessian matrix can accurately measure the noise stability and, consequently, the generalization of GNNs. This applies to twice-differentiable and Lipschitz-continuous activations.\n        *   **Hessian Regularization Algorithm**: Proposes an algorithm that performs gradient updates on perturbed weight matrices, effectively minimizing the average loss of multiple perturbed models, which is equivalent to regularizing the GNN's Hessian in expectation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        1.  **Numerical Comparison of Bounds**: Evaluated the numerical values of their spectral norm-based bounds against prior maximum degree-based bounds for GCNs and MPNNs with varying layers.\n        2.  **Correlation of Hessian Measure**: Measured the Hessian-based generalization measure and compared it with empirically observed generalization gaps of GNNs.\n        3.  **Fine-tuning Algorithm Performance**: Applied their Hessian regularization algorithm for fine-tuning pretrained GNNs on graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Bounds Comparison**: On real-world graphs (IMDB-B, IMDB-M, COLLAB, REDDIT-B, REDDIT-M), the spectral norm bounds were found to be orders of magnitude smaller (e.g., `10^3` to `10^27` times smaller) than prior `d^(l-1)` bounds, even when accounting for weight norms (Figure 1a, Figure 2). For GCNs using normalized adjacency matrices, the graph dependence reduces to 1, providing exponential improvement.\n        *   **Hessian Measure Correlation**: The Hessian-based generalization measure accurately correlated with the empirically observed generalization gaps of GNNs (Figure 1b), demonstrating its practical utility as a diagnostic tool.\n        *   **Fine-tuning Performance**: The proposed Hessian regularization algorithm improved test performance on several graph-level classification tasks using Molecular graphs, demonstrating its practical benefit in robust fine-tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The theoretical analysis for the Hessian-based measure specifically applies to GNNs with twice-differentiable and Lipschitz-continuous activation functions (e.g., tanh, sigmoid).\n    *   **Scope of Applicability**: Primarily focuses on graph-level prediction tasks, though the authors note that the results permit extension to node prediction tasks. The unified model covers a broad range of common GNN architectures.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ju2023prm} significantly advances the theoretical understanding of generalization in GNNs by providing the first sharp, non-vacuous generalization bounds that scale with the spectral norm of the graph diffusion matrix, rather than the often-large maximum degree. This offers a more realistic and tighter quantification of GNN complexity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a practical, Hessian-based tool for measuring GNN generalization, which can guide model design and hyperparameter tuning.\n        *   Inspires the development of robust fine-tuning methods for GNNs by leveraging stability properties, as demonstrated by their proposed algorithm.\n        *   The developed tools and analysis techniques (e.g., uniform convergence of Hessians) may be useful for studying other aspects of GNNs, such as extrapolation to different graph sizes.",
        "year": 2023,
        "citation_key": "ju2023prm"
      },
      {
        "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
        "abstract": "Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users’ trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
        "summary": "This paper, \"A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability\" by Dai et al. \\cite{dai2022hsi}, provides a detailed review of the critical aspects of trustworthiness in Graph Neural Networks (GNNs).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Despite the rapid advancements and widespread application of GNNs in high-stakes scenarios (e.g., financial analysis, drug discovery), they suffer from significant trustworthiness issues. These include vulnerability to privacy leakage, adversarial attacks, inherent biases leading to unfair outcomes, and a lack of interpretability \\cite{dai2022hsi}.\n    *   **Importance and Challenge:** These issues pose substantial risks of unintentional harm to users and society, severely limiting the adoption of GNNs in critical real-world applications. The unique complexities of graph topology and the message-passing mechanism in GNNs mean that trustworthy AI solutions designed for independent and identically distributed (i.i.d) data are often not directly applicable, necessitating dedicated research into trustworthy GNNs \\cite{dai2022hsi}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a comprehensive survey, building upon existing, more narrowly focused surveys on GNN robustness, explainability, and fairness \\cite{dai2022hsi}. It also acknowledges general trustworthy AI surveys, but highlights their limitations regarding graph-structured data \\cite{dai2022hsi}.\n    *   **Limitations of Previous Solutions:**\n        *   Prior GNN-specific surveys often lack comprehensive coverage, omitting crucial dimensions like privacy and fairness, or failing to include emerging techniques (e.g., scalable attacks, backdoor attacks, self-explainable GNNs) \\cite{dai2022hsi}.\n        *   Surveys on trustworthy AI systems primarily focus on i.i.d data, making their findings less relevant or directly transferable to GNNs due to the distinct challenges posed by graph topology and message-passing mechanisms \\cite{dai2022hsi}.\n        *   Compared to a concurrent survey on trustworthy GNNs, this work covers more recent advanced topics such as machine unlearning, model ownership verification, scalable adversarial attacks, fair contrastive learning, explanation-enhanced fairness, and self-explainable GNNs \\cite{dai2022hsi}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** As a survey paper, the core method involves systematically reviewing, categorizing, and synthesizing the vast literature on GNN trustworthiness. For each of the four aspects (privacy, robustness, fairness, explainability), the paper provides:\n        *   A detailed taxonomy of existing methods \\cite{dai2022hsi}.\n        *   Formulation of general frameworks for different categories of trustworthy GNNs \\cite{dai2022hsi}.\n        *   Discussions on future research directions and the interconnections between these aspects \\cite{dai2022hsi}.\n    *   **Novelty/Difference:** The innovation lies in its *holistic and up-to-date coverage* of all four critical trustworthiness dimensions specifically for GNNs, addressing a significant gap in the literature. It provides a structured and comprehensive overview, including recent advancements, and explicitly discusses the interdependencies between privacy, robustness, fairness, and explainability, which is crucial for achieving truly trustworthy GNNs \\cite{dai2022hsi}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The paper does not propose new algorithms but contributes by:\n        *   Presenting a comprehensive taxonomy of privacy attacks (membership inference, property inference, reconstruction, model extraction) and defense methods on GNNs \\cite{dai2022hsi}.\n        *   Categorizing various adversarial attack and defense methods for GNN robustness, including recent advances like scalable attacks, graph backdoor attacks, and self-supervised learning defenses \\cite{dai2022hsi}.\n        *   Thoroughly discussing fairness in GNNs, covering biases, fairness definitions on graph data, and various fair GNN models \\cite{dai2022hsi}.\n        *   Providing a taxonomic summary of methodologies for GNN explainability, detailing motivations, challenges, and experimental settings \\cite{dai2022hsi}.\n    *   **Theoretical Insights or Analysis:** The paper offers insights into the unique challenges of achieving trustworthiness in GNNs due to their message-passing mechanism and graph topology. It highlights the interconnectedness of privacy, robustness, fairness, and explainability, suggesting that advancements in one area can inform or impact others (e.g., explanations aiding in debugging adversarial attacks or evaluating bias) \\cite{dai2022hsi}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, no new experiments were conducted.\n    *   **Key Performance Metrics and Comparison Results:** The paper summarizes the experimental validation from the reviewed literature. It lists relevant graph datasets used in privacy research and discusses the applications and datasets employed by various fair GNN models, providing context for the empirical findings of the surveyed works \\cite{dai2022hsi}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The survey explicitly focuses on the computational aspects of trustworthiness (privacy, robustness, fairness, explainability) and does not delve into the \"Respect for human autonomy\" principle, which is more aligned with human-computer interaction \\cite{dai2022hsi}. While it discusses connections, a unified framework for simultaneously addressing all four aspects remains an open challenge, which the paper identifies as a future direction \\cite{dai2022hsi}.\n    *   **Scope of Applicability:** The survey's scope is strictly limited to Graph Neural Networks and their trustworthiness, covering various GNN architectures and graph analysis tasks (node classification, link prediction, graph classification, community detection) \\cite{dai2022hsi}.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** This survey significantly advances the state-of-the-art by providing a consolidated, comprehensive, and up-to-date resource on trustworthy GNNs. It systematically organizes fragmented knowledge, clarifies the current landscape, and highlights emerging research directions, thereby serving as a foundational reference for the field \\cite{dai2022hsi}.\n    *   **Potential Impact on Future Research:** It is expected to guide future research by identifying critical gaps and challenges in privacy, robustness, fairness, and explainability for GNNs. By emphasizing the interconnections between these aspects, it encourages the development of more holistic and integrated solutions for building truly trustworthy GNN systems, moving beyond isolated problem-solving \\cite{dai2022hsi}.",
        "year": 2022,
        "citation_key": "dai2022hsi"
      }
    ],
    "layer3_papers": [
      {
        "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
        "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
        "year": 2024,
        "citation_key": "li202444f"
      }
    ],
    "layer2_summary": null
  },
  "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0": {
    "seed_title": "E(n) Equivariant Graph Neural Networks",
    "summary": "*Evolution Analysis:*\n\nThe provided chain of papers reveals two major, interconnected trends in the evolution of Graph Neural Networks: **1) Deepening Expressivity and Robustness through Geometric and Structural Symmetries** and **2) Broadening the Application Landscape of GNNs to Complex, Non-Euclidean Data**. These trends collectively push GNNs towards more powerful, stable, and versatile models capable of tackling increasingly complex real-world problems.\n\n### Trend 1: Deepening Expressivity and Robustness through Geometric and Structural Symmetries\n\nThis trend focuses on enhancing the fundamental capabilities of GNNs by rigorously incorporating geometric information and respecting inherent symmetries, leading to more robust and expressive models.\n\n*   **Methodological progression**: The journey begins with practical architectural designs for geometric equivariance, then moves to principled ways to inject identity and positional information, followed by a rigorous theoretical understanding of geometric expressivity, and culminates in novel hybrid architectures designed to overcome inherent limitations like over-squashing.\n\n*   **Problem evolution**:\n    *   Initially, the challenge was to develop GNNs that are efficiently and scalably equivariant to Euclidean transformations (rotations, translations, reflections, permutations) for 3D structures, moving beyond computationally expensive higher-order representations or methods limited to 3D. This was addressed by **[satorras2021pzl] E(n) Equivariant Graph Neural Networks (2021)**.\n    *   Subsequently, GNNs faced limitations in distinguishing nodes that are matched under graph automorphism, leading to poor performance on node-set tasks. Existing positional encoding (PE) methods were unstable, particularly with multiple eigenvalues or small eigengaps, and lacked guaranteed permutation equivariance. **[wang2022p2r] Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks (2022)** tackled this by introducing a principled and provably stable PE mechanism.\n    *   As geometric GNNs gained traction, a critical gap emerged: the lack of a theoretical framework to characterize their expressive power, analogous to the Weisfeiler-Leman (WL) test for non-geometric graphs, especially concerning the distinct roles of invariant versus equivariant layers. **[joshi20239d0] On the Expressive Power of Geometric Graph Neural Networks (2023)** filled this void.\n    *   Finally, a fundamental limitation of spatial Message Passing GNNs (MPGNNs) — their limited receptive field and the \"over-squashing\" phenomenon, which severely restricts long-range information exchange and expressivity — became a major hurdle. **[geisler2024wli] Spatio-Spectral Graph Neural Networks (2024)** directly confronted this.\n\n*   **Key innovations**:\n    *   **[satorras2021pzl]** introduced the Equivariant Graph Neural Network (EGNN) with its Equivariant Graph Convolutional Layer (EGCL), which directly updates coordinates using relative differences, avoiding spherical harmonics and scaling to N-dimensions with a simpler architecture.\n    *   **[wang2022p2r]** proposed the PEG architecture, which uses separate channels for node and positional features, imposing O(p) equivariance for positional features and achieving provable stability by depending on a larger eigengap.\n    *   **[joshi20239d0]** developed the Geometric Weisfeiler-Leman (GWL) test, a symmetry-aware generalization of the WL test for geometric graphs, providing a theoretical upper bound for expressivity and insights into the power of equivariant layers and higher-order tensors.\n    *   **[geisler2024wli]** unveiled Spatio-Spectral Graph Neural Networks (S2GNNs), a novel hybrid architecture that synergistically combines local spatial message passing with global spectral filtering. This innovation provably vanquishes over-squashing, offers superior approximation bounds, and provides stable positional encodings \"for free.\"\n\n### Trend 2: Broadening the Application Landscape of GNNs to Complex, Non-Euclidean Data\n\nThis trend highlights the expansion of GNNs into diverse scientific and real-world domains, often requiring innovative approaches to represent data as graphs.\n\n*   **Methodological progression**: This trend is characterized by the *application* and *adaptation* of GNNs (often Message Passing Neural Networks) to new domains. A significant methodological focus within this trend is on how to *construct* the graph representation when it is not explicitly given, and on developing taxonomies and best practices for domain-specific GNN application.\n\n*   **Problem evolution**:\n    *   Traditional machine learning methods for chemistry and materials science relied on hand-crafted features, lacked end-to-end learning, and struggled to directly process graph-structured data with inherent physical symmetries. **[reiser2022b08] Graph neural networks for materials science and chemistry (2022)**, a review, consolidated the shift towards GNNs as a solution.\n    *   Traditional time series analysis methods struggled to explicitly model complex inter-temporal and inter-variable *spatial* relationships in multivariate time series. There was also a lack of a comprehensive survey on GNNs for time series. **[jin2023ijy] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection (2023)** addressed this by providing a structured overview.\n    *   Mechanistic epidemic models suffered from oversimplified assumptions, and general deep learning models (CNNs, RNNs) failed to incorporate crucial *relational data* (e.g., human mobility, contact tracing) for accurate epidemic forecasting. **[liu20242g6] A Review of Graph Neural Networks in Epidemic Modeling (2024)** provided the first comprehensive review of GNNs in this critical public health domain.\n\n*   **Key innovations**:\n    *   **[reiser2022b08]** (as a review) highlighted the Message Passing Neural Network (MPNN) framework as foundational, emphasizing the integration of geometric data and symmetry-equivariant representations (building on works like [satorras2021pzl]) and periodic extensions for crystal structures in materials science.\n    *   **[jin2023ijy]** (as a review) introduced a novel taxonomy for GNNs in time series (GNN4TS) and provided a detailed categorization of heuristic-based and learning-based graph construction methods, which are crucial for applying GNNs where graph structures are implicit.\n    *   **[liu20242g6]** (as a review) presented novel hierarchical taxonomies for epidemic tasks and GNN methodologies, along with a systematic examination of existing methods and graph construction techniques tailored for epidemic modeling.\n\n### Synthesis\n\nThese works collectively demonstrate a powerful intellectual trajectory in Graph Neural Networks: moving from foundational architectural innovations that respect fundamental physical symmetries to a rigorous theoretical understanding of their expressive power, and then to sophisticated hybrid models that overcome inherent limitations. Simultaneously, the field is rapidly expanding the applicability of GNNs to diverse, complex domains by developing systematic approaches for graph construction and domain-specific adaptations. Their collective contribution is the advancement of GNNs into a more mature, theoretically grounded, and broadly applicable paradigm for machine learning on structured data, capable of addressing previously intractable problems in science and engineering.",
    "path": [
      "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "5e6db511e736f77f844bbeebaa2b177427abada1",
      "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f"
    ],
    "layer1_papers": [
      {
        "title": "E(n) Equivariant Graph Neural Networks",
        "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",
        "summary": "Here's a focused summary of the paper \"E(n) Equivariant Graph Neural Networks\" \\cite{satorras2021pzl} for a literature review:\n\n---\n\n### E(n) Equivariant Graph Neural Networks \\cite{satorras2021pzl}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Developing Graph Neural Networks (GNNs) that are equivariant to Euclidean transformations (rotations, translations, reflections, and permutations) in n-dimensional spaces, particularly for tasks involving 3D structures (e.g., molecules, point clouds, N-body systems).\n    *   **Importance & Challenge**: Many physical and geometric problems exhibit these symmetries, and enforcing equivariance acts as a powerful inductive bias, improving model generalization and data efficiency. Existing methods often rely on computationally expensive higher-order representations (e.g., spherical harmonics) and are typically limited to 3-dimensional spaces, making them less scalable and efficient.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Higher-order representations**: Methods like Tensor Field Networks (TFN) \\cite{satorras2021pzl} and SE(3) Transformer \\cite{satorras2021pzl} use spherical harmonics to handle transformations, allowing for higher-order representations.\n        *   **Lie Algebra methods**: Parametrize transformations by mapping kernels on the Lie Algebra \\cite{satorras2021pzl}.\n        *   **Radial Field**: An E(n) equivariant network for 3D point clouds, but limited to positional data without propagating node features \\cite{satorras2021pzl}.\n        *   **E(n)-invariant GNNs**: Methods like Schnet \\cite{satorras2021pzl} achieve E(n) invariance by using relative distances, but do not maintain equivariance for vector outputs.\n    *   **Limitations of Previous Solutions**:\n        *   **Computational expense**: Higher-order representations (e.g., spherical harmonics) require costly computations.\n        *   **Dimensionality restriction**: Many methods are limited to 3-dimensional spaces (E(3) or SE(3) equivariance).\n        *   **Scope**: Some methods only handle positional data or achieve invariance rather than full equivariance for vector quantities.\n    *   **Positioning**: The proposed E(n)-Equivariant Graph Neural Network (EGNN) offers a simpler architecture that avoids spherical harmonics, scales to higher dimensions, and achieves competitive or superior performance compared to existing methods.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method (Equivariant Graph Convolutional Layer - EGCL)**:\n        *   **Node Features & Coordinates**: Operates on both node embeddings ($h_i$) and n-dimensional coordinates ($x_i$).\n        *   **Edge Operation (Eq. 3)**: Computes messages ($m_{ij}$) by taking as input the node embeddings ($h_i, h_j$), edge attributes ($a_{ij}$), and crucially, the *squared relative distance* between coordinates ($||x_i - x_j||^2$). This ensures E(n) invariance for messages.\n        *   **Coordinate Update (Eq. 4)**: Updates the position of each particle ($x_i$) by adding a weighted sum of relative differences ($x_i - x_j$) from neighbors. The weights are derived from the edge embeddings ($m_{ij}$) via a scalar function $\\phi_x$. This operation is key to preserving E(n) equivariance for coordinates.\n        *   **Node Embedding Update (Eqs. 5 & 6)**: Aggregates messages ($m_i$) and updates node embeddings ($h_i$) in a standard GNN fashion, ensuring $h_i$ remains E(n) invariant.\n    *   **Novelty**:\n        *   **Direct Coordinate Update**: Integrates coordinate updates directly into the message-passing framework using relative differences, which naturally preserves E(n) equivariance without complex transformations.\n        *   **Avoidance of Higher-Order Representations**: Does not require spherical harmonics or other computationally expensive higher-order representations, simplifying the model.\n        *   **Scalability to N-dimensions**: The formulation is inherently applicable to arbitrary n-dimensional spaces, unlike many prior E(3)-specific methods.\n        *   **Simplicity and Flexibility**: Achieves strong equivariance with a relatively simple set of equations, allowing the edge embedding to carry rich information.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of the E(n)-Equivariant Graph Neural Network (EGNN) architecture, specifically the Equivariant Graph Convolutional Layer (EGCL).\n    *   **Efficiency & Scalability**: A method for achieving E(n) equivariance that avoids computationally expensive higher-order representations and is easily scaled to higher-dimensional spaces.\n    *   **System Design**: A message-passing framework that simultaneously updates both invariant node features and equivariant coordinates, with information exchange between them.\n    *   **Extensions**:\n        *   A variant for explicitly tracking and updating particle velocities, useful for dynamical systems.\n        *   A mechanism for inferring graph edges when not explicitly provided, by learning soft edge values.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Dynamical Systems Modeling (N-body system)**: Forecasting particle positions in a 3D charged N-body system over 1000 timesteps.\n        *   (Mentioned in abstract but not detailed in provided text: Representation learning in graph autoencoders, predicting molecular properties on QM9 dataset).\n    *   **Key Performance Metrics**: Mean Squared Error (MSE) for position prediction, and average forward pass time.\n    *   **Comparison Results (N-body system)**:\n        *   **EGNN achieved the lowest MSE (0.0071)**, significantly outperforming:\n            *   Linear baseline (0.0819)\n            *   SE(3) Transformer (0.0244)\n            *   Tensor Field Network (0.0155)\n            *   Graph Neural Network (non-equivariant) (0.0107)\n            *   Radial Field (0.0104)\n        *   **Forward Pass Time**: EGNN had a competitive forward pass time (0.0062s), faster than SE(3) Transformer (0.1346s) and TFN (0.0343s), and comparable to GNN (0.0032s) and Radial Field (0.0039s).\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The proof of equivariance relies on the assumption that initial node features ($h_l$) are E(n) invariant for the subsequent $h_{l+1}$ to remain E(n) invariant.\n    *   **General GNN Scaling**: While the core EGNN mechanism scales to higher dimensions, the general challenge of scaling fully connected graphs to very large point clouds (due to message aggregation) remains, though the paper proposes an edge inference mechanism to address this.\n    *   **Scope of Applicability**: Primarily designed for tasks where E(n) equivariance is desired for both scalar (type-0) and vector (type-1) representations, such as physical simulations, molecular dynamics, and 3D geometric learning.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Provides a more efficient, simpler, and scalable approach to E(n) equivariant GNNs compared to methods relying on complex higher-order representations or being restricted to 3D.\n    *   **Improved Performance**: Demonstrates superior or competitive performance on challenging equivariant tasks like N-body simulations.\n    *   **Potential Impact**: Offers a robust and practical framework for developing deep learning models in fields requiring geometric reasoning and symmetry exploitation, including physics, chemistry, materials science, and robotics, by enabling more accurate and generalizable predictions.",
        "year": 2021,
        "citation_key": "satorras2021pzl"
      }
    ],
    "layer2_papers": [
      {
        "title": "On the Expressive Power of Geometric Graph Neural Networks",
        "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
        "summary": "Here's a focused summary of the paper by \\cite{joshi20239d0} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: On the Expressive Power of Geometric Graph Neural Networks \\cite{joshi20239d0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Characterizing and understanding the expressive power of Graph Neural Networks (GNNs) for *geometric graphs* embedded in Euclidean space. Standard GNNs and the Weisfeiler-Leman (WL) test, widely used for non-geometric graphs, are inapplicable to these systems.\n    *   **Importance & Challenge**: Geometric graphs, such as biomolecules, materials, and physical simulations, inherently possess both relational structure and geometric attributes (e.g., 3D coordinates, velocities). These geometric attributes must respect physical symmetries (permutations, rotation, reflection, translation). Standard GNNs fail to account for these symmetries, leading to a loss of physical meaning and transformation behavior. The challenge lies in developing a theoretical framework that can assess how well geometric GNNs can distinguish between geometrically non-isomorphic graphs while respecting these symmetries.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the established Weisfeiler-Leman (WL) graph isomorphism test, which has been a powerful tool for analyzing the expressive power of non-geometric GNNs.\n    *   **Limitations of Previous Solutions**: The standard WL framework and non-geometric GNNs do not directly apply to geometric graphs because they lack the ability to account for spatial symmetries. Geometric graphs exhibit a stronger notion of \"geometric isomorphism\" that requires invariance or equivariance to Euclidean transformations, which the WL test does not capture. Existing geometric GNNs (both G-equivariant and G-invariant) have shown empirical success, but their theoretical expressive power remained largely uncharacterized.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Geometric Weisfeiler-Leman (GWL) test**, a novel generalization of the WL test designed for discriminating geometric graphs while respecting physical symmetries.\n        *   GWL iteratively updates node \"colors\" and auxiliary \"geometric objects\" (`g(t)_i`) that aggregate geometric information from progressively larger t-hop neighborhoods.\n        *   It employs a **G-orbit injective and G-invariant function (I-HASH)** to compute scalar node colors, ensuring that neighborhoods identical up to a group action receive the same color.\n        *   Crucially, the aggregation of geometric objects (`g(t)_i`) is designed to be **injective and G-equivariant**, preserving local geometric orientation and information.\n    *   **Novelty/Difference**:\n        *   **Symmetry-Aware Isomorphism Test**: GWL is the first theoretical framework to extend the WL test to geometric graphs, explicitly incorporating invariance/equivariance to permutations, rotations, reflections, and translations.\n        *   **Granular Expressivity Analysis**: Unlike binary universal approximation, GWL provides a discrimination-based perspective, offering a more granular and practically insightful lens into geometric GNN expressivity by linking it to the ability to distinguish geometric (sub-)graphs.\n        *   **Unified Framework**: It provides a theoretical upper bound for the expressive power of both G-equivariant and G-invariant geometric GNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the **Geometric Weisfeiler-Leman (GWL) test** as a theoretical upper bound for the expressive power of geometric GNNs.\n    *   **Theoretical Insights/Analysis**:\n        *   **Characterization of Expressivity**: GWL formally characterizes how key design choices influence geometric GNN expressivity:\n            *   **Invariant Layers' Limitations**: Invariant GNNs (and the proposed Invariant GWL, IGWL) have limited expressivity; they cannot distinguish \"1-hop identical\" geometric graphs and fail to compute non-local geometric properties (e.g., volume, centroid).\n            *   **Equivariant Layers' Power**: Equivariant GNNs (and GWL) distinguish a larger class of graphs by propagating geometric information beyond local neighborhoods through stacking equivariant layers.\n            *   **Role of Higher-Order Tensors/Scalarization**: Higher-order tensors and scalarization (e.g., `IGWL(k)` with higher body order `k`) are shown to enable maximally powerful geometric GNNs by providing more complete descriptors of local geometry (e.g., beyond just distances and angles).\n            *   **Depth**: Increasing depth (number of layers) allows GWL and equivariant GNNs to aggregate information from larger k-hop neighborhoods, distinguishing k-hop distinct graphs.\n        *   **Equivalence to Universal Approximation**: The paper proves an equivalence between a model's ability to discriminate geometric graphs (via GWL) and its universal approximation capabilities for G-invariant functions.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Synthetic experiments were performed to demonstrate the practical implications of the theoretical findings. These experiments are available at `https://github.com/chaitjo/geometric-gnn-dojo`.\n    *   **Key Performance Metrics & Comparison Results**: The experiments focused on illustrating:\n        *   **Geometric Oversquashing with Increased Depth**: Demonstrating how increasing depth in geometric GNNs can lead to oversquashing, a phenomenon where distinct geometric information becomes indistinguishable.\n        *   **Utility of Higher Order Spherical Tensors**: Providing counterexamples that highlight the necessity and utility of higher-order spherical tensors for distinguishing complex local geometries that lower-order invariants (like distances and angles) cannot resolve. This empirically supports the theoretical claim about the importance of scalarization body order.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   GWL assumes finite-sized geometric graphs and features drawn from countable datasets.\n        *   The `HASH` and `I-HASH` functions are idealized injective and G-orbit injective maps, respectively, which are not necessarily continuous in practical implementations.\n        *   The theoretical analysis focuses on the *upper bound* of expressivity, and achieving this bound in practical GNNs requires specific conditions on aggregation, update, and readout functions (e.g., injectivity, equivariance).\n    *   **Scope of Applicability**: The framework is applicable to geometric graphs embedded in Euclidean space, particularly relevant for systems in biochemistry, material science, and physical simulations where physical symmetries are crucial. It primarily focuses on distinguishing geometric graphs and characterizing the expressive power of GNNs in this context.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the theoretical understanding of geometric GNNs by providing the first principled framework (GWL) to analyze their expressive power in a symmetry-aware manner. It extends the foundational WL test to a critical domain where standard GNNs fall short.\n    *   **Potential Impact on Future Research**:\n        *   **Guiding Architecture Design**: The insights derived from GWL (e.g., the importance of equivariant layers, higher-order tensors, and understanding depth limitations) can directly guide the design of more powerful and expressive geometric GNN architectures.\n        *   **Benchmarking and Evaluation**: GWL provides a theoretical benchmark against which the expressive power of new geometric GNN models can be rigorously evaluated.\n        *   **Deeper Theoretical Understanding**: It offers a more granular and practically relevant perspective on expressivity compared to universal approximation, fostering deeper theoretical investigations into the capabilities and limitations of geometric deep learning models.",
        "year": 2023,
        "citation_key": "joshi20239d0"
      },
      {
        "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
        "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and imposes $O(p)$ (orthogonal group) equivariance w.r.t. the positional features simultaneously, where $p$ is the dimension of used positional features. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Graph Neural Networks (GNNs) often perform poorly on tasks based on sets of nodes (e.g., link prediction, motif prediction, relation prediction) compared to single-node or whole-graph tasks \\cite{wang2022p2r}.\n    *   This failure stems from GNNs' intrinsic inability to distinguish nodes that are matched under graph automorphism, leading to a loss of node identities \\cite{wang2022p2r}.\n    *   Existing solutions using augmented node features (random features or deterministic distance encoding) suffer from slow convergence, inaccurate predictions, high computational complexity, or lack of generalizability and stability \\cite{wang2022p2r}.\n    *   Specifically, previous Positional Encoding (PE) methods, which use absolute node positions, struggle to guarantee permutation equivariance and stability, especially when graphs have multiple eigenvalues or small eigengaps, making them unreliable for practical networks \\cite{wang2022p2r}.\n\n*   **Related Work & Positioning**\n    *   **Random Features (RF)**: Augment GNNs with random node features to distinguish nodes and guarantee permutation equivariance. Limitations: often hard to converge, noisy, and inaccurate due to injected randomness \\cite{wang2022p2r}.\n    *   **Deterministic Distance Encoding (DE)**: Define extra features based on distances from a node to a target node set. Limitations: theoretically sound and empirically strong, but introduces huge memory and time consumption as features are sample-specific and cannot be shared \\cite{wang2022p2r}.\n    *   **Previous Positional Encoding (PE) Methods**:\n        *   Randomize PE (e.g., distances to random anchor nodes) to ensure permutation equivariance. Limitations: slow convergence, subpar performance \\cite{wang2022p2r}.\n        *   Use eigenvectors of randomly permuted graph Laplacian, or perturb signs of eigenvectors. Limitations: problematic when the Laplacian matrix has multiple eigenvalues (common in real-world graphs), and unstable even with distinct eigenvalues if eigengaps are small (sensitivity depends on the inverse of the smallest eigengap) \\cite{wang2022p2r}.\n    *   **Positioning**: The work by \\cite{wang2022p2r} proposes a principled solution to address the instability and lack of equivariance in GNNs using PE, particularly for large graphs and smaller training sets, where previous methods are conjectured to fail more severely.\n\n*   **Technical Approach & Innovation**\n    *   **Core Idea**: `PEG` (Positional Encoding GNN) uses separate channels to update original node features and positional features \\cite{wang2022p2r}.\n    *   **Equivariance Properties**: `PEG` imposes permutation equivariance with respect to the original node features and simultaneously imposes O(p) (orthogonal group) equivariance with respect to the p-dimensional positional features \\cite{wang2022p2r}. This O(p) equivariance allows for rotation and reflection of positional features without changing their meaning, addressing the non-uniqueness of eigenvectors.\n    *   **Stability Mechanism**: For Laplacian Eigenmap (LE) as PE, `PEG` is provably stable. Its sensitivity to graph perturbation depends only on the gap between the p-th and (p+1)-th eigenvalues of the graph Laplacian, rather than the smallest gap between any two consecutive eigenvalues (which is the case for previous methods) \\cite{wang2022p2r}. This significantly improves stability, especially when small eigengaps exist among the first `p` eigenvalues.\n    *   **Generalizability**: The approach applies to a broad range of PE techniques formulated as matrix factorization (e.g., Laplacian Eigenmap, Deepwalk) \\cite{wang2022p2r}.\n\n*   **Key Technical Contributions**\n    *   **Novel GNN Layer (`PEG`)**: Introduction of a new class of GNN layers that explicitly handles positional features in separate channels, ensuring both permutation equivariance for node features and O(p) equivariance for positional features \\cite{wang2022p2r}.\n    *   **Rigorous Mathematical Analysis**: Provides a principled study of equivariance and stability issues in GNNs with PE, including a formal definition of PE-stability and PE-equivariance \\cite{wang2022p2r}.\n    *   **Provable Stability**: Mathematical proof that `PEG` is provably stable, with its sensitivity to graph perturbations depending on a much larger eigengap (between `p` and `p+1` eigenvalues) compared to previous methods (which depend on the smallest eigengap among the first `p+1` eigenvalues) \\cite{wang2022p2r}.\n    *   **Addressing Eigenvector Ambiguity**: The O(p) equivariance directly addresses the non-uniqueness of eigenvectors when multiple eigenvalues exist, a critical limitation of prior PE methods \\cite{wang2022p2r}.\n\n*   **Experimental Validation**\n    *   **Task**: Extensive link prediction experiments \\cite{wang2022p2r}.\n    *   **Datasets**: Evaluated over 8 real-world networks \\cite{wang2022p2r}.\n    *   **Performance Metrics**: Not explicitly stated in the provided text, but implied to be accuracy/performance for link prediction.\n    *   **Key Results**:\n        *   `PEG` achieves comparable performance to strong baselines based on Deterministic Distance Encoding (DE) while having significantly lower training and inference complexity \\cite{wang2022p2r}.\n        *   `PEG` significantly outperforms other baselines that do not use DE \\cite{wang2022p2r}.\n        *   The performance gap is further enlarged in domain-shift link prediction scenarios (training and testing on networks from different domains), demonstrating `PEG`'s strong generalization and transferability \\cite{wang2022p2r}.\n\n*   **Limitations & Scope**\n    *   **Experiment Setting**: The paper conjectures that the instability issues of previous models become more severe for larger graphs and smaller training sets, which is the focus of their experiments. An extensive study of this point is left for future work \\cite{wang2022p2r}.\n    *   **PE Techniques**: While applicable to matrix factorization-based PE (LE, Deepwalk), the generalizability to *all* PE techniques is not explicitly stated \\cite{wang2022p2r}.\n    *   **Node-Set Tasks**: The primary focus is on node-set-based tasks, with link prediction as the main experimental validation \\cite{wang2022p2r}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `PEG` provides a principled and provably stable method for incorporating positional encoding into GNNs, overcoming critical limitations of previous approaches related to equivariance and stability, especially for graphs with complex eigenvalue structures \\cite{wang2022p2r}.\n    *   **Improved Generalization and Scalability**: Demonstrates superior generalization and transferability, particularly in domain-shift scenarios, and achieves competitive performance with significantly lower computational cost compared to DE-based methods \\cite{wang2022p2r}.\n    *   **Potential Impact**: Opens avenues for more reliable and powerful GNN applications in node-set-based tasks, which are prevalent in real-world scenarios (e.g., drug discovery, social network analysis). The mathematical framework for stability analysis of GNNs with PE is a significant theoretical contribution \\cite{wang2022p2r}.",
        "year": 2022,
        "citation_key": "wang2022p2r"
      },
      {
        "title": "Graph neural networks for materials science and chemistry",
        "abstract": "Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs. Graph neural networks are machine learning models that directly access the structural representation of molecules and materials. This Review discusses state-of-the-art architectures and applications of graph neural networks in materials science and chemistry, indicating a possible road-map for their further development.",
        "summary": "This paper, a review article by \\cite{reiser2022b08}, provides a comprehensive overview of Graph Neural Networks (GNNs) in chemistry and materials science.\n\n### 1. Research Problem & Motivation\n*   **Specific Technical Problem:** The paper addresses the challenge of effectively leveraging machine learning for tasks in chemistry and materials science, such as predicting material properties, accelerating simulations, designing new materials, and predicting synthesis routes. Traditional machine learning methods often rely on hand-crafted feature representations, which can be labor-intensive and may not fully capture the complex structural and geometric information inherent in molecules and materials.\n*   **Importance and Challenge:** This problem is crucial because it can significantly accelerate the materials development cycle, from discovery to synthesis. It is challenging due to the irregular, graph-structured nature of molecular and material data (atoms and bonds), which traditional deep learning models (like CNNs for grid-like data) are not inherently designed to handle. Capturing complete atomic-level representations, incorporating physical laws, and dealing with various scales (from atomic to larger phenomena like doping) are key difficulties.\n\n### 2. Related Work & Positioning\n*   **Relation to Existing Approaches:** This work positions GNNs as a powerful alternative and advancement over conventional machine learning models (e.g., decision tree ensembles, densely connected neural networks, random forests, Gaussian process regression) that typically require predefined feature representations (e.g., compositional or fixed-sized vectors, or descriptors like ACSF, SOAP, MBTR). It also relates GNNs to Convolutional Neural Networks (CNNs) by interpreting them as a generalization of CNNs to irregular graph structures.\n*   **Limitations of Previous Solutions:**\n    *   **Hand-crafted Features:** Previous methods often rely on hand-crafted feature representations, which can be incomplete, less flexible, and may not capture all relevant information, especially geometric dependencies.\n    *   **Lack of End-to-End Learning:** Many traditional approaches separate feature engineering from model training, limiting end-to-end optimization.\n    *   **Inability to Directly Process Graph Data:** Conventional deep learning models are not naturally suited for graph-structured data, requiring conversion or flattening that can lose topological information.\n    *   **Data Requirements:** While GNNs can require significant data, the paper notes that traditional methods with hand-crafted features often struggle to learn complex relationships without extensive feature engineering.\n\n### 3. Technical Approach & Innovation\n*   **Core Technical Method:** The paper focuses on **Message Passing Graph Neural Networks (MPNNs)** as the unifying framework for most GNNs in chemistry and materials science \\cite{reiser2022b08}.\n    *   MPNNs operate by iteratively propagating information (messages) between neighboring nodes (atoms) and updating node embeddings based on these messages.\n    *   This process is repeated for a fixed number of steps (K), allowing information to travel within the K-hop neighborhood.\n    *   Finally, a graph-level embedding is obtained by pooling all node embeddings, which is then used for prediction tasks (regression or classification).\n    *   The core operations involve learnable functions for message generation ($M_t$) and node updates ($U_t$), and a permutation-invariant readout function ($R$).\n*   **Novelty/Difference:**\n    *   **Direct Graph Processing:** GNNs directly operate on the natural graph representation of molecules and materials (atoms as nodes, bonds as edges), eliminating the need for extensive manual feature engineering.\n    *   **Representation Learning:** They learn informative molecular/material representations end-to-end, allowing the model to discover relevant features from the raw structural data.\n    *   **Incorporation of Geometric Information:** GNNs can flexibly incorporate geometric information (distances, angles) as additional edge or node features, or through specialized architectures, which is crucial for quantum-mechanical properties.\n    *   **Symmetry Awareness:** The approach emphasizes the importance of incorporating physical symmetries (rotational, translational, permutation, periodicity for crystals) into the model's representation or architecture, leading to more data-efficient and robust models.\n\n### 4. Key Technical Contributions\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   **Formalization of MPNNs:** The paper highlights the MPNN framework \\cite{reiser2022b08} as a foundational and widely applicable scheme for GNNs in this domain, detailing its message passing, node update, and readout phases.\n    *   **Integration of Geometric Data:** Discussion of how GNNs integrate geometric information (distances, bond/dihedral angles) as features, often expanded into Gaussian-like, radial, or spherical Fourier-Bessel functions, moving beyond purely topological graphs.\n    *   **Symmetry-Equivariant Representations:** Emphasis on the development of equivariant representations \\cite{reiser2022b08} that are invariant or equivariant under translation, rotation, and permutation operations, crucial for predicting tensorial properties and reducing data requirements.\n    *   **Periodic Extensions:** Introduction of periodic extensions of crystal graphs \\cite{reiser2022b08} to handle solid crystals and periodic structures, incorporating periodicity and space group symmetries.\n*   **System Design or Architectural Innovations (as discussed in the review):**\n    *   **Edge Updates and Skip Connections:** Mention of extensions like D-MPNN \\cite{reiser2022b08} (directed edge embeddings, message passing between edges) and the use of skip connections \\cite{reiser2022b08} to alleviate issues like over-smoothing.\n    *   **Attention Mechanisms:** Integration of masked self-attention layers, as seen in Graph Attention Networks (GATs) \\cite{reiser2022b08} and AttentiveFingerprint models \\cite{reiser2022b08}, to weigh the importance of neighboring nodes.\n    *   **Graph Pooling/Coarsening:** Discussion of algorithms \\cite{reiser2022b08} to reduce input representation and condense structural information for larger molecules.\n*   **Theoretical Insights or Analysis:**\n    *   **Generalization of CNNs:** GNNs are presented as a generalization of CNNs to irregular graph structures, providing a theoretical link to established deep learning paradigms.\n    *   **Addressing GNN Limitations:** Acknowledgment of open research questions regarding GNNs' limited expressive performance \\cite{reiser2022b08} (e.g., comparison to Weisfeiler-Lehman hierarchy \\cite{reiser2022b08}), over-squashing \\cite{reiser2022b08}, and over-smoothing \\cite{reiser2022b08}, and discussion of proposed solutions like hypergraph representations \\cite{reiser2022b08} and higher-order graph networks \\cite{reiser2022b08}.\n\n### 5. Experimental Validation\n*   **Experiments Conducted (as reported in the review):** The paper itself is a review, so it doesn't present new experimental results. However, it summarizes the empirical validation of GNNs by referencing:\n    *   **Benchmark Datasets:** A wide range of benchmark datasets are listed for both molecules (e.g., QM7, QM9, PDBBind, Tox21) and crystals (e.g., Materials Project (MP), Open Quantum Materials Database (OQMD), Open Catalyst Project (OC20)) \\cite{reiser2022b08}. These datasets are used for supervised tasks like regression (e.g., quantum calculations, protein binding affinity, formation energy) and classification (e.g., toxicity, blood-brain barrier penetration).\n    *   **Performance Comparison:** The review highlights that GNNs have \"outperformed conventional machine learning models in predicting molecular properties throughout the last years\" \\cite{reiser2022b08}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **QM9 Benchmark:** Figure 1c in \\cite{reiser2022b08} specifically shows the Mean Absolute Error (MAE) for the prediction of internal, HOMO, and LUMO energies on the QM9 dataset for various GNN models since 2017. This figure visually demonstrates the continuous improvement in prediction accuracy achieved by different GNN architectures over time for quantum properties.\n    *   **General Superiority:** The text generally states that GNNs show \"a systematic advantage over traditional feature-based methods\" \\cite{reiser2022b08} and have \"outperformed conventional machine learning models in predicting molecular properties\" \\cite{reiser2022b08}.\n\n### 6. Limitations & Scope\n*   **Technical Limitations or Assumptions:**\n    *   **Data Requirements:** GNNs often come at the cost of higher data requirements, potentially limiting their applicability to scenarios where large amounts of data are available \\cite{reiser2022b08}.\n    *   **Expressive Power:** A main open research question revolves around the limited expressive performance of GNNs for specific tasks and how they compare with the Weisfeiler-Lehman hierarchy for graph isomorphism testing \\cite{reiser2022b08}.\n    *   **Over-smoothing and Over-squashing:** Challenges like over-smoothing (indistinguishable representations of neighboring nodes) and over-squashing (distortion of information from long-range dependencies) limit the depth and effectiveness of message passing \\cite{reiser2022b08}.\n    *   **Training Instability:** Training instability \\cite{reiser2022b08} is also an ongoing research subject.\n*   **Scope of Applicability:** The review focuses specifically on applications in chemistry and materials science, primarily for predicting molecular and material properties, accelerating simulations, and aiding in material design. While GNNs have broader applications, this paper's scope is confined to these domains.\n\n### 7. Technical Significance\n*   **Advancement of State-of-the-Art:** This review highlights how GNNs significantly advance the technical state-of-the-art by providing an end-to-end, representation-learning approach that directly processes graph-structured data. This capability allows for more accurate and flexible modeling of complex chemical and material systems compared to traditional methods relying on hand-crafted features. The continuous improvement shown on benchmarks like QM9 \\cite{reiser2022b08} underscores their superior predictive power.\n*   **Potential Impact on Future Research:**\n    *   **Accelerated Discovery:** GNNs have the potential to revolutionize virtual materials design and accelerate the discovery of new materials and drugs by enabling more efficient property prediction and inverse design.\n    *   **Integration of Physics:** The flexibility to incorporate physical laws and symmetries into GNN architectures opens avenues for developing more physically informed and robust models.\n    *   **Addressing Current Limitations:** Ongoing research into improving expressive power, mitigating over-smoothing/over-squashing, and enhancing data efficiency will further broaden GNN applicability.\n    *   **New Architectures:** The field is ripe for developing novel GNN architectures that can better handle complex geometries, periodic boundary conditions, and multi-scale phenomena in materials science.",
        "year": 2022,
        "citation_key": "reiser2022b08"
      },
      {
        "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
        "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Traditional time series analysis methods (e.g., SVR, GBDT, VAR, ARIMA, CNN, RNN, Transformers) struggle to explicitly model complex inter-temporal and inter-variable relationships, particularly spatial relations in non-Euclidean data, which are prevalent in multivariate time series.\n    *   **Importance/Challenge**: Time series data is ubiquitous and critical across various domains (e.g., cloud computing, transportation, IoT). Accurately analyzing this data requires capturing intricate spatial-temporal dependencies, which existing methods often fail to do effectively, leading to less accurate results. Graph Neural Networks (GNNs) offer a promising avenue to address this by explicitly modeling these non-Euclidean relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: This paper is a comprehensive survey that reviews the burgeoning field of GNNs for time series analysis (GNN4TS).\n    *   **Limitations of previous solutions**: Prior time series analysis methods (traditional and deep learning) lack explicit modeling of non-Euclidean spatial relationships. Existing surveys on GNNs or spatial-temporal data are often limited in scope, focusing on specific domains (e.g., traffic, urban computing) or tasks (e.g., forecasting), and do not provide a holistic view of GNNs across the full spectrum of time series analysis tasks.\n    *   **Positioning**: This survey \\cite{jin2023ijy} aims to fill this gap by providing the *first comprehensive and up-to-date review* of GNN4TS, encompassing four fundamental tasks: forecasting, classification, anomaly detection, and imputation, without restricting to specific domains or tasks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (of the survey)**: The paper systematically reviews and categorizes GNN-based approaches for time series analysis. It defines key concepts like spatial-temporal graphs and GNN operations (AGGREGATE, COMBINE). A significant part of its technical approach is detailing how graph structures are generated for time series data.\n    *   **Innovation (of the survey)**:\n        *   **Unified and Structured Taxonomy**: Presents a novel framework to categorize existing GNN4TS works from both task-oriented (forecasting, classification, anomaly detection, imputation) and methodology-oriented perspectives (spatial/temporal dependency modeling, model architecture).\n        *   **Detailed Graph Construction Methods**: Provides a comprehensive discussion and categorization of strategies for generating graph structures when not readily available, including:\n            *   **Heuristic-based Graphs**: Spatial Proximity (e.g., geographical distance, Gaussian kernel), Pairwise Connectivity (e.g., transportation networks), Pairwise Similarity (e.g., cosine similarity, Pearson correlation, DTW), and Functional Dependence (e.g., Granger causality, transfer entropy).\n            *   **Learning-based Graphs**: Approaches that learn graph structures directly from data end-to-end with the downstream task, often using embedding comparisons or attention mechanisms, with sparsification techniques.\n\n*   **Key Technical Contributions**\n    *   **Novel Taxonomy**: Introduces a comprehensive, task-oriented taxonomy for GNN4TS, covering forecasting, classification, anomaly detection, and imputation, and a methodology-oriented classification based on spatial/temporal dependency modeling and model architecture.\n    *   **Comprehensive Review of Graph Construction**: Systematically outlines and categorizes various heuristic-based and learning-based methods for constructing graph structures from time series data, which is crucial for applying GNNs.\n    *   **Consolidation of Knowledge**: For the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities.\n    *   **Future Research Agenda**: Identifies and discusses potential future research directions in the field of GNN4TS.\n\n*   **Experimental Validation**\n    *   As a survey, this paper does not present new experimental results. Instead, it synthesizes findings from the reviewed literature.\n    *   **Key Performance Metrics and Comparison Results (from reviewed papers)**: The survey highlights that GNN-based approaches \"demonstrate promising results\" and \"significant advantages\" in modeling real-world time series data compared to traditional methods. It notes that GNNs enable the capture of \"diverse and intricate relationships\" and have shown \"promising outcomes\" across forecasting, classification, anomaly detection, and imputation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The survey \\cite{jin2023ijy} focuses specifically on GNNs for time series analysis (GNN4TS) across four core tasks: forecasting, classification, anomaly detection, and imputation. It primarily discusses spatial GNNs.\n    *   **Technical Limitations (of the field, as identified by the survey)**: The field still faces challenges in graph structure generation (especially for complex, dynamic relationships), scalability for very large time series, and interpretability of GNN models.\n\n*   **Technical Significance**\n    *   **Advances the technical state-of-the-art**: By providing the first comprehensive and structured overview of GNN4TS, the survey consolidates fragmented knowledge, making the field more accessible and understandable for researchers and practitioners. It clarifies the landscape of GNN applications in time series.\n    *   **Potential impact on future research**: The survey \\cite{jin2023ijy} serves as a foundational resource, guiding future research by identifying open questions, emerging trends, and promising directions in GNN4TS. It underscores the power of GNNs in explicitly modeling complex spatial-temporal dependencies, thereby encouraging further innovation in this area.",
        "year": 2023,
        "citation_key": "jin2023ijy"
      }
    ],
    "layer3_papers": [
      {
        "title": "Spatio-Spectral Graph Neural Networks",
        "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{geisler2024wli}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Spatial Message Passing Graph Neural Networks (MPGNNs) suffer from a limited \"receptive field\" (typically ℓ-hop neighborhood) and \"over-squashing,\" which severely restricts information exchange between distant nodes. This limits their expressivity and ability to model long-range interactions.\n    *   **Importance and Challenge**: Modeling long-range interactions is crucial for many graph-based tasks, as evidenced by the success of global models like transformers. Over-squashing makes MPGNNs ineffective for problems requiring global information, posing a significant challenge to their applicability in complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of MPGNNs, which have achieved breakthroughs in various domains. It draws inspiration from similar synergistic compositions in molecular point clouds (Kosmala et al., 2023) and sequence models like Mamba (Gu & Dao, 2023) or Hyena (Poli et al., 2023), which offer transformer-like properties with superior scalability.\n    *   **Limitations of Previous Solutions**: MPGNNs are inherently limited by their local message-passing scheme and the over-squashing phenomenon. The design space for spectral GNNs, in contrast to spatial MPGNNs, has been largely unexplored, leaving potential for novel architectural advancements.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Spatio-Spectral Graph Neural Networks (S2GNNs) \\cite{geisler2024wli}, a new modeling paradigm that synergistically combines spatially and spectrally parametrized graph filters.\n        *   S2GNNs utilize a partial eigendecomposition to enable spectral filters that are *spectrally bounded* (operating on a truncated frequency spectrum) but *spatially unbounded*, allowing for global information propagation.\n        *   The architecture can combine spatial and spectral filters additively or sequentially.\n        *   The spectral filter is defined as `Spectral(l)(H(l−1);V,λ) =V [ˆg(l)ϑ(λ)⊙ [V⊤f(l)θ(H(l−1))]]` (Eq. 3), ensuring permutation equivariance.\n    *   **Novelty/Difference**:\n        *   **Synergistic Combination**: The core innovation is the principled combination of local spatial message passing with global spectral filtering, addressing the limitations of each individually.\n        *   **Spectral Domain Neural Network**: The paper proposes the first neural network designed to operate directly in the spectral domain, allowing for data-dependent filtering and channel mixing with negligible computational cost for truncated spectra \\cite{geisler2024wli}.\n        *   **Directed Graph Filters**: S2GNNs generalize spectrally parametrized filters to directed graphs, expanding their applicability.\n        *   **Expressive Positional Encodings**: They introduce stable positional encodings (PEs) derived almost \"for free\" from the partial eigendecomposition, making S2GNNs strictly more expressive than the 1-Weisfeiler-Lehman (WL) test \\cite{geisler2024wli}.\n        *   **Over-squashing Mitigation**: The global nature of spectral filters inherently vanquishes over-squashing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   The S2GNN architecture itself, which integrates spatial and spectral filtering for enhanced graph learning \\cite{geisler2024wli}.\n        *   A method for parametrizing spectral filters using Gaussian smearing and linear transformations, designed for stability and expressivity.\n        *   The concept and implementation of a neural network operating directly within the spectral domain.\n        *   Adaptation of spectral filters for directed graphs.\n        *   Novel, cost-effective positional encodings derived from the graph's eigendecomposition.\n    *   **Theoretical Insights or Analysis**:\n        *   **Over-squashing Vanquished**: Theorem 2 formally proves that S2GNNs overcome over-squashing by demonstrating a uniformly lower-bounded Jacobian sensitivity, ensuring effective long-range information exchange.\n        *   **Superior Approximation Bounds**: Theorems 3 and 4 establish strictly tighter approximation-theoretic error bounds for S2GNNs compared to MPGNNs. This is particularly significant for approximating discontinuous or unsmooth ground truth filters, where MPGNNs converge exceedingly slowly.\n        *   **Locality and Spectral Smoothness**: The analysis connects the locality of a filter to the smoothness of its Fourier transform, providing a complementary perspective on MPGNN limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Empirical verification of MPGNN shortcomings (e.g., over-squashing on \"Clique Path\" graphs) and how S2GNNs overcome them.\n        *   Approximation quality comparison of S2GNNs against purely spatial and spectral filters for a discontinuous target filter.\n        *   Performance evaluation on the challenging peptides-func long-range benchmark tasks (Dwivedi et al., 2022).\n        *   Scalability tests on large graphs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   S2GNNs achieved a new state-of-the-art on the peptides-func benchmark, outperforming MPGNNs and graph transformers with approximately 40% fewer parameters \\cite{geisler2024wli}.\n        *   They demonstrated competitive performance with state-of-the-art sequence models.\n        *   Empirical results on \"Clique Path\" graphs confirmed that spectral filters (and thus S2GNNs) do not exhibit over-squashing (Fig. 5).\n        *   S2GNNs scaled efficiently to millions of nodes on a 40 GB GPU with vanilla full-graph training, exhibiting runtime and space complexity equivalent to MPGNNs \\cite{geisler2024wli}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**:\n        *   The efficiency of spectral filters relies on truncating the frequency spectrum (parameter `k` or `λcut`).\n        *   Theoretical guarantees for approximation assume the spectral filter is bandlimited and a universal approximator on a specific interval.\n        *   The default choice for the spectral filter's frequency band is low frequencies, though the method is adaptable.\n    *   **Scope of Applicability**: S2GNNs are designed as general-purpose GNNs, particularly well-suited for tasks requiring effective long-range interactions and where over-squashing is a concern. They are applicable to both undirected and directed graphs.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: S2GNNs introduce a novel and theoretically grounded GNN paradigm that fundamentally addresses the long-standing issues of over-squashing and limited receptive fields in MPGNNs \\cite{geisler2024wli}. By synergistically combining spatial and spectral approaches, they achieve superior expressivity and approximation capabilities, validated by state-of-the-art empirical performance on challenging long-range tasks.\n    *   **Potential Impact on Future Research**: This work opens up a significant new design space for GNN architectures, encouraging further exploration into hybrid spatial-spectral models. The proposed spectral domain neural network, directed graph filters, and \"free\" positional encodings offer concrete avenues for future research, potentially leading to more powerful, efficient, and expressive GNNs for a wider range of applications.",
        "year": 2024,
        "citation_key": "geisler2024wli"
      },
      {
        "title": "A Review of Graph Neural Networks in Epidemic Modeling",
        "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
        "summary": "This paper, `A Review of Graph Neural Networks in Epidemic Modeling` by Liu et al. \\cite{liu20242g6}, provides a comprehensive survey of Graph Neural Networks (GNNs) in the context of epidemic modeling.\n\nHere's a focused summary for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional mechanistic epidemic models suffer from oversimplified or fixed assumptions, leading to sub-optimal predictive power and an inability to efficiently capture complex relational information. Existing data-driven approaches like CNNs and RNNs also fall short in incorporating crucial relational data (e.g., human mobility, geographic connections, contact tracing) essential for accurate epidemic forecasting.\n    *   **Importance & Challenge**: Accurate and timely epidemic modeling is critical for public health decision-making, resource allocation, and effective intervention strategies. The challenge lies in developing models that can effectively integrate and leverage the complex, dynamic relational data inherent in disease transmission networks to provide more precise and generalizable predictions.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Mechanistic Models**: Such as SIR, SEIR, and SIRD, mathematically describe disease transmission but are limited by their reliance on fixed and often oversimplified assumptions.\n        *   **Data-driven Models (CNNs, RNNs)**: While successful in some epidemiological predictive tasks (e.g., forecasting case counts), they often lack the capability to incorporate relational information effectively.\n    *   **Limitations of Previous Solutions**: Both mechanistic and general deep learning models struggle to capture the intricate relational dynamics crucial for understanding and predicting disease spread, leading to biases and compromised accuracy.\n    *   **Positioning of this Work**: This paper \\cite{liu20242g6} distinguishes itself as a *comprehensive and pioneering review* specifically focused on the application of GNNs in epidemic modeling. Unlike prior surveys that are often narrow in scope (e.g., specific viruses, single tasks, or general deep learning without deep GNN integration), this work offers a broader and more detailed overview of GNN-based approaches across a spectrum of epidemic tasks, aiming to bridge existing literature gaps.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a systematic and exhaustive review of GNNs in epidemic modeling. This involves:\n        *   Developing hierarchical taxonomies for both epidemic tasks (categorized into Detection, Surveillance, Prediction, and Projection) and GNN methodologies (categorized into Neural Models and Hybrid Models).\n        *   Providing detailed explanations and definitions for each task category.\n        *   Systematically examining existing GNN-based methodologies, including their technical details, data resources, and graph construction techniques.\n    *   **Novelty**: The innovation lies in providing the *first comprehensive and structured review* dedicated solely to GNNs in epidemiology. It offers novel taxonomies for organizing the field, a meticulous examination of existing methods, and a systematic identification of limitations and future research directions, thereby fostering interdisciplinary synergy.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Taxonomies**: Introduction of hierarchical taxonomies for epidemic tasks (Detection, Surveillance, Prediction, Projection) and GNN methodologies (Neural Models, Hybrid Models), providing a structured framework for understanding the field.\n    *   **Systematic Review Framework**: Establishment of a comprehensive framework for analyzing and categorizing GNN applications, encompassing task objectives, data types, graph construction techniques, and technical details of various GNN models.\n    *   **Identification of Limitations and Future Directions**: Systematically discusses the limitations of current GNN methods in epidemiology and proposes concrete, prospective research directions to guide future advancements.\n    *   **Resource Compilation**: Provides a curated list of relevant papers (via a GitHub repository) to serve as a valuable resource for researchers in this interdisciplinary domain.\n\n*   **5. Experimental Validation**\n    *   As a review paper, \\cite{liu20242g6} does not present new experimental results. Instead, its \"validation\" comes from the comprehensive synthesis of empirical evidence reported in the *reviewed literature*. The paper highlights that GNNs have demonstrated significant success in various epidemiological tasks, such as infection prediction, outbreak source detection, and intervention modeling, by effectively capturing relational dynamics and yielding more precise predictions compared to traditional methods. The thoroughness and systematic organization of the reviewed works implicitly validate the claims made about the utility and potential of GNNs in this field.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations (of reviewed methods, as identified by the paper)**: The paper points out that traditional mechanistic models suffer from oversimplified assumptions, while general deep learning models (CNNs, RNNs) often fail to incorporate crucial relational information. While GNNs offer advantages, the paper's unprovided Section 5 is dedicated to discussing the limitations of existing GNN methods from diverse perspectives.\n    *   **Scope of Applicability**: The review focuses exclusively on the application of Graph Neural Networks within the domain of *epidemic modeling*. It covers a broad range of epidemic tasks and GNN methodologies, aiming for a comprehensive overview within this specific interdisciplinary intersection.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper \\cite{liu20242g6} significantly advances the technical state-of-the-art by providing the first comprehensive and structured overview of GNN applications in epidemiology. It consolidates fragmented knowledge, clarifies the landscape of existing methods, and underscores the unique advantages of GNNs in handling complex relational data for epidemic tasks.\n    *   **Potential Impact**:\n        *   **Guidance for Researchers**: Offers a clear roadmap and foundational understanding for researchers entering or working in this interdisciplinary field, facilitating model selection and task comprehension.\n        *   **Promotion of Interdisciplinary Synergy**: Aims to bridge the GNN and epidemiology communities, fostering collaborative research and accelerating advancements in both fields.\n        *   **Identification of Future Research**: Systematically outlines promising future research directions, stimulating innovation and guiding efforts to address current limitations in GNN-based epidemic modeling.",
        "year": 2024,
        "citation_key": "liu20242g6"
      }
    ],
    "layer2_summary": null
  },
  "347e837b1aa03c9d17c69a522929000f0a0f0a51": {
    "seed_title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
    "summary": "It appears that the list of \"Papers to reference (sorted chronologically)\" is empty. To perform the requested analysis on the evolution of scientific ideas in \"Graph Neural Networks\" through a chain of connected papers, I need the actual list of papers with their summaries.\n\nPlease provide the papers to reference, and I will then be able to complete the analysis according to the specified format.",
    "path": [
      "347e837b1aa03c9d17c69a522929000f0a0f0a51"
    ],
    "layer1_papers": [
      {
        "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
        "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
        "summary": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
        "year": 2019,
        "citation_key": "sarlin20198a6"
      }
    ],
    "layer2_papers": [],
    "layer3_papers": [],
    "layer2_summary": null
  },
  "e4715a13f6364b1c81e64f247651c3d9e80b6808": {
    "seed_title": "Link Prediction Based on Graph Neural Networks",
    "summary": "## 1. Chronological Progression Analysis:\n\nThe evolution of Graph Neural Networks (GNNs) through this citation path reveals a journey from foundational model development for specific tasks to a more mature phase focused on addressing core architectural limitations, enhancing robustness, improving evaluation rigor, and exploring new paradigms for generalization and semantic understanding.\n\n1.  **[zhang2018kdl] Link Prediction Based on Graph Neural Networks (2018)**\n    *   **Methodological/Conceptual Shift:** Introduced GNNs (specifically the SEAL framework) as a superior alternative to predefined heuristics and prior neural methods (like WLNM) for link prediction. The key shift was from fixed, assumption-laden methods to learning general graph structure features from local subgraphs using GNNs, backed by a novel theoretical justification (`$\\beta$-decaying heuristic theory`).\n    *   **Problems Addressed:** Limitations of predefined heuristic methods for link prediction; inability of previous neural approaches to effectively learn high-order features from local subgraphs or incorporate diverse node features; lack of theoretical justification for learning high-order heuristics from local subgraphs.\n    *   **Innovations/Capabilities:** The SEAL framework, `$\\beta$-decaying heuristic theory` (unifying heuristics and justifying local learning), a novel structural node labeling scheme, and comprehensive integration of structural, latent, and explicit node features.\n    *   **External Influences:** The rising prominence of GNNs as a powerful tool for graph data.\n\n2.  **[li2023o4c] Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking (2023)**\n    *   **Methodological/Conceptual Shift:** A significant shift from *developing* GNN models to *critically evaluating* their performance and the prevailing benchmarking practices for link prediction. This paper emphasizes scientific rigor and realistic assessment.\n    *   **Problems Addressed:** Unreliable comparisons and hindered progress in GNN-based link prediction due to underreported baseline performance, inconsistent data splits/metrics, and unrealistic negative sampling strategies.\n    *   **Innovations/Capabilities:** A standardized and reproducible benchmarking methodology; Heuristic Related Sampling Technique (HeaRT) for generating challenging, realistic negative samples.\n    *   **External Influences:** A 5-year gap from [zhang2018kdl] indicates a period of rapid GNN proliferation, leading to a need for better evaluation standards and reproducible research.\n\n3.  **[zeng20237gv] Substructure Aware Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Focused on overcoming a fundamental architectural limitation of conventional GNNs: their limited expressive power (1-Weisfeiler-Leman test equivalent). It introduces explicit mechanisms to inject higher-order substructural information.\n    *   **Problems Addressed:** GNNs' inability to perceive crucial higher-order substructures; scalability and complexity issues of direct higher-order GNNs; lack of generalization in predefined hand-crafted substructure methods.\n    *   **Innovations/Capabilities:** The Substructure Aware Graph Neural Network (SAGNN) framework; novel \"Cut subgraph\" extraction based on Edge Betweenness Centrality; an efficient random walk return probability encoding for subgraphs; theoretical proof of enhanced expressiveness beyond 1-WL.\n    *   **External Influences:** Growing awareness of the theoretical limitations of standard GNNs and the demand for more expressive models for complex tasks.\n\n4.  **[liu2023v3e] Learning Strong Graph Neural Networks with Weak Information (2023)**\n    *   **Methodological/Conceptual Shift:** Addressed the practical challenge of GNN robustness in real-world scenarios where data is *simultaneously* incomplete (weak structure, features, and labels), moving beyond single-aspect data deficiencies.\n    *   **Problems Addressed:** Significant performance degradation of GNNs when input data contains \"weak information\"; existing solutions only addressing one type of deficiency; the challenge of handling *simultaneously occurring and mutually affecting* data deficiencies; the \"stray node problem\" (isolated nodes).\n    *   **Innovations/Capabilities:** D2PT (Dual-channel Diffused Propagation then Transformation) framework; a dual-channel architecture (DPT backbone for long-range propagation, global graph for stray nodes); prototype contrastive alignment for mutual benefit between channels.\n    *   **External Influences:** Increasing real-world deployment of GNNs, highlighting the need for models robust to imperfect data.\n\n5.  **[mao202313j] Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All? (2023)**\n    *   **Methodological/Conceptual Shift:** Shifted from GNN design to *diagnosing and theoretically analyzing* their inherent behavioral limitations on structurally diverse nodes within a single graph. Introduced the concept of \"structural disparity.\"\n    *   **Problems Addressed:** GNNs' \"performance disparity\" on nodes exhibiting mixed homophilic and heterophilic patterns; overlooking nuanced GNN behavior on diverse node subgroups; lack of theoretical explanation for this disparity.\n    *   **Innovations/Capabilities:** First rigorous analysis of GNN performance disparity on subgroups; novel CSBM-Structure (CSBM-S) model for generating graphs with mixed patterns; a non-i.i.d PAC-Bayesian generalization bound for GNNs; theoretical insights into how aggregation impacts feature distances.\n    *   **External Influences:** Recognition that real-world graphs are complex and rarely purely homophilic or heterophilic.\n\n6.  **[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)**\n    *   **Methodological/Conceptual Shift:** Introduced a *new paradigm* for GNN training and adaptation: prompting, inspired by advancements in NLP. This aims to unify pre-training and diverse downstream tasks without fine-tuning the entire model.\n    *   **Problems Addressed:** Inconsistent objectives between GNN pre-training and downstream fine-tuning; heavy reliance on large amounts of task-specific labeled data; lack of a universal graph prompting approach for diverse tasks (node and graph classification).\n    *   **Innovations/Capabilities:** The GraphPrompt framework; a unified task template (subgraph similarity) for pre-training and downstream tasks; novel learnable prompts that guide the `ReadOut` operation for task-specific aggregation, enabling few-shot learning.\n    *   **External Influences:** The remarkable success of prompting in Natural Language Processing, suggesting its applicability to GNNs for data efficiency and generalization.\n\n7.  **[longa202399q] Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities (2023)**\n    *   **Methodological/Conceptual Shift:** A comprehensive *survey* and systematization of Temporal GNNs (TGNNs). This is a meta-level contribution, providing a foundational framework, formalizing learning settings, tasks, and proposing a taxonomy for a rapidly growing subfield.\n    *   **Problems Addressed:** Lack of a comprehensive, systematized overview of TGNNs; absence of rigorous formalization for learning settings and tasks; need for a unified taxonomy for existing TGNN approaches.\n    *   **Innovations/Capabilities:** A coherent formalization of temporal graph learning settings and tasks; a novel taxonomy for TGNN methods (snapshot-based vs. event-based); clear definitions for various temporal graph types.\n    *   **External Influences:** The rapid growth of research in dynamic graphs and GNNs, necessitating a structured overview to guide future work.\n\n8.  **[han2024rkj] Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach (2024)**\n    *   **Methodological/Conceptual Shift:** Directly addressed the \"one-size-fits-all\" filtering problem (diagnosed by [mao202313j]) by proposing an adaptive, node-wise filtering mechanism using a Mixture of Experts (MoE) approach.\n    *   **Problems Addressed:** Suboptimal performance of uniform global filters on graphs with mixed homophilic/heterophilic patterns; the challenge of adaptively applying appropriate filters to individual nodes without explicit ground truth on node patterns.\n    *   **Innovations/Capabilities:** NODE-MOE framework; theoretical demonstration of the benefits of node-wise filtering; a novel gating model for expert selection based on contextual features; diverse expert GNNs initialized with varied filter types; a filter smoothing loss for stable training.\n    *   **External Influences:** The diagnostic work like [mao202313j] created a clear problem statement for this solution. Advances in MoE architectures in other machine learning domains.\n\n9.  **[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)**\n    *   **Methodological/Conceptual Shift:** Pushed GNNs into the multi-modal domain, specifically aligning GNNs with Large Language Models (LLMs) using *multi-modal prompt learning* under *extremely weak text supervision*. This enables semantic understanding and zero-shot generalization.\n    *   **Problems Addressed:** GNNs' lack of real-world semantic understanding; challenges of graph-text alignment for general graph data (data scarcity, weak text supervision, diverse task levels, conceptual gaps); instability of prior graph prompting methods.\n    *   **Innovations/Capabilities:** Morpher (Multi-modal Prompt Learning for GNNs) paradigm; an improved, stable graph prompt design; multi-modal (graph and text) prompt learning; a cross-modal projector; enabling CLIP-style zero-shot generalization for GNNs.\n    *   **External Influences:** The explosive success of LLMs and multi-modal models (like CLIP) in other domains, inspiring similar integration for graphs to enhance semantic capabilities.\n\n---\n\n## 2. Evolution Analysis:\n\nThe trajectory of Graph Neural Networks research, as traced through these nine papers, reveals two overarching trends: first, a deep dive into **Enhancing GNN Core Capabilities: Expressiveness, Robustness, and Adaptability to Structural Heterogeneity**, and second, a significant pivot **Towards Generalization, Semantic Understanding, and Unified Learning Paradigms**.\n\n### Trend 1: Enhancing GNN Core Capabilities: Expressiveness, Robustness, and Adaptability to Structural Heterogeneity\n\n*Methodological progression*: The initial foray into GNNs for specific tasks, exemplified by **[zhang2018kdl] Link Prediction Based on Graph Neural Networks (2018)**, established the foundation. This work introduced the SEAL framework, demonstrating GNNs' ability to learn general graph structure features for link prediction, supported by a novel `$\\beta$-decaying heuristic theory` that justified learning high-order information from local subgraphs. However, as GNNs gained traction, their inherent limitations became apparent. **[zeng20237gv] Substructure Aware Graph Neural Networks (2023)** directly addressed the 1-Weisfeiler-Leman (1-WL) expressiveness bottleneck, a fundamental architectural constraint of many GNNs. It proposed the SAGNN framework, which enhances GNNs by explicitly encoding and injecting higher-order substructural information through novel \"Cut subgraph\" extraction and random walk-based encoding. This marked a methodological shift from simple message passing to incorporating richer, explicit structural context.\n\n*Problem evolution*: While **[zhang2018kdl]** solved the problem of learning adaptive link prediction heuristics, it implicitly assumed ideal graph data. Real-world applications quickly exposed GNNs' fragility. **[liu2023v3e] Learning Strong Graph Neural Networks with Weak Information (2023)** tackled the critical problem of GNN performance degradation when facing simultaneously weak structure, features, and labels—a common real-world scenario. This moved beyond single-aspect data deficiencies to a holistic approach for robustness. Concurrently, **[mao202313j] Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All? (2023)** diagnosed a deeper issue: GNNs' \"performance disparity\" on nodes with mixed homophilic and heterophilic patterns within the same graph. This highlighted the limitation of GNNs' uniform aggregation mechanisms. The problem then evolved from diagnosis to solution with **[han2024rkj] Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach (2024)**, which directly addressed the \"one-size-fits-all\" filtering problem by proposing an adaptive, node-wise filtering approach. The need for robust and adaptable GNNs also spurred meta-level contributions like **[li2023o4c] Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking (2023)**, which critiqued existing evaluation practices for link prediction, pushing for more realistic and rigorous benchmarking to ensure models are truly robust. Similarly, **[longa202399q] Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities (2023)** systematized the field of Temporal GNNs, reflecting the growing need for GNNs to adapt to dynamic structural changes over time.\n\n*Key innovations*: **[zeng20237gv]**'s \"Cut subgraph\" and random walk encoding significantly boosted GNN expressiveness beyond 1-WL. **[liu2023v3e]**'s D2PT framework, with its dual-channel architecture and prototype contrastive alignment, provided a unified solution for learning with weak information. **[mao202313j]** offered a novel theoretical framework (non-i.i.d PAC-Bayesian generalization bound) and the CSBM-Structure model to explain structural disparity. Building on this, **[han2024rkj]** introduced the NODE-MOE framework with a novel gating model and diverse expert GNNs, enabling adaptive node-wise filtering. These innovations collectively pushed GNNs towards greater architectural power, resilience to imperfect data, and adaptability to complex graph structures.\n\n### Trend 2: Towards Generalization, Semantic Understanding, and Unified Learning Paradigms\n\n*Methodological progression*: As GNNs became more robust, the focus shifted towards making them more data-efficient, generalizable, and capable of understanding semantics. This led to a methodological pivot inspired by advancements in other AI fields. **[liu2023ent] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks (2023)** introduced a groundbreaking \"prompting\" paradigm for GNNs, drawing inspiration from NLP. Instead of traditional fine-tuning, it proposed a unified task template (subgraph similarity) and learnable prompts to guide the `ReadOut` operation, enabling a single pre-trained GNN to adapt to diverse downstream tasks (node and graph classification) with limited labels. This represented a significant step towards more flexible and data-efficient GNN adaptation. This concept was further advanced by **[li202444f] Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? (2024)**, which pushed the boundaries into multi-modal learning. This paper proposed Morpher, a multi-modal prompt learning paradigm that aligns independently pre-trained GNNs with Large Language Models (LLMs) using *extremely weak text supervision*. This is a profound methodological shift towards integrating external semantic knowledge into GNNs.\n\n*Problem evolution*: The problem evolved from simply making GNNs work to making them work *smarter* and *more broadly*. The \"pre-train, fine-tune\" paradigm in GNNs suffered from objective inconsistency and heavy reliance on task-specific labeled data, a problem **[liu2023ent]** directly addressed by unifying pre-training and downstream tasks. The next major challenge, tackled by **[li202444f]**, was GNNs' inherent lack of real-world semantic understanding, unlike vision or language models. This paper confronted the difficulties of graph-text alignment for general graph data, particularly with data scarcity and extremely weak text supervision, and the instability of prior graph prompting methods. The goal was to imbue GNNs with a \"language understanding\" capability, enabling them to leverage the rich semantic space of LLMs.\n\n*Key innovations*: **[liu2023ent]**'s GraphPrompt framework, with its unified task template and learnable `ReadOut` prompts, was a breakthrough for few-shot learning and task unification in GNNs. **[li202444f]** introduced the Morpher paradigm, which is the first to perform graph-text multi-modal prompt learning for GNNs under weak text supervision. Its improved, stable graph prompt design and cross-modal projector enabled CLIP-style zero-shot generalization for GNNs, allowing them to predict unseen classes without explicit training data. These innovations are crucial for developing GNNs that are not only powerful but also highly adaptable, data-efficient, and capable of semantic reasoning, opening doors to broader real-world applications.\n\n---\n\n### 3. Synthesis:\n\nThis collection of papers collectively illustrates a dynamic intellectual trajectory in Graph Neural Networks, moving from foundational model development and theoretical justification to a sophisticated focus on addressing inherent architectural limitations, enhancing real-world robustness, and pioneering new paradigms for data-efficient generalization and multi-modal semantic understanding. Their collective contribution is to transform GNNs from specialized graph processing tools into more expressive, resilient, and semantically aware AI models capable of tackling complex, imperfect, and diverse real-world graph data.",
    "path": [
      "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "707142f242ee4e40489062870ca53810cb33d404",
      "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "b88f456daaf29860d2b59c621be3bd878a581a59",
      "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "14c59d6dab548ef023b8a49df4a26b966fe9d00a"
    ],
    "layer1_papers": [
      {
        "title": "Link Prediction Based on Graph Neural Networks",
        "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{zhang2018kdl}\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: Addressing the limitations of predefined heuristic methods for link prediction in network-structured data. Existing heuristics (e.g., common neighbors, Katz index) make strong assumptions about link formation, which often fail in diverse real-world networks (e.g., protein-protein interaction networks).\n*   **Importance & Challenge**: Link prediction is crucial for applications like friend recommendation, knowledge graph completion, and metabolic network reconstruction. The challenge lies in automatically learning a suitable \"heuristic\" from a given network that adapts to its specific link formation patterns, rather than relying on fixed, potentially inaccurate assumptions. Previous attempts (like WLNM \\cite{zhang2018kdl}) struggled with effectively learning high-order features from local subgraphs without requiring computationally expensive large `h`-hop subgraphs (approaching the entire network).\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   **Heuristic Methods**: The paper positions itself as an improvement over traditional heuristic methods (e.g., Common Neighbors, Adamic-Adar, Katz, PageRank, SimRank) by learning general graph structure features instead of using predefined ones.\n    *   **Latent Feature Methods**: Acknowledges and incorporates latent features (e.g., from DeepWalk, node2vec, LINE) and explicit node attributes, but aims to learn structural features directly from the graph.\n    *   **Supervised Heuristic Learning**: Builds upon the paradigm of learning heuristics.\n*   **Limitations of Previous Solutions**:\n    *   **Predefined Heuristics**: Strong, often incorrect assumptions about link existence.\n    *   **WLNM \\cite{zhang2018kdl} (closest prior work)**:\n        *   Uses fully-connected neural networks on fixed-size adjacency matrices, requiring truncation of subgraphs and potentially losing structural information.\n        *   Cannot effectively incorporate latent or explicit node features.\n        *   Lacked theoretical justification for learning high-order heuristics from local subgraphs.\n    *   **Other Supervised Methods**: Still rely on combinations of *predefined* heuristics, rather than learning *general* graph structure features from scratch.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The SEAL (Subgraph Embedding for Link prediction) framework learns general graph structure features for link prediction from local enclosing subgraphs using a Graph Neural Network (GNN).\n*   **Novelty/Difference**:\n    *   **`$\\beta$-decaying Heuristic Theory`**: A novel theoretical contribution proving that a wide range of high-order heuristics (including Katz, rooted PageRank, and SimRank) can be unified under a `$\\beta$-decaying` framework. Crucially, it demonstrates that these heuristics can be accurately approximated from small `h`-hop enclosing subgraphs, with approximation error decreasing exponentially with `h`. This resolves the perceived need for large `h` to capture high-order information.\n    *   **SEAL Framework**:\n        *   **GNN-based Learning**: Replaces the fully-connected neural network of WLNM \\cite{zhang2018kdl} with a GNN, which is inherently better suited for learning from variable-sized graph structures and extracting local substructure features.\n        *   **Comprehensive Node Information Matrix**: Constructs a rich node feature matrix for the GNN input, comprising:\n            *   **Structural Node Labels**: A novel labeling scheme that assigns integer labels to nodes within the enclosing subgraph based on their relative positions and roles concerning the target nodes `x` and `y`. This helps the GNN understand the context of each node.\n            *   **Node Embeddings**: Incorporates latent features from network embedding methods.\n            *   **Node Attributes**: Integrates explicit side information.\n        *   **Enclosing Subgraph Extraction**: Extracts `h`-hop enclosing subgraphs around target link candidates, providing localized structural context.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms, Methods, or Techniques**:\n    *   The `$\\beta$-decaying heuristic theory`, which unifies a broad class of link prediction heuristics and provides theoretical justification for learning from local subgraphs.\n    *   Proof that high-order heuristics like Katz, PageRank, and SimRank are `$\\beta$-decaying and can be effectively approximated from `h`-hop enclosing subgraphs.\n    *   The SEAL framework, a GNN-based approach for link prediction that learns general graph structure features.\n    *   A novel structural node labeling scheme for nodes within enclosing subgraphs, enabling GNNs to understand node roles relative to the target link.\n*   **System Design or Architectural Innovations**:\n    *   Integration of GNNs with local enclosing subgraphs for link prediction, allowing for variable-sized inputs and effective feature learning.\n    *   A flexible node information matrix construction that combines structural labels, latent embeddings, and explicit attributes.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**: SEAL was extensively evaluated against a wide range of baseline methods on multiple datasets.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The abstract states that SEAL achieved \"unprecedented performance, working consistently well on a wide range of problems.\"\n    *   It \"outperforms all heuristic methods, latent feature methods, and recent network embedding methods by large margins.\"\n    *   Crucially, SEAL \"also outperforms the previous state-of-the-art method, WLNM \\cite{zhang2018kdl}.\"\n    *   Empirically verified \"much improved performance\" over WLNM.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The `$\\beta$-decaying theory` relies on specific properties of the function `f(x,y,l)` and the decaying factor `$\\beta$`. While shown to hold for many common heuristics, it might not encompass all possible link formation mechanisms.\n    *   The effectiveness of the `h`-hop enclosing subgraph approach depends on the assumption that remote parts of the network contribute exponentially less to link existence.\n*   **Scope of Applicability**: Primarily focused on undirected graphs. The computational cost of extracting subgraphs, while mitigated by the theory suggesting small `h`, could still be a factor for extremely large or dense networks if `h` needs to be larger for certain problems.\n\n### 7. Technical Significance\n\n*   **Advancement of Technical State-of-the-Art**:\n    *   Provides a robust theoretical foundation for learning link prediction heuristics from local subgraphs, challenging the conventional wisdom that high-order heuristics require global network information.\n    *   Demonstrates the superior capability of GNNs for learning complex graph structure features in the context of link prediction, surpassing traditional heuristics and prior neural network approaches.\n    *   Introduces a principled way to combine structural, latent, and explicit features for enhanced link prediction performance.\n*   **Potential Impact on Future Research**:\n    *   Encourages further development of GNN architectures specifically designed for local subgraph analysis in various graph learning tasks.\n    *   Opens new avenues for analyzing and designing link prediction heuristics based on the `$\\beta$-decaying` framework.\n    *   Promotes the use of structural node labeling as a powerful technique to inject domain-specific structural information into GNNs.\n    *   Could inspire similar local-to-global learning paradigms in other graph-based prediction problems.",
        "year": 2018,
        "citation_key": "zhang2018kdl"
      }
    ],
    "layer2_papers": [
      {
        "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
        "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT",
        "summary": "This paper, `Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking` by Li et al. \\cite{li2023o4c}, critically examines the evaluation practices of Graph Neural Networks (GNNs) for link prediction and proposes a new, more realistic benchmarking framework.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses significant pitfalls in the current evaluation of GNN-based link prediction methods, which lead to unreliable comparisons and hinder progress.\n    *   **Importance and Challenge**: These pitfalls include: (1) underreported performance of existing baselines due to poor hyperparameter tuning or non-standard settings; (2) a lack of unified data splits and evaluation metrics across different studies and datasets; and (3) an unrealistic evaluation setting that uses \"easy\" negative samples, which do not reflect real-world scenarios (e.g., negative samples often lack common neighbors, making them trivial to classify). This problem is crucial because it obscures the true capabilities of GNN models and makes it difficult to identify genuinely superior approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself not by proposing a new GNN architecture, but by scrutinizing and improving the *evaluation methodology* for existing and future GNN-based link prediction models. It acknowledges the proliferation of GNN methods for link prediction but highlights the absence of a comprehensive, critical examination of their evaluation.\n    *   **Limitations of Previous Solutions**: The paper directly addresses the limitations of previous *evaluation practices*, which are the three pitfalls mentioned above. It argues that current benchmarking efforts are inconsistent and often misleading, preventing fair comparisons and accurate assessment of model performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        1.  **Reproducible and Fair Comparison**: The authors first conduct a rigorous re-evaluation of 17 prominent link prediction methods (including heuristics, embedding methods, GNNs, and GNN+Pairwise Info models) across 7 diverse datasets (Cora, Citeseer, Pubmed, ogbl-collab, ogbl-ddi, ogbl-ppa, ogbl-citation2). This involves unified data splits, consistent evaluation metrics (AUC, MRR, Hits@K), and a comprehensive hyperparameter search for all models.\n        2.  **New Evaluation Setting with Heuristic Related Sampling Technique (HeaRT)**: To address the unrealistic negative sampling, the paper proposes a novel evaluation strategy. This strategy tailors negative samples to each positive sample by restricting them to be \"corruptions\" (i.e., sharing one node with the positive sample). Within this more realistic pool, a **Heuristic Related Sampling Technique (HeaRT)** is introduced to select *hard* negative samples based on a combination of multiple structural heuristics.\n    *   **Novelty**: The primary novelty lies in two aspects: (1) the systematic and comprehensive re-benchmarking effort itself, which corrects widespread misrepresentations of model performance and establishes a unified evaluation framework; and (2) the introduction of HeaRT, a novel negative sampling technique that generates challenging, realistic negative samples, thereby creating a more robust and real-world aligned evaluation task for link prediction.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A standardized and reproducible benchmarking methodology for GNN-based link prediction, including unified data splits, metrics, and rigorous hyperparameter tuning.\n        *   HeaRT: A novel negative sampling technique that generates hard, heuristic-related negative samples for link prediction evaluation, making the task more challenging and realistic.\n    *   **System Design/Architectural Innovations**: While not proposing a new GNN architecture, the paper significantly innovates the *evaluation framework* for GNNs in link prediction.\n    *   **Theoretical Insights/Analysis**: Provides empirical insights into the actual performance of GNNs and the profound impact of evaluation settings on reported results.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A \"fair comparison\" of 17 methods on 7 datasets under existing, but unified, evaluation settings.\n        *   Evaluation of these methods under the proposed new evaluation setting utilizing HeaRT.\n    *   **Key Performance Metrics**: AUC, MRR, and Hits@K (with K varying based on dataset).\n    *   **Comparison Results (from fair comparison under existing settings)**:\n        *   **Better than Reported Performance**: Many models, including GCN, GAE, and Neo-GNN, achieved significantly higher performance than previously reported when properly tuned and evaluated under consistent settings. For example, Neo-GNN's Hits@50 on `ogbl-collab` increased from 57.52 to 66.13 \\cite{li2023o4c}. Heuristic methods on `ogbl-citation2` also saw substantial improvements (e.g., MRR around 75% vs. 50%) by treating the graph as undirected, consistent with GNN evaluations.\n        *   **Divergence from Reported Results**: Some methods, like BUDDY on `ogbl-ddi`, showed lower performance than previously reported, attributed to differences in training negative sampling strategies, further highlighting inconsistencies in prior work.\n        *   The new HeaRT evaluation setting is designed to expose new challenges and opportunities by aligning evaluation with real-world situations, though specific results from this setting are not detailed in the provided abstract and introduction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on homogeneous graphs. For large OGB datasets, hyperparameter search was constrained due to computational cost. The HeaRT method, while more realistic, still samples *K* negatives due to the prohibitive cost of considering all possible corruptions.\n    *   **Scope of Applicability**: The findings and proposed benchmark are most directly applicable to GNN-based link prediction on homogeneous graphs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: This work significantly advances the technical state-of-the-art by establishing a robust, reproducible, and more realistic benchmarking framework for GNN-based link prediction. It corrects previous misrepresentations of model performance, providing a clearer picture of current capabilities.\n    *   **Potential Impact**: It provides a reliable foundation for future research, enabling fair comparisons and accelerating the development of more robust and generalizable link prediction models. By exposing the limitations of current evaluation practices and offering a more challenging setting, it encourages the community to develop models that perform better in real-world applications, thereby fostering more impactful research.",
        "year": 2023,
        "citation_key": "li2023o4c"
      },
      {
        "title": "Substructure Aware Graph Neural Networks",
        "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Conventional Graph Neural Networks (GNNs) are fundamentally limited by the expressive power of the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \\cite{zeng20237gv}.\n    *   This limitation prevents GNNs from perceiving crucial higher-order substructures, which are vital for many downstream tasks (e.g., functional groups in organic chemistry) \\cite{zeng20237gv}.\n    *   The problem is challenging because directly constructing higher-order GNNs leads to scalability and complexity issues, while using predefined hand-crafted substructures compromises generalization ability \\cite{zeng20237gv}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to overcome 1-WL limitations include: higher-order GNNs (suffering from high computational cost), predefined hand-crafted substructures (lacking generalization), inductive coloring methods (often task-specific), and positional encoding methods (facing issues like global sign ambiguity or limited inductive generalization) \\cite{zeng20237gv}.\n    *   This work positions itself by introducing a novel framework, Substructure Aware Graph Neural Networks (SAGNN), that leverages subgraphs to enhance GNN expressiveness. It aims to overcome the 1-WL barrier without incurring the high complexity of higher-order GNNs or the generalization issues of fixed substructures, by focusing on flexible subgraph extraction and encoding \\cite{zeng20237gv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the Substructure Aware Graph Neural Network (SAGNN) framework, which injects subgraph-level structural information into node features during message passing \\cite{zeng20237gv}.\n    *   **Novel Subgraph Extraction:** Introduces the \"Cut subgraph,\" which is obtained from the original graph by continuously and selectively removing edges with the highest Edge Betweenness Centrality (EBC) until the graph is split into a specified number of connected blocks \\cite{zeng20237gv}. This allows for capturing structural information at various granularities.\n    *   **Novel Subgraph Encoding:** Extends the random walk encoding paradigm to compute \"return probabilities\" of a rooted node within a subgraph for a given number of steps \\cite{zeng20237gv}. This method efficiently captures rich structural information and is used as a node feature.\n    *   **Subgraph Information Injection:** The encoded subgraph features (from both Ego-networks and Cut subgraphs) are concatenated with initial node features and original graph structural representations. These enhanced features are then propagated through two parallel message passing channels (Ego channel and Cut channel) \\cite{zeng20237gv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Subgraph Definition:** The \"Cut subgraph\" extraction strategy, which dynamically partitions the graph based on edge centrality to reveal meaningful substructures \\cite{zeng20237gv}.\n    *   **Novel Subgraph Encoding Method:** An efficient random walk return probability encoding for subgraphs, designed to capture structural information with reduced time complexity and improved expressiveness \\cite{zeng20237gv}.\n    *   **Framework Design:** The Substructure Aware Graph Neural Network (SAGNN) framework, which seamlessly integrates these novel subgraph features into standard GNN message passing architectures \\cite{zeng20237gv}.\n    *   **Theoretical Insight:** Provides theoretical proof that any 1-WL GNN equipped with components of the SAGNN framework is strictly more powerful than 1-WL \\cite{zeng20237gv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on various well-proven graph tasks, including graph classification on TUDatasets (MUTAG, PTC, PROTEINS, NCI1, IMDB-B) and graph regression for drug constrained solubility prediction (ZINC-FULL) \\cite{zeng20237gv}.\n    *   **Key Performance Metrics:** Accuracy for classification tasks and Mean Absolute Error (MAE) for regression tasks \\cite{zeng20237gv}.\n    *   **Comparison Results:**\n        *   Achieved state-of-the-art (SOTA) performance across a variety of datasets and when integrated with different base GNN models (e.g., GIN, PNA) \\cite{zeng20237gv}.\n        *   Demonstrated a maximum performance improvement of 83% compared to base models and 32% compared to previous SOTA methods \\cite{zeng20237gv}.\n        *   GNNs equipped with SAGNN performed flawlessly even on graphs that are known to fail the 3-WL test, indicating significantly enhanced structure perception \\cite{zeng20237gv}.\n\n*   **Limitations & Scope**\n    *   The provided abstract and introduction do not explicitly detail specific technical limitations or assumptions inherent to the SAGNN framework itself.\n    *   The scope of applicability is broad, covering general graph learning tasks such as graph classification and regression, particularly in domains where higher-order structural information is crucial, like chemo-informatics \\cite{zeng20237gv}. The method is designed to enhance the expressiveness of existing 1-WL GNNs.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** Significantly pushes the expressive power of GNNs beyond the 1-WL limit by leveraging subgraph information, offering a more scalable alternative to complex higher-order GNNs \\cite{zeng20237gv}.\n    *   **Improved Structure Perception:** Enables GNNs to perceive and utilize higher-order substructures that were previously indistinguishable, leading to superior performance on challenging graph tasks and even on graphs that fail the 3-WL test \\cite{zeng20237gv}.\n    *   **Practical Impact:** Provides a general and flexible framework that can be integrated with various existing GNN architectures, leading to substantial performance gains on real-world graph learning tasks \\cite{zeng20237gv}.\n    *   **Future Research:** The novel subgraph extraction and encoding methods open new avenues for exploring dynamic and expressive ways to incorporate substructural information into graph learning models \\cite{zeng20237gv}.",
        "year": 2023,
        "citation_key": "zeng20237gv"
      },
      {
        "title": "Learning Strong Graph Neural Networks with Weak Information",
        "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{liu2023v3e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) suffer significant performance degradation when input graph data contains \"weak information,\" which encompasses incomplete structure (missing edges), incomplete features (missing node attributes), and insufficient labels.\n    *   **Importance and Challenge**:\n        *   Most GNNs assume ideal, complete data, which is often invalid in real-world scenarios due to privacy concerns, data collection errors, or high annotation costs.\n        *   Existing solutions primarily address only *one type* of weak information (e.g., structure learning, feature imputation, label-efficient learning).\n        *   The critical challenge is to develop a universal and effective GNN framework that can handle *simultaneously occurring and mutually affecting* diverse data deficiencies, particularly in \"extreme GLWI\" scenarios where structure, features, and labels are all weak.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges prior work in graph structure learning, attribute completion, and label-efficient graph learning, which are specialized GNN designs to handle specific data deficiencies.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-aspect focus**: A major limitation is that most existing methods only consider data deficiency from a single perspective (e.g., only weak structure or only weak labels). They fail to address scenarios where multiple types of weak information coexist and interact.\n        *   **Computational Cost**: Many specialized methods require complex, carefully-crafted learning procedures, leading to high computational costs and reduced efficiency on large-scale graphs.\n        *   The paper positions itself as the *first attempt* to investigate graph learning with *extremely weak information* where all three aspects (structure, features, labels) are simultaneously incomplete.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **D2PT (Dual-channel Diffused Propagation then Transformation)**, a dual-channel GNN framework designed to enable effective information propagation on graphs with weak information.\n    *   **Novelty/Differentiation**:\n        *   **Empirically-driven Design Principles**: Based on empirical analysis, the authors identify two key design focal points: 1) enabling **long-range propagation** in GNNs, and 2) allowing information propagation to **stray nodes** (isolated from the largest connected component).\n        *   **Dual-Channel Architecture**:\n            *   **Channel 1 (DPT Backbone)**: Performs efficient long-range information propagation on the *input graph* (even with incomplete structure) using a graph diffusion-based backbone.\n            *   **Channel 2 (Global Graph)**: Learns a *global graph* by connecting nodes sharing similar semantics, derived from the propagated features. This channel specifically addresses the \"stray node problem\" by providing connections for isolated nodes.\n        *   **Prototype Contrastive Alignment**: A novel algorithm that aligns class-level prototypes learned from both channels. This mechanism allows the two different information propagation processes to mutually benefit, enhancing the model's ability to handle GLWI.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **D2PT Framework**: A principled dual-channel GNN architecture for comprehensive GLWI, integrating long-range propagation and global semantic connections.\n        *   **Graph Diffusion-based DPT Backbone**: An efficient mechanism for long-range message passing, crucial for recovering missing information and spreading supervision signals.\n        *   **Global Graph Construction**: A method to dynamically learn a global graph based on node semantic similarities, specifically designed to connect \"stray nodes\" and facilitate information flow to them.\n        *   **Prototype Contrastive Alignment**: A novel contrastive learning objective that aligns class-level prototypes between the two channels, ensuring consistency and mutual enhancement of learned representations.\n    *   **Problem Formulation**: The paper formally defines and investigates the \"extreme GLWI scenario,\" where structure, features, and labels are simultaneously deficient, advancing the research scope beyond single-aspect deficiencies.\n    *   **Empirical Analysis**: Provides a comprehensive analysis of the impact of data deficiency on GNNs, identifying long-range propagation and the stray node problem as critical factors, which directly guides the D2PT design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Performance evaluation of D2PT against various baseline GNNs (GCN, GAT, SGC, PPNP, APPNP) and GLWI-specific methods across different GLWI scenarios.\n        *   Ablation studies to validate the effectiveness of D2PT's components (e.g., dual-channel design, prototype alignment, global graph).\n        *   Efficiency analysis (training time, inference time).\n        *   Generalization capability across different datasets and GLWI settings.\n    *   **Key Performance Metrics**: Node classification accuracy.\n    *   **Comparison Results**:\n        *   D2PT consistently demonstrates **superior performance** over baseline GNNs and state-of-the-art GLWI methods in various weak information scenarios, including weak structure, weak features, weak labels, and especially the challenging \"extreme GLWI\" scenario.\n        *   The empirical analysis (Figure 2) shows that graph diffusion-based models with long-range propagation (like DPT) generally outperform shallow GNNs in basic GLWI scenarios, motivating D2PT's design.\n        *   Ablation studies confirm the individual contributions of the dual-channel design, global graph, and prototype contrastive alignment to D2PT's overall effectiveness.\n        *   D2PT also exhibits **high efficiency**, demonstrating competitive or superior performance with reasonable computational costs.\n    *   **Datasets**: Experiments were conducted on eight real-world benchmark datasets (e.g., Cora, CiteSeer, PubMed, Coauthor-CS, Amazon-Photo, etc.).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While D2PT addresses a broad range of weak information, the specific mechanisms for constructing the global graph and performing prototype alignment might have dependencies on the quality of initial feature embeddings or the number of classes. The paper does not explicitly detail theoretical guarantees for the prototype alignment's convergence or optimality.\n    *   **Scope of Applicability**: The method is primarily validated for semi-supervised node classification tasks. Its direct applicability to other graph learning tasks (e.g., link prediction, graph classification) or different types of graph data (e.g., heterogeneous graphs, dynamic graphs) is not explicitly explored in this paper.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023v3e} significantly advances the state-of-the-art by providing a unified, effective, and efficient solution for the complex problem of graph learning with *simultaneously occurring* weak information (structure, features, and labels). This moves beyond the single-aspect focus of prior research.\n    *   **Potential Impact on Future Research**:\n        *   **Robust GNN Design**: The D2PT framework offers a blueprint for designing more robust GNNs that can operate reliably in real-world, imperfect data environments.\n        *   **Multi-modal/Multi-source Weakness**: The dual-channel approach and prototype alignment could inspire future research into handling other forms of multi-modal or multi-source data deficiencies in graph learning.\n        *   **Diffusion-based GNNs**: Reinforces the importance of long-range propagation and graph diffusion mechanisms for handling data sparsity and incompleteness.\n        *   **Addressing Stray Nodes**: The explicit strategy for connecting stray nodes via semantic similarity provides a valuable direction for improving connectivity in sparse or fragmented graphs.",
        "year": 2023,
        "citation_key": "liu2023v3e"
      },
      {
        "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
        "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit a \"performance disparity\" on real-world graphs, performing well on nodes that align with the graph's dominant structural pattern (e.g., homophilic nodes in a homophilic graph) but struggling on nodes exhibiting the opposite pattern (e.g., heterophilic nodes in a homophilic graph) \\cite{mao202313j}. This disparity arises because real-world graphs inherently possess \"structural disparity,\" meaning they are a mixture of both homophilic and heterophilic node patterns.\n    *   **Importance & Challenge**: Existing GNN research often focuses on overall graph performance, overlooking the nuanced behavior of GNNs on these diverse node subgroups. Understanding this performance disparity is critical for developing GNNs that are robust and effective across all node types in complex, real-world graph structures \\cite{mao202313j}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior studies on GNN effectiveness primarily analyze performance on the entire graph and typically focus on graphs that are predominantly either homophilic or heterophilic \\cite{mao202313j}.\n    *   **Limitations of previous solutions**: These approaches fail to account for the common real-world scenario where graphs contain a mixture of homophilic and heterophilic patterns. They do not provide insights into GNN performance on specific node subgroups, potentially masking scenarios where GNNs perform poorly on minority patterns despite achieving good overall results \\cite{mao202313j}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Empirical Disparity Analysis**: Systematically investigates GNN performance by categorizing testing nodes based on their local homophily ratios, comparing a vanilla GNN (GCN) against MLP-based models (vanilla MLP and GLNN) \\cite{mao202313j}.\n        *   **Theoretical Analysis of Aggregation**: Examines how the GNN aggregation mechanism differentially impacts nodes with varying structural patterns. This involves introducing a novel graph generation model, CSBM-Structure (CSBM-S), and deriving theoretical insights into aggregated feature distances and class probabilities \\cite{mao202313j}.\n        *   **Generalization Bound**: Develops a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs to formally explain the underlying causes of the observed performance disparity \\cite{mao202313j}.\n        *   **Discriminative Ability Quantification**: Empirically analyzes the discriminative ability of GNNs on majority versus minority nodes using a \"relative discriminative ratio\" based on class prototypes \\cite{mao202313j}.\n    *   **Novelty/Difference**:\n        *   **First Rigorous Disparity Analysis**: This work is novel in its focused and systematic investigation of GNN performance on *subgroups* of nodes defined by their local structural patterns (homophilic vs. heterophilic), moving beyond aggregate performance metrics \\cite{mao202313j}.\n        *   **Novel CSBM-Structure (CSBM-S) Model**: Introduces a new variant of the Contextual Stochastic Block Model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes within a single graph, providing a more realistic testbed for structural disparity \\cite{mao202313j}.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound**: Presents a novel theoretical framework that precisely identifies aggregated feature distance and homophily ratio differences between training and testing nodes as critical factors driving performance disparity \\cite{mao202313j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Framework**: A non-i.i.d PAC-Bayesian generalization bound for GNNs that rigorously explains performance disparity by identifying aggregated feature distance and homophily ratio differences between training and testing nodes as key factors \\cite{mao202313j}.\n    *   **Novel Graph Generation Model**: Introduction of CSBM-Structure (CSBM-S), a variant of CSBM, which enables controlled experimental studies of graphs exhibiting mixed homophilic and heterophilic patterns \\cite{mao202313j}.\n    *   **Theoretical Insights into Aggregation**: Demonstrates that GNN aggregation creates a significant feature distance between homophilic and heterophilic node subgroups *within the same class*, and that differences in homophily ratio directly influence class prediction probabilities \\cite{mao202313j}.\n    *   **Practical Implications**: Provides insights into the effectiveness of deeper GNNs and identifies an overlooked distribution shift factor, leading to a new scenario for the graph out-of-distribution problem \\cite{mao202313j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Structural Disparity Verification**: Analyzed node homophily ratio distributions on real-world homophilic (Ogbn-arxiv, Pubmed) and heterophilic (Chameleon, Squirrel) datasets, confirming the pervasive existence of mixed structural patterns \\cite{mao202313j}.\n        *   **Subgroup Performance Comparison**: Conducted experiments comparing the accuracy of GCN against vanilla MLP and GLNN on test nodes, specifically grouped by their homophily ratio ranges \\cite{mao202313j}.\n        *   **Discriminative Ratio Analysis**: Empirically calculated a \"relative discriminative ratio\" to quantify the ease of prediction for majority versus minority nodes, observing its variation with the number of aggregation layers \\cite{mao202313j}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Accuracy Differences**: Demonstrated that in homophilic graphs, MLP-based models often outperform GCN on heterophilic (minority) nodes, while GCN performs better on homophilic (majority) nodes. Conversely, in heterophilic graphs, MLP models frequently outperform GCN on homophilic (minority) nodes, with GCN excelling on heterophilic (majority) nodes \\cite{mao202313j}.\n        *   **Discriminative Ratio**: The analysis aimed to illustrate how aggregation influences the discriminative ability, with a lower ratio indicating that majority nodes are easier to predict than minority nodes \\cite{mao202313j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical analysis (e.g., Lemma 1) employs simplifying assumptions like balanced class distribution and shared aggregated feature variance for mathematical elegance \\cite{mao202313j}. The CSBM-S model assumes consistent degree distributions and specific probability relationships \\cite{mao202313j}.\n    *   **Scope of Applicability**: The empirical validation primarily focuses on GCN and MLP-based models, and the findings are most directly applicable to semi-supervised node classification tasks \\cite{mao202313j}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of GNN behavior by providing a detailed, subgroup-level analysis of performance, directly addressing the challenges posed by structural disparity in real-world graphs \\cite{mao202313j}.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The identified factors (aggregated feature distance, homophily ratio differences) and the proposed PAC-Bayesian bound offer a theoretical foundation to guide the development of new GNN architectures and training strategies that are more robust and equitable across diverse node patterns \\cite{mao202313j}.\n        *   **Graph Out-of-Distribution (OOD) Problem**: By highlighting an overlooked distribution shift factor related to structural patterns, the work introduces a new dimension to the graph OOD problem, opening new avenues for research in OOD generalization for graphs \\cite{mao202313j}.\n        *   **Deeper GNNs**: Provides novel insights into the mechanisms behind the effectiveness of deeper GNNs in the context of structural disparity \\cite{mao202313j}.",
        "year": 2023,
        "citation_key": "mao202313j"
      },
      {
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
        "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
        "summary": "Here's a focused summary of the paper \"GraphPrompt : Unifying Pre-Training and Downstream Tasks for Graph Neural Networks\" \\cite{liu2023ent} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) typically require large amounts of task-specific labeled data for supervised learning. The \"pre-train, fine-tune\" paradigm, while reducing labeling, suffers from inconsistent objectives between pre-training and downstream tasks, leading to suboptimal performance. Existing graph prompting methods are limited, often only supporting specific tasks like node classification, lacking a universal approach for diverse downstream tasks (e.g., both node and graph classification).\n    *   **Importance & Challenge**: Reducing the reliance on extensive labeled data is crucial for GNN applicability. The challenge lies in: 1) unifying pre-training and various downstream tasks into a common framework to enable effective knowledge transfer, and 2) designing effective, task-specific prompts for graphs that can guide the pre-trained model without fine-tuning, similar to advancements in NLP.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon graph representation learning (graph embedding, GNNs) and graph pre-training techniques. It draws inspiration from prompting in natural language processing (NLP) to bridge the pre-training/downstream task gap.\n    *   **Limitations of Previous Solutions**:\n        *   Traditional GNNs: Heavily dependent on large, task-specific labeled data.\n        *   \"Pre-train, fine-tune\" GNNs: Suffer from objective inconsistency between pre-training (e.g., link prediction) and downstream tasks (e.g., node classification), limiting generalization.\n        *   Meta-learning (e.g., L2P-GNN): Simulates fine-tuning but doesn't fundamentally address the objective discrepancy if downstream tasks differ from simulation.\n        *   Prior graph prompting (e.g., GPPT): Limited to specific tasks (e.g., node classification), lacking a universal design for different downstream tasks like graph classification.\n        *   Other \"GraphPrompt\" models: Some exist but focus on NLP tasks with auxiliary graphs, not general GNN pre-training and prompting.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liu2023ent} proposes **GraphPrompt**, a novel framework that unifies pre-training and downstream tasks (node and graph classification) into a common \"subgraph similarity\" template, and employs task-specific learnable prompts to guide the `ReadOut` operation.\n    *   **Novelty/Difference**:\n        *   **Unified Task Template**: Both pre-training (link prediction) and downstream tasks (node/graph classification) are reformulated as calculating the similarity between (sub)graph representations. This is achieved by representing all instances (nodes, graphs) as subgraphs: a node's contextual subgraph for node-level tasks, and the entire graph as its maximum subgraph for graph-level tasks.\n        *   **Learnable Prompts for `ReadOut`**: Instead of fine-tuning the entire GNN, \\cite{liu2023ent} introduces a novel task-specific learnable prompt that modifies the `ReadOut` operation. This prompt acts as parameters for the aggregation function, allowing different downstream tasks to adaptively fuse node representations into a subgraph representation, thereby \"pulling\" relevant knowledge from the frozen pre-trained model in a task-specific manner.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A unification framework, **GraphPrompt**, that casts both pre-training (link prediction) and diverse downstream tasks (node and graph classification) as instances of learning subgraph similarity.\n        *   A novel prompting strategy that utilizes a learnable prompt to guide the `ReadOut` operation for task-specific aggregation, enabling effective exploitation of the pre-trained model without fine-tuning.\n    *   **System Design/Architectural Innovations**: The framework integrates a pre-trained GNN with a flexible prompting mechanism that adapts the `ReadOut` function based on the downstream task, allowing a single pre-trained model to serve multiple tasks.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on five public datasets. The evaluation focuses on few-shot learning settings for both node and graph classification tasks.\n    *   **Key Performance Metrics & Comparison Results**: The paper demonstrates that **GraphPrompt** achieves superior performance compared to state-of-the-art approaches across the evaluated datasets, particularly in few-shot scenarios. This indicates its effectiveness in transferring knowledge and adapting to new tasks with limited labels.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations of **GraphPrompt** itself. The framework assumes the availability of label-free graphs for pre-training and focuses on the few-shot learning setting for downstream tasks.\n    *   **Scope of Applicability**: Primarily applicable to graph representation learning using GNNs, specifically for pre-training and subsequent adaptation to node classification and graph classification tasks, especially under limited supervision (few-shot learning). The unification is based on graph topology and subgraph similarity.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023ent} significantly advances the state-of-the-art by providing a universal and unified framework for GNN pre-training and prompting that can flexibly support diverse downstream tasks (node and graph classification). It effectively bridges the objective gap between pre-training and downstream tasks, a long-standing challenge in GNNs.\n    *   **Potential Impact**: This work has the potential to make GNNs more efficient and broadly applicable by reducing the need for extensive task-specific labeled data. It opens new avenues for research into more sophisticated graph prompting mechanisms and unified pre-training objectives, potentially leading to more robust and generalizable GNN models.",
        "year": 2023,
        "citation_key": "liu2023ent"
      },
      {
        "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
        "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
        "summary": "Here's a focused summary of the paper \\cite{longa202399q} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** While Graph Neural Networks (GNNs) excel at learning on static graph-structured data, many real-world systems are dynamic, with graphs and their attributes changing over time. Traditional graph models and static GNNs are ill-equipped to capture this temporal evolution \\cite{longa202399q}.\n    *   **Importance and Challenge:** Extending GNN capabilities to temporal graphs (Temporal GNNs or TGNNs) is a promising research area. However, the field lacked a comprehensive, systematized overview, a rigorous formalization of learning settings and tasks, and a unified taxonomy for existing TGNN approaches \\cite{longa202399q}. Existing surveys were either too general, too specific, or did not provide in-depth coverage of TGNNs \\cite{longa202399q}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a survey that specifically focuses on GNN-based models for temporal graphs, acknowledging other approaches like matrix factorization, temporal motif-based methods, random walks (e.g., DyANE \\cite{longa202399q}), temporal point processes (e.g., DyRep \\cite{longa202399q}), NMF, and other deep learning techniques (e.g., DynGem \\cite{longa202399q}, TRRN \\cite{longa202399q}, STAR \\cite{longa202399q}, TSNet \\cite{longa202399q}) that also address temporal graphs \\cite{longa202399q}.\n    *   **Limitations of Previous Solutions (Surveys):** Previous surveys either discussed general temporal graph learning techniques with only brief mentions of GNNs, focused on narrow topics like temporal link prediction or graph generation, or provided GNN overviews without deep coverage of temporal aspects \\cite{longa202399q}. This work aims to fill the gap by providing the first comprehensive systematization of GNN-based methods for temporal graphs \\cite{longa202399q}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** This paper is a comprehensive survey that systematically organizes and formalizes the field of Temporal Graph Neural Networks (TGNNs). It does not propose a new TGNN model but rather provides a foundational framework for understanding existing ones \\cite{longa202399q}.\n    *   **Novelty/Difference:** The core innovation lies in its structured approach to the literature:\n        *   It introduces a rigorous formalization of learning settings and tasks specific to temporal graphs \\cite{longa202399q}.\n        *   It proposes a novel taxonomy that categorizes existing TGNN approaches based on how the temporal aspect is represented (Snapshot-based Temporal Graphs - STG, or Event-based Temporal Graphs - ETG) and processed \\cite{longa202399q}.\n        *   It defines distinct learning settings: transductive vs. inductive, and past vs. future prediction, for temporal graph tasks \\cite{longa202399q}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A coherent formalization of different learning settings (transductive, inductive, past prediction, future prediction) and tasks (e.g., temporal node classification, link prediction) for temporal graphs, unifying disparate definitions found in the literature \\cite{longa202399q}.\n        *   A comprehensive taxonomy that groups existing TGNN methods based on their temporal representation strategy (snapshot-based vs. event-based) and the mechanism used to incorporate time \\cite{longa202399q}.\n        *   Formal definitions for various types of temporal graphs, including Static Graphs (SG), Temporal Graphs (TG), Discrete Time Temporal Graphs (DTTG), Snapshot-based Temporal Graphs (STG), and Event-based Temporal Graphs (ETG) \\cite{longa202399q}.\n    *   **Theoretical Insights or Analysis:** The paper provides a structured theoretical framework for the field, clarifying fundamental concepts, learning objectives, and evaluation contexts for TGNNs \\cite{longa202399q}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{longa202399q} does not present new experimental results from its own proposed models. Instead, it reviews and categorizes the experimental validations performed by the individual TGNN papers it surveys.\n    *   **Key Performance Metrics and Comparison Results:** The paper notes that TGNNs have achieved state-of-the-art results on tasks such as temporal link prediction, node classification, and edge classification \\cite{longa202399q}. It sets the context for understanding how these models are typically evaluated, but does not provide specific metrics or comparative results within the survey itself.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the survey):** The survey explicitly focuses on methods that leverage and adapt the GNN framework to temporal graphs, acknowledging but not deeply exploring other non-GNN approaches for temporal graph learning \\cite{longa202399q}.\n    *   **Limitations of Current TGNN Methods (as identified by the survey):** The paper concludes with a discussion of the most relevant open challenges and limitations of current TGNN methods, from both research and application perspectives, though these specific challenges are not detailed in the provided abstract/introduction \\cite{longa202399q}.\n    *   **Scope of Applicability:** TGNNs are applicable to a wide range of dynamic systems, including recommendation systems, social network analysis, transportation systems, face-to-face interactions, human mobility, and epidemic modeling \\cite{longa202399q}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematized overview of Temporal Graph Neural Networks, filling a critical gap in the existing literature \\cite{longa202399q}.\n    *   **Potential Impact on Future Research:**\n        *   It offers a unified formalization and common language, which is crucial for facilitating comparison, understanding, and the development of new TGNN models \\cite{longa202399q}.\n        *   By highlighting limitations and open challenges, it provides a clear roadmap for future research directions in the field \\cite{longa202399q}.\n        *   The proposed taxonomy and formal definitions will serve as a foundational reference for researchers and practitioners working with dynamic graph data \\cite{longa202399q}.",
        "year": 2023,
        "citation_key": "longa202399q"
      }
    ],
    "layer3_papers": [
      {
        "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
        "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Traditional Graph Neural Networks (GNNs) employ a uniform global filter (e.g., low-pass for homophilic graphs, high-pass for heterophilic graphs) across all nodes \\cite{han2024rkj}. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, and these patterns can vary significantly even among different communities within the same graph \\cite{han2024rkj}.\n    *   **Why important and challenging:** A single global filter is suboptimal and can adversely affect performance on nodes with differing patterns, leading to misclassification \\cite{han2024rkj}. The challenge lies in adaptively applying appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns \\cite{han2024rkj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Most GNNs, including those with learnable graph convolutions, apply a uniform global filter across all nodes \\cite{han2024rkj}.\n    *   **Limitations of previous solutions:** The \"one-size-fits-all\" filtering strategy is ineffective for graphs with mixed homophilic and heterophilic patterns. A global filter optimized for one pattern (e.g., low-pass for homophily) can incur significant losses for nodes exhibiting other patterns (e.g., heterophily) \\cite{han2024rkj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that leverages a Mixture of Experts (MoE) approach to adaptively select and apply appropriate filters for different nodes \\cite{han2024rkj}.\n    *   **What makes this approach novel:** NODE-MOE moves beyond uniform global filtering by dynamically applying distinct filters to individual nodes based on their specific structural patterns. It integrates a gating model to assign weights to different \"expert\" GNNs, each potentially equipped with a different filter type, allowing for node-specific processing \\cite{han2024rkj}.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical insights:** The paper theoretically demonstrates, using a Contextual Stochastic Block Model (CSBM), that a global filter optimized for one pattern can incur significant losses for nodes with other patterns, while node-wise filtering can achieve linear separability for all nodes under mild conditions (Theorem 1) \\cite{han2024rkj}.\n    *   **Novel framework:** Introduction of NODE-MOE, a flexible and efficient Mixture of Experts framework for node-wise filtering in GNNs \\cite{han2024rkj}.\n    *   **Gating Model design:** A novel gating model that estimates node patterns by incorporating contextual features `[X, |AX-X|, |A^2X-X|]` and uses a GNN (e.g., GIN) with low-pass filters to ensure neighboring nodes receive similar expert selections, leveraging community detection capabilities \\cite{han2024rkj}.\n    *   **Expert Model strategy:** Utilizes GNNs with learnable graph convolutions as experts, initialized with diverse filter types (e.g., low-pass, constant, high-pass) to encourage specialization and handle different structural patterns \\cite{han2024rkj}.\n    *   **Filter Smoothing Loss:** Introduction of a `L_s = sum(|f_o(x_i) - f_o(x_{i-1})|^2)` loss to ensure learned filters exhibit smooth behavior in the spectral domain, mitigating training challenges and improving interpretability \\cite{han2024rkj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on both homophilic (Cora, CiteSeer) and heterophilic (Chameleon, Squirrel) graph datasets \\cite{han2024rkj}.\n    *   **Key performance metrics and comparison results:** The experiments demonstrate the effectiveness of NODE-MOE, illustrating significant performance improvement on both types of graphs \\cite{han2024rkj}. The gating model is shown to efficiently assign different nodes to their suitable filters (Section 4.3) \\cite{han2024rkj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The framework addresses the challenge of incorporating various filters and selecting appropriate ones without ground truth on node patterns. The filter smoothing loss is introduced to mitigate issues like filter oscillations and improve interpretability when training multiple filters simultaneously \\cite{han2024rkj}.\n    *   **Scope of applicability:** NODE-MOE is primarily designed for node classification tasks on graphs that exhibit a complex mixture of homophilic and heterophilic structural patterns \\cite{han2024rkj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** NODE-MOE represents a significant advancement by moving beyond the limitations of uniform global filtering in GNNs. It provides a principled and adaptive approach to handle the diverse structural patterns prevalent in real-world graphs, which is a common challenge for existing GNN models \\cite{han2024rkj}.\n    *   **Potential impact on future research:** This work opens avenues for future research into more sophisticated gating mechanisms, dynamic expert selection, and the application of node-wise filtering to other graph learning tasks beyond node classification. The theoretical foundation also encourages further analysis of GNN behavior on mixed-pattern graphs \\cite{han2024rkj}.",
        "year": 2024,
        "citation_key": "han2024rkj"
      },
      {
        "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
        "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation and bullet format:\n\n*   **CITATION**: \\cite{li202444f}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Building transferable Graph Neural Networks (GNNs) with a Contrastive Language-Image Pre-training (CLIP)-like pipeline is challenging for general graph data. Specifically, how to adapt pre-trained GNNs to a semantic embedding space given limited downstream data (few samples and extremely weak text supervision).\n    *   **Importance & Challenge**:\n        *   GNNs, optimized by numerical labels, lack real-world semantic understanding, unlike vision models benefiting from natural language supervision (e.g., CLIP).\n        *   **Challenges for general graph data**:\n            1.  **Data Scarcity & Weak Text Supervision**: Graph datasets are scarce, and text labels are often very short (e.g., a few tokens), making joint pre-training of graph and text encoders impractical.\n            2.  **Diverse Task Levels**: Graph tasks exist at node, edge, and graph levels.\n            3.  **Conceptual Gaps**: The same graph structure can have different interpretations across domains, unlike consistent language tokens or visual objects.\n        *   Even with independently pre-trained GNNs (via self-supervision) and Large Language Models (LLMs), effectively aligning them and adapting to diverse downstream tasks remains non-trivial.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: CLIP-style frameworks have been successfully extended to vision, video, 3D images, speech, and audio, demonstrating enhanced transferability through text alignment.\n    *   **Graph-Text Alignment in Specific Domains**: Previous work explored graph-text alignment primarily in molecular domains (e.g., Luo et al., 2023) and text-attributed graphs (e.g., Wen and Fang, 2023), where sufficient paired graph-text data is available for joint pre-training.\n    *   **Limitations of Previous Solutions**:\n        *   These existing graph-text alignment methods are not suitable for general graph data due to the scarcity of graph data and the *extremely weak* nature of text supervision (e.g., single-word labels).\n        *   Direct fine-tuning of large GNNs or LLMs with limited downstream data is inefficient and resource-intensive.\n        *   The state-of-the-art graph prompting method (AIO by Sun et al., 2023a) suffers from unstable optimization and poor representation learning due to dense, overwhelming cross-connections between prompt tokens and input graph nodes.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{li202444f} proposes **Morpher (Multi-modal Prompt Learning for Graph Neural Networks)**, a prompting-based paradigm that aligns pre-trained GNN representations with the semantic embedding space of pre-trained LLMs. It achieves this by simultaneously learning both graph prompts and text prompts, while keeping the parameters of the GNN and LLM frozen.\n    *   **Key Steps**:\n        1.  **Improved Graph Prompt Design**: Addresses the instability of prior graph prompting by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. It constrains cross-connections to be sparse (at most `ne/a` prompt tokens per node) and uses cosine similarity for connection calculation, preventing prompt features from overwhelming original graph features.\n        2.  **Multi-modal Prompting**: Introduces tunable text prompts (`Pt_theta`) and the *improved* graph prompts (`Pg_theta`).\n        3.  **Cross-modal Projector**: A `tanh`-activated linear layer (`Proj_theta(v) := tanh(Wv+b)`) maps the `dg`-dimensional graph embeddings to the `dt`-dimensional text embedding space, resolving dimension mismatch.\n        4.  **Semantic Alignment**: Graph embeddings (after prompting and readout) are normalized and projected. Text embeddings (after prompting and readout) are normalized to a unit sphere after mean subtraction.\n        5.  **Contrastive Learning**: An in-batch similarity-based contrastive loss (`LG->T`) is used to train the graph prompts, text prompts, and the cross-modal projector, aligning the graph and text representations in the shared semantic space.\n    *   **Novelty/Difference**:\n        *   First paradigm to perform graph-text multi-modal prompt learning for GNNs, specifically designed for scenarios with *extremely weak text supervision* and *independently pre-trained* GNNs and LLMs.\n        *   Introduces a novel, stable graph prompt design that overcomes the limitations of previous methods by ensuring balanced connections.\n        *   Enables CLIP-style zero-shot generalization for GNNs to unseen classes, a capability previously unexplored for general graph data with weak supervision.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Improved Graph Prompt Design**: A new method for constructing graph prompts that ensures stable training and prevents prompt features from overwhelming original graph information by balancing cross-connections.\n        *   **Morpher Paradigm**: The first graph-text multi-modal prompt learning framework that effectively adapts pre-trained GNNs to semantic spaces of LLMs using only weak text supervision, without fine-tuning the backbone models.\n    *   **System Design/Architectural Innovations**:\n        *   Integration of independently pre-trained GNNs and LLMs via a cross-modal projector and multi-modal prompt learning, creating a flexible and efficient adaptation mechanism.\n    *   **Theoretical Insights/Analysis**:\n        *   Analysis of the instability issue in existing graph prompt designs, attributing it to the imbalance of connections and the nature of sparse input features.\n        *   Demonstrates that semantic text embedding spaces can be leveraged without joint pre-training, and prompt learning is a superior adaptation strategy for limited data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Few-shot Learning**: Evaluated graph-level classification performance under a challenging few-shot setting (<= 10 labeled samples per class).\n        *   **Multi-task-level Learning**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Cross-domain Generalization**: (Mentioned in abstract/intro, but details not in provided text).\n        *   **Zero-shot Generalization**: Demonstrated a CLIP-style zero-shot classification prototype for GNNs to predict unseen classes.\n    *   **Datasets**: Real-world graph datasets including molecular (MUTAG), bioinformatic (ENZYMES, PROTEINS), computer vision (MSRC_21C), and citation networks (Cora, CiteSeer, PubMed). Text labels were real-world class names, typically <= 5 words.\n    *   **GNN Backbones & Pre-training**: GCN, GAT, GraphTransformer (GT) pre-trained with GraphCL and SimGRACE (also GraphMAE, MVGRL in Appendix). LLM encoders: RoBERTa (main), ELECTRA, DistilBERT (Appendix).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **ImprovedAIO**: Consistently outperformed all existing graph prompting baselines (e.g., AIO) and traditional fine-tuning methods in few-shot graph-level classification across various datasets and GNN backbones. This improvement is attributed to its stable training and optimization.\n        *   **Morpher**: Achieved further *absolute accuracy improvement* over \"ImprovedAIO\" and all other baselines across all evaluated datasets (e.g., up to 79.33% Acc on MUTAG with GraphCL+GAT, compared to 74.67% for ImprovedAIO and 70.00% for fine-tune).\n        *   **Significance**: Morpher's superior performance, even with extremely weak text supervision, validates its ability to dynamically adapt and align graph and language representation spaces, effectively leveraging semantic information.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of independently pre-trained GNNs and LLMs.\n        *   Relies on \"extremely weak text supervision,\" which, while a strength, also defines the specific problem setting.\n        *   The problem setup primarily focuses on graph-level classification, though node/edge tasks can be reformulated.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to scenarios where graph data is scarce, and text supervision for labels is minimal (e.g., single-word or short phrase labels).\n        *   Designed for adapting existing pre-trained GNNs rather than end-to-end joint pre-training.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **Bridging GNNs and LLMs**: \\cite{li202444f} provides a novel and effective method to bridge the gap between GNNs and LLMs, enabling GNNs to \"understand language\" even with minimal textual input, a significant step towards more semantically aware graph models.\n        *   **Robust Graph Prompting**: Introduces a more stable and effective graph prompting mechanism, addressing critical issues in prior designs and improving the adaptability of GNNs.\n        *   **Zero-shot Generalization for GNNs**: Establishes the first prototype for CLIP-style zero-shot classification for GNNs, allowing them to generalize to unseen classes without explicit training data for those classes, which is crucial for real-world applications with evolving data.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research in multi-modal learning for graphs, particularly in low-resource settings.\n        *   Could inspire further work on integrating semantic knowledge from LLMs into GNNs for various tasks beyond classification (e.g., graph generation, reasoning).\n        *   The improved graph prompt design could become a foundational component for future graph prompting research.\n        *   The zero-shot capability has implications for developing more adaptable and generalizable GNNs in domains where new classes frequently emerge.",
        "year": 2024,
        "citation_key": "li202444f"
      }
    ],
    "layer2_summary": null
  },
  "398d6f4432e6aa7acf21c0bbaaebac48998faad3": {
    "seed_title": "Graph Neural Networks for Social Recommendation",
    "summary": "**1. Evolution Analysis (Chronological List):**\n\n*   **[fan2019k6u] Graph Neural Networks for Social Recommendation (2019)**\n    *   **Methodological/Conceptual Shift:** This paper represents an early, task-oriented application of GNNs, focusing on integrating diverse, heterogeneous information sources (user-item interactions, opinions, social relations) into a unified GNN framework for a specific real-world problem (social recommendation). The core conceptual shift is leveraging GNNs to learn rich latent factors by explicitly modeling multiple graph structures and node attributes.\n    *   **Problems Addressed:**\n        *   Effectively combining information from two distinct graphs: a user-user social graph and a user-item interaction graph.\n        *   Jointly capturing both user-item interactions and their associated explicit opinions (e.g., rating scores).\n        *   Differentiating the importance of heterogeneous social relations (tie strengths) and user-item interactions.\n    *   **Innovations/Capabilities Introduced:**\n        *   **GraphRec Framework:** A comprehensive GNN architecture specifically designed for social recommendation.\n        *   **Opinion Embedding Vectors:** A novel mechanism to incorporate explicit rating opinions into interaction representations.\n        *   **Attention Mechanisms:** Introduction of three distinct attention mechanisms (item, social, user attention) to dynamically weigh contributions from different neighbors and interactions, moving beyond simpler, uniform aggregation.\n    *   **Temporal Context:** Published in 2019, it reflects the burgeoning interest in applying GNNs to complex, multi-relational data, demonstrating their power in learning representations for recommendation systems.\n\n*   **[mao202313j] Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All? (2023)**\n    *   **Methodological/Conceptual Shift:** This paper marks a significant shift from *applying* GNNs to a specific task to *analyzing the fundamental limitations and intrinsic behavior* of GNNs themselves. It moves from designing GNNs for a problem to understanding *why* existing GNNs might fail on certain parts of a graph. The conceptual shift is towards recognizing and rigorously analyzing \"structural disparity\" (mixed homophilic and heterophilic patterns) as a pervasive issue in real-world graphs.\n    *   **Problems Addressed (that [fan2019k6u] left unsolved or unexplored):**\n        *   The inherent \"performance disparity\" of GNNs, where they perform well on nodes aligning with the graph's dominant structural pattern but struggle on minority patterns.\n        *   The \"one-size-fits-all\" nature of GNN aggregation, which implicitly assumes a uniform structural pattern across the entire graph, leading to suboptimal performance on graphs with mixed homophilic and heterophilic patterns.\n        *   Lack of theoretical understanding of the underlying causes of this performance disparity.\n    *   **Innovations/Capabilities Introduced:**\n        *   **Rigorous Disparity Analysis:** The first systematic investigation of GNN performance on *subgroups* of nodes defined by their local homophily ratios.\n        *   **CSBM-Structure (CSBM-S) Model:** A novel graph generation model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes, enabling controlled experimental studies.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound:** A novel theoretical framework that formally explains performance disparity by identifying aggregated feature distance and homophily ratio differences as critical factors.\n    *   **Temporal Context:** The 4-year gap (2019-2023) suggests that as GNNs became more widely adopted and applied to diverse datasets, their limitations on complex, heterogeneous real-world graphs became apparent, prompting deeper analytical research into their fundamental mechanisms.\n\n*   **[han2024rkj] Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach (2024)**\n    *   **Methodological/Conceptual Shift:** This paper directly builds upon the problem identified by [mao202313j], shifting from *diagnosing* the problem of structural disparity to *proposing a concrete architectural solution*. The core conceptual shift is from global, uniform filtering (even with attention) to adaptive, node-wise filtering, recognizing that different nodes within the same graph may require fundamentally different aggregation strategies.\n    *   **Problems Addressed (that [mao202313j] identified but didn't solve):**\n        *   The practical challenge of designing GNNs that can effectively handle graphs with a complex mix of homophilic and heterophilic patterns at the node level.\n        *   Overcoming the suboptimality of a single global filter for diverse node patterns, which leads to misclassification.\n        *   How to adaptively select and apply appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns.\n    *   **Innovations/Capabilities Introduced:**\n        *   **NODE-MOE Framework:** A novel Mixture of Experts (MoE) GNN architecture for adaptive node-wise filtering.\n        *   **Novel Gating Model:** Learns to estimate node patterns by incorporating contextual features and a GNN, ensuring community-aware expert selection.\n        *   **Diverse Expert Models:** Utilizes GNNs with learnable graph convolutions, initialized with varied filter types (low-pass, high-pass, constant) to encourage specialization in different structural patterns.\n        *   **Filter Smoothing Loss:** A regularization technique to ensure stable and interpretable learning of multiple filters.\n    *   **Temporal Context:** The rapid follow-up (2023-2024) indicates the urgency and importance of addressing the \"structural disparity\" problem once it was rigorously identified. It showcases a quick transition from theoretical understanding to practical solution development.\n\n---\n\n**2. Evolution Analysis (Cohesive Narrative):**\n\nThe progression of research from [fan2019k6u] to [han2024rkj] reveals a significant intellectual trajectory in Graph Neural Networks: a shift from **task-specific GNN design with enhanced aggregation** towards a deeper **understanding and mitigation of fundamental GNN limitations related to graph structural heterogeneity**.\n\n*Trend 1: From Task-Specific GNNs to Foundational Analysis of GNN Behavior*\n\n*   *Methodological progression*: The journey begins with [fan2019k6u] Graph Neural Networks for Social Recommendation (2019), which exemplifies the early wave of GNN research focused on applying these models to complex real-world problems. Its methodology centers on designing a bespoke GNN architecture, GraphRec, to integrate diverse data types (user-item interactions, opinions, social relations) for a specific task. This involves developing sophisticated aggregation mechanisms, such as attention networks, to weigh different information sources. The progression then shifts dramatically with [mao202313j] Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All? (2023). Instead of proposing a new GNN model, this paper employs a rigorous analytical methodology, combining empirical observations, theoretical modeling (e.g., the novel CSBM-Structure model), and formal generalization bounds (non-i.i.d PAC-Bayesian) to dissect the intrinsic behavior of GNNs. This represents a move from constructive model building to foundational scientific inquiry.\n*   *Problem evolution*: [fan2019k6u] addresses the practical challenges of building effective social recommender systems, specifically how to coherently integrate dual graphs, capture opinions, and handle heterogeneous social strengths within a GNN. While it enhances aggregation with attention, it implicitly assumes that a single, albeit weighted, aggregation strategy can serve all nodes. [mao202313j] then uncovers a more fundamental problem: GNNs exhibit \"performance disparity\" on real-world graphs due to inherent \"structural disparity\" (mixed homophilic and heterophilic patterns). It highlights that the \"one-size-fits-all\" aggregation strategy, even with attention, fundamentally struggles with nodes that deviate from the graph's dominant structural pattern. This problem was largely unexplored in earlier task-focused GNNs like GraphRec.\n*   *Key innovations*: [fan2019k6u]'s key innovations include the GraphRec framework itself, opinion embedding vectors for joint interaction-opinion capture, and multiple attention mechanisms (item, social, user attention) to handle various forms of heterogeneity in a task-specific manner. In contrast, [mao202313j] introduces the first rigorous analysis of GNN performance disparity, the novel CSBM-Structure model for generating mixed-pattern graphs, and a groundbreaking non-i.i.d PAC-Bayesian generalization bound that theoretically explains the root causes of this disparity, providing insights into the limitations of uniform GNN aggregation.\n\n*Trend 2: From Global/Uniform Aggregation to Adaptive, Node-Wise Filtering*\n\n*   *Methodological progression*: Building on the insights from [mao202313j], the research progresses to [han2024rkj] Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach (2024). While [fan2019k6u] used attention to *modulate* a global aggregation, and [mao202313j] *diagnosed* the failure of global aggregation, [han2024rkj] introduces a radical methodological shift: \"node-wise filtering.\" This involves moving from a single, globally applied filter to dynamically selecting and applying *different types of filters* for *individual nodes*. The core methodology is the Mixture of Experts (MoE) paradigm, where a gating model learns to assign nodes to specialized \"expert\" GNNs, each potentially equipped with a distinct filter type.\n*   *Problem evolution*: [mao202313j] clearly articulated that a global filter, optimized for one structural pattern (e.g., homophily), incurs significant losses for nodes exhibiting other patterns (e.g., heterophily). This leaves the problem of *how to practically overcome* this limitation. [han2024rkj] directly tackles this by addressing the challenge of adaptively applying appropriate filters to individual nodes based on their specific, often mixed, structural patterns, without requiring explicit ground truth on these patterns. It solves the \"one-size-fits-all\" problem identified by its predecessor.\n*   *Key innovations*: [han2024rkj]'s primary innovation is the NODE-MOE framework, which enables flexible and efficient node-wise filtering. This includes a novel gating model that leverages contextual features and a GNN to estimate node patterns and make community-aware expert selections. Furthermore, it introduces the strategy of initializing expert GNNs with diverse filter types (low-pass, constant, high-pass) to encourage specialization, and a Filter Smoothing Loss to ensure stable and interpretable learning of these diverse filters. This framework provides a concrete solution to the fundamental aggregation problem highlighted by [mao202313j].\n\n**3. Synthesis:**\n\nThis chain of research illustrates a maturation of the Graph Neural Network field, moving from initial successes in applying GNNs to complex tasks by enhancing aggregation, to a critical self-reflection on their fundamental limitations. Collectively, these works advance GNNs by highlighting the crucial role of structural heterogeneity, providing a deep theoretical understanding of its impact, and offering a practical, adaptive framework for robust GNN performance across diverse graph patterns.",
    "path": [
      "398d6f4432e6aa7acf21c0bbaaebac48998faad3",
      "707142f242ee4e40489062870ca53810cb33d404",
      "f5aa366ff70215f06ae6501c322eba2f0934a7c3"
    ],
    "layer1_papers": [
      {
        "title": "Graph Neural Networks for Social Recommendation",
        "abstract": "In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.",
        "summary": "Here's a focused summary of the paper \\cite{fan2019k6u} for a literature review:\n\n### Technical Paper Analysis: Graph Neural Networks for Social Recommendation \\cite{fan2019k6u}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses challenges in building social recommender systems using Graph Neural Networks (GNNs). Specifically, it aims to effectively integrate diverse information sources—user-item interactions, associated opinions, and heterogeneous social relations—within a unified GNN framework to learn better user and item latent factors for recommendation.\n    *   **Importance and Challenge**:\n        *   **Importance**: Social relations are crucial for boosting recommendation performance, as users are influenced by their social circles. GNNs offer a powerful paradigm for learning representations on graph data, which naturally represents social and user-item interactions.\n        *   **Challenges**:\n            1.  **Dual Graph Integration**: Users are involved in two distinct graphs: a user-user social graph and a user-item interaction graph. Coherently combining information from both is challenging.\n            2.  **Interaction and Opinion Capture**: The user-item graph contains not just interactions but also users' explicit opinions (e.g., rating scores), which need to be jointly captured.\n            3.  **Heterogeneous Social Strengths**: Social networks often have varied tie strengths (strong vs. weak ties), and treating all social relations equally can degrade recommendation quality. Distinguishing these heterogeneous strengths is vital.\n\n2.  **Related Work & Positioning**\n    *   The paper positions itself within the growing field of social recommender systems and deep neural networks for graph data (GNNs).\n    *   **Limitations of Previous Solutions (Implied)**: Traditional social recommendation methods might not fully leverage the topological structure and node information simultaneously, or might struggle with the complex interplay of interactions, opinions, and heterogeneous social ties. Mean-based aggregators in GNNs, for instance, are noted as a limitation for not differentiating the importance of neighbors or interactions. The paper aims to overcome these by introducing attention mechanisms and opinion embeddings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **GraphRec**, a novel Graph Neural Network framework for social recommendations. GraphRec consists of three main components: user modeling, item modeling, and rating prediction.\n    *   **Novelty/Difference**:\n        *   **Unified Dual-Graph Modeling**: GraphRec coherently models both the user-user social graph and the user-item graph by learning user latent factors from two distinct perspectives: \"item-space\" (via user-item interactions) and \"social-space\" (via social connections), which are then combined.\n        *   **Opinion-Aware Interaction Representation**: For both user and item modeling, it introduces \"opinion embedding vectors\" (`er`) for each rating level. These are concatenated with item/user embeddings and processed through an MLP to create \"opinion-aware interaction representations\" (`xia` for items, `fjt` for users), allowing the model to jointly capture interactions and their associated opinions.\n        *   **Attention Mechanisms for Heterogeneity**:\n            *   **Item Attention (`αia`)**: Assigns individualized weights to each user-item interaction when learning item-space user latent factors, allowing different interactions to contribute differently based on their relevance to the target user.\n            *   **Social Attention (`βio`)**: Differentiates the importance (tie strengths) of social friends when learning social-space user latent factors, addressing the challenge of heterogeneous social relations.\n            *   **User Attention (`µjt`)**: Differentiates the importance of users interacting with an item when learning item latent factors, capturing heterogeneous influence from user-item interactions.\n        *   These attention mechanisms replace simpler mean-based aggregators, providing a more nuanced aggregation.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **GraphRec Framework**: A comprehensive GNN architecture specifically designed for social recommendation, integrating dual graphs and various forms of heterogeneity.\n        *   **Opinion Embedding and Opinion-Aware Interaction Representation**: A principled method to jointly capture interaction and opinion information by fusing item/user embeddings with dedicated opinion embeddings via MLPs.\n        *   **Attention-based Aggregation for Heterogeneous Information**: Introduction of three distinct attention mechanisms (item attention, social attention, user attention) to dynamically weigh the contributions of interactions, social friends, and interacting users, respectively, moving beyond uniform aggregation.\n    *   **System Design/Architectural Innovations**:\n        *   **Two-Stream User Modeling**: Learning user latent factors by separately aggregating information from the item-space (user-item graph) and social-space (user-user graph) and then combining them.\n        *   **Multi-Layer Perceptrons (MLPs)**: Used extensively for transforming and combining embeddings, and for parameterizing attention networks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted to demonstrate the effectiveness of the proposed GraphRec framework.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that experiments were performed on \"two real-world datasets\" and demonstrated the \"effectiveness of the proposed framework GraphRec.\" (Specific datasets and metrics are not detailed in the provided snippet but are typically found in the full paper's experimental section).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract and introduction do not explicitly state technical limitations or assumptions beyond the challenges it aims to solve. However, GNNs generally assume graph structure is available and can be computationally intensive for very large graphs. The model's reliance on explicit ratings for opinion embeddings might be a limitation in implicit feedback scenarios.\n    *   **Scope of Applicability**: The framework is designed for social recommender systems where both user-item interaction data (with opinions/ratings) and user-user social network data are available.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{fan2019k6u} advances the technical state-of-the-art by providing a principled and comprehensive GNN-based solution that simultaneously addresses three critical challenges in social recommendation: dual-graph integration, joint interaction-opinion modeling, and heterogeneous social tie strengths. Its use of attention mechanisms for fine-grained weighting of different information sources is a significant improvement over simpler aggregation methods.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in:\n        *   Developing more sophisticated attention mechanisms for complex graph structures.\n        *   Extending GNNs to handle other forms of heterogeneity (e.g., different types of items, multi-faceted social relations).\n        *   Applying similar dual-graph and opinion-aware modeling techniques to other domains beyond recommendation where multiple interconnected graphs and nuanced node attributes exist.\n        *   Exploring the interpretability of the learned attention weights to understand user preferences and social influence.",
        "year": 2019,
        "citation_key": "fan2019k6u"
      }
    ],
    "layer2_papers": [
      {
        "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
        "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Graph Neural Networks (GNNs) exhibit a \"performance disparity\" on real-world graphs, performing well on nodes that align with the graph's dominant structural pattern (e.g., homophilic nodes in a homophilic graph) but struggling on nodes exhibiting the opposite pattern (e.g., heterophilic nodes in a homophilic graph) \\cite{mao202313j}. This disparity arises because real-world graphs inherently possess \"structural disparity,\" meaning they are a mixture of both homophilic and heterophilic node patterns.\n    *   **Importance & Challenge**: Existing GNN research often focuses on overall graph performance, overlooking the nuanced behavior of GNNs on these diverse node subgroups. Understanding this performance disparity is critical for developing GNNs that are robust and effective across all node types in complex, real-world graph structures \\cite{mao202313j}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior studies on GNN effectiveness primarily analyze performance on the entire graph and typically focus on graphs that are predominantly either homophilic or heterophilic \\cite{mao202313j}.\n    *   **Limitations of previous solutions**: These approaches fail to account for the common real-world scenario where graphs contain a mixture of homophilic and heterophilic patterns. They do not provide insights into GNN performance on specific node subgroups, potentially masking scenarios where GNNs perform poorly on minority patterns despite achieving good overall results \\cite{mao202313j}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Empirical Disparity Analysis**: Systematically investigates GNN performance by categorizing testing nodes based on their local homophily ratios, comparing a vanilla GNN (GCN) against MLP-based models (vanilla MLP and GLNN) \\cite{mao202313j}.\n        *   **Theoretical Analysis of Aggregation**: Examines how the GNN aggregation mechanism differentially impacts nodes with varying structural patterns. This involves introducing a novel graph generation model, CSBM-Structure (CSBM-S), and deriving theoretical insights into aggregated feature distances and class probabilities \\cite{mao202313j}.\n        *   **Generalization Bound**: Develops a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs to formally explain the underlying causes of the observed performance disparity \\cite{mao202313j}.\n        *   **Discriminative Ability Quantification**: Empirically analyzes the discriminative ability of GNNs on majority versus minority nodes using a \"relative discriminative ratio\" based on class prototypes \\cite{mao202313j}.\n    *   **Novelty/Difference**:\n        *   **First Rigorous Disparity Analysis**: This work is novel in its focused and systematic investigation of GNN performance on *subgroups* of nodes defined by their local structural patterns (homophilic vs. heterophilic), moving beyond aggregate performance metrics \\cite{mao202313j}.\n        *   **Novel CSBM-Structure (CSBM-S) Model**: Introduces a new variant of the Contextual Stochastic Block Model that explicitly allows for the simultaneous presence of homophilic and heterophilic nodes within a single graph, providing a more realistic testbed for structural disparity \\cite{mao202313j}.\n        *   **Non-i.i.d PAC-Bayesian Generalization Bound**: Presents a novel theoretical framework that precisely identifies aggregated feature distance and homophily ratio differences between training and testing nodes as critical factors driving performance disparity \\cite{mao202313j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Theoretical Framework**: A non-i.i.d PAC-Bayesian generalization bound for GNNs that rigorously explains performance disparity by identifying aggregated feature distance and homophily ratio differences between training and testing nodes as key factors \\cite{mao202313j}.\n    *   **Novel Graph Generation Model**: Introduction of CSBM-Structure (CSBM-S), a variant of CSBM, which enables controlled experimental studies of graphs exhibiting mixed homophilic and heterophilic patterns \\cite{mao202313j}.\n    *   **Theoretical Insights into Aggregation**: Demonstrates that GNN aggregation creates a significant feature distance between homophilic and heterophilic node subgroups *within the same class*, and that differences in homophily ratio directly influence class prediction probabilities \\cite{mao202313j}.\n    *   **Practical Implications**: Provides insights into the effectiveness of deeper GNNs and identifies an overlooked distribution shift factor, leading to a new scenario for the graph out-of-distribution problem \\cite{mao202313j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Structural Disparity Verification**: Analyzed node homophily ratio distributions on real-world homophilic (Ogbn-arxiv, Pubmed) and heterophilic (Chameleon, Squirrel) datasets, confirming the pervasive existence of mixed structural patterns \\cite{mao202313j}.\n        *   **Subgroup Performance Comparison**: Conducted experiments comparing the accuracy of GCN against vanilla MLP and GLNN on test nodes, specifically grouped by their homophily ratio ranges \\cite{mao202313j}.\n        *   **Discriminative Ratio Analysis**: Empirically calculated a \"relative discriminative ratio\" to quantify the ease of prediction for majority versus minority nodes, observing its variation with the number of aggregation layers \\cite{mao202313j}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Accuracy Differences**: Demonstrated that in homophilic graphs, MLP-based models often outperform GCN on heterophilic (minority) nodes, while GCN performs better on homophilic (majority) nodes. Conversely, in heterophilic graphs, MLP models frequently outperform GCN on homophilic (minority) nodes, with GCN excelling on heterophilic (majority) nodes \\cite{mao202313j}.\n        *   **Discriminative Ratio**: The analysis aimed to illustrate how aggregation influences the discriminative ability, with a lower ratio indicating that majority nodes are easier to predict than minority nodes \\cite{mao202313j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The theoretical analysis (e.g., Lemma 1) employs simplifying assumptions like balanced class distribution and shared aggregated feature variance for mathematical elegance \\cite{mao202313j}. The CSBM-S model assumes consistent degree distributions and specific probability relationships \\cite{mao202313j}.\n    *   **Scope of Applicability**: The empirical validation primarily focuses on GCN and MLP-based models, and the findings are most directly applicable to semi-supervised node classification tasks \\cite{mao202313j}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of GNN behavior by providing a detailed, subgroup-level analysis of performance, directly addressing the challenges posed by structural disparity in real-world graphs \\cite{mao202313j}.\n    *   **Potential Impact on Future Research**:\n        *   **Informed GNN Design**: The identified factors (aggregated feature distance, homophily ratio differences) and the proposed PAC-Bayesian bound offer a theoretical foundation to guide the development of new GNN architectures and training strategies that are more robust and equitable across diverse node patterns \\cite{mao202313j}.\n        *   **Graph Out-of-Distribution (OOD) Problem**: By highlighting an overlooked distribution shift factor related to structural patterns, the work introduces a new dimension to the graph OOD problem, opening new avenues for research in OOD generalization for graphs \\cite{mao202313j}.\n        *   **Deeper GNNs**: Provides novel insights into the mechanisms behind the effectiveness of deeper GNNs in the context of structural disparity \\cite{mao202313j}.",
        "year": 2023,
        "citation_key": "mao202313j"
      }
    ],
    "layer3_papers": [
      {
        "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
        "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Traditional Graph Neural Networks (GNNs) employ a uniform global filter (e.g., low-pass for homophilic graphs, high-pass for heterophilic graphs) across all nodes \\cite{han2024rkj}. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, and these patterns can vary significantly even among different communities within the same graph \\cite{han2024rkj}.\n    *   **Why important and challenging:** A single global filter is suboptimal and can adversely affect performance on nodes with differing patterns, leading to misclassification \\cite{han2024rkj}. The challenge lies in adaptively applying appropriate filters to individual nodes based on their specific structural patterns without explicit ground truth on node patterns \\cite{han2024rkj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Most GNNs, including those with learnable graph convolutions, apply a uniform global filter across all nodes \\cite{han2024rkj}.\n    *   **Limitations of previous solutions:** The \"one-size-fits-all\" filtering strategy is ineffective for graphs with mixed homophilic and heterophilic patterns. A global filter optimized for one pattern (e.g., low-pass for homophily) can incur significant losses for nodes exhibiting other patterns (e.g., heterophily) \\cite{han2024rkj}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces NODE-MOE (Node-wise Filtering via Mixture of Experts), a novel GNN framework that leverages a Mixture of Experts (MoE) approach to adaptively select and apply appropriate filters for different nodes \\cite{han2024rkj}.\n    *   **What makes this approach novel:** NODE-MOE moves beyond uniform global filtering by dynamically applying distinct filters to individual nodes based on their specific structural patterns. It integrates a gating model to assign weights to different \"expert\" GNNs, each potentially equipped with a different filter type, allowing for node-specific processing \\cite{han2024rkj}.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical insights:** The paper theoretically demonstrates, using a Contextual Stochastic Block Model (CSBM), that a global filter optimized for one pattern can incur significant losses for nodes with other patterns, while node-wise filtering can achieve linear separability for all nodes under mild conditions (Theorem 1) \\cite{han2024rkj}.\n    *   **Novel framework:** Introduction of NODE-MOE, a flexible and efficient Mixture of Experts framework for node-wise filtering in GNNs \\cite{han2024rkj}.\n    *   **Gating Model design:** A novel gating model that estimates node patterns by incorporating contextual features `[X, |AX-X|, |A^2X-X|]` and uses a GNN (e.g., GIN) with low-pass filters to ensure neighboring nodes receive similar expert selections, leveraging community detection capabilities \\cite{han2024rkj}.\n    *   **Expert Model strategy:** Utilizes GNNs with learnable graph convolutions as experts, initialized with diverse filter types (e.g., low-pass, constant, high-pass) to encourage specialization and handle different structural patterns \\cite{han2024rkj}.\n    *   **Filter Smoothing Loss:** Introduction of a `L_s = sum(|f_o(x_i) - f_o(x_{i-1})|^2)` loss to ensure learned filters exhibit smooth behavior in the spectral domain, mitigating training challenges and improving interpretability \\cite{han2024rkj}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on both homophilic (Cora, CiteSeer) and heterophilic (Chameleon, Squirrel) graph datasets \\cite{han2024rkj}.\n    *   **Key performance metrics and comparison results:** The experiments demonstrate the effectiveness of NODE-MOE, illustrating significant performance improvement on both types of graphs \\cite{han2024rkj}. The gating model is shown to efficiently assign different nodes to their suitable filters (Section 4.3) \\cite{han2024rkj}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The framework addresses the challenge of incorporating various filters and selecting appropriate ones without ground truth on node patterns. The filter smoothing loss is introduced to mitigate issues like filter oscillations and improve interpretability when training multiple filters simultaneously \\cite{han2024rkj}.\n    *   **Scope of applicability:** NODE-MOE is primarily designed for node classification tasks on graphs that exhibit a complex mixture of homophilic and heterophilic structural patterns \\cite{han2024rkj}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** NODE-MOE represents a significant advancement by moving beyond the limitations of uniform global filtering in GNNs. It provides a principled and adaptive approach to handle the diverse structural patterns prevalent in real-world graphs, which is a common challenge for existing GNN models \\cite{han2024rkj}.\n    *   **Potential impact on future research:** This work opens avenues for future research into more sophisticated gating mechanisms, dynamic expert selection, and the application of node-wise filtering to other graph learning tasks beyond node classification. The theoretical foundation also encourages further analysis of GNN behavior on mixed-pattern graphs \\cite{han2024rkj}.",
        "year": 2024,
        "citation_key": "han2024rkj"
      }
    ],
    "layer2_summary": null
  },
  "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9": {
    "seed_title": "How Powerful are Graph Neural Networks?",
    "summary": "\n\nI apologize, but I cannot complete the analysis as the list of \"Papers to reference (sorted chronologically):\" was not provided in your prompt. Please provide the papers and their summaries so I can proceed with the task.",
    "path": [
      "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9"
    ],
    "layer1_papers": [
      {
        "title": "How Powerful are Graph Neural Networks?",
        "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
        "summary": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
        "year": 2018,
        "citation_key": "xu2018c8q"
      }
    ],
    "layer2_papers": [],
    "layer3_papers": [],
    "layer2_summary": null
  }
}