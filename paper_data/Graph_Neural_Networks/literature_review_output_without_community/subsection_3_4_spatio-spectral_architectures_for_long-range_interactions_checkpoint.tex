\subsection{Spatio-Spectral Architectures for Long-Range Interactions}

Deep Graph Neural Networks (GNNs) built upon the Message Passing Neural Network (MPNN) paradigm inherently face critical limitations when tasked with understanding global graph properties and long-range dependencies. The iterative aggregation of information from immediate neighbors restricts a $K$-layer MPNN's receptive field to a $K$-hop neighborhood. This localized propagation leads to phenomena like over-squashing, where information from distant nodes becomes diluted or lost, and restricted receptive fields, making it exponentially difficult for information to traverse large graphs. Consequently, deeper MPNNs often suffer from vanishing gradients and oversmoothing, where node representations become indistinguishable, severely hindering their expressivity for tasks requiring holistic graph understanding.

To overcome these challenges, a significant line of research has focused on integrating spectral information into GNN architectures, moving beyond purely spatial aggregation. Early efforts recognized the value of the graph Laplacian's eigenvectors as positional encodings (PEs) \cite{dwivedi20239ab, dwivedi2021af0}. By providing GNNs with a sense of global position and structure, these PEs enhance the models' ability to differentiate between structurally similar but topologically distinct nodes, thereby improving expressivity. However, these approaches typically treat spectral information as an auxiliary feature, informing spatial message passing rather than fundamentally altering the filtering mechanism.

More direct integration of spectral properties into the core convolution operation has been explored through spectral GNNs, which approximate filters in the graph spectral domain. Traditional spectral GNNs often employ polynomial filters (e.g., Chebyshev polynomials in ChebyNet or simplified forms in GCNs) to avoid computationally expensive eigendecomposition \cite{kipf2016semi, defferrard2016convolutional}. While efficient and localized in the node space, these polynomial filters have inherent limitations. Their frequency response is often constrained, making it difficult to model sharp changes or capture complex global patterns. Furthermore, achieving higher-order neighborhood information with these filters typically requires deeper stacks, which can exacerbate over-smoothing and sensitivity to noise \cite{bianchi20194ea}. Recent theoretical work by \cite{wang2022u2l} has delved into the expressive power of *linear* spectral GNNs, demonstrating that under certain mild conditions (e.g., no multiple eigenvalues of the graph Laplacian), these models can achieve universal approximation. This challenges the conventional assumption that non-linearity is strictly necessary for high expressiveness in spectral GNNs and establishes a novel connection between spectral universality and the 1-Weisfeiler-Leman (1-WL) test, offering insights into how spectral properties can constrain graph symmetry and differentiate non-isomorphic nodes.

A significant advancement in this direction is the introduction of Spatio-Spectral Graph Neural Networks (S2GNNs) \cite{geisler2024wli}. This novel architectural paradigm proposes operating directly in the spectral domain by employing spatio-spectral filters that synergistically combine local spatial message passing with global spectral filtering. S2GNNs are designed to achieve spatially unbounded information propagation by applying a learnable function of the graph Laplacian's eigenvalues to the node features in the spectral domain, effectively performing a global filtering operation. This global spectral filtering is then combined with a local spatial aggregation step, creating a powerful hybrid mechanism. By intrinsically integrating both local and global perspectives, S2GNNs directly address the over-squashing problem and the restricted receptive fields inherent in purely spatial GNNs. They offer superior approximation bounds and provide stable positional encodings "for free," significantly enhancing expressivity for tasks requiring global graph understanding and long-range dependencies. This approach offers a principled way to capture holistic graph properties and mitigate the vanishing gradient problem in deep GNNs, representing a distinct architectural solution that moves beyond K-hop aggregation or explicit subgraph/path encoding by fundamentally redesigning the information flow.

Beyond S2GNNs, other architectures also tackle long-range dependencies through different forms of global information integration. Graph Transformers \cite{wu20221la, chen2022mmu} represent a prominent alternative, leveraging self-attention mechanisms to compute pairwise relationships between all nodes, thereby achieving a global receptive field. Similar to S2GNNs, Graph Transformers often utilize spectral information, typically through Laplacian positional encodings, to inform their attention mechanisms. However, S2GNNs distinguish themselves by making global spectral filtering a core, learnable component of the propagation rule itself, rather than solely relying on attention informed by auxiliary spectral features.

Further innovations in spectral filtering include the use of Auto-Regressive Moving Average (ARMA) filters in GNNs \cite{bianchi20194ea}. Unlike polynomial filters, ARMA filters offer a more flexible frequency response, capable of modeling a wider variety of spectral characteristics and capturing longer-range dynamics with fewer parameters. While direct implementation of ARMA filters involves computationally intractable matrix inversions, \cite{bianchi20194ea} approximates their effect through recursive update rules, ensuring computational efficiency and stability. This recursive formulation, combined with skip connections, helps mitigate over-smoothing and allows for inductive inference on unseen graph topologies.

Another approach to achieving global understanding, albeit primarily in the spatial domain, is seen in models like GloGNN \cite{li2022315}. GloGNN aims to find "global homophily" even in heterophilous graphs by performing node neighborhood aggregation from the *entire set of nodes* in the graph. It learns a signed coefficient matrix that captures correlations between all nodes, effectively assigning positive weights to homophilous nodes and potentially negative weights to heterophilous ones. Crucially, GloGNN employs an acceleration technique that reduces the computational complexity of this global aggregation from quadratic/cubic to linear time, making it efficient for large graphs. While not strictly spatio-spectral, GloGNN's ability to leverage global node correlations for improved representations aligns with the broader goal of capturing long-range interactions.

In conclusion, the evolution of spatio-spectral architectures marks a pivotal shift towards more powerful GNNs capable of overcoming the long-standing challenges of over-squashing and limited receptive fields. By intrinsically combining local spatial and global spectral information, models like S2GNNs \cite{geisler2024wli}, alongside advancements in spectral filter design \cite{bianchi20194ea, wang2022u2l} and global attention mechanisms \cite{wu20221la}, offer principled paths to capturing holistic graph properties and enabling more robust representation learning. Future research will likely focus on further optimizing these hybrid architectures, exploring their theoretical limits, and integrating them with other advanced techniques to handle even larger and more complex graph structures requiring truly global understanding.