\subsection{Mitigating Oversmoothing and Modeling Complex Dynamics}

The persistent challenge of oversmoothing, where node representations in deep Graph Neural Networks (GNNs) become indistinguishable after multiple layers of message passing, fundamentally limits model depth and expressivity. This phenomenon, alongside the broader challenge of modeling complex, non-Markovian graph dynamics, has spurred extensive research into novel architectural designs, regularization techniques, and mathematical frameworks. While oversmoothing was initially considered the primary impediment to deep GNNs, recent analyses suggest a more nuanced understanding, identifying other factors like trainability and information bottlenecks as equally critical for effective long-range information propagation \cite{peng2024t2s, alon2020fok}.

A comprehensive understanding of oversmoothing begins with its formal definition. Rusch et al. \cite{rusch2023xev} propose an axiomatic definition, characterizing oversmoothing as the layer-wise exponential convergence of a node-similarity measure (e.g., graph Dirichlet energy) to zero. This rigorous framework allows for a more precise evaluation of mitigation strategies. However, the narrative around oversmoothing has evolved. Peng et al. \cite{peng2024t2s} argue that while oversmoothing is a contributing factor, the *trainability challenges* of deep Multi-Layer Perceptrons (MLPs) embedded within GNNs are often the more dominant reason for performance degradation in deep architectures. They contend that many methods ostensibly designed to combat oversmoothing primarily improve MLP trainability. Concurrently, Alon et al. \cite{alon2020fok} introduced the concept of "over-squashing," a distinct information bottleneck where the exponentially growing receptive field of a node is compressed into a fixed-size vector. This compression leads to significant information loss for long-range dependencies, particularly problematic for tasks requiring interactions between distant nodes, and challenges the sole focus on oversmoothing for all deep GNN failures. These critical perspectives highlight that effective deep GNN design requires addressing a multifaceted set of challenges beyond just feature convergence.

To counteract oversmoothing and enhance expressivity, various architectural modifications and regularization techniques have been developed. A common strategy involves **residual or skip connections**, exemplified by GCNII \cite{chen2020gcnii} and Jumping Knowledge Networks (JK-Nets) \cite{xu2018powerful}. These methods allow information from earlier layers to bypass later layers, preventing complete feature homogenization and enabling deeper models. However, while effective, they can increase model complexity or parameter count. **Normalization techniques**, such as PairNorm \cite{zhao2019pairnorm} and NodeNorm \cite{zhou2020nodenorm}, explicitly regularize node features to prevent them from collapsing into a narrow subspace. PairNorm, for instance, normalizes features based on their pairwise differences, directly counteracting the smoothing effect \cite{rusch2023xev}. **Regularization methods** like DropEdge \cite{rong2019dropedge}, which randomly drops edges during training, introduce noise that prevents over-reliance on local neighborhoods and can implicitly mitigate oversmoothing by altering the graph topology. Papp et al. \cite{papp20211ac} further explored the use of random node dropouts in DropGNN, not just for regularization, but to increase expressiveness by allowing the GNN to observe diverse neighborhood patterns across multiple runs. This approach can indirectly help maintain distinct features in deeper models by enhancing their ability to distinguish subtle structural differences.

Another architectural approach focuses on **decoupling the depth and scope** of GNNs. Zeng et al. \cite{zeng2022jhz} propose SHADOW-GNN, which first extracts a localized subgraph (bounded scope) and then applies a GNN of arbitrary depth *only* on this subgraph. This strategy performs "local-smoothing" rather than global oversmoothing, theoretically preventing feature collapse and enhancing expressivity, particularly for large graphs where traditional deep GNNs suffer from neighbor explosion and oversmoothing. This method offers a principled way to build deeper GNNs without incurring prohibitive computational costs or global oversmoothing, though its effectiveness relies heavily on the quality of the subgraph extraction function.

Beyond direct architectural fixes, researchers have explored alternative propagation schemes and fundamental mathematical dynamics to address oversmoothing and model complex graph behaviors. The **Predict then Propagate** framework, notably Personalized Propagation of Neural Predictions (PPNP) and its approximation APPNP \cite{klicpera20186xu}, decouples initial feature transformation from subsequent graph diffusion. By introducing a "teleport probability," these models allow for extensive message passing over large neighborhoods without representations collapsing, effectively mitigating oversmoothing and enabling deeper propagation without increasing the neural network's parameter count. Zhu et al. \cite{zhu2021zc3} provide a unified optimization framework that interprets the propagation mechanisms of various GNNs, including PPNP/APPNP, as optimal solutions to a common objective function comprising feature fitting and graph Laplacian regularization. This framework allows for the principled design of GNNs with flexible low-pass or high-pass filtering capabilities (e.g., GNN-LF, GNN-HF), which can explicitly alleviate oversmoothing by controlling frequency components.

Further advancing the modeling of complex graph dynamics, Liu et al. \cite{liu2021ee2} introduced **Elastic Graph Neural Networks (EGNNs)**. Unlike traditional GNNs that rely on $L_2$-based graph smoothing, which enforces uniform global smoothness, EGNNs incorporate $L_1$-based graph smoothing. This allows for adaptive local smoothness, enabling the preservation of discontinuities and sharp changes between different graph regions while still smoothing within homogeneous areas. The derived Elastic Message Passing (EMP) scheme, based on a primal-dual optimization algorithm, offers a novel way to balance global and local smoothing, leading to more robust and accurate representations, particularly against adversarial attacks.

A more fundamental redefinition of graph dynamics is presented by **Fractional-order Graph Neural Networks (FROND)** \cite{kang2024fsk}. Traditional continuous GNNs often model graph dynamics using integer-order differential equations, leading to rapid, exponential convergence of node representations and thus oversmoothing. FROND, however, leverages fractional calculus to replace integer-order derivatives with fractional derivatives. This mathematical shift enables the modeling of non-local, memory-dependent graph dynamics, which are prevalent in real-world processes exhibiting long-range dependencies and non-Markovian behavior. Crucially, the fractional-order dynamics inherently mitigate oversmoothing by promoting a slower, algebraic convergence of node representations, preventing them from becoming indistinguishable too quickly. This approach offers a novel pathway to enhance expressivity, robustness, and generalization for continuous GNNs by fundamentally altering the nature of information propagation.

In conclusion, mitigating oversmoothing and effectively modeling complex graph dynamics are multifaceted challenges in deep GNNs. The field has progressed from architectural heuristics like residual connections and normalization, which provide effective engineering solutions, to more sophisticated approaches. These include decoupling depth and scope \cite{zeng2022jhz}, developing adaptive smoothing mechanisms like Elastic GNNs \cite{liu2021ee2}, and unifying propagation under optimization frameworks \cite{zhu2021zc3}. The introduction of FROND \cite{kang2024fsk} represents a significant theoretical advancement, offering a fundamental re-framing of graph dynamics through fractional calculus. By enabling algebraic convergence and modeling non-local, memory-dependent interactions, FROND addresses oversmoothing at its mathematical core and opens new avenues for capturing complex real-world processes. Future research should continue to explore the interplay between oversmoothing, trainability, and over-squashing, integrating these diverse mitigation strategies and novel mathematical frameworks to build truly deep, expressive, and robust GNNs capable of handling the intricate dynamics of real-world graphs.