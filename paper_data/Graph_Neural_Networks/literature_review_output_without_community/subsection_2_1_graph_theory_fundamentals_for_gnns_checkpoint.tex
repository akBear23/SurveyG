\subsection{Graph Theory Fundamentals for GNNs}

Graph Neural Networks (GNNs) are powerful tools for analyzing graph-structured data, but their efficacy fundamentally relies on a deep understanding of the underlying graph theory concepts. This section delineates the essential graph theory fundamentals that form the bedrock for designing, comprehending, and analyzing GNN architectures, providing the necessary vocabulary and structural understanding to navigate the complexities of graph data.

A graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ is defined by a set of nodes (or vertices) $\mathcal{V}$ and a set of edges (or links) $\mathcal{E}$ connecting them. The way this structure is represented is crucial for computational processing by GNNs. The most common representation is the \textbf{Adjacency Matrix} $\mathbf{A} \in \{0,1\}^{N \times N}$, where $N=|\mathcal{V}|$. An entry $\mathbf{A}_{ij}=1$ indicates an edge between node $i$ and node $j$, and $0$ otherwise. For GNNs, this matrix is central to the message passing mechanism, where information is aggregated from neighboring nodes. For instance, models like those discussed in \cite{klicpera20186xu} for node classification, which decouple prediction from propagation, heavily rely on the adjacency matrix to define the diffusion process, often using a normalized version ($\tilde{\mathbf{A}}$). Nodes in real-world graphs often possess descriptive information, represented by a \textbf{Node Feature Matrix} $\mathbf{X} \in \mathbb{R}^{N \times D}$, where $D$ is the dimensionality of node features. GNNs combine this attribute information with the graph's topology to learn rich representations. The SEAL framework for link prediction \cite{zhang2018kdl}, for example, constructs a comprehensive node information matrix that integrates structural node labels, latent embeddings, and explicit node attributes, demonstrating the importance of rich feature representation alongside structure. Less common but equally valid for sparse graphs is the \textbf{Edge List}, a simple enumeration of node pairs $(u, v)$ for each edge. While not directly used for matrix operations in the same way as $\mathbf{A}$, it underpins the construction of $\mathbf{A}$ and is often the raw input format for graph data.

Graphs can be categorized based on their structural properties, each presenting unique challenges and opportunities for GNNs. \textbf{Undirected Graphs} feature symmetric relationships (if $i$ is connected to $j$, then $j$ is connected to $i$), meaning $\mathbf{A}_{ij} = \mathbf{A}_{ji}$. In contrast, \textbf{Directed Graphs} represent asymmetric relationships, where $\mathbf{A}_{ij}$ does not necessarily imply $\mathbf{A}_{ji}$. GNNs must adapt their message passing to respect the directionality of information flow in directed graphs. Edges can also carry quantitative information. \textbf{Weighted Graphs} assign a numerical weight $w_{ij}$ to each edge, indicating strength, distance, or capacity. The adjacency matrix then becomes $\mathbf{A}_{ij} = w_{ij}$. GNNs designed for such graphs must incorporate these weights into their aggregation functions. Furthermore, graphs can be \textbf{Homogeneous}, where all nodes and edges are of the same type, or \textbf{Heterogeneous}, featuring multiple types of nodes and/or edges. Heterogeneous graphs, common in knowledge graphs or social networks, require GNNs to distinguish and process different semantic relationships. The increasing complexity of real-world data often leads to \textbf{Attributed Graphs}, where both nodes and edges carry rich descriptive features, as exemplified by multi-modal GNNs that align graph and text data \cite{li202444f}, leveraging both structural and semantic attributes.

Understanding fundamental graph properties is paramount for GNN design. The \textbf{Neighborhood} of a node $v$, denoted $\mathcal{N}(v)$, comprises all nodes directly connected to $v$. This concept is central to the message passing paradigm of GNNs, where a node's representation is iteratively updated by aggregating information from its neighbors. The "local enclosing subgraphs" utilized by \cite{zhang2018kdl} are essentially defined by these neighborhoods. However, relying solely on immediate neighborhoods can limit expressiveness, leading to the need for models that capture \textbf{higher-order substructural information}, such as paths and cycles, as addressed by \cite{zeng20237gv} through "Cut subgraph" extraction to enhance GNN expressiveness beyond simple 1-Weisfeiler-Leman equivalence. \textbf{Connectivity} describes how well nodes are connected within a graph. A graph can be fully connected, disconnected, or contain isolated "stray nodes." Robust GNNs must handle varying degrees of connectivity, especially when dealing with "weak information" or sparse graphs, a challenge explicitly tackled by \cite{liu2023v3e} with its dual-channel architecture designed to manage simultaneously occurring data deficiencies and stray nodes. The nature of connections within neighborhoods also varies. \textbf{Homophily} describes the tendency for connected nodes to be similar, while \textbf{Heterophily} indicates dissimilarity. GNNs often struggle with graphs exhibiting "structural disparity," where both homophilic and heterophilic patterns coexist \cite{mao202313j}. This limitation of "one-size-fits-all" filtering in GNNs has led to innovations like "node-wise filtering" using a Mixture of Experts approach \cite{han2024rkj}, which adaptively applies different aggregation strategies based on local neighborhood characteristics. Finally, the dynamic nature of many real-world systems necessitates understanding \textbf{Temporal Graphs}, where nodes and edges evolve over time. This introduces challenges in modeling time-varying adjacency and features, requiring specialized Temporal GNNs (TGNNs) as surveyed by \cite{longa202399q}, which formalizes learning settings and tasks for such dynamic structures.

In summary, a comprehensive grasp of graph theory fundamentals—from diverse representations and graph types to intrinsic properties like neighborhoods, connectivity, and structural patterns—is indispensable for developing effective GNNs. While GNN architectures have advanced significantly, as evidenced by efforts to enhance expressiveness \cite{zeng20237gv}, improve robustness to weak information \cite{liu2023v3e}, and adapt to structural disparities \cite{mao202313j, han2024rkj}, the unique challenges posed by the inherent topology of graph-structured data remain a central focus. Future research will continue to refine how GNNs leverage these foundational concepts to process increasingly complex, dynamic, and imperfect graph data, pushing the boundaries of what graph-based machine learning can achieve.