\subsection*{The Message Passing Neural Network (MPNN) Framework}

The Message Passing Neural Network (MPNN) framework represents a cornerstone in the evolution of Graph Neural Networks (GNNs), providing a unifying and highly influential paradigm for learning on graph-structured data \cite{Gilmer2017}. While earlier works, such as the foundational Graph Neural Network by Scarselli et al. \cite{Scarselli2009}, introduced the concept of iterative information propagation, the MPNN framework, formally introduced by Gilmer et al. in 2017 \cite{Gilmer2017}, brought together a diverse array of GNN models under a common computational abstraction \cite{jegelka20222lq}. This formalization clarified the iterative process by which nodes aggregate information from their local neighborhoods, transform their features, and update their representations. This spatial, neighborhood-centric view offered a flexible and often more scalable alternative to concurrent spectral GNNs, which typically relied on graph Laplacian eigen-decompositions \cite{wang2022u2l}, thereby forming the core mechanism for many modern GNN architectures, including prominent examples like Graph Convolutional Networks (GCNs) and GraphSAGE, which are explored in detail in Section 2.3.

At its essence, the MPNN framework describes a multi-layer process where, for each layer $k$, a node $v$ updates its hidden state $h_v^{k+1}$ based on its previous state $h_v^k$ and messages received from its neighbors. This process is typically decomposed into two main phases:
\begin{enumerate}
    \item \textbf{Message Computation:} For each neighbor $u \in \mathcal{N}(v)$, a message $m_{uv}^{k+1}$ is computed. This message often depends on the hidden states of both the sending node $u$ and the receiving node $v$, and potentially on the features of the edge $e_{uv}$ connecting them: $m_{uv}^{k+1} = M_k(h_u^k, h_v^k, e_{uv})$. The function $M_k$ is typically a neural network (e.g., a Multi-Layer Perceptron, MLP) that learns to extract relevant information from the neighbor's state and edge features.
    \item \textbf{Aggregation and Update Function:} The messages from all neighbors are first aggregated into a single neighborhood representation, and then combined with the node's current state to produce the new hidden state. This involves an aggregation function $\bigoplus$ and an update function $U_k$: $h_v^{k+1} = U_k(h_v^k, \bigoplus_{u \in \mathcal{N}(v)} m_{uv}^{k+1})$. The aggregation function is critical for handling variable-sized neighborhoods and must be permutation-invariant, meaning the order in which messages are aggregated does not affect the final result. Common choices include sum, mean, or max operations \cite{Gilmer2017}. Critically, the choice of aggregation function significantly impacts the model's expressive power. While mean and max aggregators are simple and robust to varying node degrees, they are not injective, meaning distinct multisets of neighbor features can be mapped to the same aggregated representation, leading to information loss and limiting discriminative power. In contrast, the sum aggregator is injective for distinct multisets of features, provided the feature space is sufficiently rich, making it theoretically more powerful for distinguishing graph structures \cite{Xu2018, jegelka20222lq}. The update function $U_k$ typically involves another neural network (e.g., an MLP or a Gated Recurrent Unit, GRU) that integrates the aggregated information with the node's own features.
\end{enumerate}
This iterative message passing allows GNNs to effectively propagate and integrate local information across the graph, enabling nodes to learn context-aware representations that capture their structural role and feature interactions within the graph.

Despite its versatility, the MPNN paradigm faces inherent theoretical limits and practical challenges. A significant line of research, initiated by Xu et al. \cite{Xu2018} and further elaborated by Morris et al. \cite{morris20185sd}, established a crucial connection between the discriminative power of standard MPNNs and the 1-Weisfeiler-Lehman (1-WL) test of graph isomorphism. This theoretical framework demonstrates that MPNNs are at most as powerful as the 1-WL test, meaning they cannot distinguish between certain non-isomorphic graphs that the 1-WL test also fails to differentiate. This limitation stems from their reliance on local neighborhood aggregation, which struggles to capture global or higher-order structural patterns such as girth, circumference, or k-clique counts \cite{garg2020z6o, jegelka20222lq}. A detailed exploration of the Weisfeiler-Lehman test and its implications for GNN expressivity, including architectures designed to overcome these bounds, is provided in Section 3.1 and Section 3.2.

Another prominent issue in deep MPNNs is the problem of over-smoothing, where repeated message passing can cause node representations to become increasingly similar, eventually making them indistinguishable regardless of their initial features or structural positions \cite{Li2018}. This phenomenon, often observed in GCNs due to their averaging aggregation, limits the effective depth of MPNNs and hinders their ability to capture fine-grained structural information. Recent analyses have shown that over-smoothing is not exclusive to simple GCNs but also affects more complex architectures, including attention-based GNNs and graph transformers, where it can lead to an exponential loss of expressive power \cite{wu2023aqs}. Furthermore, some research suggests that while over-smoothing is a significant factor, the primary challenge in training deep GNNs might lie more fundamentally in the trainability issues of the underlying MLPs, with methods designed to mitigate over-smoothing often implicitly improving MLP trainability \cite{peng2024t2s}. This nuanced understanding highlights the need for more robust designs to allow for deeper GNNs, which is further discussed in Section 4.4.

Beyond these theoretical and architectural limitations, practical challenges include scaling MPNNs to extremely large graphs (Section 4.1) and their inherent inability to model complex, memory-dependent graph dynamics with a fixed number of message-passing iterations \cite{pflueger2024qi6}.

In conclusion, the MPNN framework provides a powerful and widely adopted blueprint for designing GNNs, enabling effective local information propagation and representation learning on graphs through its well-defined message computation, aggregation, and update functions. However, its inherent limitations in expressive power (bounded by the 1-WL test), the challenge of over-smoothing in deep models, and the computational demands for large-scale graphs continue to drive active research. Addressing these challenges involves exploring architectures that surpass the 1-WL limit (Section 3.1, 3.2), developing strategies to mitigate over-smoothing (Section 4.4), and innovating scalable solutions for web-scale applications (Section 4.1), thereby pushing the boundaries of GNN capabilities and applicability.