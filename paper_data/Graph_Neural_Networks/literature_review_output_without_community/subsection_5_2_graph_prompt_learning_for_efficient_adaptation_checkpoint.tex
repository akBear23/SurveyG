\subsection{Graph Prompt Learning for Efficient Adaptation}

The remarkable capabilities of Graph Neural Networks (GNNs) in learning from graph-structured data are often hampered by the practical challenge of efficiently adapting pre-trained models to new, often data-scarce, downstream tasks. Traditional transfer learning, typically involving full fine-tuning of the entire GNN, is computationally demanding, requires substantial labeled data for each new task, and frequently suffers from a significant "objective gap" between the pre-training pretext task and the specific downstream objective \cite{hu2019r47}. This inefficiency and task-specificity have spurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods for GNNs, with graph prompt learning emerging as a particularly potent paradigm, drawing inspiration from its transformative success in Natural Language Processing (NLP). While other PEFT methods like adapter modules (e.g., inserting small, learnable neural networks between GNN layers) also exist for graphs, prompt learning distinguishes itself by modifying the input or output interface rather than the internal architecture.

Early foundational work by \cite{hu2019r47} systematically investigated the "pre-train, fine-tune" paradigm for GNNs, proposing self-supervised strategies at both node and graph levels (e.g., Context Prediction, Attribute Masking) to learn robust representations from unlabeled data. While these methods significantly improved generalization and accelerated convergence during fine-tuning, they still necessitated updating all or most model parameters. This approach remained inefficient and susceptible to objective misalignment, especially when the downstream task's nature diverged significantly from the pre-training objective, motivating the search for more efficient adaptation strategies.

The concept of "prompt tuning" was explicitly introduced to GNNs by \cite{sun2022d18} with their Graph Pre-training and Prompt Tuning (GPPT) framework. Recognizing the "inherent training objective gap" as a core bottleneck, GPPT proposed to reformulate downstream node classification tasks to mimic the pre-training objective, such as masked edge prediction. This was achieved by employing a novel "graph prompting function" and "token pairs" that effectively transformed the downstream task's input to align with the pre-trained model's expected input format and objective. By doing so, GPPT facilitated knowledge transfer without extensive fine-tuning, accelerating convergence and improving performance in few-shot settings. However, a critical limitation of GPPT was its task-specific nature; its prompting mechanism was often tailored to particular pre-training tasks (e.g., edge prediction) and lacked universality across diverse GNN pre-training strategies, restricting its broad applicability.

To address this universality challenge, \cite{fang2022tjj} introduced a more general and theoretically grounded approach with Graph Prompt Feature (GPF). Instead of structural modification or task reformulation, GPF shifted the prompt application to direct manipulation of the *input graph's feature space*. This innovation involved learning a small set of universal prompt vectors that are either concatenated or, more commonly, *added* to the original node features ($\mathbf{x}_i' = \mathbf{x}_i + \mathbf{p}$), effectively creating learnable "virtual features" to steer the pre-trained GNN towards the downstream task without altering its core architecture. GPF was the first universal prompt-based tuning method for GNNs, demonstrating compatibility with virtually any pre-trained GNN and pre-training strategy. Crucially, it was accompanied by rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to *any* prompting function, often enabling it to surpass the performance of full fine-tuning, particularly in low-data regimes. Despite its universality and theoretical backing, the optimal design and dimensionality of these feature-space prompts remain largely heuristic, and their lack of structural grounding can make them difficult to interpret.

Building upon the success of universal prompt tuning, \cite{liu2023ent} aimed for broader universality across *task types*, not just pre-training strategies. Their GraphPrompt framework introduced a novel prompting mechanism that modifies the GNN's `ReadOut` aggregation function, which is typically used to generate graph-level representations from node embeddings. By unifying various pre-training tasks (e.g., link prediction) and diverse downstream tasks (e.g., node and graph classification) into a common "subgraph similarity" template, GraphPrompt utilizes task-specific learnable prompts to guide the `ReadOut` operation. This prompt acts as learnable parameters for the aggregation function, allowing adaptive fusion of node representations. This approach allows a single pre-trained model to effectively serve multiple downstream tasks in few-shot settings, further bridging objective inconsistencies by adapting the final aggregation step to the task at hand. However, its effectiveness might be constrained by how well diverse tasks can be framed as subgraph similarity, and modifying only the ReadOut function might not be sufficient for highly complex or structurally sensitive downstream tasks.

Further advancing the paradigm, \cite{sun2023vsl} proposed "All in One: Multi-Task Prompting for Graph Neural Networks," which explicitly unifies the format of graph prompts and language prompts using concepts like prompt tokens and insertion patterns. Their method reformulates various downstream problems to graph-level tasks and employs meta-learning to efficiently learn a better initialization for multi-task prompts. This approach aims to make prompting more reliable and general for different tasks, but the computational overhead of meta-learning and the potential oversimplification of node/edge-level tasks when reframing them as graph-level problems are important considerations.

Marking a significant expansion of the paradigm's scope beyond simple parameter-efficient adaptation, recent work has begun integrating prompt learning with Large Language Models (LLMs) to imbue GNNs with real-world semantic understanding. \cite{li202444f} introduced Morpher, a pioneering paradigm for graph-text multi-modal prompt learning, even under *extremely weak text supervision*. Morpher addresses the complex challenge of aligning independently pre-trained GNNs and LLMs by proposing an improved, stable graph prompt design and a cross-modal projector with contrastive learning. This innovative framework enables GNNs to achieve CLIP-style zero-shot generalization to unseen classes, significantly enhancing their semantic capabilities and opening new avenues for applications requiring rich contextual understanding by leveraging the vast semantic knowledge encoded in LLMs. This development represents a critical step towards more intelligent and semantically aware graph AI systems, a topic explored further in the following subsection.

In conclusion, graph prompt learning has rapidly evolved from a foundational concept for efficient adaptation to a sophisticated paradigm capable of universal application across pre-training strategies and diverse downstream tasks, culminating in multi-modal semantic understanding. This progression, from task-specific structural prompts (GPPT) to universal feature-space prompts (GPF) and unified ReadOut-based frameworks (GraphPrompt), and finally to multi-modal integration (Morpher), has significantly enhanced GNNs' practical utility. By effectively bridging the objective gap, enabling robust few-shot learning, and reducing the need for extensive labeled data and full fine-tuning, graph prompt learning makes GNNs more adaptable, flexible, and scalable in diverse and data-constrained application contexts. However, challenges remain in developing more robust and interpretable prompt designs, particularly for complex structural tasks, and in addressing the scalability and computational overhead of advanced prompt learning techniques for extremely large and dynamic graphs.