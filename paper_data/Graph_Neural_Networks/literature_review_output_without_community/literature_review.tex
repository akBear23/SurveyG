\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 328 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction_to_graph_neural_networks}

\section{Introduction to Graph Neural Networks}
\label{sec:introduction\_to\_graph\_neural\_networks}

\subsection{The Rise of Graph-Structured Data}
\label{sec:1\_1\_the\_rise\_of\_graph-structured\_data}

The digital age has ushered in an unprecedented era where data is increasingly generated and organized in complex, interconnected structures, fundamentally shifting from traditional tabular formats to intricate graphs. This paradigm is evident across a myriad of domains, from the vast expanse of social networks and the intricate pathways of biological systems to the semantic richness of knowledge graphs, the personalized recommendations of e-commerce platforms, and the dynamic flows within transportation networks \cite{wu2022ptq, wang2023zr0}. Such graph-structured data inherently captures complex, non-i.i.d. relational dependencies and rich structural information that are crucial for understanding the underlying phenomena. The rapid growth of research in Graph Neural Networks (GNNs) itself underscores the critical importance and ubiquity of graph data in modern applications, highlighting a significant paradigm shift in data representation and analysis \cite{wu2022ptq}.

However, traditional machine learning models, primarily designed for Euclidean data where instances are assumed to be independent and identically distributed (i.i.d.), inherently struggle to effectively process and leverage this topological and relational information. Their limitations stem from a fundamental inability to capture the non-linear interactions between entities, the influence propagation across connections, and the rich structural context embedded within these interconnected datasets. Traditional models, by flattening graph structures into feature vectors or treating relationships as independent features, fundamentally discard this crucial relational context. This loss of topological information means they cannot discern how a node's properties are influenced by its neighbors, how information flows through the network, or how global graph patterns emerge from local interactions. Such an approach leads to suboptimal performance and a superficial understanding of the underlying phenomena when confronted with the intricate dependencies inherent in graph data \cite{wang2023zr0}.

The necessity for models that can explicitly leverage graph topology is exemplified in diverse real-world scenarios where interconnectedness is paramount. In social networks, understanding community structures, predicting user behavior, or detecting anomalies requires analyzing the intricate web of connections rather than isolated user profiles. Similarly, web-scale recommender systems, dealing with billions of users and items, fundamentally rely on modeling complex user-item interaction graphs to provide personalized suggestions. Traditional matrix factorization or content-based methods often fall short in capturing the nuanced, multi-hop relationships and the propagation of preferences across the graph, which are crucial for accurate recommendations \cite{ying20189jc, fan2019k6u}. The sheer scale and dynamic nature of these graphs further exacerbate the limitations of conventional approaches, demanding models that can efficiently learn from and adapt to evolving relational data.

Beyond social and recommendation graphs, the prevalence of graph-structured data extends to critical domains like scientific discovery and infrastructure management. In biological systems, protein-protein interaction networks, gene regulatory networks, and molecular graphs are inherently relational, where the function of a component is determined by its interactions within the larger system. Analyzing these structures is vital for drug discovery, disease understanding, and materials science \cite{wu2022ptq}. Furthermore, in smart cities and environmental monitoring, multivariate time series data from sensors often exhibit complex spatial dependencies (e.g., traffic flow between intersections, air quality measurements across a region). Treating these time series independently ignores critical spatial correlations, making traditional models less effective for tasks like forecasting or anomaly detection. The explicit modeling of these non-Euclidean spatial relationships through graph structures has proven crucial for accurate analysis in such domains \cite{wu2020hi3, jin2023ijy}.

In essence, the digital transformation has rendered graph-structured data a ubiquitous and indispensable representation for complex systems across nearly every scientific and industrial domain. The inherent limitations of traditional machine learning, which are ill-equipped to handle the non-i.i.d. and relational nature of such data, have created a compelling need for a new generation of learning paradigms. This fundamental shift underscores the critical importance of developing specialized graph-aware models that can effectively leverage the rich topological and relational information embedded within these interconnected datasets. Graph Neural Networks have emerged as a powerful response to this challenge, promising to unlock deeper insights and drive innovation across fields by directly learning from the structural and relational complexities of the modern data landscape. This review will delve into the foundational concepts, architectural advancements, and diverse applications of these transformative models.
\subsection{Overview of Graph Neural Networks}
\label{sec:1\_2\_overview\_of\_graph\_neural\_networks}

Graph Neural Networks (GNNs) represent a transformative and rapidly evolving class of deep learning models specifically engineered to operate directly on graph-structured data, thereby addressing the inherent limitations of traditional neural networks when confronted with non-Euclidean domains \cite{wu2022ptq, khemani2024i8r}. Unlike grid-like or sequential data, graphs possess irregular structures and complex relational dependencies, making conventional convolutional or recurrent architectures ill-suited for effectively capturing their rich topological and feature information \cite{khemani2024i8r}. The fundamental purpose of GNNs is to learn meaningful, context-aware representations for individual nodes and entire graphs by synergistically leveraging both node features and the intricate relational information encoded within the graph's topology.

At their core, GNNs universally employ an iterative message-passing paradigm, a mechanism often formalized within the Message Passing Neural Network (MPNN) framework \cite{gilmer2017neural}. This process typically unfolds over multiple layers, where each node iteratively refines its representation by exchanging and integrating information with its local neighborhood. Specifically, in each layer, a node first computes "messages" based on its current representation and the representations of its neighbors. These messages are then "aggregated" from all incoming neighbors, typically using permutation-invariant functions such as summation, mean, or max pooling, to generate a compact summary of the local neighborhood. Finally, the node's representation is "updated" by combining its previous state with this aggregated neighborhood information, often through a learnable neural network. This iterative propagation and aggregation mechanism enables GNNs to effectively gather and integrate information from increasingly distant parts of their local receptive fields, allowing them to learn rich, context-aware representations that encapsulate both local structural patterns and feature interactions \cite{wu2022ptq, khemani2024i8r}.

The initial promise of this paradigm was quickly demonstrated by pioneering architectures that laid the conceptual groundwork for the field. Broadly, early GNNs could be categorized into two main families: spectral and spatial methods. Spectral GNNs, such as the seminal Graph Convolutional Networks (GCNs) \cite{kipf2017semi}, adapt convolutional operations from the spectral domain of graph signal processing, leveraging the graph Laplacian to define filters that operate on graph signals. These models implicitly capture global graph properties through spectral decomposition, though their theoretical underpinnings and expressive power are still subjects of ongoing research \cite{wang2022u2l}. In parallel, spatial GNNs, exemplified by GraphSAGE \cite{hamilton2017inductive}, directly define convolutions by aggregating features from a node's immediate neighbors in the spatial domain. GraphSAGE, in particular, introduced inductive capabilities by learning a function that aggregates information from a sampled set of neighbors, enabling generalization to unseen nodes and graphs. These early models showcased remarkable efficacy across canonical graph learning tasks, including node classification, link prediction, and graph classification, thereby catalyzing broader research into deep learning on non-Euclidean data \cite{khemani2024i8r}.

Despite these significant initial breakthroughs, foundational GNN architectures inherently faced several critical challenges that limited their immediate applicability and expressive power. Issues such as the limited ability to distinguish between certain non-isomorphic graphs (a challenge related to the Weisfeiler-Leman test, explored in Section 3.1) highlighted limitations in their structural expressivity \cite{wang2022u2l}. Furthermore, scalability emerged as a major hurdle, as many early models struggled to efficiently process real-world graphs comprising millions or billions of nodes and edges due to computational and memory constraints \cite{khemani2024i8r}. Problems like oversmoothing, where node representations become indistinguishable after many layers, and difficulties in handling imperfect or noisy data also underscored the need for more robust and adaptable models. Moreover, the theoretical understanding of GNN generalization capabilities, particularly how they perform on unseen data sampled from underlying graph manifolds, remained nascent \cite{wang2024cb8}. These inherent limitations of early GNNs, while not diminishing their foundational importance, collectively motivated a rich and diverse research agenda, driving the development of advanced architectures and methodologies explored in subsequent sections of this review, aimed at enhancing expressivity, improving scalability, and ensuring robustness and trustworthiness for real-world deployment.
\subsection{Scope and Organization of the Review}
\label{sec:1\_3\_scope\_\_and\_\_organization\_of\_the\_review}

This comprehensive literature review aims to provide a structured and accessible understanding of Graph Neural Networks (GNNs), a field that has witnessed rapid and extensive growth, making it challenging to gain a global perspective \cite{wu2022ptq, khemani2024i8r}. Our objective is to trace the intellectual trajectories of GNN research from its foundational principles to its most recent innovations and real-world impact. The review is meticulously organized both thematically and chronologically, guiding the reader through the complex landscape of GNN development, highlighting key advancements, persistent limitations, and promising future directions.

The thematic scope of this review is broad, encompassing the critical dimensions that define the GNN research landscape. We begin by establishing the \textbf{foundational theories} and early architectural paradigms that laid the groundwork for GNNs. This naturally leads to an exploration of methods designed to enhance their \textbf{expressive power}, addressing theoretical limitations such as those posed by the Weisfeiler-Leman (WL) test and practical issues like over-squashing. Subsequently, the review delves into crucial aspects of GNN \textbf{maturation and robustness}, including strategies for scaling to large graphs, handling structural heterogeneity, and learning effectively from imperfect data. A significant portion is dedicated to \textbf{advanced learning paradigms}, such as pre-training, prompt learning, and multi-modal integration with large language models, which are pivotal for improving generalization and semantic understanding. Finally, we examine the \textbf{diverse real-world applications} of GNNs and address the critical need for \textbf{rigorous evaluation, trustworthiness, and future research directions} to ensure responsible and impactful deployment.

The pedagogical progression adopted in this review is designed to offer a coherent narrative of GNN development, mirroring the field's evolution from theoretical underpinnings to practical deployment and cutting-edge innovation.

\begin{itemize}
    \item \textbf{Section 1: Introduction to Graph Neural Networks} sets the stage by discussing the rise of graph-structured data and providing a high-level overview of GNNs, establishing their significance and the scope of this review.
    \item \textbf{Section 2: Foundational Concepts and Early Paradigms} lays the essential groundwork. It introduces fundamental graph theory concepts pertinent to GNNs and details the Message Passing Neural Network (MPNN) framework, which unifies many GNN architectures. This section also examines early, influential GNN models like GCNs and GraphSAGE, demonstrating their initial promise \cite{khemani2024i8r}.
    \item \textbf{Section 3: Enhancing Expressive Power and Theoretical Limits} delves into the fundamental limitations of GNNs, particularly concerning their ability to distinguish graph structures. It explores the Weisfeiler-Leman (WL) test as a theoretical benchmark and reviews advanced techniques, including higher-order, substructure-aware, geometric, equivariant, and spatio-spectral GNNs, developed to overcome these expressivity bottlenecks \cite{wu2022ptq}.
    \item \textbf{Section 4: Scaling, Robustness, and Adaptability} addresses the critical challenges of deploying GNNs in real-world scenarios. This section covers techniques for scaling GNNs to web-scale graphs, methods for handling structural heterogeneity and imperfect data, and novel mathematical frameworks for mitigating oversmoothing and modeling complex graph dynamics.
    \item \textbf{Section 5: Advanced Learning Paradigms: Pre-training, Prompting, and Multi-modality} explores cutting-edge approaches that enhance GNNs' generalization and data efficiency. It covers foundational strategies for GNN pre-training, the emerging field of graph prompt learning for efficient adaptation, and the groundbreaking integration of GNNs with Large Language Models (LLMs) through multi-modal prompt learning.
    \item \textbf{Section 6: Applications and Real-World Impact of GNNs} showcases the diverse and significant real-world impact of GNNs across various domains. It highlights their successful application in recommender systems, multivariate time series analysis \cite{sahili2023f2x}, scientific discovery (e.g., materials science, epidemic modeling), and fundamental graph tasks like link prediction.
    \item \textbf{Section 7: Evaluation, Trustworthiness, and Future Directions} concludes the review by addressing critical aspects beyond model development. It emphasizes the importance of rigorous benchmarking \cite{liu2022a5y} and delves into the crucial dimensions of trustworthiness—privacy, robustness, fairness, and explainability—essential for responsible GNN deployment. Finally, it identifies key open challenges and promising future research directions that will shape the next generation of GNNs \cite{wu2022ptq, khemani2024i8r}.
\end{itemize}

By adopting this structured approach, this review aims to provide a clear roadmap for readers, from novices seeking foundational knowledge to experienced researchers looking for a comprehensive overview of the latest advancements and future frontiers. This organization facilitates a deeper understanding of the field's evolution, the interplay between theoretical breakthroughs and practical necessities, and the intellectual trajectories that continue to drive innovation in graph-aware artificial intelligence.


\label{sec:foundational_concepts_and_early_paradigms}

\section{Foundational Concepts and Early Paradigms}
\label{sec:foundational\_concepts\_\_and\_\_early\_paradigms}

\subsection{Graph Theory Fundamentals for GNNs}
\label{sec:2\_1\_graph\_theory\_fundamentals\_for\_gnns}

Graph Neural Networks (GNNs) are powerful tools for analyzing graph-structured data, but their efficacy fundamentally relies on a deep understanding of the underlying graph theory concepts. This section delineates the essential graph theory fundamentals that form the bedrock for designing, comprehending, and analyzing GNN architectures, providing the necessary vocabulary and structural understanding to navigate the complexities of graph data.

A graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ is defined by a set of nodes (or vertices) $\mathcal{V}$ and a set of edges (or links) $\mathcal{E}$ connecting them. The way this structure is represented is crucial for computational processing by GNNs. The most common representation is the \textbf{Adjacency Matrix} $\mathbf{A} \in \{0,1\}^{N \times N}$, where $N=|\mathcal{V}|$. An entry $\mathbf{A}\_{ij}=1$ indicates an edge between node $i$ and node $j$, and $0$ otherwise. For GNNs, this matrix is central to the message passing mechanism, where information is aggregated from neighboring nodes. For instance, models like those discussed in \cite{klicpera20186xu} for node classification, which decouple prediction from propagation, heavily rely on the adjacency matrix to define the diffusion process, often using a normalized version ($\tilde{\mathbf{A}}$). Nodes in real-world graphs often possess descriptive information, represented by a \textbf{Node Feature Matrix} $\mathbf{X} \in \mathbb{R}^{N \times D}$, where $D$ is the dimensionality of node features. GNNs combine this attribute information with the graph's topology to learn rich representations. The SEAL framework for link prediction \cite{zhang2018kdl}, for example, constructs a comprehensive node information matrix that integrates structural node labels, latent embeddings, and explicit node attributes, demonstrating the importance of rich feature representation alongside structure. Less common but equally valid for sparse graphs is the \textbf{Edge List}, a simple enumeration of node pairs $(u, v)$ for each edge. While not directly used for matrix operations in the same way as $\mathbf{A}$, it underpins the construction of $\mathbf{A}$ and is often the raw input format for graph data.

Graphs can be categorized based on their structural properties, each presenting unique challenges and opportunities for GNNs. \textbf{Undirected Graphs} feature symmetric relationships (if $i$ is connected to $j$, then $j$ is connected to $i$), meaning $\mathbf{A}\_{ij} = \mathbf{A}\_{ji}$. In contrast, \textbf{Directed Graphs} represent asymmetric relationships, where $\mathbf{A}\_{ij}$ does not necessarily imply $\mathbf{A}\_{ji}$. GNNs must adapt their message passing to respect the directionality of information flow in directed graphs. Edges can also carry quantitative information. \textbf{Weighted Graphs} assign a numerical weight $w\_{ij}$ to each edge, indicating strength, distance, or capacity. The adjacency matrix then becomes $\mathbf{A}\_{ij} = w\_{ij}$. GNNs designed for such graphs must incorporate these weights into their aggregation functions. Furthermore, graphs can be \textbf{Homogeneous}, where all nodes and edges are of the same type, or \textbf{Heterogeneous}, featuring multiple types of nodes and/or edges. Heterogeneous graphs, common in knowledge graphs or social networks, require GNNs to distinguish and process different semantic relationships. The increasing complexity of real-world data often leads to \textbf{Attributed Graphs}, where both nodes and edges carry rich descriptive features, as exemplified by multi-modal GNNs that align graph and text data \cite{li202444f}, leveraging both structural and semantic attributes.

Understanding fundamental graph properties is paramount for GNN design. The \textbf{Neighborhood} of a node $v$, denoted $\mathcal{N}(v)$, comprises all nodes directly connected to $v$. This concept is central to the message passing paradigm of GNNs, where a node's representation is iteratively updated by aggregating information from its neighbors. The "local enclosing subgraphs" utilized by \cite{zhang2018kdl} are essentially defined by these neighborhoods. However, relying solely on immediate neighborhoods can limit expressiveness, leading to the need for models that capture \textbf{higher-order substructural information}, such as paths and cycles, as addressed by \cite{zeng20237gv} through "Cut subgraph" extraction to enhance GNN expressiveness beyond simple 1-Weisfeiler-Leman equivalence. \textbf{Connectivity} describes how well nodes are connected within a graph. A graph can be fully connected, disconnected, or contain isolated "stray nodes." Robust GNNs must handle varying degrees of connectivity, especially when dealing with "weak information" or sparse graphs, a challenge explicitly tackled by \cite{liu2023v3e} with its dual-channel architecture designed to manage simultaneously occurring data deficiencies and stray nodes. The nature of connections within neighborhoods also varies. \textbf{Homophily} describes the tendency for connected nodes to be similar, while \textbf{Heterophily} indicates dissimilarity. GNNs often struggle with graphs exhibiting "structural disparity," where both homophilic and heterophilic patterns coexist \cite{mao202313j}. This limitation of "one-size-fits-all" filtering in GNNs has led to innovations like "node-wise filtering" using a Mixture of Experts approach \cite{han2024rkj}, which adaptively applies different aggregation strategies based on local neighborhood characteristics. Finally, the dynamic nature of many real-world systems necessitates understanding \textbf{Temporal Graphs}, where nodes and edges evolve over time. This introduces challenges in modeling time-varying adjacency and features, requiring specialized Temporal GNNs (TGNNs) as surveyed by \cite{longa202399q}, which formalizes learning settings and tasks for such dynamic structures.

In summary, a comprehensive grasp of graph theory fundamentals—from diverse representations and graph types to intrinsic properties like neighborhoods, connectivity, and structural patterns—is indispensable for developing effective GNNs. While GNN architectures have advanced significantly, as evidenced by efforts to enhance expressiveness \cite{zeng20237gv}, improve robustness to weak information \cite{liu2023v3e}, and adapt to structural disparities \cite{mao202313j, han2024rkj}, the unique challenges posed by the inherent topology of graph-structured data remain a central focus. Future research will continue to refine how GNNs leverage these foundational concepts to process increasingly complex, dynamic, and imperfect graph data, pushing the boundaries of what graph-based machine learning can achieve.
\subsection{The Message Passing Neural Network (MPNN) Framework}
\label{sec:2\_2\_the\_message\_passing\_neural\_network\_(mpnn)\_framework}

The Message Passing Neural Network (MPNN) framework represents a cornerstone in the evolution of Graph Neural Networks (GNNs), providing a unifying and highly influential paradigm for learning on graph-structured data \cite{Gilmer2017}. While earlier works, such as the foundational Graph Neural Network by Scarselli et al. \cite{Scarselli2009}, introduced the concept of iterative information propagation, the MPNN framework, formally introduced by Gilmer et al. in 2017 \cite{Gilmer2017}, brought together a diverse array of GNN models under a common computational abstraction \cite{jegelka20222lq}. This formalization clarified the iterative process by which nodes aggregate information from their local neighborhoods, transform their features, and update their representations. This spatial, neighborhood-centric view offered a flexible and often more scalable alternative to concurrent spectral GNNs, which typically relied on graph Laplacian eigen-decompositions \cite{wang2022u2l}, thereby forming the core mechanism for many modern GNN architectures, including prominent examples like Graph Convolutional Networks (GCNs) and GraphSAGE, which are explored in detail in Section 2.3.

At its essence, the MPNN framework describes a multi-layer process where, for each layer $k$, a node $v$ updates its hidden state $h\_v^{k+1}$ based on its previous state $h\_v^k$ and messages received from its neighbors. This process is typically decomposed into two main phases:
\begin{enumerate}
    \item \textbf{Message Computation:} For each neighbor $u \in \mathcal{N}(v)$, a message $m\_{uv}^{k+1}$ is computed. This message often depends on the hidden states of both the sending node $u$ and the receiving node $v$, and potentially on the features of the edge $e\_{uv}$ connecting them: $m\_{uv}^{k+1} = M\_k(h\_u^k, h\_v^k, e\_{uv})$. The function $M\_k$ is typically a neural network (e.g., a Multi-Layer Perceptron, MLP) that learns to extract relevant information from the neighbor's state and edge features.
    \item \textbf{Aggregation and Update Function:} The messages from all neighbors are first aggregated into a single neighborhood representation, and then combined with the node's current state to produce the new hidden state. This involves an aggregation function $\bigoplus$ and an update function $U\_k$: $h\_v^{k+1} = U\_k(h\_v^k, \bigoplus\_{u \in \mathcal{N}(v)} m\_{uv}^{k+1})$. The aggregation function is critical for handling variable-sized neighborhoods and must be permutation-invariant, meaning the order in which messages are aggregated does not affect the final result. Common choices include sum, mean, or max operations \cite{Gilmer2017}. Critically, the choice of aggregation function significantly impacts the model's expressive power. While mean and max aggregators are simple and robust to varying node degrees, they are not injective, meaning distinct multisets of neighbor features can be mapped to the same aggregated representation, leading to information loss and limiting discriminative power. In contrast, the sum aggregator is injective for distinct multisets of features, provided the feature space is sufficiently rich, making it theoretically more powerful for distinguishing graph structures \cite{Xu2018, jegelka20222lq}. The update function $U\_k$ typically involves another neural network (e.g., an MLP or a Gated Recurrent Unit, GRU) that integrates the aggregated information with the node's own features.
\end{enumerate}
This iterative message passing allows GNNs to effectively propagate and integrate local information across the graph, enabling nodes to learn context-aware representations that capture their structural role and feature interactions within the graph.

Despite its versatility, the MPNN paradigm faces inherent theoretical limits and practical challenges. A significant line of research, initiated by Xu et al. \cite{Xu2018} and further elaborated by Morris et al. \cite{morris20185sd}, established a crucial connection between the discriminative power of standard MPNNs and the 1-Weisfeiler-Lehman (1-WL) test of graph isomorphism. This theoretical framework demonstrates that MPNNs are at most as powerful as the 1-WL test, meaning they cannot distinguish between certain non-isomorphic graphs that the 1-WL test also fails to differentiate. This limitation stems from their reliance on local neighborhood aggregation, which struggles to capture global or higher-order structural patterns such as girth, circumference, or k-clique counts \cite{garg2020z6o, jegelka20222lq}. A detailed exploration of the Weisfeiler-Lehman test and its implications for GNN expressivity, including architectures designed to overcome these bounds, is provided in Section 3.1 and Section 3.2.

Another prominent issue in deep MPNNs is the problem of over-smoothing, where repeated message passing can cause node representations to become increasingly similar, eventually making them indistinguishable regardless of their initial features or structural positions \cite{Li2018}. This phenomenon, often observed in GCNs due to their averaging aggregation, limits the effective depth of MPNNs and hinders their ability to capture fine-grained structural information. Recent analyses have shown that over-smoothing is not exclusive to simple GCNs but also affects more complex architectures, including attention-based GNNs and graph transformers, where it can lead to an exponential loss of expressive power \cite{wu2023aqs}. Furthermore, some research suggests that while over-smoothing is a significant factor, the primary challenge in training deep GNNs might lie more fundamentally in the trainability issues of the underlying MLPs, with methods designed to mitigate over-smoothing often implicitly improving MLP trainability \cite{peng2024t2s}. This nuanced understanding highlights the need for more robust designs to allow for deeper GNNs, which is further discussed in Section 4.4.

Beyond these theoretical and architectural limitations, practical challenges include scaling MPNNs to extremely large graphs (Section 4.1) and their inherent inability to model complex, memory-dependent graph dynamics with a fixed number of message-passing iterations \cite{pflueger2024qi6}.

In conclusion, the MPNN framework provides a powerful and widely adopted blueprint for designing GNNs, enabling effective local information propagation and representation learning on graphs through its well-defined message computation, aggregation, and update functions. However, its inherent limitations in expressive power (bounded by the 1-WL test), the challenge of over-smoothing in deep models, and the computational demands for large-scale graphs continue to drive active research. Addressing these challenges involves exploring architectures that surpass the 1-WL limit (Section 3.1, 3.2), developing strategies to mitigate over-smoothing (Section 4.4), and innovating scalable solutions for web-scale applications (Section 4.1), thereby pushing the boundaries of GNN capabilities and applicability.
\subsection{Early GNN Architectures and Breakthroughs}
\label{sec:2\_3\_early\_gnn\_architectures\_\_and\_\_breakthroughs}

The conceptualization of neural networks operating on graph-structured data predates the deep learning revolution, with pioneering work by \cite{gori2005new} and \cite{scarselli2009graph} laying the theoretical groundwork for Graph Neural Networks (GNNs). These early models introduced the fundamental idea of iteratively propagating information across graph nodes to learn stable, context-aware representations, often relying on fixed-point iterations. While foundational, their practical application was limited by computational challenges and difficulties in scaling to large graphs \cite{wu2022ptq}. The advent of deep learning, coupled with advancements in computational resources, revitalized this field, necessitating novel architectures capable of processing complex relational information efficiently.

A significant breakthrough came with the adaptation of convolutional operations, highly successful in Euclidean data like images, to the irregular structure of graphs. This led to the development of spectral Graph Convolutional Networks (GCNs), initially explored by \cite{bruna2014spectral}. These models leveraged spectral graph theory, defining convolution as a filtering operation in the graph Fourier domain. While theoretically elegant, these early spectral methods often required computationally expensive eigendecompositions of the graph Laplacian and produced non-localized filters, limiting their practical applicability \cite{wu2022ptq}.

The seminal work by \cite{kipf2017semi} introduced a simplified, layer-wise propagation rule for GCNs, which could be interpreted as a first-order approximation of spectral convolutions. This spatial approximation significantly reduced computational complexity, making GCNs practical for semi-supervised node classification tasks. GCNs learn meaningful node embeddings by aggregating information from local neighborhoods, essentially performing a weighted average of neighbor features. This approach demonstrated the ability of deep learning to effectively capture structural dependencies and node features to generate powerful representations. However, these early GCN formulations were primarily transductive, meaning they learned embeddings for a fixed graph and struggled to generalize to unseen nodes or entirely new graphs without retraining. Furthermore, as Message Passing Neural Networks (MPNNs), GCNs are theoretically limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) test \cite{xu2018powerful, morris2018weisfeiler, balcilar20215ga}. This limitation prevents them from distinguishing certain non-isomorphic graphs and counting complex graph substructures, which became a critical area of subsequent research.

To overcome the transductive nature and enable generalization to unseen nodes, \cite{hamilton2017inductive} introduced GraphSAGE (SAmple and aggreGatE). GraphSAGE pioneered an inductive framework by learning a set of aggregation functions that sample and combine features from a node's local neighborhood. Instead of relying on a fixed adjacency matrix, GraphSAGE trains a function that can generate embeddings for any node by aggregating information from its sampled neighbors. This inductive capability was a significant step forward, broadening the applicability of GNNs to dynamic graphs and moderately large datasets by proving their capacity to generalize beyond the training graph. GraphSAGE offered various aggregation functions (e.g., mean, LSTM, max-pooling), allowing flexibility in how neighborhood information is integrated. Despite its inductive power, GraphSAGE, as an MPNN, also inherits the 1-WL expressivity limitations, motivating further research into more powerful architectures \cite{xu2018powerful}.

Concurrent with these developments, other influential architectures emerged, such as Graph Attention Networks (GATs) \cite{velickovic2018graph}. GATs introduced an attention mechanism to GNNs, allowing each node to assign different weights to its neighbors during aggregation, rather than relying on a fixed or uniformly learned weighting scheme. This enabled GATs to handle varying neighborhood sizes and potentially capture more complex relationships, offering another avenue for enhancing GNN performance and interpretability.

These pioneering architectures, from the foundational spectral methods and the practical GCNs to the inductive GraphSAGE and the attention-based GATs, collectively demonstrated the immense potential of GNNs to learn rich representations from graph-structured data. By progressively addressing challenges related to computational efficiency, generalization, and the flexibility of message passing, these early models not only catalyzed broader research and application development but also established a robust framework for subsequent innovations in the field of deep learning on graphs. They proved the viability of deep learning on non-Euclidean data structures, inspiring a new generation of graph-aware intelligent systems, while simultaneously highlighting critical limitations in expressivity and scalability that would drive future research.


\label{sec:enhancing_expressive_power_and_theoretical_limits}

\section{Enhancing Expressive Power and Theoretical Limits}
\label{sec:enhancing\_expressive\_power\_\_and\_\_theoretical\_limits}

\subsection{Weisfeiler-Leman Test and Expressivity Bounds}
\label{sec:3\_1\_weisfeiler-leman\_test\_\_and\_\_expressivity\_bounds}

The Weisfeiler-Leman (WL) test serves as a cornerstone theoretical tool for rigorously characterizing the expressive power of Graph Neural Networks (GNNs). Specifically, its 1-dimensional variant (1-WL), often referred to as the color refinement algorithm, provides a fundamental framework for understanding the inherent limitations of standard Message Passing Neural Networks (MPNNs) in distinguishing between non-isomorphic graphs \cite{morris20185sd, xu2019powerful, jegelka20222lq}. The 1-WL test operates through an iterative node color refinement process: nodes are initially assigned colors (e.g., based on their features or a constant), and in each subsequent step, a node's color is updated based on its current color and the multiset of colors of its immediate neighbors. This process continues until no node's color changes, indicating a fixed point, or until a maximum number of iterations is reached. If two graphs cannot be distinguished by their final color distributions, they are considered 1-WL equivalent.

A foundational result in GNN theory established that standard MPNNs, which aggregate information from local neighborhoods in a permutation-invariant manner, are provably no more powerful than the 1-WL test in distinguishing non-isomorphic graphs \cite{morris20185sd, xu2019powerful, dinverno2024vkw, feng2022914}. This implies that if the 1-WL test fails to differentiate between two graphs, a standard MPNN, such as a Graph Convolutional Network (GCN) or GraphSAGE, will also fail to distinguish them. The Graph Isomorphism Network (GIN) was specifically designed to achieve the maximum discriminative power of the 1-WL test by employing an injective aggregation function \cite{xu2019powerful, jegelka20222lq}. This equivalence to the 1-WL test creates a significant expressivity bottleneck, limiting GNNs' ability to capture complex structural properties and differentiate between certain non-isomorphic graphs. For instance, the 1-WL test, and consequently standard MPNNs, cannot distinguish between non-isomorphic regular graphs (e.g., certain strongly regular graphs) or even simple graphs like cycles of the same length but different structures if their local neighborhoods appear identical \cite{jegelka20222lq}.

The implications of this theoretical bound are profound, as many real-world graph problems demand finer distinctions than the 1-WL test can provide. The inability to distinguish between structurally different graphs poses a challenge for tasks such as molecular property prediction, where subtle structural variations can lead to drastically different chemical properties. More broadly, a wide range of GNNs, including popular architectures like GCN, GAT, and GIN, have been shown to be incapable of computing fundamental graph properties such as girth, circumference, diameter, radius, or counting k-cliques \cite{garg2020z6o, jegelka20222lq}. This limitation arises because these properties often require global information or the ability to distinguish between local structures that appear identical to a 1-WL perspective.

This foundational understanding, developed through early theoretical work, has motivated extensive research into developing more powerful GNN architectures capable of surpassing these limitations. The 1-WL test is merely the first step in a hierarchy of increasingly powerful Weisfeiler-Leman tests (k-WL tests), where k-WL operates on k-tuples of nodes rather than individual nodes, allowing for the detection of more intricate structural differences \cite{morris20185sd, jegelka20222lq}. This hierarchy provides a theoretical roadmap for designing GNNs with enhanced expressive power. For example, while standard MPNNs are bounded by 1-WL, extensions like K-hop message passing GNNs, which aggregate information from neighbors up to K hops away, have been shown to be strictly more powerful than 1-WL, capable of distinguishing almost all regular graphs with a modest K. However, even K-hop message passing is bounded by the 3-WL test, failing on some simple regular graphs \cite{feng20225sa}. This highlights a chronological progression in understanding the limitations and exploring methods to incrementally enhance GNN expressivity.

The ongoing challenge of GNN expressivity is continually addressed by the research community, not only through novel architectural designs but also through rigorous evaluation. The theoretical limits of GNNs underscore the need for benchmarks that include datasets challenging enough to differentiate the expressive power of various GNN architectures. For instance, comprehensive benchmarking efforts, such as those by \cite{dwivedi20239ab}, implicitly acknowledge the importance of GNN expressivity by highlighting that traditional, small datasets often fail to differentiate the capabilities of various GNN models. These benchmarks are designed to include datasets that test specific theoretical graph properties, thereby enabling researchers to evaluate how well GNNs can discriminate between complex graph structures.

In conclusion, the 1-WL test remains a critical theoretical lens through which the expressive power of GNNs is understood and evaluated. The inherent limitations it reveals for standard MPNNs have spurred continuous innovation in GNN architecture design, driving the quest for models that can capture increasingly intricate graph properties and distinctions. This pursuit is consistently validated and guided by robust theoretical analysis and rigorous benchmarking efforts, pushing the boundaries of what GNNs can model and understand about complex graph structures.
\subsection{Higher-Order and Substructure-Aware GNNs}
\label{sec:3\_2\_higher-order\_\_and\_\_substructure-aware\_gnns}

The foundational Message Passing Neural Networks (MPNNs) are known to be limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \cite{xu2018powerful, morris2019weisfeiler}. This inherent limitation restricts their ability to distinguish between many non-isomorphic graph structures and capture intricate relational dependencies, thereby motivating extensive research into advanced GNN architectures designed to incorporate higher-order structural information and explicit subgraph patterns. The goal is to surpass the 1-WL expressivity limit, leading to more discriminative representations crucial for tasks requiring fine-grained structural understanding.

Early attempts to enhance GNN expressivity focused on extending the receptive field and implicitly capturing richer local information. A straightforward approach involves increasing the number of message passing layers, effectively allowing nodes to aggregate information from $K$-hop neighbors. However, the theoretical underpinnings of $K$-hop message passing were not fully characterized until recently. \cite{feng20225sa} provided a rigorous theoretical analysis, formally distinguishing between different $K$-hop kernels (shortest path distance vs. graph diffusion) and proving that $K$-hop message passing (for $K>1$) is strictly more powerful than 1-WL. Crucially, they demonstrated that while $K$-hop GNNs can distinguish almost all regular graphs, their expressive power is ultimately bounded by the 3-WL test. To further enhance this, they introduced the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework, which integrates information from the "peripheral subgraph" (edges within a hop's neighbors), allowing for more expressive local structural features without significant computational overhead.

Beyond simple extensions of the receptive field, other methods sought to break the symmetries that limit 1-WL by introducing controlled randomness or explicit positional information. \cite{abboud2020x5e} made a surprising theoretical contribution by proving that standard MPNNs, when augmented with Random Node Initialization (RNI), become universal approximators for any function defined on graphs of a fixed order. This means RNI can overcome the 1-WL barrier without the prohibitive computational costs of higher-order GNNs (e.g., $k$-GNNs which operate on $k$-tuples of nodes). The universality is achieved because RNI individualizes input graphs with high probability, allowing MPNNs to capture arbitrary Boolean functions. While RNI can lead to slower convergence, partial randomization of initial node features was shown to improve practical performance. Similarly, \cite{papp20211ac} proposed DropGNN, a novel paradigm that increases expressiveness by executing multiple independent runs of a standard GNN on randomly perturbed versions of the input graph (nodes are dropped out with a certain probability). By aggregating embeddings from these diverse runs, DropGNN allows the GNN to observe various slightly altered neighborhood patterns, enabling it to distinguish graph structures otherwise indistinguishable by standard message-passing GNNs, with theoretical proofs demonstrating its expressiveness beyond the WL-test.

A more direct approach to substructure awareness involves explicitly encoding richer subgraph patterns. An early example, though primarily focused on link prediction, was the SEAL (Subgraph Embedding for Link prediction) framework by \cite{zhang2018kdl}. SEAL introduced the concept of learning general graph structure features from local enclosing subgraphs. By providing a $\beta$-decaying heuristic theory, it justified approximating high-order heuristics from relatively small $h$-hop enclosing subgraphs. This framework implicitly extended the GNN's receptive field by focusing on localized structural patterns and employed a novel structural node labeling scheme within these subgraphs, injecting explicit structural context.

Building upon the idea of leveraging subgraphs, \cite{zhang2021kc7} introduced Nested Graph Neural Networks (NGNNs). NGNNs adopt a two-level GNN architecture designed to encode \textit{rooted subgraphs} rather than just rooted subtrees (which 1-WL and standard GNNs primarily encode). For each node, a local rooted subgraph is extracted, and an inner "base GNN" is applied independently to learn intermediate representations within that subgraph. These subgraph-level representations are then aggregated by an "outer GNN" to form the final node or graph representations. NGNNs are proven to be strictly more powerful than the 1-WL test and standard message passing GNNs, capable of discriminating almost all $r$-regular graphs while maintaining linear time and space complexity, addressing a critical trade-off between expressivity and efficiency.

More recently, \cite{zeng20237gv} introduced Substructure Aware Graph Neural Networks (SAGNNs) to explicitly inject higher-order substructural information. SAGNNs achieve this through two key innovations: a "Cut subgraph" extraction method, which identifies meaningful substructures by selectively removing edges with high Edge Betweenness Centrality, and an efficient random walk return probability encoding mechanism. This encoding leverages explicit path information within extracted subgraphs, providing a richer topological representation than traditional aggregation. The authors theoretically proved that SAGNNs enhance expressiveness beyond the 1-WL test, allowing them to distinguish complex graph structures that are isomorphic according to 1-WL but structurally distinct in more intricate ways, even outperforming models on graphs that fool the 3-WL test.

Collectively, these works illustrate a clear progression in GNN design towards a more discriminative understanding of complex graph structures. From the initial theoretical characterization of $K$-hop message passing and the clever use of randomization to break symmetries, to the explicit encoding of higher-order substructural patterns and path-based information, the field is moving beyond simple message passing. While approaches like RNI and DropGNN offer memory-efficient ways to enhance expressivity, methods like NGNNs and SAGNNs directly tackle the challenge by integrating richer, explicitly defined subgraph features. The ongoing challenge lies in balancing this enhanced expressivity with computational efficiency, ensuring generalization across diverse graph types and scales, and designing substructure features that are both informative and computationally tractable. Future research will likely continue to explore novel ways to efficiently identify, encode, and integrate higher-order structural motifs without incurring prohibitive computational costs, while also investigating the empirical gains of these provably powerful models on real-world benchmarks.
\subsection{Geometric and Equivariant GNNs}
\label{sec:3\_3\_geometric\_\_and\_\_equivariant\_gnns}

Geometric and equivariant Graph Neural Networks (GNNs) represent a crucial and distinct advancement for accurately modeling 3D spatial data, particularly in scientific domains such as chemistry, physics, and molecular biology. These fields are inherently governed by fundamental physical symmetries, including rotations, translations, and reflections. Consequently, models that inherently respect these symmetries are indispensable for learning robust and physically consistent representations of 3D structures. This area of research developed in parallel with efforts to enhance the expressivity of GNNs for abstract graphs, recognizing the unique requirements and inductive biases offered by spatial data \cite{han20227gn}.

Early approaches to embedding geometric equivariance into neural networks often relied on computationally intensive formalisms. For instance, pioneering models like Tensor Field Networks (TFN) \cite{thomas2018tensor} and SE(3) Transformers \cite{fuchs2020se} utilized spherical harmonics to represent and propagate information. These methods allowed for higher-order representations that naturally transform correctly under rotations, ensuring strict equivariance to the SO(3) group. While theoretically powerful in capturing complex angular dependencies, these architectures typically incurred significant computational overhead due to the complexity of spherical harmonic transformations and were often restricted to 3-dimensional Euclidean spaces (E(3) or SE(3) equivariance), posing challenges for scalability to larger systems or generalization to arbitrary dimensions.

A significant breakthrough in addressing these efficiency and scalability concerns came with the introduction of the E(n) Equivariant Graph Neural Network (EGNN) by \cite{satorras2021pzl}. The core innovation of EGNN lies in its Equivariant Graph Convolutional Layer (EGCL), which directly updates node coordinates using relative differences. This design inherently preserves E(n) equivariance for both scalar node features and vector coordinates without resorting to computationally expensive higher-order representations like spherical harmonics. The EGNN formulation is theoretically applicable to arbitrary $n$-dimensions, though its primary validation and impact have been in 3D applications. It demonstrated superior performance in tasks such as N-body system simulations, significantly outperforming prior methods like TFN and SE(3) Transformers by reducing prediction error by an order of magnitude while maintaining competitive computational efficiency \cite{satorras2021pzl}.

While architectural innovations like EGNN provided practical and efficient solutions, a comprehensive theoretical understanding of the expressive power of geometric GNNs remained an open problem. Filling this gap, \cite{joshi20239d0} developed the Geometric Weisfeiler-Leman (GWL) test, a symmetry-aware generalization of the classical Weisfeiler-Leman test for non-geometric graphs. This theoretical framework rigorously characterizes the expressive capabilities of geometric GNNs, providing upper bounds on their ability to distinguish geometric structures. The GWL test operates by iteratively refining node "colors" and auxiliary "geometric objects" using G-orbit injective and G-equivariant hash functions that aggregate geometric information (e.g., distances, angles, dihedral angles) from progressively larger neighborhoods. This framework offers critical insights into the distinct contributions of invariant versus equivariant layers, demonstrating that while invariant layers have limited expressivity, equivariant layers, by propagating geometric information, can distinguish a larger class of graphs. Furthermore, the work highlighted the importance of higher-order tensors or scalarization (e.g., higher body order in IGWL(k)) for achieving maximally powerful geometric GNNs, as these provide more complete descriptors of local geometry beyond simple distances and angles.

The theoretical insights from the GWL test, particularly regarding the need for richer geometric descriptors, directly motivated subsequent architectural advancements. Building upon these insights into geometric feature integration, \cite{klicpera20215fk} introduced GemNet, a Universal Directional Graph Neural Network for molecules. GemNet provided a theoretical proof of universality for GNNs utilizing spherical (S2) representations, demonstrating that they can be universal approximators for rotationally equivariant predictions, achieving similar expressivity to more complex SO(3) representations but with greater efficiency. A key technical innovation of GemNet is its two-hop message passing scheme, which explicitly and systematically incorporates not only interatomic distances and angles but also crucial dihedral angles through directed edge embeddings. This deeper integration of higher-order geometric information proved vital for accurately predicting quantum mechanical properties and accelerating molecular dynamics simulations, particularly for complex, dynamic molecular geometries.

Despite the success and widespread adoption of EGNN and similar architectures, the question of whether higher-degree representations are truly unnecessary in equivariant GNNs has been critically re-examined. \cite{cen2024md8} theoretically demonstrated that equivariant GNNs, including EGNN-like models, can degenerate to a zero function if the degree of output representations is fixed to 1 or other specific values, especially when dealing with highly symmetric structures. This suggests a limitation in EGNN's expressivity for certain complex geometric patterns, as its reliance on first-degree steerable vectors (Cartesian coordinates) can be insufficient to distinguish all geometrically non-isomorphic graphs. To address this, they proposed HEGNN, a high-degree version of EGNN that incorporates higher-degree steerable vectors while maintaining efficiency through a scalarization trick. Their empirical results on N-body and MD17 datasets showed substantial improvements, indicating that judicious use of higher-degree representations can indeed enhance expressivity, aligning with the GWL's theoretical findings.

The development of these geometrically aware GNNs has been transformative for scientific applications. As highlighted by reviews from \cite{reiser2022b08} and \cite{han20227gn}, GNNs, particularly those integrating geometric data and symmetry-equivariant representations, have become indispensable in materials science and chemistry. They enable robust, end-to-end learning from graph-structured data, facilitating accurate predictions of molecular properties, reaction pathways, and crystal structures by inherently respecting the underlying physical symmetries.

In conclusion, the evolution of geometric and equivariant GNNs reflects a clear progression from efficient architectural designs, like EGNN, to robust theoretical frameworks, such as the GWL test, and advanced models like GemNet and HEGNN that integrate higher-order geometric features. By embedding physical symmetries directly into their learning mechanisms, these GNNs have significantly enhanced their ability to learn robust and physically consistent representations of 3D structures. However, purely spatial message passing GNNs, including many geometric variants, still face inherent limitations such as "over-squashing" \cite{alon2020bottleneck}, which restricts the receptive field and impedes long-range information propagation crucial for understanding complex molecular interactions. This limitation arises because local message passing over fixed-radius neighborhoods struggles to efficiently transmit information across large molecular structures. Future research will likely focus on extending these models to incorporate more sophisticated higher-order geometric features, exploring gauge equivariant GNNs (which are essential for symmetries beyond Euclidean, such as those found in particle physics and lattice field theory), and developing even more expressive and efficient equivariant architectures for handling multi-scale geometric information and dynamic systems while mitigating issues like over-squashing.
\subsection{Spatio-Spectral Architectures for Long-Range Interactions}
\label{sec:3\_4\_spatio-spectral\_architectures\_for\_long-range\_interactions}

Deep Graph Neural Networks (GNNs) built upon the Message Passing Neural Network (MPNN) paradigm inherently face critical limitations when tasked with understanding global graph properties and long-range dependencies. The iterative aggregation of information from immediate neighbors restricts a $K$-layer MPNN's receptive field to a $K$-hop neighborhood. This localized propagation leads to phenomena like over-squashing, where information from distant nodes becomes diluted or lost, and restricted receptive fields, making it exponentially difficult for information to traverse large graphs. Consequently, deeper MPNNs often suffer from vanishing gradients and oversmoothing, where node representations become indistinguishable, severely hindering their expressivity for tasks requiring holistic graph understanding.

To overcome these challenges, a significant line of research has focused on integrating spectral information into GNN architectures, moving beyond purely spatial aggregation. Early efforts recognized the value of the graph Laplacian's eigenvectors as positional encodings (PEs) \cite{dwivedi20239ab, dwivedi2021af0}. By providing GNNs with a sense of global position and structure, these PEs enhance the models' ability to differentiate between structurally similar but topologically distinct nodes, thereby improving expressivity. However, these approaches typically treat spectral information as an auxiliary feature, informing spatial message passing rather than fundamentally altering the filtering mechanism.

More direct integration of spectral properties into the core convolution operation has been explored through spectral GNNs, which approximate filters in the graph spectral domain. Traditional spectral GNNs often employ polynomial filters (e.g., Chebyshev polynomials in ChebyNet or simplified forms in GCNs) to avoid computationally expensive eigendecomposition \cite{kipf2016semi, defferrard2016convolutional}. While efficient and localized in the node space, these polynomial filters have inherent limitations. Their frequency response is often constrained, making it difficult to model sharp changes or capture complex global patterns. Furthermore, achieving higher-order neighborhood information with these filters typically requires deeper stacks, which can exacerbate over-smoothing and sensitivity to noise \cite{bianchi20194ea}. Recent theoretical work by \cite{wang2022u2l} has delved into the expressive power of \textit{linear} spectral GNNs, demonstrating that under certain mild conditions (e.g., no multiple eigenvalues of the graph Laplacian), these models can achieve universal approximation. This challenges the conventional assumption that non-linearity is strictly necessary for high expressiveness in spectral GNNs and establishes a novel connection between spectral universality and the 1-Weisfeiler-Leman (1-WL) test, offering insights into how spectral properties can constrain graph symmetry and differentiate non-isomorphic nodes.

A significant advancement in this direction is the introduction of Spatio-Spectral Graph Neural Networks (S2GNNs) \cite{geisler2024wli}. This novel architectural paradigm proposes operating directly in the spectral domain by employing spatio-spectral filters that synergistically combine local spatial message passing with global spectral filtering. S2GNNs are designed to achieve spatially unbounded information propagation by applying a learnable function of the graph Laplacian's eigenvalues to the node features in the spectral domain, effectively performing a global filtering operation. This global spectral filtering is then combined with a local spatial aggregation step, creating a powerful hybrid mechanism. By intrinsically integrating both local and global perspectives, S2GNNs directly address the over-squashing problem and the restricted receptive fields inherent in purely spatial GNNs. They offer superior approximation bounds and provide stable positional encodings "for free," significantly enhancing expressivity for tasks requiring global graph understanding and long-range dependencies. This approach offers a principled way to capture holistic graph properties and mitigate the vanishing gradient problem in deep GNNs, representing a distinct architectural solution that moves beyond K-hop aggregation or explicit subgraph/path encoding by fundamentally redesigning the information flow.

Beyond S2GNNs, other architectures also tackle long-range dependencies through different forms of global information integration. Graph Transformers \cite{wu20221la, chen2022mmu} represent a prominent alternative, leveraging self-attention mechanisms to compute pairwise relationships between all nodes, thereby achieving a global receptive field. Similar to S2GNNs, Graph Transformers often utilize spectral information, typically through Laplacian positional encodings, to inform their attention mechanisms. However, S2GNNs distinguish themselves by making global spectral filtering a core, learnable component of the propagation rule itself, rather than solely relying on attention informed by auxiliary spectral features.

Further innovations in spectral filtering include the use of Auto-Regressive Moving Average (ARMA) filters in GNNs \cite{bianchi20194ea}. Unlike polynomial filters, ARMA filters offer a more flexible frequency response, capable of modeling a wider variety of spectral characteristics and capturing longer-range dynamics with fewer parameters. While direct implementation of ARMA filters involves computationally intractable matrix inversions, \cite{bianchi20194ea} approximates their effect through recursive update rules, ensuring computational efficiency and stability. This recursive formulation, combined with skip connections, helps mitigate over-smoothing and allows for inductive inference on unseen graph topologies.

Another approach to achieving global understanding, albeit primarily in the spatial domain, is seen in models like GloGNN \cite{li2022315}. GloGNN aims to find "global homophily" even in heterophilous graphs by performing node neighborhood aggregation from the \textit{entire set of nodes} in the graph. It learns a signed coefficient matrix that captures correlations between all nodes, effectively assigning positive weights to homophilous nodes and potentially negative weights to heterophilous ones. Crucially, GloGNN employs an acceleration technique that reduces the computational complexity of this global aggregation from quadratic/cubic to linear time, making it efficient for large graphs. While not strictly spatio-spectral, GloGNN's ability to leverage global node correlations for improved representations aligns with the broader goal of capturing long-range interactions.

In conclusion, the evolution of spatio-spectral architectures marks a pivotal shift towards more powerful GNNs capable of overcoming the long-standing challenges of over-squashing and limited receptive fields. By intrinsically combining local spatial and global spectral information, models like S2GNNs \cite{geisler2024wli}, alongside advancements in spectral filter design \cite{bianchi20194ea, wang2022u2l} and global attention mechanisms \cite{wu20221la}, offer principled paths to capturing holistic graph properties and enabling more robust representation learning. Future research will likely focus on further optimizing these hybrid architectures, exploring their theoretical limits, and integrating them with other advanced techniques to handle even larger and more complex graph structures requiring truly global understanding.


\label{sec:scaling,_robustness,_and_adaptability}

\section{Scaling, Robustness, and Adaptability}
\label{sec:scaling,\_robustness,\_\_and\_\_adaptability}

\subsection{Scaling GNNs for Web-Scale Applications}
\label{sec:4\_1\_scaling\_gnns\_for\_web-scale\_applications}

The practical deployment of Graph Neural Networks (GNNs) in industrial settings, particularly for web-scale applications involving billions of nodes and edges, necessitates overcoming formidable computational and memory bottlenecks \cite{khemani2024i8r, wu2022ptq}. Early GNN architectures, while theoretically powerful, were often computationally prohibitive due to their reliance on full-graph operations (e.g., spectral convolutions requiring the full graph Laplacian) or the implicit assumption that the entire graph could reside in memory. This limitation severely restricted their applicability to smaller datasets and prevented their adoption in real-world systems like large-scale recommender platforms \cite{wu2020dc8, gao2022f3h}. Addressing this scalability gap has been a critical research frontier, leading to diverse methodological advancements.

One of the earliest and most influential paradigms for scaling GNNs is \textbf{neighborhood sampling}, which aims to construct localized computation graphs for each node or minibatch, thereby avoiding full-graph computations. A pivotal methodological leap in this direction was PinSage \cite{ying20189jc}, developed for Pinterest's recommender system. PinSage fundamentally re-architected how GNNs interact with massive graphs by performing efficient, localized convolutions through dynamic, on-the-fly neighborhood sampling. Instead of processing the entire graph, it constructs computation graphs for each minibatch by sampling neighbors, eliminating the need for the full graph Laplacian and making training feasible for graphs with billions of nodes and edges. PinSage further refines this by integrating random walks to derive "importance scores" for neighbors, leading to an Importance Pooling aggregation strategy where neighbor features are weighted based on their L1-normalized visit counts. This nuanced approach significantly improves the quality of learned embeddings. The system's robustness is enhanced by a producer-consumer architecture for minibatch generation, decoupling CPU-bound tasks (sampling, feature fetching) from GPU-bound tasks (model computation), maximizing GPU utilization and training throughput. For scalable inference, PinSage employs an efficient MapReduce pipeline to generate billions of node embeddings. While highly effective, sampling-based methods like PinSage and its predecessor GraphSAGE \cite{hamilton2017inductive} can introduce variance into gradient estimates, potentially affecting convergence and requiring careful tuning.

Beyond PinSage, other sampling strategies have emerged to enhance efficiency and reduce variance. FastGCN \cite{chen2018fastgcn} proposes sampling nodes for each layer rather than neighbors for each node, effectively treating graph convolutions as integral transforms and using Monte Carlo approximations. LADIES (Layer-wise Adaptive DEpth-wise Importance Sampling) \cite{zou2019layerwise} further refines this by employing layer-wise importance sampling to reduce variance, especially in deep GNNs, by prioritizing more influential neighbors. These methods offer different trade-offs between sampling efficiency, variance reduction, and computational complexity.

Another major family of scaling techniques involves \textbf{graph partitioning or clustering}. Cluster-GCN \cite{chiang2019clustergcn} addresses the memory bottleneck by partitioning the graph into several subgraphs and performing message passing within these clusters. This approach significantly reduces the memory footprint and allows for deeper GNNs, but it can suffer from information loss at cluster boundaries. GraphSAINT \cite{zou2019graphsaint} takes a different approach by sampling subgraphs (either nodes or edges) at each iteration to construct computation graphs, allowing for full-batch training on sampled subgraphs while maintaining a global view of the graph structure. This method aims to reduce the variance associated with sampling by ensuring that sampled subgraphs are representative of the original graph structure. The trade-off here lies in the complexity of sampling and the potential for redundant computations if subgraphs overlap significantly.

More recent advancements have focused on \textbf{simplified architectures and graph preprocessing} to improve scalability. Simplified Graph Convolution (SGC) \cite{wu2019simplifying} demonstrates that removing non-linearities between GNN layers can drastically speed up computation while maintaining competitive performance, particularly for transductive tasks. This highlights that for certain applications, complex non-linear transformations may not always be necessary, allowing for more efficient models.

Graph condensation techniques, such as GCOND (Graph Condensation) \cite{jin2021pf0}, offer a novel approach by learning a small, synthetic graph that preserves the GNN's predictive performance when trained on it, effectively distilling the essence of a large graph into a much smaller one. This drastically reduces training time and storage requirements, proving invaluable for scenarios requiring frequent retraining or hyperparameter search. Similarly, graph sparsification and pruning methods, like the Unified GNN Sparsification (UGS) framework \cite{chen2021x8i}, simultaneously prune the graph adjacency matrix and model weights to accelerate GNN inference on large graphs without compromising predictive performance. Furthermore, graph rewiring and preprocessing techniques, such as GPER (Graph Rewiring and Preprocessing based on Effective Resistance) \cite{shen2024exf}, can reduce graph size by over 50\\% while mitigating common GNN issues like over-smoothing and over-squashing, thereby improving both performance and scalability. Approximate PageRank-based models like PPRGo \cite{bojchevski2020c51} also offer significant speed gains by utilizing efficient approximations of information diffusion, enabling rapid training and prediction on massive graphs.

In summary, scaling GNNs for web-scale applications has evolved through a multi-faceted approach, encompassing sophisticated neighborhood sampling, intelligent graph partitioning, architectural simplifications, and advanced graph preprocessing techniques. While PinSage pioneered the industrial application of sampling-based GNNs, the field has since diversified, offering a spectrum of solutions, each with its own trade-offs regarding computational cost, memory footprint, and approximation quality. The continuous development of these methodologies, often combining elements from different paradigms, remains crucial for bridging the gap between academic research and the practical demands of handling unprecedented data volumes in real-world industrial deployments \cite{khemani2024i8r, wu2022ptq}.
\subsection{Addressing Structural Heterogeneity and Disparity}
\label{sec:4\_2\_addressing\_structural\_heterogeneity\_\_and\_\_disparity}

Real-world graphs rarely conform to a single, uniform structural pattern; instead, they often exhibit a complex interplay of mixed homophilic (nodes with similar attributes connect) and heterophilic (nodes with dissimilar attributes connect) connections. This inherent structural diversity, rigorously termed 'structural disparity' \cite{mao202313j}, poses a pervasive challenge for Graph Neural Networks (GNNs). Traditional "one-size-fits-all" aggregation strategies, which typically assume homophily and perform low-pass filtering by averaging neighbor features, often lead to suboptimal performance and significant disparities across different node subgroups \cite{zheng2022qxr}.

The challenge of heterophily has been a long-standing concern in GNN research. Early approaches sought to mitigate the detrimental effects of aggregating dissimilar neighbor information. A common strategy involved explicitly separating a node's own features (ego-features) from its neighbors' aggregated features (neighbor-features) before combining them, allowing the model to weigh their importance differently. Other methods expanded the receptive field to capture higher-order or multi-hop neighbors, recognizing that homophilous nodes might be structurally distant in heterophilic graphs \cite{zheng2022qxr}. For instance, models like MixHop or GPR-GNN implicitly or explicitly aggregated information from multi-hop neighborhoods to capture both local and broader structural patterns. The theoretical underpinnings for these diverse aggregation strategies were later unified by frameworks such as that proposed by \cite{zhu2021zc3}, which interpreted various GNN propagation mechanisms as optimal solutions to an objective function incorporating flexible graph convolutional kernels. This framework demonstrated how GNNs could be designed with specific low-pass, high-pass, or all-pass filtering capabilities, providing a principled way to understand and develop models that could adapt to different frequency components of graph signals.

Despite these advancements, a comprehensive understanding and systematic solution for graphs exhibiting \textit{mixed} homophily and heterophily within a single structure remained elusive. The fundamental limitations of uniform aggregation were rigorously diagnosed by \cite{mao202313j}. This seminal work formally introduced "structural disparity," highlighting the pervasive existence of mixed patterns and demonstrating that GNNs suffer from significant "performance disparity" on minority structural patterns. Through extensive diagnostic analyses and a novel theoretical framework, \cite{mao202313j} provided compelling evidence that a single, globally applied aggregation strategy is inherently flawed for such complex structural mixtures. This research underscored the critical need for GNNs that can dynamically adapt their aggregation behavior based on the local structural context of each node.

Building directly upon these diagnostic insights, recent innovations have focused on developing adaptive filtering mechanisms that move beyond global assumptions. \cite{luan202272y} revisited heterophily from the perspective of post-aggregation node similarity and introduced new, more informative homophily metrics. Based on these insights, they proposed the Adaptive Channel Mixing (ACM) framework, which augments baseline GNNs by adaptively exploiting three distinct channels—aggregation (low-pass), diversification (high-pass, implemented as $I - \hat{A}$), and identity—node-wisely and locally in each layer. This allows GNNs to extract richer localized information tailored to diverse node heterophily situations, demonstrating significant performance gains on heterophilic datasets without incurring substantial computational overhead.

Further advancing this adaptive paradigm, \cite{han2024rkj} proposed Node-wise Filtering in Graph Neural Networks via a Mixture of Experts (NODE-MOE). This framework represents a significant leap towards truly localized information processing by dynamically applying different aggregation strategies to individual nodes based on their local structural context. NODE-MOE employs a sophisticated gating mechanism that learns to estimate a node's structural pattern from contextual features, subsequently selecting and applying an appropriate "expert" GNN filter. These experts are initialized with diverse filter types, such as low-pass (for homophilic patterns), high-pass (for heterophilic patterns), and constant filters, allowing the model to adaptively capture relevant information for each node. This dynamic, node-wise approach significantly enhances GNN robustness and accuracy across diverse structural patterns within a single graph, effectively moving beyond the limitations of uniform aggregation. Concurrently, other approaches like GloGNN \cite{li2022315} tackle heterophily by learning a signed coefficient matrix for global aggregation, implicitly combining low-pass and high-pass filtering across all nodes, and achieving linear time complexity for scalability.

The progression from early, often heuristic-driven, attempts to mitigate heterophily to the rigorous diagnosis of structural disparity \cite{mao202313j}, and finally to sophisticated adaptive, node-wise filtering mechanisms like ACM \cite{luan202272y} and NODE-MOE \cite{han2024rkj}, underscores a critical evolution in GNN research. While these adaptive methods offer powerful solutions, challenges remain. Future research could focus on developing more interpretable gating mechanisms for mixture-of-experts models, exploring the computational overhead of such adaptive approaches on web-scale graphs, or designing novel expert GNN architectures that can capture even finer-grained structural nuances beyond simple low-pass and high-pass filtering. Furthermore, integrating these adaptive filtering techniques with mechanisms for handling dynamic graph structures or multi-modal node features could lead to even more robust and versatile GNNs capable of addressing the multifaceted structural complexities prevalent in real-world networks \cite{zheng2022qxr}.
\subsection{Learning with Weak Information and Imperfect Data}
\label{sec:4\_3\_learning\_with\_weak\_information\_\_and\_\_imperfect\_data}

Real-world graph data is inherently noisy and often suffers from pervasive imperfections, including incomplete graph structures, sparse node features, and limited or noisy labels. These deficiencies rarely occur in isolation; instead, they frequently manifest simultaneously, posing significant challenges for Graph Neural Networks (GNNs) and necessitating robust learning frameworks capable of operating effectively under such weak information regimes. The ability to learn from imperfect data is paramount for the practical deployment of GNNs, where perfect datasets are rare and costly to acquire.

Addressing incomplete graph structures is a foundational challenge. Early approaches to infer missing structural information often focused on link prediction. For instance, the SEAL framework \cite{zhang2018kdl} introduced a novel $\beta$-decaying heuristic theory, theoretically justifying the learning of high-order heuristics from local enclosing subgraphs. This method is particularly valuable when global graph information is incomplete or noisy, as it allows GNNs to infer missing links by leveraging local structural patterns. More broadly, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) emerged as powerful unsupervised methods to reconstruct graph structures and learn latent representations by minimizing the reconstruction error of the adjacency matrix, effectively filling in missing links and denoise existing ones based on node features. However, these methods often assume relatively clean node features to infer structure.

Beyond simple missing links, real-world graphs can suffer from structural noise, where spurious or adversarial edges exist. Simultaneously, label scarcity is a common problem, limiting the supervision available for GNN training. The Robust GNN with Sparse Labels (RS-GNN) framework \cite{dai2022xze} directly tackles this dual challenge. RS-GNN proposes a unified approach that concurrently learns a denoised and densified graph structure while training a robust GNN classifier. It employs a novel MLP-based link predictor that leverages node attributes to assign small weights to noisy edges (connecting dissimilar nodes) and predict missing links between similar nodes, thereby both denoising and densifying the graph. This densification is crucial for increasing the involvement of unlabeled nodes in message passing, mitigating label sparsity. Furthermore, RS-GNN incorporates a feature similarity weighted edge-reconstruction loss and label smoothness regularization, explicitly guiding the learning process to be robust against structural noise and effective with limited labels. This represents a significant step towards integrated solutions for multiple data imperfections. Similarly, \cite{zhang2024370} explored the potential of Large Language Models (LLMs) to improve the adversarial robustness of GNNs, proposing LLM4RGNN. This framework distills the inference capabilities of LLMs to identify malicious edges and predict important missing edges, thereby recovering a more robust graph structure against topology perturbations, which can be viewed as a severe form of structural imperfection.

The challenge of limited labels is also extensively addressed through semi-supervised and self-supervised learning paradigms. Traditional label propagation techniques, which smooth labels over the graph structure, have seen a resurgence. For example, the Correct and Smooth (C\\&S) method \cite{huang20209zd} demonstrated that combining shallow models (ignoring graph structure) with simple label propagation steps can match or exceed state-of-the-art GNNs on many transductive node classification benchmarks, particularly with sparse labels. C\\&S effectively exploits label correlation by spreading residual errors and smoothing predictions, highlighting the power of directly incorporating label information. Building on this, self-training frameworks have gained traction. Distribution-Consistent Graph Self-Training (DC-GST) \cite{wang2024htw} addresses few-shot node classification with sparse labels by explicitly bridging the distribution shift between labeled and unlabeled nodes during self-training. It identifies informative pseudo-labeled nodes and uses a distribution-shift-aware edge predictor to augment the graph, enhancing generalizability and pseudo-label assignment. Furthermore, self-supervised GNNs, as explored by \cite{wei20246l2}, offer a powerful way to learn robust node representations from abundant unlabeled data, which is beneficial when both features are sparse and labels are scarce. By flexibly combining different types of attribute graph information, these models can mine deeper features, improving adaptability to complex and diverse graph data.

When multiple deficiencies—incomplete structure, sparse features, and limited labels—co-occur, integrated frameworks become essential. The Dual-channel Diffused Propagation then Transformation (D2PT) framework \cite{liu2023v3e} exemplifies such a multi-faceted approach. D2PT employs a dual-channel architecture, processing both the input graph and a dynamically learned global graph, and integrates prototype contrastive alignment to foster mutual benefits between these channels. This design enables GNNs to effectively handle simultaneously occurring data deficiencies and specifically addresses challenges like the 'stray node problem' in sparse graphs, ensuring strong performance even with extremely weak or noisy information. The strength of D2PT lies in its ability to leverage both local and global structural information, while contrastive learning helps in learning discriminative representations despite label scarcity and feature sparsity.

In summary, the literature demonstrates a clear progression from addressing individual data imperfections to developing sophisticated, integrated frameworks that can robustly learn from simultaneously occurring deficiencies across structure, features, and labels. While early methods focused on specific aspects like link prediction \cite{zhang2018kdl} or label propagation \cite{huang20209zd}, recent advancements like RS-GNN \cite{dai2022xze} and D2PT \cite{liu2023v3e} offer comprehensive solutions by jointly learning robust graph structures, rich node features, and accurate predictions under weak supervision. Future directions in this area include developing more adaptive strategies that can dynamically adjust to varying levels of data imperfection, exploring the theoretical guarantees for these complex joint optimization problems, and further integrating advanced AI paradigms like LLMs for robust graph structure inference \cite{zhang2024370}. Additionally, the development of comprehensive benchmarks for evaluating GNNs under label noise, such as NoisyGL \cite{wang2024481}, is crucial for ensuring fair comparisons and accelerating progress in this critical domain. These efforts collectively aim to enhance GNNs' resilience and practical utility in the pervasive imperfect data environments of the real world.
\subsection{Mitigating Oversmoothing and Modeling Complex Dynamics}
\label{sec:4\_4\_mitigating\_oversmoothing\_\_and\_\_modeling\_complex\_dynamics}

The persistent challenge of oversmoothing, where node representations in deep Graph Neural Networks (GNNs) become indistinguishable after multiple layers of message passing, fundamentally limits model depth and expressivity. This phenomenon, alongside the broader challenge of modeling complex, non-Markovian graph dynamics, has spurred extensive research into novel architectural designs, regularization techniques, and mathematical frameworks. While oversmoothing was initially considered the primary impediment to deep GNNs, recent analyses suggest a more nuanced understanding, identifying other factors like trainability and information bottlenecks as equally critical for effective long-range information propagation \cite{peng2024t2s, alon2020fok}.

A comprehensive understanding of oversmoothing begins with its formal definition. Rusch et al. \cite{rusch2023xev} propose an axiomatic definition, characterizing oversmoothing as the layer-wise exponential convergence of a node-similarity measure (e.g., graph Dirichlet energy) to zero. This rigorous framework allows for a more precise evaluation of mitigation strategies. However, the narrative around oversmoothing has evolved. Peng et al. \cite{peng2024t2s} argue that while oversmoothing is a contributing factor, the \textit{trainability challenges} of deep Multi-Layer Perceptrons (MLPs) embedded within GNNs are often the more dominant reason for performance degradation in deep architectures. They contend that many methods ostensibly designed to combat oversmoothing primarily improve MLP trainability. Concurrently, Alon et al. \cite{alon2020fok} introduced the concept of "over-squashing," a distinct information bottleneck where the exponentially growing receptive field of a node is compressed into a fixed-size vector. This compression leads to significant information loss for long-range dependencies, particularly problematic for tasks requiring interactions between distant nodes, and challenges the sole focus on oversmoothing for all deep GNN failures. These critical perspectives highlight that effective deep GNN design requires addressing a multifaceted set of challenges beyond just feature convergence.

To counteract oversmoothing and enhance expressivity, various architectural modifications and regularization techniques have been developed. A common strategy involves \textbf{residual or skip connections}, exemplified by GCNII \cite{chen2020gcnii} and Jumping Knowledge Networks (JK-Nets) \cite{xu2018powerful}. These methods allow information from earlier layers to bypass later layers, preventing complete feature homogenization and enabling deeper models. However, while effective, they can increase model complexity or parameter count. \textbf{Normalization techniques}, such as PairNorm \cite{zhao2019pairnorm} and NodeNorm \cite{zhou2020nodenorm}, explicitly regularize node features to prevent them from collapsing into a narrow subspace. PairNorm, for instance, normalizes features based on their pairwise differences, directly counteracting the smoothing effect \cite{rusch2023xev}. \textbf{Regularization methods} like DropEdge \cite{rong2019dropedge}, which randomly drops edges during training, introduce noise that prevents over-reliance on local neighborhoods and can implicitly mitigate oversmoothing by altering the graph topology. Papp et al. \cite{papp20211ac} further explored the use of random node dropouts in DropGNN, not just for regularization, but to increase expressiveness by allowing the GNN to observe diverse neighborhood patterns across multiple runs. This approach can indirectly help maintain distinct features in deeper models by enhancing their ability to distinguish subtle structural differences.

Another architectural approach focuses on \textbf{decoupling the depth and scope} of GNNs. Zeng et al. \cite{zeng2022jhz} propose SHADOW-GNN, which first extracts a localized subgraph (bounded scope) and then applies a GNN of arbitrary depth \textit{only} on this subgraph. This strategy performs "local-smoothing" rather than global oversmoothing, theoretically preventing feature collapse and enhancing expressivity, particularly for large graphs where traditional deep GNNs suffer from neighbor explosion and oversmoothing. This method offers a principled way to build deeper GNNs without incurring prohibitive computational costs or global oversmoothing, though its effectiveness relies heavily on the quality of the subgraph extraction function.

Beyond direct architectural fixes, researchers have explored alternative propagation schemes and fundamental mathematical dynamics to address oversmoothing and model complex graph behaviors. The \textbf{Predict then Propagate} framework, notably Personalized Propagation of Neural Predictions (PPNP) and its approximation APPNP \cite{klicpera20186xu}, decouples initial feature transformation from subsequent graph diffusion. By introducing a "teleport probability," these models allow for extensive message passing over large neighborhoods without representations collapsing, effectively mitigating oversmoothing and enabling deeper propagation without increasing the neural network's parameter count. Zhu et al. \cite{zhu2021zc3} provide a unified optimization framework that interprets the propagation mechanisms of various GNNs, including PPNP/APPNP, as optimal solutions to a common objective function comprising feature fitting and graph Laplacian regularization. This framework allows for the principled design of GNNs with flexible low-pass or high-pass filtering capabilities (e.g., GNN-LF, GNN-HF), which can explicitly alleviate oversmoothing by controlling frequency components.

Further advancing the modeling of complex graph dynamics, Liu et al. \cite{liu2021ee2} introduced \textbf{Elastic Graph Neural Networks (EGNNs)}. Unlike traditional GNNs that rely on $L\_2$-based graph smoothing, which enforces uniform global smoothness, EGNNs incorporate $L\_1$-based graph smoothing. This allows for adaptive local smoothness, enabling the preservation of discontinuities and sharp changes between different graph regions while still smoothing within homogeneous areas. The derived Elastic Message Passing (EMP) scheme, based on a primal-dual optimization algorithm, offers a novel way to balance global and local smoothing, leading to more robust and accurate representations, particularly against adversarial attacks.

A more fundamental redefinition of graph dynamics is presented by \textbf{Fractional-order Graph Neural Networks (FROND)} \cite{kang2024fsk}. Traditional continuous GNNs often model graph dynamics using integer-order differential equations, leading to rapid, exponential convergence of node representations and thus oversmoothing. FROND, however, leverages fractional calculus to replace integer-order derivatives with fractional derivatives. This mathematical shift enables the modeling of non-local, memory-dependent graph dynamics, which are prevalent in real-world processes exhibiting long-range dependencies and non-Markovian behavior. Crucially, the fractional-order dynamics inherently mitigate oversmoothing by promoting a slower, algebraic convergence of node representations, preventing them from becoming indistinguishable too quickly. This approach offers a novel pathway to enhance expressivity, robustness, and generalization for continuous GNNs by fundamentally altering the nature of information propagation.

In conclusion, mitigating oversmoothing and effectively modeling complex graph dynamics are multifaceted challenges in deep GNNs. The field has progressed from architectural heuristics like residual connections and normalization, which provide effective engineering solutions, to more sophisticated approaches. These include decoupling depth and scope \cite{zeng2022jhz}, developing adaptive smoothing mechanisms like Elastic GNNs \cite{liu2021ee2}, and unifying propagation under optimization frameworks \cite{zhu2021zc3}. The introduction of FROND \cite{kang2024fsk} represents a significant theoretical advancement, offering a fundamental re-framing of graph dynamics through fractional calculus. By enabling algebraic convergence and modeling non-local, memory-dependent interactions, FROND addresses oversmoothing at its mathematical core and opens new avenues for capturing complex real-world processes. Future research should continue to explore the interplay between oversmoothing, trainability, and over-squashing, integrating these diverse mitigation strategies and novel mathematical frameworks to build truly deep, expressive, and robust GNNs capable of handling the intricate dynamics of real-world graphs.


\label{sec:advanced_learning_paradigms:_pre-training,_prompting,_and_multi-modality}

\section{Advanced Learning Paradigms: Pre-training, Prompting, and Multi-modality}
\label{sec:advanced\_learning\_paradigms:\_pre-training,\_prompting,\_\_and\_\_multi-modality}

\subsection{Strategies for GNN Pre-training}
\label{sec:5\_1\_strategies\_for\_gnn\_pre-training}

The pervasive challenge of labeled data scarcity in real-world graph-structured datasets has spurred the development of sophisticated pre-training strategies for Graph Neural Networks (GNNs). These strategies aim to learn generalizable node and graph representations from vast amounts of unlabeled data, thereby enhancing transfer learning performance and significantly reducing the reliance on extensive, costly manual annotation for downstream tasks. The evolution of GNN pre-training mirrors advancements in other deep learning domains, moving from simple self-supervised tasks to more complex contrastive, generative, and meta-learning approaches designed to capture intrinsic graph properties and improve transferability.

Early foundational work established a systematic approach to learning transferable representations by leveraging self-supervision. \cite{hu2019r47} pioneered a combined node- and graph-level self-supervised pre-training strategy, addressing the common issue of "negative transfer" that arises from ad-hoc methods. Their approach introduced novel node-level tasks, such as Context Prediction, where the GNN learns to predict the structural context of a node's neighborhood, and Attribute Masking, which trains the GNN to infer masked node or edge attributes. By integrating these predictive tasks with regularized graph-level pre-training, this method demonstrated significant improvements in generalization, particularly for molecular and protein prediction, laying crucial groundwork for subsequent developments in self-supervised GNNs. These generative tasks inherently encourage the GNN to understand local graph topology and feature dependencies.

A significant paradigm shift in self-supervised GNN pre-training emerged with contrastive learning, which aims to maximize the agreement between different "views" of the same graph or node while pushing apart representations of dissimilar entities. \cite{xie2021n52} provides a unified review, categorizing these methods and grounding contrastive learning in the principle of mutual information maximization. This perspective highlights how various methods differ primarily in their strategies for generating positive and negative views and their choice of mutual information estimators. One of the earliest and most influential contrastive methods is Deep Graph Infomax (DGI), which maximizes the mutual information between node embeddings and a global graph summary. DGI learns node representations by distinguishing positive (node-summary pairs from the same graph) from negative (node-summary pairs from different graphs or shuffled nodes) examples, thereby encouraging the GNN to capture information relevant to the entire graph context. Following DGI, augmentation-based contrastive learning became prominent. Methods like Graph Contrastive Learning (GraphCL) and GRACE generate two distinct augmented views of a graph (e.g., by randomly dropping edges, masking features, or perturbing node attributes) and then train the GNN to maximize the similarity between the representations of these two views of the \textit{same} graph instance, while minimizing similarity to views from \textit{other} graph instances. This approach forces the GNN to learn representations that are robust to structural and feature perturbations, capturing essential, invariant properties of the graph.

While effective, traditional contrastive methods often rely on carefully constructed negative samples, which can be computationally costly or challenging to define optimally for complex graph structures. To address this, \cite{zhang20211dl} introduced Canonical Correlation Analysis-based Self-Supervised Graph Neural Networks (CCA-SSG). This conceptually simple yet effective model generates two views of an input graph through data augmentation, similar to other contrastive methods. However, instead of instance-level discrimination with negative samples, CCA-SSG optimizes an innovative feature-level objective inspired by classical Canonical Correlation Analysis. This objective essentially aims to discard augmentation-variant information by learning invariant representations and prevents degenerated solutions by decorrelating features in different dimensions. Their theoretical analysis further connects this objective to the Information Bottleneck Principle, offering a principled way to learn robust representations without the complexities of negative sampling.

More recently, a resurgence of generative pre-training has been observed, particularly inspired by Masked Autoencoders (MAE) in computer vision. This paradigm, often termed Masked Graph Modeling (MGM), involves masking a portion of the input graph (e.g., node features, edges, or even entire subgraphs) and training the GNN to reconstruct the masked elements. For example, \cite{sun2022d18} utilizes a masked edge prediction task during pre-training, where the model learns to predict missing edges, thereby implicitly capturing structural regularities. This approach, exemplified by methods like GraphMAE (not explicitly in provided papers but representative of the paradigm), offers advantages by avoiding the need for negative samples inherent in contrastive learning and can be particularly effective in capturing fine-grained local graph information and feature dependencies.

Critically comparing these paradigms, contrastive learning methods often excel at learning discriminative embeddings, which are highly effective for tasks like node or graph classification where distinguishing between entities is paramount. However, their performance can be sensitive to the quality of data augmentations and the computational cost and complexity of negative sampling. Generative methods, including early predictive tasks and the more recent MGM, tend to capture richer local structural and feature information by forcing the model to understand dependencies for reconstruction. They inherently avoid the negative sampling problem, which can simplify training and improve stability. The choice between these often depends on the downstream task's requirements: discriminative power for classification versus fine-grained structural understanding for tasks like link prediction or property prediction.

Beyond the core pre-training tasks, a significant challenge in GNN transfer learning is the "inherent training objective gap" between the pre-training pretext task and the downstream task, which can lead to limited or even negative transfer (\cite{sun2022d18}, \cite{huang2024tdd}). To bridge this gap, novel strategies have emerged. \cite{sun2022d18} proposes Graph Pre-training and Prompt Tuning (GPPT), which reformulates the downstream task to mimic the structure of the pre-training task (e.g., node classification as an edge prediction-like task involving "token pairs" of candidate labels and node entities). This allows pre-trained GNNs to be applied without tedious fine-tuning, significantly accelerating convergence and improving few-shot performance. Similarly, \cite{lu20213kr} introduces L2P-GNN (Learning to Pre-train GNNs), a meta-learning approach that explicitly optimizes for the model's ability to quickly adapt to new tasks during the pre-training phase, inspired by MAML. L2P-GNN employs a dual adaptation mechanism for self-supervised learning of both node-level (via link prediction) and graph-level (via contrastive learning on sub-structures) representations, ensuring the learned parameters are inherently optimized for rapid fine-tuning. Further addressing this, \cite{huang2024tdd} introduces "task consistency" to quantify the similarity between pre-training and downstream tasks and proposes "Bridge-Tune," an intermediate fine-tuning step designed to diminish the impact of task differences.

The challenge of heterogeneity in real-world graphs also necessitates specialized pre-training strategies. Traditional GNNs often struggle with heterogeneous information networks (HINs) due to their diverse node and edge types, which can lead to redundancy and over-reliance on initial features. \cite{wei20246l2} explores self-supervised GNNs specifically for HINs, proposing a framework that flexibly combines different types of additional information from the attribute graph and its nodes. While the specific self-supervisory mechanism is described at a high level, the work aims to improve the adaptability of GNNs to the diversity and complexity of HINs, enabling better extraction of deep features and improving overall model performance in such intricate graph structures by learning robust, type-aware representations.

In summary, GNN pre-training has evolved from basic generative tasks that predict local attributes and contexts to sophisticated contrastive learning paradigms that leverage mutual information maximization and invariant learning, and a renewed interest in masked graph modeling. Crucially, recent advancements have focused on bridging the objective gap between pre-training and downstream tasks through prompt learning and meta-learning, significantly enhancing transferability. These strategies, whether predictive, contrastive, or meta-learning based, are fundamental for learning robust, transferable representations from unlabeled data, thereby mitigating the labeled data bottleneck and enabling GNNs to achieve strong performance on a wide array of downstream tasks with limited supervision. Future advancements are expected to focus on developing more universally applicable pre-training objectives, improving augmentation strategies, and extending these methods to even more complex and heterogeneous graph structures, while also considering the computational efficiency and theoretical guarantees of transferability.
\subsection{Graph Prompt Learning for Efficient Adaptation}
\label{sec:5\_2\_graph\_prompt\_learning\_for\_efficient\_adaptation}

The remarkable capabilities of Graph Neural Networks (GNNs) in learning from graph-structured data are often hampered by the practical challenge of efficiently adapting pre-trained models to new, often data-scarce, downstream tasks. Traditional transfer learning, typically involving full fine-tuning of the entire GNN, is computationally demanding, requires substantial labeled data for each new task, and frequently suffers from a significant "objective gap" between the pre-training pretext task and the specific downstream objective \cite{hu2019r47}. This inefficiency and task-specificity have spurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods for GNNs, with graph prompt learning emerging as a particularly potent paradigm, drawing inspiration from its transformative success in Natural Language Processing (NLP). While other PEFT methods like adapter modules (e.g., inserting small, learnable neural networks between GNN layers) also exist for graphs, prompt learning distinguishes itself by modifying the input or output interface rather than the internal architecture.

Early foundational work by \cite{hu2019r47} systematically investigated the "pre-train, fine-tune" paradigm for GNNs, proposing self-supervised strategies at both node and graph levels (e.g., Context Prediction, Attribute Masking) to learn robust representations from unlabeled data. While these methods significantly improved generalization and accelerated convergence during fine-tuning, they still necessitated updating all or most model parameters. This approach remained inefficient and susceptible to objective misalignment, especially when the downstream task's nature diverged significantly from the pre-training objective, motivating the search for more efficient adaptation strategies.

The concept of "prompt tuning" was explicitly introduced to GNNs by \cite{sun2022d18} with their Graph Pre-training and Prompt Tuning (GPPT) framework. Recognizing the "inherent training objective gap" as a core bottleneck, GPPT proposed to reformulate downstream node classification tasks to mimic the pre-training objective, such as masked edge prediction. This was achieved by employing a novel "graph prompting function" and "token pairs" that effectively transformed the downstream task's input to align with the pre-trained model's expected input format and objective. By doing so, GPPT facilitated knowledge transfer without extensive fine-tuning, accelerating convergence and improving performance in few-shot settings. However, a critical limitation of GPPT was its task-specific nature; its prompting mechanism was often tailored to particular pre-training tasks (e.g., edge prediction) and lacked universality across diverse GNN pre-training strategies, restricting its broad applicability.

To address this universality challenge, \cite{fang2022tjj} introduced a more general and theoretically grounded approach with Graph Prompt Feature (GPF). Instead of structural modification or task reformulation, GPF shifted the prompt application to direct manipulation of the \textit{input graph's feature space}. This innovation involved learning a small set of universal prompt vectors that are either concatenated or, more commonly, \textit{added} to the original node features ($\mathbf{x}\_i' = \mathbf{x}\_i + \mathbf{p}$), effectively creating learnable "virtual features" to steer the pre-trained GNN towards the downstream task without altering its core architecture. GPF was the first universal prompt-based tuning method for GNNs, demonstrating compatibility with virtually any pre-trained GNN and pre-training strategy. Crucially, it was accompanied by rigorous theoretical derivations demonstrating that GPF can achieve an equivalent effect to \textit{any} prompting function, often enabling it to surpass the performance of full fine-tuning, particularly in low-data regimes. Despite its universality and theoretical backing, the optimal design and dimensionality of these feature-space prompts remain largely heuristic, and their lack of structural grounding can make them difficult to interpret.

Building upon the success of universal prompt tuning, \cite{liu2023ent} aimed for broader universality across \textit{task types}, not just pre-training strategies. Their GraphPrompt framework introduced a novel prompting mechanism that modifies the GNN's \texttt{ReadOut} aggregation function, which is typically used to generate graph-level representations from node embeddings. By unifying various pre-training tasks (e.g., link prediction) and diverse downstream tasks (e.g., node and graph classification) into a common "subgraph similarity" template, GraphPrompt utilizes task-specific learnable prompts to guide the \texttt{ReadOut} operation. This prompt acts as learnable parameters for the aggregation function, allowing adaptive fusion of node representations. This approach allows a single pre-trained model to effectively serve multiple downstream tasks in few-shot settings, further bridging objective inconsistencies by adapting the final aggregation step to the task at hand. However, its effectiveness might be constrained by how well diverse tasks can be framed as subgraph similarity, and modifying only the ReadOut function might not be sufficient for highly complex or structurally sensitive downstream tasks.

Further advancing the paradigm, \cite{sun2023vsl} proposed "All in One: Multi-Task Prompting for Graph Neural Networks," which explicitly unifies the format of graph prompts and language prompts using concepts like prompt tokens and insertion patterns. Their method reformulates various downstream problems to graph-level tasks and employs meta-learning to efficiently learn a better initialization for multi-task prompts. This approach aims to make prompting more reliable and general for different tasks, but the computational overhead of meta-learning and the potential oversimplification of node/edge-level tasks when reframing them as graph-level problems are important considerations.

Marking a significant expansion of the paradigm's scope beyond simple parameter-efficient adaptation, recent work has begun integrating prompt learning with Large Language Models (LLMs) to imbue GNNs with real-world semantic understanding. \cite{li202444f} introduced Morpher, a pioneering paradigm for graph-text multi-modal prompt learning, even under \textit{extremely weak text supervision}. Morpher addresses the complex challenge of aligning independently pre-trained GNNs and LLMs by proposing an improved, stable graph prompt design and a cross-modal projector with contrastive learning. This innovative framework enables GNNs to achieve CLIP-style zero-shot generalization to unseen classes, significantly enhancing their semantic capabilities and opening new avenues for applications requiring rich contextual understanding by leveraging the vast semantic knowledge encoded in LLMs. This development represents a critical step towards more intelligent and semantically aware graph AI systems, a topic explored further in the following subsection.

In conclusion, graph prompt learning has rapidly evolved from a foundational concept for efficient adaptation to a sophisticated paradigm capable of universal application across pre-training strategies and diverse downstream tasks, culminating in multi-modal semantic understanding. This progression, from task-specific structural prompts (GPPT) to universal feature-space prompts (GPF) and unified ReadOut-based frameworks (GraphPrompt), and finally to multi-modal integration (Morpher), has significantly enhanced GNNs' practical utility. By effectively bridging the objective gap, enabling robust few-shot learning, and reducing the need for extensive labeled data and full fine-tuning, graph prompt learning makes GNNs more adaptable, flexible, and scalable in diverse and data-constrained application contexts. However, challenges remain in developing more robust and interpretable prompt designs, particularly for complex structural tasks, and in addressing the scalability and computational overhead of advanced prompt learning techniques for extremely large and dynamic graphs.
\subsection{Multi-modal GNNs and LLM Integration}
\label{sec:5\_3\_multi-modal\_gnns\_\_and\_\_llm\_integration}

The integration of Graph Neural Networks (GNNs) with Large Language Models (LLMs) represents a pivotal advancement, aiming to imbue GNNs with real-world semantic understanding, enhance their robustness, and overcome data scarcity challenges. This convergence leverages the rich semantic knowledge encoded in LLMs with the structural reasoning capabilities of GNNs, enabling new paradigms for generalization and application. This section explores the diverse ways LLMs are being integrated into GNN frameworks, moving from feature enrichment to structural enhancement and, most significantly, multi-modal semantic alignment for zero-shot generalization.

One fundamental paradigm involves leveraging LLMs to enrich node features or assist in graph construction, thereby grounding GNNs in semantically rich textual information. In text classification tasks, for instance, GNNs often rely on initial node representations derived from contextualized word embeddings generated by advanced language models like BERT \cite{wang2023wrg}. This approach allows GNNs to process text-rich graphs by translating complex textual semantics into dense vector representations, which the GNN then processes structurally. This is crucial for understanding document-level or corpus-level relationships, where the meaning of nodes (words or documents) is deeply intertwined with their linguistic context. By using LLM-generated features, GNNs can capture more nuanced semantic relationships that might be missed by simpler word embeddings or bag-of-words models, thereby improving their performance on tasks like sentiment analysis or topic classification \cite{wang2023wrg}.

Another emerging paradigm focuses on utilizing LLMs for graph structure enhancement and robustness, addressing the inherent vulnerabilities of GNNs to topology perturbations. GNNs are known to be susceptible to adversarial attacks that modify graph edges, leading to significant performance degradation. \cite{zhang2024370} explored whether LLMs could improve GNN robustness through their framework, LLM4RGNN. This approach proposes an LLM-based robust graph structure inference mechanism that distills the powerful inference capabilities of a large LLM (e.g., GPT-4) into a local LLM. This local LLM is then used to identify malicious edges introduced by adversaries and to predict missing important edges, thereby recovering a more robust graph structure. While empirical results from LLM4RGNN demonstrate that LLMs can indeed improve robustness, the study also highlights that GNNs still exhibit significant vulnerability, with accuracy decreases averaging 23.1\\% under topology attacks \cite{zhang2024370}. This suggests that while LLMs offer promising avenues for intelligent structural inference and repair, the computational overhead of distilling LLM capabilities and the inherent challenges of achieving comprehensive adversarial robustness in graphs remain substantial. The complexity of graph adversarial attacks, which can subtly alter topology to mislead GNNs, requires more sophisticated and computationally efficient LLM-driven defense mechanisms.

The most direct and impactful leap towards semantic understanding and multi-modal integration, particularly for zero-shot generalization, is presented by \cite{li202444f} with their Morpher paradigm. This work directly addresses the GNNs' historical lack of real-world semantic understanding and the challenge of aligning independently pre-trained graph and text models, especially under conditions of extremely weak text supervision. Morpher introduces a multi-modal prompt learning framework that aligns graph and text embeddings using cross-modal projectors and contrastive learning. A key innovation lies in its improved, stable graph prompt design, which overcomes the instability issues of prior graph prompting methods by balancing cross-connections between prompt tokens and input graph nodes with the original graph's inner-connections. This ensures that prompt features augment rather than overwhelm the original graph information, facilitating robust cross-modal alignment. By leveraging the rich semantic knowledge encoded in LLMs, Morpher enables GNNs to achieve CLIP-style zero-shot generalization, allowing them to effectively process and reason about unseen classes and concepts without explicit training data. This capability significantly expands GNN applications, addressing data scarcity challenges and opening new avenues for GNNs to operate in semantically rich and dynamic environments where new categories frequently emerge.

However, the Morpher paradigm, while groundbreaking, also presents certain limitations and challenges that warrant critical examination. Its effectiveness is highly contingent on the quality and pre-training objectives of the independently pre-trained graph and text models. The cross-modal alignment process itself introduces considerable computational overhead and complexity, particularly when scaling to very large graphs and integrating with extensive LLMs. Furthermore, the sensitivity of contrastive learning to negative sampling strategies can impact performance, and achieving robust semantic alignment under extremely weak text supervision remains a non-trivial task. This weak supervision, while a strength in terms of data efficiency, can also lead to semantic drift or misalignment if the textual cues are too sparse or ambiguous. Scalability of such multi-modal GNN-LLM systems to web-scale graphs, a challenge for GNNs generally, becomes even more pronounced with the added complexity of LLM integration and cross-modal alignment.

In conclusion, the integration of LLMs with GNNs represents a profound evolution in graph machine learning, moving beyond purely structural reasoning to incorporate rich semantic understanding. The diverse paradigms, from LLM-driven feature enrichment and structural robustness enhancement to advanced multi-modal semantic alignment like Morpher, collectively push the boundaries of GNN capabilities. While Morpher offers a compelling vision for CLIP-style zero-shot generalization, future research must critically examine the computational costs and efficiency of these complex systems, especially for large-scale graphs. Further investigation is needed into the robustness of cross-modal alignment under noisy or sparse data conditions, and the interpretability of decisions made by these integrated models. Exploring hybrid architectures where LLMs act as intelligent reasoners or knowledge providers, dynamically interacting with GNNs that provide structural context, and extending these paradigms to incorporate other modalities beyond text (e.g., vision, audio) will be crucial for developing truly comprehensive, intelligent, and adaptable graph-aware AI systems.


\label{sec:applications_and_real-world_impact_of_gnns}

\section{Applications and Real-World Impact of GNNs}
\label{sec:applications\_\_and\_\_real-world\_impact\_of\_gnns}

\subsection{Recommender Systems}
\label{sec:6\_1\_recommender\_systems}

Recommender systems are indispensable components of modern digital ecosystems, tasked with alleviating information overload by predicting user preferences and suggesting relevant items. Graph Neural Networks (GNNs) have emerged as a transformative paradigm in this domain, leveraging their inherent ability to model complex relational data. They excel at capturing intricate user-item interactions, as well as integrating diverse auxiliary information such as social networks, explicit opinions, and item attributes \cite{wu2020dc8, gao2022f3h}. By propagating information across these rich, multi-faceted graph structures, GNNs learn sophisticated latent factors, leading to significantly more accurate, personalized, and increasingly explainable recommendations \cite{he202455s}.

One of the earliest and most critical challenges for GNNs in recommendation was scaling to the massive datasets encountered in industrial applications. Addressing this, \cite{ying20189jc} introduced PinSage, a pioneering framework that demonstrated the viability of GNNs for web-scale recommender systems. While the detailed architectural innovations of PinSage, such as efficient neighborhood sampling and a producer-consumer architecture, are discussed in Section 4.1, its impact on recommender systems was profound. PinSage proved that GNNs could effectively learn high-quality item embeddings from billions of user interactions, leading to substantial improvements in user engagement and driving commercial success at Pinterest. Its success underscored the potential of GNNs to move beyond academic benchmarks to real-world deployment.

Building upon the foundation of scalable GNNs, subsequent research focused on enriching user and item representations by integrating more diverse auxiliary information. \cite{fan2019k6u} proposed GraphRec, a comprehensive GNN framework tailored for social recommendation. GraphRec innovatively models both user-user social graphs and user-item interaction graphs, learning distinct user latent factors from "item-space" and "social-space" perspectives. A key contribution was its use of "opinion embedding vectors" to jointly capture user-item interactions and their associated explicit opinions (e.g., rating scores), moving beyond binary interaction signals. Furthermore, GraphRec employed multiple attention mechanisms to dynamically weigh the contributions of different neighbors and interaction types, leading to more nuanced user modeling and enhanced personalized recommendations by leveraging the principle of social influence \cite{sharma2022liz}.

Beyond social networks, knowledge graphs (KGs) have proven to be another powerful source of auxiliary information for GNN-based recommenders. KGs, which represent entities and their relationships, can significantly alleviate data sparsity and cold-start problems by providing rich semantic context for items and users \cite{he202455s, ye20226hn}. GNNs are particularly adept at traversing and embedding these multi-relational graphs, allowing them to infer complex connections between items and users that are not explicit in interaction data. For instance, models can leverage GNNs to propagate information about item attributes (e.g., genre, director for movies) or hierarchical categories, leading to more informed recommendations, especially for less popular items or new users.

Another significant area where GNNs have excelled is session-based recommendation (SBR), which aims to predict the next item a user will interact with based on their current anonymous session. Traditional sequential models often struggle to capture the complex, non-linear transitions between items within a session. \cite{wu2018t43} introduced SR-GNN, one of the first works to model session sequences as graph-structured data, allowing GNNs to capture intricate item transitions that are difficult for conventional sequential methods. Extending this, \cite{wang2020khd} proposed Global Context Enhanced Graph Neural Networks (GCE-GNN), which learns item embeddings from both session-level graphs (modeling pairwise transitions within a session) and a global graph (modeling transitions across all sessions). By aggregating these two levels of representations with attention mechanisms, GCE-GNN provides a more comprehensive understanding of user preferences, demonstrating the power of GNNs in capturing both local and global sequential patterns.

As GNNs become more prevalent in high-stakes applications like recommender systems, addressing trustworthiness aspects such as fairness and explainability becomes paramount. GNNs, like other machine learning models, can inherit and even amplify biases present in historical interaction data or graph structures, leading to discriminatory recommendations \cite{dai2020p5t}. Research in fair GNNs, such as REDRESS \cite{dong202183w} and FairVGNN \cite{wang2022531}, focuses on mitigating these biases, for instance, by promoting individual fairness or preventing sensitive attribute leakage during feature propagation. While the detailed methodologies for ensuring fairness in GNNs are explored in Section 7.2, their application to recommender systems is crucial for ethical deployment. Furthermore, the initial claim regarding "explainable recommendations" is being actively addressed. For example, \cite{lyu2023ao0} proposed Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation, which leverages semantic knowledge from external knowledge bases to learn comprehensive user/item representations and generate human-like explanations, thereby making the recommendation process more transparent and trustworthy.

In conclusion, GNNs have profoundly transformed recommender systems by offering a powerful framework to model complex user-item interaction graphs and integrate diverse auxiliary information. The field has progressed from pioneering efforts in scaling GNNs for industrial deployment \cite{ying20189jc} and enriching representations with social \cite{fan2019k6u} and knowledge graph data, to effectively modeling dynamic user behaviors in session-based contexts \cite{wu2018t43, wang2020khd}. Crucially, the community is also addressing the ethical implications, developing methods for fair \cite{dong202183w, dai2020p5t, wang2022531} and explainable recommendations \cite{lyu2023ao0}. While challenges such as structural disparity (as discussed in Section 4.2), cold-start problems, and handling highly dynamic graphs persist, the continuous innovation in GNN architectures promises even more accurate, personalized, and responsible recommendation engines in the future.
\subsection{Multivariate Time Series Analysis}
\label{sec:6\_2\_multivariate\_time\_series\_analysis}

Multivariate time series analysis is a critical domain across various fields, including smart cities, environmental monitoring, finance, and healthcare, where accurately modeling intricate spatial-temporal dependencies among numerous interconnected variables is paramount \cite{jin2023ijy, sahili2023f2x}. Traditional methods, such as ARIMA, VAR, or even deep learning models like CNNs and RNNs, often struggle by treating variables independently or relying on implicit assumptions about their relationships. This limitation leads to suboptimal performance, particularly when the underlying system exhibits complex, non-Euclidean spatial structures \cite{jin2023ijy, jin2023e18}. Graph Neural Networks (GNNs) have emerged as a powerful paradigm to overcome these shortcomings, explicitly modeling the non-Euclidean spatial relationships among time series variables, such as sensors in a network, traffic flow between locations, or interactions in a biological system \cite{sahili2023f2x}. This explicit modeling offers significant advantages over conventional approaches, enabling a deeper understanding and more accurate analysis of complex, interconnected dynamics.

A fundamental and often challenging aspect of applying GNNs to multivariate time series is the construction of an appropriate graph structure that accurately reflects the underlying inter-variable dependencies. The evolution of graph construction techniques for time series data has progressed significantly:
\begin{enumerate}
    \item \textbf{Heuristic-based Graphs:} Early approaches frequently relied on readily available or simple heuristic-based graphs. These often derive from geographical proximity (e.g., Gaussian kernels based on distance), statistical similarities (e.g., Pearson correlation, Dynamic Time Warping (DTW), cosine similarity), or causal relationships (e.g., Granger causality, transfer entropy) \cite{jin2023ijy, jin2023e18}. While straightforward, these methods can be limited by their reliance on predefined rules, potentially missing latent or dynamic dependencies.
    \item \textbf{Learning-based Graphs:} The field has rapidly progressed towards more sophisticated learning-based methods that infer optimal graph structures directly from data, especially when explicit dependencies are unknown, highly dynamic, or too complex to be captured by simple heuristics \cite{jin2023ijy}. These approaches often leverage techniques such as:
    \begin{itemize}
        \item \textbf{Embedding-based methods:} Nodes (time series variables) are first embedded into a latent space, and then graph edges are inferred based on the similarity or distance between these embeddings.
        \item \textbf{Attention mechanisms:} Self-attention layers can learn dynamic connectivity weights between all pairs of nodes, effectively constructing a soft adjacency matrix that adapts to the input data.
        \item \textbf{Learnable adjacency matrices:} Some models directly optimize a learnable adjacency matrix, often initialized randomly or with a simple heuristic, and refined during training alongside the GNN parameters. For instance, \cite{wu2020hi3} proposed a general GNN framework for multivariate time series forecasting that includes a novel graph learning module. This module automatically extracts uni-directed relations among variables, allowing for the integration of external knowledge and addressing scenarios where dependencies are not known in advance.
        \item \textbf{Generative models:} More advanced techniques might employ generative models to infer graph structures that best explain the observed time series data.
    \end{itemize}
    Such learning-based approaches make GNNs highly adaptable to diverse and complex real-world systems by capturing implicit and dynamic relationships \cite{jin2023e18}.
\end{enumerate}

The integration of GNNs with temporal dynamics has led to the formalization of Spatio-Temporal Graph Neural Networks (STGNNs) or Temporal GNNs (TGNNs), specifically designed to handle dynamic graph structures and evolving relationships over time \cite{sahili2023f2x, longa202399q}. These models are crucial for capturing both static spatial relationships and their temporal evolution, making them suitable for a wide array of multivariate time series tasks including forecasting, classification, imputation, and anomaly detection \cite{jin2023ijy}.

\textbf{Pioneering STGNN Architectures:} Early influential STGNN models laid the groundwork by combining graph convolutions with various temporal modeling techniques:
\begin{itemize}
    \item \textbf{Spatio-Temporal Graph Convolutional Networks (STGCN)} \cite{yu2018spatio}: One of the earliest and most influential models, STGCN integrates graph convolutions with 1D convolutional neural networks (CNNs) to simultaneously capture spatial and temporal dependencies. While effective, STGCN often relies on a predefined or static graph, limiting its ability to adapt to evolving relationships.
    \item \textbf{Diffusion Convolutional Recurrent Neural Network (DCRNN)} \cite{li2018diffusion}: DCRNN leverages diffusion convolution to model spatial dependencies, capturing information propagation on the graph based on random walks. This is then integrated into a sequence-to-sequence recurrent neural network (RNN) architecture to handle temporal dynamics. DCRNN excels at modeling flow-like data but can suffer from the inherent limitations of RNNs, such as difficulties with long-range temporal dependencies and parallelization.
    \item \textbf{Graph WaveNet} \cite{wu2019graph}: This architecture enhances STGNNs by introducing an adaptive adjacency matrix, which is learned directly from the data, allowing it to capture implicit spatial dependencies beyond predefined graph structures. It combines this with dilated causal convolutions to model long-range temporal patterns, mitigating issues like vanishing gradients in deep temporal models. Graph WaveNet addresses the static graph limitation of STGCN and the long-range dependency issues of DCRNN, but its adaptive matrix can increase computational complexity.
\end{itemize}

\textbf{Recent Advancements: Transformer-based STGNNs:} While the pioneering models demonstrated significant improvements, the field has seen a shift towards Transformer-based architectures to better capture long-range temporal dependencies and dynamic spatial relationships. These models leverage self-attention mechanisms, which allow them to weigh the importance of different time steps and spatial neighbors dynamically, overcoming the limitations of fixed-size receptive fields in CNNs and sequential processing in RNNs. For instance, models integrate spatial graph attention with temporal self-attention, enabling more powerful and parallelizable learning of complex spatio-temporal patterns. An example in traffic prediction is the Dual Cross-Scale Transformer (DCST), which explicitly preserves "topology-free patterns" alongside graph-regularized ones, demonstrating the growing recognition that GNNs alone might not capture all relevant dynamics \cite{zhou2024t2r}. These Transformer-based STGNNs often achieve superior performance on many benchmarks, particularly for tasks requiring a global understanding of spatio-temporal dynamics and mitigating issues like over-squashing in deep graph structures.

The practical impact of GNNs in multivariate time series analysis is exemplified by real-world deployments. For instance, Google Maps has successfully deployed a GNN-based estimator for Estimated Time of Arrival (ETA) prediction \cite{derrowpinion2021mwn}. This system models road networks using "supersegments" and leverages sophisticated featurization combining real-time and historical traffic data with learnable embeddings. Crucially, it employs robust training regimes, including MetaGradients for dynamic learning rate tuning and semi-supervised methods like graph auto-encoders, to ensure stability and performance in a large-scale production environment. This deployment showcases the ability of GNNs to provide significant improvements over traditional methods, such as a 40\\% reduction in negative ETA outcomes in cities like Sydney, highlighting their efficacy and scalability in critical applications \cite{derrowpinion2021mwn}. Beyond traffic, GNNs are also being applied to complex systems like global weather forecasting, demonstrating comparable skill to operational physical models \cite{keisler2022t7p}.

Despite significant progress, the field continues to evolve, facing ongoing challenges that are specific to STGNNs.
\begin{enumerate}
    \item \textbf{Scalability:} While GNNs offer powerful modeling capabilities, their application to extremely large time series datasets with massive, dynamic graphs and high update rates remains a key concern \cite{sahili2023f2x, cini2022pjy}. The computational complexity often scales quadratically with sequence length and graph size, hindering real-time applications. Efforts to address this include randomized RNNs and pre-computation techniques to improve efficiency \cite{cini2022pjy}, but handling massive, evolving graphs efficiently is still an active research area.
    \item \textbf{Interpretability:} Interpreting the decisions of STGNNs, especially those with dynamically learned graph structures or complex attention mechanisms, is crucial for trustworthy deployment \cite{jin2023ijy, longa202399q}. Understanding \textit{why} a particular spatio-temporal dependency was learned or \textit{how} it influenced a prediction is challenging, particularly when the underlying graph itself is a latent variable.
    \item \textbf{Robustness to Imperfect Data and Distribution Shifts:} Real-world time series data is often noisy, incomplete, or subject to sudden changes. STGNNs need to be robust to missing values, sensor failures, and, more critically, to \textit{distribution shifts} in both temporal patterns and the underlying graph structure over long time horizons. A learned graph that is optimal for one period might become suboptimal or misleading in another.
    \item \textbf{Over-squashing:} In deep STGNNs, repeated message passing can lead to node representations becoming indistinguishable, a phenomenon known as over-squashing. This limits the effective receptive field and the ability to capture long-range spatio-temporal dependencies, which are often critical in multivariate time series.
\end{enumerate}
Future research directions will likely focus on developing more adaptive and efficient graph learning mechanisms that can handle dynamic graph topologies and attribute changes, enhancing the explainability of STGNN predictions, improving robustness to various data imperfections and distribution shifts, and integrating multi-modal data sources to further enrich the understanding of complex multivariate time series dynamics \cite{jin2023ijy, longa202399q, jin2023e18}.
\subsection{Scientific Discovery: Materials Science and Epidemic Modeling}
\label{sec:6\_3\_scientific\_discovery:\_materials\_science\_\_and\_\_epidemic\_modeling}

Graph Neural Networks (GNNs) are profoundly transforming scientific discovery by enabling the analysis of complex relational data across diverse domains, particularly in materials science, chemistry, and epidemic modeling. Their ability to model intricate interactions and inherent symmetries provides powerful tools for accelerating research and informing critical interventions, moving beyond traditional methods that often relied on laborious experimentation or oversimplified assumptions.

In materials science and chemistry, GNNs have emerged as a cornerstone for predicting molecular properties, designing novel materials, and simulating chemical reactions. The inherent graph structure of molecules and crystals, where atoms are nodes and bonds are edges, makes GNNs a natural fit. A key advantage of GNNs in this domain is their capacity to leverage geometric equivariance, respecting fundamental physical symmetries such as rotations and translations, which are crucial for accurately describing 3D molecular and material structures. Pioneering work like E(n) Equivariant Graph Neural Networks (EGNNs) \cite{satorras2021pzl} and Neural Equivariant Interatomic Potentials (NequIP) \cite{batzner2021t07} exemplify this, by directly incorporating geometric information and symmetry constraints into their architectures. NequIP, for instance, has demonstrated state-of-the-art accuracy in learning interatomic potentials from \textit{ab-initio} calculations, achieving this with up to three orders of magnitude fewer training data than existing models. This remarkable data efficiency challenges the notion that deep neural networks always require massive datasets, enabling high-fidelity molecular dynamics simulations over extended timescales \cite{batzner2021t07}.

The impact of GNNs extends to accelerating specific discovery processes. For instance, in catalyst discovery, GNNs have been employed to significantly speed up the calculation of transition state energies, a critical bottleneck in understanding reaction mechanisms. The CatTSunami framework, leveraging pretrained GNNs, achieved a 28x speedup in finding transition states energetically similar to density functional theory (DFT) calculations. More impressively, for enumerating complete reaction networks, it demonstrated a staggering 1500x speedup, enabling the replication of complex ammonia synthesis activity volcanoes in just days, a task that would otherwise take decades of DFT computation \cite{wander2024nnn}. Beyond atomic-level interactions, GNNs are also proving adept at predicting macroscopic mechanical behaviors. By mapping material meshes to graphs, GNNs can predict stress, strain, and deformation fields in various material systems, including composites and metamaterials. These models can capture complex nonlinear phenomena like plasticity and buckling instability, effectively learning the physical relationships between material microstructure, base properties, and boundary conditions \cite{maurizi202293p}. This capability offers a powerful surrogate modeling approach, accelerating the design and optimization of materials with desired mechanical properties. The integration of GNNs with Large Language Models (LLMs) further enhances their predictive power, with Hybrid-LLM-GNNs showing promise for enhanced materials property prediction by leveraging semantic understanding alongside structural information \cite{li2024gue}. While GNNs offer significant advantages, challenges remain, including high data requirements for certain applications and the need for consistent benchmarking to guide development, as highlighted by platforms like MatDeepLearn \cite{fung20212kw}.

Beyond materials, GNNs play a crucial role in epidemic modeling, where they capture complex relational data, such as contact networks, mobility patterns, and geographical proximity, for tasks like disease detection, surveillance, and prediction. Traditional mechanistic epidemic models often suffer from oversimplified assumptions, while general deep learning models like CNNs and RNNs struggle to explicitly incorporate the crucial relational data that drives disease spread. GNNs bridge this gap by offering powerful tools for public health interventions and understanding disease dynamics \cite{liu20242g6}. For instance, during the COVID-19 pandemic, GNNs have been instrumental in spatio-temporal forecasting. CausalGNN \cite{wang202201n} introduced a framework that explicitly incorporates causal mechanisms into GNNs, guiding the learning of graph embeddings and combining graph features with epidemiological context. This approach, using an attention-based dynamic GNN module, has demonstrated superior performance in forecasting daily new cases of COVID-19 at global, US state, and US county levels compared to a broad range of baselines, providing more robust and accurate predictions for public health decision-making \cite{wang202201n}.

For GNNs to be truly effective and trustworthy in public health and scientific discovery, their robustness, interpretability, and generalization capabilities are paramount. In dynamic scenarios like epidemics, where contact networks or mobility patterns can change rapidly, GNNs must be robust to "structure shift." Methodologies like the Cluster Information Transfer (CIT) mechanism \cite{xia20247w9} are crucial for learning invariant representations, thereby improving Out-of-Distribution (OOD) generalization, which is vital for models deployed in evolving real-world conditions. Furthermore, understanding complex disease dynamics often requires modeling non-local, memory-dependent processes, which traditional GNNs can struggle with due to oversmoothing. Frameworks like FROND \cite{kang2024fsk} leverage fractional calculus in continuous GNNs to inherently mitigate oversmoothing and more accurately represent intricate disease progression, offering a fundamental new way to enhance expressivity for such complex phenomena. Finally, for GNNs to inform actionable scientific and public health policy, interpretability is critical. While general explainability frameworks are discussed elsewhere, their application here is to provide insights into \textit{why} a GNN predicts certain material properties or \textit{which} factors drive disease spread, thereby building trust and enabling targeted interventions \cite{chen2024woq, bui2024zy9}. Rigorous benchmarking, as advocated by community-standard frameworks \cite{dwivedi20239ab}, is essential for validating these high-stakes models.

In conclusion, GNNs are rapidly advancing interdisciplinary scientific research by providing sophisticated tools for modeling and understanding complex systems. From leveraging geometric equivariance for precise material design and accelerating catalyst discovery to capturing intricate relational dynamics for robust epidemic prediction, GNNs are enabling discoveries and insights previously unattainable. The field continues to push boundaries in expressivity, robustness, and interpretability, with future directions involving scaling these models to even larger, dynamic graphs and integrating them with other advanced AI paradigms to unlock deeper insights and accelerate scientific progress.
\subsection{Link Prediction and Graph Completion}
\label{sec:6\_4\_link\_prediction\_\_and\_\_graph\_completion}

Link prediction, a cornerstone task in graph analysis, involves inferring missing connections or predicting future interactions within a graph. Its significance spans diverse applications, from completing knowledge graphs and discovering drug interactions to analyzing social networks and powering recommender systems. Historically, approaches relied on predefined heuristics (e.g., common neighbors, Adamic-Adar) or matrix factorization methods. However, these methods often struggle to capture the complex, non-linear structural patterns inherent in real-world graphs, motivating the advent of Graph Neural Networks (GNNs) in this domain.

A foundational shift occurred with the introduction of the SEAL (Subgraph Embedding for Link prediction) framework by \cite{zhang2018kdl}. This work demonstrated GNNs' superior capability over traditional methods by learning general graph structure features directly from local enclosing subgraphs. Crucially, \cite{zhang2018kdl} provided a novel theoretical justification through the $\beta$-decaying heuristic theory, proving that high-order heuristics (like Katz index or rooted PageRank) can be accurately approximated from small $h$-hop enclosing subgraphs. This insight resolved the perceived need for global network information, enabling GNNs to effectively capture complex structural patterns for graph completion by integrating structural node labels, latent embeddings, and explicit attributes within these localized contexts.

While SEAL showcased the power of subgraph-based GNNs, many early GNNs for link prediction adopted a simpler paradigm: learning node embeddings via message passing and then using a simple decoder (e.g., dot product or MLP) on concatenated node embeddings to predict link existence. However, these basic GNNs often struggle to capture the rich structural information (e.g., neighborhood overlap, common neighbors, triangle counts) that is crucial for accurate link prediction, sometimes performing worse than simple heuristic methods \cite{yun2022s4i}. This limitation stems from their inherent architectural constraints, often equivalent to the 1-Weisfeiler-Leman test, which limits their ability to distinguish between certain non-isomorphic graphs and count specific graph motifs.

To address this expressiveness bottleneck and enhance GNNs' ability to leverage structural cues, models like Neo-GNNs \cite{yun2022s4i} were proposed. Neo-GNNs explicitly learn useful structural features from the adjacency matrix and estimate overlapped neighborhoods, effectively generalizing neighborhood overlap-based heuristic methods and handling multi-hop structural patterns. Building on the success of subgraph-based GNNs like SEAL, further advancements aimed to combine their expressivity with the efficiency of full-graph GNNs. \cite{chamberlain2022fym} introduced ELPH (Efficient Link Prediction with Hashing) and BUDDY, which approximate key subgraph components (like distance-based node labels) using "subgraph sketching" techniques (e.g., HyperLogLog, MinHashing). This allows ELPH to achieve the expressive power of subgraph GNNs, capable of counting triangles and distinguishing automorphic nodes, but within a full-graph message passing framework, leading to significantly faster and more scalable link prediction without explicit subgraph extraction. BUDDY further enhances scalability by precomputing these sketches, enabling application to massive datasets that exceed GPU memory.

A particularly critical application of link prediction is Knowledge Graph Completion (KGC), where the goal is to infer missing facts (relations between entities) in multi-relational graphs. GNNs have proven highly effective for KGC, as extensively surveyed by \cite{ye20226hn}. Specialized GNN architectures, such as Relational Graph Convolutional Networks (R-GCNs) and Compositional GNNs (CompGCNs), have been developed to handle the diverse relation types and heterogeneous nature of knowledge graphs, learning embeddings that capture both entity and relation semantics for accurate link prediction.

As real-world graphs are inherently dynamic, the need for GNNs to handle temporal information for link prediction is paramount. For instance, in sequential recommendation, the task of predicting the next item a user will interact with can be framed as a dynamic link prediction problem. \cite{zhang20212ke} proposed Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which models user sequences and dynamic collaborative signals within a unified framework, converting next-item prediction into a link prediction task in an evolving graph. For a broader understanding of how GNNs are adapted for temporal graphs, including temporal link prediction, readers can refer to the comprehensive survey by \cite{longa202399q}.

The reliability of link prediction models is also a significant concern, especially in high-stakes applications. This necessitates rigorous evaluation and uncertainty quantification. \cite{li2023o4c} critically evaluated existing benchmarking practices for GNN-based link prediction, exposing common pitfalls such as underreported baseline performance, inconsistent data splits, and the use of "easy" negative samples that do not reflect real-world challenges. They proposed a standardized benchmarking methodology and introduced the Heuristic Related Sampling Technique (HeaRT) to generate more challenging and realistic negative samples, thereby fostering more robust and reproducible evaluations. Furthermore, to provide statistically guaranteed uncertainty estimates for GNN-based link predictions, \cite{zhao2024g5p} introduced conformalized link prediction, a distribution-free and model-agnostic approach built upon conformal prediction. This method addresses the challenges of applying conformal prediction to dependent graph data and improves efficiency by aligning graph structure with a power-law distribution, ensuring more reliable predictions.

In summary, GNNs have profoundly transformed link prediction and graph completion. The journey began with foundational models like SEAL \cite{zhang2018kdl} demonstrating superiority over traditional heuristics by learning from local subgraphs. Subsequent research focused on enhancing GNNs' ability to capture crucial structural information \cite{yun2022s4i} and developing efficient, expressive subgraph-aware models that avoid explicit subgraph construction \cite{chamberlain2022fym}. The field has also seen significant advancements in specialized applications like Knowledge Graph Completion \cite{ye20226hn} and dynamic link prediction \cite{zhang20212ke}. Concurrently, there has been a critical emphasis on rigorous benchmarking \cite{li2023o4c} and ensuring the trustworthiness of predictions through uncertainty quantification \cite{zhao2024g5p}. Future research will likely continue to push the boundaries of scalability, dynamic modeling, and the integration of multi-modal information to build truly intelligent and reliable graph completion systems.


\label{sec:evaluation,_trustworthiness,_and_future_directions}

\section{Evaluation, Trustworthiness, and Future Directions}
\label{sec:evaluation,\_trustworthiness,\_\_and\_\_future\_directions}

\subsection{Benchmarking and Rigorous Evaluation}
\label{sec:7\_1\_benchmarking\_\_and\_\_rigorous\_evaluation}

\label{sec:benchmarking}

The rapid proliferation of Graph Neural Network (GNN) architectures, while indicative of their immense potential, has concurrently underscored a critical challenge in the field: the pervasive lack of robust, standardized, and reproducible benchmarking methodologies. Without such foundational frameworks, scientific rigor is compromised, fair comparisons become elusive, and the true impact of architectural advancements can be obscured, ultimately hindering reliable progress and community development. This issue mirrors broader challenges in machine learning research regarding reproducibility and the verifiability of reported gains.

Initially, the success of pioneering GNN models, such as the SEAL framework for link prediction \cite{zhang2018kdl}, which demonstrated "unprecedented performance" by learning general graph structure features from local enclosing subgraphs, highlighted the need for rigorous evaluation. However, the very enthusiasm surrounding these early breakthroughs inadvertently led to inconsistent evaluation practices. A significant pitfall in GNN benchmarking, particularly for tasks like link prediction, has been the prevalence of underreported baseline performance, inconsistent data splits, and unrealistic negative sampling strategies \cite{li2023o4c}. These inconsistencies frequently result in inflated performance claims and impede accurate, apples-to-apples comparisons between models. To address this, \cite{li2023o4c} introduced a refined benchmarking methodology for link prediction. Their key innovation, the Heuristic Related Sampling Technique (HeaRT), generates more challenging and realistic negative samples by restricting them to "corruptions" (sharing one node with the positive sample) and selecting hard negatives based on structural heuristics. This approach ensures a more truthful assessment of model capabilities, moving beyond trivial negative samples that can artificially inflate performance metrics.

Beyond task-specific issues, the broader GNN community struggled with general evaluation challenges. Many studies relied on small, non-discriminative datasets (e.g., Cora, Citeseer) that often failed to statistically differentiate complex GNN architectures from simpler baselines. Furthermore, inconsistent experimental settings, including varying parameter budgets, made fair comparisons nearly impossible, making it difficult to attribute performance gains to genuine architectural innovation rather than increased model capacity \cite{dwivedi20239ab}. To rectify this, \cite{dwivedi20239ab} developed a comprehensive, open-source, and modular benchmarking framework for GNNs. This framework introduced 12 medium-scale datasets, encompassing both real-world graphs (e.g., ZINC, OGB-COLLAB) and synthetic graphs specifically designed to test theoretical graph properties (e.g., distinguishing isomorphic graphs, identifying specific substructures like cycles). Crucially, it enforced fixed parameter budgets (e.g., 100k and 500k parameters) for all models, ensuring that performance differences were genuinely attributable to architectural design. This standardized environment has proven instrumental in validating fundamental GNN components, such as the efficacy of Graph Positional Encoding (PE) using Laplacian eigenvectors for improving Message-Passing GCNs on anonymous graphs \cite{dwivedi20239ab}.

The imperative for rigorous evaluation extends beyond predictive accuracy to critical dimensions like robustness and explainability, which are paramount for real-world deployment. In the realm of robustness, a major concern has been the overly optimistic estimates of GNN defense mechanisms due to their evaluation against weak, non-adaptive attacks \cite{mujkanovic20238fi}. Drawing parallels with the computer vision community, \cite{mujkanovic20238fi} introduced a systematic, 6-step methodology for designing strong \textit{adaptive attacks} for GNNs, which account for an adversary's knowledge of the defense. Their comprehensive analysis revealed that most existing GNN defenses offer "no or only marginal improvement" against such adaptive adversaries, highlighting a critical gap in evaluation practices and the urgent need for more robust benchmarks. Similarly, \cite{gosch20237yi} critically re-evaluated adversarial training for GNNs, demonstrating that prior reported robustness gains were often artifacts of biased \textit{transductive} learning settings, where models could memorize the training graph. They advocated for and validated adversarial training in a \textit{fully inductive setting} with \textit{locally constrained attacks} as a more realistic and challenging benchmark for robustness. Addressing the scalability challenge, \cite{geisler2021dcq} developed sparsity-aware first-order optimization attacks (PR-BCD, GR-BCD) and novel surrogate losses (MCE, tanh margin) to enable rigorous robustness evaluation of GNNs on web-scale graphs with millions of nodes, overcoming the prohibitive memory and computational costs of previous methods.

Furthermore, the evaluation of GNNs under imperfect data conditions and for interpretability has also seen dedicated benchmarking efforts. Recognizing the prevalence of label noise in real-world graph data, \cite{wang2024481} introduced NoisyGL, the first comprehensive benchmark for GNNs under label noise. This benchmark provides unified experimental settings and interfaces to enable fair comparisons and deeper analyses of methods designed to handle noisy labels, a crucial aspect of real-world applicability. For explainability, which is vital for trust and transparency in high-stakes applications, \cite{agarwal2022xfp} developed GRAPHXAI, a general-purpose framework, and SHAPEGG EN, a novel synthetic graph generator. SHAPEGG EN creates diverse datasets with \textit{guaranteed reliable ground-truth explanations}, addressing the critical limitation of existing datasets lacking such information. GRAPHXAI integrates this generator with real-world datasets, GNN models, and a suite of tailored evaluation metrics (e.g., Graph Explanation Accuracy, Faithfulness, Stability, Fairness), enabling systematic benchmarking of GNN explainers and revealing significant weaknesses in current methods, particularly on heterophilic graphs and for fairness properties.

In conclusion, the evolution of GNN research has underscored the paramount importance of robust and standardized benchmarking. From refining task-specific evaluations like link prediction \cite{li2023o4c} to establishing comprehensive frameworks for general GNNs \cite{dwivedi20239ab}, and extending to specialized benchmarks for robustness \cite{mujkanovic20238fi, gosch20237yi, geisler2021dcq}, label noise \cite{wang2024481}, and explainability \cite{agarwal2022xfp}, these efforts are collectively fostering a healthier scientific environment. Such rigorous and reproducible frameworks are indispensable for accurately assessing the true impact of architectural innovations, guiding future research and development effectively, and ensuring the credibility and trustworthy deployment of GNNs as the field continues to mature and integrate with other advanced AI paradigms.
\subsection{Trustworthiness: Privacy, Robustness, Fairness, and Explainability}
\label{sec:7\_2\_trustworthiness:\_privacy,\_robustness,\_fairness,\_\_and\_\_explainability}

The responsible deployment of Graph Neural Networks (GNNs) in high-stakes real-world applications hinges critically on their trustworthiness, encompassing dimensions of privacy preservation, robustness against adversarial manipulations, fairness in predictions, and the explainability of their decisions. Ensuring these properties is paramount for integrating GNNs into critical infrastructure and societal systems, addressing the unique challenges posed by graph-structured data. Comprehensive surveys by \cite{dai2022hsi} and \cite{zhang20222g3} provide foundational overviews, systematically categorizing existing methods and highlighting the distinct challenges of privacy leakage, adversarial attacks, inherent biases, and interpretability in graph domains. These works emphasize that the non-Euclidean, interconnected nature of graph data necessitates specialized approaches for trustworthiness, differentiating GNNs from traditional machine learning models. Furthermore, rigorous evaluation of GNNs, as advocated by benchmarking efforts like \cite{dwivedi20239ab}, is crucial for assessing their trustworthiness, enabling fair and reproducible comparisons essential for validating claims of reliability.

\textbf{Privacy Preservation} in GNNs is a critical concern, as graph data often contains sensitive relational information. The unique challenge lies in protecting not only node features but also the intricate graph structure itself. A seminal work by \cite{he2020kz4} introduced and systematically studied "link stealing attacks," demonstrating that an adversary can infer the existence of links in a training graph from the black-box outputs of a GNN model. This vulnerability arises because GNNs' message-passing mechanisms inherently encode structural information into node representations. Beyond link stealing, other privacy threats include membership inference (determining if a specific node was part of the training graph) and attribute inference (recovering sensitive node features) \cite{zhang20222g3}. To counter these threats, research explores several defense paradigms. Differential privacy (DP) can be applied to GNNs by injecting noise during message passing or gradient updates, offering quantifiable privacy guarantees at the cost of some utility \cite{zhang20222g3}. Another promising direction is Federated Learning (FL), which inherently protects data locality by training GNNs on decentralized graph partitions without sharing raw data. Surveys like \cite{liu2022gcg} provide an overview of Federated GNNs (FedGNNs), while recent applications such as \cite{hausleitner2024vw0} demonstrate their utility in privacy-aware disease classification by collaboratively training GNNs on subgraphs of a Protein-Protein Interaction network, integrating human-in-the-loop mechanisms for enhanced transparency.

\textbf{Robustness} is a multifaceted challenge for GNNs, encompassing resilience against adversarial attacks, generalization to out-of-distribution (OOD) data, and mitigation of inherent model limitations like oversmoothing. GNNs are particularly vulnerable to adversarial attacks, which can involve subtle perturbations to node features or graph structure (e.g., adding/deleting edges) to induce misclassifications \cite{dai2022hsi, zhang20222g3}. These attacks can be categorized into poisoning attacks (manipulating training data) and evasion attacks (manipulating test data), with defenses often involving adversarial training, certified defenses, or robust aggregation mechanisms. Beyond adversarial threats, understanding and improving GNN generalization is crucial. \cite{ju2023prm} significantly advances theoretical understanding by deriving sharp, non-vacuous PAC-Bayesian bounds for GNN generalization, scaling with the spectral norm of the graph diffusion matrix. These bounds offer orders of magnitude tighter guarantees than previous vacuous bounds, providing a stronger theoretical basis for building reliable GNNs. Addressing a key practical limitation, \cite{kang2024fsk} introduces FROND, a novel framework leveraging fractional calculus in continuous GNNs to model non-local, memory-dependent dynamics. This approach inherently mitigates the pervasive oversmoothing problem, which degrades GNN performance in deep layers, thereby enhancing model robustness and reliability. Furthermore, \cite{xia20247w9} tackles OOD generalization by proposing the Cluster Information Transfer (CIT) mechanism, a plug-in method designed to learn invariant representations against "structure shift" in graph data, crucial for maintaining robustness in dynamic real-world environments.

\textbf{Fairness} in GNN predictions is paramount, especially given the potential for GNNs to amplify existing societal biases due to graph homophily (the tendency for similar nodes to connect) and the message-passing mechanism. Bias can manifest as disparate impact (group fairness, e.g., demographic parity or equal opportunity) or disparate treatment (individual fairness, e.g., similar individuals receiving dissimilar outcomes) \cite{zhang20222g3, dai2022hsi}. Addressing this, research has explored various strategies. \textbf{Pre-processing} methods aim to debias the input graph data itself. For instance, \cite{dong2021qcg} proposes EDITS, a model-agnostic framework that defines and mitigates "attribute bias" and "structural bias" in attributed networks \textit{before} GNN processing, using Wasserstein-1 distance to quantify bias in both node features and graph structure. This approach offers a universal solution for data debiasing, applicable to any GNN. \textbf{In-processing} methods modify the GNN training process. FairGNN \cite{dai2020p5t} introduces an adversarial framework that includes a GCN-based sensitive attribute estimator to predict missing sensitive attribute information, allowing the GNN classifier to be trained to fool an adversary attempting to predict sensitive attributes from its representations. This approach effectively eliminates discrimination while maintaining high accuracy, even with limited sensitive attribute data. For \textbf{individual fairness}, \cite{dong202183w} proposes REDRESS, a novel ranking-based framework that refines the notion of individual fairness from a ranking perspective. This method addresses the practical difficulties of Lipschitz-based definitions by formulating a joint optimization problem that balances model utility and ranking-based individual fairness, making it a plug-and-play solution for existing GNN architectures.

The \textbf{Interpretability} of GNNs is another critical pillar of trustworthiness, allowing users to understand and trust model decisions, especially in sensitive applications. Recent research has moved beyond merely providing explanations to rigorously ensuring their faithfulness and structural awareness. \cite{chen2024woq} critically examines the interpretability of existing GNN explainers, introducing the Subgraph Multilinear Extension (SubMT) framework to prove limitations of attention-based methods and proposing GMT with random subgraph sampling for more faithful interpretations. This work highlights the need for explanations that truly reflect the model's reasoning process. Building on this, \cite{bui2024zy9} proposes the Myerson-Taylor interaction index and the MAGE explainer, which axiomatically account for graph structure and high-order interactions. This method identifies not only influential nodes and edges but also complex motifs that positively or negatively impact predictions, providing a deeper, more contextually accurate understanding of GNN behavior. Survey papers also categorize explainability methods into intrinsically interpretable GNNs and post-hoc explainers, detailing motivations, challenges, and experimental settings \cite{zhang20222g3, dai2022hsi}.

In conclusion, the pursuit of trustworthy GNNs is rapidly maturing, with substantial progress in developing theoretically grounded and practically robust solutions across privacy, robustness, fairness, and explainability. While the field has made significant strides in understanding and mitigating robustness issues and providing more faithful interpretations, dedicated research into privacy-preserving mechanisms, especially against sophisticated structural inference attacks, and ensuring fairness across diverse populations with practical, scalable solutions remains crucial. Future directions will likely involve more interdisciplinary approaches, combining advanced cryptographic techniques for privacy, robust causal inference methods for fairness, and continued development of transparent and verifiable explanation frameworks, all while striving for holistic solutions that address these interconnected dimensions concurrently.
\subsection{Open Challenges and Future Research}
\label{sec:7\_3\_open\_challenges\_\_and\_\_future\_research}

Despite the remarkable progress in Graph Neural Networks (GNNs), the field is still nascent, facing several fundamental open challenges that define critical avenues for future research. These challenges span from enhancing the intrinsic expressive power of GNNs and developing more robust and adaptive models for complex real-world graphs, to exploring novel mathematical foundations and deepening their integration with other advanced AI paradigms. Ultimately, these efforts aim to shape the next generation of graph-aware AI, ensuring its efficiency, scalability, and trustworthiness.

A primary and enduring challenge lies in further enhancing GNN expressivity, particularly for complex graph structures, and overcoming theoretical limitations like the Weisfeiler-Leman (WL) barrier and practical issues such as over-squashing. While Section 3 detailed advancements in higher-order, substructure-aware, geometric, and spatio-spectral GNNs, the quest for universally expressive yet efficient architectures continues. Theoretical analyses, as surveyed by \cite{jegelka20222lq}, underscore the inherent limitations of standard message-passing GNNs, motivating the exploration of alternative paradigms. For instance, \cite{papp20211ac} introduced DropGNN, demonstrating that random node dropouts during both training and testing can increase expressiveness beyond the WL-test with relatively low overhead, by allowing the GNN to observe diverse neighborhood patterns. However, the computational cost of multiple runs and the challenge of ensuring sufficient observation of higher-order dropouts remain practical considerations. Future research must balance theoretical expressivity with computational tractability and generalization capabilities, potentially through novel architectural designs or by integrating insights from other fields.

Beyond expressivity, developing GNNs that are robust to real-world imperfections and can generalize effectively to out-of-distribution (OOD) data is paramount. Real-world graphs often exhibit inherent noise, incompleteness, and structural heterogeneity, including mixed homophilic and heterophilic patterns (structural disparity). While Section 4.2 discussed adaptive filtering mechanisms like NODE-MOE, the challenge of consistently learning invariant representations under significant "structure shift" remains. \cite{xia20247w9} introduced the Cluster Information Transfer (CIT) mechanism to address OOD generalization by learning invariant representations, yet its generalizability across diverse shift types requires further investigation. Similarly, effectively handling heterophilous graphs, where traditional GNNs struggle, necessitates new approaches. \cite{li2022315}'s GloGNN, which learns global node correlations with linear time complexity, offers a promising direction by moving beyond local aggregation, but its applicability to extremely sparse or dynamic graphs needs further exploration.

A particularly critical and emerging area is the integration of \textit{causal inference} into GNNs to move beyond spurious correlations and enhance robustness and interpretability. Traditional GNNs often learn statistical correlations that may not reflect true causal mechanisms, leading to poor generalization on unseen data. Future research will focus on developing GNNs that can disentangle causal and non-causal information. For example, \cite{zhao2024qw6} proposed an Information-based Causal Learning (ICL) framework to optimize causal information flow, while \cite{fan2022m67}'s DisC framework addresses severe bias by learning disentangled causal substructures. However, the challenges lie in rigorously defining causality in complex graph structures, developing scalable methods for causal discovery, and effectively integrating causal mechanisms into GNN architectures without sacrificing predictive performance.

The trustworthiness of GNNs, encompassing privacy, robustness, fairness, and explainability, remains a guiding principle for future development, building upon the foundational work discussed in Section 7.2.
\begin{itemize}
    \item \textbf{Explainability:} While progress has been made in post-hoc explanations (\cite{chen2024woq}, \cite{bui2024zy9}), a key frontier is the development of \textit{inherently interpretable} GNN architectures that provide transparent decision-making processes, rather than just post-hoc rationalizations. Future work needs to establish more rigorous metrics for evaluating the faithfulness and comprehensibility of explanations, especially for complex graph tasks.
    \item \textbf{Fairness:} Addressing algorithmic bias in GNNs is crucial. Research must move towards proactive and holistic fairness solutions that account for the unique ways bias can be amplified through graph structures and message passing (\cite{dai2020p5t}). This includes developing methods for individual fairness (\cite{dong202183w}), model-agnostic debiasing of input graph data (\cite{dong2021qcg}), and ensuring fairness in multi-modal or dynamic graph settings. The challenge of defining and measuring fairness in complex, interconnected data remains significant.
    \item \textbf{Privacy:} The vulnerability of GNNs to privacy attacks, such as link stealing (\cite{he2020kz4}), necessitates robust privacy-preserving mechanisms. Future research will focus on integrating advanced techniques like differential privacy, homomorphic encryption, and secure multi-party computation directly into GNN training and inference pipelines. The emerging field of federated graph neural networks (\cite{liu2022gcg}) offers a promising avenue for privacy-preserving collaborative learning on decentralized graph data, but challenges in data heterogeneity and communication overhead persist. A comprehensive understanding of these issues is provided by surveys like \cite{dai2022hsi}, which will guide future privacy-aware GNN design.
\end{itemize}

A particularly promising and transformative avenue for future research involves deepening the integration of GNNs with other advanced AI paradigms, especially Large Language Models (LLMs), for multi-modal understanding and enhanced transferability. As highlighted by \cite{li202444f}'s Morpher, aligning GNNs with LLMs, even under extremely weak text supervision, enables CLIP-style zero-shot generalization, opening new frontiers for semantic understanding and adaptability in low-resource settings. However, significant challenges remain in effectively aligning diverse modalities (e.g., graph structure, node features, text, images), handling noisy or incomplete multi-modal data, and ensuring that the unique structural information encoded by GNNs is not diluted or misrepresented by the dominant semantic knowledge of LLMs. Future research will explore more sophisticated cross-modal attention mechanisms, unified representation learning, and novel prompting strategies that leverage the strengths of both graph and language models.

Finally, the continuous need for more efficient, scalable, and trustworthy GNNs for emerging applications and ethical considerations remains a guiding principle. Scaling GNNs to truly web-scale, dynamic graphs with billions of nodes and edges, particularly in decentralized or federated environments, requires fundamental architectural innovations beyond current sampling or partitioning strategies. The exploration of novel mathematical foundations, such as fractional calculus within the FROND framework (\cite{kang2024fsk}), offers a fresh perspective on designing inherently robust GNNs that mitigate issues like oversmoothing and model complex, memory-dependent graph dynamics. Furthermore, the establishment of rigorous benchmarking frameworks, as advocated by \cite{dwivedi20239ab}, is crucial for fair comparison and identifying truly impactful architectural advancements. Future research will likely focus on a synergistic approach, combining architectural innovations for enhanced expressivity, advanced mathematical tools for inherent robustness, multi-modal integration for broader applicability, and a strong emphasis on ethical AI principles, all while upholding efficiency and scalability to shape the next generation of graph-aware AI and its profound societal impact.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{328}

\bibitem{wang2024oi8}
Yuanqing Wang, and Kyunghyun Cho (2024). \textit{Non-convolutional Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{li2022315}
Xiang Li, Renyu Zhu, Yao Cheng, et al. (2022). \textit{Finding Global Homophily in Graph Neural Networks When Meeting Heterophily}. International Conference on Machine Learning.

\bibitem{kang2024fsk}
Qiyu Kang, Kai Zhao, Qinxu Ding, et al. (2024). \textit{Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND}. International Conference on Learning Representations.

\bibitem{gao2022f3h}
Chen Gao, Xiang Wang, Xiangnan He, et al. (2022). \textit{Graph Neural Networks for Recommender System}. Web Search and Data Mining.

\bibitem{li2023o4c}
Juanhui Li, Harry Shomer, Haitao Mao, et al. (2023). \textit{Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking}. Neural Information Processing Systems.

\bibitem{michel2023hc4}
Gaspard Michel, Giannis Nikolentzos, J. Lutzeyer, et al. (2023). \textit{Path Neural Networks: Expressive and Accurate Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{chen2022mmu}
Chaoqi Chen, Yushuang Wu, Qiyuan Dai, et al. (2022). \textit{A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{yuan2021pgd}
Hao Yuan, Haiyang Yu, Jie Wang, et al. (2021). \textit{On Explainability of Graph Neural Networks via Subgraph Explorations}. International Conference on Machine Learning.

\bibitem{dong202183w}
Yushun Dong, Jian Kang, H. Tong, et al. (2021). \textit{Individual Fairness for Graph Neural Networks: A Ranking based Approach}. Knowledge Discovery and Data Mining.

\bibitem{cappart2021xrp}
Quentin Cappart, D. Chételat, Elias Boutros Khalil, et al. (2021). \textit{Combinatorial optimization and reasoning with graph neural networks}. International Joint Conference on Artificial Intelligence.

\bibitem{dong2021qcg}
Yushun Dong, Ninghao Liu, B. Jalaeian, et al. (2021). \textit{EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks}. The Web Conference.

\bibitem{li20245zy}
Zhixun Li, Yushun Dong, Qiang Liu, et al. (2024). \textit{Rethinking Fair Graph Neural Networks from Re-balancing}. Knowledge Discovery and Data Mining.

\bibitem{zhao2020bmj}
Tong Zhao, Yozen Liu, Leonardo Neves, et al. (2020). \textit{Data Augmentation for Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{joshi20239d0}
Chaitanya K. Joshi, and Simon V. Mathis (2023). \textit{On the Expressive Power of Geometric Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{sun2022d18}
Mingchen Sun, Kaixiong Zhou, Xingbo He, et al. (2022). \textit{GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{derrowpinion2021mwn}
Austin Derrow-Pinion, Jennifer She, David Wong, et al. (2021). \textit{ETA Prediction with Graph Neural Networks in Google Maps}. International Conference on Information and Knowledge Management.

\bibitem{chen2020bvl}
Yu Chen, Lingfei Wu, and Mohammed J. Zaki (2020). \textit{Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings}. Neural Information Processing Systems.

\bibitem{zeng2022jhz}
Hanqing Zeng, Muhan Zhang, Yinglong Xia, et al. (2022). \textit{Decoupling the Depth and Scope of Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{yuan20208v3}
Haonan Yuan, Jiliang Tang, Xia Hu, et al. (2020). \textit{XGNN: Towards Model-Level Explanations of Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{xie2021n52}
Yaochen Xie, Zhao Xu, Zhengyang Wang, et al. (2021). \textit{Self-Supervised Learning of Graph Neural Networks: A Unified Review}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mitra2024x43}
Shaswata Mitra, Trisha Chakraborty, Subash Neupane, et al. (2024). \textit{Use of Graph Neural Networks in Aiding Defensive Cyber Operations}. arXiv.org.

\bibitem{zhang2021kc7}
Muhan Zhang, and Pan Li (2021). \textit{Nested Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{wang2022p2r}
Hongya Wang, Haoteng Yin, Muhan Zhang, et al. (2022). \textit{Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{lu20213kr}
Yuanfu Lu, Xunqiang Jiang, Yuan Fang, et al. (2021). \textit{Learning to Pre-train Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{fan2022m67}
Shaohua Fan, Xiao Wang, Yanhu Mo, et al. (2022). \textit{Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure}. Neural Information Processing Systems.

\bibitem{zhang2020b0m}
Zaixi Zhang, Jinyuan Jia, Binghui Wang, et al. (2020). \textit{Backdoor Attacks to Graph Neural Networks}. ACM Symposium on Access Control Models and Technologies.

\bibitem{cui2022mjr}
Hejie Cui, Wei Dai, Yanqiao Zhu, et al. (2022). \textit{BrainGB: A Benchmark for Brain Network Analysis With Graph Neural Networks}. IEEE Transactions on Medical Imaging.

\bibitem{bui2024zy9}
Ngoc Bui, Hieu Trung Nguyen, Viet Anh Nguyen, et al. (2024). \textit{Explaining Graph Neural Networks via Structure-aware Interaction Index}. International Conference on Machine Learning.

\bibitem{liu2022a5y}
Chuang Liu, Yibing Zhan, Chang Li, et al. (2022). \textit{Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities}. International Joint Conference on Artificial Intelligence.

\bibitem{jin2023ijy}
Ming Jin, Huan Yee Koh, Qingsong Wen, et al. (2023). \textit{A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{ying2019rza}
Rex Ying, Dylan Bourgeois, Jiaxuan You, et al. (2019). \textit{GNNExplainer: Generating Explanations for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{liu2020w3t}
Meng Liu, Hongyang Gao, and Shuiwang Ji (2020). \textit{Towards Deeper Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{longa202399q}
Antonio Longa, Veronica Lachi, G. Santin, et al. (2023). \textit{Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities}. Trans. Mach. Learn. Res..

\bibitem{papp20211ac}
P. Papp, Karolis Martinkus, Lukas Faber, et al. (2021). \textit{DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{chang2021yyt}
Jianxin Chang, Chen Gao, Y. Zheng, et al. (2021). \textit{Sequential Recommendation with Graph Neural Networks}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{mujkanovic20238fi}
Felix Mujkanovic, Simon Geisler, Stephan Gunnemann, et al. (2023). \textit{Are Defenses for Graph Neural Networks Robust?}. Neural Information Processing Systems.

\bibitem{you2021uxi}
Jiaxuan You, Jonathan M. Gomes-Selman, Rex Ying, et al. (2021). \textit{Identity-aware Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{luo2024euy}
Dongsheng Luo, Tianxiang Zhao, Wei Cheng, et al. (2024). \textit{Towards Inductive and Efficient Explanations for Graph Neural Networks}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{cui2022pap}
Hejie Cui, Wei Dai, Yanqiao Zhu, et al. (2022). \textit{Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{dai2022xze}
Enyan Dai, Wei-dong Jin, Hui Liu, et al. (2022). \textit{Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels}. Web Search and Data Mining.

\bibitem{wang2023wrg}
Kunze Wang, Yihao Ding, and S. Han (2023). \textit{Graph Neural Networks for Text Classification: A Survey}. Artificial Intelligence Review.

\bibitem{khemani2024i8r}
Bharti Khemani, S. Patil, K. Kotecha, et al. (2024). \textit{A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions}. Journal of Big Data.

\bibitem{agarwal2022xfp}
Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, et al. (2022). \textit{Evaluating explainability for graph neural networks}. Scientific Data.

\bibitem{dwivedi20239ab}
Vijay Prakash Dwivedi, Chaitanya K. Joshi, T. Laurent, et al. (2023). \textit{Benchmarking Graph Neural Networks}. Journal of machine learning research.

\bibitem{abboud2020x5e}
Ralph Abboud, .Ismail .Ilkan Ceylan, Martin Grohe, et al. (2020). \textit{The Surprising Power of Graph Neural Networks with Random Node Initialization}. International Joint Conference on Artificial Intelligence.

\bibitem{liu2023v3e}
Yixin Liu, Kaize Ding, Jianling Wang, et al. (2023). \textit{Learning Strong Graph Neural Networks with Weak Information}. Knowledge Discovery and Data Mining.

\bibitem{liu2021ee2}
Xiaorui Liu, W. Jin, Yao Ma, et al. (2021). \textit{Elastic Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{balcilar20215ga}
M. Balcilar, P. Héroux, Benoit Gaüzère, et al. (2021). \textit{Breaking the Limits of Message Passing Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{hu2019r47}
Weihua Hu, Bowen Liu, Joseph Gomes, et al. (2019). \textit{Strategies for Pre-training Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{chamberlain2022fym}
B. Chamberlain, S. Shirobokov, Emanuele Rossi, et al. (2022). \textit{Graph Neural Networks for Link Prediction with Subgraph Sketching}. International Conference on Learning Representations.

\bibitem{reiser2022b08}
Patrick Reiser, Marlen Neubert, Andr'e Eberhard, et al. (2022). \textit{Graph neural networks for materials science and chemistry}. Communications Materials.

\bibitem{li2021orq}
Guohao Li, Matthias Müller, Bernard Ghanem, et al. (2021). \textit{Training Graph Neural Networks with 1000 Layers}. International Conference on Machine Learning.

\bibitem{wang2022u2l}
Xiyuan Wang, and Muhan Zhang (2022). \textit{How Powerful are Spectral Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{zhang2021wgf}
Zaixin Zhang, Qi Liu, Hao Wang, et al. (2021). \textit{ProtGNN: Towards Self-Explaining Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{garg2020z6o}
Vikas K. Garg, S. Jegelka, and T. Jaakkola (2020). \textit{Generalization and Representational Limits of Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{fatemi2021dmb}
Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi (2021). \textit{SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zhang2021jqr}
Yuyu Zhang, Xinshi Chen, Yuan Yang, et al. (2021). \textit{Graph Neural Networks}. Deep Learning on Graphs.

\bibitem{varbella20242iz}
Anna Varbella, Kenza Amara, B. Gjorgiev, et al. (2024). \textit{PowerGraph: A power grid benchmark dataset for graph neural networks}. Neural Information Processing Systems.

\bibitem{rusch2023xev}
T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra (2023). \textit{A Survey on Oversmoothing in Graph Neural Networks}. arXiv.org.

\bibitem{chen2020e6g}
Zhengdao Chen, Lei Chen, Soledad Villar, et al. (2020). \textit{Can graph neural networks count substructures?}. Neural Information Processing Systems.

\bibitem{zhang20222g3}
He Zhang, Bang Wu, Xingliang Yuan, et al. (2022). \textit{Trustworthy Graph Neural Networks: Aspects, Methods, and Trends}. Proceedings of the IEEE.

\bibitem{han2024rkj}
Haoyu Han, Juanhui Li, Wei Huang, et al. (2024). \textit{Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach}. arXiv.org.

\bibitem{rossi2020otv}
Emanuele Rossi, Fabrizio Frasca, B. Chamberlain, et al. (2020). \textit{SIGN: Scalable Inception Graph Neural Networks}. arXiv.org.

\bibitem{wu2022vcx}
Yingmin Wu, Xiang Wang, An Zhang, et al. (2022). \textit{Discovering Invariant Rationales for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{morris20185sd}
Christopher Morris, Martin Ritzert, Matthias Fey, et al. (2018). \textit{Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{dai2022hsi}
Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, et al. (2022). \textit{A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability}. Machine Intelligence Research.

\bibitem{wang2024j6z}
Yuwen Wang, Shunyu Liu, Tongya Zheng, et al. (2024). \textit{Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{ju2023prm}
Haotian Ju, Dongyue Li, Aneesh Sharma, et al. (2023). \textit{Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion}. International Conference on Artificial Intelligence and Statistics.

\bibitem{liu20242g6}
Zewen Liu, Guancheng Wan, B. A. Prakash, et al. (2024). \textit{A Review of Graph Neural Networks in Epidemic Modeling}. Knowledge Discovery and Data Mining.

\bibitem{zhang2018kdl}
Muhan Zhang, and Yixin Chen (2018). \textit{Link Prediction Based on Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{bianchi20194ea}
F. Bianchi, Daniele Grattarola, L. Livi, et al. (2019). \textit{Graph Neural Networks With Convolutional ARMA Filters}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{ma2021sim}
Yao Ma, Xiaorui Liu, Neil Shah, et al. (2021). \textit{Is Homophily a Necessity for Graph Neural Networks?}. International Conference on Learning Representations.

\bibitem{li202444f}
Li, Lecheng Zheng, Bowen Jin, et al. (2024). \textit{Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{he2020kz4}
Xinlei He, Jinyuan Jia, M. Backes, et al. (2020). \textit{Stealing Links from Graph Neural Networks}. USENIX Security Symposium.

\bibitem{fang2022tjj}
Taoran Fang, Yunchao Zhang, Yang Yang, et al. (2022). \textit{Universal Prompt Tuning for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{chen2024woq}
Yongqiang Chen, Yatao Bian, Bo Han, et al. (2024). \textit{How Interpretable Are Interpretable Graph Neural Networks?}. International Conference on Machine Learning.

\bibitem{liu2023ent}
Zemin Liu, Xingtong Yu, Yuan Fang, et al. (2023). \textit{GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks}. The Web Conference.

\bibitem{dong20225aw}
Guimin Dong, Mingyue Tang, Zhiyuan Wang, et al. (2022). \textit{Graph Neural Networks in IoT: A Survey}. ACM Trans. Sens. Networks.

\bibitem{fan2019k6u}
Wenqi Fan, Yao Ma, Qing Li, et al. (2019). \textit{Graph Neural Networks for Social Recommendation}. The Web Conference.

\bibitem{you2020drv}
Jiaxuan You, Rex Ying, and J. Leskovec (2020). \textit{Design Space for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{cai2020k4b}
Chen Cai, and Yusu Wang (2020). \textit{A Note on Over-Smoothing for Graph Neural Networks}. arXiv.org.

\bibitem{gosch20237yi}
Lukas Gosch, Simon Geisler, Daniel Sturm, et al. (2023). \textit{Adversarial Training for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zhang2020jrt}
Xiang Zhang, and M. Zitnik (2020). \textit{GNNGuard: Defending Graph Neural Networks against Adversarial Attacks}. Neural Information Processing Systems.

\bibitem{alon2020fok}
Uri Alon, and Eran Yahav (2020). \textit{On the Bottleneck of Graph Neural Networks and its Practical Implications}. International Conference on Learning Representations.

\bibitem{zhu2021zc3}
Meiqi Zhu, Xiao Wang, C. Shi, et al. (2021). \textit{Interpreting and Unifying Graph Neural Networks with An Optimization Framework}. The Web Conference.

\bibitem{zou2021qkz}
Xu Zou, Qinkai Zheng, Yuxiao Dong, et al. (2021). \textit{TDGIA: Effective Injection Attacks on Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{xu2019l8n}
Kaidi Xu, Hongge Chen, Sijia Liu, et al. (2019). \textit{Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective}. International Joint Conference on Artificial Intelligence.

\bibitem{xia20247w9}
Donglin Xia, Xiao Wang, Nian Liu, et al. (2024). \textit{Learning Invariant Representations of Graph Neural Networks via Cluster Generalization}. Neural Information Processing Systems.

\bibitem{wu2020dc8}
Shiwen Wu, Fei Sun, Fei Sun, et al. (2020). \textit{Graph Neural Networks in Recommender Systems: A Survey}. ACM Computing Surveys.

\bibitem{bianchi20239ee}
F. Bianchi, and Veronica Lachi (2023). \textit{The expressive power of pooling in Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{vu2020zkj}
Minh N. Vu, and M. Thai (2020). \textit{PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{gao20213kp}
Chen Gao, Yu Zheng, Nian Li, et al. (2021). \textit{A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions}. Trans. Recomm. Syst..

\bibitem{bessadok2021bfy}
Alaa Bessadok, M. Mahjoub, and I. Rekik (2021). \textit{Graph Neural Networks in Network Neuroscience}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{wang20214ku}
Xiao Wang, Hongrui Liu, Chuan Shi, et al. (2021). \textit{Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration}. Neural Information Processing Systems.

\bibitem{geisler2024wli}
Simon Geisler, Arthur Kosmala, Daniel Herbst, et al. (2024). \textit{Spatio-Spectral Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{zeng20237gv}
DingYi Zeng, Wanlong Liu, Wenyu Chen, et al. (2023). \textit{Substructure Aware Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{jin2020dh4}
Wei Jin, Yao Ma, Xiaorui Liu, et al. (2020). \textit{Graph Structure Learning for Robust Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{dai2020p5t}
Enyan Dai, and Suhang Wang (2020). \textit{Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information}. Web Search and Data Mining.

\bibitem{klicpera20215fk}
Johannes Klicpera, Florian Becker, and Stephan Gunnemann (2021). \textit{GemNet: Universal Directional Graph Neural Networks for Molecules}. Neural Information Processing Systems.

\bibitem{dwivedi2021af0}
Vijay Prakash Dwivedi, A. Luu, T. Laurent, et al. (2021). \textit{Graph Neural Networks with Learnable Structural and Positional Representations}. International Conference on Learning Representations.

\bibitem{feng20225sa}
Jiarui Feng, Yixin Chen, Fuhai Li, et al. (2022). \textit{How Powerful are K-hop Message Passing Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{satorras2021pzl}
Victor Garcia Satorras, Emiel Hoogeboom, and M. Welling (2021). \textit{E(n) Equivariant Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{mao202313j}
Haitao Mao, Zhikai Chen, Wei Jin, et al. (2023). \textit{Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?}. Neural Information Processing Systems.

\bibitem{zgner2019bbi}
Daniel Zügner, and Stephan Günnemann (2019). \textit{Adversarial Attacks on Graph Neural Networks via Meta Learning}. International Conference on Learning Representations.

\bibitem{yuan2020fnk}
Hao Yuan, Haiyang Yu, Shurui Gui, et al. (2020). \textit{Explainability in Graph Neural Networks: A Taxonomic Survey}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{finkelshtein202301z}
Ben Finkelshtein, Xingyue Huang, Michael M. Bronstein, et al. (2023). \textit{Cooperative Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{lucic2021p70}
Ana Lucic, Maartje ter Hoeve, Gabriele Tolomei, et al. (2021). \textit{CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks}. International Conference on Artificial Intelligence and Statistics.

\bibitem{zheng2022qxr}
Xin Zheng, Yixin Liu, Shirui Pan, et al. (2022). \textit{Graph Neural Networks for Graphs with Heterophily: A Survey}. arXiv.org.

\bibitem{dai2023tuj}
Enyan Dai, M. Lin, Xiang Zhang, et al. (2023). \textit{Unnoticeable Backdoor Attacks on Graph Neural Networks}. The Web Conference.

\bibitem{jin2023e18}
G. Jin, Yuxuan Liang, Yuchen Fang, et al. (2023). \textit{Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ying20189jc}
Rex Ying, Ruining He, Kaifeng Chen, et al. (2018). \textit{Graph Convolutional Neural Networks for Web-Scale Recommender Systems}. Knowledge Discovery and Data Mining.

\bibitem{hu2020u8o}
Ziniu Hu, Yuxiao Dong, Kuansan Wang, et al. (2020). \textit{GPT-GNN: Generative Pre-Training of Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{luan202272y}
Sitao Luan, Chenqing Hua, Qincheng Lu, et al. (2022). \textit{Revisiting Heterophily For Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{klicpera20186xu}
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann (2018). \textit{Predict then Propagate: Graph Neural Networks meet Personalized PageRank}. International Conference on Learning Representations.

\bibitem{chen2019s47}
Deli Chen, Yankai Lin, Wei Li, et al. (2019). \textit{Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2022531}
Yu Wang, Yuying Zhao, Yushun Dong, et al. (2022). \textit{Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage}. Knowledge Discovery and Data Mining.

\bibitem{zhou20213lg}
Kaixiong Zhou, Xiao Huang, D. Zha, et al. (2021). \textit{Dirichlet Energy Constrained Learning for Deep Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{jegelka20222lq}
S. Jegelka (2022). \textit{Theory of Graph Neural Networks: Representation and Learning}. arXiv.org.

\bibitem{jin2021pf0}
Wei Jin, Lingxiao Zhao, Shichang Zhang, et al. (2021). \textit{Graph Condensation for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{geisler2021dcq}
Simon Geisler, Tobias Schmidt, Hakan cSirin, et al. (2021). \textit{Robustness of Graph Neural Networks at Scale}. Neural Information Processing Systems.

\bibitem{wu20193b0}
Zonghan Wu, Shirui Pan, Fengwen Chen, et al. (2019). \textit{A Comprehensive Survey on Graph Neural Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{xu2018c8q}
Keyulu Xu, Weihua Hu, J. Leskovec, et al. (2018). \textit{How Powerful are Graph Neural Networks?}. International Conference on Learning Representations.

\bibitem{zhou20188n6}
Jie Zhou, Ganqu Cui, Zhengyan Zhang, et al. (2018). \textit{Graph Neural Networks: A Review of Methods and Applications}. AI Open.

\bibitem{batzner2021t07}
Simon L. Batzner, Albert Musaelian, Lixin Sun, et al. (2021). \textit{E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials}. Nature Communications.

\bibitem{sarlin20198a6}
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, et al. (2019). \textit{SuperGlue: Learning Feature Matching With Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{wu2020hi3}
Zonghan Wu, Shirui Pan, Guodong Long, et al. (2020). \textit{Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{wu2018t43}
Shu Wu, Yuyuan Tang, Yanqiao Zhu, et al. (2018). \textit{Session-based Recommendation with Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020c3j}
Jiong Zhu, Yujun Yan, Lingxiao Zhao, et al. (2020). \textit{Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs}. Neural Information Processing Systems.

\bibitem{wang2019t4a}
Minjie Wang, Da Zheng, Zihao Ye, et al. (2019). \textit{Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.}. Unpublished manuscript.

\bibitem{li2020fil}
Mengzhang Li, and Zhanxing Zhu (2020). \textit{Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting}. AAAI Conference on Artificial Intelligence.

\bibitem{satorras20174cv}
Victor Garcia Satorras, and Joan Bruna (2017). \textit{Few-Shot Learning with Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{zhou20195xo}
Yaqin Zhou, Shangqing Liu, J. Siow, et al. (2019). \textit{Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{oono2019usb}
Kenta Oono, and Taiji Suzuki (2019). \textit{Graph Neural Networks Exponentially Lose Expressive Power for Node Classification}. International Conference on Learning Representations.

\bibitem{shi2019vl4}
Lei Shi, Yifan Zhang, Jian Cheng, et al. (2019). \textit{Skeleton-Based Action Recognition With Directed Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{wu20221la}
Zhanghao Wu, Paras Jain, Matthew A. Wright, et al. (2022). \textit{Representing Long-Range Context for Graph Neural Networks with Global Attention}. Neural Information Processing Systems.

\bibitem{wang2020khd}
Ziyang Wang, Wei Wei, G. Cong, et al. (2020). \textit{Global Context Enhanced Graph Neural Networks for Session-based Recommendation}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{wang2019vol}
Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, et al. (2019). \textit{Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems}. Knowledge Discovery and Data Mining.

\bibitem{zhong2019kka}
Peixiang Zhong, Di Wang, and C. Miao (2019). \textit{EEG-Based Emotion Recognition Using Regularized Graph Neural Networks}. IEEE Transactions on Affective Computing.

\bibitem{zhao2021po9}
Tianxiang Zhao, Xiang Zhang, and Suhang Wang (2021). \textit{GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks}. Web Search and Data Mining.

\bibitem{lv20219al}
Qingsong Lv, Ming Ding, Qiang Liu, et al. (2021). \textit{Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks}. Knowledge Discovery and Data Mining.

\bibitem{yu201969a}
Yue Yu, Jie Chen, Tian Gao, et al. (2019). \textit{DAG-GNN: DAG Structure Learning with Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{tang2022g66}
Jianheng Tang, Jiajin Li, Zi-Chao Gao, et al. (2022). \textit{Rethinking Graph Neural Networks for Anomaly Detection}. International Conference on Machine Learning.

\bibitem{zhao2021lls}
Jianan Zhao, Xiao Wang, C. Shi, et al. (2021). \textit{Heterogeneous Graph Structure Learning for Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{keisler2022t7p}
R. Keisler (2022). \textit{Forecasting Global Weather with Graph Neural Networks}. arXiv.org.

\bibitem{li2020mk1}
Maosen Li, Siheng Chen, Yangheng Zhao, et al. (2020). \textit{Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction}. Computer Vision and Pattern Recognition.

\bibitem{wu2022ptq}
Lingfei Wu, P. Cui, Jian Pei, et al. (2022). \textit{Graph Neural Networks: Foundation, Frontiers and Applications}. Knowledge Discovery and Data Mining.

\bibitem{liu2021qyl}
Zhenguang Liu, Peng Qian, Xiaoyang Wang, et al. (2021). \textit{Combining Graph Neural Networks With Expert Knowledge for Smart Contract Vulnerability Detection}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang20211dl}
Hengrui Zhang, Qitian Wu, Junchi Yan, et al. (2021). \textit{From Canonical Correlation Analysis to Self-supervised Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{shen202037i}
Yifei Shen, Yuanming Shi, Jun Zhang, et al. (2020). \textit{Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis}. IEEE Journal on Selected Areas in Communications.

\bibitem{zhang2020tdy}
Yufeng Zhang, Xueli Yu, Zeyu Cui, et al. (2020). \textit{Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang20209zd}
Qian Huang, Horace He, Abhay Singh, et al. (2020). \textit{Combining Label Propagation and Simple Models Out-performs Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{schaefer2022rsz}
S. Schaefer, Daniel Gehrig, and D. Scaramuzza (2022). \textit{AEGNN: Asynchronous Event-based Graph Neural Networks}. Computer Vision and Pattern Recognition.

\bibitem{chen20201cf}
Tianwen Chen, and R. C. Wong (2020). \textit{Handling Information Loss of Graph Neural Networks for Session-based Recommendation}. Knowledge Discovery and Data Mining.

\bibitem{shen2022gcz}
Yifei Shen, Jun Zhang, Shenghui Song, et al. (2022). \textit{Graph Neural Networks for Wireless Communications: From Theory to Practice}. IEEE Transactions on Wireless Communications.

\bibitem{sharma2022liz}
Kartik Sharma, Yeon-Chang Lee, S. Nambi, et al. (2022). \textit{A Survey of Graph Neural Networks for Social Recommender Systems}. ACM Computing Surveys.

\bibitem{chen2021x8i}
Tianlong Chen, Yongduo Sui, Xuxi Chen, et al. (2021). \textit{A Unified Lottery Ticket Hypothesis for Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{chen2022ifd}
Cen Chen, Kenli Li, Wei Wei, et al. (2022). \textit{Hierarchical Graph Neural Networks for Few-Shot Learning}. IEEE transactions on circuits and systems for video technology (Print).

\bibitem{li2022hw4}
Jiachen Li, Siheng Chen, Xiaoyong Pan, et al. (2022). \textit{Cell clustering for spatial transcriptomics data with graph neural networks}. Nature Computational Science.

\bibitem{yun2022s4i}
Seongjun Yun, Seoyoon Kim, Junhyun Lee, et al. (2022). \textit{Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction}. Neural Information Processing Systems.

\bibitem{wijesinghe20225ms}
Asiri Wijesinghe, and Qing Wang (2022). \textit{A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"}. International Conference on Learning Representations.

\bibitem{cini20213l6}
Andrea Cini, Ivan Marisca, and C. Alippi (2021). \textit{Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{wu2023zm5}
Lingfei Wu, Yu Chen, Kai Shen, et al. (2023). \textit{Graph Neural Networks for Natural Language Processing: A Survey}. Found. Trends Mach. Learn..

\bibitem{li2022a34}
Tianfu Li, Zheng Zhou, Sinan Li, et al. (2022). \textit{The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study}. Mechanical systems and signal processing.

\bibitem{velickovic2023p4r}
Petar Velickovic (2023). \textit{Everything is Connected: Graph Neural Networks}. Current Opinion in Structural Biology.

\bibitem{jiang2020gaq}
Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, et al. (2020). \textit{Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models}. Journal of Cheminformatics.

\bibitem{sun2023vsl}
Xiangguo Sun, Hongtao Cheng, Jia Li, et al. (2023). \textit{All in One: Multi-Task Prompting for Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{zhang2021f18}
Xiao-Meng Zhang, Li Liang, Lin Liu, et al. (2021). \textit{Graph Neural Networks and Their Current Applications in Bioinformatics}. Frontiers in Genetics.

\bibitem{bojchevski2020c51}
Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, et al. (2020). \textit{Scaling Graph Neural Networks with Approximate PageRank}. Knowledge Discovery and Data Mining.

\bibitem{xia2023bpu}
Jun Xia, Chengshuai Zhao, Bozhen Hu, et al. (2023). \textit{Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules}. International Conference on Learning Representations.

\bibitem{rahmani2023kh4}
Saeed Rahmani, Asiye Baghbani, N. Bouguila, et al. (2023). \textit{Graph Neural Networks for Intelligent Transportation Systems: A Survey}. IEEE transactions on intelligent transportation systems (Print).

\bibitem{chen2024gbe}
Hao Chen, Yuan-Qi Bei, Qijie Shen, et al. (2024). \textit{Macro Graph Neural Networks for Online Billion-Scale Recommender Systems}. The Web Conference.

\bibitem{liao202120x}
Wenlong Liao, B. Bak‐Jensen, J. Pillai, et al. (2021). \textit{A Review of Graph Neural Networks and Their Applications in Power Systems}. Journal of Modern Power Systems and Clean Energy.

\bibitem{hin2022g19}
David Hin, Andrey Kan, Huaming Chen, et al. (2022). \textit{LineVD: Statement-level Vulnerability Detection using Graph Neural Networks}. IEEE Working Conference on Mining Software Repositories.

\bibitem{tsitsulin20209pl}
Anton Tsitsulin, John Palowitch, Bryan Perozzi, et al. (2020). \textit{Graph Clustering with Graph Neural Networks}. Journal of machine learning research.

\bibitem{fung20212kw}
Victor Fung, Jiaxin Zhang, Eric Juarez, et al. (2021). \textit{Benchmarking graph neural networks for materials chemistry}. npj Computational Materials.

\bibitem{wang2021mxw}
Yongxin Wang, Kris Kitani, and Xinshuo Weng (2021). \textit{Joint Object Detection and Multi-Object Tracking with Graph Neural Networks}. IEEE International Conference on Robotics and Automation.

\bibitem{wang2020nbg}
Danqing Wang, Pengfei Liu, Y. Zheng, et al. (2020). \textit{Heterogeneous Graph Neural Networks for Extractive Document Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{jha2022cj8}
Kanchan Jha, S. Saha, and Hiteshi Singh (2022). \textit{Prediction of protein–protein interaction using graph neural networks}. Scientific Reports.

\bibitem{schuetz2021cod}
M. Schuetz, J. K. Brubaker, and H. Katzgraber (2021). \textit{Combinatorial optimization with physics-inspired graph neural networks}. Nature Machine Intelligence.

\bibitem{shen2021sbk}
Meng Shen, Jinpeng Zhang, Liehuang Zhu, et al. (2021). \textit{Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks}. IEEE Transactions on Information Forensics and Security.

\bibitem{bo2023rwt}
Deyu Bo, Chuan Shi, Lele Wang, et al. (2023). \textit{Specformer: Spectral Graph Neural Networks Meet Transformers}. International Conference on Learning Representations.

\bibitem{zhang20212ke}
Mengqi Zhang, Shu Wu, Xueli Yu, et al. (2021). \textit{Dynamic Graph Neural Networks for Sequential Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{wei20246l2}
Jianjun Wei, Yue Liu, Xin Huang, et al. (2024). \textit{Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks}. 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA).

\bibitem{yu2020u32}
Feng Yu, Yanqiao Zhu, Q. Liu, et al. (2020). \textit{TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{he2021x8v}
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, et al. (2021). \textit{FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks}. arXiv.org.

\bibitem{wu20210h4}
Yulei Wu, Hongning Dai, and Haina Tang (2021). \textit{Graph Neural Networks for Anomaly Detection in Industrial Internet of Things}. IEEE Internet of Things Journal.

\bibitem{kofinas2024t2b}
Miltiadis Kofinas, Boris Knyazev, Yan Zhang, et al. (2024). \textit{Graph Neural Networks for Learning Equivariant Representations of Neural Networks}. International Conference on Learning Representations.

\bibitem{li2021v1l}
Shuangli Li, Jingbo Zhou, Tong Xu, et al. (2021). \textit{Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity}. Knowledge Discovery and Data Mining.

\bibitem{balcilar2021di1}
M. Balcilar, G. Renton, P. Héroux, et al. (2021). \textit{Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective}. International Conference on Learning Representations.

\bibitem{zhang2020f4l}
Muhan Zhang, Pan Li, Yinglong Xia, et al. (2020). \textit{Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning}. Neural Information Processing Systems.

\bibitem{bilot20234ui}
Tristan Bilot, Nour El Madhoun, K. A. Agha, et al. (2023). \textit{Graph Neural Networks for Intrusion Detection: A Survey}. IEEE Access.

\bibitem{wu2023303}
Qitian Wu, Yiting Chen, Chenxiao Yang, et al. (2023). \textit{Energy-based Out-of-Distribution Detection for Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{huang2021lpu}
P. Huang, Han-Hung Lee, Hwann-Tzong Chen, et al. (2021). \textit{Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation}. AAAI Conference on Artificial Intelligence.

\bibitem{suresh202191q}
Susheel Suresh, Vinith Budde, Jennifer Neville, et al. (2021). \textit{Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns}. Knowledge Discovery and Data Mining.

\bibitem{liu2022gcg}
R. Liu, and Han Yu (2022). \textit{Federated Graph Neural Networks: Overview, Techniques, and Challenges}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{wang202201n}
Lijing Wang, A. Adiga, Jiangzhuo Chen, et al. (2022). \textit{CausalGNN: Causal-Based Graph Neural Networks for Spatio-Temporal Epidemic Forecasting}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2021c3l}
Fan Zhou, and Chengtai Cao (2021). \textit{Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay}. AAAI Conference on Artificial Intelligence.

\bibitem{vasimuddin2021x7c}
Vasimuddin, Sanchit Misra, Guixiang Ma, et al. (2021). \textit{DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks}. International Conference for High Performance Computing, Networking, Storage and Analysis.

\bibitem{eliasof202189g}
Moshe Eliasof, E. Haber, and Eran Treister (2021). \textit{PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations}. Neural Information Processing Systems.

\bibitem{huang2023fk1}
Kexin Huang, Ying Jin, E. Candès, et al. (2023). \textit{Uncertainty Quantification over Graph with Conformalized Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{fey2021smn}
Matthias Fey, J. E. Lenssen, F. Weichert, et al. (2021). \textit{GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings}. International Conference on Machine Learning.

\bibitem{nguyen2021g12}
Van-Anh Nguyen, D. Q. Nguyen, Van Nguyen, et al. (2021). \textit{ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection}. 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion).

\bibitem{innan2023fa7}
Nouhaila Innan, Abhishek Sawaika, Ashim Dhor, et al. (2023). \textit{Financial Fraud Detection using Quantum Graph Neural Networks}. Quantum Machine Intelligence.

\bibitem{guo2022hu1}
Jia Guo, and Chenyang Yang (2022). \textit{Learning Power Allocation for Multi-Cell-Multi-User Systems With Heterogeneous Graph Neural Networks}. IEEE Transactions on Wireless Communications.

\bibitem{maurizi202293p}
M. Maurizi, Chao Gao, and F. Berto (2022). \textit{Predicting stress, strain and deformation fields in materials and structures with graph neural networks}. Scientific Reports.

\bibitem{ye20226hn}
Zi Ye, Y. J. Kumar, G. O. Sing, et al. (2022). \textit{A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs}. IEEE Access.

\bibitem{liu2021efj}
Zemin Liu, Trung-Kien Nguyen, and Yuan Fang (2021). \textit{Tail-GNN: Tail-Node Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{du2021kn9}
Lun Du, Xiaozhou Shi, Qiang Fu, et al. (2021). \textit{GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily}. The Web Conference.

\bibitem{xu20226vc}
Weizhi Xu, Jun Wu, Qiang Liu, et al. (2022). \textit{Evidence-aware Fake News Detection with Graph Neural Networks}. The Web Conference.

\bibitem{wang2023a6u}
Shaocong Wang, Yi Li, Dingchen Wang, et al. (2023). \textit{Echo state graph neural networks with analogue random resistive memory arrays}. Nature Machine Intelligence.

\bibitem{bing2022oka}
Rui Bing, Guan Yuan, Mu Zhu, et al. (2022). \textit{Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications}. Artificial Intelligence Review.

\bibitem{lyu2023ao0}
Ziyu Lyu, Yue Wu, Junjie Lai, et al. (2023). \textit{Knowledge Enhanced Graph Neural Networks for Explainable Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{peng2021gbb}
Hao Peng, Ruitong Zhang, Yingtong Dou, et al. (2021). \textit{Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks}. ACM Trans. Inf. Syst..

\bibitem{xia2021s85}
Ying Xia, Chun-Qiu Xia, Xiaoyong Pan, et al. (2021). \textit{GraphBind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues}. Nucleic Acids Research.

\bibitem{feng2022914}
Aosong Feng, Chenyu You, Shiqiang Wang, et al. (2022). \textit{KerGNNs: Interpretable Graph Neural Networks with Graph Kernels}. AAAI Conference on Artificial Intelligence.

\bibitem{paper2022mw4}
Unknown Authors (2022). \textit{Graph Neural Networks: Foundations, Frontiers, and Applications}. Unpublished manuscript.

\bibitem{luan2021g2p}
Sitao Luan, Chenqing Hua, Qincheng Lu, et al. (2021). \textit{Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?}. arXiv.org.

\bibitem{waikhom20226fa}
Lilapati Waikhom, and Ripon Patgiri (2022). \textit{A survey of graph neural networks in various learning paradigms: methods, applications, and challenges}. Artificial Intelligence Review.

\bibitem{tang2021h2z}
Siyi Tang, Jared A. Dunnmon, Khaled Saab, et al. (2021). \textit{Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis}. International Conference on Learning Representations.

\bibitem{thost20211ln}
Veronika Thost, and Jie Chen (2021). \textit{Directed Acyclic Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{chai2022nf9}
Ziwei Chai, Siqi You, Yang Yang, et al. (2022). \textit{Can Abnormality be Detected by Graph Neural Networks?}. International Joint Conference on Artificial Intelligence.

\bibitem{sun20239ly}
Chengcheng Sun, Chenhao Li, Xiang Lin, et al. (2023). \textit{Attention-based graph neural networks: a survey}. Artificial Intelligence Review.

\bibitem{zhang2022atq}
Mengqi Zhang, Shu Wu, Meng Gao, et al. (2022). \textit{Personalized Graph Neural Networks With Attention Mechanism for Session-Aware Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{munikoti2022k7d}
Sai Munikoti, D. Agarwal, L. Das, et al. (2022). \textit{Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{huoh2023i97}
Ting-Li Huoh, Yan Luo, Peilong Li, et al. (2023). \textit{Flow-Based Encrypted Network Traffic Classification With Graph Neural Networks}. IEEE Transactions on Network and Service Management.

\bibitem{han20227gn}
Jiaqi Han, Yu Rong, Tingyang Xu, et al. (2022). \textit{Geometrically Equivariant Graph Neural Networks: A Survey}. arXiv.org.

\bibitem{kim2022yql}
Hwan Kim, Byung Suk Lee, Won-Yong Shin, et al. (2022). \textit{Graph Anomaly Detection With Graph Neural Networks: Current Status and Challenges}. IEEE Access.

\bibitem{zhang2022uih}
Zeyang Zhang, Xin Wang, Ziwei Zhang, et al. (2022). \textit{Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift}. Neural Information Processing Systems.

\bibitem{zhou2022a3h}
Yang Zhou, Jiuhong Xiao, Yuee Zhou, et al. (2022). \textit{Multi-Robot Collaborative Perception With Graph Neural Networks}. IEEE Robotics and Automation Letters.

\bibitem{wu2023aqs}
Xinyi Wu, A. Ajorlou, Zihui Wu, et al. (2023). \textit{Demystifying Oversmoothing in Attention-Based Graph Neural Networks}. Neural Information Processing Systems.

\bibitem{long2022l97}
Yahui Long, Min Wu, Yong Liu, et al. (2022). \textit{Pre-training graph neural networks for link prediction in biomedical networks}. Bioinform..

\bibitem{cini2022pjy}
Andrea Cini, Ivan Marisca, F. Bianchi, et al. (2022). \textit{Scalable Spatiotemporal Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023ann}
Zhen Zhang, Mohammed Haroon Dupty, Fan Wu, et al. (2023). \textit{Factor Graph Neural Networks}. Journal of machine learning research.

\bibitem{chang2023ex5}
Jianxin Chang, Chen Gao, Xiangnan He, et al. (2023). \textit{Bundle Recommendation and Generation With Graph Neural Networks}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{wang2023zr0}
J. Wang (2023). \textit{A survey on graph neural networks}. EAI Endorsed Trans. e Learn..

\bibitem{zhao2022fvg}
Xusheng Zhao, Jia Wu, Hao Peng, et al. (2022). \textit{Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis}. Neural Networks.

\bibitem{sahili2023f2x}
Zahraa Al Sahili, and M. Awad (2023). \textit{Spatio-Temporal Graph Neural Networks: A Survey}. arXiv.org.

\bibitem{levie2023c1s}
R. Levie (2023). \textit{A graphon-signal analysis of graph neural networks}. Neural Information Processing Systems.

\bibitem{wang2024nuq}
Pengcheng Wang, Linping Tao, Mingwei Tang, et al. (2024). \textit{Incorporating syntax and semantics with dual graph neural networks for aspect-level sentiment analysis}. Engineering applications of artificial intelligence.

\bibitem{dong2024dx0}
Hu Dong, Longjie Li, Dongwen Tian, et al. (2024). \textit{Dynamic link prediction by learning the representation of node-pair via graph neural networks}. Expert systems with applications.

\bibitem{zhao2024oyr}
Pengju Zhao, Wenjie Liao, Yuli Huang, et al. (2024). \textit{Beam layout design of shear wall structures based on graph neural networks}. Automation in Construction.

\bibitem{chen2024h2c}
Ming Chen, Yajian Jiang, Xiujuan Lei, et al. (2024). \textit{Drug-Target Interactions Prediction Based on Signed Heterogeneous Graph Neural Networks}. Chinese journal of electronics.

\bibitem{foroutan2024nhg}
P. Foroutan, and Salim Lahmiri (2024). \textit{Deep Learning-Based Spatial-Temporal Graph Neural Networks for Price Movement Classification in Crude Oil and Precious Metal Markets}. Machine Learning with Applications.

\bibitem{wander2024nnn}
Brook Wander, Muhammed Shuaibi, John R. Kitchin, et al. (2024). \textit{CatTSunami: Accelerating Transition State Energy Calculations with Pretrained Graph Neural Networks}. ACS Catalysis.

\bibitem{li20248gg}
Duantengchuan Li, Yuxuan Gao, Zhihao Wang, et al. (2024). \textit{Homogeneous graph neural networks for third-party library recommendation}. Information Processing & Management.

\bibitem{duan2024que}
Yifan Duan, Guibin Zhang, Shilong Wang, et al. (2024). \textit{CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks}. arXiv.org.

\bibitem{praveen202498y}
R. Praveen, Aktalina Torogeldieva, B. Saravanan, et al. (2024). \textit{Enhancing Intellectual Property Rights(IPR) Transparency with Blockchain and Dual Graph Neural Networks}. 2024 First International Conference on Software, Systems and Information Technology (SSITCON).

\bibitem{wang2024p88}
Huiwei Wang, Tianhua Liu, Ziyu Sheng, et al. (2024). \textit{Explanatory subgraph attacks against Graph Neural Networks}. Neural Networks.

\bibitem{jing2024az0}
Baoyu Jing, Dawei Zhou, Kan Ren, et al. (2024). \textit{Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024370}
Zhongjian Zhang, Xiao Wang, Huichi Zhou, et al. (2024). \textit{Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?}. Knowledge Discovery and Data Mining.

\bibitem{kanatsoulis2024l6i}
Charilaos I. Kanatsoulis, and Alejandro Ribeiro (2024). \textit{Counting Graph Substructures with Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{mishra2024v89}
Rajat Mishra, and S. Shridevi (2024). \textit{Knowledge graph driven medicine recommendation system using graph neural networks on longitudinal medical records}. Scientific Reports.

\bibitem{fang2024p34}
Zhenyao Fang, and Qimin Yan (2024). \textit{Towards accurate prediction of configurational disorder properties in materials using graph neural networks}. npj Computational Materials.

\bibitem{zhang202483k}
Jintu Zhang, Luigi Bonati, Enrico Trizio, et al. (2024). \textit{Descriptor-Free Collective Variables from Geometric Graph Neural Networks.}. Journal of Chemical Theory and Computation.

\bibitem{yin20241mx}
Nan Yin, Mengzhu Wang, Li Shen, et al. (2024). \textit{Continuous Spiking Graph Neural Networks}. arXiv.org.

\bibitem{yan20240up}
Liuxi Yan, and Yaoqun Xu (2024). \textit{XGBoost-Enhanced Graph Neural Networks: A New Architecture for Heterogeneous Tabular Data}. Applied Sciences.

\bibitem{shen2024exf}
Xu Shen, P. Lió, Lintao Yang, et al. (2024). \textit{Graph Rewiring and Preprocessing for Graph Neural Networks Based on Effective Resistance}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{manivannan2024830}
S. K. Manivannan, Venkatesh Kavididevi, D. Muthukumaran, et al. (2024). \textit{Graph Neural Networks for Resource Allocation Optimization in Healthcare Industry}. 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS).

\bibitem{he202455s}
Xingyang He (2024). \textit{Graph neural networks in recommender systems}. Applied and Computational Engineering.

\bibitem{zhao2024qw6}
Zhe Zhao, Pengkun Wang, Haibin Wen, et al. (2024). \textit{A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{yan2024ikq}
Yafeng Yan, Shuyao He, Zhou Yu, et al. (2024). \textit{Investigation of Customized Medical Decision Algorithms Utilizing Graph Neural Networks}. 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE).

\bibitem{xia2024xc9}
Zaishuo Xia, Han Yang, Binghui Wang, et al. (2024). \textit{GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations}. International Conference on Learning Representations.

\bibitem{zhou2024t2r}
Yicheng Zhou, P. Wang, Hao Dong, et al. (2024). \textit{Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{lu2024eu9}
Shengyao Lu, Keith G. Mills, Jiao He, et al. (2024). \textit{GOAt: Explaining Graph Neural Networks via Graph Output Attribution}. International Conference on Learning Representations.

\bibitem{wang2024cb8}
Zhiyang Wang, J. Cerviño, and Alejandro Ribeiro (2024). \textit{A Manifold Perspective on the Statistical Generalization of Graph Neural Networks}. arXiv.org.

\bibitem{li2024yyl}
Dilong Li, Chenghui Lu, Zi-xing Chen, et al. (2024). \textit{Graph Neural Networks in Point Clouds: A Survey}. Remote Sensing.

\bibitem{castroospina2024iy2}
A. Castro-Ospina, M. Solarte-Sanchez, L. Vega-Escobar, et al. (2024). \textit{Graph-Based Audio Classification Using Pre-Trained Models and Graph Neural Networks}. Italian National Conference on Sensors.

\bibitem{zhao2024g5p}
Tianyi Zhao, Jian Kang, and Lu Cheng (2024). \textit{Conformalized Link Prediction on Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{duan2024efz}
Wei Duan, Jie Lu, Yu Guang Wang, et al. (2024). \textit{Layer-diverse Negative Sampling for Graph Neural Networks}. Trans. Mach. Learn. Res..

\bibitem{luo2024h2k}
Xuexiong Luo, Jia Wu, Jian Yang, et al. (2024). \textit{Graph Neural Networks for Brain Graph Learning: A Survey}. International Joint Conference on Artificial Intelligence.

\bibitem{carlo2024a3g}
Alessandro De Carlo, D. Ronchi, Marco Piastra, et al. (2024). \textit{Predicting ADMET Properties from Molecule SMILE: A Bottom-Up Approach Using Attention-Based Graph Neural Networks}. Pharmaceutics.

\bibitem{zandi2024dgs}
Sahab Zandi, Kamesh Korangi, Mar'ia 'Oskarsd'ottir, et al. (2024). \textit{Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction}. European Journal of Operational Research.

\bibitem{zhao2024aer}
Haihong Zhao, Bo Yang, Jiaxu Cui, et al. (2024). \textit{Effective Fault Scenario Identification for Communication Networks via Knowledge-Enhanced Graph Neural Networks}. IEEE Transactions on Mobile Computing.

\bibitem{yao2024pyk}
Rufan Yao, Zhenhua Shen, Xinyi Xu, et al. (2024). \textit{Knowledge mapping of graph neural networks for drug discovery: a bibliometric and visualized analysis}. Frontiers in Pharmacology.

\bibitem{vinh20243q3}
Tuan Vinh, Loc Nguyen, Quang H. Trinh, et al. (2024). \textit{Predicting Cardiotoxicity of Molecules Using Attention-Based Graph Neural Networks}. Journal of Chemical Information and Modeling.

\bibitem{ashraf202443e}
Inaam Ashraf, Janine Strotherm, L. Hermes, et al. (2024). \textit{Physics-Informed Graph Neural Networks for Water Distribution Systems}. AAAI Conference on Artificial Intelligence.

\bibitem{smith2024q8n}
Zachary Smith, Michael Strobel, Bodhi P. Vani, et al. (2024). \textit{Graph Attention Site Prediction (GrASP): Identifying Druggable Binding Sites Using Graph Neural Networks with Attention}. Journal of Chemical Information and Modeling.

\bibitem{abadal2024w7e}
S. Abadal, Pablo Galván, Alberto Mármol, et al. (2024). \textit{Graph neural networks for electroencephalogram analysis: Alzheimer's disease and epilepsy use cases}. Neural Networks.

\bibitem{pflueger2024qi6}
Maximilian Pflueger, David J. Tena Cucala, and Egor V. Kostylev (2024). \textit{Recurrent Graph Neural Networks and Their Connections to Bisimulation and Logic}. AAAI Conference on Artificial Intelligence.

\bibitem{mohammadi202476q}
H. Mohammadi, and Waldemar Karwowski (2024). \textit{Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions}. Brain Science.

\bibitem{sui2024xh9}
Yongduo Sui, Xiang Wang, Tianlong Chen, et al. (2024). \textit{Inductive Lottery Ticket Learning for Graph Neural Networks}. Journal of Computational Science and Technology.

\bibitem{peng2024t2s}
Jie Peng, Runlin Lei, and Zhewei Wei (2024). \textit{Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep Graph Neural Networks}. International Conference on Information and Knowledge Management.

\bibitem{zhao2024e2x}
Shan Zhao, Ioannis Prapas, Ilektra Karasante, et al. (2024). \textit{Causal Graph Neural Networks for Wildfire Danger Prediction}. arXiv.org.

\bibitem{nabian2024vto}
M. A. Nabian (2024). \textit{X-MeshGraphNet: Scalable Multi-Scale Graph Neural Networks for Physics Simulation}. arXiv.org.

\bibitem{cen2024md8}
Jiacheng Cen, Anyi Li, Ning Lin, et al. (2024). \textit{Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?}. Neural Information Processing Systems.

\bibitem{yang2024vy7}
Yachao Yang, Yanfeng Sun, Shaofan Wang, et al. (2024). \textit{Graph Neural Networks with Soft Association between Topology and Attribute}. AAAI Conference on Artificial Intelligence.

\bibitem{li2024gue}
Youjia Li, Vishu Gupta, Muhammed Nur Talha Kilic, et al. (2024). \textit{Hybrid-LLM-GNN: Integrating Large Language Models and Graph Neural Networks for Enhanced Materials Property Prediction}. Digital Discovery.

\bibitem{guo2024zoe}
Zhenbei Guo, Fuliang Li, Jiaxing Shen, et al. (2024). \textit{ConfigReco: Network Configuration Recommendation With Graph Neural Networks}. IEEE Network.

\bibitem{gnanabaskaran20245dg}
A. Gnanabaskaran, K. Bharathi, S. P. Nandakumar, et al. (2024). \textit{Enhanced Drug-Drug Interaction Prediction with Graph Neural Networks and SVM}. 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS).

\bibitem{wang20245it}
Beibei Wang, Bo Jiang, and Chris H. Q. Ding (2024). \textit{FL-GNNs: Robust Network Representation via Feature Learning Guided Graph Neural Networks}. IEEE Transactions on Network Science and Engineering.

\bibitem{abode2024m4z}
Daniel Abode, Ramoni O. Adeogun, and Gilberto Berardinelli (2024). \textit{Power Control for 6G In-Factory Subnetworks With Partial Channel Information Using Graph Neural Networks}. IEEE Open Journal of the Communications Society.

\bibitem{zhao20244un}
Tianxiang Zhao, Xiang Zhang, and Suhang Wang (2024). \textit{Disambiguated Node Classification with Graph Neural Networks}. The Web Conference.

\bibitem{hausleitner2024vw0}
Christian Hausleitner, Heimo Mueller, Andreas Holzinger, et al. (2024). \textit{Collaborative weighting in federated graph neural networks for disease classification with the human-in-the-loop}. Scientific Reports.

\bibitem{zhao2024g7h}
Shan Zhao, Zhaiyu Chen, Zhitong Xiong, et al. (2024). \textit{Beyond Grid Data: Exploring graph neural networks for Earth observation}. IEEE Geoscience and Remote Sensing Magazine.

\bibitem{rusch2024fgp}
T. Konstantin Rusch, Nathan Kirk, M. Bronstein, et al. (2024). \textit{Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks}. Proceedings of the National Academy of Sciences of the United States of America.

\bibitem{wang2024htw}
Fali Wang, Tianxiang Zhao, and Suhang Wang (2024). \textit{Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels}. Web Search and Data Mining.

\bibitem{liu2024sbb}
Bingyao Liu, Iris Li, Jianhua Yao, et al. (2024). \textit{Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).

\bibitem{fang2024zd6}
Zhenyao Fang, and Qimin Yan (2024). \textit{Leveraging Persistent Homology Features for Accurate Defect Formation Energy Predictions via Graph Neural Networks}. Chemistry of Materials.

\bibitem{benedikt2024153}
Michael Benedikt, Chia-Hsuan Lu, Boris Motik, et al. (2024). \textit{Decidability of Graph Neural Networks via Logical Characterizations}. International Colloquium on Automata, Languages and Programming.

\bibitem{zhang20241k0}
Yuelin Zhang, Jiacheng Cen, Jiaqi Han, et al. (2024). \textit{Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning}. International Conference on Machine Learning.

\bibitem{graziani2024lgd}
Caterina Graziani, Tamara Drucks, Fabian Jogl, et al. (2024). \textit{The Expressive Power of Path-Based Graph Neural Networks}. International Conference on Machine Learning.

\bibitem{shi2024g4z}
Dai Shi, Andi Han, Lequan Lin, et al. (2024). \textit{Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks}. International Journal of Machine Learning and Cybernetics.

\bibitem{yuan2024b8b}
Jiang Yuan, Shanxiong Chen, Bofeng Mo, et al. (2024). \textit{R-GNN: recurrent graph neural networks for font classification of oracle bone inscriptions}. Heritage Science.

\bibitem{wang2024kx8}
Haitao Wang, Zelin Liu, Mingjun Li, et al. (2024). \textit{A Gearbox Fault Diagnosis Method Based on Graph Neural Networks and Markov Transform Fields}. IEEE Sensors Journal.

\bibitem{abuhantash202458c}
Ferial Abuhantash, Mohd Khalil Abu Hantash, and Aamna AlShehhi (2024). \textit{Comorbidity-based framework for Alzheimer’s disease classification using graph neural networks}. Scientific Reports.

\bibitem{abbahaddou2024bq2}
Yassine Abbahaddou, Sofiane Ennadir, J. Lutzeyer, et al. (2024). \textit{Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks}. International Conference on Learning Representations.

\bibitem{huang2024tdd}
Renhong Huang, Jiarong Xu, Xin Jiang, et al. (2024). \textit{Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Networks}. AAAI Conference on Artificial Intelligence.

\bibitem{jiang202448s}
Yue Jiang, Changkong Zhou, Vikas Garg, et al. (2024). \textit{Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces}. International Conference on Human Factors in Computing Systems.

\bibitem{wang20246bq}
Bin Wang, Yadong Xu, Manyi Wang, et al. (2024). \textit{Gear Fault Diagnosis Method Based on the Optimized Graph Neural Networks}. IEEE Transactions on Instrumentation and Measurement.

\bibitem{silva2024trs}
Thiago H. Silva, and Daniel Silver (2024). \textit{Using graph neural networks to predict local culture}. Environment and Planning B Urban Analytics and City Science.

\bibitem{zhang2024ctj}
Xin Zhang, Zhen Xu, Yue Liu, et al. (2024). \textit{Robust Graph Neural Networks for Stability Analysis in Dynamic Networks}. 2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE).

\bibitem{sun2024ztz}
Mengfang Sun, Wenying Sun, Ying Sun, et al. (2024). \textit{Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis}. 2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE).

\bibitem{zeng2024fpp}
Xin Zeng, Fan-Fang Meng, Meng-Liang Wen, et al. (2024). \textit{GNNGL-PPI: multi-category prediction of protein-protein interactions using graph neural networks based on global graphs and local subgraphs}. BMC Genomics.

\bibitem{chen20241tu}
Ziang Chen, Xiaohan Chen, Jialin Liu, et al. (2024). \textit{Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs}. arXiv.org.

\bibitem{fujita2024crj}
Takaaki Fujita (2024). \textit{Superhypergraph Neural Networks and Plithogenic Graph Neural Networks: Theoretical Foundations}. arXiv.org.

\bibitem{saleh2024d2a}
Mahdi Saleh, Michael Sommersperger, N. Navab, et al. (2024). \textit{Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact}. IEEE International Conference on Robotics and Automation.

\bibitem{aburidi2024023}
Mohammed Aburidi, and Roummel F. Marcia (2024). \textit{Topological Adversarial Attacks on Graph Neural Networks Via Projected Meta Learning}. IEEE Conference on Evolving and Adaptive Intelligent Systems.

\bibitem{wang2024481}
Zhonghao Wang, Danyu Sun, Sheng Zhou, et al. (2024). \textit{NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise}. Neural Information Processing Systems.

\bibitem{horck2024a8s}
Rostislav Horcík, and Gustav Sír (2024). \textit{Expressiveness of Graph Neural Networks in Planning Domains}. International Conference on Automated Planning and Scheduling.

\bibitem{sun2024pix}
Jianshan Sun, Suyuan Mei, Kun Yuan, et al. (2024). \textit{Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2024r82}
Langsha Li, Feng Qiang, and Li Ma (2024). \textit{Advancing Cybersecurity: Graph Neural Networks in Threat Intelligence Knowledge Graphs}. International Conference on Algorithms, Software Engineering, and Network Security.

\bibitem{luo20240ot}
Renqiang Luo, Huafei Huang, Shuo Yu, et al. (2024). \textit{FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{li202492k}
Shouheng Li, F. Geerts, Dongwoo Kim, et al. (2024). \textit{Towards Bridging Generalization and Expressivity of Graph Neural Networks}. International Conference on Learning Representations.

\bibitem{liao20249wq}
Yidong Liao, Xiao-Ming Zhang, and Chris Ferrie (2024). \textit{Graph Neural Networks on Quantum Computers}. arXiv.org.

\bibitem{wang2024ged}
Yufeng Wang, and Charith Mendis (2024). \textit{TGLite: A Lightweight Programming Framework for Continuous-Time Temporal Graph Neural Networks}. International Conference on Architectural Support for Programming Languages and Operating Systems.

\bibitem{liu20245da}
Ping Liu, Haichao Wei, Xiaochen Hou, et al. (2024). \textit{LinkSAGE: Optimizing Job Matching Using Graph Neural Networks}. Knowledge Discovery and Data Mining.

\bibitem{varghese2024ygs}
Alan John Varghese, Zhen Zhang, and G. Karniadakis (2024). \textit{SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification}. Neural Networks.

\bibitem{dinverno2024vkw}
Giuseppe Alessio D’Inverno, M. Bianchini, and F. Scarselli (2024). \textit{VC dimension of Graph Neural Networks with Pfaffian activation functions}. Neural Networks.

\end{thebibliography}

\end{document}