\subsection{Early GNN Architectures and Breakthroughs}

The conceptualization of neural networks operating on graph-structured data predates the deep learning revolution, with pioneering work by \cite{gori2005new} and \cite{scarselli2009graph} laying the theoretical groundwork for Graph Neural Networks (GNNs). These early models introduced the fundamental idea of iteratively propagating information across graph nodes to learn stable, context-aware representations, often relying on fixed-point iterations. While foundational, their practical application was limited by computational challenges and difficulties in scaling to large graphs \cite{wu2022ptq}. The advent of deep learning, coupled with advancements in computational resources, revitalized this field, necessitating novel architectures capable of processing complex relational information efficiently.

A significant breakthrough came with the adaptation of convolutional operations, highly successful in Euclidean data like images, to the irregular structure of graphs. This led to the development of spectral Graph Convolutional Networks (GCNs), initially explored by \cite{bruna2014spectral}. These models leveraged spectral graph theory, defining convolution as a filtering operation in the graph Fourier domain. While theoretically elegant, these early spectral methods often required computationally expensive eigendecompositions of the graph Laplacian and produced non-localized filters, limiting their practical applicability \cite{wu2022ptq}.

The seminal work by \cite{kipf2017semi} introduced a simplified, layer-wise propagation rule for GCNs, which could be interpreted as a first-order approximation of spectral convolutions. This spatial approximation significantly reduced computational complexity, making GCNs practical for semi-supervised node classification tasks. GCNs learn meaningful node embeddings by aggregating information from local neighborhoods, essentially performing a weighted average of neighbor features. This approach demonstrated the ability of deep learning to effectively capture structural dependencies and node features to generate powerful representations. However, these early GCN formulations were primarily transductive, meaning they learned embeddings for a fixed graph and struggled to generalize to unseen nodes or entirely new graphs without retraining. Furthermore, as Message Passing Neural Networks (MPNNs), GCNs are theoretically limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) test \cite{xu2018powerful, morris2018weisfeiler, balcilar20215ga}. This limitation prevents them from distinguishing certain non-isomorphic graphs and counting complex graph substructures, which became a critical area of subsequent research.

To overcome the transductive nature and enable generalization to unseen nodes, \cite{hamilton2017inductive} introduced GraphSAGE (SAmple and aggreGatE). GraphSAGE pioneered an inductive framework by learning a set of aggregation functions that sample and combine features from a node's local neighborhood. Instead of relying on a fixed adjacency matrix, GraphSAGE trains a function that can generate embeddings for any node by aggregating information from its sampled neighbors. This inductive capability was a significant step forward, broadening the applicability of GNNs to dynamic graphs and moderately large datasets by proving their capacity to generalize beyond the training graph. GraphSAGE offered various aggregation functions (e.g., mean, LSTM, max-pooling), allowing flexibility in how neighborhood information is integrated. Despite its inductive power, GraphSAGE, as an MPNN, also inherits the 1-WL expressivity limitations, motivating further research into more powerful architectures \cite{xu2018powerful}.

Concurrent with these developments, other influential architectures emerged, such as Graph Attention Networks (GATs) \cite{velickovic2018graph}. GATs introduced an attention mechanism to GNNs, allowing each node to assign different weights to its neighbors during aggregation, rather than relying on a fixed or uniformly learned weighting scheme. This enabled GATs to handle varying neighborhood sizes and potentially capture more complex relationships, offering another avenue for enhancing GNN performance and interpretability.

These pioneering architectures, from the foundational spectral methods and the practical GCNs to the inductive GraphSAGE and the attention-based GATs, collectively demonstrated the immense potential of GNNs to learn rich representations from graph-structured data. By progressively addressing challenges related to computational efficiency, generalization, and the flexibility of message passing, these early models not only catalyzed broader research and application development but also established a robust framework for subsequent innovations in the field of deep learning on graphs. They proved the viability of deep learning on non-Euclidean data structures, inspiring a new generation of graph-aware intelligent systems, while simultaneously highlighting critical limitations in expressivity and scalability that would drive future research.