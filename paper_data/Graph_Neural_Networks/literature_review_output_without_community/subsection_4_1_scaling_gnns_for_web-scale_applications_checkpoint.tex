\subsection{Scaling GNNs for Web-Scale Applications}

The practical deployment of Graph Neural Networks (GNNs) in industrial settings, particularly for web-scale applications involving billions of nodes and edges, necessitates overcoming formidable computational and memory bottlenecks \cite{khemani2024i8r, wu2022ptq}. Early GNN architectures, while theoretically powerful, were often computationally prohibitive due to their reliance on full-graph operations (e.g., spectral convolutions requiring the full graph Laplacian) or the implicit assumption that the entire graph could reside in memory. This limitation severely restricted their applicability to smaller datasets and prevented their adoption in real-world systems like large-scale recommender platforms \cite{wu2020dc8, gao2022f3h}. Addressing this scalability gap has been a critical research frontier, leading to diverse methodological advancements.

One of the earliest and most influential paradigms for scaling GNNs is **neighborhood sampling**, which aims to construct localized computation graphs for each node or minibatch, thereby avoiding full-graph computations. A pivotal methodological leap in this direction was PinSage \cite{ying20189jc}, developed for Pinterest's recommender system. PinSage fundamentally re-architected how GNNs interact with massive graphs by performing efficient, localized convolutions through dynamic, on-the-fly neighborhood sampling. Instead of processing the entire graph, it constructs computation graphs for each minibatch by sampling neighbors, eliminating the need for the full graph Laplacian and making training feasible for graphs with billions of nodes and edges. PinSage further refines this by integrating random walks to derive "importance scores" for neighbors, leading to an Importance Pooling aggregation strategy where neighbor features are weighted based on their L1-normalized visit counts. This nuanced approach significantly improves the quality of learned embeddings. The system's robustness is enhanced by a producer-consumer architecture for minibatch generation, decoupling CPU-bound tasks (sampling, feature fetching) from GPU-bound tasks (model computation), maximizing GPU utilization and training throughput. For scalable inference, PinSage employs an efficient MapReduce pipeline to generate billions of node embeddings. While highly effective, sampling-based methods like PinSage and its predecessor GraphSAGE \cite{hamilton2017inductive} can introduce variance into gradient estimates, potentially affecting convergence and requiring careful tuning.

Beyond PinSage, other sampling strategies have emerged to enhance efficiency and reduce variance. FastGCN \cite{chen2018fastgcn} proposes sampling nodes for each layer rather than neighbors for each node, effectively treating graph convolutions as integral transforms and using Monte Carlo approximations. LADIES (Layer-wise Adaptive DEpth-wise Importance Sampling) \cite{zou2019layerwise} further refines this by employing layer-wise importance sampling to reduce variance, especially in deep GNNs, by prioritizing more influential neighbors. These methods offer different trade-offs between sampling efficiency, variance reduction, and computational complexity.

Another major family of scaling techniques involves **graph partitioning or clustering**. Cluster-GCN \cite{chiang2019clustergcn} addresses the memory bottleneck by partitioning the graph into several subgraphs and performing message passing within these clusters. This approach significantly reduces the memory footprint and allows for deeper GNNs, but it can suffer from information loss at cluster boundaries. GraphSAINT \cite{zou2019graphsaint} takes a different approach by sampling subgraphs (either nodes or edges) at each iteration to construct computation graphs, allowing for full-batch training on sampled subgraphs while maintaining a global view of the graph structure. This method aims to reduce the variance associated with sampling by ensuring that sampled subgraphs are representative of the original graph structure. The trade-off here lies in the complexity of sampling and the potential for redundant computations if subgraphs overlap significantly.

More recent advancements have focused on **simplified architectures and graph preprocessing** to improve scalability. Simplified Graph Convolution (SGC) \cite{wu2019simplifying} demonstrates that removing non-linearities between GNN layers can drastically speed up computation while maintaining competitive performance, particularly for transductive tasks. This highlights that for certain applications, complex non-linear transformations may not always be necessary, allowing for more efficient models.

Graph condensation techniques, such as GCOND (Graph Condensation) \cite{jin2021pf0}, offer a novel approach by learning a small, synthetic graph that preserves the GNN's predictive performance when trained on it, effectively distilling the essence of a large graph into a much smaller one. This drastically reduces training time and storage requirements, proving invaluable for scenarios requiring frequent retraining or hyperparameter search. Similarly, graph sparsification and pruning methods, like the Unified GNN Sparsification (UGS) framework \cite{chen2021x8i}, simultaneously prune the graph adjacency matrix and model weights to accelerate GNN inference on large graphs without compromising predictive performance. Furthermore, graph rewiring and preprocessing techniques, such as GPER (Graph Rewiring and Preprocessing based on Effective Resistance) \cite{shen2024exf}, can reduce graph size by over 50\% while mitigating common GNN issues like over-smoothing and over-squashing, thereby improving both performance and scalability. Approximate PageRank-based models like PPRGo \cite{bojchevski2020c51} also offer significant speed gains by utilizing efficient approximations of information diffusion, enabling rapid training and prediction on massive graphs.

In summary, scaling GNNs for web-scale applications has evolved through a multi-faceted approach, encompassing sophisticated neighborhood sampling, intelligent graph partitioning, architectural simplifications, and advanced graph preprocessing techniques. While PinSage pioneered the industrial application of sampling-based GNNs, the field has since diversified, offering a spectrum of solutions, each with its own trade-offs regarding computational cost, memory footprint, and approximation quality. The continuous development of these methodologies, often combining elements from different paradigms, remains crucial for bridging the gap between academic research and the practical demands of handling unprecedented data volumes in real-world industrial deployments \cite{khemani2024i8r, wu2022ptq}.