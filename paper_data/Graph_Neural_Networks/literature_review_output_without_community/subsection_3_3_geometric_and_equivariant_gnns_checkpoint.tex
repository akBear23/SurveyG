\subsection*{Geometric and Equivariant GNNs}

Geometric and equivariant Graph Neural Networks (GNNs) represent a crucial and distinct advancement for accurately modeling 3D spatial data, particularly in scientific domains such as chemistry, physics, and molecular biology. These fields are inherently governed by fundamental physical symmetries, including rotations, translations, and reflections. Consequently, models that inherently respect these symmetries are indispensable for learning robust and physically consistent representations of 3D structures. This area of research developed in parallel with efforts to enhance the expressivity of GNNs for abstract graphs, recognizing the unique requirements and inductive biases offered by spatial data \cite{han20227gn}.

Early approaches to embedding geometric equivariance into neural networks often relied on computationally intensive formalisms. For instance, pioneering models like Tensor Field Networks (TFN) \cite{thomas2018tensor} and SE(3) Transformers \cite{fuchs2020se} utilized spherical harmonics to represent and propagate information. These methods allowed for higher-order representations that naturally transform correctly under rotations, ensuring strict equivariance to the SO(3) group. While theoretically powerful in capturing complex angular dependencies, these architectures typically incurred significant computational overhead due to the complexity of spherical harmonic transformations and were often restricted to 3-dimensional Euclidean spaces (E(3) or SE(3) equivariance), posing challenges for scalability to larger systems or generalization to arbitrary dimensions.

A significant breakthrough in addressing these efficiency and scalability concerns came with the introduction of the E(n) Equivariant Graph Neural Network (EGNN) by \cite{satorras2021pzl}. The core innovation of EGNN lies in its Equivariant Graph Convolutional Layer (EGCL), which directly updates node coordinates using relative differences. This design inherently preserves E(n) equivariance for both scalar node features and vector coordinates without resorting to computationally expensive higher-order representations like spherical harmonics. The EGNN formulation is theoretically applicable to arbitrary $n$-dimensions, though its primary validation and impact have been in 3D applications. It demonstrated superior performance in tasks such as N-body system simulations, significantly outperforming prior methods like TFN and SE(3) Transformers by reducing prediction error by an order of magnitude while maintaining competitive computational efficiency \cite{satorras2021pzl}.

While architectural innovations like EGNN provided practical and efficient solutions, a comprehensive theoretical understanding of the expressive power of geometric GNNs remained an open problem. Filling this gap, \cite{joshi20239d0} developed the Geometric Weisfeiler-Leman (GWL) test, a symmetry-aware generalization of the classical Weisfeiler-Leman test for non-geometric graphs. This theoretical framework rigorously characterizes the expressive capabilities of geometric GNNs, providing upper bounds on their ability to distinguish geometric structures. The GWL test operates by iteratively refining node "colors" and auxiliary "geometric objects" using G-orbit injective and G-equivariant hash functions that aggregate geometric information (e.g., distances, angles, dihedral angles) from progressively larger neighborhoods. This framework offers critical insights into the distinct contributions of invariant versus equivariant layers, demonstrating that while invariant layers have limited expressivity, equivariant layers, by propagating geometric information, can distinguish a larger class of graphs. Furthermore, the work highlighted the importance of higher-order tensors or scalarization (e.g., higher body order in IGWL(k)) for achieving maximally powerful geometric GNNs, as these provide more complete descriptors of local geometry beyond simple distances and angles.

The theoretical insights from the GWL test, particularly regarding the need for richer geometric descriptors, directly motivated subsequent architectural advancements. Building upon these insights into geometric feature integration, \cite{klicpera20215fk} introduced GemNet, a Universal Directional Graph Neural Network for molecules. GemNet provided a theoretical proof of universality for GNNs utilizing spherical (S2) representations, demonstrating that they can be universal approximators for rotationally equivariant predictions, achieving similar expressivity to more complex SO(3) representations but with greater efficiency. A key technical innovation of GemNet is its two-hop message passing scheme, which explicitly and systematically incorporates not only interatomic distances and angles but also crucial dihedral angles through directed edge embeddings. This deeper integration of higher-order geometric information proved vital for accurately predicting quantum mechanical properties and accelerating molecular dynamics simulations, particularly for complex, dynamic molecular geometries.

Despite the success and widespread adoption of EGNN and similar architectures, the question of whether higher-degree representations are truly unnecessary in equivariant GNNs has been critically re-examined. \cite{cen2024md8} theoretically demonstrated that equivariant GNNs, including EGNN-like models, can degenerate to a zero function if the degree of output representations is fixed to 1 or other specific values, especially when dealing with highly symmetric structures. This suggests a limitation in EGNN's expressivity for certain complex geometric patterns, as its reliance on first-degree steerable vectors (Cartesian coordinates) can be insufficient to distinguish all geometrically non-isomorphic graphs. To address this, they proposed HEGNN, a high-degree version of EGNN that incorporates higher-degree steerable vectors while maintaining efficiency through a scalarization trick. Their empirical results on N-body and MD17 datasets showed substantial improvements, indicating that judicious use of higher-degree representations can indeed enhance expressivity, aligning with the GWL's theoretical findings.

The development of these geometrically aware GNNs has been transformative for scientific applications. As highlighted by reviews from \cite{reiser2022b08} and \cite{han20227gn}, GNNs, particularly those integrating geometric data and symmetry-equivariant representations, have become indispensable in materials science and chemistry. They enable robust, end-to-end learning from graph-structured data, facilitating accurate predictions of molecular properties, reaction pathways, and crystal structures by inherently respecting the underlying physical symmetries.

In conclusion, the evolution of geometric and equivariant GNNs reflects a clear progression from efficient architectural designs, like EGNN, to robust theoretical frameworks, such as the GWL test, and advanced models like GemNet and HEGNN that integrate higher-order geometric features. By embedding physical symmetries directly into their learning mechanisms, these GNNs have significantly enhanced their ability to learn robust and physically consistent representations of 3D structures. However, purely spatial message passing GNNs, including many geometric variants, still face inherent limitations such as "over-squashing" \cite{alon2020bottleneck}, which restricts the receptive field and impedes long-range information propagation crucial for understanding complex molecular interactions. This limitation arises because local message passing over fixed-radius neighborhoods struggles to efficiently transmit information across large molecular structures. Future research will likely focus on extending these models to incorporate more sophisticated higher-order geometric features, exploring gauge equivariant GNNs (which are essential for symmetries beyond Euclidean, such as those found in particle physics and lattice field theory), and developing even more expressive and efficient equivariant architectures for handling multi-scale geometric information and dynamic systems while mitigating issues like over-squashing.