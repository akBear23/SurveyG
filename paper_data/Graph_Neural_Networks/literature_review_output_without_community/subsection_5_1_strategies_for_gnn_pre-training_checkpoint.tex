\subsection{Strategies for GNN Pre-training}
The pervasive challenge of labeled data scarcity in real-world graph-structured datasets has spurred the development of sophisticated pre-training strategies for Graph Neural Networks (GNNs). These strategies aim to learn generalizable node and graph representations from vast amounts of unlabeled data, thereby enhancing transfer learning performance and significantly reducing the reliance on extensive, costly manual annotation for downstream tasks. The evolution of GNN pre-training mirrors advancements in other deep learning domains, moving from simple self-supervised tasks to more complex contrastive, generative, and meta-learning approaches designed to capture intrinsic graph properties and improve transferability.

Early foundational work established a systematic approach to learning transferable representations by leveraging self-supervision. \cite{hu2019r47} pioneered a combined node- and graph-level self-supervised pre-training strategy, addressing the common issue of "negative transfer" that arises from ad-hoc methods. Their approach introduced novel node-level tasks, such as Context Prediction, where the GNN learns to predict the structural context of a node's neighborhood, and Attribute Masking, which trains the GNN to infer masked node or edge attributes. By integrating these predictive tasks with regularized graph-level pre-training, this method demonstrated significant improvements in generalization, particularly for molecular and protein prediction, laying crucial groundwork for subsequent developments in self-supervised GNNs. These generative tasks inherently encourage the GNN to understand local graph topology and feature dependencies.

A significant paradigm shift in self-supervised GNN pre-training emerged with contrastive learning, which aims to maximize the agreement between different "views" of the same graph or node while pushing apart representations of dissimilar entities. \cite{xie2021n52} provides a unified review, categorizing these methods and grounding contrastive learning in the principle of mutual information maximization. This perspective highlights how various methods differ primarily in their strategies for generating positive and negative views and their choice of mutual information estimators. One of the earliest and most influential contrastive methods is Deep Graph Infomax (DGI), which maximizes the mutual information between node embeddings and a global graph summary. DGI learns node representations by distinguishing positive (node-summary pairs from the same graph) from negative (node-summary pairs from different graphs or shuffled nodes) examples, thereby encouraging the GNN to capture information relevant to the entire graph context. Following DGI, augmentation-based contrastive learning became prominent. Methods like Graph Contrastive Learning (GraphCL) and GRACE generate two distinct augmented views of a graph (e.g., by randomly dropping edges, masking features, or perturbing node attributes) and then train the GNN to maximize the similarity between the representations of these two views of the *same* graph instance, while minimizing similarity to views from *other* graph instances. This approach forces the GNN to learn representations that are robust to structural and feature perturbations, capturing essential, invariant properties of the graph.

While effective, traditional contrastive methods often rely on carefully constructed negative samples, which can be computationally costly or challenging to define optimally for complex graph structures. To address this, \cite{zhang20211dl} introduced Canonical Correlation Analysis-based Self-Supervised Graph Neural Networks (CCA-SSG). This conceptually simple yet effective model generates two views of an input graph through data augmentation, similar to other contrastive methods. However, instead of instance-level discrimination with negative samples, CCA-SSG optimizes an innovative feature-level objective inspired by classical Canonical Correlation Analysis. This objective essentially aims to discard augmentation-variant information by learning invariant representations and prevents degenerated solutions by decorrelating features in different dimensions. Their theoretical analysis further connects this objective to the Information Bottleneck Principle, offering a principled way to learn robust representations without the complexities of negative sampling.

More recently, a resurgence of generative pre-training has been observed, particularly inspired by Masked Autoencoders (MAE) in computer vision. This paradigm, often termed Masked Graph Modeling (MGM), involves masking a portion of the input graph (e.g., node features, edges, or even entire subgraphs) and training the GNN to reconstruct the masked elements. For example, \cite{sun2022d18} utilizes a masked edge prediction task during pre-training, where the model learns to predict missing edges, thereby implicitly capturing structural regularities. This approach, exemplified by methods like GraphMAE (not explicitly in provided papers but representative of the paradigm), offers advantages by avoiding the need for negative samples inherent in contrastive learning and can be particularly effective in capturing fine-grained local graph information and feature dependencies.

Critically comparing these paradigms, contrastive learning methods often excel at learning discriminative embeddings, which are highly effective for tasks like node or graph classification where distinguishing between entities is paramount. However, their performance can be sensitive to the quality of data augmentations and the computational cost and complexity of negative sampling. Generative methods, including early predictive tasks and the more recent MGM, tend to capture richer local structural and feature information by forcing the model to understand dependencies for reconstruction. They inherently avoid the negative sampling problem, which can simplify training and improve stability. The choice between these often depends on the downstream task's requirements: discriminative power for classification versus fine-grained structural understanding for tasks like link prediction or property prediction.

Beyond the core pre-training tasks, a significant challenge in GNN transfer learning is the "inherent training objective gap" between the pre-training pretext task and the downstream task, which can lead to limited or even negative transfer (\cite{sun2022d18}, \cite{huang2024tdd}). To bridge this gap, novel strategies have emerged. \cite{sun2022d18} proposes Graph Pre-training and Prompt Tuning (GPPT), which reformulates the downstream task to mimic the structure of the pre-training task (e.g., node classification as an edge prediction-like task involving "token pairs" of candidate labels and node entities). This allows pre-trained GNNs to be applied without tedious fine-tuning, significantly accelerating convergence and improving few-shot performance. Similarly, \cite{lu20213kr} introduces L2P-GNN (Learning to Pre-train GNNs), a meta-learning approach that explicitly optimizes for the model's ability to quickly adapt to new tasks during the pre-training phase, inspired by MAML. L2P-GNN employs a dual adaptation mechanism for self-supervised learning of both node-level (via link prediction) and graph-level (via contrastive learning on sub-structures) representations, ensuring the learned parameters are inherently optimized for rapid fine-tuning. Further addressing this, \cite{huang2024tdd} introduces "task consistency" to quantify the similarity between pre-training and downstream tasks and proposes "Bridge-Tune," an intermediate fine-tuning step designed to diminish the impact of task differences.

The challenge of heterogeneity in real-world graphs also necessitates specialized pre-training strategies. Traditional GNNs often struggle with heterogeneous information networks (HINs) due to their diverse node and edge types, which can lead to redundancy and over-reliance on initial features. \cite{wei20246l2} explores self-supervised GNNs specifically for HINs, proposing a framework that flexibly combines different types of additional information from the attribute graph and its nodes. While the specific self-supervisory mechanism is described at a high level, the work aims to improve the adaptability of GNNs to the diversity and complexity of HINs, enabling better extraction of deep features and improving overall model performance in such intricate graph structures by learning robust, type-aware representations.

In summary, GNN pre-training has evolved from basic generative tasks that predict local attributes and contexts to sophisticated contrastive learning paradigms that leverage mutual information maximization and invariant learning, and a renewed interest in masked graph modeling. Crucially, recent advancements have focused on bridging the objective gap between pre-training and downstream tasks through prompt learning and meta-learning, significantly enhancing transferability. These strategies, whether predictive, contrastive, or meta-learning based, are fundamental for learning robust, transferable representations from unlabeled data, thereby mitigating the labeled data bottleneck and enabling GNNs to achieve strong performance on a wide array of downstream tasks with limited supervision. Future advancements are expected to focus on developing more universally applicable pre-training objectives, improving augmentation strategies, and extending these methods to even more complex and heterogeneous graph structures, while also considering the computational efficiency and theoretical guarantees of transferability.