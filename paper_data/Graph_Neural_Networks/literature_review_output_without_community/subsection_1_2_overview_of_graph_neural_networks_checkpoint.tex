\subsection*{Overview of Graph Neural Networks}
Graph Neural Networks (GNNs) represent a transformative and rapidly evolving class of deep learning models specifically engineered to operate directly on graph-structured data, thereby addressing the inherent limitations of traditional neural networks when confronted with non-Euclidean domains \cite{wu2022ptq, khemani2024i8r}. Unlike grid-like or sequential data, graphs possess irregular structures and complex relational dependencies, making conventional convolutional or recurrent architectures ill-suited for effectively capturing their rich topological and feature information \cite{khemani2024i8r}. The fundamental purpose of GNNs is to learn meaningful, context-aware representations for individual nodes and entire graphs by synergistically leveraging both node features and the intricate relational information encoded within the graph's topology.

At their core, GNNs universally employ an iterative message-passing paradigm, a mechanism often formalized within the Message Passing Neural Network (MPNN) framework \cite{gilmer2017neural}. This process typically unfolds over multiple layers, where each node iteratively refines its representation by exchanging and integrating information with its local neighborhood. Specifically, in each layer, a node first computes "messages" based on its current representation and the representations of its neighbors. These messages are then "aggregated" from all incoming neighbors, typically using permutation-invariant functions such as summation, mean, or max pooling, to generate a compact summary of the local neighborhood. Finally, the node's representation is "updated" by combining its previous state with this aggregated neighborhood information, often through a learnable neural network. This iterative propagation and aggregation mechanism enables GNNs to effectively gather and integrate information from increasingly distant parts of their local receptive fields, allowing them to learn rich, context-aware representations that encapsulate both local structural patterns and feature interactions \cite{wu2022ptq, khemani2024i8r}.

The initial promise of this paradigm was quickly demonstrated by pioneering architectures that laid the conceptual groundwork for the field. Broadly, early GNNs could be categorized into two main families: spectral and spatial methods. Spectral GNNs, such as the seminal Graph Convolutional Networks (GCNs) \cite{kipf2017semi}, adapt convolutional operations from the spectral domain of graph signal processing, leveraging the graph Laplacian to define filters that operate on graph signals. These models implicitly capture global graph properties through spectral decomposition, though their theoretical underpinnings and expressive power are still subjects of ongoing research \cite{wang2022u2l}. In parallel, spatial GNNs, exemplified by GraphSAGE \cite{hamilton2017inductive}, directly define convolutions by aggregating features from a node's immediate neighbors in the spatial domain. GraphSAGE, in particular, introduced inductive capabilities by learning a function that aggregates information from a sampled set of neighbors, enabling generalization to unseen nodes and graphs. These early models showcased remarkable efficacy across canonical graph learning tasks, including node classification, link prediction, and graph classification, thereby catalyzing broader research into deep learning on non-Euclidean data \cite{khemani2024i8r}.

Despite these significant initial breakthroughs, foundational GNN architectures inherently faced several critical challenges that limited their immediate applicability and expressive power. Issues such as the limited ability to distinguish between certain non-isomorphic graphs (a challenge related to the Weisfeiler-Leman test, explored in Section 3.1) highlighted limitations in their structural expressivity \cite{wang2022u2l}. Furthermore, scalability emerged as a major hurdle, as many early models struggled to efficiently process real-world graphs comprising millions or billions of nodes and edges due to computational and memory constraints \cite{khemani2024i8r}. Problems like oversmoothing, where node representations become indistinguishable after many layers, and difficulties in handling imperfect or noisy data also underscored the need for more robust and adaptable models. Moreover, the theoretical understanding of GNN generalization capabilities, particularly how they perform on unseen data sampled from underlying graph manifolds, remained nascent \cite{wang2024cb8}. These inherent limitations of early GNNs, while not diminishing their foundational importance, collectively motivated a rich and diverse research agenda, driving the development of advanced architectures and methodologies explored in subsequent sections of this review, aimed at enhancing expressivity, improving scalability, and ensuring robustness and trustworthiness for real-world deployment.