\subsection{Learning with Weak Information and Imperfect Data}

Real-world graph data is inherently noisy and often suffers from pervasive imperfections, including incomplete graph structures, sparse node features, and limited or noisy labels. These deficiencies rarely occur in isolation; instead, they frequently manifest simultaneously, posing significant challenges for Graph Neural Networks (GNNs) and necessitating robust learning frameworks capable of operating effectively under such weak information regimes. The ability to learn from imperfect data is paramount for the practical deployment of GNNs, where perfect datasets are rare and costly to acquire.

Addressing incomplete graph structures is a foundational challenge. Early approaches to infer missing structural information often focused on link prediction. For instance, the SEAL framework \cite{zhang2018kdl} introduced a novel $\beta$-decaying heuristic theory, theoretically justifying the learning of high-order heuristics from local enclosing subgraphs. This method is particularly valuable when global graph information is incomplete or noisy, as it allows GNNs to infer missing links by leveraging local structural patterns. More broadly, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) emerged as powerful unsupervised methods to reconstruct graph structures and learn latent representations by minimizing the reconstruction error of the adjacency matrix, effectively filling in missing links and denoise existing ones based on node features. However, these methods often assume relatively clean node features to infer structure.

Beyond simple missing links, real-world graphs can suffer from structural noise, where spurious or adversarial edges exist. Simultaneously, label scarcity is a common problem, limiting the supervision available for GNN training. The Robust GNN with Sparse Labels (RS-GNN) framework \cite{dai2022xze} directly tackles this dual challenge. RS-GNN proposes a unified approach that concurrently learns a denoised and densified graph structure while training a robust GNN classifier. It employs a novel MLP-based link predictor that leverages node attributes to assign small weights to noisy edges (connecting dissimilar nodes) and predict missing links between similar nodes, thereby both denoising and densifying the graph. This densification is crucial for increasing the involvement of unlabeled nodes in message passing, mitigating label sparsity. Furthermore, RS-GNN incorporates a feature similarity weighted edge-reconstruction loss and label smoothness regularization, explicitly guiding the learning process to be robust against structural noise and effective with limited labels. This represents a significant step towards integrated solutions for multiple data imperfections. Similarly, \cite{zhang2024370} explored the potential of Large Language Models (LLMs) to improve the adversarial robustness of GNNs, proposing LLM4RGNN. This framework distills the inference capabilities of LLMs to identify malicious edges and predict important missing edges, thereby recovering a more robust graph structure against topology perturbations, which can be viewed as a severe form of structural imperfection.

The challenge of limited labels is also extensively addressed through semi-supervised and self-supervised learning paradigms. Traditional label propagation techniques, which smooth labels over the graph structure, have seen a resurgence. For example, the Correct and Smooth (C\&S) method \cite{huang20209zd} demonstrated that combining shallow models (ignoring graph structure) with simple label propagation steps can match or exceed state-of-the-art GNNs on many transductive node classification benchmarks, particularly with sparse labels. C\&S effectively exploits label correlation by spreading residual errors and smoothing predictions, highlighting the power of directly incorporating label information. Building on this, self-training frameworks have gained traction. Distribution-Consistent Graph Self-Training (DC-GST) \cite{wang2024htw} addresses few-shot node classification with sparse labels by explicitly bridging the distribution shift between labeled and unlabeled nodes during self-training. It identifies informative pseudo-labeled nodes and uses a distribution-shift-aware edge predictor to augment the graph, enhancing generalizability and pseudo-label assignment. Furthermore, self-supervised GNNs, as explored by \cite{wei20246l2}, offer a powerful way to learn robust node representations from abundant unlabeled data, which is beneficial when both features are sparse and labels are scarce. By flexibly combining different types of attribute graph information, these models can mine deeper features, improving adaptability to complex and diverse graph data.

When multiple deficiencies—incomplete structure, sparse features, and limited labels—co-occur, integrated frameworks become essential. The Dual-channel Diffused Propagation then Transformation (D2PT) framework \cite{liu2023v3e} exemplifies such a multi-faceted approach. D2PT employs a dual-channel architecture, processing both the input graph and a dynamically learned global graph, and integrates prototype contrastive alignment to foster mutual benefits between these channels. This design enables GNNs to effectively handle simultaneously occurring data deficiencies and specifically addresses challenges like the 'stray node problem' in sparse graphs, ensuring strong performance even with extremely weak or noisy information. The strength of D2PT lies in its ability to leverage both local and global structural information, while contrastive learning helps in learning discriminative representations despite label scarcity and feature sparsity.

In summary, the literature demonstrates a clear progression from addressing individual data imperfections to developing sophisticated, integrated frameworks that can robustly learn from simultaneously occurring deficiencies across structure, features, and labels. While early methods focused on specific aspects like link prediction \cite{zhang2018kdl} or label propagation \cite{huang20209zd}, recent advancements like RS-GNN \cite{dai2022xze} and D2PT \cite{liu2023v3e} offer comprehensive solutions by jointly learning robust graph structures, rich node features, and accurate predictions under weak supervision. Future directions in this area include developing more adaptive strategies that can dynamically adjust to varying levels of data imperfection, exploring the theoretical guarantees for these complex joint optimization problems, and further integrating advanced AI paradigms like LLMs for robust graph structure inference \cite{zhang2024370}. Additionally, the development of comprehensive benchmarks for evaluating GNNs under label noise, such as NoisyGL \cite{wang2024481}, is crucial for ensuring fair comparisons and accelerating progress in this critical domain. These efforts collectively aim to enhance GNNs' resilience and practical utility in the pervasive imperfect data environments of the real world.