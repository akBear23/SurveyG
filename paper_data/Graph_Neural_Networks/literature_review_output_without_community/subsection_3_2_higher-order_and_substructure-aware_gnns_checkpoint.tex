\subsection{Higher-Order and Substructure-Aware GNNs}

The foundational Message Passing Neural Networks (MPNNs) are known to be limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \cite{xu2018powerful, morris2019weisfeiler}. This inherent limitation restricts their ability to distinguish between many non-isomorphic graph structures and capture intricate relational dependencies, thereby motivating extensive research into advanced GNN architectures designed to incorporate higher-order structural information and explicit subgraph patterns. The goal is to surpass the 1-WL expressivity limit, leading to more discriminative representations crucial for tasks requiring fine-grained structural understanding.

Early attempts to enhance GNN expressivity focused on extending the receptive field and implicitly capturing richer local information. A straightforward approach involves increasing the number of message passing layers, effectively allowing nodes to aggregate information from $K$-hop neighbors. However, the theoretical underpinnings of $K$-hop message passing were not fully characterized until recently. \cite{feng20225sa} provided a rigorous theoretical analysis, formally distinguishing between different $K$-hop kernels (shortest path distance vs. graph diffusion) and proving that $K$-hop message passing (for $K>1$) is strictly more powerful than 1-WL. Crucially, they demonstrated that while $K$-hop GNNs can distinguish almost all regular graphs, their expressive power is ultimately bounded by the 3-WL test. To further enhance this, they introduced the K-hop Peripheral-subgraph-enhanced Graph Neural Network (KP-GNN) framework, which integrates information from the "peripheral subgraph" (edges within a hop's neighbors), allowing for more expressive local structural features without significant computational overhead.

Beyond simple extensions of the receptive field, other methods sought to break the symmetries that limit 1-WL by introducing controlled randomness or explicit positional information. \cite{abboud2020x5e} made a surprising theoretical contribution by proving that standard MPNNs, when augmented with Random Node Initialization (RNI), become universal approximators for any function defined on graphs of a fixed order. This means RNI can overcome the 1-WL barrier without the prohibitive computational costs of higher-order GNNs (e.g., $k$-GNNs which operate on $k$-tuples of nodes). The universality is achieved because RNI individualizes input graphs with high probability, allowing MPNNs to capture arbitrary Boolean functions. While RNI can lead to slower convergence, partial randomization of initial node features was shown to improve practical performance. Similarly, \cite{papp20211ac} proposed DropGNN, a novel paradigm that increases expressiveness by executing multiple independent runs of a standard GNN on randomly perturbed versions of the input graph (nodes are dropped out with a certain probability). By aggregating embeddings from these diverse runs, DropGNN allows the GNN to observe various slightly altered neighborhood patterns, enabling it to distinguish graph structures otherwise indistinguishable by standard message-passing GNNs, with theoretical proofs demonstrating its expressiveness beyond the WL-test.

A more direct approach to substructure awareness involves explicitly encoding richer subgraph patterns. An early example, though primarily focused on link prediction, was the SEAL (Subgraph Embedding for Link prediction) framework by \cite{zhang2018kdl}. SEAL introduced the concept of learning general graph structure features from local enclosing subgraphs. By providing a $\beta$-decaying heuristic theory, it justified approximating high-order heuristics from relatively small $h$-hop enclosing subgraphs. This framework implicitly extended the GNN's receptive field by focusing on localized structural patterns and employed a novel structural node labeling scheme within these subgraphs, injecting explicit structural context.

Building upon the idea of leveraging subgraphs, \cite{zhang2021kc7} introduced Nested Graph Neural Networks (NGNNs). NGNNs adopt a two-level GNN architecture designed to encode *rooted subgraphs* rather than just rooted subtrees (which 1-WL and standard GNNs primarily encode). For each node, a local rooted subgraph is extracted, and an inner "base GNN" is applied independently to learn intermediate representations within that subgraph. These subgraph-level representations are then aggregated by an "outer GNN" to form the final node or graph representations. NGNNs are proven to be strictly more powerful than the 1-WL test and standard message passing GNNs, capable of discriminating almost all $r$-regular graphs while maintaining linear time and space complexity, addressing a critical trade-off between expressivity and efficiency.

More recently, \cite{zeng20237gv} introduced Substructure Aware Graph Neural Networks (SAGNNs) to explicitly inject higher-order substructural information. SAGNNs achieve this through two key innovations: a "Cut subgraph" extraction method, which identifies meaningful substructures by selectively removing edges with high Edge Betweenness Centrality, and an efficient random walk return probability encoding mechanism. This encoding leverages explicit path information within extracted subgraphs, providing a richer topological representation than traditional aggregation. The authors theoretically proved that SAGNNs enhance expressiveness beyond the 1-WL test, allowing them to distinguish complex graph structures that are isomorphic according to 1-WL but structurally distinct in more intricate ways, even outperforming models on graphs that fool the 3-WL test.

Collectively, these works illustrate a clear progression in GNN design towards a more discriminative understanding of complex graph structures. From the initial theoretical characterization of $K$-hop message passing and the clever use of randomization to break symmetries, to the explicit encoding of higher-order substructural patterns and path-based information, the field is moving beyond simple message passing. While approaches like RNI and DropGNN offer memory-efficient ways to enhance expressivity, methods like NGNNs and SAGNNs directly tackle the challenge by integrating richer, explicitly defined subgraph features. The ongoing challenge lies in balancing this enhanced expressivity with computational efficiency, ensuring generalization across diverse graph types and scales, and designing substructure features that are both informative and computationally tractable. Future research will likely continue to explore novel ways to efficiently identify, encode, and integrate higher-order structural motifs without incurring prohibitive computational costs, while also investigating the empirical gains of these provably powerful models on real-world benchmarks.