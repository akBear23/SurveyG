\subsection{Multivariate Time Series Analysis}

Multivariate time series analysis is a critical domain across various fields, including smart cities, environmental monitoring, finance, and healthcare, where accurately modeling intricate spatial-temporal dependencies among numerous interconnected variables is paramount \cite{jin2023ijy, sahili2023f2x}. Traditional methods, such as ARIMA, VAR, or even deep learning models like CNNs and RNNs, often struggle by treating variables independently or relying on implicit assumptions about their relationships. This limitation leads to suboptimal performance, particularly when the underlying system exhibits complex, non-Euclidean spatial structures \cite{jin2023ijy, jin2023e18}. Graph Neural Networks (GNNs) have emerged as a powerful paradigm to overcome these shortcomings, explicitly modeling the non-Euclidean spatial relationships among time series variables, such as sensors in a network, traffic flow between locations, or interactions in a biological system \cite{sahili2023f2x}. This explicit modeling offers significant advantages over conventional approaches, enabling a deeper understanding and more accurate analysis of complex, interconnected dynamics.

A fundamental and often challenging aspect of applying GNNs to multivariate time series is the construction of an appropriate graph structure that accurately reflects the underlying inter-variable dependencies. The evolution of graph construction techniques for time series data has progressed significantly:
\begin{enumerate}
    \item \textbf{Heuristic-based Graphs:} Early approaches frequently relied on readily available or simple heuristic-based graphs. These often derive from geographical proximity (e.g., Gaussian kernels based on distance), statistical similarities (e.g., Pearson correlation, Dynamic Time Warping (DTW), cosine similarity), or causal relationships (e.g., Granger causality, transfer entropy) \cite{jin2023ijy, jin2023e18}. While straightforward, these methods can be limited by their reliance on predefined rules, potentially missing latent or dynamic dependencies.
    \item \textbf{Learning-based Graphs:} The field has rapidly progressed towards more sophisticated learning-based methods that infer optimal graph structures directly from data, especially when explicit dependencies are unknown, highly dynamic, or too complex to be captured by simple heuristics \cite{jin2023ijy}. These approaches often leverage techniques such as:
    \begin{itemize}
        \item \textbf{Embedding-based methods:} Nodes (time series variables) are first embedded into a latent space, and then graph edges are inferred based on the similarity or distance between these embeddings.
        \item \textbf{Attention mechanisms:} Self-attention layers can learn dynamic connectivity weights between all pairs of nodes, effectively constructing a soft adjacency matrix that adapts to the input data.
        \item \textbf{Learnable adjacency matrices:} Some models directly optimize a learnable adjacency matrix, often initialized randomly or with a simple heuristic, and refined during training alongside the GNN parameters. For instance, \cite{wu2020hi3} proposed a general GNN framework for multivariate time series forecasting that includes a novel graph learning module. This module automatically extracts uni-directed relations among variables, allowing for the integration of external knowledge and addressing scenarios where dependencies are not known in advance.
        \item \textbf{Generative models:} More advanced techniques might employ generative models to infer graph structures that best explain the observed time series data.
    \end{itemize}
    Such learning-based approaches make GNNs highly adaptable to diverse and complex real-world systems by capturing implicit and dynamic relationships \cite{jin2023e18}.
\end{enumerate}

The integration of GNNs with temporal dynamics has led to the formalization of Spatio-Temporal Graph Neural Networks (STGNNs) or Temporal GNNs (TGNNs), specifically designed to handle dynamic graph structures and evolving relationships over time \cite{sahili2023f2x, longa202399q}. These models are crucial for capturing both static spatial relationships and their temporal evolution, making them suitable for a wide array of multivariate time series tasks including forecasting, classification, imputation, and anomaly detection \cite{jin2023ijy}.

\textbf{Pioneering STGNN Architectures:} Early influential STGNN models laid the groundwork by combining graph convolutions with various temporal modeling techniques:
\begin{itemize}
    \item \textbf{Spatio-Temporal Graph Convolutional Networks (STGCN)} \cite{yu2018spatio}: One of the earliest and most influential models, STGCN integrates graph convolutions with 1D convolutional neural networks (CNNs) to simultaneously capture spatial and temporal dependencies. While effective, STGCN often relies on a predefined or static graph, limiting its ability to adapt to evolving relationships.
    \item \textbf{Diffusion Convolutional Recurrent Neural Network (DCRNN)} \cite{li2018diffusion}: DCRNN leverages diffusion convolution to model spatial dependencies, capturing information propagation on the graph based on random walks. This is then integrated into a sequence-to-sequence recurrent neural network (RNN) architecture to handle temporal dynamics. DCRNN excels at modeling flow-like data but can suffer from the inherent limitations of RNNs, such as difficulties with long-range temporal dependencies and parallelization.
    \item \textbf{Graph WaveNet} \cite{wu2019graph}: This architecture enhances STGNNs by introducing an adaptive adjacency matrix, which is learned directly from the data, allowing it to capture implicit spatial dependencies beyond predefined graph structures. It combines this with dilated causal convolutions to model long-range temporal patterns, mitigating issues like vanishing gradients in deep temporal models. Graph WaveNet addresses the static graph limitation of STGCN and the long-range dependency issues of DCRNN, but its adaptive matrix can increase computational complexity.
\end{itemize}

\textbf{Recent Advancements: Transformer-based STGNNs:} While the pioneering models demonstrated significant improvements, the field has seen a shift towards Transformer-based architectures to better capture long-range temporal dependencies and dynamic spatial relationships. These models leverage self-attention mechanisms, which allow them to weigh the importance of different time steps and spatial neighbors dynamically, overcoming the limitations of fixed-size receptive fields in CNNs and sequential processing in RNNs. For instance, models integrate spatial graph attention with temporal self-attention, enabling more powerful and parallelizable learning of complex spatio-temporal patterns. An example in traffic prediction is the Dual Cross-Scale Transformer (DCST), which explicitly preserves "topology-free patterns" alongside graph-regularized ones, demonstrating the growing recognition that GNNs alone might not capture all relevant dynamics \cite{zhou2024t2r}. These Transformer-based STGNNs often achieve superior performance on many benchmarks, particularly for tasks requiring a global understanding of spatio-temporal dynamics and mitigating issues like over-squashing in deep graph structures.

The practical impact of GNNs in multivariate time series analysis is exemplified by real-world deployments. For instance, Google Maps has successfully deployed a GNN-based estimator for Estimated Time of Arrival (ETA) prediction \cite{derrowpinion2021mwn}. This system models road networks using "supersegments" and leverages sophisticated featurization combining real-time and historical traffic data with learnable embeddings. Crucially, it employs robust training regimes, including MetaGradients for dynamic learning rate tuning and semi-supervised methods like graph auto-encoders, to ensure stability and performance in a large-scale production environment. This deployment showcases the ability of GNNs to provide significant improvements over traditional methods, such as a 40\% reduction in negative ETA outcomes in cities like Sydney, highlighting their efficacy and scalability in critical applications \cite{derrowpinion2021mwn}. Beyond traffic, GNNs are also being applied to complex systems like global weather forecasting, demonstrating comparable skill to operational physical models \cite{keisler2022t7p}.

Despite significant progress, the field continues to evolve, facing ongoing challenges that are specific to STGNNs.
\begin{enumerate}
    \item \textbf{Scalability:} While GNNs offer powerful modeling capabilities, their application to extremely large time series datasets with massive, dynamic graphs and high update rates remains a key concern \cite{sahili2023f2x, cini2022pjy}. The computational complexity often scales quadratically with sequence length and graph size, hindering real-time applications. Efforts to address this include randomized RNNs and pre-computation techniques to improve efficiency \cite{cini2022pjy}, but handling massive, evolving graphs efficiently is still an active research area.
    \item \textbf{Interpretability:} Interpreting the decisions of STGNNs, especially those with dynamically learned graph structures or complex attention mechanisms, is crucial for trustworthy deployment \cite{jin2023ijy, longa202399q}. Understanding *why* a particular spatio-temporal dependency was learned or *how* it influenced a prediction is challenging, particularly when the underlying graph itself is a latent variable.
    \item \textbf{Robustness to Imperfect Data and Distribution Shifts:} Real-world time series data is often noisy, incomplete, or subject to sudden changes. STGNNs need to be robust to missing values, sensor failures, and, more critically, to *distribution shifts* in both temporal patterns and the underlying graph structure over long time horizons. A learned graph that is optimal for one period might become suboptimal or misleading in another.
    \item \textbf{Over-squashing:} In deep STGNNs, repeated message passing can lead to node representations becoming indistinguishable, a phenomenon known as over-squashing. This limits the effective receptive field and the ability to capture long-range spatio-temporal dependencies, which are often critical in multivariate time series.
\end{enumerate}
Future research directions will likely focus on developing more adaptive and efficient graph learning mechanisms that can handle dynamic graph topologies and attribute changes, enhancing the explainability of STGNN predictions, improving robustness to various data imperfections and distribution shifts, and integrating multi-modal data sources to further enrich the understanding of complex multivariate time series dynamics \cite{jin2023ijy, longa202399q, jin2023e18}.