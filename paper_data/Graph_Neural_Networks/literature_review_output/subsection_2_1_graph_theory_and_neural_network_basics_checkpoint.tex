\subsection*{Graph Theory and Neural Network Basics}

The analysis of structured data, particularly in the form of graphs, presents inherent challenges for conventional neural networks (NNs) which are primarily designed for processing Euclidean data with fixed-size, ordered inputs. Graph Neural Networks (GNNs) overcome this limitation by fundamentally integrating concepts from graph theory with the core principles of neural networks, thereby enabling models to learn rich representations directly from non-Euclidean graph structures \cite{wu2022ptq}. This subsection establishes the foundational understanding of both graph theory and neural networks, elucidating how their conceptual fusion forms the bedrock of GNN architectures.

Graph theory provides the mathematical language for describing relationships and interactions within complex systems. A graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ is formally defined by a set of nodes (or vertices) $\mathcal{V}$ and a set of edges $\mathcal{E}$ connecting pairs of nodes. Edges can be either undirected, signifying a symmetric relationship between two nodes, or directed, indicating an asymmetric flow of information or influence. Furthermore, graphs can be weighted, where each edge is associated with a numerical value representing the strength, cost, or capacity of the connection. The structural connectivity of a graph is typically represented by an adjacency matrix $\mathbf{A}$, where $\mathbf{A}_{ij} = 1$ if an edge exists between node $i$ and node $j$, and $0$ otherwise (or the edge weight for weighted graphs). Beyond these basic types, real-world graphs often exhibit greater complexity, including heterogeneous graphs, where nodes and edges can belong to different types (e.g., users and items in a recommender system), and attributed graphs, where nodes and/or edges are endowed with feature vectors that provide additional descriptive information. A comprehensive understanding of these diverse structural properties is indispensable, as GNNs are explicitly designed to leverage this connectivity and associated features to derive meaningful insights.

Neural networks, conversely, are powerful computational models renowned for their ability to approximate complex functions and learn hierarchical representations from data. At their core, NNs consist of multiple layers, each performing a linear transformation (a weighted sum of inputs, governed by learnable parameters or weights) followed by a non-linear activation function (e.g., ReLU, sigmoid, tanh). The network learns by iteratively adjusting these weights through an optimization process, typically stochastic gradient descent, guided by backpropagation to minimize a defined loss function. This layered architecture allows NNs to progressively extract abstract features from raw input data. However, the fundamental assumption of standard NNs—that inputs are fixed-size vectors with a predefined order—renders them incompatible with graph data. Graphs are inherently permutation-invariant (the order in which nodes are listed does not change the graph's structure) and variable-sized, making direct application of traditional NNs challenging.

The innovation of GNNs lies in adapting these fundamental neural network principles to operate directly on graph structures, thereby addressing the unique characteristics of non-Euclidean data \cite{wu2022ptq}. The concept of a neural network "layer" is reimagined in the graph domain. Instead of processing a flat vector, a GNN layer operates on node features within the context of their local graph neighborhood. The pioneering theoretical models, such as those proposed by \cite{Gori05} and further formalized by \cite{Scarselli09}, conceptualized GNNs as systems that iteratively propagate and aggregate information across the graph until node representations stabilize. This iterative process, often referred to as "message passing" or "neighborhood aggregation," forms the central mechanism of a generic GNN layer.

Abstractly, a GNN layer involves two primary steps for each node:
\begin{enumerate}
    \item \textbf{Aggregation}: Each node gathers information (or "messages") from its immediate neighbors. This aggregation function must be permutation-invariant, meaning the order in which neighbors' features are collected does not affect the outcome. Common abstract aggregation operations might include summation, averaging, or taking the maximum of neighbor features.
    \item \textbf{Update}: After aggregating information from its neighbors, a node combines this aggregated message with its own current feature representation. This combination typically involves a learnable transformation and a non-linear activation function, mirroring the operations within a standard neural network neuron. The output is a new, enriched representation for the node.
\end{enumerate}
This iterative application of aggregation and update steps across multiple layers allows GNNs to learn increasingly sophisticated and context-aware representations for each node, effectively encoding information from its multi-hop neighborhood into its feature vector. The parameters governing these aggregation and update functions within each graph layer are learnable and optimized end-to-end via backpropagation, just like in traditional neural networks. This adaptation enables GNNs to capture the intricate relational patterns and structural dependencies inherent in graph data, addressing the unique challenges posed by non-Euclidean structures and laying the groundwork for more advanced architectures discussed in subsequent sections.