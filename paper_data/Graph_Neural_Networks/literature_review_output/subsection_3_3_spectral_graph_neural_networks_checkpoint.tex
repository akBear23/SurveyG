\subsection{Spectral Graph Neural Networks}

Spectral Graph Neural Networks (GNNs) represent a foundational class of architectures that draw inspiration from spectral graph theory, analyzing graph signals in the frequency domain to capture global graph properties and long-range dependencies. This approach leverages the eigen-decomposition of the graph Laplacian, where eigenvectors form an orthonormal basis for graph signals, and eigenvalues correspond to frequencies, enabling principled filtering operations.

Early spectral convolutions, such as those pioneered by Bruna et al. \cite{bruna2013spectral}, directly applied filters in the spectral domain. While theoretically sound, these methods faced significant practical limitations: the prohibitive computational complexity of eigen-decomposition (typically $O(N^3)$ for dense graphs or $O(N^2)$ for sparse graphs) for large graphs, and the non-transferability of the learned spectral basis to graphs with different structures. To address these challenges, simplified polynomial approximations of spectral filters emerged. The most influential of these is the Graph Convolutional Network (GCN) by Kipf and Welling \cite{kipf2016semi}. GCNs approximate spectral filters with low-order polynomials, achieving remarkable computational efficiency and localization in the node space. Crucially, this spectral simplification translates directly into the widely adopted spatial message-passing update rule, establishing a profound duality between spectral theory and practical spatial GNNs.

While effective, these simplified polynomial filters inherently act as low-pass filters, leading to a critical challenge: over-smoothing. As GNNs deepen, node representations become increasingly indistinguishable, converging to a non-informative constant, which severely limits their expressive power for complex tasks \cite{rusch2023xev}. The theoretical underpinnings of this phenomenon were rigorously explored by Cai et al. \cite{cai2020k4b}, who demonstrated that the Dirichlet energy of node embeddings, a measure of their discriminative power in the frequency domain, exponentially converges to zero with increasing layers. Beyond over-smoothing, another related but distinct challenge is *over-squashing*, where the hierarchical aggregation of information through local neighborhoods leads to an exponential shrinkage of the effective receptive field, causing information from distant nodes to become diluted or bottlenecked due to the tree-like structure of message passing \cite{alon2020bottleneck}. Spectral methods, by their nature, can offer a principled way to mitigate over-squashing by providing a global perspective on information flow, effectively creating "shortcuts" in the frequency domain that bypass local topological bottlenecks. Architectural solutions to over-smoothing and over-squashing, such as decoupling propagation from transformation or using skip connections, are further discussed in Section 5.1.

To overcome the limitations of simple polynomial filters and enhance the frequency response, research has branched into several directions. Bianchi et al. \cite{bianchi20194ea} introduced Graph Neural Networks with Convolutional ARMA Filters. This work proposed a novel graph convolutional layer based on Auto-Regressive Moving Average (ARMA) filters, which offer a more versatile class of rational filters capable of modeling a wider variety of frequency responses and capturing longer dynamics with fewer parameters than traditional polynomial filters. By approximating the ARMA filter through a recursive update rule with skip connections, they mitigated over-smoothing and maintained computational efficiency.

Further theoretical investigations have delved into the expressive power of spectral GNNs. Wang et al. \cite{wang2022u2l} rigorously analyzed linear spectral GNNs, demonstrating that even without non-linearities, they can achieve universal approximation under specific conditions (e.g., no multiple eigenvalues, no missing frequency components). Their work connects these universality conditions to the discriminative power of the 1-Weisfeiler-Leman test, bridging spectral expressivity with graph isomorphism testing. They also provide an optimization perspective on why different polynomial bases, despite having the same expressive power, lead to varying empirical performance, advocating for orthogonal bases like Jacobi polynomials (used in their proposed JacobiConv) for faster convergence. This highlights that the choice of filter basis is not merely an implementation detail but impacts both theoretical guarantees and practical performance. From a generalization perspective, Wang et al. \cite{wang2024cb8} provided a manifold perspective, proving that generalization bounds of GNNs in the spectral domain decrease with graph size and increase with the spectral continuity constants of filter functions, offering insights into practical GNN design.

More recent advancements have sought to synergistically combine the strengths of local spatial message passing with global spectral filtering, or to design more adaptive and powerful spectral filters. Spatio-Spectral Graph Neural Networks (S2GNNs) \cite{geisler2024wli} exemplify this trend, proposing architectures that operate directly in the spectral domain while incorporating local spatial information. These methods leverage eigen-decomposition to provide a principled way to capture global graph properties and enhance expressivity for long-range interactions, directly mitigating the over-squashing problem by enabling global information flow.

Beyond fixed filter designs, the field is moving towards more flexible and adaptive spectral approaches. Bo et al. \cite{bo2023rwt} introduced Specformer, which encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. This approach moves beyond scalar-to-scalar functions and fixed-order polynomials, offering greater expressiveness and flexibility for non-local graph convolution. Similarly, for specific tasks like anomaly detection, Tang et al. \cite{tang2022g66} proposed the Beta Wavelet Graph Neural Network (BWGNN) with spectral and spatial localized band-pass filters. This design specifically addresses the "right-shift" phenomenon observed in anomalous graphs, where spectral energy concentrates more on high frequencies, demonstrating the utility of tailored spectral filters. Han et al. \cite{han2024rkj} further pushed the adaptivity, proposing Node-wise Filtering via Mixture of Experts (NODE-MOE) to dynamically apply distinct filters to individual nodes based on their specific structural patterns (homophilic or heterophilic). While NODE-MOE is a general adaptive filtering framework, its motivation to overcome the limitations of uniform global filters for mixed graph patterns is highly relevant to designing more versatile spectral filters that can adjust to local graph characteristics.

Novel mathematical foundations are also being explored to redefine signal propagation in the frequency domain. Kang et al. \cite{kang2024fsk} introduced the FRactional-Order graph Neural Dynamical network (FROND) framework, which generalizes continuous GNNs by incorporating Caputo fractional derivatives. This allows for modeling non-local, memory-dependent dynamics in graph signals, leading to an algebraic rate of convergence to stationarity, which inherently mitigates over-smoothing more effectively than traditional integer-order diffusion processes.

Addressing the critical scalability challenge of eigen-decomposition for large graphs, Li et al. \cite{li2022315} proposed GloGNN. While not strictly a spectral GNN in the traditional sense, GloGNN achieves spectral-like effects by learning a signed coefficient matrix for global aggregation. This allows it to implicitly combine low-pass and high-pass filtering effects to handle heterophily, and crucially, it achieves linear time complexity for global aggregation, circumventing the computational bottleneck of explicit eigen-decomposition for large graphs. This represents a pragmatic solution to achieve global information capture with spectral-like properties at scale.

In conclusion, spectral GNNs have evolved from direct but computationally intensive spectral filtering to efficient polynomial approximations, and further to more expressive rational filters, learnable set-to-set filters, and sophisticated hybrid architectures. These advancements, including S2GNNs, fractional calculus-based models, and adaptive filtering mechanisms, leverage the frequency domain to enhance expressivity for long-range interactions and provide a principled way to capture global graph properties, directly addressing challenges like over-smoothing and over-squashing. Despite these advancements, persistent challenges remain in developing truly adaptive spectral filters that can dynamically adjust to varying local graph structures and heterophily levels, and in addressing the fundamental scalability and transferability issues of explicit eigen-decomposition for very large and dynamic graphs. Future research will likely focus on more efficient approximations, hybrid spatial-spectral designs, and novel mathematical frameworks that offer the benefits of spectral analysis without its computational drawbacks.