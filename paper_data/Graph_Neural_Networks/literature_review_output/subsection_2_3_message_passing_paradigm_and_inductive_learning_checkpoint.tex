\subsection{Message Passing Paradigm and Inductive Learning}
The evolution of Graph Neural Networks (GNNs) marked a pivotal shift towards scalable and effective architectures rooted in the message-passing paradigm, fundamentally transforming how neural networks process graph-structured data. This paradigm, where nodes iteratively aggregate information from their neighbors, enabled GNNs to move beyond early theoretical models towards practical applications by defining a localized and computationally efficient mechanism for learning node representations.

A cornerstone of this transformation was the introduction of Graph Convolutional Networks (GCNs) by \cite{Kipf17}. This seminal work provided a highly efficient, localized, first-order approximation of spectral graph convolutions. Prior spectral methods, such as ChebyNet \cite{Defferrard16}, relied on computationally intensive eigen-decomposition of the graph Laplacian or polynomial filters, limiting their applicability to large graphs. GCNs ingeniously simplified this by restricting the convolutional filter to a first-order neighborhood and introducing a self-loop, effectively propagating and transforming node features through a layer-wise aggregation of neighbor information. This simplification made graph convolutions practical and scalable, establishing GCNs as a foundational baseline for semi-supervised node classification. From a unified optimization perspective, GCNs' propagation mechanism can be interpreted as an optimal solution to an objective function that balances feature fitting with graph Laplacian regularization, essentially performing a low-pass filtering operation on node features \cite{zhu2021zc3}. However, GCNs were primarily designed for transductive settings, requiring the full adjacency matrix during training, which limited their ability to generalize to unseen nodes or scale to very large, dynamic graphs.

Addressing these critical limitations, \cite{Hamilton17} introduced GraphSAGE (SAmple and aggreGatE), a framework specifically designed for inductive representation learning on large graphs. GraphSAGE innovated by proposing an efficient neighbor sampling strategy, allowing models to learn aggregation functions that generalize to unseen nodes and entire graphs without requiring the full graph structure at training time. By learning how to aggregate information from a sampled, fixed-size set of neighbors, GraphSAGE enabled the generation of node embeddings for new nodes by simply running the learned aggregation functions. This represented a significant step towards practical scalability and applicability in dynamic graph environments, where new nodes are constantly added. Subsequent work, such as GNNAutoScale \cite{fey2021smn}, has continued to build on these principles, developing frameworks to scale arbitrary message-passing GNNs to even larger graphs while provably maintaining expressive power by leveraging historical embeddings.

Further enhancing the flexibility and expressive power of the message-passing paradigm, \cite{Velickovic18} developed Graph Attention Networks (GATs). GATs introduced learned attention weights, allowing each node to assign varying importance to its neighbors during the aggregation process, rather than relying on fixed or uniformly weighted averages as in earlier GCN variants. This self-attention mechanism enables GATs to capture more nuanced relationships between nodes, providing greater flexibility in information aggregation and improving the model's ability to generalize to unseen nodes and graph structures by adaptively focusing on relevant neighbors. While GATs provided enhanced flexibility and often improved performance, this came at the cost of increased computational complexity and memory requirements during training and inference compared to simpler models like GCNs. The versatility of attention-based aggregation has been widely adopted, extending beyond traditional graph tasks to areas like robust feature matching in computer vision, as demonstrated by SuperGlue \cite{sarlin20198a6}.

While these models significantly advanced the practical utility of GNNs, their theoretical expressive power became a subject of critical investigation. \cite{xu2018c8q} provided a fundamental analysis, demonstrating that many popular message-passing GNN variants, including GCNs, GraphSAGE, and GATs, are at most as powerful as the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test. This work highlighted inherent limitations in their ability to distinguish certain complex non-isomorphic graph structures. To overcome this, the same paper introduced Graph Isomorphism Networks (GINs), which were provably shown to be maximally powerful within the message-passing framework by employing a learnable, injective aggregation function (e.g., a multi-layer perceptron). Despite GINs matching the 1-WL test's discriminative power, even these advanced GNNs still face representational limits; for instance, they cannot compute fundamental graph properties such as girth, diameter, or the number of k-cliques \cite{garg2020z6o}.

In conclusion, the message-passing paradigm, spearheaded by GCNs, GraphSAGE, GATs, and GINs, marked a transformative period in GNN research, enabling scalable, inductive, and expressive learning on graph-structured data. The discovery of the Weisfeiler-Lehman test's upper bound on expressivity, and the subsequent development of GINs to meet this bound, catalyzed a new line of inquiry focused on designing architectures that could systematically transcend the 1-WL test barrier. This ongoing quest for higher discriminative power, alongside efforts to address practical challenges like scalability and robustness, forms the core of the advancements explored in the subsequent sections.