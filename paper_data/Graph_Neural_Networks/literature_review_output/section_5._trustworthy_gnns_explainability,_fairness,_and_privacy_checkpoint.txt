\section*{5. Trustworthy GNNs: Explainability, Fairness, and Privacy}

The widespread adoption of Graph Neural Networks (GNNs) across sensitive domains, from healthcare and finance to social media analysis and cybersecurity, has elevated the concept of "trustworthiness" from a desirable feature to an indispensable requirement. While previous sections have explored the architectural foundations (Section 2), expressive power (Section 3), and robustness against adversarial attacks (Section 4) of GNNs, these advancements alone are insufficient for responsible deployment. Trustworthy GNNs must not only perform accurately and robustly but also be **explainable** (allowing humans to understand their decisions), **fair** (mitigating biases and ensuring equitable outcomes), and **private** (protecting sensitive information). This section delves into the burgeoning research dedicated to these three critical pillars of trustworthy GNNs, recognizing their paramount importance for fostering confidence and ethical use in real-world scenarios \cite{zhang20222g3, dai2022hsi}.

The pursuit of trustworthy GNNs represents a significant intellectual shift in the field, moving beyond purely performance-driven metrics to encompass ethical, societal, and practical considerations. Early GNN research primarily focused on enhancing predictive accuracy and scalability, often treating models as black boxes. However, the realization that GNNs can perpetuate and even amplify biases present in training data, make opaque decisions in critical applications, or inadvertently leak sensitive information, has spurred a concerted effort to imbue them with human-centric qualities. This shift is characterized by several key developments. Firstly, the demand for **Explainable Graph Neural Networks (XGNNs)** has led to diverse methodologies, ranging from post-hoc techniques that pinpoint crucial subgraphs or features for a given prediction to intrinsically interpretable architectures. This addresses the fundamental challenge of understanding *why* a GNN makes a particular decision, a critical step for debugging, auditing, and building user trust.

Secondly, the growing awareness of algorithmic bias has driven extensive research into **fairness and bias mitigation in GNNs**. GNNs, by their very nature of propagating information across a graph, can easily spread and reinforce biases embedded in node features or graph topology, leading to discriminatory outcomes for certain demographic groups. Research in this area explores pre-processing techniques to debias input data, in-processing methods that integrate fairness constraints into the training objective, and post-processing adjustments to model outputs. These efforts aim to ensure that GNNs operate equitably, preventing disparate impacts that could arise from training data disparities.

Finally, the inherent interconnectedness of graph data poses unique **privacy challenges**. Sensitive information about individuals (nodes) or their relationships (edges) can be inferred from GNN models or their outputs, even if raw data is not directly exposed. This has necessitated the development of **privacy-preserving GNNs**, employing techniques such as differential privacy, federated learning, and cryptographic methods to protect sensitive information during both training and inference. These advancements are crucial for deploying GNNs in domains like social networks, medical informatics, and financial transactions, where data privacy is a legal and ethical imperative.

Collectively, these three areas are not isolated but often interlinked, presenting complex trade-offs and unresolved tensions. For instance, enhancing privacy might inadvertently reduce model utility or make explanations harder to generate. Similarly, debiasing efforts could impact model accuracy or require access to sensitive attributes, raising privacy concerns. The field is actively grappling with these multifaceted challenges, striving to develop holistic frameworks that can simultaneously optimize for explainability, fairness, and privacy, thereby paving the way for the responsible and ethical deployment of GNN technology in real-world scenarios.

\subsection*{Explainable Graph Neural Networks (XGNNs)}

The inherent complexity and black-box nature of deep learning models, including Graph Neural Networks (GNNs), pose significant challenges for their adoption in high-stakes applications where transparency and accountability are paramount. Users and stakeholders often demand to understand *why* a GNN made a particular prediction, especially when those decisions impact individuals or critical systems. This necessity has given rise to the burgeoning field of Explainable Graph Neural Networks (XGNNs), which aims to shed light on the internal workings and decision-making processes of these models. The development of XGNNs builds upon the foundational GNN architectures discussed in Section 2 and complements the robustness efforts highlighted in Section 4, as understanding failure modes often requires interpretability.

**Context and Motivation:**
Traditional GNNs, with their iterative message-passing mechanisms, aggregate information from complex, non-Euclidean structures, making it difficult to trace the influence of specific input components on the final output. Unlike image or text data, where salient regions or words can be highlighted, graph explanations must consider both node features and the intricate topology. Early attempts at GNNs focused solely on predictive performance, but the demand for trust, debugging capabilities, and compliance with regulations (e.g., GDPR's "right to explanation") has spurred a significant research focus on interpretability. The core problem is to provide human-intelligible insights into a GNN's predictions, which, for graphs, often means identifying crucial subgraphs, nodes, edges, or features \cite{yuan2020fnk}.

**Method Family A: Post-hoc Local Explanations (Subgraph/Feature Importance)**
*   **Problem Solved:** This family of methods aims to explain individual GNN predictions by identifying the most influential input components (e.g., a critical subgraph, key nodes, or salient features) that led to a specific output. This addresses the need for instance-level transparency, allowing users to understand a single decision.
*   **Core Innovation & Mechanism:** These approaches typically operate *after* a GNN has been trained (post-hoc) and focus on local explanations for a specific input graph.
    *   **GNNExplainer \cite{ying2019rza}:** One of the pioneering works, GNNExplainer, formulates explanation as an optimization problem. For a given GNN prediction on an input graph, it learns a compact subgraph and a subset of node features that are most influential for that prediction. It achieves this by maximizing the mutual information between the masked subgraph (or features) and the GNN's prediction, effectively identifying a minimal, yet sufficient, explanatory subgraph.
    *   **SubgraphX \cite{yuan2021pgd}:** Building on the intuition that humans often understand graph decisions through coherent substructures, SubgraphX directly identifies important *connected* subgraphs. Its core innovation lies in employing Monte Carlo Tree Search (MCTS) to efficiently explore the vast space of possible subgraphs and leveraging Shapley values from cooperative game theory to quantify the importance of each subgraph. Shapley values inherently capture interactions among different graph components, providing a fair and theoretically grounded measure. To address the computational intractability of exact Shapley values, \cite{yuan2021pgd} proposes graph-specific approximation schemes, leveraging the L-hop receptive field of GNNs.
    *   **CF-GNNExplainer \cite{lucic2021p70}:** This method provides *counterfactual explanations* by identifying minimal changes to the input graph (e.g., adding/removing edges or changing node features) that would alter the GNN's prediction to a desired outcome. This offers insights into what *would have had to be different* for a different prediction, which can be very informative for understanding decision boundaries.
    *   **PGExplainer \cite{luo2024euy}:** This approach focuses on generating *inductive and efficient* explanations. Unlike GNNExplainer which trains a separate explainer for each GNN, PGExplainer learns a policy network that can generate explanations for *any* GNN, making it more generalizable and efficient for multiple models or unseen graphs.
    *   **GOAt \cite{lu2024eu9}:** Graph Output Attribution (GOAt) explains GNNs via structure-aware interaction index, focusing on how different parts of the graph contribute to the final output, particularly for graph-level tasks.
*   **Evidence:** GNNExplainer \cite{ying2019rza} demonstrated the ability to identify human-interpretable subgraphs. SubgraphX \cite{yuan2021pgd} empirically showed "significantly improved explanations" and "better explanations for a variety of GNN models" compared to existing methods, providing more coherent and relevant subgraphs. CF-GNNExplainer \cite{lucic2021p70} offers actionable insights by highlighting critical structural dependencies.
*   **Theoretical Limitations:** The "fidelity" of post-hoc explanations (how accurately they reflect the true model reasoning) is often not guaranteed. Shapley values, while theoretically sound, are approximations in practice for graphs. The definition of a "good" explanation remains subjective and domain-dependent, leading to challenges in rigorous evaluation \cite{agarwal2022xfp, chen2024woq}.
*   **Practical Limitations:** These methods can be computationally expensive, especially for large graphs or when exact Shapley values are desired. The interpretability of the identified subgraphs can vary, and human users might still struggle with complex patterns. The process often requires tuning specific parameters for the explainer itself.
*   **Comparison:** SubgraphX \cite{yuan2021pgd} distinguishes itself from GNNExplainer \cite{ying2019rza} by explicitly searching for *connected* subgraphs and using a game-theoretic approach (Shapley values) for importance, addressing the limitation that GNNExplainer's identified important nodes/edges might not form a coherent subgraph. PGExplainer \cite{luo2024euy} further improves efficiency and generalizability by learning an *inductive* explainer, contrasting with instance-specific explainers. The evolution shows a clear trend from simple feature importance to more complex structural (subgraph) and counterfactual explanations, aiming for deeper insights into GNN reasoning.

**Method Family B: Intrinsically Interpretable GNNs and Global Explanations**
*   **Problem Solved:** Instead of explaining individual predictions post-hoc, this family aims to design GNNs that are inherently interpretable or provide global insights into the model's general decision-making logic. This addresses the need for model-level transparency.
*   **Core Innovation & Mechanism:** These approaches embed interpretability directly into the GNN architecture or learn general patterns that characterize the model's behavior.
    *   **XGNN \cite{yuan20208v3}:** XGNN focuses on *model-level* explanations. It learns a policy network that generates graph structures (e.g., motifs, patterns) that are most likely to trigger a specific prediction from the target GNN. This allows users to understand what types of graph patterns the GNN generally responds to, providing a global perspective on its learned logic.
    *   **ProtGNN \cite{zhang2021wgf}:** ProtGNN is a prototype-based GNN that learns a set of representative prototypes (subgraphs) during training. When making a prediction, it explains the decision by identifying which prototypes are most similar to the input graph, providing a human-understandable "reason by example." This is similar to case-based reasoning.
    *   **KerGNNs \cite{feng2022914}:** Interpretable GNNs with Graph Kernels (KerGNNs) leverage the interpretability of graph kernels. By mapping graphs into a feature space where similarity is easily understood, KerGNNs aim to make the GNN's decision process more transparent through these kernel-based representations.
    *   **Invariant Rationales \cite{wu2022vcx}:** This work focuses on discovering invariant rationales for GNNs, aiming to identify stable and robust explanatory patterns that hold across different contexts or perturbations, contributing to more reliable explanations.
    *   **Global Interactive Patterns \cite{wang2024j6z}:** This research aims to unveil global interactive patterns across graphs, moving towards interpretable GNNs by identifying how different parts of the graph interact to influence overall predictions.
*   **Evidence:** XGNN \cite{yuan20208v3} provides insights into the types of graph structures that influence a GNN's decisions, which can be crucial for understanding model biases or vulnerabilities. ProtGNN \cite{zhang2021wgf} offers human-understandable prototype explanations, making the reasoning process transparent through concrete examples.
*   **Theoretical Limitations:** Intrinsically interpretable models often face a trade-off with model expressiveness and accuracy. Simpler, more transparent architectures might not capture the full complexity of real-world graph data as effectively as complex black-box GNNs. The definition of "global insight" can also be abstract and hard to quantify.
*   **Practical Limitations:** Learning prototypes or generating complex graph patterns (as in XGNN) can be computationally resource-intensive. The generalizability of global explanations to specific, nuanced local predictions might be limited.
*   **Comparison:** This family contrasts sharply with post-hoc methods. While post-hoc methods (e.g., SubgraphX \cite{yuan2021pgd}) explain *a specific prediction*, intrinsically interpretable models (e.g., ProtGNN \cite{zhang2021wgf}) or global explainers (e.g., XGNN \cite{yuan20208v3}) aim to explain *the model itself* or its general behavior. XGNN \cite{yuan20208v3} provides *model-level* explanations by generating patterns, which is distinct from *instance-level* explanations.

**Synthesis and Implications:**
The field of XGNNs is characterized by a fundamental tension between the fidelity of explanations (how accurately they reflect the model's true reasoning) and their interpretability (how easily humans can understand them). The evolution shows a clear trend from simple feature importance to more sophisticated structural (subgraph) and counterfactual explanations, and from instance-level insights to model-level understanding. A major unresolved debate is the lack of standardized, objective metrics for evaluating the "goodness" of explanations \cite{agarwal2022xfp}, making direct comparisons challenging. Most papers implicitly assume that providing explanations will lead to increased trust, but the cognitive aspects of human-AI interaction with XGNNs are still underexplored. Future work needs to focus on more robust and generalizable explanation methods, better evaluation protocols, and potentially hybrid approaches that combine the strengths of both local and global explanations.

\subsection*{Fairness and Bias Mitigation in GNNs}

The increasing deployment of GNNs in critical decision-making processes, such as loan default prediction \cite{zandi2024dgs}, credit risk assessment \cite{liu2024sbb}, and recommender systems \cite{gao2022f3h}, necessitates a rigorous examination of their fairness. GNNs, by their very nature of propagating information across interconnected entities, are particularly susceptible to learning and amplifying biases present in the training data, leading to discriminatory outcomes. This section explores the crucial research dedicated to ensuring fairness and mitigating bias in GNNs, building upon the broader discussion of data imperfections and robustness from Section 4. The goal is to ensure that GNNs operate equitably, preventing disparate impacts that can arise from training data disparities or structural inequalities in the graph.

**Context and Motivation:**
Bias in GNNs can manifest in several ways: (1) **Node feature bias:** Disparities in attributes (e.g., sensitive demographic information) can lead to biased representations. (2) **Structural bias:** Graph topology itself might reflect societal biases (e.g., homophily leading to segregation, or certain groups having fewer connections). (3) **Algorithmic bias:** The GNN's learning process might inadvertently amplify these biases during message passing. The consequences of unfair GNNs can be severe, ranging from perpetuating social inequalities to eroding public trust. Therefore, developing techniques to identify, measure, and mitigate these biases is paramount for the ethical and responsible deployment of GNNs. A key challenge is the trade-off between achieving fairness and maintaining predictive utility, as debiasing often comes with a performance cost.

**Method Family A: Pre-processing and In-processing Debiasing**
*   **Problem Solved:** This family aims to mitigate biases in GNN predictions by either modifying the input data before training (pre-processing) or incorporating fairness-aware mechanisms directly into the GNN's training process (in-processing). This addresses the root causes of bias by intervening early in the model lifecycle.
*   **Core Innovation & Mechanism:**
    *   **Pre-processing: Data Debiasing and Graph Rewiring:**
        *   **EDITS \cite{dong2021qcg}:** This method focuses on modeling and mitigating data bias for GNNs. It identifies and corrects biased information propagation by analyzing the underlying data distribution and adjusting it to reduce disparities before the GNN processes it. This can involve re-weighting nodes or edges, or modifying node features to be less correlated with sensitive attributes.
        *   **Graph Rewiring for Fairness:** Similar to robustness efforts in Section 4 \cite{shen2024exf}, graph rewiring can be used to create a more balanced or less biased graph structure. This might involve adding connections between underrepresented groups or removing spurious links that perpetuate bias.
    *   **In-processing: Fairness-Aware GNN Training:** These methods integrate fairness considerations directly into the GNN's learning objective or architecture.
        *   **Fair GNNs with Limited Sensitive Attribute Information \cite{dai2020p5t}:** This work addresses the practical challenge where sensitive attribute information might be limited or unavailable. It proposes methods to learn fair GNNs by inferring or approximating sensitive attributes, or by using proxy features, and then applying debiasing techniques.
        *   **Individual Fairness for GNNs \cite{dong202183w}:** This paper focuses on *individual fairness*, ensuring that similar individuals (nodes) receive similar predictions, regardless of their sensitive attributes. It proposes a ranking-based approach to achieve this, aiming for consistency in outcomes for comparable entities.
        *   **Debiasing via Learning Disentangled Causal Substructure \cite{fan2022m67}:** This innovative approach aims to debias GNNs by learning disentangled causal substructures. The core idea is to separate features that causally influence the outcome from those that are merely correlated with sensitive attributes (confounders). By disentangling these factors, the GNN can make predictions based on causal relationships, reducing spurious bias.
        *   **Rethinking Fair GNNs from Re-balancing \cite{li20245zy}:** This work re-examines fairness through a re-balancing lens, proposing techniques that adjust the weights of samples or the aggregation process during training to ensure that different groups are treated equitably. This can involve re-weighting loss contributions or modifying message passing to counteract biased information flow.
        *   **Mitigating Sensitive Attribute Leakage \cite{wang2022531}:** This research focuses on preventing GNNs from inferring sensitive attributes from non-sensitive features or graph structure and then using this leaked information to make biased predictions. It proposes mechanisms to obscure or decorrelate sensitive information during representation learning.
*   **Evidence:** Pre-processing methods like EDITS \cite{dong2021qcg} have shown effectiveness in reducing bias metrics while maintaining competitive utility. In-processing methods, such as those by \cite{fan2022m67} and \cite{li20245zy}, demonstrate significant improvements in various fairness metrics (e.g., demographic parity, equal opportunity) on benchmark datasets, often with a controlled trade-off in accuracy. \cite{wang2022531} shows that mitigating attribute leakage can lead to fairer outcomes.
*   **Theoretical Limitations:** Defining and measuring fairness in complex graph contexts is inherently challenging, with multiple, sometimes conflicting, definitions (e.g., individual vs. group fairness, disparate treatment vs. disparate impact). The underlying causal mechanisms of bias in graphs are often difficult to model explicitly, making disentanglement complex \cite{fan2022m67}. The trade-off between fairness and utility is a persistent theoretical tension \cite{luo20240ot}.
*   **Practical Limitations:** Many methods require access to sensitive attribute information, which might be unavailable, incomplete, or legally restricted (e.g., due to privacy concerns). Implementing sophisticated debiasing techniques can add computational overhead and complexity to GNN training. The generalizability of debiasing methods across different graph types and bias definitions is also an open question.
*   **Comparison:** Pre-processing methods (e.g., EDITS \cite{dong2021qcg}) are generally simpler to implement but might not fully address biases introduced during the GNN's learning process. In-processing methods (e.g., \cite{fan2022m67, li20245zy, wang2022531}) offer more fine-grained control over bias mitigation by integrating fairness directly into the model's learning, but are often more complex. The evolution shows a clear trend towards more sophisticated in-processing methods that aim to address bias at a deeper, causal level.

**Method Family B: Post-processing Debiasing**
*   **Problem Solved:** Adjusting the predictions of a trained GNN *after* inference to achieve fairer outcomes, without modifying the model itself. This addresses scenarios where model re-training is not feasible or sensitive attributes are only available post-inference.
*   **Core Innovation & Mechanism:** These methods typically involve re-calibrating prediction scores or adjusting decision thresholds based on fairness criteria. For example, if a model shows disparate impact for a certain group, its predictions for that group might be adjusted to align with a desired fairness metric (e.g., equalizing false positive rates across groups).
*   **Evidence:** Can be effective in simple, well-defined scenarios to achieve specific fairness metrics.
*   **Theoretical Limitations:** Post-processing cannot correct fundamental biases learned by the GNN during training; it merely adjusts the output. This means the underlying biased reasoning of the model remains. It also often requires access to sensitive attributes to perform the adjustment.
*   **Practical Limitations:** The adjustments can sometimes be non-transparent, obscuring the original model's reasoning. It might not generalize well across different fairness metrics or complex decision landscapes.
*   **Comparison:** Post-processing is generally the least invasive but also the least powerful debiasing strategy. It's a reactive measure, contrasting with the proactive nature of pre-processing and in-processing methods that aim to prevent bias from being learned in the first place.

**Synthesis and Implications:**
The field of fairness in GNNs is rapidly evolving, moving from simple demographic parity adjustments to more nuanced approaches that consider individual fairness \cite{dong202183w}, causal disentanglement \cite{fan2022m67}, and sensitive attribute leakage \cite{wang2022531}. A critical tension exists between achieving different fairness notions, as optimizing for one (e.g., equal opportunity) might degrade another (e.g., demographic parity). The fundamental trade-off between fairness and accuracy remains a central challenge, with researchers continually seeking methods that minimize this conflict. Most research implicitly assumes that sensitive attributes are well-defined, but in reality, these can be fluid and context-dependent. Future research needs to focus on robust causal inference in graphs, developing methods that work with limited or no sensitive attribute information, and creating unified frameworks that can navigate the complex interplay between different fairness definitions and other trustworthiness dimensions like explainability and privacy.

\subsection*{Privacy-Preserving Graph Neural Networks}

The interconnected nature of graph data, which often represents relationships between individuals, organizations, or sensitive entities, makes privacy a paramount concern for Graph Neural Networks (GNNs). Deploying GNNs in domains like social networks, medical records, financial transactions, or even cybersecurity \cite{mitra2024x43, bilot20234ui} risks exposing sensitive information about nodes (individuals) or edges (relationships). Adversaries might attempt to infer private attributes, reconstruct parts of the graph, or even identify individuals from the GNN model or its outputs. This section examines the critical efforts to enhance privacy in GNNs, including methods to protect sensitive information during training and inference, thereby fostering greater confidence and ethical use of GNN technology. This area builds upon the general architectural principles of GNNs (Section 2) and is closely related to the robustness against adversarial attacks (Section 4), as some privacy attacks are essentially inference attacks.

**Context and Motivation:**
The message-passing paradigm, which aggregates information from neighbors, is a double-edged sword: while it enables powerful representation learning, it also facilitates the propagation and potential leakage of sensitive information. For example, a GNN trained on a social network might inadvertently reveal a user's political affiliation or health status based on their connections and interactions. Traditional data anonymization techniques often fall short for graphs, as structural patterns can still enable re-identification. Therefore, specialized privacy-preserving techniques are essential to safeguard sensitive information, comply with regulations (e.g., GDPR, HIPAA), and maintain user trust. The central challenge often revolves around balancing strong privacy guarantees with acceptable model utility and computational efficiency.

**Method Family A: Differential Privacy (DP) for GNNs**
*   **Problem Solved:** Differential Privacy (DP) provides a strong, mathematically rigorous guarantee that the presence or absence of any single individual's data in the training set does not significantly alter the model's output. For GNNs, this means protecting individual node features or edges from being inferred from the trained model.
*   **Core Innovation & Mechanism:** DP is typically achieved by injecting calibrated noise into the learning process.
    *   **DP-SGD for GNNs:** The most common approach involves adapting Differentially Private Stochastic Gradient Descent (DP-SGD) to GNN training. During each training step, noise is added to the gradients computed for each mini-batch before they are used to update the model parameters. This ensures that the gradient contribution of any single data point is obscured.
    *   **Output Perturbation:** Alternatively, noise can be added to the final node embeddings or predictions generated by the GNN. This provides privacy for the model's outputs but does not protect the training process itself.
*   **Evidence:** DP-GNNs offer quantifiable privacy guarantees, expressed by a privacy budget ($\epsilon$). A smaller $\epsilon$ implies stronger privacy. While specific quantitative results vary, DP has been shown to provide strong protection against various inference attacks, including membership inference and attribute inference.
*   **Theoretical Limitations:** DP often introduces a significant trade-off with model utility (accuracy). Stronger privacy guarantees (smaller $\epsilon$) typically lead to a more substantial degradation in performance. The optimal amount and type of noise to add, especially for graph structures, is an active research area. Applying DP to graph topology (edges) is more complex than to node features, as small changes in structure can have a large impact on graph properties.
*   **Practical Limitations:** DP-SGD can be computationally more expensive than standard SGD due to the per-example gradient clipping and noise addition. Tuning the privacy parameters (e.g., $\epsilon$, noise scale) requires expertise and careful consideration of the privacy-utility trade-off. The utility degradation can make DP-GNNs less competitive with non-private counterparts in some tasks.
*   **Comparison:** DP offers the strongest theoretical privacy guarantees among the methods discussed, making it a gold standard for privacy. However, this often comes at a higher cost in terms of model utility and computational overhead compared to federated learning, which focuses on data locality rather than noise injection.

**Method Family B: Federated Learning (FL) for GNNs**
*   **Problem Solved:** Federated Learning enables the collaborative training of a global GNN model across multiple decentralized data sources (clients) without requiring clients to share their raw graph data. This protects data locality and individual privacy by keeping sensitive information on local devices.
*   **Core Innovation & Mechanism:** The FL paradigm is adapted for graph-structured data.
    *   **FedGraphNN \cite{he2021x8v}:** This work proposes a comprehensive federated learning system and benchmark specifically for GNNs. It addresses challenges unique to FL on graphs, such as data heterogeneity (clients having different graph structures or feature distributions), communication efficiency (reducing the amount of data exchanged between clients and server), and the aggregation of graph-specific model updates. Clients train local GNNs on their private subgraphs or full graphs, and only model parameters or gradients are sent to a central server for aggregation.
    *   **Graph-specific FL Challenges \cite{liu2022gcg}:** A survey by \cite{liu2022gcg} highlights several challenges: (1) **Non-IID data:** Graph data is often inherently non-IID across clients, which can degrade FL performance. (2) **Graph partitioning:** How to partition a large graph across clients while preserving useful local structure is crucial. (3) **Communication overhead:** Aggregating GNN models can still be substantial. (4) **Privacy attacks:** FL alone is not immune to inference attacks (e.g., reconstructing data from shared gradients) or poisoning attacks by malicious clients, often necessitating additional privacy mechanisms like DP or secure aggregation.
*   **Evidence:** FL-GNNs enable collaborative model training in scenarios where data cannot be centrally collected due to privacy regulations or logistical constraints (e.g., training a GNN for disease prediction across multiple hospitals, or for traffic prediction across different city departments).
*   **Theoretical Limitations:** While FL prevents direct sharing of raw data, it does not provide the same strong, provable privacy guarantees as DP against sophisticated inference attacks on gradients or model updates. The effectiveness of FL is highly dependent on the degree of data heterogeneity across clients.
*   **Practical Limitations:** Communication overhead can still be significant for large GNN models or frequent updates. Managing and synchronizing training across many clients can be complex. The system is vulnerable to malicious clients who might send poisoned updates or attempt to infer information from aggregated models.
*   **Comparison:** FL provides a practical approach to privacy by decentralizing data, contrasting with DP's noise injection. It is particularly relevant for scenarios where data cannot be centrally collected. However, FL often needs to be combined with other PETs (like DP) to achieve stronger privacy guarantees against advanced attacks.

**Method Family C: Other Privacy-Enhancing Technologies (PETs) and Anonymization**
*   **Problem Solved:** Protecting specific sensitive information (e.g., link existence, node identity, attribute values) through cryptographic methods or data transformation, often providing very strong privacy guarantees.
*   **Core Innovation & Mechanism:**
    *   **Homomorphic Encryption (HE):** HE allows computations to be performed on encrypted data without decrypting it. A GNN could theoretically operate on encrypted node features and adjacency matrices, producing encrypted predictions. This means a server could train or infer with a GNN without ever seeing the raw sensitive data.
    *   **Secure Multi-Party Computation (SMC):** SMC enables multiple parties to jointly compute a function over their private inputs while ensuring that no party learns anything about the other parties' inputs beyond what can be inferred from the function's output. This could be used for private aggregation of gradients or features in a GNN.
    *   **Graph Anonymization:** This involves modifying the graph structure or node attributes to prevent re-identification of individuals or sensitive links. Techniques include k-anonymity (ensuring each node is indistinguishable from at least k-1 other nodes), edge generalization, or adding/removing edges to obscure sensitive patterns.
*   **Evidence:** HE and SMC offer cryptographic-level privacy guarantees, making them highly secure against various attacks. Graph anonymization can be effective in preventing re-identification attacks.
*   **Theoretical Limitations:** HE and SMC are notoriously computationally expensive, often increasing computation time by several orders of magnitude, making them currently impractical for training large-scale GNNs. Graph anonymization, while effective for re-identification, often significantly distorts the original graph properties, leading to a substantial degradation in model utility and potentially altering the underlying graph semantics.
*   **Practical Limitations:** The high computational cost and complexity of implementing HE and SMC are major barriers to their widespread adoption for GNNs. Graph anonymization techniques require careful tuning to balance privacy and utility, and the choice of anonymization strategy can heavily influence the downstream GNN performance.
*   **Comparison:** These methods offer the strongest privacy guarantees, often cryptographic, but come with the highest computational and implementation costs. They represent the "gold standard" of privacy but are far from scalable for current GNNs. They contrast with DP and FL by offering a different paradigm for privacy, focusing on cryptographic protection or structural obfuscation rather than noise injection or data decentralization.

**Synthesis and Implications:**
The field of privacy-preserving GNNs is characterized by a fundamental and persistent trade-off between privacy guarantees and model utility/efficiency. The evolution shows a spectrum of solutions, from strong theoretical guarantees of DP, through the practical distributed training of FL, to the high-security but computationally intensive cryptographic methods. A major unresolved challenge is developing scalable and efficient privacy-preserving GNNs that can achieve strong privacy without severely compromising performance. Most research implicitly assumes that the adversary model is well-defined, but real-world attacks can be more sophisticated. Future research needs to focus on hybrid approaches that combine the strengths of different PETs (e.g., FL with DP or secure aggregation), developing more efficient cryptographic schemes, and creating robust privacy-preserving methods that can handle the unique complexities of dynamic and heterogeneous graph data. The interplay between privacy, fairness, and explainability is also a critical area for future investigation, as optimizing for one dimension might inadvertently impact the others.