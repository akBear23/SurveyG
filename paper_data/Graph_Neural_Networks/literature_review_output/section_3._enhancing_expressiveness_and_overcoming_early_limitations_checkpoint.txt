\section*{3. Enhancing Expressiveness and Overcoming Early Limitations}

The foundational Graph Neural Network (GNN) architectures, as discussed in Section 2, laid the groundwork for learning representations on graph-structured data by extending the principles of deep learning to non-Euclidean domains. Models like Graph Convolutional Networks (GCNs) \cite{kipf2016semi} and Graph Attention Networks (GATs) \cite{velickovic2017graph} demonstrated remarkable success in tasks such as node classification and link prediction. However, these early models, largely rooted in the message-passing paradigm, quickly encountered inherent limitations that constrained their representational power, robustness, and applicability to the full spectrum of real-world graph complexities. A critical theoretical bottleneck, as established by \cite{xu2018c8q} and \cite{morris20185sd}, was their equivalence to the 1-Weisfeiler-Leman (1-WL) graph isomorphism test, which severely restricted their ability to distinguish between structurally distinct non-isomorphic graphs. This limitation meant that many GNNs could not capture intricate structural patterns, such as specific cycle counts or higher-order motifs, which are crucial for understanding complex systems \cite{chen2020e6g, oono2019usb}.

Beyond expressiveness, two significant practical challenges emerged as the field sought to apply GNNs to more complex and larger datasets. Firstly, the phenomenon of **over-smoothing** became a pervasive issue, particularly when attempting to build deeper GNN architectures \cite{chen2019s47, cai2020k4b}. As information propagated through multiple layers via repeated neighborhood aggregation, node representations tended to converge, becoming indistinguishable and losing their unique discriminative power. This directly contradicted the success of deep neural networks in other domains and fundamentally limited the effective depth of GNNs. Secondly, the implicit **homophily assumption**, where connected nodes are expected to be similar, proved to be a major impediment for real-world graphs exhibiting **heterophily** \cite{ma2021sim, zhu2020c3j}. In such graphs, connected nodes often possess dissimilar features or labels (e.g., a student connected to a professor in a social network), and traditional GNNs, designed to smooth features across neighbors, performed poorly.

This section delves into the crucial advancements that have pushed GNNs beyond these initial limitations, marking a significant intellectual evolution in the field. It explores the development of sophisticated mechanisms designed to enhance representational power, moving beyond the 1-WL test by incorporating higher-order information or entirely novel non-convolutional paradigms. A substantial focus is placed on the diverse strategies developed to tackle the over-smoothing problem, ranging from architectural modifications to more fundamental re-thinking of information dynamics, thereby enabling the construction of deeper, more effective GNNs. Furthermore, the section addresses the critical challenge of adapting GNNs to diverse graph structures, particularly those exhibiting heterophily, moving beyond the traditional homophily assumption to handle the inherent complexities of real-world networks. This progression reflects a field grappling with the fundamental trade-offs between expressiveness, efficiency, and generalizability, continuously seeking to build models that are not only powerful but also robust and adaptable across a myriad of applications, from recommender systems \cite{gao2022f3h} to computer vision \cite{chen2022mmu}. The innovations discussed here represent a collective effort to unlock the full potential of GNNs, transforming them from promising initial models into versatile and robust tools capable of tackling the intricate challenges posed by complex graph data.

\subsection*{Higher-Order and Advanced Message Passing Mechanisms}

The inherent limitations of early Graph Neural Networks (GNNs) in capturing complex structural information, primarily due to their equivalence to the 1-Weisfeiler-Leman (1-WL) graph isomorphism test, became a significant bottleneck for their representational power \cite{xu2018c8q, morris20185sd}. As discussed in Section 2, models like GCNs \cite{kipf2016semi} and GATs \cite{velickovic2017graph} struggle to distinguish between certain non-isomorphic graphs, such as regular graphs with the same degree sequence but different cycle structures \cite{chen2020e6g}. This fundamental constraint spurred extensive research into designing higher-order and advanced message-passing mechanisms that could surpass the 1-WL barrier, thereby enhancing GNNs' expressiveness and their ability to learn richer, more discriminative graph representations.

One prominent family of approaches to enhance expressiveness involves directly mimicking **higher-order Weisfeiler-Leman (k-WL) tests**.
*   **Context**: The motivation here is to overcome the 1-WL limitation by considering not just individual nodes and their immediate neighbors, but larger structural units like k-tuples of nodes.
*   **Mechanism**: These "k-GNNs" or "k-WL GNNs" operate on sets of k-nodes, where messages are passed between these k-tuples. For example, a 2-WL GNN would consider pairs of nodes and their relationships, allowing it to distinguish graphs that 1-WL cannot, such as non-isomorphic regular graphs \cite{morris20185sd}. The aggregation function is extended to combine information from neighboring k-tuples.
*   **Core Innovation**: The direct increase in the theoretical discriminative power by operating on more complex structural primitives (k-tuples) rather than just single nodes.
*   **Theoretical Limitations**: While theoretically more expressive, achieving k-WL power comes at a steep computational cost. The number of k-tuples grows polynomially with the number of nodes ($N^k$), making these models prohibitively expensive for large graphs even for small $k$. This scalability issue is a significant practical barrier, limiting their widespread adoption.
*   **Practical Limitations**: The increased complexity also makes these models harder to implement and train, requiring specialized data structures and aggregation schemes.
*   **Comparison**: These methods directly address the expressiveness problem by elevating the "resolution" of graph comparison, but they introduce a severe trade-off with computational efficiency. This exemplifies a recurring tension in GNN research: the pursuit of higher expressiveness often comes at the expense of scalability.

A distinct and increasingly influential approach to enhancing expressiveness focuses on incorporating **path information**. Paths inherently encode richer, non-local structural relationships that go beyond immediate neighborhood aggregates.
*   **Context**: Building on the understanding that local message passing is insufficient for complex structural patterns, researchers sought to leverage longer-range dependencies.
*   **Method**: **Path Neural Networks (PathNNs)**, introduced by \cite{michel2023hc4}, exemplify this direction. PathNNs update node representations by aggregating information from various paths emanating from each node.
*   **Mechanism**: For each path length, a recurrent layer (e.g., RNN) is used to encode paths into vectors. The crucial innovation lies in operating on "annotated sets of paths" ($\tilde{SP}$, $\tilde{SP+}$, $\tilde{AP}$). Nodes within these paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths. This creates a richer, hierarchical structural encoding that captures context beyond simple feature aggregation. For instance, the All Simple Paths ($\tilde{AP}$) variant aggregates all simple paths up to a fixed length $K$.
*   **Evidence**: \cite{michel2023hc4} formally demonstrated that `AP-Trees` are strictly more powerful than `WL-Trees`. More importantly, PathNN variants operating on annotated sets of paths are strictly more powerful than the 1-WL algorithm. The most expressive variant, $\tilde{AP}$, was empirically shown to distinguish graphs that are even 3-WL indistinguishable, a significant leap in discriminative power. This was validated on synthetic datasets designed to test expressive power and on real-world graph classification and regression tasks, where PathNNs achieved high performance.
*   **Theoretical Limitations**: While powerful, finding all simple paths in a graph is NP-hard. PathNNs address this by fixing a maximum path length $K$, which is a practical constraint. The theoretical proofs for expressiveness rely on the recursive annotation scheme, which is computationally intensive for long paths or dense graphs.
*   **Practical Limitations**: The scalability of the $\tilde{AP}$ variant can be challenging, as the number of simple paths can grow exponentially with length $K$ and graph density. This limits the practical maximum path length that can be effectively used.
*   **Comparison**: PathNNs offer a compelling, path-centric alternative to k-WL GNNs. While both aim for higher expressiveness, PathNNs achieve this by explicitly modeling sequential structural information, providing a different lens than k-tuples. This approach highlights a methodological trend towards incorporating more explicit structural knowledge into GNN architectures, moving beyond implicit aggregation.

A more radical departure from the traditional message-passing paradigm is the development of **non-convolutional GNNs**. These models fundamentally rethink how information is propagated and aggregated, aiming to overcome not only expressiveness limitations but also other pathologies like over-smoothing and over-squashing.
*   **Context**: The **Random Walk with Unifying Memory (RUM) neural network** \cite{wang2024oi8} directly confronts the fundamental technical problems of *convolution-based* GNNs: limited expressiveness (beyond 1-WL), over-smoothing, and over-squashing (difficulty learning long-range dependencies due to information bottleneck). RUM argues that convolution-based GNNs inherently suffer from these issues, necessitating a completely different approach.
*   **Mechanism**: RUM is an entirely *convolution-free* architecture. Instead of aggregating messages from direct neighbors, it stochastically samples finite-length random walks for each node. An RNN (specifically a GRU) then processes the sequence of node embeddings along these walks. This RNN merges "semantic representations" (node features) with "anonymous experiment" features, which describe the *topological environment* of a walk by recording the first unique occurrence of a node. Node representations are then formed by averaging the unifying memories of walks terminating at that node, and graph representations are summed from these node representations.
*   **Core Innovation**: The "anonymous experiment" is a novel method for encoding topological context, and the use of RNNs to process random walk trajectories provides a fundamentally different and non-local information propagation mechanism. The architecture is designed to jointly remedy the three pathologies.
*   **Theoretical Contributions**: \cite{wang2024oi8} theoretically demonstrates that RUM is more expressive than the 1-WL test, capable of distinguishing non-isomorphic graphs that 1-WL equivalent GNNs cannot (e.g., cycle sizes, radius). This is a significant theoretical claim, validated empirically on synthetic datasets. Furthermore, RUM is theoretically and experimentally shown to attenuate over-smoothing (its expected Dirichlet energy does not diminish even with long walks) and alleviate over-squashing (slower decay in inter-node Jacobian).
*   **Practical Advantages**: RUM is shown to be faster and more scalable than existing convolutional and many walk-based GNNs, with a runtime complexity of $O(|V|lkD)$ that is agnostic to the number of edges $|E|$. It is naturally compatible with mini-batching.
*   **Theoretical Limitations**: The theoretical proofs for expressiveness and over-smoothing alleviation rely on assumptions such as universal and injective functions for the internal mappings ($\phi_x, \phi_u, f$) and connected, unweighted, undirected graphs. While alleviating over-squashing, RUM does not fully solve the information bottleneck with exponentially growing reception fields but rather improves the gradient flow.
*   **Comparison**: RUM represents a significant paradigm shift, offering a compelling alternative to convolution-based GNNs by jointly addressing expressiveness, over-smoothing, and over-squashing through a non-convolutional, random-walk-based approach. This contrasts with methods that incrementally improve message passing or rely on higher-order WL tests. It highlights a convergent research direction towards models that can capture global structural information more effectively and efficiently.

In summary, the quest for higher expressiveness in GNNs has led to diverse and innovative solutions. From the direct, but computationally intensive, approach of k-WL GNNs to the path-centric and recursively annotated PathNNs, and finally to the entirely non-convolutional RUM, the field has moved beyond the strict confines of 1-WL equivalence. These advancements underscore a critical tension: how to achieve high discriminative power without incurring prohibitive computational costs. The shift towards incorporating more explicit structural information (paths, random walks) and rethinking the fundamental propagation mechanisms represents a mature understanding of the limitations of early, simplistic message-passing models. This continuous evolution is crucial for GNNs to tackle the increasingly complex and diverse graph learning tasks encountered in real-world applications \cite{gao2022f3h, chen2022mmu}.

\subsection*{Tackling Over-smoothing and Enabling Deep GNNs}

The promise of deep learning lies in its ability to learn hierarchical representations through multiple layers of non-linear transformations. However, for Graph Neural Networks (GNNs), this promise has been significantly hampered by the pervasive problem of **over-smoothing** \cite{chen2019s47, cai2020k4b}. As discussed in Section 2, over-smoothing occurs when, after several layers of message passing, the representations of all nodes in a connected component converge to a single, indistinguishable vector. This loss of discriminative power fundamentally limits the effective depth of GNNs, contrasting sharply with the success of very deep architectures in domains like computer vision (e.g., ResNets with hundreds of layers). Overcoming over-smoothing is paramount for building truly deep and powerful GNNs capable of capturing long-range dependencies and complex hierarchical features.

Several strategies have emerged to tackle over-smoothing, broadly falling into categories of architectural modifications, regularization techniques, and more fundamental re-thinking of information dynamics.

**1. Architectural Modifications and Skip Connections:**
*   **Context**: Inspired by the success of residual connections in deep Convolutional Neural Networks (CNNs) \cite{he2016deep}, early attempts to deepen GNNs focused on similar architectural enhancements. The core idea is to allow information from initial layers to bypass intermediate layers, preventing complete feature homogenization.
*   **Mechanism**:
    *   **Residual Connections**: Adding the input of a layer to its output, often after a transformation, helps propagate initial features and stabilize gradients. DeepGCNs \cite{li2019deepgcn} and GCNII \cite{chen2020simple} are prime examples. GCNII, for instance, uses an "initial residual" connection that adds a weighted sum of the initial node features to the output of each layer, alongside a "identity mapping" that adds the previous layer's output. This allows the model to retain information from the input layer, mitigating the rapid convergence of features.
    *   **Skip Connections**: More generally, these allow features from earlier layers to be directly fed into later layers or the final readout layer. This helps preserve local information that might otherwise be smoothed out.
    *   **Normalization Layers**: Techniques like Layer Normalization or Batch Normalization are also employed to stabilize activations and prevent features from collapsing.
*   **Core Innovation**: Adapting established deep learning principles to GNNs to facilitate deeper architectures.
*   **Evidence**: These methods have enabled the training of significantly deeper GNNs. For example, \cite{li2021orq} demonstrated the ability to train GNNs with up to 1000 layers using residual connections, achieving competitive performance. Similarly, \cite{zeng2022jhz} proposed decoupling the depth and scope of GNNs, allowing for deeper models without necessarily increasing the receptive field size, which can also contribute to over-smoothing.
*   **Theoretical Limitations**: While these techniques *alleviate* over-smoothing, they often do not fundamentally *prevent* the feature convergence problem. The underlying message-passing mechanism still tends to smooth features, and skip connections merely provide a bypass, not a cure. The Dirichlet energy, a measure of feature smoothness, still tends to decrease, albeit at a slower rate \cite{zhou20213lg}.
*   **Practical Limitations**: Even with skip connections, very deep GNNs can still be challenging to train due to vanishing/exploding gradients and increased computational cost.

**2. Regularization and Graph Rewiring:**
*   **Context**: Over-smoothing is a consequence of excessive information propagation and aggregation. Regularization aims to disrupt this process or modify the graph structure to prevent rapid feature mixing.
*   **Mechanism**:
    *   **DropEdge**: \cite{rong2019dropedge} proposed randomly dropping edges during training. This acts as a stochastic regularization technique, preventing any single path from dominating information flow and forcing the model to rely on diverse paths. It effectively creates a slightly different graph at each training step, reducing the over-smoothing effect.
    *   **Graph Rewiring**: This involves dynamically or statically modifying the graph structure. For instance, GPR-GNN \cite{klicpera20186xu} uses personalized PageRank to adaptively combine features from different hops, which can be seen as a form of implicit rewiring. Other methods dynamically add or remove edges based on learned node similarities or other criteria \cite{jin2020dh4}.
*   **Core Innovation**: Introducing stochasticity or modifying graph topology to control information flow and prevent feature homogenization.
*   **Evidence**: DropEdge has been shown to improve performance and enable deeper GNNs by mitigating over-smoothing. Graph rewiring techniques can also improve robustness and performance on specific tasks.
*   **Theoretical Limitations**: DropEdge can reduce the overall information flow, potentially hindering the learning of long-range dependencies if too many edges are dropped. Graph rewiring can be computationally expensive and may alter the original graph semantics, requiring careful justification.
*   **Practical Limitations**: The optimal dropout rate for DropEdge can be dataset-dependent. Dynamic rewiring methods can add significant overhead to training.

**3. Decoupled Propagation and Transformation:**
*   **Context**: Traditional GNN layers combine feature transformation and propagation. Decoupling these steps can offer more control over the smoothing process.
*   **Mechanism**: Approaches like APPNP \cite{klicpera20186xu} and SGC \cite{wu2019simplifying} separate the non-linear feature transformation from the linear propagation step. Initial node features are transformed once, and then these transformed features are propagated linearly across the graph for multiple steps. GCNII \cite{chen2020simple} also leverages this idea by propagating initial features through a linear combination of previous layer's features and initial features.
*   **Core Innovation**: Preventing repeated non-linear transformations from rapidly mixing features across layers, allowing for deeper propagation without immediate over-smoothing. This also allows for decoupling the "depth" (number of propagation steps) from the "scope" (number of non-linear transformations) \cite{zeng2022jhz}.
*   **Evidence**: These models demonstrate improved performance and deeper architectures compared to standard GCNs, particularly on tasks benefiting from extensive propagation.
*   **Theoretical Limitations**: The reliance on linear propagation after initial transformation might limit the model's capacity to learn complex, non-linear interactions across multiple hops.
*   **Practical Limitations**: The optimal number of propagation steps needs to be tuned, and too many steps can still lead to over-smoothing.

**4. Fundamental Re-thinking of Information Dynamics: Fractional Calculus and Non-Convolutional GNNs:**
*   **Context**: The limitations of existing methods suggest that over-smoothing might be an inherent property of the Markovian, integer-order dynamics assumed by most GNNs. A more fundamental change to the information propagation mechanism is needed.
*   **Method Family A: Fractional-Order Graph Neural Networks (e.g., FROND \cite{kang2024fsk})**:
    *   **Mechanism**: The **FRactional-Order graph Neural Dynamical network (FROND)** framework \cite{kang2024fsk} generalizes continuous GNNs by replacing the integer-order differential operator (which models instantaneous, local changes) with the Caputo fractional derivative. This allows FROND to inherently integrate the entire historical trajectory of node features into their update process, enabling the capture of non-local and memory-dependent dynamics. For the fractional linear diffusion model (F-GRAND-l), this can be interpreted as a non-Markovian random walk where the walker's complete path history influences future steps.
    *   **Core Innovation**: Introducing non-local fractional derivatives to model memory-dependent dynamics in continuous GNNs. This fundamentally alters the dynamics of information flow.
    *   **Theoretical Contributions**: \cite{kang2024fsk} analytically established that the non-Markovian random walk in FROND leads to a slow *algebraic* rate of convergence to stationarity, which inherently mitigates over-smoothing, unlike the exponentially swift convergence in Markovian integer-order models. This provides a strong theoretical basis for its effectiveness.
    *   **Evidence**: Fractional adaptations of various continuous GNNs (e.g., F-GRAND, F-GRAND++) consistently demonstrated improved performance compared to their integer-order counterparts across diverse datasets.
    *   **Theoretical Limitations**: The framework relies on numerical FDE solvers, which may introduce computational considerations. The primary focus is on $\beta \in (0,1]$ for initial conditions, though broader definitions are mentioned.
    *   **Comparison**: FROND offers a principled, theoretically grounded solution to over-smoothing by altering the *dynamics* of information flow, rather than just architectural tweaks or regularization. It represents a significant advancement by generalizing existing continuous GNNs and opening new research directions for modeling complex, memory-dependent feature updates.

*   **Method Family B: Non-Convolutional GNNs (e.g., RUM \cite{wang2024oi8})**:
    *   **Context**: Over-smoothing is identified as a core pathology of *convolution-based* GNNs.
    *   **Mechanism**: As detailed in the previous subsection, the **Random Walk with Unifying Memory (RUM) neural network** \cite{wang2024oi8} is an entirely convolution-free architecture that processes random walk trajectories using RNNs. This mechanism fundamentally avoids the local, iterative averaging that leads to over-smoothing in traditional GNNs.
    *   **Core Innovation**: Jointly remedies over-smoothing, limited expressiveness, and over-squashing through a novel non-convolutional paradigm.
    *   **Theoretical Contributions**: RUM is theoretically and empirically shown to attenuate over-smoothing, with its expected Dirichlet energy not diminishing even with long walks, unlike convolutional GNNs.
    *   **Comparison**: RUM represents a radical departure that addresses over-smoothing as a systemic issue of convolution, offering a comprehensive solution alongside enhanced expressiveness. It contrasts with FROND by achieving non-Markovian dynamics through random walks and RNNs, rather than fractional derivatives in continuous models.

In conclusion, tackling over-smoothing is a multi-faceted challenge that has driven significant innovation in GNN architecture and theory. While architectural modifications and regularization techniques provide practical improvements, the field is increasingly exploring more fundamental solutions, such as leveraging fractional calculus for memory-dependent dynamics \cite{kang2024fsk} or entirely abandoning convolution-based message passing in favor of random walk-based approaches \cite{wang2024oi8}. This evolution signifies a shift from incremental fixes to a deeper understanding of the underlying mechanisms of information propagation, paving the way for truly deep and robust GNNs capable of capturing complex, multi-scale graph patterns. The ongoing challenge remains to develop models that are not only deep and expressive but also computationally efficient and scalable for real-world applications \cite{gao2022f3h}.

\subsection*{Adapting to Diverse Graph Structures and Heterophily}

A critical assumption implicitly embedded in many early Graph Neural Network (GNN) architectures, particularly those based on simple neighborhood aggregation (e.g., GCNs \cite{kipf2016semi}), is **homophily**. This principle posits that connected nodes in a graph tend to share similar features or labels \cite{mcpherson2001birds, ma2021sim}. While this assumption holds true for many real-world networks, such as citation networks where linked papers often belong to the same research area, it severely limits the performance of these GNNs on **heterophilous graphs**. In heterophilous networks, connected nodes are frequently dissimilar, or even antagonistic, in their attributes or labels (e.g., protein-protein interaction networks, financial transaction graphs, or social networks connecting individuals with diverse interests) \cite{zhu2020c3j, luan2021g2p}. The smoothing effect inherent in standard message passing, which averages or sums neighbor features, becomes detrimental in such scenarios, as it blends dissimilar information, leading to degraded node representations. Adapting GNNs to effectively handle diverse graph structures, especially those exhibiting heterophily, has become a crucial research direction.

**1. Explicit Heterophily-Aware Aggregation Mechanisms:**
*   **Context**: The direct smoothing of dissimilar features in heterophilous graphs leads to poor performance. The motivation is to design aggregation functions that can intelligently differentiate between homophilous and heterophilous connections.
*   **Mechanism**: This family of methods modifies the message-passing scheme to explicitly account for node dissimilarity.
    *   **Separate Aggregation**: Some models propose separate aggregation mechanisms for different types of neighbors. For instance, H2GCN \cite{zhu2020c3j} aggregates information from both 1-hop neighbors (which might be heterophilous) and higher-order neighbors (which are more likely to be homophilous due to structural properties), then combines these distinct representations.
    *   **Adaptive Weighting**: Other approaches introduce learnable weights or attention mechanisms that can assign lower weights to dissimilar neighbors or even negative weights to "repulsive" connections. WRGAT \cite{luan202272y} and FAGCN \cite{bo2021beyond} are examples where attention mechanisms are designed to be sensitive to feature differences, allowing the model to adaptively weigh messages.
    *   **High-Pass Filters**: Some methods incorporate high-pass filters alongside traditional low-pass filters to capture feature differences, rather than just similarities \cite{bianchi20194ea}.
*   **Core Innovation**: Directly countering the detrimental smoothing effect by making aggregation mechanisms sensitive to node similarity/dissimilarity.
*   **Evidence**: These models generally demonstrate improved performance on benchmark heterophilous datasets (e.g., Chameleon, Squirrel) compared to traditional GNNs.
*   **Theoretical Limitations**: Many of these methods still rely on local neighborhood information. Determining the optimal "neighborhood size" or the best way to combine information from different hops can be challenging and dataset-dependent. The explicit estimation of homophily/heterophily can be noisy or difficult.
*   **Practical Limitations**: These models often require careful tuning of hyperparameters related to aggregation strategies. The computational overhead can also increase due to more complex aggregation functions or multiple aggregation paths.

**2. Global Homophily and Adaptive Coefficient Learning (e.g., GloGNN/GloGNN++ \cite{li2022315}):**
*   **Context**: While local heterophily-aware aggregations are useful, they may still struggle with adaptively setting personalized optimal neighborhood sizes or leveraging *global* homophily where similar nodes might be topologically distant. Existing methods often face prohibitive computational costs for truly global aggregation.
*   **Method**: **GloGNN and GloGNN++** \cite{li2022315} propose a novel framework that generates node embeddings by aggregating information from *all global nodes* in the graph, not just local neighbors.
*   **Mechanism**: In each layer, a **signed coefficient matrix $\mathbf{Z}^{(l)}$** is learned, where $\mathbf{Z}^{(l)}_{ij}$ describes the importance of node $j$ to node $i$. This matrix is derived from an optimization problem inspired by the linear subspace model and is regularized by nodes' multi-hop reachabilities to incorporate both feature and topology similarity. Crucially, $\mathbf{Z}^{(l)}$ allows signed values, enabling it to assign large positive coefficients to homophilous nodes and small positive or even negative coefficients to heterophilous ones, implicitly combining low-pass and high-pass filtering.
*   **Core Innovation**: The primary innovation is the concept of **global aggregation with learned, signed coefficients** that adaptively capture correlations between *all* nodes, effectively finding global homophily. A significant technical contribution is the **linear time complexity acceleration** for this global aggregation. By transforming the aggregation equation and reordering matrix multiplications, the time complexity is reduced from $O(N^3)$ or $O(N^2c)$ to $O(k^2N)$ (where $k$ is the number of labels, typically small), making global aggregation computationally feasible for large graphs.
*   **Theoretical Contributions**: \cite{li2022315} provides a theoretical proof that both the learned coefficient matrix $\mathbf{Z}^{(l)}$ and the generated node embedding matrix $\mathbf{H}^{(l+1)}$ exhibit a **grouping effect**. This means nodes with similar features and local structures (even if topologically distant) will have similar coefficient vectors and representation vectors, explaining the model's effectiveness in leveraging global homophily.
*   **Evidence**: Extensive experiments on 15 benchmark datasets (covering diverse domains, scales, and heterophilies) demonstrated that GloGNN and GloGNN++ achieve superior performance and are highly efficient compared to 11 state-of-the-art GNN models.
*   **Limitations**: The paper primarily focuses on node classification tasks. While efficient, the linear time complexity still depends on the number of labels $k$, which might be large in some multi-class settings.
*   **Comparison**: GloGNN represents a significant advancement by moving beyond local or multi-hop aggregation to a principled and efficient global approach. It addresses the limitations of previous heterophily methods by adaptively learning node correlations across the entire graph, rather than relying on fixed neighborhoods or explicit homophily estimation. This highlights a methodological trend towards more adaptive and globally informed aggregation strategies.

**3. Graph Transformation and Rewiring:**
*   **Context**: Instead of modifying the GNN architecture, some approaches modify the graph structure itself to make it more amenable to existing GNNs.
*   **Mechanism**: This involves pre-processing the graph to either enhance homophily (e.g., adding edges between similar but disconnected nodes) or create auxiliary structures that capture heterophilous relationships (e.g., meta-paths in heterogeneous information networks \cite{lv20219al, zhao2021lls}). For instance, virtual nodes can be added to connect distant but related nodes, or edges can be rewired based on feature similarity.
*   **Core Innovation**: Shifting the burden of handling heterophily from the GNN model to the graph structure itself.
*   **Evidence**: These methods can improve performance on heterophilous graphs by creating a more "homophilous-friendly" topology.
*   **Theoretical Limitations**: Graph transformation can be computationally intensive, especially for large graphs. It may also inadvertently alter the original graph semantics or introduce spurious connections, requiring careful validation.
*   **Practical Limitations**: The design of effective graph transformation rules often requires domain expertise and can be specific to certain graph types.

**4. Decoupling Feature Transformation and Propagation:**
*   **Context**: Similar to its role in mitigating over-smoothing, decoupling feature transformation from propagation can also help with heterophily.
*   **Mechanism**: By performing non-linear feature transformations locally at each node *before* propagation, and then propagating these transformed features, the model can prevent the immediate mixing of raw, dissimilar features. This allows nodes to learn richer individual representations before interacting with potentially heterophilous neighbors. GCNII \cite{chen2020simple}, with its initial residual connections, implicitly benefits from this by preserving original feature information.
*   **Core Innovation**: Controlling when and how features are mixed, allowing for initial feature refinement before aggregation.
*   **Evidence**: Models employing this strategy can show improved robustness on heterophilous graphs.
*   **Theoretical Limitations**: The extent to which this fully addresses heterophily depends on the power of the initial feature transformation and the subsequent propagation mechanism.
*   **Practical Limitations**: Still subject to the limitations of the underlying propagation mechanism.

The challenge of adapting GNNs to diverse graph structures, particularly those exhibiting heterophily, has driven the field to move beyond the simplistic homophily assumption. The evolution from local, ad-hoc modifications to more principled, global, and efficient solutions like GloGNN \cite{li2022315} signifies a mature understanding of this problem. This shift is crucial for expanding the applicability of GNNs to a broader range of real-world scenarios, where heterophily is often the norm rather than the exception. The ongoing debate on whether homophily is a "necessity" or a "curse" for GNNs \cite{ma2021sim, luan2021g2p} continues to fuel research into more robust and adaptive graph learning paradigms. Furthermore, the rigorous evaluation of these advanced models, as highlighted by \cite{li2023o4c} in the context of link prediction, is essential to ensure that innovations truly translate into practical gains across diverse and challenging graph datasets.