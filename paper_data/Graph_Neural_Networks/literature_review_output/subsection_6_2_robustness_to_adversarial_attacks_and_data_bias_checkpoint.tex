\subsection{Robustness to Adversarial Attacks and Data Bias}

The increasing deployment of Graph Neural Networks (GNNs) in critical applications necessitates a thorough understanding of their trustworthiness, particularly concerning robustness to adversarial attacks and mitigation of data bias \cite{zhang20222g3}. GNNs are inherently vulnerable to subtle, often imperceptible, perturbations in their graph structure or node features, which can drastically alter predictions and undermine their reliability. This section explores various attack strategies, corresponding defense mechanisms, and methods for learning invariant rationales to enhance GNN resilience.

Initial investigations into GNN vulnerabilities highlighted the susceptibility to adversarial manipulations. \cite{zgner2019bbi} pioneered the study of global poisoning attacks, formulating them as a bilevel optimization problem solved via meta-gradients to degrade the overall performance of GNNs on discrete graph structures. Concurrently, \cite{xu2019l8n} addressed topology attacks by proposing an optimization-based framework that leverages convex relaxation to enable gradient-based perturbations on discrete graph structures, demonstrating effective attack generation and laying groundwork for adversarial training. However, these early attack methods often faced scalability challenges, limiting their applicability to large real-world graphs. To overcome this, \cite{geisler2021dcq} introduced sparsity-aware first-order optimization attacks (PR-BCD and GR-BCD) and novel surrogate losses (Masked Cross Entropy, tanh margin loss), enabling global GNN attacks on graphs orders of magnitude larger than previously possible and achieving significantly stronger attacks. Beyond general performance degradation, specific attack vectors target sensitive information or introduce stealthy backdoors. \cite{he2020kz4} demonstrated "link stealing attacks," revealing that GNN outputs inherently encode significant structural information about their training graphs, even under black-box access, posing a critical privacy threat. Furthermore, \cite{zhang2020b0m} introduced subgraph-based backdoor attacks for graph classification, where a predefined trigger pattern injected into training graphs causes the GNN to misclassify any triggered test graph to a target label, highlighting a stealthy and persistent form of manipulation.

In response to these vulnerabilities, various defense mechanisms have been proposed. \cite{zhang2020jrt} introduced GNNGuard, a general algorithm that defends against training-time poisoning attacks by dynamically estimating neighbor importance and employing layer-wise graph memory to prune suspicious edges and revise message passing. \cite{liu2021ee2} proposed Elastic Graph Neural Networks, which enhance robustness by integrating L1-based graph smoothing into their message passing scheme. This approach allows for adaptive local smoothness, preserving discontinuities while providing robustness against adversarial attacks, a limitation of traditional L2-based smoothing methods. Adversarial training, as proposed by \cite{xu2019l8n}, also emerged as a defense, where GNNs are trained to minimize loss under worst-case topology perturbations. However, a critical re-evaluation by \cite{mujkanovic20238fi} exposed a significant limitation in the evaluation of many existing GNN defenses. Their comprehensive analysis, using custom-designed adaptive attacks, revealed that most defenses offer "no or only marginal improvement" against adversaries aware of the defense mechanism, challenging earlier optimistic claims and emphasizing the need for more rigorous evaluation against adaptive threats.

Beyond adversarial robustness, GNNs must also contend with inherent data biases that can lead to unfair or unreliable predictions. \cite{dong2021qcg} addressed this by proposing EDITS, a novel, model-agnostic framework that mitigates bias directly in the input attributed network by defining and optimizing against "attribute bias" and "structural bias." This approach shifts the focus from debiasing GNN models or outcomes to debiasing the input data itself, overcoming the limitation of model-specific debiasing. Addressing fairness under practical constraints, \cite{dai2020p5t} introduced FairGNN, a framework for fair node classification that effectively handles limited sensitive attribute information by estimating missing values and employing an adversarial network to ensure predictions are independent of sensitive attributes. Building on this, \cite{wang2022531} further improved fairness by mitigating "sensitive attribute leakage" during feature propagation, a phenomenon where non-sensitive features become correlated with sensitive ones after GNN message passing. More recently, \cite{li20245zy} rethought fair GNNs from a re-balancing perspective, proposing FairGB, which uses Counterfactual Node Mixup (CNM) to generate unbiased ego-networks and Contribution Alignment Loss (CAL) to balance group contributions, demonstrating superior fairness and utility with minimal architectural changes.

To enhance overall reliability and prevent malicious manipulation, research is also focusing on discovering and learning invariant rationales, making GNNs more resilient to spurious correlations and out-of-distribution (OOD) shifts. \cite{wu2022vcx} proposed Discovering Invariant Rationale (DIR), a novel invariant learning strategy for intrinsically interpretable GNNs. DIR identifies causal patterns that are stable across different data distributions by generating "interventional distributions" through causal interventions on non-causal graph components. This approach helps GNNs learn from true causal features rather than shortcut features or data biases, thereby improving OOD generalization and making explanations more faithful.

In conclusion, while significant progress has been made in understanding and mitigating adversarial attacks and data bias in GNNs, the field faces ongoing challenges. The critical finding that many existing defenses are vulnerable to adaptive attacks underscores the need for more robust and theoretically grounded defense mechanisms. Future directions must focus on developing inherently robust GNN architectures, certified robustness guarantees, and advanced causal learning frameworks that can consistently identify invariant rationales to ensure GNNs are not only performant but also truly trustworthy, fair, and resilient to malicious manipulation in high-stakes real-world applications.