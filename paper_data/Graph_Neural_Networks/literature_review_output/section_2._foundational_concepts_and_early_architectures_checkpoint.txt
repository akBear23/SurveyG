\section*{2. Foundational Concepts and Early Architectures}

The emergence of Graph Neural Networks (GNNs) represents a paradigm shift in machine learning, offering a powerful framework to process and learn from data intrinsically structured as graphs. This section delves into the foundational concepts that underpin GNNs, tracing their intellectual lineage from the essential background of graph representation learning to the development of early, influential architectural designs. At its core, GNNs leverage the "message-passing" paradigm, an iterative process where nodes aggregate information from their local neighborhoods to update their representations. This mechanism, while intuitive and effective for many tasks, also introduces inherent limitations, particularly concerning the models' discriminative power. A critical aspect explored here is the theoretical expressiveness of GNNs, benchmarked against the Weisfeiler-Leman (WL) graph isomorphism test, which provides a formal measure of their ability to distinguish between structurally different graphs. By establishing the initial capabilities, core design principles, and inherent limitations of these early GNN models, this section lays the groundwork for understanding the subsequent advancements and the ongoing research efforts to overcome these fundamental challenges.

The journey into GNNs begins with the fundamental problem of graph representation learning: how to embed complex, irregular graph structures into low-dimensional vector spaces such that the topological and feature information is preserved and useful for downstream tasks \cite{wu20193b0, zhou20188n6}. Unlike Euclidean data, graphs lack a canonical ordering of nodes or a fixed neighborhood size, making direct application of traditional deep learning architectures, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), challenging. Early approaches to graph learning often relied on handcrafted features or matrix factorization techniques, which struggled with scalability and the ability to capture complex, non-linear patterns. GNNs address this by extending the principles of deep learning to graphs, allowing models to learn features directly from the graph structure and node attributes. This capability is crucial because many real-world systems, from social networks and biological pathways to knowledge graphs and transportation systems, are naturally represented as graphs, where relationships between entities are as important as the entities themselves \cite{velickovic2023p4r, jegelka20222lq}.

The core innovation of early GNNs lies in their ability to iteratively aggregate and transform information across the graph. This iterative process, often referred to as message passing, enables nodes to gather increasingly rich contextual information from their multi-hop neighborhoods. Each layer of a GNN can be conceptualized as a step in this information propagation, where a node's representation is updated based on its previous state and the aggregated messages from its neighbors. This local aggregation mechanism, inspired by classical signal processing on graphs, forms the backbone of most GNN architectures. However, the simplicity and local nature of this paradigm also lead to identifiable limitations. For instance, the implicit assumption of "homophily," where connected nodes tend to be similar, often restricts the performance of many early GNNs on "heterophilous" graphs, where connected nodes are frequently dissimilar \cite{ma2021sim, zhu2020c3j}. Furthermore, the repeated aggregation can lead to "over-smoothing," a phenomenon where node representations become indistinguishable as the number of layers increases, thereby limiting the depth and discriminative power of GNNs \cite{chen2019s47, cai2020k4b}.

A critical theoretical lens through which the capabilities of early GNNs are evaluated is their relationship to the Weisfeiler-Leman (WL) graph isomorphism test \cite{weisfeiler1968reduction}. This test provides a hierarchical framework for assessing the discriminative power of graph algorithms, with the 1-WL test serving as a common benchmark for many message-passing GNNs. Pioneering work by \cite{xu2018c8q} formally established that many popular GNNs are at most as powerful as the 1-WL test, meaning they cannot distinguish between certain non-isomorphic graphs that the 1-WL test fails to differentiate. This theoretical limitation spurred significant research into designing more expressive GNN architectures, such as higher-order GNNs \cite{morris20185sd} or those that incorporate richer structural information like paths \cite{michel2023hc4}. The tension between achieving high expressiveness, computational efficiency, and scalability remains a central theme in GNN research, with early models often sacrificing one for the others. This section will systematically unpack these foundational elements, highlighting the initial successes, inherent trade-offs, and the critical questions that continue to drive innovation in the field.

\subsection*{Graph Representation Learning Fundamentals}
Graph representation learning forms the bedrock upon which Graph Neural Networks (GNNs) are built, addressing the fundamental challenge of transforming complex, non-Euclidean graph structures into a format amenable to machine learning algorithms \cite{wu20193b0, zhou20188n6}. The core objective is to embed nodes, edges, or entire graphs into low-dimensional vector spaces, often referred to as "embeddings," such that their structural roles, features, and relational properties are preserved. These embeddings can then be used for various downstream tasks, including node classification, link prediction, and graph classification \cite{velickovic2023p4r, jegelka20222lq}.

Historically, graph analysis relied on handcrafted features (e.g., node degrees, clustering coefficients, PageRank scores) or matrix factorization techniques applied to adjacency or Laplacian matrices. While these methods provided some insights, they suffered from several limitations. Handcrafted features often required domain expertise, were not generalizable across different graph types, and struggled to capture complex, multi-hop dependencies. Matrix factorization methods, such as those used in early network embedding techniques like DeepWalk \cite{perozzi2014deepwalk} and Node2Vec \cite{grover2016node2vec}, could learn embeddings by modeling random walks or neighborhood similarities. However, these methods were typically transductive, meaning they could not generalize to unseen nodes or graphs without retraining, and often decoupled feature learning from the end task, limiting their effectiveness.

The advent of GNNs marked a significant departure, introducing an end-to-end, inductive learning paradigm for graph representations. The fundamental idea is to learn a mapping function that transforms a node's features and its local graph structure into a dense vector representation. This is achieved through an iterative process of feature aggregation and transformation. For a given node $v$, its embedding at layer $k$, denoted as $\mathbf{h}_v^{(k)}$, is typically computed by aggregating information from its neighbors and its own representation from the previous layer $k-1$. This process can be generalized as:
$\mathbf{h}_v^{(k)} = \text{UPDATE}^{(k)}(\mathbf{h}_v^{(k-1)}, \text{AGGREGATE}^{(k)}(\{\mathbf{h}_u^{(k-1)} \mid u \in \mathcal{N}(v)\}))$,
where $\mathcal{N}(v)$ denotes the set of neighbors of node $v$, and $\text{AGGREGATE}$ and $\text{UPDATE}$ are learnable functions, often implemented as neural networks \cite{hamilton2017inductive}.

This framework inherently addresses the non-Euclidean nature of graphs by operating directly on the graph topology. The permutation invariance of the aggregation function (e.g., sum, mean, max) ensures that the learned representations are independent of the order in which neighbors are processed, a crucial property for graph data. This mechanism allows GNNs to capture relational inductive biases, meaning they learn patterns that are robust to permutations of node indices and can generalize to graphs with different structures. For instance, a GNN can learn that a node with many high-degree neighbors plays a specific structural role, regardless of the specific identities of those neighbors.

However, this foundational approach also introduced several critical challenges. A primary concern is the **homophily assumption** \cite{ma2021sim}. Many early GNNs, by design, implicitly assume that connected nodes share similar features or labels. This is because the aggregation functions typically average or sum neighbor features, effectively smoothing them. While effective for homophilous graphs (e.g., citation networks where connected papers are often on similar topics), this assumption severely limits performance on **heterophilous graphs** (e.g., protein-protein interaction networks where connected proteins might have diverse functions) \cite{zhu2020c3j, luan2021g2p}. The field recognized this tension, leading to the development of models like GloGNN \cite{li2022315}, which explicitly addresses heterophily by learning a *signed coefficient matrix* for global aggregation, allowing it to assign positive weights to homophilous nodes and negative or small positive weights to heterophilous ones. This innovation demonstrates a critical evolution from simple local smoothing to adaptive, global, and heterophily-aware aggregation.

Another significant challenge in graph representation learning, particularly as GNNs become deeper, is **over-smoothing** \cite{chen2019s47, cai2020k4b}. As nodes repeatedly aggregate information from their neighbors across multiple layers, their representations tend to converge to a single, indistinguishable vector, losing their unique discriminative power. This phenomenon fundamentally limits the effective depth of GNNs, contrasting with the success of very deep architectures in other domains. While techniques like residual connections \cite{li2021orq, zeng2022jhz} and dropout variants \cite{rong2019dropedge} were introduced to mitigate this, the problem persists. A novel approach from \cite{kang2024fsk} introduces the FRactional-Order graph Neural Dynamical network (FROND), which generalizes continuous GNNs by employing Caputo fractional derivatives. This allows FROND to inherently integrate memory-dependent dynamics into node updates, leading to an algebraic (slower) rate of convergence to stationarity, thereby analytically mitigating over-smoothing compared to the exponential convergence of integer-order models. This highlights a critical shift towards understanding and leveraging the temporal dynamics of information propagation to build more robust representations.

The efficiency and scalability of graph representation learning are also paramount. Early GNNs often required access to the entire graph during training, which is infeasible for large-scale graphs. This led to the development of sampling-based approaches like GraphSAGE \cite{hamilton2017inductive}, which samples a fixed number of neighbors for aggregation, making training mini-batch compatible and scalable. The challenge of evaluating these learned representations also became apparent. As highlighted by \cite{li2023o4c}, current evaluation practices for tasks like link prediction often suffer from pitfalls such as inconsistent data splits, poor hyperparameter tuning, and unrealistic negative sampling. Their proposed Heuristic Related Sampling Technique (HeaRT) addresses this by generating *harder*, more realistic negative samples, providing a more robust benchmark for assessing the true capabilities of GNNs. This emphasizes that the quality of learned representations is not only dependent on the model architecture but also on the rigor of its evaluation. In essence, graph representation learning provides the necessary foundation for GNNs to operate, but its inherent challenges have continuously pushed the field towards more sophisticated and robust architectural designs, aggregation mechanisms, and evaluation methodologies.

\subsection*{The Message Passing Paradigm and Early Models}
The message-passing paradigm is the cornerstone of most Graph Neural Networks (GNNs), providing an elegant and unified framework for information propagation and aggregation across graph structures \cite{gilmer2017neural, wu20193b0}. This paradigm formalizes the intuitive idea that a node's representation can be iteratively refined by exchanging "messages" with its neighbors and combining these messages with its own current state. This section elucidates the core mechanics of message passing and introduces early, influential GNN architectures that instantiated this principle, critically examining their innovations and inherent limitations.

The general message-passing framework, as formalized by \cite{gilmer2017neural}, involves two main steps at each layer $k$:
1.  **Message Generation**: For each node $v$, messages $m_{uv}^{(k)}$ are generated from its neighbors $u \in \mathcal{N}(v)$, often as a function of their previous layer's embeddings $\mathbf{h}_u^{(k-1)}$ and the edge features $e_{uv}$.
2.  **Aggregation and Update**: Each node $v$ aggregates the incoming messages from its neighbors, $\sum_{u \in \mathcal{N}(v)} m_{uv}^{(k)}$, and then updates its own embedding $\mathbf{h}_v^{(k)}$ by combining this aggregated message with its previous embedding $\mathbf{h}_v^{(k-1)}$.

This iterative process allows information to flow across the graph, enabling nodes to capture increasingly global structural and feature information as the number of layers increases. The permutation invariance of the aggregation function (e.g., sum, mean, max) is crucial, ensuring that the node embeddings are independent of the arbitrary ordering of neighbors.

One of the earliest and most influential instantiations of this paradigm was the **Graph Convolutional Network (GCN)**, proposed by \cite{kipf2016semi}.
*   **Context**: GCNs aimed to generalize the concept of convolution from grid-like data (images) to arbitrary graph structures, drawing inspiration from spectral graph theory but simplifying it for practical application. They sought to address the challenge of semi-supervised node classification efficiently.
*   **Mechanism**: GCNs achieve message passing through a simplified, first-order approximation of spectral graph convolutions. The core operation for updating node features $\mathbf{H}^{(k)}$ at layer $k$ is given by $\mathbf{H}^{(k)} = \tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}$, where $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$ is the adjacency matrix with self-loops, $\tilde{\mathbf{D}}$ is its degree matrix, and $\mathbf{W}^{(k)}$ is a learnable weight matrix. This effectively averages the features of a node's neighbors (including itself) and applies a linear transformation.
*   **Conditions for Success**: GCNs demonstrated strong performance on semi-supervised node classification tasks, particularly on homophilous graphs like citation networks (e.g., Cora, Citeseer, Pubmed) \cite{kipf2016semi}. Their simplicity and efficiency made them a popular baseline.
*   **Theoretical Limitations**: Despite their success, GCNs are theoretically limited in their expressive power, being at most as powerful as the 1-Weisfeiler-Leman (1-WL) test \cite{xu2018c8q}. This means they cannot distinguish between certain non-isomorphic graphs, limiting their ability to capture complex structural patterns.
*   **Practical Limitations**: GCNs suffer from **over-smoothing** when stacked deeply \cite{chen2019s47, cai2020k4b}, where node representations become indistinguishable. They also employ fixed, unweighted aggregation, implicitly assuming all neighbors contribute equally, which is suboptimal for graphs with varying neighbor importance or heterophily \cite{ma2021sim}.

Building on the message-passing framework and addressing some limitations of GCNs, **Graph Attention Networks (GATs)** were introduced by \cite{velickovic2017graph}.
*   **Context**: GATs aimed to overcome the fixed-weight aggregation of GCNs by allowing the model to learn adaptive weights for each neighbor, making the aggregation process more flexible and powerful.
*   **Mechanism**: GATs introduce an attention mechanism where each node computes an attention coefficient for each of its neighbors, indicating the importance of that neighbor's features. These coefficients are then used to compute a weighted sum of neighbor features. The attention mechanism is typically implemented using a single-layer feedforward neural network and a LeakyReLU activation, followed by a softmax normalization over neighbors. Multiple attention heads can be used to stabilize the learning process.
*   **Conditions for Success**: GATs showed improved performance over GCNs on various node classification and graph classification tasks, especially in scenarios where different neighbors have varying importance or when dealing with inductive settings \cite{velickovic2017graph}.
*   **Theoretical Limitations**: Despite their adaptive aggregation, GATs are still generally considered to be at most as powerful as the 1-WL test \cite{xu2018c8q}, inheriting the same expressiveness limitations as GCNs.
*   **Practical Limitations**: GATs still face the over-smoothing problem in deep architectures \cite{wu2023aqs}. The computational cost of computing attention coefficients for every neighbor can also be higher than GCNs, especially for dense graphs.

The limitations of these early message-passing models, particularly over-smoothing and restricted expressiveness, spurred significant research into developing deeper and more robust GNNs. For instance, **deep GNNs** attempted to mitigate over-smoothing through architectural innovations like residual connections, similar to those in CNNs \cite{li2021orq, zeng2022jhz}. However, these often only partially alleviated the problem, as the fundamental smoothing operation remained. A more radical departure is seen in the **FRactional-Order graph Neural Dynamical network (FROND)** framework \cite{kang2024fsk}.
*   **Context**: FROND addresses the limitations of traditional continuous GNNs, which rely on integer-order differential equations and struggle to capture long-term dependencies and memory effects, leading to over-smoothing.
*   **Mechanism**: It replaces the integer-order differential operator with the Caputo fractional derivative. This allows FROND to inherently integrate the entire historical trajectory of node features, enabling memory-dependent dynamics. The paper analytically shows that this leads to a slow *algebraic* rate of convergence to stationarity, thereby mitigating over-smoothing compared to the *exponential* convergence of Markovian integer-order models.
*   **Comparison**: FROND represents a generalization of existing continuous GNNs, offering a novel theoretical foundation for building deeper and more robust models by leveraging non-local, memory-dependent dynamics.
*   **Implications**: This approach offers a new class of GNNs that can model complex, non-Markovian feature updates, potentially unlocking deeper insights into graph dynamics and providing a principled way to overcome over-smoothing.

Another significant innovation challenging the conventional message-passing paradigm is the **Random Walk with Unifying Memory (RUM) neural network** \cite{wang2024oi8}.
*   **Context**: RUM directly confronts the fundamental technical problems of *convolution-based* GNNs: limited expressiveness (beyond 1-WL), over-smoothing, and over-squashing (difficulty learning long-range dependencies).
*   **Mechanism**: RUM is an entirely *non-convolutional* architecture. Instead of aggregating messages from direct neighbors, it samples finite-length random walks for each node. An RNN (specifically GRU) processes the sequence of node embeddings along these walks, merging "semantic representations" (node features) with "anonymous experiment" features (topological environment based on unique node occurrences in the walk). Node representations are then formed by averaging the unifying memories of walks terminating at that node.
*   **Innovation**: The "anonymous experiment" is a novel way to encode topological context, and the use of RNNs on random walks provides a fundamentally different information propagation mechanism.
*   **Theoretical Contributions**: RUM is theoretically shown to be more expressive than the 1-WL test, capable of distinguishing graphs that 1-WL equivalent GNNs cannot. It also theoretically and empirically attenuates over-smoothing (non-diminishing Dirichlet energy) and alleviates over-squashing (slower decay in inter-node Jacobian).
*   **Comparison**: RUM offers a compelling alternative to the message-passing paradigm, jointly addressing multiple long-standing GNN limitations without relying on convolution operators. Its runtime complexity is agnostic to the number of edges, making it highly scalable.
*   **Implications**: RUM signifies a potential paradigm shift, suggesting that effective graph learning can occur outside the traditional message-passing framework, opening avenues for more robust and scalable GNN designs.

In summary, the message-passing paradigm provided a powerful initial framework for GNNs, leading to influential models like GCN and GAT. However, their inherent limitations in expressiveness, over-smoothing, and handling heterophily quickly became apparent. These challenges have driven the field to explore more sophisticated aggregation mechanisms, novel theoretical foundations like fractional calculus, and entirely new non-convolutional architectures, pushing the boundaries of what GNNs can achieve.

\subsection*{Theoretical Expressiveness: The Weisfeiler-Leman Test}
The theoretical expressiveness of Graph Neural Networks (GNNs) is a critical dimension for understanding their capabilities and limitations, particularly their power to distinguish between non-isomorphic graphs. This aspect is predominantly benchmarked against the **Weisfeiler-Leman (WL) graph isomorphism test**, a hierarchy of algorithms designed to test graph isomorphism \cite{weisfeiler1968reduction}. This section delves into the relationship between GNNs and the WL test, highlighting how this theoretical framework has shaped the development of more powerful GNN architectures.

Graph isomorphism is the problem of determining whether two graphs are structurally identical, even if their nodes are labeled differently. This is a fundamental problem in graph theory, and its computational complexity remains an open question. For GNNs, the ability to distinguish non-isomorphic graphs is crucial because it dictates their capacity to learn unique representations for structurally distinct inputs, which is essential for tasks like graph classification or property prediction. If a GNN cannot differentiate between two non-isomorphic graphs, it will assign them identical embeddings, leading to incorrect predictions for tasks that depend on structural nuances.

The **1-Weisfeiler-Leman (1-WL) test**, also known as the color refinement algorithm, serves as the most common benchmark for GNN expressiveness.
*   **Mechanism**: The 1-WL test iteratively refines node "colors" (labels) based on the multiset of colors of their neighbors. Initially, all nodes are assigned a color based on their features (or a default color if no features exist). In each iteration, a node's new color is determined by hashing its current color together with the multiset of colors of its neighbors. This process continues until no node's color changes, or a fixed number of iterations is reached. If two graphs can be distinguished by their final color distributions, they are non-isomorphic. However, if they have the same final color distribution, the 1-WL test cannot distinguish them, even if they are non-isomorphic.
*   **Connection to GNNs**: Pioneering work by \cite{xu2018c8q} and \cite{morris20185sd} formally established a deep connection between the message-passing paradigm of GNNs and the 1-WL test. They demonstrated that many standard message-passing GNNs, including GCNs \cite{kipf2016semi} and GATs \cite{velickovic2017graph}, are *at most as powerful as the 1-WL test*. This means that if the 1-WL test cannot distinguish two non-isomorphic graphs, these GNNs also cannot distinguish them. This theoretical insight provided a crucial upper bound on the discriminative power of a broad class of GNNs.
*   **Limitations of 1-WL**: The 1-WL test, and consequently 1-WL equivalent GNNs, fail to distinguish many common non-isomorphic graphs. For example, they cannot differentiate between regular graphs (where all nodes have the same degree) that are non-isomorphic, or certain pairs of graphs with the same degree sequence but different higher-order structural properties (e.g., specific cycle counts) \cite{chen2020e6g, oono2019usb}. This limitation implies that standard GNNs might struggle with tasks requiring fine-grained structural understanding, such as counting substructures \cite{kanatsoulis2024l6i}.

The recognition of this expressiveness bottleneck sparked a significant research direction: designing GNNs that surpass the 1-WL test.
*   **Higher-Order GNNs**: \cite{morris20185sd} proposed higher-order GNNs, such as the k-WL GNN, which leverage k-tuples of nodes (instead of individual nodes) to perform message passing, effectively mimicking higher-dimensional WL tests. While theoretically more expressive (e.g., 2-WL can distinguish all regular graphs), these models often incur significantly higher computational complexity, scaling polynomially with $k$ and the number of nodes, making them less practical for large graphs.
*   **Path Neural Networks (PathNNs)** \cite{michel2023hc4} offer a compelling alternative to enhance expressiveness by explicitly leveraging path information.
    *   **Context**: PathNNs directly address the 1-WL limitation by recognizing that paths encode richer structural information than local neighborhoods alone.
    *   **Mechanism**: PathNNs encode paths of various lengths emanating from each node using recurrent layers. The key innovation lies in operating on "annotated sets of paths," where nodes within paths are recursively labeled with hashes of their shorter annotated path sets. This hierarchical encoding enriches the structural context beyond simple neighborhood aggregation.
    *   **Theoretical Evidence**: \cite{michel2023hc4} formally demonstrated that their `AP-Trees` (All Simple Paths) are strictly more powerful than `WL-Trees`, and crucially, that PathNN variants operating on annotated sets of paths (`˜SP`, `˜SP+`, `˜AP`) are strictly more powerful than the 1-WL algorithm. The most expressive variant (`˜AP`) was even shown to distinguish graphs that are 3-WL indistinguishable.
    *   **Limitations**: While powerful, finding all simple paths (especially for the `AP` variant) is NP-hard, necessitating approximations by fixing a maximum path length $K$. The computational complexity can still be a practical limitation for dense graphs or long paths.
    *   **Comparison**: PathNNs represent a significant advancement by providing a path-centric approach to surpass WL limits, offering a distinct mechanism compared to higher-order GNNs which operate on k-tuples.

Another notable approach challenging the WL barrier is the **Random Walk with Unifying Memory (RUM) neural network** \cite{wang2024oi8}.
*   **Context**: RUM aims to jointly remedy the limited expressiveness (beyond WL), over-smoothing, and over-squashing problems inherent in *convolutional* GNNs.
*   **Mechanism**: By processing random walk trajectories using RNNs and incorporating "anonymous experiment" features that encode topological environments, RUM captures structural information in a fundamentally different way than message passing.
*   **Theoretical Evidence**: \cite{wang2024oi8} theoretically demonstrates that RUM is more expressive than the 1-WL test, capable of distinguishing non-isomorphic graphs that 1-WL equivalent GNNs cannot. This is validated empirically on synthetic datasets designed to test expressiveness (e.g., distinguishing cycle sizes).
*   **Implications**: RUM's non-convolutional nature and reliance on random walk trajectories provide a novel avenue for achieving higher expressiveness, suggesting that moving beyond the direct message-passing paradigm can unlock greater discriminative power.

The theoretical understanding of GNN expressiveness, largely framed by the WL test, has been instrumental in guiding architectural innovations. It highlights a recurring tension: simpler, more scalable GNNs often sacrifice expressiveness, while more powerful models tend to incur higher computational costs. This tension is further complicated by the need for robust empirical validation. As emphasized by \cite{li2023o4c}, even highly expressive models require rigorous benchmarking with realistic evaluation settings (e.g., hard negative sampling for link prediction) to truly assess their practical utility. The ongoing quest for GNNs that are simultaneously expressive, efficient, and scalable remains a central challenge, with the WL test serving as a constant theoretical compass for progress.