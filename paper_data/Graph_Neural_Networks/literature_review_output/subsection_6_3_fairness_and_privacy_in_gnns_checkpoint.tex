\subsection{Fairness and Privacy in GNNs}
The increasing deployment of Graph Neural Networks (GNNs) in critical domains necessitates a rigorous examination of their ethical implications, particularly concerning fairness and privacy. GNNs, by their inherent nature of aggregating and propagating information across interconnected entities, can inadvertently perpetuate or even amplify societal biases and expose sensitive information, demanding robust mitigation strategies \cite{zhang20222g3, dai2022hsi}. This section delves into methodologies for ensuring fair outcomes and protecting sensitive data, exploring techniques that identify and mitigate biases, alongside mechanisms designed to safeguard private information.

\subsubsection{Ensuring Fairness in GNNs}
Ensuring fair outcomes in GNNs is a significant challenge, as biases can originate from imbalanced graph structures, node features, or the message-passing mechanism itself, often exacerbated by the homophilous nature of many real-world graphs. The field distinguishes between group fairness (fairness across predefined demographic groups) and individual fairness (similar individuals receiving similar predictions), both of which present unique challenges in graph contexts \cite{zhang20222g3, dai2022hsi}.

Early efforts, such as FairGNN by \cite{dai2020p5t}, addressed GNN bias, particularly when sensitive attribute information (e.g., gender, race) is limited. FairGNN employs a GCN-based sensitive attribute estimator to infer missing protected attributes, enabling an adversarial debiasing framework to learn fair node representations. This in-processing approach aims to make the learned representations independent of sensitive attributes. Building on the understanding of bias sources, \cite{dong2021qcg} shifted the focus to a data-centric, pre-processing approach with EDITS. This method models and mitigates both "attribute bias" (from node features) and "structural bias" (from graph topology) directly in the input attributed network using Wasserstein-1 distance, offering a model-agnostic solution that can be applied before any GNN model.

While group fairness is crucial, individual fairness presents distinct complexities. \cite{dong202183w} introduced REDRESS, a ranking-based framework to enhance individual fairness in GNNs. This approach circumvents the practical difficulties associated with traditional Lipschitz-condition-based definitions of individual fairness by reframing it from a ranking perspective, making it a plug-and-play solution for various GNN architectures. Further refining the understanding of bias propagation, \cite{wang2022531} identified "sensitive attribute leakage" as a critical issue where GNN feature propagation dynamically alters feature correlations, exacerbating discrimination. Their FairVGNN framework mitigates this by using a generative adversarial debiasing module to mask sensitive-correlated features and an adaptive weight clamping module for the GNN encoder.

More recently, researchers have explored causal and invariant learning perspectives for debiasing. \cite{li20245zy} re-evaluated fair GNNs from a re-balancing perspective, proposing FairGB. This method uses Counterfactual Node Mixup (CNM) to generate unbiased ego-networks and a Contribution Alignment Loss (CAL) to balance group contributions, offering a robust alternative to simple re-balancing and adversarial methods by providing theoretical insights into causal paths. Complementing these efforts, \cite{wu2022vcx} introduced Discovering Invariant Rationale (DIR), an invariant learning strategy for intrinsically interpretable GNNs. DIR aims to identify causal patterns that are stable across different data distributions, thus ensuring fairness and robustness by filtering out spurious correlations that might lead to biased decisions. Similarly, \cite{fan2022m67} proposed DisC (Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure) to tackle severe bias by explicitly disentangling causal and bias substructures using an edge mask generator and dual GNNs, further decorrelating them in the embedding space. This approach is particularly effective in scenarios where bias is severe and dominates GNN learning.

The choice between these methods often involves trade-offs between model utility, the strength of fairness guarantees, and computational overhead. Pre-processing methods like EDITS offer model agnosticism but might lose information, while in-processing adversarial methods like FairGNN and FairVGNN can be more tightly integrated but are model-specific. Causal and invariant learning approaches like FairGB, DIR, and DisC aim for more fundamental debiasing by targeting the underlying causal mechanisms, promising better generalization and interpretability.

\subsubsection{Protecting Privacy in GNNs}
Privacy is another paramount concern in GNNs, especially given the sensitive nature of graph connections and node attributes. GNNs can inadvertently leak private information through their outputs or learned representations, leading to various privacy attacks \cite{zhang20222g3, dai2022hsi}.

A major vulnerability is the potential for **link stealing attacks**, where an adversary can infer private connections from the black-box outputs of a GNN. \cite{he2020kz4} pioneered this area by demonstrating that GNN outputs inherently encode significant structural information about their training graphs, even with limited adversary knowledge. Their work introduced a comprehensive threat model and various attack methodologies (unsupervised, supervised, and transfer attacks), showing high success rates in inferring links. This highlights a critical privacy leakage that demands robust countermeasures. Beyond link stealing, GNNs are also susceptible to other inference attacks:
\begin{itemize}
    \item \textbf{Membership Inference Attacks}: These attacks aim to determine whether a specific node or subgraph was part of the GNN's training dataset \cite{zhang20222g3, dai2022hsi}. This can reveal sensitive information about individuals or entities whose data was used in training.
    \item \textbf{Attribute Inference Attacks}: Adversaries can infer sensitive node features (e.g., income, health status) from publicly available information or GNN outputs, leveraging the message-passing mechanism that propagates feature information across the graph \cite{zhang20222g3, dai2022hsi}.
\end{itemize}

To counter these threats, various privacy-preserving mechanisms have been explored. **Differential Privacy (DP)** stands out as a strong, mathematically provable privacy guarantee. DP mechanisms add carefully calibrated noise to data or computations to obscure individual contributions while preserving overall statistical properties \cite{zhang20222g3, dai2022hsi}. In GNNs, DP can be applied at different levels:
\begin{itemize}
    \item \textbf{Node Feature Perturbation}: Adding noise directly to node features before training, ensuring that the presence or absence of a single node's features does not significantly alter the model's output.
    \item \textbf{Gradient Perturbation}: Applying DP-SGD (Differential Private Stochastic Gradient Descent) during GNN training, adding noise to gradients to protect individual contributions to the model updates.
    \item \textbf{Graph Structure Perturbation}: This is particularly challenging due to the discrete and interconnected nature of graphs. Techniques involve adding or deleting edges with certain probabilities, or perturbing the adjacency matrix, to protect the privacy of connections \cite{zhang20222g3}. However, perturbing graph structure can significantly impact graph utility and model performance.
\end{itemize}
The main challenge with DP in GNNs lies in balancing the privacy budget with model utility, especially for graph structures where small perturbations can have far-reaching effects due to message passing.

Another promising approach is **Federated Learning (FL)**, which enables collaborative training of GNNs across multiple data owners without centralizing raw data \cite{liu2022gcg}. In FL for GNNs, local GNN models are trained on private graph partitions, and only model updates (e.g., gradients or aggregated parameters) are shared with a central server. This inherently protects raw node features and graph structures from being exposed, making it suitable for scenarios with distributed and sensitive graph data, such as medical networks or financial transaction graphs. However, FL itself can still be vulnerable to inference attacks if not combined with additional privacy mechanisms like DP.

The comprehensive surveys by \cite{zhang20222g3} and \cite{dai2022hsi} synthesize these concerns, providing taxonomies for privacy attacks and defense mechanisms, and highlighting the unique challenges of graph data for trustworthiness. Despite significant progress in identifying and mitigating fairness and privacy issues, challenges remain in developing universally applicable, scalable, and provably robust solutions that do not incur significant trade-offs in model utility. Future research must continue to explore advanced differential privacy mechanisms tailored for graph structures, more sophisticated adversarial training methods that withstand adaptive attacks, and integrated frameworks that jointly optimize for fairness, privacy, and utility, ensuring GNNs operate ethically, securely, and without perpetuating or amplifying societal inequalities.