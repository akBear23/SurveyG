\section*{1. Introduction}
The landscape of machine learning has undergone a profound transformation with the advent of Graph Neural Networks (GNNs), a paradigm specifically engineered to process and learn from data structured as graphs \cite{wu20193b0, zhou20188n6, paper2022mw4}. Unlike traditional machine learning models that primarily operate on Euclidean data (e.g., images, text sequences, tabular data), GNNs are uniquely positioned to leverage the rich relational information and intricate topological structures inherent in graph-structured data \cite{velickovic2023p4r}. This capability is paramount in an increasingly interconnected world, where complex systems across science, technology, and society are naturally represented as graphs. From social networks and biological systems to molecular structures, transportation networks, and knowledge graphs, the ubiquity of graph-structured data has rendered GNNs an indispensable tool for extracting meaningful insights and making accurate predictions \cite{khemani2024i8r, wang2023zr0}.

The fundamental motivation behind the development of GNNs stems from the inherent limitations of conventional deep learning architectures when confronted with non-Euclidean data. Convolutional Neural Networks (CNNs), for instance, excel at processing grid-like data by exploiting local connectivity and translational invariance, but they lack a natural mechanism to handle the irregular and often dynamic connectivity patterns of graphs \cite{wu20193b0}. Similarly, Recurrent Neural Networks (RNNs) and Transformers are designed for sequential data, struggling to capture the non-linear, multi-directional dependencies prevalent in graphs. GNNs address this critical gap by extending the principles of deep learning, particularly the concept of localized feature aggregation, to arbitrary graph topologies. They operate on the premise of "message passing," where nodes iteratively aggregate information from their neighbors, thereby learning representations that encode both local structural patterns and node features \cite{gilmer2017neural}. This iterative message passing mechanism allows GNNs to effectively capture relational inductive biases, enabling them to model complex interactions and dependencies that are crucial for understanding graph-structured phenomena \cite{battaglia2018relational}.

The growing importance of GNNs is evidenced by their widespread adoption and state-of-the-art performance across an astonishingly diverse array of fields. In recommender systems, GNNs have revolutionized personalized recommendations by modeling user-item interactions as graphs, capturing high-order relationships that significantly enhance prediction accuracy \cite{gao2022f3h, ying20189jc, wu2020dc8, fan2019k6u, chang2021yyt, gao20213kp, sharma2022liz, wang2019vol, wu2018t43, yu2020u32, wang2020khd, chen20201cf, zhang20212ke, zhang2022atq, chang2023ex5, he202455s, chen2024gbe, sun2024pix}. In drug discovery and materials science, they are instrumental in predicting molecular properties and interactions by treating molecules as graphs, accelerating the design of new compounds and materials \cite{jiang2020gaq, reiser2022b08, batzner2021t07, klicpera20215fk, fung20212kw, li2021v1l, xia2023bpu, smith2024q8n, carlo2024a3g, vinh20243q3, gnanabaskaran20245dg, li2024gue, fang2024zd6, fang2024p34, zhang202483k, wander2024nnn, yao2024pyk}. Computer vision has seen GNNs applied to tasks ranging from object detection and scene understanding to point cloud processing and human pose estimation, where they model relationships between visual entities or geometric components \cite{chen2022mmu, sarlin20198a6, shi2019vl4, wang2021mxw, huang2021lpu, li2024yyl, castroospina2024iy2}. Furthermore, GNNs are making significant inroads into critical infrastructure domains such as intelligent transportation systems \cite{li2020fil, jin2023e18, rahmani2023kh4, zhou2024t2r}, wireless communication networks \cite{shen202037i, shen2022gcz, guo2022hu1, abode2024m4z}, and power systems \cite{liao202120x, varbella20242iz, ashraf202443e}, optimizing resource allocation and predicting system dynamics. Their utility extends to cybersecurity for intrusion detection and anomaly detection \cite{mitra2024x43, bilot20234ui, wu20210h4, shen2021sbk, tang2022g66, chai2022nf9, huoh2023i97, kim2022yql, zhao2024aer, li2024r82} and even to epidemic modeling for forecasting disease spread \cite{wang202201n, liu20242g6}.

Despite their remarkable success, the development of GNNs has not been without its challenges. Early GNNs, often based on spectral graph theory or simplified message-passing schemes, quickly encountered limitations in expressiveness, struggling to distinguish between non-isomorphic graphs that are structurally distinct \cite{xu2018c8q, morris20185sd, oono2019usb, garg2020z6o, chen2020e6g, wijesinghe20225ms, balcilar2021di1, wang2022u2l, feng20225sa, chen20241tu, kanatsoulis2024l6i, graziani2024lgd, benedikt2024153, horck2024a8s}. A pervasive issue, particularly in deep GNNs, is "over-smoothing," where node representations converge to indistinguishable values across layers, leading to a loss of discriminative power \cite{li2021orq, chen2019s47, cai2020k4b, liu2020w3t, rusch2023xev, zeng2022jhz, wu2023aqs, peng2024t2s}. Furthermore, many foundational GNNs implicitly assume "homophily," meaning connected nodes share similar features or labels. This assumption breaks down in "heterophilous" graphs, common in real-world scenarios, where connected nodes are often dissimilar \cite{ma2021sim, zhu2020c3j, luan2021g2p, luan202272y, zheng2022qxr, du2021kn9, bing2022oka, mao202313j}. Scalability to large graphs, robustness against adversarial attacks \cite{zhang2020b0m, xu2019l8n, zgner2019bbi, he2020kz4, gosch20237yi, zhang2020jrt, mujkanovic20238fi, dai2023tuj, xia2024xc9, aburidi2024023, wang2024p88, zhang2024370, abbahaddou2024bq2}, and the critical need for interpretability and fairness \cite{yuan20208v3, ying2019rza, yuan2020fnk, dong202183w, dong2021qcg, lucic2021p70, agarwal2022xfp, cui2022pap, wu2022vcx, zhang2021wgf, wang20214ku, dai2022hsi, zhang20222g3, chen2024woq, luo2024euy, bui2024zy9, lyu2023ao0, wang2024j6z, lu2024eu9, luo20240ot} represent ongoing research frontiers. The field has also recognized the necessity for more rigorous and realistic evaluation methodologies, moving beyond superficial comparisons to truly assess model capabilities \cite{li2023o4c, dwivedi20239ab}.

This comprehensive literature review aims to consolidate the vast and rapidly evolving body of knowledge surrounding GNNs. It seeks to provide a structured understanding of their foundational principles, evolutionary trajectory, diverse methodologies, and far-reaching applications. By critically analyzing the strengths and limitations of various GNN architectures and techniques, this review endeavors to identify persistent research gaps and illuminate promising future directions. The subsequent sections will delve into the evolution of GNN architectures, explore advanced methodologies, survey their applications across various domains, and discuss the critical challenges and future opportunities that will shape the next generation of graph machine learning.

\subsection*{1.1. The Rise of Graph Neural Networks}
The emergence of Graph Neural Networks (GNNs) marks a pivotal moment in machine learning, driven by the increasing recognition that much of the world's data is inherently relational and best represented as graphs \cite{wu20193b0, zhou20188n6}. Traditional machine learning, particularly deep learning, has achieved monumental success on data with regular grid structures like images (CNNs) and sequences like text (RNNs, Transformers). However, these architectures fundamentally struggle with the irregular, non-Euclidean nature of graphs, where nodes have varying numbers of neighbors and no inherent ordering. This fundamental mismatch motivated the development of GNNs, which generalize the concept of convolution and aggregation to arbitrary graph structures \cite{velickovic2023p4r, jegelka20222lq}.

The foundational concept of GNNs can be traced back to early works that attempted to extend neural networks to graphs by iteratively propagating information across edges \cite{gori2005new, scarselli2009graph}. These early models laid the groundwork for the "message passing" paradigm, where each node updates its representation by aggregating information from its immediate neighbors and its own previous state. This iterative process allows nodes to gather increasingly rich contextual information from their local neighborhoods, effectively learning representations that capture both node features and structural roles. The resurgence of interest in GNNs was significantly catalyzed by the introduction of models like Graph Convolutional Networks (GCNs) \cite{kipf2016semi} and Graph Attention Networks (GATs) \cite{velickovic2017graph}, which provided scalable and effective implementations of this message-passing framework. GCNs, for instance, simplified spectral graph convolutions into a first-order approximation, demonstrating compelling performance on semi-supervised node classification tasks \cite{kipf2016semi}. Building on this, GATs introduced an attention mechanism, allowing nodes to assign different weights to their neighbors, thereby learning more flexible and powerful aggregation functions \cite{velickovic2017graph}. This innovation addressed a key limitation of GCNs, which treated all neighbors equally, and exemplified the field's shift towards more adaptive and expressive aggregation schemes.

The initial success of these models quickly highlighted both the immense potential and the inherent limitations of the nascent GNN paradigm. A critical tension emerged regarding the "expressive power" of GNNs, specifically their ability to distinguish between non-isomorphic graphs. Early theoretical analyses, notably by \cite{xu2018c8q} and \cite{morris20185sd}, demonstrated that many standard GNNs are no more powerful than the 1-dimensional Weisfeiler-Lehman (1-WL) test, a heuristic for graph isomorphism. This implies that these GNNs cannot differentiate between certain structurally distinct graphs, limiting their capacity to capture complex graph patterns \cite{oono2019usb, garg2020z6o}. This limitation spurred significant research into developing more expressive GNN architectures, such as higher-order GNNs \cite{morris20185sd} or those incorporating richer structural information. For example, \cite{michel2023hc4} introduced Path Neural Networks (PathNNs) which explicitly leverage path information, showing that by operating on "annotated sets of paths," their variants can surpass the expressive power of the 1-WL algorithm and even distinguish graphs indistinguishable by the 3-WL algorithm. This directly addresses the expressiveness bottleneck by moving beyond simple local neighborhood aggregation to capture more global and complex structural patterns.

Another significant challenge that quickly became apparent was "over-smoothing," where repeated message passing in deep GNNs causes node representations to become increasingly similar, eventually converging to a single point, thus losing discriminative power \cite{chen2019s47, cai2020k4b, liu2020w3t, rusch2023xev}. This phenomenon fundamentally limits the depth of GNNs, contrasting sharply with the success of very deep architectures in other domains like computer vision. Various strategies have been proposed to mitigate over-smoothing, including residual connections \cite{li2021orq, zeng2022jhz}, regularization techniques like DropEdge \cite{rong2019dropedge}, and more sophisticated aggregation schemes. For instance, \cite{kang2024fsk} introduces FROND, a framework that leverages fractional calculus to generalize continuous GNNs. By using fractional-order derivatives, FROND inherently integrates the entire historical trajectory of node features, enabling memory-dependent dynamics and analytically demonstrating an algebraic (slower) rate of convergence to stationarity, thereby mitigating over-smoothing compared to the exponential convergence of integer-order models. This represents a novel theoretical and architectural approach to a persistent problem.

Furthermore, the implicit assumption of "homophily" (the tendency of connected nodes to be similar) in many early GNNs posed a significant hurdle for real-world graphs that often exhibit "heterophily" (connected nodes being dissimilar) \cite{ma2021sim, zhu2020c3j}. Social networks, for example, might connect individuals with diverse interests, while protein-protein interaction networks can link functionally distinct proteins. This tension between model assumption and data reality led to the development of GNNs specifically designed to handle heterophilous graphs. \cite{li2022315} addresses this by proposing GloGNN, which learns a signed coefficient matrix for *all* nodes, effectively capturing global homophily and heterophily simultaneously. Instead of relying on fixed local neighborhoods, GloGNN performs global aggregation with learned coefficients, allowing it to adaptively assign positive coefficients to homophilous nodes and negative/small positive coefficients to heterophilous ones. Crucially, it achieves this with linear time complexity, overcoming the computational burden of naive global aggregation. This exemplifies a critical evolution in GNN design, moving from local, homophily-biased aggregation to more adaptive, global, and heterophily-aware mechanisms. The emergence of non-convolutional GNNs, such as the Random Walk with Unifying Memory (RUM) neural network proposed by \cite{wang2024oi8}, further illustrates the field's drive to overcome these fundamental limitations. RUM, by processing graph information solely through random walk trajectories and RNNs, offers a joint remedy to limited expressiveness, over-smoothing, and over-squashing, demonstrating superior theoretical properties and competitive empirical performance without relying on traditional convolution operators. This signifies a paradigm shift, exploring alternatives to the message-passing framework to unlock deeper and more robust graph representations.

\subsection*{1.2. Importance of Graph-Structured Data}
The pervasive nature of graph-structured data in the modern world underscores the critical importance of Graph Neural Networks (GNNs). Graphs serve as a natural and powerful abstraction for complex systems where entities (nodes) interact or relate to one another (edges) \cite{velickovic2023p4r, wu2022ptq}. Unlike tabular data or sequences, graphs explicitly encode relational information, which is often the most critical aspect for understanding system behavior and making accurate predictions. This section elaborates on why graph-structured data is so important and how GNNs are uniquely positioned to unlock its value.

One of the primary reasons for the importance of graph-structured data lies in its ability to model intricate relationships that are often overlooked or simplified by other data representations. Consider social networks, where individuals are nodes and friendships or interactions are edges. Understanding influence, community detection, or personalized recommendations fundamentally relies on analyzing these complex relational patterns \cite{fan2019k6u, sharma2022liz}. Traditional machine learning models, if applied to such data, would typically flatten the graph into node features (e.g., demographics) or aggregate edge features, thereby losing the rich topological context. GNNs, through their message-passing mechanism, directly operate on this relational structure, allowing information to flow and aggregate across the graph, thereby learning representations that intrinsically encode a node's position within the network and its interactions with neighbors \cite{wu20193b0}. This relational inductive bias is what makes GNNs so powerful for graph-structured data.

The impact of GNNs on diverse scientific and industrial domains further highlights the importance of graph data. In biology and chemistry, molecules are inherently graphs, with atoms as nodes and chemical bonds as edges. GNNs have become indispensable for tasks such as predicting molecular properties, drug-target interactions, and protein-protein interactions \cite{jiang2020gaq, reiser2022b08, batzner2021t07, klicpera20215fk, li2021v1l, xia2021s85, jha2022cj8, zeng2024fpp, chen2024h2c}. For instance, \cite{xia2023bpu} introduces Mole-BERT, rethinking pre-training GNNs for molecules, showcasing the specialized adaptations required to capture intricate chemical information. Similarly, in neuroscience, brain connectivity networks are modeled as graphs, enabling GNNs to analyze brain disorders and understand neural activity patterns \cite{bessadok2021bfy, cui2022mjr, cui2022pap, zhao2022fvg, mohammadi202476q, abadal2024w7e, luo2024h2k}. The ability of GNNs to learn from these complex, irregular structures has opened new avenues for discovery and understanding in these fields.

Beyond scientific applications, graph data is crucial in various real-world systems. Recommender systems, as extensively surveyed by \cite{gao2022f3h, wu2020dc8, gao20213kp}, represent user-item interactions as bipartite graphs, where GNNs can effectively capture high-order collaborative filtering signals, leading to more accurate and diverse recommendations \cite{ying20189jc, wang2019vol, chang2021yyt}. In urban computing, GNNs are employed for traffic flow forecasting, ride-sharing optimization, and intelligent transportation systems, where roads, sensors, and vehicles form dynamic graphs \cite{li2020fil, wu2020hi3, jin2023e18, rahmani2023kh4}. For example, \cite{jin2023ijy} provides a comprehensive survey on GNNs for time series, covering forecasting, classification, imputation, and anomaly detection, underscoring their versatility in dynamic graph settings. The financial sector benefits from GNNs in fraud detection, where transactions and entities form complex networks, allowing GNNs to identify anomalous patterns indicative of fraudulent activities \cite{inan2023fa7, duan2024que, liu2024sbb, sun2024ztz}. Even in computer vision, GNNs are increasingly used to model relationships between objects in a scene, parse human poses, or process 3D point clouds and meshes, as detailed in the task-oriented survey by \cite{chen2022mmu}.

The importance of graph-structured data also brings forth unique challenges that GNNs are designed to address. The inherent non-Euclidean nature means that standard operations like convolution or pooling, which assume fixed-size inputs and local translational invariance, are not directly applicable. GNNs overcome this by defining aggregation functions that are permutation-invariant to the order of neighbors, ensuring that the learned representations are independent of how the adjacency list is ordered \cite{xu2018c8q}. Furthermore, real-world graphs are often dynamic, evolving over time with new nodes and edges appearing or disappearing. This necessitates the development of dynamic GNNs capable of adapting to continuous changes, as highlighted by works like \cite{longa202399q} on temporal graphs and \cite{zhang2022uih} on spatio-temporal distribution shifts. The challenge of heterophily, where connected nodes are dissimilar, is another critical aspect of real-world graph data. While early GNNs struggled with this, models like GloGNN \cite{li2022315} demonstrate how to effectively capture global homophily even in heterophilous settings by learning adaptive, signed aggregation coefficients, showcasing the field's progress in handling complex graph properties. The need for robust and fair GNNs is also paramount given the sensitivity of graph data in applications like social networks and healthcare \cite{dong202183w, dong2021qcg, dai2020p5t, wang2022531, li20245zy, fan2022m67, xia20247w9, luo20240ot}. This continuous innovation in GNN architectures and training methodologies underscores the profound and growing importance of graph-structured data as a fundamental data type in modern machine learning.

\subsection*{1.3. Scope and Structure of the Review}
This comprehensive literature review is meticulously designed to provide a structured and critical understanding of Graph Neural Networks (GNNs), spanning their foundational principles, evolutionary trajectory, diverse methodologies, and far-reaching applications. Given the rapid proliferation of research in this domain, a systematic consolidation of knowledge is essential to identify key advancements, persistent challenges, and promising future directions. The scope of this review encompasses the core architectural designs, theoretical underpinnings, practical considerations, and societal implications of GNNs, while also highlighting critical evaluation practices.

The review begins by establishing the fundamental motivation for GNNs, recognizing the inherent limitations of traditional machine learning models when confronted with non-Euclidean, graph-structured data. As discussed in the preceding subsections, the ubiquity of relational information across scientific, technological, and social domains necessitates specialized tools capable of effectively processing and learning from graphs \cite{wu20193b0, zhou20188n6, velickovic2023p4r}. This introductory section sets the stage by briefly touching upon the transformative impact GNNs have achieved and the critical challenges they address, guiding the reader through the subsequent, more detailed exploration.

The subsequent sections of this review will be structured to provide a logical and progressive understanding of the GNN landscape. We will first delve into the **Evolution of GNN Architectures**, tracing their development from early spectral and spatial approaches to more advanced and specialized designs. This section will critically compare foundational models like GCNs \cite{kipf2016semi} and GATs \cite{velickovic2017graph} with later innovations that address limitations such as expressiveness \cite{xu2018c8q, michel2023hc4}, over-smoothing \cite{chen2019s47, kang2024fsk}, and scalability \cite{fey2021smn, vasimuddin2021x7c, bojchevski2020c51}. For instance, the shift from purely convolutional designs to non-convolutional, walk-based architectures like RUM \cite{wang2024oi8} will be analyzed as a response to fundamental architectural bottlenecks.

Following this, the review will explore **Advanced Methodologies and Training Paradigms** in GNNs. This includes a detailed examination of techniques for handling graph heterogeneity \cite{li2022315, zheng2022qxr, du2021kn9}, dynamic graphs \cite{longa202399q, jin2023e18}, and large-scale graphs \cite{fey2021smn}. We will also cover advancements in self-supervised learning \cite{xie2021n52, fatemi2021dmb, zhang20211dl}, pre-training strategies \cite{hu2019r47, hu2020u8o, lu20213kr, sun2022d18, liu2023ent, sun2023vsl}, and data augmentation techniques \cite{zhao2020bmj}. A critical analysis of the trade-offs between model complexity, computational efficiency, and performance will be central to this section.

The review will then transition to a comprehensive overview of **Applications of GNNs Across Diverse Domains**. This section will systematically categorize and discuss the impact of GNNs in areas such as recommender systems \cite{gao2022f3h, wu2020dc8}, computer vision \cite{chen2022mmu}, natural language processing \cite{wu2023zm5, wang2023wrg, wang2024nuq, li202444f}, bioinformatics and materials science \cite{reiser2022b08, jiang2020gaq, zhang2021f18}, and critical infrastructure management \cite{shen2022gcz, liao202120x, rahmani2023kh4}. Each application area will be accompanied by examples of specific problems GNNs solve, their core innovations in that context, and their demonstrated empirical success.

Finally, the review will dedicate a section to **Critical Challenges, Trustworthiness, and Future Directions**. This crucial part will synthesize the unresolved debates and persistent limitations within the GNN field, including issues of interpretability and explainability \cite{yuan2021pgd, yuan2020fnk, agarwal2022xfp, chen2024woq}, fairness and bias mitigation \cite{dong202183w, dong2021qcg, dai2020p5t, li20245zy}, and robustness against adversarial attacks \cite{zhang2020b0m, xu2019l8n, zgner2019bbi, gosch20237yi, mujkanovic20238fi}. A particular focus will be placed on the need for rigorous and realistic evaluation methodologies, as highlighted by \cite{li2023o4c}, which critically examines current pitfalls in benchmarking link prediction models. This section will also project future research trajectories, such as the integration of GNNs with large language models \cite{li2024gue}, quantum computing \cite{liao20249wq}, and novel theoretical frameworks like fractional calculus \cite{kang2024fsk}.

By adopting this structured approach, this literature review aims to consolidate fragmented knowledge, identify critical research gaps, and provide a holistic, analytical understanding of the GNN landscape. It serves as a valuable resource for both seasoned researchers and newcomers, guiding them through the complexities and exciting opportunities in this rapidly evolving field.