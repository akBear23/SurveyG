\subsection{Pre-training and Prompt-based Adaptation}

The efficacy of Graph Neural Networks (GNNs) in real-world applications is often hampered by the scarcity of labeled graph data. To overcome this, advanced techniques for efficient knowledge transfer and adaptation have emerged, primarily through foundational self-supervised pre-training and the innovative paradigm of prompt tuning. These strategies aim to learn generalizable representations from abundant unlabeled graph data, minimize the 'objective gap' between pre-training and downstream tasks, and enable data-efficient adaptation.

Early efforts laid the groundwork for pre-training GNNs to learn robust, transferable representations. \cite{hu2019r47} pioneered a systematic investigation into GNN pre-training strategies, proposing a combined node- and graph-level self-supervised approach. Their work introduced novel self-supervised tasks like Context Prediction and Attribute Masking, demonstrating significant generalization improvements and effectively mitigating "negative transfer" often encountered in naive pre-training. Building upon this, \cite{hu2020u8o} advanced the field by proposing GPT-GNN, a generative pre-training framework that explicitly models the intricate dependency between node attributes and graph structure during an attributed graph generation task. This approach provided a more holistic and semantically rich pre-training objective, crucial for complex real-world graphs. Further refining the pre-training paradigm, \cite{lu20213kr} introduced L2P-GNN, which leverages meta-learning to explicitly optimize for rapid adaptation during the pre-training process. By framing pre-training as an optimization-based meta-learning problem, L2P-GNN directly addresses the objective divergence between pre-training and fine-tuning, ensuring the learned initialization is inherently optimized for quick transferability to new tasks.

Despite these advancements in learning robust initializations, a persistent "objective gap" often remained between pre-training pretext tasks and diverse downstream tasks, necessitating costly and inefficient fine-tuning. This challenge spurred the development of prompt-based adaptation for GNNs, a paradigm shift inspired by its success in natural language processing. \cite{sun2022d18} introduced GPPT, the first framework to apply prompt tuning to GNNs. It ingeniously reformulates downstream node classification tasks to mimic the pre-training objective (e.g., masked edge prediction) by using a novel "graph prompting function" and "token pairs," thereby bridging the objective gap and enabling efficient knowledge transfer without tedious fine-tuning. However, GPPT's prompt design was somewhat specialized. Addressing this limitation, \cite{fang2022tjj} proposed Universal Prompt Tuning for GNNs, introducing Graph Prompt Feature (GPF) which modifies the *input graph's feature space* rather than altering its structure or reformulating tasks. This innovation provided a universal and theoretically grounded approach, compatible with *any* pre-trained GNN and *any* pre-training strategy, demonstrating superior performance over fine-tuning with significantly fewer tunable parameters.

Further extending the versatility of prompt tuning, \cite{liu2023ent} introduced GraphPrompt, a framework that unifies pre-training and diverse downstream tasks (node and graph classification) into a common "subgraph similarity" template. GraphPrompt employs novel learnable prompts that guide the `ReadOut` aggregation operation, allowing a single pre-trained GNN to adapt to various tasks in few-shot settings by adaptively fusing node representations. Most recently, the paradigm has expanded to multi-modal learning. \cite{li202444f} proposed Morpher, a multi-modal prompt learning framework that aligns independently pre-trained GNNs with Large Language Models (LLMs) under *extremely weak text supervision*. This groundbreaking work enables GNNs to gain real-world semantic understanding and achieve CLIP-style zero-shot generalization to unseen classes, significantly enhancing data efficiency and model versatility by bridging the conceptual gap between graph structure and natural language semantics.

In conclusion, the evolution from foundational self-supervised pre-training to sophisticated prompt-based adaptation marks a critical intellectual trajectory in GNN research. While initial pre-training strategies focused on learning generalizable representations and mitigating negative transfer, the advent of prompt tuning has revolutionized adaptation by minimizing the objective gap and enabling highly efficient, few-shot, and even zero-shot generalization. The latest advancements, particularly in multi-modal prompt learning, are pushing GNNs towards greater semantic awareness and versatility. However, an ongoing challenge lies in developing truly universal pre-training objectives that inherently support diverse downstream tasks without requiring complex prompt engineering, paving the way for more intrinsically adaptable and robust GNN architectures.