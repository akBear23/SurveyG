\subsection{Evolution of Graph Representation Learning}

The ubiquitous presence of graph-structured data across domains, encompassing social networks, biological systems, knowledge graphs, and recommender systems, has consistently driven the demand for effective methods to analyze and extract insights \cite{wu20193b0, wang2023zr0}. Historically, understanding these intricate relationships relied on traditional graph algorithms. These methods, including shortest path computations (e.g., Dijkstra's algorithm \cite{dijkstra1959note}), centrality measures (e.g., degree, betweenness, closeness \cite{freeman1978centrality}), and community detection algorithms (e.g., Louvain method \cite{blondel2008fast}, Girvan-Newman \cite{girvan2002community}), are predominantly combinatorial. While foundational for graph theory, their utility in modern machine learning pipelines is limited. They typically operate on explicit graph structures, struggle with the inherent sparsity and high dimensionality of real-world networks, and become computationally prohibitive for large-scale, complex, and dynamic graphs \cite{zhang2021jqr}. Crucially, these methods often yield scalar outputs or discrete labels, such as a single centrality score or a community ID, rather than rich, continuous feature vectors that are essential for end-to-end learning within deep neural networks. Their reliance on predefined heuristics further restricts their adaptability to varying graph properties and diverse downstream tasks.

A subsequent wave of approaches emerged, focusing on learning low-dimensional, continuous representations (embeddings) of nodes or entire graphs. Often termed "shallow" methods, these techniques aimed to capture structural proximity in a vector space where similar nodes are embedded closely together. Examples include matrix factorization methods, which decompose adjacency or Laplacian matrices to derive latent representations, and random walk-based models. Pioneering among these were DeepWalk \cite{Perozzi14} and Node2Vec \cite{Grover16}, which generated node sequences via truncated random walks and then applied word2vec-like models to learn embeddings. These methods offered significant improvements in tasks such as link prediction and node classification by providing a more compact and amenable representation for traditional machine learning classifiers, often outperforming purely combinatorial approaches.

Despite their advancements, these shallow embedding techniques suffered from critical limitations that underscored the need for a more sophisticated paradigm. They were predominantly *transductive*, meaning they could only learn representations for nodes present during training and struggled to generalize to unseen nodes or dynamic graphs where the topology evolves over time \cite{wu20193b0, wang2023zr0}. Furthermore, their capacity to automatically learn hierarchical features from complex graph topologies was limited; they often relied on fixed feature engineering or simple aggregation rules, failing to capture intricate, multi-level patterns inherent in rich graph data. While some simpler graph-based methods, like label propagation techniques, can be surprisingly effective for specific transductive node classification tasks by leveraging label correlation \cite{huang20209zd}, they do not address the fundamental need for learning rich, generalizable feature representations directly from graph structure. This inability to adapt to new structures, coupled with a lack of end-to-end learning capabilities, highlighted a fundamental constraint in handling the increasing scale, complexity, and dynamism of real-world graph data.

This recognition spurred a profound paradigm shift towards *learning-based representations*, emphasizing the need for models that could automatically learn features and patterns directly from graph structures, thereby overcoming the constraints of prior methods. The advent of neural network models specifically designed to operate on non-Euclidean graph data marked the genesis of Graph Neural Networks (GNNs). This new class of models was conceptualized to extend the successes of deep learning from Euclidean domains (like images and text) to arbitrarily structured graphs, enabling automatic feature extraction and end-to-end learning. Unlike traditional graph kernels, which often rely on hand-crafted combinatorial features, GNNs learn these features adaptively \cite{feng2022914}. The core idea behind GNNs is to iteratively aggregate information from a node's local neighborhood through learnable transformations, allowing nodes to learn representations that reflect their structural role and feature context within the graph. This iterative message-passing mechanism fundamentally addresses the limitations of previous methods by enabling:
\begin{enumerate}
    \item \textbf{Inductive Generalization}: Learnable aggregation functions allow GNNs to process unseen nodes and entire graphs, a critical capability for dynamic and evolving networks.
    \item \textbf{Hierarchical Feature Learning}: Multiple layers of message passing enable the capture of increasingly complex and abstract patterns, moving beyond simple proximity to understand multi-level structural roles.
    \item \textbf{End-to-End Learning}: GNNs integrate feature learning directly into the optimization of downstream tasks, allowing for more powerful and task-specific representations.
    \item \textbf{Scalability}: While challenging, the localized nature of message passing, especially with techniques like neighbor sampling, offers a more scalable pathway than global matrix operations.
\end{enumerate}
The theoretical underpinnings of GNNs, particularly their connection to the Weisfeiler-Leman test and their ability to approximate multiset functions, further establish their enhanced representational power compared to earlier approaches \cite{jegelka20222lq}. This conceptual breakthrough set the stage for subsequent architectural advancements that would transform GNNs into a practical and powerful tool, capable of automatically learning expressive features and patterns from complex graph data, thereby establishing a new frontier in graph representation learning. The subsequent sections will delve into the foundational models and architectural breakthroughs that propelled GNNs into mainstream artificial intelligence.