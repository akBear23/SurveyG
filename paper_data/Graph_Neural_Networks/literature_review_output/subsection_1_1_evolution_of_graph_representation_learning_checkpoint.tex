\subsection*{Evolution of Graph Representation Learning}

The analysis of graph-structured data has historically presented a formidable challenge in machine learning, necessitating methods capable of discerning intricate relational patterns that underpin complex systems. Initially, approaches to graph analysis were rooted in traditional graph theory algorithms, such as shortest path computations, connectivity analysis, and various centrality measures, which provided foundational insights into network structure. Complementing these were statistical methods like spectral clustering \cite{ng2002spectral}, which leveraged the eigenvalues and eigenvectors of graph matrices to partition nodes, and graph kernels \cite{gartner2003graph, shervashidze2011weisfeiler}, which measured graph similarity by comparing substructures or paths. More recently, the concept of integrating graph kernels into GNNs has emerged to enhance interpretability and representation ability \cite{feng2022914}. Manifold learning methods, such as Laplacian Eigenmaps \cite{belkin2003laplacian}, further aimed to embed graph structures into lower-dimensional Euclidean spaces while preserving topological properties.

However, these pre-neural approaches, while valuable for specific tasks on smaller, static graphs, suffered from significant limitations. They often relied on extensive manual feature engineering, requiring domain expertise to define relevant graph properties. A major bottleneck was their high computational cost, frequently exhibiting polynomial complexity, such as $O(N^3)$ for many kernel methods or spectral decompositions on a graph with $N$ nodes \cite{shervashidze2011weisfeiler}, rendering them impractical for large-scale graphs. Furthermore, these methods struggled inherently with generalization to unseen nodes or dynamic graph structures due to their transductive nature, and their capacity to capture complex, multi-hop structural information or integrate rich node features was constrained, leading to a bottleneck in extracting deeper insights from increasingly complex real-world graphs \cite{wu2022ptq}.

A crucial step towards automated feature extraction came with early graph embedding techniques, which sought to learn low-dimensional vector representations for nodes or entire graphs. Methods like matrix factorization (e.g., Singular Value Decomposition on adjacency matrices) provided a way to capture latent structural information. More notably, random-walk based algorithms such as DeepWalk \cite{perozzi2014deepwalk} and Node2Vec \cite{grover2016node2vec} offered a more scalable approach to learn node embeddings by simulating random walks and applying word2vec-like models. These were joined by other influential methods like LINE \cite{tang2015line}, which aimed to preserve both first-order and second-order proximities. These techniques effectively captured proximity information within the graph, allowing for downstream tasks like node classification or link prediction by learning fixed embeddings for each node. However, these early embedding methods frequently operated in an unsupervised or semi-supervised manner, often overlooking explicit node attributes and struggling to capture the full spectrum of complex, multi-hop structural dependencies that are vital for many predictive tasks \cite{zhang2018kdl}. Their inherent limitations—particularly their transductive nature, meaning they cannot directly generalize to unseen nodes or dynamically evolving graphs without complete retraining \cite{zhang2018kdl, ying20189jc}, insufficient capacity to leverage rich node features effectively, and limited ability for complex relational reasoning—highlighted a pressing need for a more powerful paradigm \cite{khemani2024i8r}.

This confluence of limitations necessitated a fundamental paradigm shift towards learning-based representations, where neural network models specifically designed for graphs emerged as a promising solution \cite{wu2022ptq}. The conceptual genesis of Graph Neural Networks (GNNs) marked this pivotal moment, driven by the ambition to extend the success of deep learning from Euclidean data (like images and text) to the irregular, non-Euclidean structures inherent in graphs \cite{khemani2024i8r}. Pioneering works by Gori et al. \cite{gori2005new} and Scarselli et al. \cite{scarselli2009graph} laid the theoretical foundations for this new class of models, proposing a framework to learn node representations by iteratively propagating and aggregating information across graph connections until a stable fixed-point was reached. This approach offered a powerful mechanism to automatically learn features and patterns directly from the graph topology and node attributes, moving beyond hand-crafted features or simple proximity embeddings. By enabling neural networks to process and aggregate information across graph connections, GNNs offered a powerful mechanism to overcome the constraints of prior methods, particularly in handling large-scale, complex, and dynamic graphs, and in capturing intricate, multi-hop structural dependencies. This foundational shift set the essential context for the development and widespread adoption of GNNs, which form the core subject of this review.