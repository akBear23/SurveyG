\subsection{Geometric and Equivariant GNNs}

Graph Neural Networks (GNNs) are increasingly applied to tasks involving 3D structures like molecules, proteins, and physical systems, where respecting geometric symmetries and transformations is paramount. This section explores the development of GNNs specifically designed to incorporate these symmetries, leading to more data-efficient, robust, and physically consistent models.

Early efforts to enforce geometric symmetries often relied on complex higher-order representations or were limited to 3D spaces, posing challenges for scalability and efficiency. A significant advancement in this area is the introduction of E(n) Equivariant Graph Neural Networks (EGNNs) by \cite{satorras2021pzl}. The core innovation of EGNNs lies in their Equivariant Graph Convolutional Layer (EGCL), which directly updates node coordinates using relative differences and aggregates messages based on squared relative distances, thereby preserving E(n) equivariance for both scalar features and vector positions without resorting to computationally expensive spherical harmonics. This simpler and more efficient architecture has demonstrated superior performance in tasks such as N-body system simulations, significantly outperforming prior methods like SE(3) Transformer and Tensor Field Networks. Building upon this foundation, \cite{batzner2021t07} introduced Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network that learns interatomic potentials for molecular dynamics simulations. NequIP leverages E(3)-equivariant convolutions for interactions of geometric tensors, achieving state-of-the-art accuracy and remarkable data efficiency, often requiring orders of magnitude less training data than other models, which is crucial for scientific applications using high-fidelity quantum chemical data.

While these architectural innovations proved highly effective, a comprehensive theoretical framework to characterize the expressive power of geometric GNNs was lacking, similar to the Weisfeiler-Leman (WL) test for non-geometric graphs \cite{morris20185sd, xu2018c8q}. To address this, \cite{joshi20239d0} developed the Geometric Weisfeiler-Leman (GWL) test, a symmetry-aware generalization of the WL test for geometric graphs. This framework provides a theoretical upper bound for the expressivity of geometric GNNs and offers insights into the distinct roles and power of invariant versus equivariant layers and higher-order tensors, thereby guiding the principled design of more powerful architectures.

Further enhancing the robustness and expressivity of geometric GNNs, the stability of positional encodings (PEs) became a critical area of focus. Existing PE methods often suffered from instability, particularly when dealing with graph automorphisms or small eigengaps, and lacked guaranteed permutation equivariance. \cite{wang2022p2r} tackled this by proposing the PEG architecture, which employs separate channels for node and positional features and imposes O(p) equivariance for positional features. This approach achieves provable stability by depending on a larger eigengap, ensuring that the positional information used by geometric GNNs is both consistent and reliable.

Despite these advancements, a fundamental limitation of spatial Message Passing GNNs (MPGNNs) in geometric contexts is the "over-squashing" phenomenon, which restricts long-range information exchange and limits their receptive field and expressive power. To overcome this, \cite{geisler2024wli} introduced Spatio-Spectral Graph Neural Networks (S2GNNs), a novel hybrid architecture that synergistically combines local spatial message passing with global spectral filtering. S2GNNs are provably capable of vanquishing over-squashing, offer superior approximation bounds, and provide stable positional encodings "for free," representing a significant step towards more powerful and robust geometric GNNs for complex 3D structures.

In conclusion, the field of geometric and equivariant GNNs has rapidly evolved from practical architectural designs that enforce E(n) equivariance \cite{satorras2021pzl} and demonstrate their data efficiency in scientific domains \cite{batzner2021t07}, to establishing rigorous theoretical foundations for their expressive power \cite{joshi20239d0}. Subsequent work has addressed crucial aspects like stable positional encoding \cite{wang2022p2r} and fundamental architectural limitations such as over-squashing \cite{geisler2024wli}. While significant progress has been made in building physically consistent and data-efficient models, ongoing challenges include further scaling these computationally intensive models to extremely large and dense geometric graphs, and developing more sophisticated hybrid architectures that can seamlessly integrate diverse geometric priors and multi-scale information.