\subsection{Handling Structural Heterogeneity and Imperfect Data}
Real-world graphs rarely conform to idealized structures, often exhibiting a complex interplay of homophilic and heterophilic patterns, alongside pervasive issues of missing information, noisy connections, and weak supervision. Developing Graph Neural Networks (GNNs) that are robust to these complexities is paramount for reliable performance and broad applicability. This subsection explores advanced GNN architectures and techniques designed to adapt to structural disparities and learn effectively from imperfect graph data.

A significant challenge for conventional GNNs stems from their implicit assumption of homophily, where connected nodes share similar attributes or labels. \cite{ma2021sim} critically challenged this strict assumption, demonstrating that GCNs can perform well on certain heterophilous graphs by distinguishing between "good" and "bad" heterophily based on neighborhood patterns. This nuanced understanding paved the way for more adaptive designs, which were systematically categorized in a comprehensive survey by \cite{zheng2022qxr}, highlighting key design principles for GNNs operating in heterophilic environments. To explicitly address mixed structural patterns, early adaptive filtering mechanisms emerged. \cite{luan202272y} introduced the Adaptive Channel Mixing (ACM) framework, which augments baseline GNNs by adaptively combining low-pass, high-pass, and identity filters in a node-wise and local manner, allowing GNNs to extract richer, localized information. Building on this, \cite{mao202313j} rigorously demystified "structural disparity" within single graphs, demonstrating that a "one-size-fits-all" filtering approach leads to significant performance disparities on minority structural patterns. Directly addressing this limitation, \cite{han2024rkj} proposed NODE-MOE, a novel Mixture of Experts approach for node-wise filtering. NODE-MOE employs a sophisticated gating model to dynamically select and apply different "expert" GNN filters (e.g., low-pass, high-pass) to individual nodes, significantly enhancing GNN robustness to mixed homophilic and heterophilic patterns. Complementing these node-wise approaches, \cite{li2022315} developed GloGNN and GloGNN++, which find global homophily in heterophilous graphs by learning a signed coefficient matrix for all nodes, effectively combining low-pass and high-pass filtering with linear time complexity. Furthermore, \cite{bianchi20194ea} introduced GNNs with Convolutional ARMA Filters, offering more flexible frequency responses than traditional polynomial filters, which can implicitly better handle diverse structural patterns. More recently, \cite{yang2024vy7} proposed Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA), which utilizes separate embeddings for attributes and structures and establishes interconnections through soft association. GNN-SATA further employs a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) to dynamically remove or add edges, making the model better fit for graphs with varying degrees of homophily or heterophily, demonstrating improved accuracy, especially on highly heterophilic datasets.

Beyond adapting to existing but complex structural patterns, a separate and equally critical challenge arises when the graph structure itself is unreliable, incomplete, or altogether absent, or when node features and labels are scarce. Strategies for learning with such imperfect data primarily include structure learning, data augmentation, and self-supervised learning. When the graph structure is noisy, incomplete, or entirely absent, \cite{chen2020bvl} introduced Iterative Deep Graph Learning (IDGL), an end-to-end framework that jointly and iteratively learns optimal graph structures and GNN parameters, even from raw features. This approach is robust to initial graph imperfections and offers a scalable anchor-based version (IDGL-ANCH). Similarly, \cite{fatemi2021dmb} proposed SLAPS, which leverages self-supervision through a denoising autoencoder to guide structure learning, effectively addressing the "supervision starvation" problem where many edges receive insufficient supervision from the primary task. For enhancing GNN resilience through data augmentation, \cite{zhao2020bmj} developed the GAUG framework. GAUG uses a learned edge predictor (e.g., a Graph Auto-Encoder) to strategically add missing intra-class edges and remove noisy inter-class edges, thereby "denoising" and "completing" the graph structure.

The broader paradigm of self-supervised learning (SSL) has emerged as a powerful tool for learning with weak or incomplete graph information, as comprehensively reviewed by \cite{xie2021n52}. SSL methods for GNNs aim to leverage abundant unlabeled graph data by generating self-supervision signals, often through contrastive learning or predictive tasks, to learn robust representations. Addressing the challenging scenario of "extreme weak information," where structure, features, and labels are simultaneously deficient, \cite{liu2023v3e} introduced D2PT (Dual-channel Diffused Propagation then Transformation). D2PT employs a dual-channel architecture, combining information from the input graph with a learned global graph (to connect stray nodes), and uses prototype contrastive alignment to ensure mutual benefit between channels. This aligns with the principles of SSL by creating auxiliary tasks to learn from limited explicit supervision. Similarly, \cite{wei20246l2} proposed a self-supervised GNN model specifically for heterogeneous information networks, aiming to flexibly combine different types of additional information and mine deep features, thereby improving adaptability to graph diversity and complexity, especially when initial structure and attribute information might be insufficient or redundant. Furthermore, to enhance GNN resilience against "structure shift" (where test graph topology differs from training), \cite{xia20247w9} proposed the Cluster Information Transfer (CIT) mechanism. CIT learns invariant representations by applying spectral clustering and transferring node representations between cluster statistics in the embedding space, without explicit graph modification, implicitly addressing potential imperfections in the structural alignment between training and test data. In a more specific application of handling missing data, \cite{cini20213l6} introduced GRIN, a novel GNN architecture for multivariate time series imputation. GRIN reconstructs missing temporal data by learning spatio-temporal representations through message passing, demonstrating the utility of GNNs in filling gaps in real-world datasets where relational information is crucial.

In conclusion, significant strides have been made in making GNNs robust to the complexities of real-world graphs. Adaptive filtering mechanisms, from node-wise channel mixing \cite{luan202272y} to sophisticated mixture-of-experts approaches \cite{han2024rkj} and dynamic edge modifications \cite{yang2024vy7}, are increasingly capable of handling structural heterogeneity and mixed homophilic/heterophilic patterns. Concurrently, advancements in structure learning \cite{chen2020bvl,fatemi2021dmb}, data augmentation \cite{zhao2020bmj}, and particularly self-supervised learning paradigms \cite{xie2021n52,liu2023v3e,wei20246l2} are enhancing GNN resilience to missing information, noisy connections, and weak supervision, even in challenging scenarios like extreme weak information \cite{liu2023v3e} or structural shifts \cite{xia20247w9}. Despite these advancements, challenges remain, particularly regarding the computational cost and theoretical guarantees for dynamically learned structures, and ensuring these adaptive and learning mechanisms generalize reliably across highly diverse and unseen graph topologies. Future research will likely focus on developing more efficient and theoretically grounded adaptive mechanisms, potentially integrating meta-learning for rapid adaptation to novel graph patterns, and exploring hybrid approaches that seamlessly combine explicit structure learning with robust adaptive filtering for truly resilient GNNs.