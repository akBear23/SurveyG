The analysis of time series data, particularly in multivariate settings, presents a significant challenge due to the complex spatial-temporal dependencies that often exist between variables. Traditional time series models frequently struggle to explicitly capture these non-Euclidean relationships, necessitating the emergence of Graph Neural Networks (GNNs) as a powerful paradigm for modeling such interconnected systems \cite{jin2023ijy, sahili2023f2x}. This subsection examines the application of GNNs to two distinct yet related problems: first, Spatio-Temporal Graph Neural Networks (STGNNs) for multivariate time series analysis on largely static graph structures, and second, GNNs for truly dynamic graphs where the underlying topology itself evolves over time.

\subsubsection{Spatio-Temporal Graph Neural Networks for Multivariate Time Series Analysis}
In many real-world scenarios, multivariate time series data is inherently structured as a graph, such as sensor networks, road networks, climate monitoring stations, or even brain activity recordings. Here, the spatial relationships between variables are often static or slowly changing, while the features (e.g., traffic speed, temperature, brain signals) evolve dynamically. Spatio-Temporal Graph Neural Networks (STGNNs) are designed to capture both the spatial dependencies (via graph convolutions) and temporal patterns (via recurrent, convolutional, or attention mechanisms) simultaneously \cite{sahili2023f2x, jin2023ijy}. The versatility of STGNNs extends beyond mere forecasting to critical tasks like classification, imputation, and anomaly detection, addressing a broader spectrum of time series challenges \cite{jin2023ijy}.

Early and foundational STGNN architectures often integrated Graph Convolutional Networks (GCNs) with Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs). Models like Diffusion Convolutional Recurrent Neural Network (DCRNN) \cite{li2018dcrnn} and Spatio-Temporal Graph Convolutional Network (STGCN) \cite{yu2018stgcn} exemplify this approach. DCRNN combines graph convolutions based on diffusion processes with a sequence-to-sequence RNN framework, allowing information to propagate across the graph and evolve over time. STGCN, on the other hand, interweaves graph convolutions with 1D convolutional layers for temporal feature extraction, offering a more parallelizable alternative to RNNs for capturing local temporal patterns. These methods demonstrated significant improvements in tasks like traffic flow forecasting by explicitly modeling the road network structure and its dynamic conditions \cite{jin2023ijy}. For instance, the GNN estimator deployed in Google Maps for ETA prediction leverages the road network's graph structure, employing sophisticated featurization and robust training regimes to predict travel times across "supersegments," achieving substantial reductions in ETA errors \cite{derrowpinion2021mwn}.

Critically, the choice of temporal backbone (RNN, CNN, or attention) presents distinct trade-offs. RNN-based models excel at capturing long-range temporal dependencies but suffer from sequential processing, limiting parallelization and increasing computational cost for long sequences. CNN-based models offer better parallelizability and capture local temporal patterns efficiently but may struggle with very long-range dependencies without deep stacking or dilated convolutions. Attention mechanisms, particularly those inspired by Transformers, have gained prominence for their ability to capture global, long-range temporal dependencies and dynamic spatial relationships more flexibly \cite{jin2023ijy}. Attention-based STGNNs can adaptively weigh the importance of different temporal steps or spatial neighbors, providing highly adaptive spatio-temporal modeling. For example, \cite{wu2020hi3} proposed a framework that automatically extracts uni-directed relations among variables through a graph learning module and employs a mix-hop propagation layer alongside a dilated inception layer to effectively capture both spatial and temporal dependencies.

Beyond forecasting, STGNNs are increasingly applied to other vital time series tasks:
\begin{itemize}
    \item \textbf{Imputation:} Addressing missing values in multivariate time series is crucial. GNNs can reconstruct missing data by exploiting both temporal patterns and spatial correlations within the graph. For instance, GRIN \cite{cini20213l6} is a novel GNN architecture specifically designed for multivariate time series imputation, learning spatio-temporal representations through message passing to reconstruct missing data. More recently, Casper \cite{jing2024az0} introduced a Causality-Aware STGNN that leverages causal inference to block confounders and discover sparse causal relationships, enhancing imputation accuracy by avoiding non-causal correlations.
    \item \textbf{Anomaly Detection:} GNNs are powerful for identifying anomalous patterns in time series, which can manifest as unusual node features or structural deviations. This is particularly relevant in domains like Industrial Internet of Things (IIoT) for smart transportation, energy, and factories \cite{wu20210h4}. GNN-based methods leverage the expressive power of message passing to learn normal patterns and detect deviations \cite{kim2022yql}. For example, the Beta Wavelet Graph Neural Network (BWGNN) \cite{tang2022g66} analyzes anomalies through the graph spectrum, observing a "right-shift" phenomenon in spectral energy distribution, and uses spectral and spatial localized band-pass filters to better handle this.
    \item \textbf{Classification:} While less explicitly covered in the provided papers, STGNNs are also employed for time series classification, such as classifying different types of human activities from sensor data or medical conditions from physiological signals, by learning discriminative spatio-temporal features \cite{jin2023ijy}.
\end{itemize}
A crucial development across these applications is the ability of STGNNs to infer and adapt graph structures from sequential data itself, moving beyond reliance on pre-defined static graphs \cite{wu2020hi3, jin2023ijy}. This addresses scenarios where explicit graph connectivity is unknown or noisy.

\subsubsection{GNNs for Truly Dynamic Graphs}
While STGNNs often assume a static or slowly changing underlying graph structure, truly dynamic graphs involve scenarios where the graph topology (nodes, edges, or their attributes) evolves rapidly and significantly over time. This includes interaction networks, social graphs, or evolving knowledge graphs. This area requires specialized architectures and formalizations to capture the temporal dynamics of the graph structure itself \cite{longa202399q}. A comprehensive survey by \cite{longa202399q} formalizes various learning settings and tasks for Temporal GNNs (TGNNs), categorizing them based on temporal representation (snapshot-based vs. event-based) and processing mechanisms.

One critical aspect in dynamic graphs is the learning and adaptation of graph structures. When initial graph structures are noisy, incomplete, or entirely absent, GNNs must infer them from data. This challenge is particularly acute in dynamic settings where relationships are constantly changing. Iterative Deep Graph Learning (IDGL) \cite{chen2020bvl} is a framework that jointly and iteratively learns optimal graph structures and GNN parameters, making models more robust to imperfect topologies. Complementing this, SLAPS \cite{fatemi2021dmb} leverages self-supervision to improve structure learning for GNNs, addressing the "supervision starvation" problem inherent in learning graph topologies from limited labels (see also Section 5.3 for a broader discussion on handling imperfect data). For sequential recommendation, where user-item interactions form evolving graphs, \cite{chang2021yyt} transformed loose item sequences into dynamic item-item interest graphs, utilizing metric learning and attention for robust interest fusion (a topic further explored in Section 7.1). These methods underscore the shift from relying on fixed graph structures to adaptively learning them from evolving data.

Capturing complex temporal dynamics and memory effects beyond instantaneous changes is another significant challenge. Traditional continuous GNNs, relying on integer-order differential equations, often model Markovian updates, which might not suffice for systems with long-range dependencies or historical influence. To overcome this, FROND \cite{kang2024fsk} introduced a framework that generalizes continuous GNNs by incorporating Caputo fractional derivatives. This allows FROND to inherently capture non-local and memory-dependent dynamics, mitigating over-smoothing algebraically and offering a more nuanced understanding of real-world graph evolution. While fractional calculus has also been explored to mitigate oversmoothing in deep GNNs (see Section 5.1), its application here is distinct, focusing on modeling long-term memory in temporal dynamics. Similarly, \cite{bianchi20194ea} proposed GNNs with Convolutional ARMA Filters, which offer a more flexible frequency response than polynomial filters, enabling the capture of longer-range dynamics with fewer parameters.

The formalization of dynamic graphs also distinguishes between discrete-time dynamic graphs (DTDGs), represented as a sequence of static snapshots, and continuous-time dynamic graphs (CTDGs), where changes occur as a stream of timestamped events \cite{longa202399q}. CTDGs offer richer temporal information but pose greater computational challenges. Frameworks like TGLite \cite{wang2024ged} are emerging to provide lightweight programming abstractions and optimizations specifically for CTDG-based Temporal GNNs, facilitating the exploration of new designs and accelerating training and inference for these complex models. Furthermore, ensuring robust predictions in evolving systems requires GNNs to generalize well under "structure shift," where the underlying graph distribution changes. \cite{xia20247w9} addressed this by proposing the Cluster Information Transfer (CIT) mechanism, which learns invariant representations by manipulating node embeddings in the latent space, thereby enhancing GNN generalization to unseen graph structures.

In conclusion, the application of GNNs to time series and dynamic graphs has evolved significantly. STGNNs have become a cornerstone for multivariate time series forecasting, classification, imputation, and anomaly detection on structured data, leveraging various temporal modeling techniques combined with graph convolutions. Meanwhile, the field of truly dynamic graphs focuses on the more complex problem of evolving graph topologies, with advancements in dynamic graph structure learning, modeling memory-dependent dynamics through novel mathematical formalisms, and enhancing generalization under structural shifts. Despite these strides, challenges remain in developing unified frameworks that seamlessly handle both feature and topology evolution, scaling adaptive mechanisms to extremely large and rapidly changing graphs, and ensuring the interpretability of complex dynamic graph learning processes, especially in real-time, high-stakes applications.