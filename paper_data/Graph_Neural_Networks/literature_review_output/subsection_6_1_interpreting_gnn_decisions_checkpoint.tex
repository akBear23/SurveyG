\subsection{Interpreting GNN Decisions}

The inherent complexity of Graph Neural Networks (GNNs), arising from their non-linear message-passing mechanisms and intricate interactions within graph structures, often renders them "black-box" models. This opacity hinders trust, limits adoption in critical domains, and impedes debugging, necessitating robust methods for interpreting their decisions. The field of GNN interpretability aims to unveil *why* a GNN makes a particular prediction, encompassing both instance-level explanations for specific predictions and model-level insights into general patterns learned by the network.

Initial efforts in GNN interpretability were systematically organized by \cite{yuan2020fnk}, which provided a foundational taxonomic survey, categorizing methods and highlighting the unique challenges posed by graph data. This survey underscored the critical need for both instance-level explanations, which pinpoint influential subgraphs or features for individual predictions, and model-level explanations, which uncover general patterns learned by the GNN.

Pioneering instance-level explanations, \cite{ying2019rza} introduced GNNExplainer, a general and model-agnostic framework. GNNExplainer identifies a compact subgraph structure and a small subset of node features that are most influential for a given prediction by maximizing the mutual information between the GNN's output and potential explanations. While effective for individual instances, understanding the overall model behavior required examining numerous examples, which is often impractical. Addressing this, \cite{yuan20208v3} proposed XGNN, the first method for model-level explanations. XGNN trains a graph generator, guided by reinforcement learning, to produce graph patterns that maximally activate a specific prediction score of the target GNN, thereby revealing general patterns associated with a particular class.

However, many early explanation methods, including GNNExplainer, often relied on additive feature attribution, implicitly assuming feature independence. \cite{vu2020zkj} challenged this by introducing PGM-Explainer, which leverages Probabilistic Graphical Models (specifically Bayesian Networks) to explicitly model the *dependencies* and *conditional probabilities* among features contributing to a GNN's prediction. This provides a richer, more nuanced understanding of feature interactions beyond simple importance scores. Further, the pursuit of truly faithful explanations led to a focus on causal patterns. \cite{wu2022vcx} introduced Discovering Invariant Rationale (DIR), an invariant learning strategy that identifies *causal rationales* for GNNs by generating interventional distributions. This approach ensures explanations are based on stable, causal patterns rather than spurious correlations, leading to improved out-of-distribution generalization and more trustworthy interpretations.

Expanding on model-level interpretability, \cite{wang2024j6z} moved towards unveiling "global interactive patterns" across entire graphs, particularly for graph-level tasks. Their method employs graph coarsening and learnable graph prototypes to identify inter-cluster interactions, providing insights into long-range dependencies and global structural influences on predictions.

More recently, the field has moved towards establishing rigorous theoretical foundations to ensure the faithfulness and structure-awareness of explanations, addressing limitations in prior approaches. \cite{bui2024zy9} critically observed that traditional Shapley-based methods for GNNs often fail to account for graph structure and high-order interactions, leading to biased attributions. They introduced the **Myerson-Taylor interaction index**, an axiomatically grounded measure that inherently incorporates graph connectivity and captures high-order interactions. Their proposed MAGE explainer leverages this index to identify both positively and *negatively* influential motifs, offering a comprehensive and theoretically sound approach to instance-level explanations. Concurrently, \cite{chen2024woq} provided a profound theoretical critique of existing attention-based interpretable GNNs. They introduced the **Subgraph Multilinear Extension (SubMT)** framework and rigorously proved that many attention-based XGNNs suffer from "approximation failures" for non-linear GNNs, meaning their explanations may not faithfully represent the model's true decision-making process. To overcome this, they proposed **Graph Multilinear ne T (GMT)**, an architecture that uses random subgraph sampling to more accurately approximate SubMT, leading to provably more powerful and generalizable interpretations.

Despite these significant advancements, challenges remain. The computational cost of generating high-fidelity, structure-aware explanations, especially for very large graphs or complex high-order interactions, is still a bottleneck. Furthermore, while theoretical frameworks like SubMT and the Myerson-Taylor index provide rigorous foundations, bridging the gap between mathematical guarantees and intuitive, actionable insights for human users remains an active area of research. Future work will likely focus on developing more scalable and efficient algorithms for these theoretically sound frameworks, exploring multi-modal explanations that combine structural and feature-based insights, and developing standardized human-centric evaluation protocols to ensure explanations truly build user trust and provide actionable understanding.