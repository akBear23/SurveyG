\subsection{Standardized Evaluation Protocols and Metrics}

The burgeoning field of Graph Neural Networks (GNNs) critically relies on rigorous and standardized evaluation protocols to ensure reliable comparisons, foster reproducible research, and accurately assess model generalization capabilities. The unique characteristics of graph-structured data, such as varying topologies, non-i.i.d. (independent and identically distributed) node features, and the inherent complexity of defining ground truth for structural patterns, pose significant challenges to consistent evaluation, often leading to inconsistent experimental setups and potentially misleading performance claims \cite{dwivedi20239ab}.

Early research underscored the necessity for statistical rigor in GNN evaluation. \cite{klicpera20186xu} notably emphasized a robust experimental protocol for semi-supervised node classification. Their methodology involved conducting 100 runs across multiple random data splits and initializations, employing a fixed visible/test set split, optimizing hyperparameters on a dedicated validation set, utilizing early stopping, and crucially, performing statistical significance testing (e.g., bootstrapping for confidence intervals and paired t-tests for p-values). This meticulous approach revealed that many reported performance gains could "vanish" under careful scrutiny, highlighting the importance of robust statistical validation over single-run accuracy reports. Similarly, \cite{xu2018c8q}, while primarily investigating GNN expressive power, implicitly demonstrated the need for consistent empirical validation on diverse graph datasets to substantiate theoretical claims, advocating for a principled approach to experimental design.

A cornerstone of standardized evaluation lies in the experimental protocols for data splitting and validation. For graph data, a primary distinction exists between **transductive** and **inductive** settings. In a transductive setup, typically used for semi-supervised node classification, the entire graph structure is known during training, but only a subset of node labels is available. Evaluation then occurs on the unlabeled nodes within this *known* graph. Conversely, inductive evaluation, common in graph classification or link prediction involving unseen nodes/graphs, assesses a model's ability to generalize to *entirely new* graph structures or nodes not observed during training. The choice of splitting strategy is paramount; for instance, a simple random split of nodes might be appropriate for transductive node classification, but for graph classification, a split at the graph level is required. Challenges arise in ensuring that splits do not inadvertently leak information or create unrealistic evaluation scenarios. For example, in link prediction, ensuring that test edges are truly unseen and that negative samples are challenging and realistic is critical \cite{li2023o4c}.

Cross-validation, a standard practice in machine learning, also presents unique challenges for graph data. Traditional k-fold cross-validation can be problematic if graph connectivity is not carefully considered, potentially leading to information leakage between folds or breaking structural integrity. Therefore, repeated random splits, often with a fixed ratio for training, validation, and testing, are commonly employed, especially in transductive node-level tasks, to mitigate variance and provide more stable performance estimates. For graph-level tasks, k-fold cross-validation can be applied to the set of graphs, ensuring each graph appears in the test set.

Beyond protocols, a comprehensive understanding of **evaluation metrics** is essential for interpreting GNN performance across different tasks:

*   **Node Classification:** Common metrics include **Accuracy**, **Precision**, **Recall**, and **F1-score**. For datasets with significant class imbalance, raw accuracy can be misleading. In such cases, **Macro-F1** (averaging F1-scores per class) or **Balanced Accuracy** (average of recall for each class) are preferred over **Micro-F1** (calculating F1 globally) or overall accuracy, as they provide a more equitable assessment of performance across all classes \cite{varbella20242iz, li20245zy}. **Area Under the Receiver Operating Characteristic curve (AUC-ROC)** and **Area Under the Precision-Recall Curve (AUC-PRC)** are also widely used, particularly for binary classification or highly imbalanced datasets, as they are less sensitive to class distribution.
*   **Link Prediction:** The primary goal is to predict the existence of edges between nodes. Metrics like **AUC-ROC**, **AUC-PRC**, and **Average Precision (AP)** are standard. A critical aspect here is the strategy for **negative sampling**. \cite{li2023o4c} highlighted that unrealistic or overly simple negative sampling can inflate reported performance. They proposed a standardized methodology and a novel Heuristic Related Sampling Technique (HeaRT) to generate more challenging and realistic negative samples, ensuring more reliable evaluations for link prediction models.
*   **Graph Classification/Regression:** For classification tasks, metrics similar to node classification (Accuracy, F1-score) are used, but applied at the graph level. For graph-level regression tasks, such as predicting molecular properties or power flow, **Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, and **R2 score** are standard measures of prediction error \cite{varbella20242iz}.

The drive for standardized evaluation extends to broader trustworthiness aspects of GNNs, necessitating specialized metrics and protocols:

*   **Explainability:** The field of GNN explainability has historically suffered from a "lack of standardization," including inconsistent evaluation metrics and testbeds \cite{yuan2020fnk}. Efforts are underway to define metrics that assess the *faithfulness* (how accurately an explanation reflects the model's internal reasoning) and *plausibility* (how understandable and intuitive an explanation is to a human).
*   **Robustness:** Evaluating GNN robustness against adversarial attacks requires a systematic methodology. \cite{mujkanovic20238fi} critically revealed that many GNN defenses are evaluated against weak, non-adaptive attacks, leading to overly optimistic robustness estimates. They proposed a systematic methodology for designing strong *adaptive attacks* (e.g., Meta-PGD) to truly test defenses, emphasizing that the choice of attack methodology is itself a crucial part of standardized evaluation for robustness. Metrics include attack success rate, perturbation budget, and the degradation of predictive performance under attack.
*   **Fairness:** Addressing bias in GNNs necessitates specific fairness metrics. \cite{zhang20222g3} noted that "Accountability" in trustworthy GNNs requires "different evaluation standards and graph-based metrics." Metrics like **Statistical Parity Difference ($\Delta SP$)** and **Equal Opportunity Difference ($\Delta EO$)** are used to quantify disparities in outcomes across sensitive attributes (e.g., gender, race) in graph data, particularly in the presence of "distribution disparities" and "attribute imbalance" \cite{li20245zy}.

Despite significant progress in establishing rigorous protocols and defining relevant metrics, challenges persist. The continuous evolution of GNN architectures and the increasing complexity of real-world graph data (e.g., dynamic graphs, extremely large-scale graphs) demand adaptable and extensible evaluation methodologies. Furthermore, the development of universally accepted metrics for complex tasks like graph generation, interpretability, and causality remains an active area of research, requiring a careful balance between theoretical soundness and practical applicability. The field continues to move towards more holistic and adaptive evaluation methodologies that encompass not just predictive performance but also crucial aspects of trustworthiness, fairness, and robustness, ensuring GNNs are not only powerful but also reliable and responsible for real-world deployment.