\subsection{Overcoming the Weisfeiler-Leman Barrier}

Standard message-passing Graph Neural Networks (GNNs) have demonstrated remarkable empirical success across various domains, yet their theoretical discriminative power is fundamentally limited. A cornerstone of GNN theory establishes that many popular GNN architectures, such as Graph Convolutional Networks (GCNs) and GraphSAGE, are at most as powerful as the 1-Weisfeiler-Leman (1-WL) graph isomorphism test \cite{xu2018c8q, morris20185sd}. This equivalence implies that these GNNs cannot distinguish between certain non-isomorphic graphs or capture fine-grained structural properties like triangle counts, girth, or diameter \cite{morris20185sd, garg2020z6o, jegelka20222lq}. This limitation restricts their ability to perform tasks requiring a deeper understanding of graph topology, such as substructure counting \cite{chen2020e6g}. Consequently, a significant research thrust has focused on developing novel GNN architectures and techniques to surpass this 1-WL barrier, enhancing their expressive power for more complex graph learning tasks.

One direct approach to overcome the 1-WL limitation involves extending GNNs to operate on higher-order graph structures, mirroring the k-dimensional Weisfeiler-Leman (k-WL) tests. \cite{morris20185sd} pioneered this direction by introducing **k-GNNs**, which perform message passing on k-element subsets (k-tuples) of nodes rather than individual nodes. These k-GNNs are theoretically proven to be strictly more powerful than 1-GNNs, enabling them to distinguish graphs that confound the 1-WL test and capture higher-order motifs. While powerful, k-GNNs typically incur significantly higher computational and memory costs, growing exponentially with $k$, which limits their practical applicability for larger graphs. An alternative strategy to implicitly capture higher-order information efficiently is seen in GNNML3, which designs graph convolution supports in the spectral domain using non-linear functions of eigenvalues, allowing it to experimentally match the expressive power of 3-WL equivalent models while maintaining linear computational complexity after initial preprocessing \cite{balcilar20215ga}.

Another category of methods enhances GNN discriminative power by explicitly leveraging path or substructure information. Early work, such as the SEAL framework for link prediction, demonstrated the utility of learning high-order features from local enclosing subgraphs, supported by a `$\beta$-decaying heuristic theory` that justifies approximating complex heuristics from local contexts \cite{zhang2018kdl}. Building on the need for substructure awareness, \cite{chen2020e6g} introduced Local Relational Pooling (LRP) to enable GNNs to effectively count substructures, a task proven to be beyond the capabilities of standard Message Passing Neural Networks (MPNNs) for many connected patterns. More recently, **Path Neural Networks (PathNNs)** were proposed, which update node representations by aggregating information from various paths, including single shortest, all shortest, and all simple paths \cite{michel2023hc4}. By operating on "annotated sets of paths," PathNNs are shown to be strictly more powerful than 1-WL and can even distinguish graphs that are indistinguishable by the 3-WL algorithm. Similarly, **Substructure Aware Graph Neural Networks (SAGNNs)** inject subgraph-level structural information, derived from novel "Cut subgraphs" and random walk return probability encodings, into node features, proving to be strictly more powerful than 1-WL and achieving state-of-the-art results on tasks requiring fine-grained structural understanding \cite{zeng20237gv}. For efficiency, ELPH and BUDDY introduce "subgraph sketching," using compact, node-wise representations of neighborhoods to approximate structural features like distance-based node labels, thereby achieving the expressivity of subgraph GNNs (e.g., for triangle counting and distinguishing automorphic nodes) within a more efficient full-graph GNN framework \cite{chamberlain2022fym}.

A third set of techniques injects unique identifiers or richer structural context to boost GNN discriminative power. A surprising finding by \cite{abboud2020x5e} demonstrated that standard MPNNs, when augmented with **Random Node Initialization (RNI)**, become universal approximators for functions on graphs of a fixed order, effectively breaking the 1-WL barrier without the prohibitive cost of higher-order GNNs. This randomization individualizes nodes, allowing the GNN to distinguish otherwise isomorphic local structures. To provide more stable and learnable positional information, **Learnable Structural and Positional Encodings (LSPE)** decouple and simultaneously learn both structural and positional representations throughout the GNN layers \cite{dwivedi2021af0}. LSPE, often initialized with Random Walk Positional Encoding (RWPE), overcomes the sign ambiguity issues of Laplacian eigenvectors and significantly enhances expressivity. Complementing this, **Positional Encoding GNN (PEG)** uses separate channels for original node features and positional features (e.g., Laplacian Eigenmaps), ensuring O(p) equivariance for positional features and provable stability against graph perturbations, which is crucial for reliable discrimination in complex graphs \cite{wang2022p2r}. Beyond abstract graph structures, **E(n) Equivariant Graph Neural Networks (EGNNs)** directly incorporate geometric structural context by updating node coordinates within the message-passing framework, preserving E(n) equivariance efficiently without relying on computationally expensive higher-order representations \cite{satorras2021pzl}. This is critical for tasks in physics and chemistry, as exemplified by NequIP, which uses E(3)-equivariant convolutions for interatomic potentials, achieving high accuracy with remarkable data efficiency \cite{batzner2021t07}. More dynamically, **Cooperative Graph Neural Networks (CO-GNNs)** allow nodes to adaptively choose their communication actions (listen, broadcast, isolate), leading to dynamic and asynchronous message passing that is theoretically shown to be more expressive than 1-WL due to the variance introduced by action sampling \cite{finkelshtein202301z}. Finally, **Spatio-Spectral Graph Neural Networks (S2GNNs)** combine local spatial message passing with global spectral filtering, inherently mitigating over-squashing and providing "free" positional encodings from partial eigendecomposition, making them strictly more expressive than 1-WL and highly effective for long-range interactions \cite{geisler2024wli}.

In conclusion, the journey to overcome the Weisfeiler-Leman barrier has led to a diverse array of innovative GNN architectures. While k-GNNs offer a direct theoretical extension, their computational cost remains a challenge. More efficient solutions often involve injecting rich structural context through explicit path or substructure information, or by providing unique node identifiers and learnable positional encodings. Recent advancements explore dynamic message-passing paradigms and hybrid spatial-spectral approaches, which offer novel ways to enhance discriminative power and capture long-range dependencies. However, the trade-off between increased expressivity, computational efficiency, and generalizability to unseen graph structures remains an active area of research, pushing the boundaries of GNN design for fine-grained structural understanding.