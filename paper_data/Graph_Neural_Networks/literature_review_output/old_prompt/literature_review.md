# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-05T01:35:18.857528
**Papers analyzed:** 328

## Papers Included:
1. 9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf [paper1]
2. b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf [paper2]
3. 900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf [paper3]
4. 4fa31616b834c377c4995c346a2b17464f25692a.pdf [paper4]
5. f442378ead6282024cf5b9046daa10422fe9fc5f.pdf [paper5]
6. 20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf [paper6]
7. 741a7faf9dbefd418cda878c61c5b839ecc02977.pdf [paper7]
8. 123139463809b5acf98b95d4c8e958be334a32b5.pdf [paper8]
9. fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf [paper9]
10. d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf [paper10]
11. cd551790992d16148fe2e5ff2cc76861195e2191.pdf [paper11]
12. 458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf [paper12]
13. 27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf [paper13]
14. 5e6db511e736f77f844bbeebaa2b177427abada1.pdf [paper14]
15. e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf [paper15]
16. 5822490cf59df7f7ccb92b8901f244850b867a66.pdf [paper16]
17. ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf [paper17]
18. b4895de425a02af87713bd78ed1a29fe425753af.pdf [paper18]
19. 75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf [paper19]
20. 8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf [paper20]
21. b20589941cd52d199ba381b92e092ba7fb36d689.pdf [paper21]
22. 775a6e0f9104b282ed867871d743e3afd1e66d96.pdf [paper22]
23. facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf [paper23]
24. 94497472eecb7530a2b75c564548c540ebd61e9b.pdf [paper24]
25. 24b2aed0f130e5278325b5055711de44d247460e.pdf [paper25]
26. bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf [paper26]
27. 3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf [paper27]
28. a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf [paper28]
29. 3db15a5534050ab2cfc1d09dd772d032395515e1.pdf [paper29]
30. d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf [paper30]
31. 00358a3f17821476d93461192b9229fe7d92bb3f.pdf [paper31]
32. 639206a9a32d91386924f1c94e9760dfb43df72e.pdf [paper32]
33. b88f456daaf29860d2b59c621be3bd878a581a59.pdf [paper33]
34. 0d4184cff17f093e0487b27180be515c385feff6.pdf [paper34]
35. fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf [paper35]
36. 3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf [paper36]
37. 44b9f16ba417b90e2e7c42f9074378dd06415809.pdf [paper37]
38. 35792d528bd07aed95df46f0ecb87019cb123147.pdf [paper38]
39. a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf [paper39]
40. bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf [paper40]
41. 03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf [paper41]
42. 1fad14bcfc2b75797c686a5a05779076437a683e.pdf [paper42]
43. f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf [paper43]
44. d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf [paper44]
45. aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf [paper45]
46. c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf [paper46]
47. 140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf [paper47]
48. aafe1338caef4682069e92378f1190785ec24c2c.pdf [paper48]
49. 789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf [paper49]
50. 68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf [paper50]
51. 81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf [paper51]
52. cf30fb61a5943781144c8442563e3ef9c38df871.pdf [paper52]
53. 641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf [paper53]
54. 7de413da6e0a00e14270cfaed2a31666e7c28747.pdf [paper54]
55. 3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf [paper55]
56. 4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf [paper56]
57. 510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf [paper57]
58. ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf [paper58]
59. 90dead8a056b848be164c2e5cdadfa2e191c3265.pdf [paper59]
60. 536da0e76290aea9cbe75c29bac096aeb45ef875.pdf [paper60]
61. 21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf [paper61]
62. f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf [paper62]
63. 993377a3fc8334558463b82053904e3d684f29c0.pdf [paper63]
64. bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf [paper64]
65. 6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf [paper65]
66. cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf [paper66]
67. 6dc0932670a0b5140a426ca310bbb03783ff2240.pdf [paper67]
68. fcdd4300f937cef11af297329ed4bd2b611871e7.pdf [paper68]
69. 9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf [paper69]
70. e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf [paper70]
71. f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf [paper71]
72. 8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf [paper72]
73. 14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf [paper73]
74. e4b1d7553020258d7e537e2cfa53865359389eac.pdf [paper74]
75. 85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf [paper75]
76. 854342cf063eef4428a5441c8d317dfbabb8117f.pdf [paper76]
77. e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf [paper77]
78. d09608593caa20b79a8aaddfe19df7e31513d711.pdf [paper78]
79. 398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf [paper79]
80. 0a69c8815536a657668e089e3281ff2e963d947a.pdf [paper80]
81. c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf [paper81]
82. ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf [paper82]
83. 73366d75289c5e37481639fb54fdee28a664e2b3.pdf [paper83]
84. 3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf [paper84]
85. 4b776e7f26464e5b230c1679560f12618730dcc6.pdf [paper85]
86. 218223e91f55a1e0186f5b008b55f5e0fe350698.pdf [paper86]
87. 341880efaef452f631a4a5cd61bef5dae47741d7.pdf [paper87]
88. c9845a625e2dac5e32db172d353f81d377760a5f.pdf [paper88]
89. 3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf [paper89]
90. 018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf [paper90]
91. a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf [paper91]
92. 071e053890765ecc2ff8ef9054e9c75ec135e167.pdf [paper92]
93. db5d583782264529456a475ce8e9a90823b3a2b5.pdf [paper93]
94. 3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf [paper94]
95. edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf [paper95]
96. f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf [paper96]
97. 011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf [paper97]
98. 0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf [paper98]
99. 5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf [paper99]
100. 454304628bf10f02aba1c2cfc95891e94d09208e.pdf [paper100]
101. caf8927cf3c872698a0e97591a1205ba577bbba5.pdf [paper101]
102. 8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf [paper102]
103. 707142f242ee4e40489062870ca53810cb33d404.pdf [paper103]
104. 6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf [paper104]
105. 6ae2967bb0a5e57cc545176120a4845576e068a3.pdf [paper105]
106. 46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf [paper106]
107. 11b9f4729c8e355dec7122993076f6e2788c03c4.pdf [paper107]
108. 02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf [paper108]
109. 530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf [paper109]
110. 252351936bd6fabf4b6cd2962fa0ee613772278d.pdf [paper110]
111. 6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf [paper111]
112. 04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf [paper112]
113. 4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf [paper113]
114. ac225094aab9e7b629bc5b3343e026dea0200c70.pdf [paper114]
115. 94194703e83b5447f519fd8bcbb903916e05aaf9.pdf [paper115]
116. 0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf [paper116]
117. faa6fce9a16925eb3091271281f923bc95291ebb.pdf [paper117]
118. 2a85846fd827a157b624ee012e75cbe37344281c.pdf [paper118]
119. 5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf [paper119]
120. 3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf [paper120]
121. 81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf [paper121]
122. 62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf [paper122]
123. ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf [paper123]
124. 7456dea3a3646f2df6392773a196a5abd0d53b11.pdf [paper124]
125. 347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf [paper125]
126. 75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf [paper126]
127. 0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf [paper127]
128. 21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf [paper128]
129. 381411d740562de1e766dc8cc833844eb99dde01.pdf [paper129]
130. 24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf [paper130]
131. 572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf [paper131]
132. 00549af4bc3270e0f688acbf694f912d7ee39cad.pdf [paper132]
133. 4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf [paper133]
134. 68a024d7b70ef3989a6751678f635cbe754440fc.pdf [paper134]
135. 2b8a207189bc02d73d1dce850bcde24dbd984483.pdf [paper135]
136. b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf [paper136]
137. 594dc362b4332ae661e3d71da17d097bb4a357dd.pdf [paper137]
138. 0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf [paper138]
139. 2b1eae2cceb377cb9267b2c96294228d5e583136.pdf [paper139]
140. cda969fd7362bdf21aa1f3398078982dcb350d76.pdf [paper140]
141. 1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf [paper141]
142. 53e80869c6582d7f95ef0a351170736afd1742d0.pdf [paper142]
143. 6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf [paper143]
144. 80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf [paper144]
145. 1478c3c0225368419f68aabc6b67033531d3b4c1.pdf [paper145]
146. b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf [paper146]
147. 030046515a20a4b4f86c290361881923694e458a.pdf [paper147]
148. e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf [paper148]
149. 16351ff232f2e475c8d8347809ef905d67998fa5.pdf [paper149]
150. 23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf [paper150]
151. f1e5e65941617604923225cc4bf464e370fcae67.pdf [paper151]
152. 2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf [paper152]
153. b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf [paper153]
154. 1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf [paper154]
155. 250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf [paper155]
156. 2028710190373ef893e3055c9113e04274a152d7.pdf [paper156]
157. 18b2c7dd5f37818f74407a69985322f8a109f75f.pdf [paper157]
158. 5ab6888c67d2877f15c2b065da4216538835d141.pdf [paper158]
159. 38e320f860d54e4071d68955c774b3e4a091bfe0.pdf [paper159]
160. 6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf [paper160]
161. 2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf [paper161]
162. da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf [paper162]
163. 91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf [paper163]
164. 4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf [paper164]
165. 08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf [paper165]
166. 80c698688bb4488beaceaab5c64f701a946cb7ae.pdf [paper166]
167. 19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf [paper167]
168. 3da4626411d83c19c9919bb41dba94fff88da90e.pdf [paper168]
169. f470ac3537339514bb9d88fcad9c075441906d45.pdf [paper169]
170. 116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf [paper170]
171. 6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf [paper171]
172. 2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf [paper172]
173. 94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf [paper173]
174. 81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf [paper174]
175. d08167fd8583b0f70ba8a26821c29ea8af420826.pdf [paper175]
176. e243c89ac61aae7330792c6c3f8791f07f40d031.pdf [paper176]
177. 3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf [paper177]
178. fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf [paper178]
179. 84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf [paper179]
180. 94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf [paper180]
181. c193011099906126fe7b6cfcb04062cf4591ccf9.pdf [paper181]
182. 286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf [paper182]
183. c00673042d8cc539d903c4f30b55a71487f5c701.pdf [paper183]
184. aac77c36a9a5c24aa135538c32950096e59ba442.pdf [paper184]
185. 86ad9d1dd6626921297a8456b048f4bccafe967c.pdf [paper185]
186. d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf [paper186]
187. fc580c211689663a64f42e2ba92c864cb134ba9b.pdf [paper187]
188. ef41b29312860bc284640e35ab499053f4966bbf.pdf [paper188]
189. 064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf [paper189]
190. 99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf [paper190]
191. dc5e841197165c3c38940cc9f607fce7b09116cf.pdf [paper191]
192. 0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf [paper192]
193. 5f1913828e30c3070f32c154d2d142ec17e91189.pdf [paper193]
194. 4bc7d63595d194a6e0930019764216e6b42da0d4.pdf [paper194]
195. bda290f54791e4719917e44a7e6441d000c43ab3.pdf [paper195]
196. 5b1978e8284c8514165938bff6e3276977088f94.pdf [paper196]
197. 8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf [paper197]
198. b0d62f38592dbae23628d9700490cb11ac873182.pdf [paper198]
199. 54ff6c9ad037792e938e05985720d313512539b7.pdf [paper199]
200. 569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf [paper200]
201. e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf [paper201]
202. 2efc9d5bb114f7114b041d621e002b1562366903.pdf [paper202]
203. 84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf [paper203]
204. 91b9fa72da566afc77a07ec856c3d8da23714367.pdf [paper204]
205. 066dd731b5aaeede92d129344776783590c338d2.pdf [paper205]
206. 60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf [paper206]
207. ac44bbb4c62033d558aad57712438e5571069d9c.pdf [paper207]
208. cc827043e6c5be8337df4edb155096f9d0006020.pdf [paper208]
209. 741bf9081fe341c173f36739a50606bf2a159610.pdf [paper209]
210. f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf [paper210]
211. e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf [paper211]
212. e46aa831aac38520249dff35916937f0d094f32e.pdf [paper212]
213. a52ae33c11309a98887405db21e930a1f298d865.pdf [paper213]
214. a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf [paper214]
215. 5fb4947831352af6d6231a830a943f0f2069ee8b.pdf [paper215]
216. cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf [paper216]
217. 7544db2ae3140081b1581a99eee88960cc31415a.pdf [paper217]
218. cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf [paper218]
219. 0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf [paper219]
220. c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf [paper220]
221. b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf [paper221]
222. 9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf [paper222]
223. cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf [paper223]
224. 6b72135bf31e78ccee78478228b635201326d217.pdf [paper224]
225. 77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf [paper225]
226. cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf [paper226]
227. 20309e3990cd612a13e389e1572786e55100f03d.pdf [paper227]
228. bd4b8cad70faa48605163eaede13d62fb671f5de.pdf [paper228]
229. e925e38c5bade594237439c1d4a77e1376535697.pdf [paper229]
230. 8b9f01585a679dffe92261ecdec56425db9ef97f.pdf [paper230]
231. ab27a370d87617255455a05cb2d98c268b5fa06b.pdf [paper231]
232. 5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf [paper232]
233. 5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf [paper233]
234. 985a47671c30e2d059c568ba8eb8e2813bab9423.pdf [paper234]
235. 9208290fd7948ed14ebe55718118c401e8396159.pdf [paper235]
236. 3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf [paper236]
237. ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf [paper237]
238. f42898181e56cb6fee860143c96663ed361449e0.pdf [paper238]
239. df519a15af1e83824340212477d9d356f86f15ec.pdf [paper239]
240. 181ff84051e375be829ec230c1e65439a199171c.pdf [paper240]
241. cfc041534d57719d893ec5af01a7065621f7c410.pdf [paper241]
242. 613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf [paper242]
243. a55c59163cf138d31994afc875d46997d3ef5c4b.pdf [paper243]
244. 3492576dae538ad34a6ecae5b631651e8ddebf92.pdf [paper244]
245. 4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf [paper245]
246. ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf [paper246]
247. 5ca285d36255114938751e1787681fa17073a313.pdf [paper247]
248. adf1318ee484fe32d227a5084ed981eedb828c72.pdf [paper248]
249. 7a42822cb3102041bad5ff7058451e35e48fd15f.pdf [paper249]
250. cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf [paper250]
251. 3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf [paper251]
252. c1fbf79a695352b906d8c980608fccb99d3366ee.pdf [paper252]
253. cd90ab144ca439fad38ad952d254ef2036da6d96.pdf [paper253]
254. 1697df4909875d593e1f82aeba49f2861640017a.pdf [paper254]
255. cb2a45084f0c7bdc38271e94205603d1237945d8.pdf [paper255]
256. 21dce0407d0ee3bec185b0361593d73bb26a532e.pdf [paper256]
257. bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf [paper257]
258. 105191ce014da7d36d93d405c920a261dba3e937.pdf [paper258]
259. 10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf [paper259]
260. 7779b880700e9e3495557e076d60594d18d69277.pdf [paper260]
261. 599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf [paper261]
262. 687abbb274492f95b2c0fe82137c009754456d4c.pdf [paper262]
263. 3911024df853ccf11138d35835572ce863df51bf.pdf [paper263]
264. cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf [paper264]
265. 774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf [paper265]
266. e49178ea82233947837c135ec303852dc776dbde.pdf [paper266]
267. 0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf [paper267]
268. 046f6abdbf63fbb80d831102e7889c6801ad3545.pdf [paper268]
269. fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf [paper269]
270. cfff81fc166668790f4099cebd785cdd20f25b6d.pdf [paper270]
271. 01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf [paper271]
272. 0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf [paper272]
273. df2701c0fabf50b511182a287d112dfcc84c59b3.pdf [paper273]
274. 1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf [paper274]
275. 7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf [paper275]
276. 326430bd401c2ac820fc08a0a198ceacf1cde506.pdf [paper276]
277. 2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf [paper277]
278. c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf [paper278]
279. 478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf [paper279]
280. 817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf [paper280]
281. 0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf [paper281]
282. 93ad698088aa72fcbd5004bd59ff38c25054f319.pdf [paper282]
283. c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf [paper283]
284. 9391738dff06189f64ced951df6c1848311731dc.pdf [paper284]
285. d0cd5ede6535f617e40b58517fe593b648b737b0.pdf [paper285]
286. 819d9ef75975c78c5ce12e54af93737f4b698f55.pdf [paper286]
287. 6826db50e96adb61ecc437809a361b16ea7546a7.pdf [paper287]
288. 9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf [paper288]
289. 8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf [paper289]
290. a6c060ab3b997675075415253e0a6bc81591f32e.pdf [paper290]
291. 73765285b243a53143912b501f7afab98a0c8cb0.pdf [paper291]
292. f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf [paper292]
293. aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf [paper293]
294. dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf [paper294]
295. 9fb72c9292bf80f9825e0038d34cef57468a2757.pdf [paper295]
296. b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf [paper296]
297. 2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf [paper297]
298. 5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf [paper298]
299. 242425415e28da757bb9c7d24dd0a99654d66027.pdf [paper299]
300. da3c1508794ba0d4f070a9bc47b06575422f456f.pdf [paper300]
301. 0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf [paper301]
302. a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf [paper302]
303. c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf [paper303]
304. 67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf [paper304]
305. 12e60b9fd69f18c1c01996d108229051432b6090.pdf [paper305]
306. c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf [paper306]
307. 7bda10706047e154e22259c4b20d70240296963e.pdf [paper307]
308. a3340cd6f24e4c83bec616c7bda719737492fe74.pdf [paper308]
309. abd2ac274abe25f40f5268324d4774e67b467ef8.pdf [paper309]
310. 3d0911fabeebc22506ac3b006a553448debf03a5.pdf [paper310]
311. 1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf [paper311]
312. 413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf [paper312]
313. f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf [paper313]
314. 1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf [paper314]
315. cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf [paper315]
316. ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf [paper316]
317. 99d50bb7b0155203c908228d086eb232c34ee0a6.pdf [paper317]
318. 6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf [paper318]
319. ae683dbd44ec508f63254d864f83d6c1006dd652.pdf [paper319]
320. 6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf [paper320]
321. 675a150a89f5c3dd44bf8312b00a896716c7082b.pdf [paper321]
322. d42ebd3b0673341125e374223e0882e99557cc8c.pdf [paper322]
323. 13391f9fb2094227ecc567fef76fd95adc57e972.pdf [paper323]
324. 406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf [paper324]
325. 0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf [paper325]
326. f60492aece8e86203ed95303cb809332a11d74b5.pdf [paper326]
327. b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf [paper327]
328. ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf [paper328]

## Literature Review

### 1. Introduction

\section{Introduction}
The ubiquitous nature of graph-structured data in diverse scientific and engineering domains, ranging from social networks and biological systems to urban infrastructure and recommender systems, has presented a persistent challenge for traditional machine learning paradigms [wu20193b0, zhou20188n6, velickovic2023p4r, wu2022ptq]. Unlike Euclidean data, graphs inherently encode complex relational information, where the connections between entities are as significant as the entities themselves. Graph Neural Networks (GNNs) have emerged as a transformative paradigm, extending the success of deep learning to this non-Euclidean data landscape. By enabling models to learn representations directly from graph structures, GNNs have unlocked unprecedented capabilities for tasks such as node classification, link prediction, and graph classification, demonstrating their profound impact across numerous fields [khemani2024i8r, wang2023zr0].

The fundamental motivation behind the development of GNNs stems from the critical need to effectively process and leverage relational information. Traditional machine learning models, designed for independent and identically distributed (i.i.d.) data, often struggle to capture the intricate dependencies and structural patterns inherent in graphs. Early attempts to apply machine learning to graphs relied on hand-crafted features or graph kernel methods, which, while foundational, often lacked scalability and the ability to learn complex, hierarchical representations end-to-end [garg2020z6o]. GNNs address this by iteratively aggregating information from a node's local neighborhood, effectively propagating and transforming features across the graph structure. This message-passing mechanism allows GNNs to learn rich, context-aware node and graph embeddings, which has fueled their rapid adoption and continuous evolution. However, this powerful paradigm also introduces unique challenges, such as limitations in expressive power, susceptibility to over-smoothing, and difficulties in handling heterophilous graphs, which necessitate ongoing research and innovation. This comprehensive literature review aims to consolidate the vast and rapidly expanding knowledge base surrounding GNNs, providing a structured understanding of their evolution, diverse methodologies, impactful applications, and future trajectory, while critically identifying existing research gaps and promising directions.

\subsection{The Rise of Graph Neural Networks}
The ascent of Graph Neural Networks marks a pivotal shift in machine learning, offering a principled approach to process data residing in non-Euclidean spaces [wu20193b0, zhou20188n6]. Prior to GNNs, applying deep learning to graph-structured data was largely constrained by the need to flatten or linearize graph information, often leading to a loss of crucial topological context. GNNs overcome this by generalizing convolutional operations to arbitrary graph structures, allowing nodes to iteratively aggregate information from their neighbors, thereby learning representations that capture both node features and structural roles [velickovic2023p4r]. This message-passing paradigm, initially inspired by spectral graph theory and later refined through spatial approaches, provided the foundational mechanism for GNNs to learn from relational data [kipf2016semi, hamilton2017inductive].

However, the early success of GNNs was accompanied by recognized limitations. A significant theoretical barrier was their inherent expressive power, often proven to be no more powerful than the 1-dimensional Weisfeiler-Lehman (1-WL) isomorphism test [xu2018c8q, morris20185sd, garg2020z6o]. This meant that many GNN architectures struggled to distinguish between non-isomorphic graphs that the 1-WL test could not differentiate, limiting their ability to capture complex structural patterns like specific cycle sizes or graph diameters [chen2020e6g, oono2019usb]. This limitation spurred an "arms race" in GNN research, driving the development of more expressive architectures. For instance, Path Neural Networks (PathNNs) [michel2023hc4] explicitly leverage path information, with variants like `˜AP` demonstrating the ability to distinguish graphs that are even 3-WL indistinguishable, thereby significantly surpassing the 1-WL bottleneck. Similarly, non-convolutional GNNs, such as the Random Walk with Unifying Memory (RUM) network [wang2024oi8], have been introduced to jointly address limited expressiveness, over-smoothing, and over-squashing by entirely eschewing convolution operators in favor of random walk trajectories and recurrent neural networks. These advancements highlight a critical evolutionary trend: moving beyond local, fixed-neighborhood aggregation to incorporate more global, complex, and memory-dependent structural information, as also seen in the exploration of fractional calculus in GNNs to capture non-local dependencies and memory effects [kang2024fsk].

\subsection{Importance of Graph-Structured Data}
The intrinsic importance of graph-structured data lies in its ability to naturally represent complex systems where entities and their relationships are paramount. This relational paradigm is pervasive across virtually every scientific and industrial domain, making GNNs indispensable for unlocking insights from these intricate datasets. In social sciences, GNNs model interactions in social networks for tasks like recommendation systems [gao2022f3h, wu2020dc8, fan2019k6u, sharma2022liz], where understanding user-item relationships and social influence is crucial for personalized content delivery [ying20189jc, wang2019vol]. The sheer scale and dynamic nature of these graphs necessitate efficient and scalable GNN designs [chen2024gbe, vasimuddin2021x7c].

In molecular science and drug discovery, molecules are inherently graphs, with atoms as nodes and chemical bonds as edges. GNNs excel at learning molecular properties, predicting drug-target interactions, and accelerating materials design by capturing complex structural and chemical information [jiang2020gaq, reiser2022b08, klicpera20215fk, batzner2021t07, fung20212kw, xia2021s85, li2021v1l, xia2023bpu, wander2024nnn, carlo2024a3g, vinh20243q3, smith2024q8n, fang2024p34, zhang202483k, li2024gue, gnanabaskaran20245dg, yao2024pyk, fang2024zd6]. Similarly, in neuroscience, brain connectomes are modeled as graphs to understand brain disorders and functions [bessadok2021bfy, cui2022mjr, cui2022pap, zhao2022fvg, mohammadi202476q, luo2024h2k, abuhantash202458c, abadal2024w7e]. The ability of GNNs to model non-local dependencies and memory effects, as explored by fractional-order GNNs [kang2024fsk], is particularly relevant for capturing the complex, often non-Markovian, dynamics of biological systems.

Furthermore, GNNs have found critical applications in urban computing for traffic forecasting [li2020fil, jin2023e18, zhou2024t2r], cybersecurity for anomaly detection and threat intelligence [mitra2024x43, bilot20234ui, shen2021sbk, hin2022g19, nguyen2021g12, li2024r82], and even computer vision for tasks involving irregular data like point clouds and scene graphs [chen2022mmu]. The challenges GNNs address are fundamental: processing non-Euclidean data, handling varying neighborhood sizes, and learning complex, often long-range, dependencies. For instance, the issue of heterophily, where connected nodes have dissimilar features, is a common real-world phenomenon that traditional GNNs struggle with due to their homophily assumption [ma2021sim, zhu2020c3j]. Solutions like GloGNN [li2022315] explicitly address this by learning global node correlations, demonstrating that effective GNN design must critically evaluate and adapt to the underlying graph properties. The pervasive nature of graph-structured data, coupled with the unique capabilities of GNNs to model these relationships, underscores their transformative potential across scientific and industrial landscapes.

\subsection{Scope and Structure of the Review}
This comprehensive literature review aims to provide a structured and critical understanding of the Graph Neural Network landscape, consolidating fragmented knowledge, identifying key research gaps, and charting future directions. Our exploration begins by tracing the evolution of GNN architectures, from early spectral and spatial methods to more advanced designs that tackle challenges like limited expressiveness, over-smoothing, and over-squashing [rusch2023xev, cai2020k4b, chen2019s47, oono2019usb, peng2024t2s]. We will critically examine how innovative approaches, such as Path Neural Networks [michel2023hc4] and non-convolutional models like RUM [wang2024oi8], have pushed the boundaries of expressive power, enabling GNNs to capture more intricate structural patterns than their 1-WL-limited predecessors.

A significant portion of this review will delve into the methodological advancements, including techniques for handling graph heterophily [li2022315, ma2021sim, zhu2020c3j, zheng2022qxr], enhancing robustness against adversarial attacks [zhang2020b0m, xu2019l8n, zgner2019bbi, zhang2020jrt, gosch20237yi, geisler2021dcq, mujkanovic20238fi, dai2022xze, dai2023tuj], and improving the explainability of GNN predictions [ying2019rza, yuan20208v3, yuan2020fnk, vu2020zkj, lucic2021p70, zhang2021wgf, agarwal2022xfp, cui2022pap, chen2024woq, bui2024zy9, luo2024euy, wang2024j6z]. For instance, the challenge of explainability, often addressed by identifying important nodes or edges, is critically re-evaluated by methods like SubgraphX [yuan2021pgd], which directly identifies intuitive subgraphs, highlighting a shift towards more human-intelligible explanations. We will also analyze the critical role of evaluation methodologies, as exemplified by studies that expose pitfalls in current link prediction benchmarks and propose more realistic settings with hard negative sampling [li2023o4c]. This critical analysis will extend to identifying why certain limitations persist, often due to theoretical barriers (e.g., the inherent trade-off between local aggregation and global information capture) or practical constraints (e.g., scalability for extremely large graphs). The review will then explore the broad spectrum of applications, from recommender systems [gao2022f3h] and computer vision [chen2022mmu] to materials science and cybersecurity, before concluding with a discussion of emerging trends, open challenges, and promising future research directions.

### 2. Foundational Concepts and Early Architectures

\section{Foundational Concepts and Early Architectures}
The journey of Graph Neural Networks (GNNs) began with the fundamental challenge of extending deep learning paradigms, which primarily operate on Euclidean data structures like images and sequences, to the inherently non-Euclidean domain of graphs. This section delves into the initial theoretical underpinnings and architectural designs that laid the groundwork for modern GNNs, focusing on the core principles of graph representation learning, the ubiquitous message-passing paradigm, and the critical benchmark of theoretical expressiveness provided by the Weisfeiler-Leman (WL) graph isomorphism test. Understanding these foundational concepts is crucial, as the inherent limitations of early models, particularly concerning their discriminative power and propagation mechanisms, directly motivated the subsequent wave of architectural innovations and theoretical advancements. This period established the initial capabilities of GNNs while simultaneously revealing the deep theoretical and practical challenges that continue to drive research in the field, such as over-smoothing, over-squashing, and the struggle with heterophilous graphs.

\subsection{Graph Representation Learning Fundamentals}
Graph representation learning is concerned with mapping nodes, edges, or entire graphs into low-dimensional vector spaces, often referred to as embeddings, such that the structural and feature information of the original graph is preserved [wu20193b0, zhou20188n6]. The primary motivation for this transformation is to enable traditional machine learning algorithms, which typically operate on vector inputs, to process and learn from complex graph-structured data. Unlike grid-like data, graphs possess irregular topologies, varying neighborhood sizes, and inherent relational information, making direct application of standard convolutional or recurrent neural networks challenging.

Early approaches to graph representation learning largely relied on hand-crafted features (e.g., node degrees, clustering coefficients) or graph kernel methods [garg2020z6o]. While foundational, these methods often lacked scalability, generality, and the ability to learn hierarchical representations end-to-end. The advent of deep learning spurred the development of methods that could automatically learn these representations. The goal is to produce embeddings where nodes with similar structural roles or features are close in the embedding space, facilitating downstream tasks such as node classification, link prediction, and graph classification [velickovic2023p4r, wu2022ptq]. The challenge lies in designing architectures that can effectively aggregate information from a node's local neighborhood while also capturing global graph properties, balancing local fidelity with global context. This balance is critical for the expressive power and generalization capabilities of the learned representations.

\subsection{The Message Passing Paradigm and Early Models}
The core mechanism underpinning most Graph Neural Networks is the message-passing paradigm, also known as neighborhood aggregation [gilmer2017neural]. In this iterative process, each node updates its representation by aggregating information (messages) from its direct neighbors and combining it with its own previous state. This process is typically repeated for a fixed number of layers, allowing information to propagate across the graph and enabling nodes to incorporate information from increasingly distant neighbors. Formally, for a node $v$ at layer $k$, its new representation $h_v^{(k+1)}$ is computed as:
$$h_v^{(k+1)} = \text{UPDATE}^{(k)}\left(h_v^{(k)}, \text{AGGREGATE}^{(k)}(\{h_u^{(k)} \mid u \in \mathcal{N}(v)\})\right)$$
where $\mathcal{N}(v)$ denotes the neighbors of node $v$.

Early influential GNN architectures largely adopted this message-passing framework, albeit with different aggregation and update functions. Graph Convolutional Networks (GCNs) [kipf2016semi] emerged as a prominent early model, inspired by spectral graph theory but implemented as a localized spatial convolution. GCNs simplify the aggregation by averaging neighbor features and applying a linear transformation followed by a non-linearity. GraphSAGE [hamilton2017inductive] further popularized the spatial perspective, introducing an inductive framework that learns aggregation functions (e.g., mean, LSTM, pooling) to generate embeddings for unseen nodes. Graph Attention Networks (GATs) [velickovic2017graph] introduced an attention mechanism, allowing nodes to assign different weights to their neighbors based on their features, thereby learning more flexible aggregation patterns.

Despite their initial success, these early message-passing GNNs faced several inherent limitations. A critical issue is **over-smoothing**, where node representations become indistinguishable as the number of GNN layers increases [rusch2023xev, cai2020k4b, chen2019s47, oono2019usb, peng2024t2s]. This occurs because repeated averaging of neighbor features causes node embeddings to converge to a subspace, losing their discriminative power. This problem is exacerbated in dense graphs and limits the effective depth of GNNs. Another challenge is **over-squashing**, an information bottleneck that makes it difficult for messages to propagate effectively over long distances [alon2020fok]. As information from an exponentially growing receptive field needs to be compressed into fixed-size messages at each layer, crucial long-range dependencies can be lost. Furthermore, most early GNNs implicitly assume **homophily**, meaning connected nodes tend to have similar features or labels [ma2021sim, zhu2020c3j]. This assumption causes them to perform poorly on heterophilous graphs, where connected nodes are often dissimilar [zheng2022qxr]. The aggregation of dissimilar features can lead to noisy or misleading representations.

These limitations spurred significant research into alternative propagation mechanisms and architectural designs. For instance, the Random Walk with Unifying Memory (RUM) network [wang2024oi8] proposes a fundamentally *non-convolutional* approach, using random walk trajectories processed by RNNs to generate node representations. This departure from iterative neighborhood aggregation is shown to jointly alleviate limited expressiveness, over-smoothing, and over-squashing by providing a different information flow mechanism that doesn't rely on local averaging. Similarly, the FRactional-Order graph Neural Dynamical network (FROND) framework [kang2024fsk] introduces fractional calculus into continuous GNNs. By replacing integer-order differential equations with Caputo fractional derivatives, FROND captures non-local and memory-dependent dynamics, inherently mitigating over-smoothing through a slow algebraic rate of convergence to stationarity, contrasting with the exponentially swift convergence of Markovian integer-order models. This highlights an evolutionary trend towards more sophisticated propagation models that move beyond instantaneous, local updates. The GloGNN model [li2022315] addresses the homophily assumption by learning global node correlations through a signed coefficient matrix, demonstrating that effective GNN design must critically evaluate and adapt to the underlying graph properties, rather than relying solely on local message passing.

\subsection{Theoretical Expressiveness: The Weisfeiler-Leman Test}
A critical aspect of evaluating GNNs is their theoretical expressive power, particularly their ability to distinguish between non-isomorphic graphs. The Weisfeiler-Leman (WL) graph isomorphism test provides a widely accepted benchmark for this discriminative power [weisfeiler1968reduction]. The 1-dimensional WL (1-WL) test, also known as the color refinement algorithm, iteratively updates node "colors" (labels) based on the multiset of colors of their neighbors. If two graphs cannot be distinguished by the 1-WL test, they are considered 1-WL equivalent.

A significant theoretical finding established that many standard message-passing GNNs, including GCNs and GraphSAGE, are at most as powerful as the 1-WL test in distinguishing non-isomorphic graphs [xu2018c8q, morris20185sd, garg2020z6o]. This means that if the 1-WL test cannot differentiate two graphs, these GNNs also cannot, regardless of their learned parameters. This theoretical limitation is a major bottleneck, as the 1-WL test is known to be unable to distinguish many common non-isomorphic graphs, such as certain regular graphs or graphs that differ only in specific substructure counts (e.g., cycle sizes) [chen2020e6g, oono2019usb]. This theoretical gap prevents 1-WL equivalent GNNs from solving graph problems that require a finer understanding of graph topology.

This limitation sparked an "arms race" in GNN research, driving the development of more expressive architectures. One direction involved designing **higher-order GNNs** that mimic higher-dimensional WL tests (e.g., k-WL tests), which consider k-tuples of nodes. While theoretically more powerful, these k-GNNs often incur significantly higher computational complexity, limiting their practical applicability [morris20185sd].

A more practical and influential direction has been the development of GNNs that leverage richer structural information. Path Neural Networks (PathNNs) [michel2023hc4] represent a significant advancement in this regard. By explicitly encoding and aggregating information from various paths emanating from each node, PathNNs move beyond local neighborhood information. Crucially, the introduction of "annotated sets of paths," where nodes within paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, allows PathNNs to achieve significantly higher expressive power. Variants like `˜AP` (All Simple Paths with annotations) have been theoretically and empirically shown to distinguish graphs that are even 3-WL indistinguishable, far surpassing the 1-WL bottleneck of traditional GNNs. However, this enhanced expressiveness comes with a trade-off: finding all simple paths is NP-hard, necessitating approximations (e.g., fixed path length $K$) and careful design to manage computational complexity.

Another approach to overcome the WL limitation is to fundamentally alter the message-passing mechanism. The Random Walk with Unifying Memory (RUM) network [wang2024oi8], by being entirely non-convolutional and relying on random walk trajectories, is theoretically shown to be more expressive than the WL isomorphism test. It can distinguish non-isomorphic graphs that WL-equivalent GNNs cannot, such as those differing in cycle sizes or radius. This demonstrates that breaking from the standard convolutional aggregation paradigm can lead to architectures with superior discriminative power. The evolution of GNNs, therefore, reflects a continuous effort to overcome the theoretical limitations imposed by the WL test, balancing the need for higher expressiveness with practical considerations of computational efficiency and scalability.

### 3. Enhancing Expressiveness and Overcoming Early Limitations

\section*{3. Enhancing Expressiveness and Overcoming Early Limitations}
The initial success of Graph Neural Networks (GNNs) in extending deep learning to graph-structured data was quickly met with the identification of fundamental limitations that hindered their representational power, robustness, and applicability to real-world complexities. Early GNNs, largely constrained by the expressive power of the 1-Weisfeiler-Leman (1-WL) test, struggled to distinguish between many non-isomorphic graphs, thereby limiting their ability to capture intricate structural patterns [xu2018c8q, morris20185sd]. Furthermore, the iterative message-passing paradigm, while intuitive, introduced pathologies such as over-smoothing, where node representations become indistinguishable in deep models, and over-squashing, which impedes the propagation of long-range dependencies [rusch2023xev, alon2020fok]. Critically, the implicit homophily assumption, prevalent in many early designs, rendered these models ineffective on heterophilous graphs, where connected nodes often exhibit dissimilar features or labels [ma2021sim, zhu2020c3j].

This section delves into the crucial advancements that propelled GNNs beyond these early constraints, marking a significant evolutionary phase in the field. It explores the development of sophisticated message-passing mechanisms and higher-order architectures designed to enhance representational power, moving beyond the 1-WL bottleneck. A substantial focus is placed on the innovative strategies devised to mitigate the over-smoothing and over-squashing problems, enabling the construction of deeper, more effective GNNs. Finally, it examines the critical challenge of adapting GNNs to diverse graph structures, particularly those exhibiting heterophily, thereby broadening their applicability to the complex and varied topologies found in real-world datasets [wu2022ptq, zhang20222g3, wang2023zr0]. These advancements collectively represent an "arms race" to build more robust, expressive, and versatile GNNs capable of tackling the full spectrum of graph learning tasks.

\subsection*{3.1. Higher-Order and Advanced Message Passing Mechanisms}
The inherent limitation of many standard message-passing GNNs to the expressive power of the 1-Weisfeiler-Leman (1-WL) test became a significant theoretical bottleneck, preventing them from distinguishing common non-isomorphic graphs and capturing complex substructures [xu2018c8q, morris20185sd]. This limitation spurred an intense research effort to develop higher-order and more advanced message-passing mechanisms that could surpass the 1-WL boundary.

One prominent direction involves explicitly leveraging path information within the graph. Path Neural Networks (PathNNs) [michel2023hc4] represent a notable advancement in this area. Unlike traditional GNNs that aggregate information from direct neighbors, PathNNs encode and aggregate information from various paths emanating from each node. Their core innovation lies in the use of "annotated sets of paths," where nodes within a path are recursively annotated with hashes of their shorter path sets. This hierarchical structural encoding allows PathNNs to achieve significantly higher expressive power. Specifically, variants like `˜AP` (All Simple Paths with annotations) have been theoretically and empirically demonstrated to distinguish graphs that are even 3-WL indistinguishable, a substantial leap beyond the 1-WL capabilities of most GNNs [michel2023hc4]. However, this enhanced expressiveness comes with a practical trade-off: finding all simple paths is an NP-hard problem, necessitating approximations such as fixing a maximum path length $K$, which can limit the model's ability to capture extremely long-range dependencies or scale to very dense graphs. The computational complexity of path enumeration remains a significant practical constraint for these highly expressive models.

A distinct, yet equally impactful, approach to enhancing expressiveness involves fundamentally rethinking the message-passing paradigm itself. The Random Walk with Unifying Memory (RUM) neural network [wang2024oi8] proposes an entirely non-convolutional GNN architecture. Instead of iterative neighborhood aggregation, RUM stochastically samples finite-length random walks for each node. It then employs an RNN (specifically, GRU) to merge "semantic representations" (node embeddings along the walk) with "topological environments" (captured by a novel "anonymous experiment" function that labels nodes based on their unique traversal order). This departure from the standard convolutional aggregation allows RUM to overcome the 1-WL test's limitations, demonstrating the ability to distinguish non-isomorphic graphs that differ in properties like cycle sizes or radius, which are problematic for WL-equivalent GNNs [wang2024oi8]. The theoretical underpinnings suggest that RUM's expressiveness stems from its ability to process sequences of node features and their unique topological context along walks, rather than relying solely on local multiset aggregation. While RUM offers a compelling alternative, its theoretical proofs for expressiveness rely on assumptions about the universality and injectivity of its internal functions, and its reliance on RNNs for walk processing introduces a different set of computational considerations compared to simple aggregators.

These advancements highlight a critical evolutionary trend: moving beyond mere aggregation of neighbor features to incorporate richer structural contexts, whether through explicit path enumeration or dynamic random walk explorations. The trade-off between theoretical expressiveness and practical computational feasibility remains a central challenge, pushing researchers to develop more efficient approximation schemes and novel architectural designs that can balance these competing demands [chen2022mmu, dwivedi20239ab].

\subsection*{3.2. Tackling Over-smoothing and Enabling Deep GNNs}
The promise of deep learning lies in its ability to learn hierarchical representations through many layers. However, for GNNs, increasing depth often leads to a severe degradation in performance due to the over-smoothing problem [rusch2023xev, cai2020k4b, chen2019s47, oono2019usb]. Over-smoothing occurs when repeated aggregation of neighbor features causes node representations to converge to a single point in the embedding space, making them indistinguishable and losing their discriminative power. This phenomenon is particularly acute in dense graphs and effectively limits the practical depth of GNNs to a few layers. Closely related is the over-squashing problem, an information bottleneck where the fixed-size messages in each layer struggle to compress and propagate information from an exponentially growing receptive field, leading to a loss of crucial long-range dependencies [alon2020fok].

To overcome these fundamental barriers, researchers have explored diverse strategies. One line of work focuses on architectural modifications inspired by deep learning in other domains, such as incorporating residual connections (e.g., GCNII [chen2020simple]) or skip connections [li2021orq, liu2020w3t, zeng2022jhz]. These mechanisms help preserve initial node features and facilitate gradient flow, allowing for deeper architectures. Regularization techniques like DropEdge, which randomly removes edges during training, also help by preventing over-reliance on local neighborhoods and diversifying information paths [rong2019dropedge].

More fundamentally, novel propagation mechanisms have been proposed to inherently mitigate over-smoothing. The Random Walk with Unifying Memory (RUM) network [wang2024oi8], by adopting a non-convolutional, random walk-based approach, offers a joint remedy to over-smoothing and over-squashing. Unlike convolutional GNNs where the expected Dirichlet energy (a measure of smoothness) diminishes with depth, RUM's expected Dirichlet energy is theoretically shown to not diminish even with long walks. This implies that node representations do not necessarily converge to a single point, thus attenuating over-smoothing. Furthermore, RUM is shown to decay slower in inter-node Jacobian compared to convolutional counterparts, which helps mitigate the over-squashing problem by improving gradient flow for long-range dependencies [wang2024oi8]. This approach suggests that by moving away from local, iterative averaging, GNNs can maintain distinct node identities while integrating global context.

Another innovative direction involves integrating fractional calculus into continuous GNNs. The FRactional-Order graph Neural Dynamical network (FROND) framework [kang2024fsk] replaces integer-order differential operators with Caputo fractional derivatives. Traditional continuous GNNs, based on integer-order ODEs, model instantaneous, Markovian updates, leading to exponential convergence to stationarity and thus over-smoothing. FROND, by contrast, introduces non-local, memory-dependent dynamics, where the entire historical trajectory of node features influences their update. Theoretically, FROND's non-Markovian random walk interpretation leads to an algebraic (slower) rate of convergence to stationarity, inherently mitigating over-smoothing [kang2024fsk]. This framework generalizes existing continuous GNNs and has been shown to consistently improve performance by capturing complex, memory-dependent graph dynamics. The use of numerical FDE solvers, however, introduces computational considerations that need careful management. Both RUM and FROND exemplify a shift towards more sophisticated, theoretically grounded propagation models that move beyond simple local averaging, enabling the development of truly deep and effective GNNs.

\subsection*{3.3. Adapting to Diverse Graph Structures and Heterophily}
A foundational assumption in many early GNNs is homophily, which posits that connected nodes tend to share similar features or labels [ma2021sim]. While this holds true for many social and citation networks, real-world graphs often exhibit heterophily, where connected nodes are dissimilar [zhu2020c3j, zheng2022qxr]. Traditional message-passing GNNs, by aggressively smoothing features from neighbors, perform poorly on heterophilous graphs because aggregating dissimilar information can lead to noisy or misleading node representations. Adapting GNNs to these diverse and complex graph structures, particularly those with strong heterophily, has become a critical challenge.

Initial attempts to address heterophily often involved modifying the aggregation scheme or expanding the receptive field. Some methods leverage both low-pass (smoothing) and high-pass (emphasizing differences) filters, or dynamically adjust aggregation weights based on node similarity [gprgnn]. Others enlarge the node neighborhood to include multi-hop neighbors, hoping to find homophilous nodes further away (e.g., H2GCN, WRGAT). However, these approaches face limitations: determining optimal personalized neighborhood sizes is difficult, and they may still miss globally distant but homophilous nodes, while a naive global aggregation would be computationally prohibitive [li2022315].

The GloGNN and GloGNN++ models [li2022315] offer a significant advancement by performing node neighborhood aggregation from the *whole set of nodes* in the graph, rather than just local or multi-hop neighbors. This allows them to capture "global homophily" that might exist between distant nodes. The core innovation lies in learning a **signed coefficient matrix Z(l)** for each layer, where Z(l)ij quantifies the importance of node $j$ to node $i$. Crucially, these coefficients can be positive (for homophilous connections) or negative (for heterophilous ones), effectively combining low-pass and high-pass filtering within a single aggregation step. This learned matrix is derived from an optimization problem and regularized by multi-hop reachabilities, incorporating both feature and topological similarity. A key strength of GloGNN is its computational efficiency: despite aggregating globally, the model transforms the aggregation equation to avoid direct computation of the dense Z(l) matrix, achieving *linear time complexity* (O(k2n)) by leveraging matrix properties and the number of labels, thereby making global aggregation feasible for large graphs [li2022315]. Theoretically, GloGNN proves a "grouping effect," where nodes with similar features and local structures (even if distant) will have similar coefficient and representation vectors, explaining its effectiveness.

The development of models like GloGNN highlights an important shift from assuming local homophily to actively learning and leveraging global relationships, even in the presence of heterophily. This move requires not only sophisticated aggregation mechanisms but also efficient computational strategies to handle the increased scope of information integration. The challenge of adapting GNNs to diverse graph structures extends beyond heterophily to include issues like structural disparity [mao202313j], where different nodes may require different aggregation strategies, and the integration of heterogeneous information networks [lv20219al, wei20246l2]. These advancements collectively underscore the ongoing effort to build GNNs that are robust and flexible enough to model the intricate complexities of real-world graph data, moving beyond simplified assumptions to capture the full spectrum of relational patterns.

### 4. Advanced Methodologies: Robustness, Generalization, and Efficiency

\section*{4. Advanced Methodologies: Robustness, Generalization, and Efficiency}
The remarkable progress of Graph Neural Networks (GNNs) has been significantly driven by the development of sophisticated methodologies that address their limitations in real-world deployment. While earlier advancements focused on enhancing expressiveness and mitigating fundamental issues like over-smoothing and heterophily, the current frontier emphasizes making GNNs more robust, generalizable, and efficient. This section delves into these advanced techniques, which are crucial for transitioning GNNs from theoretical promise to practical applicability across diverse domains. It explores the evolution of self-supervised learning and pre-training, enabling GNNs to learn from vast amounts of unlabeled graph data and generalize to new tasks. Furthermore, it examines the emergence of prompt-based adaptation, a paradigm shift for efficient transfer learning, including the integration of multi-modal information with large language models. Critical concerns regarding GNN robustness against adversarial attacks and their resilience to inherent data imperfections are also addressed, highlighting the ongoing "arms race" between attackers and defenders. Finally, the section investigates geometric and equivariant GNNs, which are specifically designed to respect physical symmetries and incorporate spatial information, proving indispensable for applications in fields like molecular modeling and physics simulations. These advanced methodologies collectively aim to equip GNNs with the necessary capabilities for reliable and effective operation in complex, dynamic, and often imperfect real-world environments [wu2022ptq, zhang20222g3, velickovic2023p4r].

\subsection*{Self-Supervised Learning and Pre-training Strategies}
The success of deep learning models in other domains, particularly in natural language processing and computer vision, has been heavily reliant on large-scale pre-training using self-supervised learning (SSL) objectives, followed by fine-tuning on downstream tasks. This paradigm has proven instrumental in improving generalization and addressing data scarcity, and GNNs are increasingly adopting similar strategies [xie2021n52]. The motivation stems from the fact that labeled graph data is often expensive or difficult to obtain, while unlabeled graph structures are abundant. Self-supervised learning for GNNs typically involves creating auxiliary tasks that allow the model to learn meaningful node or graph representations without explicit human annotations.

Early approaches to pre-training GNNs focused on tasks like predicting node attributes, reconstructing graph structure (e.g., edges or subgraphs), or maximizing mutual information between node embeddings and their contexts [hu2019r47, lu20213kr]. Generative pre-training, exemplified by models like GPT-GNN [hu2020u8o], aims to generate graph structures or node features, thereby learning a comprehensive understanding of graph topology and attributes. More recently, contrastive learning has emerged as a dominant paradigm, where GNNs learn by maximizing agreement between different augmented views of the same node or graph, while minimizing agreement with negative samples [zhang20211dl]. This involves creating multiple perturbed versions of a graph (e.g., via edge dropping, feature masking, or subgraph sampling) and training the GNN to produce similar embeddings for different views of the same entity. Such methods have shown significant promise in learning robust and transferable representations, particularly for tasks like link prediction in biomedical networks [long2022l97] or enhancing feature extraction in heterogeneous information networks [wei20246l2].

While SSL and pre-training offer substantial benefits in terms of generalization and data efficiency, they present unique challenges in the graph domain. The choice of graph augmentation strategies can significantly impact the quality of learned representations, and designing augmentations that preserve essential semantic or structural information while introducing sufficient perturbation remains an active research area [zhao2020bmj]. Furthermore, the computational cost of pre-training on very large graphs can be prohibitive, necessitating scalable architectures and training strategies [vasimuddin2021x7c]. The "task-agnostic" nature of many pre-training objectives might also lead to a mismatch with specific downstream tasks, requiring careful fine-tuning or more task-aware pre-training designs. For instance, pre-training for molecular property prediction often requires domain-specific inductive biases, as seen in models like Mole-BERT [xia2023bpu]. The evolution of these strategies reflects a continuous effort to balance the generality of learned representations with the specificity required for high performance on diverse real-world applications.

\subsection*{Prompt-based Adaptation and Multi-modal Learning}
Building upon the success of pre-trained GNNs, prompt-based adaptation has emerged as a powerful paradigm for efficient transfer learning, particularly for few-shot or low-resource scenarios. Inspired by the success of prompt engineering in Large Language Models (LLMs), this approach aims to adapt a pre-trained GNN to diverse downstream tasks by formulating the task as a "prompt" rather than requiring extensive fine-tuning of all model parameters. This significantly reduces the number of trainable parameters and accelerates adaptation, making GNNs more efficient for real-world deployment across a multitude of tasks [sun2022d18, fang2022tjj].

Prompt tuning for GNNs typically involves adding a small, learnable "prompt" module (e.g., a set of virtual nodes, edges, or feature vectors) to the input graph or intermediate layers of a pre-trained GNN. This prompt is then optimized for a specific downstream task, guiding the pre-trained model to extract relevant information without altering its core learned representations. Models like GPPT [sun2022d18] and GraphPrompt [liu2023ent] demonstrate how this strategy can achieve competitive performance with full fine-tuning while being significantly more parameter-efficient. The flexibility of prompting also extends to multi-task learning, where a single pre-trained GNN can be adapted to multiple objectives using different prompts [sun2023vsl].

A particularly exciting and rapidly developing direction is the integration of GNNs with large language models (LLMs) for multi-modal learning. This addresses the limitation that GNNs primarily operate on graph topology and node features, often lacking the rich semantic understanding that LLMs possess. By leveraging LLMs, GNNs can incorporate textual descriptions, external knowledge, or even user queries to enhance their understanding of graph entities and relationships. For instance, GNNs can be used to process graph structures, while LLMs interpret associated text, with prompt-based interfaces facilitating their interaction. This synergy enables tasks like knowledge graph completion with natural language queries, text-guided graph neural networks for 3D instance segmentation [huang2021lpu], or even learning language with extremely weak text supervision on graphs [li202444f]. Recent work explores how LLMs can even improve the adversarial robustness of GNNs by providing semantic context for defense strategies [zhang2024370]. The Hybrid-LLM-GNN framework, for example, integrates LLMs and GNNs for enhanced materials property prediction, demonstrating the power of combining symbolic and structural reasoning [li2024gue]. However, challenges remain in effectively aligning the discrete, structural nature of graphs with the continuous, semantic space of language models, and in designing prompts that are robust and generalizable across diverse multi-modal tasks [castroospina2024iy2]. The interpretability of such hybrid models also becomes more complex, as disentangling the contributions of graph structure and linguistic prompts is non-trivial.

\subsection*{Robustness to Adversarial Attacks and Data Imperfections}
The deployment of GNNs in critical applications, such as cybersecurity [mitra2024x43, bilot20234ui], fraud detection [duan2024que], and medical diagnostics [abadal2024w7e], necessitates strong guarantees of robustness against adversarial attacks and resilience to inherent data imperfections. This area has become an intense "arms race" between attackers seeking to compromise GNN integrity and defenders striving to build more secure models [dai2022hsi, zhang20222g3].

Adversarial attacks on GNNs can be broadly categorized into poisoning attacks (at training time) and evasion attacks (at inference time). Poisoning attacks aim to inject malicious nodes or perturb graph structures and features in the training data to degrade model performance or induce specific misclassifications [zhang2020b0m, zou2021qkz]. Backdoor attacks, a specific type of poisoning, embed hidden triggers in the training graph such that a GNN behaves maliciously only when these triggers are present in the input [dai2023tuj]. Evasion attacks, on the other hand, involve subtly perturbing the graph structure or node features of a test instance to cause misclassification without altering the model itself [zgner2019bbi, xu2019l8n]. The non-Euclidean nature of graph data makes these attacks particularly challenging to detect and defend against, as small, imperceptible changes in topology can have cascading effects through message passing.

Defense strategies against adversarial attacks include adversarial training [gosch20237yi], where models are trained on adversarially perturbed graphs to improve their resilience. Robust aggregation mechanisms, which filter out noisy or malicious messages, and certified robustness methods, which provide provable guarantees on model predictions within a certain perturbation budget, are also actively researched [xia2024xc9, abbahaddou2024bq2]. Graph structure learning, where the GNN learns to adaptively refine or prune its input graph, can also enhance robustness by mitigating the impact of malicious edges [jin2020dh4]. However, a critical analysis reveals that many proposed defenses are evaluated against simplified attack models or specific perturbation types, and their effectiveness against adaptive, stronger attacks remains questionable [mujkanovic20238fi]. There is a clear trade-off between robustness and accuracy, where highly robust models may sacrifice some performance on clean data.

Beyond adversarial attacks, GNNs must also contend with inherent data imperfections, such as noisy labels [dai2022xze], missing features, and various forms of bias. Data augmentation techniques can help improve robustness to noise [zhao2020bmj], while methods for learning with weak information [liu2023v3e] or sparse labels [wang2024htw] are crucial for real-world datasets. Addressing data bias, which can lead to unfair or discriminatory outcomes, is another significant concern [dong2021qcg, fan2022m67]. Techniques like disentangled causal substructure learning [fan2022m67] and re-balancing strategies [li20245zy] aim to mitigate bias and improve fairness in GNN predictions. The challenge lies in developing methods that are simultaneously robust to diverse imperfections, computationally efficient, and maintain high predictive performance, often requiring a delicate balance between these competing objectives [zhang2024ctj].

\subsection*{Geometric and Equivariant Graph Neural Networks}
For domains where physical symmetries and spatial arrangements are paramount, such as molecular modeling, materials science, and physics simulations, standard GNNs that are merely permutation-invariant fall short. These applications demand models that are not only invariant to permutations of nodes but also *equivariant* to geometric transformations (e.g., rotations, translations, reflections) of the input coordinates. Geometric and equivariant GNNs are specifically designed to incorporate these inductive biases, ensuring that the model's output transforms predictably when its input undergoes a geometric transformation [han20227gn].

The core idea behind equivariant GNNs, particularly E(n)-equivariant GNNs (where E(n) is the Euclidean group in n dimensions), is to operate directly on 3D coordinates and vector features, ensuring that intermediate representations and final predictions respect the underlying symmetries of the physical system. This is achieved by designing message-passing functions that are themselves equivariant, often by using basis functions that transform correctly under rotations and translations [satorras2021pzl]. For instance, models like E(n) Equivariant Graph Neural Networks [satorras2021pzl] and GemNet [klicpera20215fk] explicitly incorporate relative positional information and directional features, enabling them to accurately predict molecular properties or interatomic potentials [batzner2021t07, reiser2022b08]. The expressive power of these models is significantly enhanced by their ability to encode geometric relationships, as demonstrated by theoretical analyses [joshi20239d0]. Positional encodings, which are critical for standard GNNs to capture structural information beyond connectivity, are also adapted to be equivariant and stable in this context [wang2022p2r].

The development of geometric GNNs has revolutionized applications in drug discovery [jiang2020gaq, li2021v1l], materials design [fung20212kw, fang2024p34], and protein structure prediction [xia2021s85], where the precise spatial arrangement of atoms and molecules dictates their properties and interactions. They allow for data-efficient learning, as the built-in symmetries reduce the need for extensive data augmentation to cover all possible orientations. Recent advancements include using geometric GNNs to derive descriptor-free collective variables for molecular dynamics simulations [zhang202483k] and exploring their application in learning equivariant representations of neural networks themselves [kofinas2024t2b]. Physics-informed GNNs, which embed physical laws directly into the network architecture, further extend this concept for applications like water distribution systems [ashraf202443e] and deformation prediction [saleh2024d2a].

However, geometric and equivariant GNNs are not without limitations. Their computational complexity can be higher due to the need to handle vector and tensor representations that transform correctly. The design of truly universal equivariant layers that can capture all relevant symmetries for arbitrary tasks remains an open challenge. Furthermore, while they excel at geometric tasks, their benefits might be less pronounced in purely topological or abstract graph problems. The question of whether high-degree representations are always necessary in equivariant GNNs is also being actively investigated, suggesting potential for more efficient designs [cen2024md8]. Despite these challenges, the ability of equivariant GNNs to bridge the gap between abstract graph structures and concrete physical realities marks a significant step towards more physically consistent and powerful graph learning models [shi2024g4z].

### 5. Trustworthy GNNs: Explainability, Fairness, and Privacy

\section*{5. Trustworthy GNNs: Explainability, Fairness, and Privacy}
The increasing deployment of Graph Neural Networks (GNNs) in sensitive and high-stakes applications, ranging from healthcare and finance to cybersecurity, necessitates a paramount focus on their trustworthiness. Beyond mere predictive accuracy, users and stakeholders demand models that are explainable, fair, and privacy-preserving. This section delves into the burgeoning research dedicated to instilling these critical attributes into GNNs, acknowledging that their responsible deployment hinges on addressing these multifaceted challenges [dai2022hsi, zhang20222g3]. Explainability aims to demystify GNN predictions, fostering transparency and user confidence by revealing the underlying reasoning. Fairness seeks to mitigate biases inherent in data and model architectures, ensuring equitable outcomes across different demographic groups. Concurrently, privacy-preserving techniques are crucial for safeguarding sensitive graph data during training and inference, upholding ethical standards and regulatory compliance. These three pillars of trustworthiness are often interconnected, presenting complex trade-offs where enhancing one aspect might inadvertently impact another, thereby creating an intricate research landscape that requires careful navigation and innovative solutions [wang20214ku].

\subsection*{Explainable Graph Neural Networks (XGNNs)}
The black-box nature of deep learning models, including GNNs, poses a significant barrier to their adoption in domains requiring transparency and accountability. Explainable Graph Neural Networks (XGNNs) aim to bridge this gap by providing insights into *why* a GNN makes a particular prediction. Early efforts in GNN explainability primarily focused on identifying important nodes, edges, or node features that contribute most to a prediction [ying2019rza]. GNNExplainer [ying2019rza], for instance, identifies a compact subgraph and a small subset of features that are crucial for a specific prediction by maximizing the mutual information between the original graph and the explanation. While foundational, these methods often yield disconnected sets of nodes or edges, which can be less intuitive for human understanding.

A significant advancement in this direction is the direct identification of crucial subgraphs as explanations. SubgraphX [yuan2021pgd] proposes a novel framework that employs Monte Carlo Tree Search (MCTS) to efficiently explore the vast space of possible subgraphs and utilizes Shapley values from cooperative game theory to quantify subgraph importance. This approach inherently captures interactions among different graph structures, providing more coherent and human-intelligible explanations compared to merely highlighting individual components. Similarly, ProtGNN [zhang2021wgf] aims for self-explaining GNNs by learning prototypes that represent common graph patterns, offering model-level insights akin to XGNN [yuan20208v3], which generates general graph patterns for explanations. More recently, methods like [bui2024zy9] propose structure-aware interaction indices, while [luo2024euy] focuses on inductive and efficient explanations, addressing scalability for larger graphs. The challenge of explaining GNNs for specific applications, such as connectome-based brain disorder analysis, has led to specialized interpretable GNNs [cui2022pap]. Counterfactual explanations, as explored by CF-GNNExplainer [lucic2021p70], provide insights by identifying minimal changes to the input graph that alter the prediction, offering a different perspective on model sensitivity.

Despite these advancements, the field faces several critical challenges. Defining and evaluating the "ground truth" for GNN explanations remains an open problem, as highlighted by [agarwal2022xfp] and critically examined by [chen2024woq], which questions how interpretable interpretable GNNs truly are. The trade-off between explanation fidelity (how accurately the explanation reflects the model's true reasoning) and human interpretability (how easily a human can understand and trust the explanation) is persistent. Furthermore, the computational cost of generating explanations, especially for complex models or large graphs, can be prohibitive, often necessitating approximation techniques like those in SubgraphX [yuan2021pgd]. The theoretical gaps preventing a universal framework for GNN explainability stem from the inherent complexity of graph data and the non-linear, iterative nature of message passing, making it difficult to attribute predictions to specific input components without simplifying assumptions. Future directions include developing more robust evaluation metrics, exploring global interactive patterns [wang2024j6z], and integrating causal reasoning to identify invariant rationales [wu2022vcx].

\subsection*{Fairness and Bias Mitigation in GNNs}
The pervasive use of GNNs in decision-making systems raises significant concerns about fairness, as biases embedded in training data or model architectures can lead to discriminatory outcomes. Ensuring fairness in GNNs is crucial for ethical AI deployment, particularly in sensitive applications like social recommendation [fan2019k6u, sharma2022liz] or credit risk assessment [liu2024sbb]. The sources of bias in GNNs are manifold, including disparities in node features, imbalanced graph structures, and the inherent homophily assumption of many GNNs, which can amplify existing societal biases [ma2021sim, zhu2020c3j].

Research in fair GNNs often distinguishes between group fairness (ensuring similar outcomes for predefined demographic groups) and individual fairness (ensuring similar outcomes for similar individuals). Dong et al. [dong202183w] address individual fairness for GNNs using a ranking-based approach, aiming to ensure that similar nodes (based on features and structure) receive similar predictions. Another critical aspect is mitigating data bias, as explored by EDITS [dong2021qcg], which models and mitigates data bias for GNNs. This involves techniques that either preprocess the graph to reduce bias, modify the GNN training process, or post-process the model's outputs. For scenarios with limited sensitive attribute information, Dai and Wang [dai2020p5t] propose methods to learn fair GNNs, highlighting the practical challenge of sensitive attribute availability.

More advanced techniques focus on disentangling causal factors to mitigate bias. Fan et al. [fan2022m67] propose debiasing GNNs via learning disentangled causal substructures, aiming to separate the influence of sensitive attributes from task-relevant features. Similarly, re-balancing strategies, as rethought by Li et al. [li20245zy], offer ways to adjust the influence of different samples or groups during training to achieve fairer outcomes. Wang et al. [wang2022531] address fairness by mitigating sensitive attribute leakage, preventing the model from inadvertently using protected information. The challenge of structural disparity in GNNs, where different nodes or communities might be treated unequally due to graph topology, is demystified by Mao et al. [mao202313j], who question whether a "one size fits all" approach is suitable.

A critical analysis reveals that achieving fairness often involves trade-offs with other desirable properties, such as accuracy or utility. Luo et al. [luo20240ot] explicitly address this by proposing FUGNN, a framework for harmonizing fairness and utility in GNNs. Methodological limitations include the difficulty in universally defining and measuring fairness across diverse tasks and datasets, as different fairness metrics can sometimes contradict each other. Furthermore, many debiasing techniques assume access to sensitive attribute information, which may not always be available or legally permissible. The theoretical gaps often lie in developing robust causal inference frameworks for graph data that can effectively identify and remove confounding biases without sacrificing predictive power. The generalizability of debiasing methods across different graph types and tasks also remains an active research area.

\subsection*{Privacy-Preserving Graph Neural Networks}
The processing of graph-structured data by GNNs inevitably involves sensitive information, such as personal connections in social networks, financial transactions, or medical records. This necessitates robust privacy-preserving mechanisms to prevent data leakage and ensure ethical use. The privacy landscape for GNNs is characterized by an "arms race" between sophisticated privacy attacks and defensive strategies. Attackers can attempt to infer sensitive node attributes, identify the existence of specific links, or even reconstruct parts of the graph structure from model outputs or gradients [he2020kz4]. For instance, link stealing attacks [he2020kz4] demonstrate how an adversary can infer the existence of links from a trained GNN, while backdoor attacks [zhang2020b0m, dai2023tuj] can be crafted to embed hidden triggers that, when activated, reveal sensitive information or induce specific malicious behaviors without being easily noticeable.

To counter these threats, several privacy-preserving techniques have been adapted for GNNs. Differential Privacy (DP) is a prominent approach that adds carefully calibrated noise to data, gradients, or model parameters during training to provide provable privacy guarantees. While effective, DP often comes at the cost of utility, as increased privacy typically leads to decreased model accuracy, a trade-off that is particularly challenging for GNNs due to their reliance on structural information. Another promising direction is Federated Learning (FL), where GNNs are trained collaboratively across multiple decentralized clients without sharing raw data, only exchanging model updates [liu2022gcg, he2021x8v]. This distributed paradigm offers a strong baseline for privacy, but FL systems can still be vulnerable to inference attacks on model updates or through malicious clients. Cooperative weighting in federated GNNs, as explored by [hausleitner2024vw0], attempts to enhance privacy and utility in distributed settings.

Beyond DP and FL, cryptographic techniques like Homomorphic Encryption (HE) and Secure Multi-Party Computation (SMC) offer stronger privacy guarantees by enabling computations on encrypted data. However, these methods are notoriously computationally expensive, making them impractical for large-scale GNN training and inference in many real-world scenarios. The methodological limitations of current privacy-preserving GNNs include the significant computational overhead of cryptographic methods, the inherent utility-privacy trade-off in DP, and the persistent challenge of ensuring robustness against sophisticated inference attacks in FL settings. The theoretical gaps often relate to developing efficient and scalable privacy-preserving primitives that can handle the complex, non-Euclidean operations of GNNs without prohibitive performance degradation. Future research needs to focus on designing hybrid approaches that combine the strengths of different techniques, such as integrating DP with FL, or developing novel GNN architectures that are inherently more privacy-preserving by design, thereby fostering greater confidence and ethical use of GNN technology in real-world scenarios.

### 6. Key Challenges and Open Problems

\section*{6. Key Challenges and Open Problems}
Despite the remarkable advancements and widespread adoption of Graph Neural Networks (GNNs) across diverse domains, from recommender systems [gao2022f3h] and drug discovery [jiang2020gaq] to urban computing [jin2023e18] and cybersecurity [mitra2024x43], the field continues to grapple with several fundamental and persistent challenges. These challenges are not merely incremental hurdles but often represent deep theoretical and practical limitations that hinder the full potential and reliable deployment of GNNs in real-world, complex scenarios [khemani2024i8r, wu2022ptq]. This section critically analyzes these key challenges, encompassing the difficulties of scaling GNNs to massive and dynamic graphs, the inherent limitations in their expressive power, the crucial problem of generalization to unseen graph structures and mitigating distribution shifts, and the continuous need for robust evaluation and standardized benchmarking. Addressing these open problems is paramount for fostering reliable progress and ensuring the responsible and effective application of GNN technology. Many of these issues are interconnected; for instance, enhancing expressiveness might exacerbate scalability problems, while poor evaluation methodologies can obscure genuine advancements in generalization.

\subsection*{6.1. Scalability to Large and Dynamic Graphs}
The sheer scale and dynamic nature of many real-world graphs present significant computational and memory challenges for GNNs. Traditional message-passing GNNs often incur high computational costs, particularly for graphs with millions or billions of nodes and edges, as each layer requires aggregating information from expanding neighborhoods [gao2022f3h]. This quadratic or even cubic complexity in terms of the number of nodes or edges makes direct application to massive graphs prohibitive. While sampling-based methods like GraphSAGE [hamilton2017inductive] or mini-batching techniques offer practical solutions by limiting the receptive field, they often come with trade-offs in terms of information loss or approximation quality. Distributed training frameworks like DistGNN [vasimuddin2021x7c] and scalable architectures such as SIGN [rossi2020otv] and GNNAutoScale [fey2021smn] have emerged to tackle this, but the fundamental challenge of processing the entire graph structure efficiently remains.

Beyond static scalability, real-world graphs are inherently dynamic, with nodes and edges appearing, disappearing, or changing attributes over time. Effectively handling these temporal graphs is a critical open problem [longa202399q, jin2023e18]. Existing dynamic GNNs often rely on snapshot-based approaches, retraining, or incremental updates, which can be computationally intensive and struggle to capture continuous temporal dependencies [li2020mk1, zhang20212ke]. For instance, in applications like ETA prediction in Google Maps [derrowpinion2021mwn] or traffic flow forecasting [li2020fil], the graph structure and features evolve rapidly, demanding models that can adapt in real-time without catastrophic forgetting [zhou2021c3l]. While some approaches like Scalable Spatiotemporal GNNs [cini2022pjy] and Spatio-Spectral GNNs [geisler2024wli] attempt to address this, the trade-off between capturing fine-grained temporal dynamics and maintaining computational efficiency is a persistent hurdle. Novel non-convolutional architectures, such as Random Walk with Unifying Memory (RUM) [wang2024oi8], offer a promising direction by achieving runtime complexity agnostic to the number of edges, which could alleviate some scalability concerns. Similarly, GloGNN [li2022315] demonstrates linear time complexity for global aggregation, showcasing how algorithmic innovations can unlock scalability for specific challenges like heterophily. However, a unified, efficient, and expressive framework for truly massive and continuously evolving graphs remains an active area of research.

\subsection*{6.2. Persistent Issues with Expressiveness and Over-squashing}
A foundational limitation of many GNNs, particularly message-passing variants, is their restricted expressive power, often bounded by the 1-Weisfeiler-Leman (1-WL) graph isomorphism test [xu2018c8q, morris20185sd]. This means they struggle to distinguish between certain non-isomorphic graphs or capture complex structural patterns like cycles of specific lengths [chen2020e6g]. This theoretical bottleneck limits their ability to learn intricate graph properties crucial for many tasks.

Two related and widely recognized phenomena further compound this: over-smoothing and over-squashing. Over-smoothing occurs when node representations become increasingly similar and indistinguishable as information propagates through many GNN layers, leading to a loss of local distinctiveness [oono2019usb, cai2020k4b, rusch2023xev]. This is a direct consequence of repeated neighborhood averaging, which acts as a low-pass filter [zhou20213lg]. While techniques like residual connections [li2021orq], DropEdge [rong2019dropedge], or advanced propagation schemes [klicpera20186xu, zeng2022jhz] aim to mitigate over-smoothing, they often do not fundamentally alter the information diffusion process that causes it. Over-squashing, on the other hand, refers to the information bottleneck that arises when aggregating information from exponentially growing receptive fields into fixed-size node embeddings, making it difficult to capture long-range dependencies [alon2020fok, wu20221la]. This is particularly problematic for tasks requiring global graph understanding or interactions between distant nodes.

Recent research has made strides in addressing these issues. Path Neural Networks (PathNNs) [michel2023hc4] directly tackle expressiveness by leveraging path information, demonstrating the ability to distinguish graphs that are 3-WL indistinguishable, significantly surpassing the 1-WL limit. This approach, however, can incur high computational costs for enumerating paths. The Random Walk with Unifying Memory (RUM) neural network [wang2024oi8] offers a novel non-convolutional paradigm that jointly remedies limited expressiveness, over-smoothing, and over-squashing. RUM is theoretically shown to be more expressive than 1-WL and to attenuate over-smoothing by maintaining non-diminishing Dirichlet energy, and it alleviates over-squashing by improving gradient flow. Similarly, the FROND framework [kang2024fsk] introduces fractional calculus to continuous GNNs, enabling the capture of non-local, memory-dependent dynamics and inherently mitigating over-smoothing through algebraic convergence. Despite these innovations, the trade-off between achieving higher expressiveness and maintaining computational efficiency and robustness remains a critical challenge. The theoretical gaps persist in developing universally applicable architectures that can overcome these fundamental limitations without introducing new complexities or relying on strong assumptions.

\subsection*{6.3. Generalization to Unseen Structures and Distribution Shifts}
A crucial challenge for the real-world deployment of GNNs is their ability to generalize effectively to unseen graph structures and to maintain performance under distribution shifts. Unlike grid-structured data (e.g., images), graphs can exhibit vast topological diversity, making inductive generalization particularly difficult. A GNN trained on one set of graphs might perform poorly on graphs with different statistical properties, node feature distributions, or underlying generative mechanisms. This is especially pertinent in applications where the target graph distribution might evolve or differ significantly from the training data, such as in anomaly detection [tang2022g66, chai2022nf9] or fraud detection [duan2024que].

The problem of distribution shift is exacerbated in dynamic graph settings, where the evolving nature of the graph can lead to spatio-temporal distribution shifts that degrade model performance [zhang2022uih]. For instance, a GNN trained for traffic prediction in one city might fail in another with different road network topology or traffic patterns. Pre-training strategies [hu2019r47, hu2020u8o] and self-supervised learning techniques [xie2021n52, fatemi2021dmb] have emerged as promising avenues to learn more generalizable graph representations. Methods like GPPT [sun2022d18] and GraphPrompt [liu2023ent] leverage pre-training and prompt tuning to enhance generalization capabilities, aiming to adapt models to new tasks or domains with minimal fine-tuning. Learning invariant representations, for example, via cluster generalization [xia20247w9], also seeks to make GNNs more robust to variations in graph structure.

However, the theoretical understanding of generalization in GNNs is still nascent [jegelka20222lq, ju2023prm]. Current PAC-Bayesian bounds, while providing theoretical guarantees, often rely on assumptions about graph diffusion processes that may not hold in complex real-world scenarios [ju2023prm]. The "arms race" dynamic here involves developing increasingly complex models that risk overfitting to specific graph structures versus designing simpler, more robust architectures that might sacrifice some expressiveness. Furthermore, identifying and mitigating the impact of distribution shifts requires robust causal inference frameworks that can disentangle spurious correlations from true causal relationships, a challenging task in graph data. The ability of GNNs to perform out-of-distribution detection [wu2023303] is also critical, allowing models to signal uncertainty when encountering novel graph patterns, thereby enhancing trustworthiness.

\subsection*{6.4. The Need for Robust Evaluation and Benchmarking}
The rapid proliferation of GNN models has unfortunately been accompanied by inconsistencies and pitfalls in evaluation methodologies, hindering fair comparisons and obscuring genuine progress. A critical analysis by Li et al. [li2023o4c] highlights several key issues in link prediction evaluation: underreported performance of existing baselines due to suboptimal hyperparameter tuning, a lack of unified data splits and evaluation metrics, and the use of unrealistic "easy" negative samples that do not reflect real-world challenges. This leads to an inflated sense of advancement, where new models might appear superior simply because they are compared against weakly tuned baselines or on simplified tasks.

The absence of standardized and challenging benchmarking frameworks makes it difficult to assess the true capabilities and limitations of novel GNN architectures. While efforts like Benchmarking Graph Neural Networks [dwivedi20239ab] and domain-specific benchmarks such as BrainGB for brain network analysis [cui2022mjr] and PowerGraph for power grids [varbella20242iz] are crucial, their widespread adoption and continuous maintenance are essential. The problem of "easy" negative samples, as identified by [li2023o4c], is particularly acute in link prediction, where randomly sampled non-existent links often lack common neighbors, making them trivial to distinguish from positive links. To address this, Li et al. [li2023o4c] propose the Heuristic Related Sampling Technique (HeaRT) to generate harder, more realistic negative samples, thereby providing a more robust evaluation setting.

Beyond link prediction, similar issues plague other GNN tasks. For instance, in explainability, defining and evaluating the "ground truth" for explanations remains an open problem, leading to subjective assessments of interpretability [agarwal2022xfp, chen2024woq]. In robustness, the effectiveness of adversarial defenses for GNNs is often evaluated under specific attack models, and their generalizability to unseen or adaptive attacks is questionable [mujkanovic20238fi]. The theoretical gaps in evaluation lie in developing universally accepted metrics that are robust to dataset variations and in designing adversarial benchmarks that truly test the limits of GNN performance under realistic conditions. The continuous need for benchmarks under specific challenging conditions, such as label noise [wang2024481], underscores the ongoing demand for rigorous and standardized evaluation to ensure that reported progress is reliable and impactful.

### 7. Applications and Real-World Impact

\section*{7. Applications and Real-World Impact}
The transformative potential of Graph Neural Networks (GNNs) extends far beyond theoretical advancements, manifesting in profound and diverse real-world impacts across a multitude of domains. GNNs excel at modeling complex, interconnected data, making them uniquely suited for tasks where relationships and structural dependencies are paramount [wu2022ptq, khemani2024i8r, zhou20188n6]. Their ability to learn representations by aggregating information from local neighborhoods, and increasingly from global graph structures, has enabled breakthroughs in areas previously challenging for traditional machine learning methods. This section delineates the broad utility of GNNs, showcasing their versatility in handling intricate data patterns from social interactions to molecular structures. We will explore their successful deployment in enhancing recommender systems, accelerating scientific discovery, enabling predictive learning in dynamic urban environments, and fortifying cybersecurity defenses, among other emerging applications. A critical examination of these applications reveals not only the strengths of GNNs but also the persistent challenges and trade-offs inherent in deploying them in complex, real-world settings, such as balancing expressiveness with scalability or ensuring robustness against adversarial manipulations. The continuous evolution of GNN architectures, including non-convolutional approaches like RUM [wang2024oi8] and fractional-order models like FROND [kang2024fsk], further expands their applicability and addresses previous limitations, paving the way for even broader impact.

\subsection*{7.1. GNNs in Recommender Systems}
Recommender systems are a quintessential application area for GNNs, where the goal is to predict user preferences for items by modeling intricate user-item interactions and leveraging auxiliary information like social networks or item attributes [gao2022f3h, wu2020dc8]. Traditional collaborative filtering methods often struggle with sparsity and cold-start problems, and their ability to capture high-order relationships is limited. GNNs naturally represent user-item interactions as a bipartite graph, allowing for the propagation of preferences and the discovery of latent patterns through message passing [ying20189jc, fan2019k6u].

Early GNN-based recommender systems, such as PinSage [ying20189jc], demonstrated significant improvements by learning embeddings for millions of items and users on large-scale graphs. Subsequent advancements have focused on modeling various aspects of recommendation, including session-based recommendations where user behavior sequences are captured as dynamic graphs [wu2018t43, zhang20212ke, zhang2022atq], and incorporating knowledge graphs to enrich item representations with semantic information [wang2019vol, lyu2023ao0]. The challenge of capturing complex, multi-relational interactions has led to the development of heterogeneous GNNs [wang2019vol] and attention mechanisms [yu2020u32, wang2020khd] to differentiate the importance of various neighbors or relationship types. For instance, [chang2021yyt] and [chang2023ex5] explore GNNs for sequential and bundle recommendations, respectively, highlighting their capacity to model complex dependencies beyond simple pairwise interactions.

Despite their success, GNNs in recommender systems face significant challenges. Scalability to billions of users and items remains a major hurdle, often requiring sampling strategies or distributed training [chen2024gbe, vasimuddin2021x7c]. The inherent homophily assumption of many GNNs can also be problematic in recommendation, as users might interact with diverse items or connect with dissimilar individuals (heterophily) [ma2021sim, li2022315]. While models like GloGNN [li2022315] offer solutions for heterophilous graphs by learning global homophily with linear time complexity, their integration into large-scale industrial recommender systems is still an active research area. Furthermore, the interpretability of GNN recommendations is crucial for user trust and system debugging [lyu2023ao0], yet explaining complex graph-based decisions remains an open problem. The need for robust evaluation practices, as highlighted by [li2023o4c] for link prediction, is equally critical in recommender systems to ensure fair comparisons and reliable progress.

\subsection*{7.2. Scientific Domains: Molecules, Materials, and Brain Networks}
GNNs have emerged as powerful tools in scientific discovery, particularly in fields dealing with intrinsically graph-structured data such as molecular science, materials discovery, and neuroscience. In **molecular science**, GNNs are revolutionizing drug discovery and chemical property prediction. Molecules are naturally represented as graphs, with atoms as nodes and chemical bonds as edges. GNNs can learn intricate molecular fingerprints, predict properties like toxicity or solubility, and even assist in de novo drug design [jiang2020gaq, carlo2024a3g, yao2024pyk]. Architectures like GemNet [klicpera20215fk] and E(3)-equivariant GNNs [satorras2021pzl, batzner2021t07] are designed to respect the physical symmetries of molecules, leading to more accurate and data-efficient predictions of interatomic potentials and binding affinities [li2021v1l, smith2024q8n]. However, the expressiveness limitations of standard GNNs can hinder their ability to distinguish complex isomers or capture subtle quantum mechanical effects, necessitating more powerful architectures like PathNNs [michel2023hc4] or those leveraging fractional calculus [kang2024fsk]. Pre-training GNNs on large molecular datasets, as explored by Mole-BERT [xia2023bpu], is also a promising direction to enhance generalization.

Similarly, in **materials discovery**, GNNs are accelerating the design of novel materials with desired properties. Crystal structures, amorphous materials, and alloys can be modeled as graphs, allowing GNNs to predict mechanical, electronic, or thermal properties [reiser2022b08, fung20212kw, maurizi202293p]. They can predict defect formation energies [fang2024zd6] or even model collective variables for molecular dynamics simulations [zhang202483k]. The challenge here lies in handling diverse material compositions and complex interatomic interactions, often requiring equivariant GNNs to maintain physical consistency [batzner2021t07]. The integration of GNNs with large language models (LLMs) is also emerging as a hybrid approach to leverage both structural and textual knowledge for materials property prediction [li2024gue].

In **brain network analysis**, GNNs offer a novel paradigm for understanding neurological disorders and cognitive functions. The human brain can be represented as a complex network (connectome), where nodes are brain regions and edges represent structural or functional connections [bessadok2021bfy, mohammadi202476q]. GNNs can analyze these networks to classify brain disorders like Alzheimer's disease [cui2022pap, abuhantash202458c, abadal2024w7e], predict disease progression, or identify biomarkers [zhao2022fvg, luo2024h2k]. The unique challenges in this domain include the small sample sizes of medical datasets, the inherent noise in neuroimaging data, and the need for interpretable models to provide clinical insights [cui2022pap]. Benchmarks like BrainGB [cui2022mjr] are crucial for standardizing evaluation and fostering robust model development. The application of GNNs in this field often requires careful consideration of heterophily, as functionally distinct brain regions might be connected, and the dynamic nature of brain activity necessitates spatio-temporal GNNs [tang2021h2z].

\subsection*{7.3. Urban Computing, Time Series, and Epidemic Modeling}
GNNs are increasingly vital in **urban computing**, where they tackle complex spatio-temporal prediction tasks essential for smart cities [jin2023e18, rahmani2023kh4]. A prime example is traffic flow forecasting, where road networks form natural graphs, and traffic conditions evolve dynamically [li2020fil, wu2020hi3, zhou2024t2r]. GNNs can capture both the spatial dependencies (e.g., how traffic in one road segment affects adjacent segments) and temporal patterns (e.g., daily commutes, rush hour effects) to provide accurate predictions. Google Maps, for instance, employs GNNs for ETA prediction, demonstrating their real-world impact on navigation and logistics [derrowpinion2021mwn]. However, these applications are highly susceptible to spatio-temporal distribution shifts [zhang2022uih], where models trained on one city or time period may not generalize well to others, necessitating robust generalization techniques.

The broader field of **time series analysis** also benefits significantly from GNNs, particularly for multivariate time series where inter-series dependencies can be modeled as graphs [jin2023ijy, wu2020hi3]. GNNs are applied to forecasting, classification, imputation (e.g., filling missing data in sensor networks [cini20213l6, jing2024az0]), and anomaly detection in diverse domains, from financial markets [foroutan2024nhg] to industrial IoT [wu20210h4]. The dynamic nature of these relationships often requires GNNs capable of handling temporal graphs [longa202399q, cini2022pjy]. The challenge lies in efficiently learning evolving graph structures and long-range temporal dependencies, which can be addressed by dynamic GNNs or by integrating concepts from fractional calculus to capture memory effects, as proposed by FROND [kang2024fsk].

In the critical area of **epidemic modeling**, GNNs offer powerful tools for predictive learning in dynamic environments. Infectious disease spread can be modeled as a diffusion process over contact networks, making GNNs well-suited for forecasting disease incidence, identifying high-risk areas, and evaluating intervention strategies [liu20242g6]. Causal-based GNNs, such as CausalGNN [wang202201n], explicitly model causal relationships in spatio-temporal data to improve epidemic forecasting, demonstrating the capacity of GNNs to move beyond correlation to causality. The dynamic and often uncertain nature of epidemic data, however, poses challenges for model robustness and uncertainty quantification [huang2023fk1], requiring GNNs that can adapt to rapid changes and provide reliable confidence estimates.

\subsection*{7.4. GNNs in Cybersecurity and Other Emerging Fields}
The growing complexity of cyber threats and interconnected systems has made **cybersecurity** a fertile ground for GNN applications. Networks, system logs, and user behaviors can all be represented as graphs, allowing GNNs to detect anomalies, identify vulnerabilities, and predict attacks [mitra2024x43, bilot20234ui]. For instance, GNNs are used for vulnerability detection in software code by learning program semantics from abstract syntax trees or control flow graphs [zhou20195xo, liu2021qyl, nguyen2021g12, hin2022g19]. They can also classify encrypted network traffic for malicious activity [shen2021sbk, huoh2023i97] or detect financial fraud by analyzing transaction networks [innan2023fa7, duan2024que, zandi2024dgs, liu2024sbb]. The challenge in cybersecurity lies in the adversarial nature of the domain, where attackers can actively try to evade detection by manipulating graph structures or features, necessitating robust and explainable GNNs [mujkanovic20238fi, wang2024p88, xia2024xc9, li2024r82]. The need for explainability is particularly acute in this field, as security analysts require insights into *why* a particular alert was triggered [yuan2021pgd].

Beyond these major areas, GNNs are making inroads into numerous **other emerging fields**:
\begin{itemize}
    \item \textbf{Internet of Things (IoT)}: GNNs analyze sensor networks for anomaly detection, resource allocation, and predictive maintenance [dong20225aw, wu20210h4].
    \item \textbf{Power Systems}: They are used for grid stability analysis, fault detection, and optimal power flow, leveraging the graph structure of power networks [liao202120x, varbella20242iz, zhang2024ctj].
    \item \textbf{Wireless Communications}: GNNs optimize resource management, interference mitigation, and network configuration in complex wireless environments [shen202037i, shen2022gcz, guo2022hu1, abode2024m4z, guo2024zoe].
    \item \textbf{Combinatorial Optimization}: GNNs are being explored to learn heuristics or even directly solve NP-hard combinatorial problems, bridging machine learning with operations research [cappart2021xrp, schuetz2021cod].
    \item \textbf{Natural Language Processing (NLP)}: While often dominated by Transformers, GNNs are used for text classification, relation extraction, and knowledge graph completion by modeling linguistic dependencies [wang2023wrg, zhang2020tdy, wang2020nbg, wu2023zm5, li202444f, wang2024nuq].
    \item \textbf{Computer Vision}: GNNs are increasingly integrated into computer vision tasks, from object detection and multi-object tracking [wang2021mxw] to point cloud processing [li2024yyl] and scene graph generation [chen2022mmu].
    \item \textbf{Healthcare and Medical Decision Making}: Beyond brain networks, GNNs are used for drug-drug interaction prediction [gnanabaskaran20245dg], resource allocation [manivannan2024830], and customized medical decision algorithms [yan2024ikq].
    \item \textbf{Earth Observation}: GNNs are being explored for wildfire danger prediction [zhao2024e2x] and other complex spatio-temporal analyses of satellite data [zhao2024g7h].
\end{itemize}
The pervasive nature of graph-structured data in these diverse fields underscores the broad utility and profound impact of GNNs. The continuous development of more expressive, scalable, and robust GNN architectures will undoubtedly unlock further applications and drive innovation across science, industry, and society.

### 8. Conclusion: Future Directions and Ethical Considerations

\section*{8. Conclusion: Future Directions and Ethical Considerations}
The journey of Graph Neural Networks (GNNs) has been marked by rapid innovation, transforming our ability to model and derive insights from complex, interconnected data. From their foundational roots in spectral graph theory and message passing [zhou20188n6, wu20193b0] to sophisticated architectures capable of capturing intricate relational patterns, GNNs have demonstrated unparalleled utility across diverse domains, as evidenced in recommender systems [gao2022f3h], scientific discovery [reiser2022b08], and cybersecurity [mitra2024x43]. This concluding section synthesizes the current state of GNN research, highlighting both the remarkable progress and the persistent challenges. It casts a forward-looking perspective on the future trajectory of GNNs, identifying emerging trends and novel paradigms that promise to push the boundaries of their capabilities. Crucially, it emphasizes the ongoing imperative to bridge theoretical advancements with practical deployment, addressing persistent challenges like scalability, generalization, and robustness. Finally, it reiterates the critical ethical considerations inherent in developing powerful AI systems, underscoring the need for responsible AI that is not only effective but also fair, transparent, and privacy-preserving, thereby guiding future research towards impactful and ethical innovation for societal benefit.

\subsection*{8.1. Emerging Trends and Novel Paradigms}
The field of GNNs is continuously evolving, driven by the need to overcome inherent limitations and adapt to increasingly complex data landscapes. One significant emerging trend is the **multi-modal integration with large language models (LLMs)**. While GNNs excel at structural reasoning, LLMs provide powerful semantic understanding. Hybrid architectures that combine these strengths are beginning to surface, for instance, in materials property prediction where `Hybrid-LLM-GNN` leverages both structural graph data and textual descriptions [li2024gue]. Similarly, research explores how GNNs can learn language with extremely weak text supervision, hinting at a synergistic future where GNNs and LLMs mutually enhance each other's capabilities in understanding complex, multi-faceted information [li202444f]. This integration promises to unlock new levels of intelligence by enabling GNNs to reason over both explicit graph structures and implicit knowledge embedded in text, addressing the limitation of GNNs often being purely structure-driven.

Another novel paradigm involves the application of **advanced mathematical frameworks**, notably fractional calculus, to GNN design. Traditional continuous GNNs, which model node feature evolution using integer-order differential equations, inherently assume Markovian dynamics, limiting their ability to capture long-term dependencies and memory effects [kang2024fsk]. The FRactional-Order graph Neural Dynamical network (FROND) framework [kang2024fsk] addresses this by employing Caputo fractional derivatives. This generalization allows GNNs to model non-local, memory-dependent dynamics, which are prevalent in real-world graphs exhibiting fractal structures or anomalous transport. Theoretically, FROND's non-Markovian random walk interpretation also leads to a slower, algebraic rate of convergence to stationarity, thereby inherently mitigating the pervasive oversmoothing problem [rusch2023xev, chen2019s47] that plagues deep GNNs. This represents a fundamental shift from local, instantaneous updates to a more holistic, history-aware information propagation mechanism.

Furthermore, the development of **non-convolutional GNN architectures** marks a significant departure from the prevalent message-passing paradigm. The Random Walk with Unifying Memory (RUM) neural network [wang2024oi8] is a prime example, entirely foregoing convolution operators. Instead, it leverages stochastic random walks and recurrent neural networks (RNNs) to process both semantic and topological features. This approach directly addresses several fundamental limitations of convolution-based GNNs: limited expressiveness (surpassing the Weisfeiler-Lehman test [xu2018c8q, morris20185sd]), over-smoothing, and over-squashing [alon2020fok]. RUM's ability to distinguish non-isomorphic graphs that 1-WL equivalent GNNs cannot, coupled with its theoretical and empirical demonstration of attenuating over-smoothing and over-squashing, positions it as a powerful alternative, especially for tasks requiring deep GNNs or capturing long-range dependencies. Similarly, Path Neural Networks (PathNNs) [michel2023hc4] enhance expressiveness by explicitly aggregating information from various paths, demonstrating the capacity to distinguish graphs indistinguishable by even the 3-WL algorithm through their "annotated sets of paths." While computationally intensive for all simple paths, the theoretical advancements highlight the potential of path-centric approaches to overcome the expressiveness bottleneck. These novel paradigms collectively represent a concerted effort to build more powerful, flexible, and theoretically grounded GNNs that can better model the inherent complexities of real-world graph data.

\subsection*{8.2. Bridging Theory and Practice}
Despite significant theoretical advancements and promising empirical results, a persistent challenge in GNN research lies in **bridging the gap between theoretical capabilities and practical deployment**. This involves addressing critical issues such as scalability, generalization, and robustness, which are paramount for real-world impact.

**Scalability** remains a bottleneck for GNNs, particularly when dealing with massive graphs containing billions of nodes and edges, common in industrial applications like recommender systems [chen2024gbe]. While approaches like `SIGN` [rossi2020otv], `GNNAutoScale` [fey2021smn], and `DistGNN` [vasimuddin2021x7c] offer solutions through sampling, distributed training, or approximate aggregation, they often introduce trade-offs with model expressiveness or accuracy. For instance, `GloGNN` [li2022315] addresses heterophily by performing global aggregation but achieves linear time complexity through clever matrix reordering, demonstrating that efficient, theoretically sound solutions for large-scale graphs are possible. However, the computational cost of exploring complex graph structures, such as all simple paths in `PathNNs` [michel2023hc4], still limits the practical depth or breadth of information aggregation. The non-convolutional nature of `RUM` [wang2024oi8], with its runtime complexity agnostic to the number of edges, offers a promising direction for inherent scalability.

**Generalization** is another critical area, ensuring that GNNs trained on one dataset or domain perform well on unseen data or different distributions. Techniques like graph pre-training and prompt tuning (`GPPT` [sun2022d18]), self-supervised learning [xie2021n52], and learning invariant representations via cluster generalization [xia20247w9] are actively being explored. However, dynamic graph neural networks often struggle under spatio-temporal distribution shifts [zhang2022uih], highlighting the need for more adaptive and robust learning paradigms. Theoretical guarantees on generalization, such as improved PAC-Bayesian bounds on graph diffusion [ju2023prm], are crucial for building more reliable models.

**Robustness** against adversarial attacks and noisy data is paramount, especially in sensitive applications like cybersecurity [mitra2024x43] or fraud detection [duan2024que]. GNNs are vulnerable to various attacks, including data poisoning and evasion attacks [zhang2020b0m, zou2021qkz, dai2023tuj], which can significantly degrade their performance [mujkanovic20238fi]. Defenses range from adversarial training [gosch20237yi] and robust aggregation schemes [zhang2020jrt] to graph structure learning for robust GNNs [jin2020dh4] and certified robustness methods [xia2024xc9]. The challenge lies in developing defenses that are effective, scalable, and do not compromise model utility. Recent explorations into whether large language models can improve the adversarial robustness of GNNs [zhang2024370] suggest a multi-faceted approach to this "arms race" dynamic.

Finally, the very foundation of GNN evaluation needs critical re-examination. As highlighted by `Li et al.` [li2023o4c], current benchmarking practices for link prediction suffer from pitfalls like underreported baselines, inconsistent data splits, and unrealistic negative sampling. Their proposed `HeaRT` technique for generating hard, heuristic-related negative samples underscores the importance of rigorous and realistic evaluation to accurately assess model performance and drive meaningful progress. Without robust evaluation, the true capabilities and limitations of GNNs remain obscured, hindering the effective translation of theoretical gains into practical, deployable solutions.

\subsection*{8.3. Ethical Considerations and Responsible AI Development}
As GNNs become increasingly powerful and pervasive, the ethical implications of their deployment demand rigorous attention. Developing **responsible AI** is not merely a technical challenge but a societal imperative, ensuring that GNNs are not only powerful but also fair, transparent, and privacy-preserving.

**Fairness** is a critical concern, especially when GNNs are applied in high-stakes domains like credit risk assessment [liu2024sbb], social recommendation [fan2019k6u], or healthcare [yan2024ikq]. GNNs can inadvertently perpetuate or even amplify existing biases present in graph data, leading to discriminatory outcomes. Research efforts focus on identifying and mitigating data bias [dong2021qcg], ensuring individual fairness [dong202183w], and developing fair GNNs that account for sensitive attribute leakage [dai2020p5t, wang2022531]. Approaches like learning disentangled causal substructures for debiasing [fan2022m67] or re-balancing techniques [li20245zy] aim to build GNNs that make equitable decisions. The `FUGNN` framework [luo20240ot] explicitly seeks to harmonize fairness and utility, acknowledging the inherent trade-offs that often exist.

**Transparency and Explainability** are crucial for building trust and enabling accountability, particularly in black-box GNN models. Users and stakeholders need to understand *why* a GNN made a particular prediction or recommendation. While methods like `GNNExplainer` [ying2019rza], `PGM-Explainer` [vu2020zkj], and `XGNN` [yuan20208v3] provide insights into node, edge, or feature importance, the concept of explaining GNN predictions via **subgraph explorations** is gaining traction. `SubgraphX` [yuan2021pgd] directly identifies important connected subgraphs using Monte Carlo Tree Search and Shapley values, offering more intuitive and human-intelligible explanations by capturing structural interactions. Other works explore invariant rationales [wu2022vcx] or global interactive patterns [wang2024j6z] to enhance interpretability. However, evaluating the quality and faithfulness of these explanations remains an active research area [agarwal2022xfp, chen2024woq], with ongoing debates about how interpretable "interpretable" GNNs truly are.

**Privacy** is another paramount concern. Graph data often contains sensitive personal information, and GNNs can be vulnerable to privacy attacks, such as link stealing [he2020kz4] or reconstruction attacks. The threat of backdoor attacks [zhang2020b0m, dai2023tuj] further complicates privacy, as malicious actors could embed hidden triggers to manipulate GNN behavior. Federated Graph Neural Networks [he2021x8v, liu2022gcg] offer a promising direction for privacy-preserving GNN training by keeping data localized, but they introduce their own challenges in terms of model aggregation and communication overhead.

Ultimately, the development of **trustworthy GNNs** encompasses these ethical considerations, alongside robustness and confidence calibration [wang20214ku, zhang20222g3, dai2022hsi]. Future research must move beyond optimizing for raw performance metrics and actively integrate principles of fairness, transparency, and privacy into the core design and evaluation of GNN architectures. This holistic approach will ensure that GNNs contribute positively to societal benefit, fostering innovation that is not only powerful but also responsible and aligned with human values.

