\section*{3. Enhancing Expressiveness and Overcoming Early Limitations}
The initial success of Graph Neural Networks (GNNs) in extending deep learning to graph-structured data was quickly met with the identification of fundamental limitations that hindered their representational power, robustness, and applicability to real-world complexities. Early GNNs, largely constrained by the expressive power of the 1-Weisfeiler-Leman (1-WL) test, struggled to distinguish between many non-isomorphic graphs, thereby limiting their ability to capture intricate structural patterns \cite{xu2018c8q, morris20185sd}. Furthermore, the iterative message-passing paradigm, while intuitive, introduced pathologies such as over-smoothing, where node representations become indistinguishable in deep models, and over-squashing, which impedes the propagation of long-range dependencies \cite{rusch2023xev, alon2020fok}. Critically, the implicit homophily assumption, prevalent in many early designs, rendered these models ineffective on heterophilous graphs, where connected nodes often exhibit dissimilar features or labels \cite{ma2021sim, zhu2020c3j}.

This section delves into the crucial advancements that propelled GNNs beyond these early constraints, marking a significant evolutionary phase in the field. It explores the development of sophisticated message-passing mechanisms and higher-order architectures designed to enhance representational power, moving beyond the 1-WL bottleneck. A substantial focus is placed on the innovative strategies devised to mitigate the over-smoothing and over-squashing problems, enabling the construction of deeper, more effective GNNs. Finally, it examines the critical challenge of adapting GNNs to diverse graph structures, particularly those exhibiting heterophily, thereby broadening their applicability to the complex and varied topologies found in real-world datasets \cite{wu2022ptq, zhang20222g3, wang2023zr0}. These advancements collectively represent an "arms race" to build more robust, expressive, and versatile GNNs capable of tackling the full spectrum of graph learning tasks.

\subsection*{3.1. Higher-Order and Advanced Message Passing Mechanisms}
The inherent limitation of many standard message-passing GNNs to the expressive power of the 1-Weisfeiler-Leman (1-WL) test became a significant theoretical bottleneck, preventing them from distinguishing common non-isomorphic graphs and capturing complex substructures \cite{xu2018c8q, morris20185sd}. This limitation spurred an intense research effort to develop higher-order and more advanced message-passing mechanisms that could surpass the 1-WL boundary.

One prominent direction involves explicitly leveraging path information within the graph. Path Neural Networks (PathNNs) \cite{michel2023hc4} represent a notable advancement in this area. Unlike traditional GNNs that aggregate information from direct neighbors, PathNNs encode and aggregate information from various paths emanating from each node. Their core innovation lies in the use of "annotated sets of paths," where nodes within a path are recursively annotated with hashes of their shorter path sets. This hierarchical structural encoding allows PathNNs to achieve significantly higher expressive power. Specifically, variants like `ËœAP` (All Simple Paths with annotations) have been theoretically and empirically demonstrated to distinguish graphs that are even 3-WL indistinguishable, a substantial leap beyond the 1-WL capabilities of most GNNs \cite{michel2023hc4}. However, this enhanced expressiveness comes with a practical trade-off: finding all simple paths is an NP-hard problem, necessitating approximations such as fixing a maximum path length $K$, which can limit the model's ability to capture extremely long-range dependencies or scale to very dense graphs. The computational complexity of path enumeration remains a significant practical constraint for these highly expressive models.

A distinct, yet equally impactful, approach to enhancing expressiveness involves fundamentally rethinking the message-passing paradigm itself. The Random Walk with Unifying Memory (RUM) neural network \cite{wang2024oi8} proposes an entirely non-convolutional GNN architecture. Instead of iterative neighborhood aggregation, RUM stochastically samples finite-length random walks for each node. It then employs an RNN (specifically, GRU) to merge "semantic representations" (node embeddings along the walk) with "topological environments" (captured by a novel "anonymous experiment" function that labels nodes based on their unique traversal order). This departure from the standard convolutional aggregation allows RUM to overcome the 1-WL test's limitations, demonstrating the ability to distinguish non-isomorphic graphs that differ in properties like cycle sizes or radius, which are problematic for WL-equivalent GNNs \cite{wang2024oi8}. The theoretical underpinnings suggest that RUM's expressiveness stems from its ability to process sequences of node features and their unique topological context along walks, rather than relying solely on local multiset aggregation. While RUM offers a compelling alternative, its theoretical proofs for expressiveness rely on assumptions about the universality and injectivity of its internal functions, and its reliance on RNNs for walk processing introduces a different set of computational considerations compared to simple aggregators.

These advancements highlight a critical evolutionary trend: moving beyond mere aggregation of neighbor features to incorporate richer structural contexts, whether through explicit path enumeration or dynamic random walk explorations. The trade-off between theoretical expressiveness and practical computational feasibility remains a central challenge, pushing researchers to develop more efficient approximation schemes and novel architectural designs that can balance these competing demands \cite{chen2022mmu, dwivedi20239ab}.

\subsection*{3.2. Tackling Over-smoothing and Enabling Deep GNNs}
The promise of deep learning lies in its ability to learn hierarchical representations through many layers. However, for GNNs, increasing depth often leads to a severe degradation in performance due to the over-smoothing problem \cite{rusch2023xev, cai2020k4b, chen2019s47, oono2019usb}. Over-smoothing occurs when repeated aggregation of neighbor features causes node representations to converge to a single point in the embedding space, making them indistinguishable and losing their discriminative power. This phenomenon is particularly acute in dense graphs and effectively limits the practical depth of GNNs to a few layers. Closely related is the over-squashing problem, an information bottleneck where the fixed-size messages in each layer struggle to compress and propagate information from an exponentially growing receptive field, leading to a loss of crucial long-range dependencies \cite{alon2020fok}.

To overcome these fundamental barriers, researchers have explored diverse strategies. One line of work focuses on architectural modifications inspired by deep learning in other domains, such as incorporating residual connections (e.g., GCNII \cite{chen2020simple}) or skip connections \cite{li2021orq, liu2020w3t, zeng2022jhz}. These mechanisms help preserve initial node features and facilitate gradient flow, allowing for deeper architectures. Regularization techniques like DropEdge, which randomly removes edges during training, also help by preventing over-reliance on local neighborhoods and diversifying information paths \cite{rong2019dropedge}.

More fundamentally, novel propagation mechanisms have been proposed to inherently mitigate over-smoothing. The Random Walk with Unifying Memory (RUM) network \cite{wang2024oi8}, by adopting a non-convolutional, random walk-based approach, offers a joint remedy to over-smoothing and over-squashing. Unlike convolutional GNNs where the expected Dirichlet energy (a measure of smoothness) diminishes with depth, RUM's expected Dirichlet energy is theoretically shown to not diminish even with long walks. This implies that node representations do not necessarily converge to a single point, thus attenuating over-smoothing. Furthermore, RUM is shown to decay slower in inter-node Jacobian compared to convolutional counterparts, which helps mitigate the over-squashing problem by improving gradient flow for long-range dependencies \cite{wang2024oi8}. This approach suggests that by moving away from local, iterative averaging, GNNs can maintain distinct node identities while integrating global context.

Another innovative direction involves integrating fractional calculus into continuous GNNs. The FRactional-Order graph Neural Dynamical network (FROND) framework \cite{kang2024fsk} replaces integer-order differential operators with Caputo fractional derivatives. Traditional continuous GNNs, based on integer-order ODEs, model instantaneous, Markovian updates, leading to exponential convergence to stationarity and thus over-smoothing. FROND, by contrast, introduces non-local, memory-dependent dynamics, where the entire historical trajectory of node features influences their update. Theoretically, FROND's non-Markovian random walk interpretation leads to an algebraic (slower) rate of convergence to stationarity, inherently mitigating over-smoothing \cite{kang2024fsk}. This framework generalizes existing continuous GNNs and has been shown to consistently improve performance by capturing complex, memory-dependent graph dynamics. The use of numerical FDE solvers, however, introduces computational considerations that need careful management. Both RUM and FROND exemplify a shift towards more sophisticated, theoretically grounded propagation models that move beyond simple local averaging, enabling the development of truly deep and effective GNNs.

\subsection*{3.3. Adapting to Diverse Graph Structures and Heterophily}
A foundational assumption in many early GNNs is homophily, which posits that connected nodes tend to share similar features or labels \cite{ma2021sim}. While this holds true for many social and citation networks, real-world graphs often exhibit heterophily, where connected nodes are dissimilar \cite{zhu2020c3j, zheng2022qxr}. Traditional message-passing GNNs, by aggressively smoothing features from neighbors, perform poorly on heterophilous graphs because aggregating dissimilar information can lead to noisy or misleading node representations. Adapting GNNs to these diverse and complex graph structures, particularly those with strong heterophily, has become a critical challenge.

Initial attempts to address heterophily often involved modifying the aggregation scheme or expanding the receptive field. Some methods leverage both low-pass (smoothing) and high-pass (emphasizing differences) filters, or dynamically adjust aggregation weights based on node similarity \cite{gprgnn}. Others enlarge the node neighborhood to include multi-hop neighbors, hoping to find homophilous nodes further away (e.g., H2GCN, WRGAT). However, these approaches face limitations: determining optimal personalized neighborhood sizes is difficult, and they may still miss globally distant but homophilous nodes, while a naive global aggregation would be computationally prohibitive \cite{li2022315}.

The GloGNN and GloGNN++ models \cite{li2022315} offer a significant advancement by performing node neighborhood aggregation from the *whole set of nodes* in the graph, rather than just local or multi-hop neighbors. This allows them to capture "global homophily" that might exist between distant nodes. The core innovation lies in learning a **signed coefficient matrix Z(l)** for each layer, where Z(l)ij quantifies the importance of node $j$ to node $i$. Crucially, these coefficients can be positive (for homophilous connections) or negative (for heterophilous ones), effectively combining low-pass and high-pass filtering within a single aggregation step. This learned matrix is derived from an optimization problem and regularized by multi-hop reachabilities, incorporating both feature and topological similarity. A key strength of GloGNN is its computational efficiency: despite aggregating globally, the model transforms the aggregation equation to avoid direct computation of the dense Z(l) matrix, achieving *linear time complexity* (O(k2n)) by leveraging matrix properties and the number of labels, thereby making global aggregation feasible for large graphs \cite{li2022315}. Theoretically, GloGNN proves a "grouping effect," where nodes with similar features and local structures (even if distant) will have similar coefficient and representation vectors, explaining its effectiveness.

The development of models like GloGNN highlights an important shift from assuming local homophily to actively learning and leveraging global relationships, even in the presence of heterophily. This move requires not only sophisticated aggregation mechanisms but also efficient computational strategies to handle the increased scope of information integration. The challenge of adapting GNNs to diverse graph structures extends beyond heterophily to include issues like structural disparity \cite{mao202313j}, where different nodes may require different aggregation strategies, and the integration of heterogeneous information networks \cite{lv20219al, wei20246l2}. These advancements collectively underscore the ongoing effort to build GNNs that are robust and flexible enough to model the intricate complexities of real-world graph data, moving beyond simplified assumptions to capture the full spectrum of relational patterns.