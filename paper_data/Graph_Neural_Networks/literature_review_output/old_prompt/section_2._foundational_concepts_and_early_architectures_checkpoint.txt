\section{Foundational Concepts and Early Architectures}
The journey of Graph Neural Networks (GNNs) began with the fundamental challenge of extending deep learning paradigms, which primarily operate on Euclidean data structures like images and sequences, to the inherently non-Euclidean domain of graphs. This section delves into the initial theoretical underpinnings and architectural designs that laid the groundwork for modern GNNs, focusing on the core principles of graph representation learning, the ubiquitous message-passing paradigm, and the critical benchmark of theoretical expressiveness provided by the Weisfeiler-Leman (WL) graph isomorphism test. Understanding these foundational concepts is crucial, as the inherent limitations of early models, particularly concerning their discriminative power and propagation mechanisms, directly motivated the subsequent wave of architectural innovations and theoretical advancements. This period established the initial capabilities of GNNs while simultaneously revealing the deep theoretical and practical challenges that continue to drive research in the field, such as over-smoothing, over-squashing, and the struggle with heterophilous graphs.

\subsection{Graph Representation Learning Fundamentals}
Graph representation learning is concerned with mapping nodes, edges, or entire graphs into low-dimensional vector spaces, often referred to as embeddings, such that the structural and feature information of the original graph is preserved \cite{wu20193b0, zhou20188n6}. The primary motivation for this transformation is to enable traditional machine learning algorithms, which typically operate on vector inputs, to process and learn from complex graph-structured data. Unlike grid-like data, graphs possess irregular topologies, varying neighborhood sizes, and inherent relational information, making direct application of standard convolutional or recurrent neural networks challenging.

Early approaches to graph representation learning largely relied on hand-crafted features (e.g., node degrees, clustering coefficients) or graph kernel methods \cite{garg2020z6o}. While foundational, these methods often lacked scalability, generality, and the ability to learn hierarchical representations end-to-end. The advent of deep learning spurred the development of methods that could automatically learn these representations. The goal is to produce embeddings where nodes with similar structural roles or features are close in the embedding space, facilitating downstream tasks such as node classification, link prediction, and graph classification \cite{velickovic2023p4r, wu2022ptq}. The challenge lies in designing architectures that can effectively aggregate information from a node's local neighborhood while also capturing global graph properties, balancing local fidelity with global context. This balance is critical for the expressive power and generalization capabilities of the learned representations.

\subsection{The Message Passing Paradigm and Early Models}
The core mechanism underpinning most Graph Neural Networks is the message-passing paradigm, also known as neighborhood aggregation \cite{gilmer2017neural}. In this iterative process, each node updates its representation by aggregating information (messages) from its direct neighbors and combining it with its own previous state. This process is typically repeated for a fixed number of layers, allowing information to propagate across the graph and enabling nodes to incorporate information from increasingly distant neighbors. Formally, for a node $v$ at layer $k$, its new representation $h_v^{(k+1)}$ is computed as:
$$h_v^{(k+1)} = \text{UPDATE}^{(k)}\left(h_v^{(k)}, \text{AGGREGATE}^{(k)}(\{h_u^{(k)} \mid u \in \mathcal{N}(v)\})\right)$$
where $\mathcal{N}(v)$ denotes the neighbors of node $v$.

Early influential GNN architectures largely adopted this message-passing framework, albeit with different aggregation and update functions. Graph Convolutional Networks (GCNs) \cite{kipf2016semi} emerged as a prominent early model, inspired by spectral graph theory but implemented as a localized spatial convolution. GCNs simplify the aggregation by averaging neighbor features and applying a linear transformation followed by a non-linearity. GraphSAGE \cite{hamilton2017inductive} further popularized the spatial perspective, introducing an inductive framework that learns aggregation functions (e.g., mean, LSTM, pooling) to generate embeddings for unseen nodes. Graph Attention Networks (GATs) \cite{velickovic2017graph} introduced an attention mechanism, allowing nodes to assign different weights to their neighbors based on their features, thereby learning more flexible aggregation patterns.

Despite their initial success, these early message-passing GNNs faced several inherent limitations. A critical issue is **over-smoothing**, where node representations become indistinguishable as the number of GNN layers increases \cite{rusch2023xev, cai2020k4b, chen2019s47, oono2019usb, peng2024t2s}. This occurs because repeated averaging of neighbor features causes node embeddings to converge to a subspace, losing their discriminative power. This problem is exacerbated in dense graphs and limits the effective depth of GNNs. Another challenge is **over-squashing**, an information bottleneck that makes it difficult for messages to propagate effectively over long distances \cite{alon2020fok}. As information from an exponentially growing receptive field needs to be compressed into fixed-size messages at each layer, crucial long-range dependencies can be lost. Furthermore, most early GNNs implicitly assume **homophily**, meaning connected nodes tend to have similar features or labels \cite{ma2021sim, zhu2020c3j}. This assumption causes them to perform poorly on heterophilous graphs, where connected nodes are often dissimilar \cite{zheng2022qxr}. The aggregation of dissimilar features can lead to noisy or misleading representations.

These limitations spurred significant research into alternative propagation mechanisms and architectural designs. For instance, the Random Walk with Unifying Memory (RUM) network \cite{wang2024oi8} proposes a fundamentally *non-convolutional* approach, using random walk trajectories processed by RNNs to generate node representations. This departure from iterative neighborhood aggregation is shown to jointly alleviate limited expressiveness, over-smoothing, and over-squashing by providing a different information flow mechanism that doesn't rely on local averaging. Similarly, the FRactional-Order graph Neural Dynamical network (FROND) framework \cite{kang2024fsk} introduces fractional calculus into continuous GNNs. By replacing integer-order differential equations with Caputo fractional derivatives, FROND captures non-local and memory-dependent dynamics, inherently mitigating over-smoothing through a slow algebraic rate of convergence to stationarity, contrasting with the exponentially swift convergence of Markovian integer-order models. This highlights an evolutionary trend towards more sophisticated propagation models that move beyond instantaneous, local updates. The GloGNN model \cite{li2022315} addresses the homophily assumption by learning global node correlations through a signed coefficient matrix, demonstrating that effective GNN design must critically evaluate and adapt to the underlying graph properties, rather than relying solely on local message passing.

\subsection{Theoretical Expressiveness: The Weisfeiler-Leman Test}
A critical aspect of evaluating GNNs is their theoretical expressive power, particularly their ability to distinguish between non-isomorphic graphs. The Weisfeiler-Leman (WL) graph isomorphism test provides a widely accepted benchmark for this discriminative power \cite{weisfeiler1968reduction}. The 1-dimensional WL (1-WL) test, also known as the color refinement algorithm, iteratively updates node "colors" (labels) based on the multiset of colors of their neighbors. If two graphs cannot be distinguished by the 1-WL test, they are considered 1-WL equivalent.

A significant theoretical finding established that many standard message-passing GNNs, including GCNs and GraphSAGE, are at most as powerful as the 1-WL test in distinguishing non-isomorphic graphs \cite{xu2018c8q, morris20185sd, garg2020z6o}. This means that if the 1-WL test cannot differentiate two graphs, these GNNs also cannot, regardless of their learned parameters. This theoretical limitation is a major bottleneck, as the 1-WL test is known to be unable to distinguish many common non-isomorphic graphs, such as certain regular graphs or graphs that differ only in specific substructure counts (e.g., cycle sizes) \cite{chen2020e6g, oono2019usb}. This theoretical gap prevents 1-WL equivalent GNNs from solving graph problems that require a finer understanding of graph topology.

This limitation sparked an "arms race" in GNN research, driving the development of more expressive architectures. One direction involved designing **higher-order GNNs** that mimic higher-dimensional WL tests (e.g., k-WL tests), which consider k-tuples of nodes. While theoretically more powerful, these k-GNNs often incur significantly higher computational complexity, limiting their practical applicability \cite{morris20185sd}.

A more practical and influential direction has been the development of GNNs that leverage richer structural information. Path Neural Networks (PathNNs) \cite{michel2023hc4} represent a significant advancement in this regard. By explicitly encoding and aggregating information from various paths emanating from each node, PathNNs move beyond local neighborhood information. Crucially, the introduction of "annotated sets of paths," where nodes within paths are recursively annotated with hashes of their respective annotated path sets of shorter lengths, allows PathNNs to achieve significantly higher expressive power. Variants like `ËœAP` (All Simple Paths with annotations) have been theoretically and empirically shown to distinguish graphs that are even 3-WL indistinguishable, far surpassing the 1-WL bottleneck of traditional GNNs. However, this enhanced expressiveness comes with a trade-off: finding all simple paths is NP-hard, necessitating approximations (e.g., fixed path length $K$) and careful design to manage computational complexity.

Another approach to overcome the WL limitation is to fundamentally alter the message-passing mechanism. The Random Walk with Unifying Memory (RUM) network \cite{wang2024oi8}, by being entirely non-convolutional and relying on random walk trajectories, is theoretically shown to be more expressive than the WL isomorphism test. It can distinguish non-isomorphic graphs that WL-equivalent GNNs cannot, such as those differing in cycle sizes or radius. This demonstrates that breaking from the standard convolutional aggregation paradigm can lead to architectures with superior discriminative power. The evolution of GNNs, therefore, reflects a continuous effort to overcome the theoretical limitations imposed by the WL test, balancing the need for higher expressiveness with practical considerations of computational efficiency and scalability.