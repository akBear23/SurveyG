\section*{5. Trustworthy GNNs: Explainability, Fairness, and Privacy}
The increasing deployment of Graph Neural Networks (GNNs) in sensitive and high-stakes applications, ranging from healthcare and finance to cybersecurity, necessitates a paramount focus on their trustworthiness. Beyond mere predictive accuracy, users and stakeholders demand models that are explainable, fair, and privacy-preserving. This section delves into the burgeoning research dedicated to instilling these critical attributes into GNNs, acknowledging that their responsible deployment hinges on addressing these multifaceted challenges \cite{dai2022hsi, zhang20222g3}. Explainability aims to demystify GNN predictions, fostering transparency and user confidence by revealing the underlying reasoning. Fairness seeks to mitigate biases inherent in data and model architectures, ensuring equitable outcomes across different demographic groups. Concurrently, privacy-preserving techniques are crucial for safeguarding sensitive graph data during training and inference, upholding ethical standards and regulatory compliance. These three pillars of trustworthiness are often interconnected, presenting complex trade-offs where enhancing one aspect might inadvertently impact another, thereby creating an intricate research landscape that requires careful navigation and innovative solutions \cite{wang20214ku}.

\subsection*{Explainable Graph Neural Networks (XGNNs)}
The black-box nature of deep learning models, including GNNs, poses a significant barrier to their adoption in domains requiring transparency and accountability. Explainable Graph Neural Networks (XGNNs) aim to bridge this gap by providing insights into *why* a GNN makes a particular prediction. Early efforts in GNN explainability primarily focused on identifying important nodes, edges, or node features that contribute most to a prediction \cite{ying2019rza}. GNNExplainer \cite{ying2019rza}, for instance, identifies a compact subgraph and a small subset of features that are crucial for a specific prediction by maximizing the mutual information between the original graph and the explanation. While foundational, these methods often yield disconnected sets of nodes or edges, which can be less intuitive for human understanding.

A significant advancement in this direction is the direct identification of crucial subgraphs as explanations. SubgraphX \cite{yuan2021pgd} proposes a novel framework that employs Monte Carlo Tree Search (MCTS) to efficiently explore the vast space of possible subgraphs and utilizes Shapley values from cooperative game theory to quantify subgraph importance. This approach inherently captures interactions among different graph structures, providing more coherent and human-intelligible explanations compared to merely highlighting individual components. Similarly, ProtGNN \cite{zhang2021wgf} aims for self-explaining GNNs by learning prototypes that represent common graph patterns, offering model-level insights akin to XGNN \cite{yuan20208v3}, which generates general graph patterns for explanations. More recently, methods like \cite{bui2024zy9} propose structure-aware interaction indices, while \cite{luo2024euy} focuses on inductive and efficient explanations, addressing scalability for larger graphs. The challenge of explaining GNNs for specific applications, such as connectome-based brain disorder analysis, has led to specialized interpretable GNNs \cite{cui2022pap}. Counterfactual explanations, as explored by CF-GNNExplainer \cite{lucic2021p70}, provide insights by identifying minimal changes to the input graph that alter the prediction, offering a different perspective on model sensitivity.

Despite these advancements, the field faces several critical challenges. Defining and evaluating the "ground truth" for GNN explanations remains an open problem, as highlighted by \cite{agarwal2022xfp} and critically examined by \cite{chen2024woq}, which questions how interpretable interpretable GNNs truly are. The trade-off between explanation fidelity (how accurately the explanation reflects the model's true reasoning) and human interpretability (how easily a human can understand and trust the explanation) is persistent. Furthermore, the computational cost of generating explanations, especially for complex models or large graphs, can be prohibitive, often necessitating approximation techniques like those in SubgraphX \cite{yuan2021pgd}. The theoretical gaps preventing a universal framework for GNN explainability stem from the inherent complexity of graph data and the non-linear, iterative nature of message passing, making it difficult to attribute predictions to specific input components without simplifying assumptions. Future directions include developing more robust evaluation metrics, exploring global interactive patterns \cite{wang2024j6z}, and integrating causal reasoning to identify invariant rationales \cite{wu2022vcx}.

\subsection*{Fairness and Bias Mitigation in GNNs}
The pervasive use of GNNs in decision-making systems raises significant concerns about fairness, as biases embedded in training data or model architectures can lead to discriminatory outcomes. Ensuring fairness in GNNs is crucial for ethical AI deployment, particularly in sensitive applications like social recommendation \cite{fan2019k6u, sharma2022liz} or credit risk assessment \cite{liu2024sbb}. The sources of bias in GNNs are manifold, including disparities in node features, imbalanced graph structures, and the inherent homophily assumption of many GNNs, which can amplify existing societal biases \cite{ma2021sim, zhu2020c3j}.

Research in fair GNNs often distinguishes between group fairness (ensuring similar outcomes for predefined demographic groups) and individual fairness (ensuring similar outcomes for similar individuals). Dong et al. \cite{dong202183w} address individual fairness for GNNs using a ranking-based approach, aiming to ensure that similar nodes (based on features and structure) receive similar predictions. Another critical aspect is mitigating data bias, as explored by EDITS \cite{dong2021qcg}, which models and mitigates data bias for GNNs. This involves techniques that either preprocess the graph to reduce bias, modify the GNN training process, or post-process the model's outputs. For scenarios with limited sensitive attribute information, Dai and Wang \cite{dai2020p5t} propose methods to learn fair GNNs, highlighting the practical challenge of sensitive attribute availability.

More advanced techniques focus on disentangling causal factors to mitigate bias. Fan et al. \cite{fan2022m67} propose debiasing GNNs via learning disentangled causal substructures, aiming to separate the influence of sensitive attributes from task-relevant features. Similarly, re-balancing strategies, as rethought by Li et al. \cite{li20245zy}, offer ways to adjust the influence of different samples or groups during training to achieve fairer outcomes. Wang et al. \cite{wang2022531} address fairness by mitigating sensitive attribute leakage, preventing the model from inadvertently using protected information. The challenge of structural disparity in GNNs, where different nodes or communities might be treated unequally due to graph topology, is demystified by Mao et al. \cite{mao202313j}, who question whether a "one size fits all" approach is suitable.

A critical analysis reveals that achieving fairness often involves trade-offs with other desirable properties, such as accuracy or utility. Luo et al. \cite{luo20240ot} explicitly address this by proposing FUGNN, a framework for harmonizing fairness and utility in GNNs. Methodological limitations include the difficulty in universally defining and measuring fairness across diverse tasks and datasets, as different fairness metrics can sometimes contradict each other. Furthermore, many debiasing techniques assume access to sensitive attribute information, which may not always be available or legally permissible. The theoretical gaps often lie in developing robust causal inference frameworks for graph data that can effectively identify and remove confounding biases without sacrificing predictive power. The generalizability of debiasing methods across different graph types and tasks also remains an active research area.

\subsection*{Privacy-Preserving Graph Neural Networks}
The processing of graph-structured data by GNNs inevitably involves sensitive information, such as personal connections in social networks, financial transactions, or medical records. This necessitates robust privacy-preserving mechanisms to prevent data leakage and ensure ethical use. The privacy landscape for GNNs is characterized by an "arms race" between sophisticated privacy attacks and defensive strategies. Attackers can attempt to infer sensitive node attributes, identify the existence of specific links, or even reconstruct parts of the graph structure from model outputs or gradients \cite{he2020kz4}. For instance, link stealing attacks \cite{he2020kz4} demonstrate how an adversary can infer the existence of links from a trained GNN, while backdoor attacks \cite{zhang2020b0m, dai2023tuj} can be crafted to embed hidden triggers that, when activated, reveal sensitive information or induce specific malicious behaviors without being easily noticeable.

To counter these threats, several privacy-preserving techniques have been adapted for GNNs. Differential Privacy (DP) is a prominent approach that adds carefully calibrated noise to data, gradients, or model parameters during training to provide provable privacy guarantees. While effective, DP often comes at the cost of utility, as increased privacy typically leads to decreased model accuracy, a trade-off that is particularly challenging for GNNs due to their reliance on structural information. Another promising direction is Federated Learning (FL), where GNNs are trained collaboratively across multiple decentralized clients without sharing raw data, only exchanging model updates \cite{liu2022gcg, he2021x8v}. This distributed paradigm offers a strong baseline for privacy, but FL systems can still be vulnerable to inference attacks on model updates or through malicious clients. Cooperative weighting in federated GNNs, as explored by \cite{hausleitner2024vw0}, attempts to enhance privacy and utility in distributed settings.

Beyond DP and FL, cryptographic techniques like Homomorphic Encryption (HE) and Secure Multi-Party Computation (SMC) offer stronger privacy guarantees by enabling computations on encrypted data. However, these methods are notoriously computationally expensive, making them impractical for large-scale GNN training and inference in many real-world scenarios. The methodological limitations of current privacy-preserving GNNs include the significant computational overhead of cryptographic methods, the inherent utility-privacy trade-off in DP, and the persistent challenge of ensuring robustness against sophisticated inference attacks in FL settings. The theoretical gaps often relate to developing efficient and scalable privacy-preserving primitives that can handle the complex, non-Euclidean operations of GNNs without prohibitive performance degradation. Future research needs to focus on designing hybrid approaches that combine the strengths of different techniques, such as integrating DP with FL, or developing novel GNN architectures that are inherently more privacy-preserving by design, thereby fostering greater confidence and ethical use of GNN technology in real-world scenarios.