{
  "nodes": [
    {
      "id": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "title": "Non-convolutional Graph Neural Networks",
      "abstract": "Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.",
      "authors": [
        "Yuanqing Wang",
        "Kyunghyun Cho"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "title": "Self-Supervised Learning of Graph Neural Networks: A Unified Review",
      "abstract": "Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.",
      "authors": [
        "Yaochen Xie",
        "Zhao Xu",
        "Zhengyang Wang",
        "Shuiwang Ji"
      ],
      "year": 2021,
      "citation_count": 353,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
      "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
      "authors": [
        "Ngoc Bui",
        "Hieu Trung Nguyen",
        "Viet Anh Nguyen",
        "Rex Ying"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "707142f242ee4e40489062870ca53810cb33d404",
      "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
      "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
      "authors": [
        "Haitao Mao",
        "Zhikai Chen",
        "Wei Jin",
        "Haoyu Han",
        "Yao Ma",
        "Tong Zhao",
        "Neil Shah",
        "Jiliang Tang"
      ],
      "year": 2023,
      "citation_count": 41,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/707142f242ee4e40489062870ca53810cb33d404",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "title": "GemNet: Universal Directional Graph Neural Networks for Molecules",
      "abstract": "Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",
      "authors": [
        "Johannes Klicpera",
        "Florian Becker",
        "Stephan Gunnemann"
      ],
      "year": 2021,
      "citation_count": 499,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "title": "A Survey on Oversmoothing in Graph Neural Networks",
      "abstract": "Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.",
      "authors": [
        "T. Konstantin Rusch",
        "Michael M. Bronstein",
        "Siddhartha Mishra"
      ],
      "year": 2023,
      "citation_count": 252,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/90dead8a056b848be164c2e5cdadfa2e191c3265",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6c96c2d4a3fbd572fef2d59cb856521ee1746789",
      "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
      "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
      "authors": [
        "Rex Ying",
        "Ruining He",
        "Kaifeng Chen",
        "Pong Eksombatchai",
        "William L. Hamilton",
        "J. Leskovec"
      ],
      "year": 2018,
      "citation_count": 3730,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6c96c2d4a3fbd572fef2d59cb856521ee1746789",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.",
      "authors": [
        "Bahare Fatemi",
        "Layla El Asri",
        "Seyed Mehran Kazemi"
      ],
      "year": 2021,
      "citation_count": 172,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "title": "Benchmarking Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.",
      "authors": [
        "Vijay Prakash Dwivedi",
        "Chaitanya K. Joshi",
        "T. Laurent",
        "Yoshua Bengio",
        "X. Bresson"
      ],
      "year": 2023,
      "citation_count": 1025,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "pdf_link": "",
      "venue": "Journal of machine learning research",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
      "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
      "authors": [
        "Li",
        "Lecheng Zheng",
        "Bowen Jin",
        "Dongqi Fu",
        "Baoyu Jing",
        "Yikun Ban",
        "Jingrui He",
        "Jiawei Han"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "title": "Sequential Recommendation with Graph Neural Networks",
      "abstract": "Sequential recommendation aims to leverage users' historical behaviors to predict their next interaction. Existing works have not yet addressed two main challenges in sequential recommendation. First, user behaviors in their rich historical sequences are often implicit and noisy preference signals, they cannot sufficiently reflect users' actual preferences. In addition, users' dynamic preferences often change rapidly over time, and hence it is difficult to capture user patterns in their historical sequences. In this work, we propose a graph neural network model called SURGE (short forSeqUential Recommendation with Graph neural nEtworks) to address these two issues. Specifically, SURGE integrates different types of preferences in long-term user behaviors into clusters in the graph by re-constructing loose item sequences into tight item-item interest graphs based on metric learning. This helps explicitly distinguish users' core interests, by forming dense clusters in the interest graph. Then, we perform cluster-aware and query-aware graph convolutional propagation and graph pooling on the constructed graph. It dynamically fuses and extracts users' current activated core interests from noisy user behavior sequences. We conduct extensive experiments on both public and proprietary industrial datasets. Experimental results demonstrate significant performance gains of our proposed method compared to state-of-the-art methods. Further studies on sequence length confirm that our method can model long behavioral sequences effectively and efficiently.",
      "authors": [
        "Jianxin Chang",
        "Chen Gao",
        "Y. Zheng",
        "Yiqun Hui",
        "Yanan Niu",
        "Yang Song",
        "Depeng Jin",
        "Yong Li"
      ],
      "year": 2021,
      "citation_count": 381,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "pdf_link": "",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "title": "Graph Neural Networks for Graphs with Heterophily: A Survey",
      "abstract": "Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we discuss the correlation between graph heterophily and various graph research domains, aiming to facilitate the development of more effective GNNs across a spectrum of practical applications and learning tasks in the graph research community. In the end, we point out the potential directions to advance and stimulate more future research and applications on heterophilic graph learning with GNNs.",
      "authors": [
        "Xin Zheng",
        "Yixin Liu",
        "Shirui Pan",
        "Miao Zhang",
        "Di Jin",
        "Philip S. Yu"
      ],
      "year": 2022,
      "citation_count": 243,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0d4184cff17f093e0487b27180be515c385feff6",
      "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks",
      "abstract": "This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the final result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical findings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks.",
      "authors": [
        "P. Papp",
        "Karolis Martinkus",
        "Lukas Faber",
        "Roger Wattenhofer"
      ],
      "year": 2021,
      "citation_count": 147,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d4184cff17f093e0487b27180be515c385feff6",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
      "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
      "authors": [
        "Johannes Klicpera",
        "Aleksandar Bojchevski",
        "Stephan Günnemann"
      ],
      "year": 2018,
      "citation_count": 1769,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ac225094aab9e7b629bc5b3343e026dea0200c70",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage",
      "abstract": "Graph Neural Networks (GNNs) have shown great power in learning node representations on graphs. However, they may inherit historical prejudices from training data, leading to discriminatory bias in predictions. Although some work has developed fair GNNs, most of them directly borrow fair representation learning techniques from non-graph domains without considering the potential problem of sensitive attribute leakage caused by feature propagation in GNNs. However, we empirically observe that feature propagation could vary the correlation of previously innocuous non-sensitive features to the sensitive ones. This can be viewed as a leakage of sensitive information which could further exacerbate discrimination in predictions. Thus, we design two feature masking strategies according to feature correlations to highlight the importance of considering feature propagation and correlation variation in alleviating discrimination. Motivated by our analysis, we propose Fair View Graph Neural Network (FairVGNN) to generate fair views of features by automatically identifying and masking sensitive-correlated features considering correlation variation after feature propagation. Given the learned fair views, we adaptively clamp weights of the encoder to avoid using sensitive-related features. Experiments on real-world datasets demonstrate that FairVGNN enjoys a better trade-off between model utility and fairness.",
      "authors": [
        "Yu Wang",
        "Yuying Zhao",
        "Yushun Dong",
        "Huiyuan Chen",
        "Jundong Li",
        "Tyler Derr"
      ],
      "year": 2022,
      "citation_count": 95,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "title": "PowerGraph: A power grid benchmark dataset for graph neural networks",
      "abstract": "Power grids are critical infrastructures of paramount importance to modern society and, therefore, engineered to operate under diverse conditions and failures. The ongoing energy transition poses new challenges for the decision-makers and system operators. Therefore, developing grid analysis algorithms is important for supporting reliable operations. These key tools include power flow analysis and system security analysis, both needed for effective operational and strategic planning. The literature review shows a growing trend of machine learning (ML) models that perform these analyses effectively. In particular, Graph Neural Networks (GNNs) stand out in such applications because of the graph-based structure of power grids. However, there is a lack of publicly available graph datasets for training and benchmarking ML models in electrical power grid applications. First, we present PowerGraph, which comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. Second, we provide ground-truth explanations for the cascading failure analysis. Finally, we perform a complete benchmarking of GNN methods for node-level and graph-level tasks and explainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse tasks that includes power flow and fault scenarios with real-world explanations, providing a valuable resource for developing improved GNN models for node-level, graph-level tasks and explainability methods in power system modeling. The dataset is available at https://figshare.com/articles/dataset/PowerGraph/22820534 and the code at https://github.com/PowerGraph-Datasets.",
      "authors": [
        "Anna Varbella",
        "Kenza Amara",
        "B. Gjorgiev",
        "G. Sansavini"
      ],
      "year": 2024,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "db5d583782264529456a475ce8e9a90823b3a2b5",
      "title": "Graph Neural Networks in Network Neuroscience",
      "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain –namely brain graph. Relying on its non-euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience.",
      "authors": [
        "Alaa Bessadok",
        "M. Mahjoub",
        "I. Rekik"
      ],
      "year": 2021,
      "citation_count": 214,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/db5d583782264529456a475ce8e9a90823b3a2b5",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "title": "Universal Prompt Tuning for Graph Neural Networks",
      "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
      "authors": [
        "Taoran Fang",
        "Yunchao Zhang",
        "Yang Yang",
        "Chunping Wang",
        "Lei Chen"
      ],
      "year": 2022,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b4895de425a02af87713bd78ed1a29fe425753af",
      "title": "Decoupling the Depth and Scope of Graph Neural Networks",
      "abstract": "State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -- to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into\"white noise\". Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.",
      "authors": [
        "Hanqing Zeng",
        "Muhan Zhang",
        "Yinglong Xia",
        "Ajitesh Srivastava",
        "Andrey Malevich",
        "R. Kannan",
        "V. Prasanna",
        "Long Jin",
        "Ren Chen"
      ],
      "year": 2022,
      "citation_count": 154,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b4895de425a02af87713bd78ed1a29fe425753af",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
      "abstract": "Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
      "authors": [
        "Hao Yuan",
        "Haiyang Yu",
        "Shurui Gui",
        "Shuiwang Ji"
      ],
      "year": 2020,
      "citation_count": 670,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6ae2967bb0a5e57cc545176120a4845576e068a3",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "title": "Trustworthy Graph Neural Networks: Aspects, Methods, and Trends",
      "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications such as recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects, such as vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterized by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarize existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. In addition, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialization of trustworthy GNNs.",
      "authors": [
        "He Zhang",
        "Bang Wu",
        "Xingliang Yuan",
        "Shirui Pan",
        "Hanghang Tong",
        "Jian Pei"
      ],
      "year": 2022,
      "citation_count": 124,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "pdf_link": "",
      "venue": "Proceedings of the IEEE",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "title": "Cooperative Graph Neural Networks",
      "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.",
      "authors": [
        "Ben Finkelshtein",
        "Xingyue Huang",
        "Michael M. Bronstein",
        "I. Ceylan"
      ],
      "year": 2023,
      "citation_count": 37,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "title": "Learning Strong Graph Neural Networks with Weak Information",
      "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",
      "authors": [
        "Yixin Liu",
        "Kaize Ding",
        "Jianling Wang",
        "Vincent Lee",
        "Huan Liu",
        "Shirui Pan"
      ],
      "year": 2023,
      "citation_count": 52,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "title": "Graph Neural Networks With Convolutional ARMA Filters",
      "abstract": "Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.",
      "authors": [
        "F. Bianchi",
        "Daniele Grattarola",
        "L. Livi",
        "C. Alippi"
      ],
      "year": 2019,
      "citation_count": 427,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "title": "Individual Fairness for Graph Neural Networks: A Ranking based Approach",
      "abstract": "Recent years have witnessed the pivotal role of Graph Neural Networks (GNNs) in various high-stake decision-making scenarios due to their superior learning capability. Close on the heels of the successful adoption of GNNs in different application domains has been the increasing societal concern that conventional GNNs often do not have fairness considerations. Although some research progress has been made to improve the fairness of GNNs, these works mainly focus on the notion of group fairness regarding different subgroups defined by a protected attribute such as gender, age, and race. Beyond that, it is also essential to study the GNN fairness at a much finer granularity (i.e., at the node level) to ensure that GNNs render similar prediction results for similar individuals to achieve the notion of individual fairness. Toward this goal, in this paper, we make an initial investigation to enhance the individual fairness of GNNs and propose a novel ranking based framework---REDRESS. Specifically, we refine the notion of individual fairness from a ranking perspective, and formulate the ranking based individual fairness promotion problem. This naturally addresses the issue of Lipschitz constant specification and distance calibration resulted from the Lipschitz condition in the conventional individual fairness definition. Our proposed framework REDRESS encapsulates the GNN model utility maximization and the ranking-based individual fairness promotion in a joint framework to enable end-to-end training. It is noteworthy mentioning that REDRESS is a plug-and-play framework and can be easily generalized to any prevalent GNN architectures. Extensive experiments on multiple real-world graphs demonstrate the superiority of REDRESS in achieving a good balance between model utility maximization and individual fairness promotion. Our open source code can be found here: https://github.com/yushundong/REDRESS.",
      "authors": [
        "Yushun Dong",
        "Jian Kang",
        "H. Tong",
        "Jundong Li"
      ],
      "year": 2021,
      "citation_count": 127,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "75e924bd79d27a23f3f93d9b1ab62a779505c8d2",
      "title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks",
      "abstract": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Guodong Long",
        "Jing Jiang",
        "Xiaojun Chang",
        "Chengqi Zhang"
      ],
      "year": 2020,
      "citation_count": 1580,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/75e924bd79d27a23f3f93d9b1ab62a779505c8d2",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "title": "Can graph neural networks count substructures?",
      "abstract": "The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting, we propose a local relational pooling approach with inspirations from Murphy et al. (2019) and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on real-world tasks.",
      "authors": [
        "Zhengdao Chen",
        "Lei Chen",
        "Soledad Villar",
        "Joan Bruna"
      ],
      "year": 2020,
      "citation_count": 338,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/536da0e76290aea9cbe75c29bac096aeb45ef875",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
      "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
      "authors": [
        "Haoyu Han",
        "Juanhui Li",
        "Wei Huang",
        "Xianfeng Tang",
        "Hanqing Lu",
        "Chen Luo",
        "Hui Liu",
        "Jiliang Tang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "title": "Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available¹.",
      "authors": [
        "Yuwen Wang",
        "Shunyu Liu",
        "Tongya Zheng",
        "Kaixuan Chen",
        "Mingli Song"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6dc0932670a0b5140a426ca310bbb03783ff2240",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4fa31616b834c377c4995c346a2b17464f25692a",
      "title": "Graph Neural Networks for Recommender System",
      "abstract": "Recently, graph neural network (GNN) has become the new state-of-the-art approach in many recommendation problems, with its strong ability to handle structured data and to explore high-order information. However, as the recommendation tasks are diverse and various in the real world, it is quite challenging to design proper GNN methods for specific problems. In this tutorial, we focus on the critical challenges of GNN-based recommendation and the potential solutions. Specifically, we start from an extensive background of recommender systems and graph neural networks. Then we fully discuss why GNNs are required in recommender systems and the four parts of challenges, including graph construction, network design, optimization, and computation efficiency. Then, we discuss how to address these challenges by elaborating on the recent advances of GNN-based recommendation models, with a systematic taxonomy from four critical perspectives: stages, scenarios, objectives, and applications. Last, we finalize this tutorial with conclusions and discuss important future directions.",
      "authors": [
        "Chen Gao",
        "Xiang Wang",
        "Xiangnan He",
        "Yong Li"
      ],
      "year": 2022,
      "citation_count": 251,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4fa31616b834c377c4995c346a2b17464f25692a",
      "pdf_link": "",
      "venue": "Web Search and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "title": "Graph neural networks for materials science and chemistry",
      "abstract": "Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs. Graph neural networks are machine learning models that directly access the structural representation of molecules and materials. This Review discusses state-of-the-art architectures and applications of graph neural networks in materials science and chemistry, indicating a possible road-map for their further development.",
      "authors": [
        "Patrick Reiser",
        "Marlen Neubert",
        "Andr'e Eberhard",
        "Luca Torresi",
        "Chen Zhou",
        "Chen Shao",
        "Houssam Metni",
        "Clint van Hoesel",
        "Henrik Schopmans",
        "T. Sommer",
        "Pascal Friederich"
      ],
      "year": 2022,
      "citation_count": 492,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "pdf_link": "",
      "venue": "Communications Materials",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e",
      "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration",
      "abstract": "Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy, whether the results are trustworthy is still unexplored. Previous studies suggest that many modern neural networks are over-confident on the predictions, however, surprisingly, we discover that GNNs are primarily in the opposite direction, i.e., GNNs are under-confident. Therefore, the confidence calibration for GNNs is highly desired. In this paper, we propose a novel trustworthy GNN model by designing a topology-aware post-hoc calibration function. Specifically, we first verify that the confidence distribution in a graph has homophily property, and this finding inspires us to design a calibration GNN model (CaGCN) to learn the calibration function. CaGCN is able to obtain a unique transformation from logits of GNNs to the calibrated confidence for each node, meanwhile, such transformation is able to preserve the order between classes, satisfying the accuracy-preserving property. Moreover, we apply the calibration GNN to self-training framework, showing that more trustworthy pseudo labels can be obtained with the calibrated confidence and further improve the performance. Extensive experiments demonstrate the effectiveness of our proposed model in terms of both calibration and accuracy.",
      "authors": [
        "Xiao Wang",
        "Hongrui Liu",
        "Chuan Shi",
        "Cheng Yang"
      ],
      "year": 2021,
      "citation_count": 126,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3b2f5884e8199544375ddcdb4fa58f44df0b1a7e",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "title": "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
      "abstract": "Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.",
      "authors": [
        "Haonan Yuan",
        "Jiliang Tang",
        "Xia Hu",
        "Shuiwang Ji"
      ],
      "year": 2020,
      "citation_count": 431,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "title": "Substructure Aware Graph Neural Networks",
      "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
      "authors": [
        "DingYi Zeng",
        "Wanlong Liu",
        "Wenyu Chen",
        "Li Zhou",
        "Malu Zhang",
        "Hong Qu"
      ],
      "year": 2023,
      "citation_count": 47,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
      "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT",
      "authors": [
        "Juanhui Li",
        "Harry Shomer",
        "Haitao Mao",
        "Shenglai Zeng",
        "Yao Ma",
        "Neil Shah",
        "Jiliang Tang",
        "Dawei Yin"
      ],
      "year": 2023,
      "citation_count": 76,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f442378ead6282024cf5b9046daa10422fe9fc5f",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "title": "BrainGB: A Benchmark for Brain Network Analysis With Graph Neural Networks",
      "abstract": "Mapping the connectome of the human brain using structural or functional connectivity has become one of the most pervasive paradigms for neuroimaging analysis. Recently, Graph Neural Networks (GNNs) motivated from geometric deep learning have attracted broad interest due to their established power for modeling complex networked data. Despite their superior performance in many fields, there has not yet been a systematic study of how to design effective GNNs for brain network analysis. To bridge this gap, we present BrainGB, a benchmark for brain network analysis with GNNs. BrainGB standardizes the process by (1) summarizing brain network construction pipelines for both functional and structural neuroimaging modalities and (2) modularizing the implementation of GNN designs. We conduct extensive experiments on datasets across cohorts and modalities and recommend a set of general recipes for effective GNN designs on brain networks. To support open and reproducible research on GNN-based brain network analysis, we host the BrainGB website at https://braingb.us with models, tutorials, examples, as well as an out-of-box Python package. We hope that this work will provide useful empirical evidence and offer insights for future research in this novel and promising direction.",
      "authors": [
        "Hejie Cui",
        "Wei Dai",
        "Yanqiao Zhu",
        "Xuan Kan",
        "Antonio Aodong Chen Gu",
        "Joshua Lukemire",
        "L. Zhan",
        "Lifang He",
        "Ying Guo",
        "Carl Yang"
      ],
      "year": 2022,
      "citation_count": 145,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "pdf_link": "",
      "venue": "IEEE Transactions on Medical Imaging",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization",
      "abstract": "Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.",
      "authors": [
        "Ralph Abboud",
        ".Ismail .Ilkan Ceylan",
        "Martin Grohe",
        "Thomas Lukasiewicz"
      ],
      "year": 2020,
      "citation_count": 238,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "title": "Discovering Invariant Rationales for Graph Neural Networks",
      "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
      "authors": [
        "Yingmin Wu",
        "Xiang Wang",
        "An Zhang",
        "Xiangnan He",
        "Tat-seng Chua"
      ],
      "year": 2022,
      "citation_count": 251,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
      "abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.",
      "authors": [
        "Christopher Morris",
        "Martin Ritzert",
        "Matthias Fey",
        "William L. Hamilton",
        "J. E. Lenssen",
        "Gaurav Rattan",
        "Martin Grohe"
      ],
      "year": 2018,
      "citation_count": 1734,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
      "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and imposes $O(p)$ (orthogonal group) equivariance w.r.t. the positional features simultaneously, where $p$ is the dimension of used positional features. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.",
      "authors": [
        "Hongya Wang",
        "Haoteng Yin",
        "Muhan Zhang",
        "Pan Li"
      ],
      "year": 2022,
      "citation_count": 119,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "title": "Robustness of Graph Neural Networks at Scale",
      "abstract": "Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.",
      "authors": [
        "Simon Geisler",
        "Tobias Schmidt",
        "Hakan cSirin",
        "Daniel Zugner",
        "Aleksandar Bojchevski",
        "Stephan Gunnemann"
      ],
      "year": 2021,
      "citation_count": 146,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "title": "Graph Condensation for Graph Neural Networks",
      "abstract": "Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures.Code is released at https://github.com/ChandlerBang/GCond.",
      "authors": [
        "Wei Jin",
        "Lingxiao Zhao",
        "Shichang Zhang",
        "Yozen Liu",
        "Jiliang Tang",
        "Neil Shah"
      ],
      "year": 2021,
      "citation_count": 170,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7456dea3a3646f2df6392773a196a5abd0d53b11",
      "title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
      "abstract": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
      "authors": [
        "Simon L. Batzner",
        "Albert Musaelian",
        "Lixin Sun",
        "M. Geiger",
        "J. Mailoa",
        "M. Kornbluth",
        "N. Molinari",
        "T. Smidt",
        "B. Kozinsky"
      ],
      "year": 2021,
      "citation_count": 1496,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7456dea3a3646f2df6392773a196a5abd0d53b11",
      "pdf_link": "",
      "venue": "Nature Communications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "title": "Is Homophily a Necessity for Graph Neural Networks?",
      "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\"like attracts like\"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions, and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.",
      "authors": [
        "Yao Ma",
        "Xiaorui Liu",
        "Neil Shah",
        "Jiliang Tang"
      ],
      "year": 2021,
      "citation_count": 246,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "252351936bd6fabf4b6cd2962fa0ee613772278d",
      "title": "Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey",
      "abstract": "With recent advances in sensing technologies, a myriad of spatio-temporal data has been generated and recorded in smart cities. Forecasting the evolution patterns of spatio-temporal data is an important yet demanding aspect of urban computing, which can enhance intelligent management decisions in various fields, including transportation, environment, climate, public safety, healthcare, and others. Traditional statistical and deep learning methods struggle to capture complex correlations in urban spatio-temporal data. To this end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed, achieving great promise in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. In this manuscript, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. Firstly, we provide a brief introduction to the construction methods of spatio-temporal graph data and the prevalent deep-learning architectures used in STGNNs. We then sort out the primary application domains and specific predictive learning tasks based on existing literature. Afterward, we scrutinize the design of STGNNs and their combination with some advanced technologies in recent years. Finally, we conclude the limitations of existing research and suggest potential directions for future work.",
      "authors": [
        "G. Jin",
        "Yuxuan Liang",
        "Yuchen Fang",
        "Zezhi Shao",
        "Jincai Huang",
        "Junbo Zhang",
        "Yu Zheng"
      ],
      "year": 2023,
      "citation_count": 265,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/252351936bd6fabf4b6cd2962fa0ee613772278d",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "title": "Elastic Graph Neural Networks",
      "abstract": "While many existing graph neural networks (GNNs) have been proven to perform $\\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \\url{https://github.com/lxiaorui/ElasticGNN}.",
      "authors": [
        "Xiaorui Liu",
        "W. Jin",
        "Yao Ma",
        "Yaxin Li",
        "Hua Liu",
        "Yiqi Wang",
        "Ming Yan",
        "Jiliang Tang"
      ],
      "year": 2021,
      "citation_count": 113,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
      "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.",
      "authors": [
        "Vijay Prakash Dwivedi",
        "A. Luu",
        "T. Laurent",
        "Yoshua Bengio",
        "X. Bresson"
      ],
      "year": 2021,
      "citation_count": 370,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/454304628bf10f02aba1c2cfc95891e94d09208e",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47",
      "title": "A Note on Over-Smoothing for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \\cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure \"expressiveness\" of embedding is conceptually clean; it leads to simpler proofs than \\cite{oono2019graph} and can handle more non-linearities.",
      "authors": [
        "Chen Cai",
        "Yusu Wang"
      ],
      "year": 2020,
      "citation_count": 299,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c6d550c3fcecf27b979be84c4cd444cc1c72bf47",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "title": "PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks",
      "abstract": "In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.",
      "authors": [
        "Minh N. Vu",
        "M. Thai"
      ],
      "year": 2020,
      "citation_count": 360,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "title": "GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks",
      "abstract": "Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis and accelerating the model convergence up to 4.32X. The code is available in: https://github.com/MingChen-Sun/GPPT.",
      "authors": [
        "Mingchen Sun",
        "Kaixiong Zhou",
        "Xingbo He",
        "Ying Wang",
        "Xin Wang"
      ],
      "year": 2022,
      "citation_count": 167,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "title": "Learning to Pre-train Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have become the defacto standard for representation learning on graphs, which derive effective node representations by recursively aggregating information from graph neighborhoods. \nWhile GNNs can be trained from scratch, pre-training GNNs to learn transferable knowledge for downstream tasks has recently been demonstrated to improve the state of the art. \nHowever, conventional GNN pre-training methods follow a two-step paradigm: 1) pre-training on abundant unlabeled data and 2) fine-tuning on downstream labeled data, between which there exists a significant gap due to the divergence of optimization objectives in the two steps. \nIn this paper, we conduct an analysis to show the divergence between pre-training and fine-tuning, and to alleviate such divergence, we propose L2P-GNN, a self-supervised pre-training strategy for GNNs. \nThe key insight is that L2P-GNN attempts to learn how to fine-tune during the pre-training process in the form of transferable prior knowledge. To encode both local and global information into the prior, L2P-GNN is further designed with a dual adaptation mechanism at both node and graph levels. \nFinally, we conduct a systematic empirical study on the pre-training of various GNN models, using both a public collection of protein graphs and a new compilation of bibliographic graphs for pre-training. Experimental results show that L2P-GNN is capable of learning effective and transferable prior knowledge that yields powerful representations for downstream tasks. \n(Code and datasets are available at https://github.com/rootlu/L2P-GNN.)",
      "authors": [
        "Yuanfu Lu",
        "Xunqiang Jiang",
        "Yuan Fang",
        "C. Shi"
      ],
      "year": 2021,
      "citation_count": 132,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/94497472eecb7530a2b75c564548c540ebd61e9b",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.",
      "authors": [
        "Uri Alon",
        "Eran Yahav"
      ],
      "year": 2020,
      "citation_count": 763,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "title": "How Powerful are Spectral Graph Neural Networks",
      "abstract": "Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.",
      "authors": [
        "Xiyuan Wang",
        "Muhan Zhang"
      ],
      "year": 2022,
      "citation_count": 232,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "title": "Strategies for Pre-training Graph Neural Networks",
      "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.",
      "authors": [
        "Weihua Hu",
        "Bowen Liu",
        "Joseph Gomes",
        "M. Zitnik",
        "Percy Liang",
        "V. Pande",
        "J. Leskovec"
      ],
      "year": 2019,
      "citation_count": 1499,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "title": "Data Augmentation for Graph Neural Networks",
      "abstract": "Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.",
      "authors": [
        "Tong Zhao",
        "Yozen Liu",
        "Leonardo Neves",
        "Oliver J. Woodford",
        "Meng Jiang",
        "Neil Shah"
      ],
      "year": 2020,
      "citation_count": 433,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.",
      "authors": [
        "Deli Chen",
        "Yankai Lin",
        "Wei Li",
        "Peng Li",
        "Jie Zhou",
        "Xu Sun"
      ],
      "year": 2019,
      "citation_count": 1180,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/94194703e83b5447f519fd8bcbb903916e05aaf9",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information",
      "abstract": "Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy.",
      "authors": [
        "Enyan Dai",
        "Suhang Wang"
      ],
      "year": 2020,
      "citation_count": 268,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "pdf_link": "",
      "venue": "Web Search and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "title": "How Interpretable Are Interpretable Graph Neural Networks?",
      "abstract": "Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.",
      "authors": [
        "Yongqiang Chen",
        "Yatao Bian",
        "Bo Han",
        "James Cheng"
      ],
      "year": 2024,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/854342cf063eef4428a5441c8d317dfbabb8117f",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "title": "Rethinking Fair Graph Neural Networks from Re-balancing",
      "abstract": "Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.",
      "authors": [
        "Zhixun Li",
        "Yushun Dong",
        "Qiang Liu",
        "Jeffrey Xu Yu"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "title": "Revisiting Heterophily For Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
      "authors": [
        "Sitao Luan",
        "Chenqing Hua",
        "Qincheng Lu",
        "Jiaqi Zhu",
        "Mingde Zhao",
        "Shuyuan Zhang",
        "Xiaoming Chang",
        "Doina Precup"
      ],
      "year": 2022,
      "citation_count": 225,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
      "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
      "authors": [
        "Qiyu Kang",
        "Kai Zhao",
        "Qinxu Ding",
        "Feng Ji",
        "Xuhao Li",
        "Wenfei Liang",
        "Yang Song",
        "Wee Peng Tay"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "title": "Generalization and Representational Limits of Graph Neural Networks",
      "abstract": "We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.",
      "authors": [
        "Vikas K. Garg",
        "S. Jegelka",
        "T. Jaakkola"
      ],
      "year": 2020,
      "citation_count": 331,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
      "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
      "authors": [
        "Yu Chen",
        "Lingfei Wu",
        "Mohammed J. Zaki"
      ],
      "year": 2020,
      "citation_count": 436,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "aafe1338caef4682069e92378f1190785ec24c2c",
      "title": "Breaking the Limits of Message Passing Graph Neural Networks",
      "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.",
      "authors": [
        "M. Balcilar",
        "P. Héroux",
        "Benoit Gaüzère",
        "P. Vasseur",
        "Sébastien Adam",
        "P. Honeine"
      ],
      "year": 2021,
      "citation_count": 137,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/aafe1338caef4682069e92378f1190785ec24c2c",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "title": "GNNGuard: Defending Graph Neural Networks against Adversarial Attacks",
      "abstract": "Deep learning methods for graphs achieve remarkable performance on many tasks. However, despite the proliferation of such methods and their success, recent findings indicate that small, unnoticeable perturbations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop GNNGuard, a general defense approach against a variety of training-time attacks that perturb the discrete graph structure. GNNGuard can be straightforwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack. GNNGuard uses network theory of homophily to learn how best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges then allow the underlying GNN to robustly propagate neural messages in the graph. GNNGuard introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across five GNNs, three defense methods, and four datasets, including a challenging human disease graph, experiments show that GNNGuard outperforms existing defense approaches by 15.3% on average. Remarkably, GNNGuard can effectively restore the state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks.",
      "authors": [
        "Xiang Zhang",
        "M. Zitnik"
      ],
      "year": 2020,
      "citation_count": 319,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/73366d75289c5e37481639fb54fdee28a664e2b3",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have shown superior performance in analyzing attributed networks in various web-based applications such as social recommendation and web search. Nevertheless, in high-stake decision-making scenarios such as online fraud detection, there is an increasing societal concern that GNNs could make discriminatory decisions towards certain demographic groups. Despite recent explorations on fair GNNs, these works are tailored for a specific GNN model. However, myriads of GNN variants have been proposed for different applications, and it is costly to fine-tune existing debiasing algorithms for each specific GNN architecture. Different from existing works that debias GNN models, we aim to debias the input attributed network to achieve fairer GNNs through feeding GNNs with less biased data. Specifically, we propose novel definitions and metrics to measure the bias in an attributed network, which leads to the optimization objective to mitigate bias. We then develop a framework EDITS to mitigate the bias in attributed networks while maintaining the performance of GNNs in downstream tasks. EDITS works in a model-agnostic manner, i.e., it is independent of any specific GNN. Experiments demonstrate the validity of the proposed bias metrics and the superiority of EDITS on both bias mitigation and utility maintenance. Open-source implementation: https://github.com/yushundong/EDITS.",
      "authors": [
        "Yushun Dong",
        "Ninghao Liu",
        "B. Jalaeian",
        "Jundong Li"
      ],
      "year": 2021,
      "citation_count": 138,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cd551790992d16148fe2e5ff2cc76861195e2191",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "title": "Training Graph Neural Networks with 1000 Layers",
      "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.24 \\pm 0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude. Please visit our project website https://www.deepgcns.org/arch/gnn1000 for more information.",
      "authors": [
        "Guohao Li",
        "Matthias Müller",
        "Bernard Ghanem",
        "V. Koltun"
      ],
      "year": 2021,
      "citation_count": 251,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cf30fb61a5943781144c8442563e3ef9c38df871",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "year": 2020,
      "citation_count": 190,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e4b1d7553020258d7e537e2cfa53865359389eac",
      "pdf_link": "",
      "venue": "USENIX Security Symposium",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
      "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
      "authors": [
        "Daniel Zügner",
        "Stephan Günnemann"
      ],
      "year": 2019,
      "citation_count": 590,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
      "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
      "authors": [
        "Ming Jin",
        "Huan Yee Koh",
        "Qingsong Wen",
        "Daniele Zambon",
        "C. Alippi",
        "G. I. Webb",
        "Irwin King",
        "Shirui Pan"
      ],
      "year": 2023,
      "citation_count": 222,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "title": "ETA Prediction with Graph Neural Networks in Google Maps",
      "abstract": "Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events---such as rush hours---that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",
      "authors": [
        "Austin Derrow-Pinion",
        "Jennifer She",
        "David Wong",
        "Oliver Lange",
        "Todd Hester",
        "L. Perez",
        "Marc Nunkesser",
        "Seongjae Lee",
        "Xueying Guo",
        "Brett Wiltshire",
        "P. Battaglia",
        "Vishal Gupta",
        "Ang Li",
        "Zhongwen Xu",
        "Alvaro Sanchez-Gonzalez",
        "Yujia Li",
        "Petar Velivckovi'c"
      ],
      "year": 2021,
      "citation_count": 251,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5822490cf59df7f7ccb92b8901f244850b867a66",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
      "abstract": "Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrifice classification accuracy on original graph.",
      "authors": [
        "Kaidi Xu",
        "Hongge Chen",
        "Sijia Liu",
        "Pin-Yu Chen",
        "Tsui-Wei Weng",
        "Mingyi Hong",
        "Xue Lin"
      ],
      "year": 2019,
      "citation_count": 474,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/341880efaef452f631a4a5cd61bef5dae47741d7",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "title": "Combinatorial optimization and reasoning with graph neural networks",
      "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have mostly focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing the former. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at researchers in both optimization and machine learning.",
      "authors": [
        "Quentin Cappart",
        "D. Chételat",
        "Elias Boutros Khalil",
        "Andrea Lodi",
        "Christopher Morris",
        "Petar Velickovic"
      ],
      "year": 2021,
      "citation_count": 388,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "title": "E(n) Equivariant Graph Neural Networks",
      "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",
      "authors": [
        "Victor Garcia Satorras",
        "Emiel Hoogeboom",
        "M. Welling"
      ],
      "year": 2021,
      "citation_count": 1132,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "title": "Towards Deeper Graph Neural Networks",
      "abstract": "Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.",
      "authors": [
        "Meng Liu",
        "Hongyang Gao",
        "Shuiwang Ji"
      ],
      "year": 2020,
      "citation_count": 640,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/639206a9a32d91386924f1c94e9760dfb43df72e",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "title": "Graph Neural Networks",
      "abstract": "Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Although conventional deep learning techniques have achieved huge successes on Euclidean data such as images, or sequence data such as text, there are many applications that are naturally or best represented with a graph structure. This gap has driven a tide in research for deep learning on graphs, among them Graph Neural Networks (GNNs) are the most successful in coping with various learning tasks across a large number of application domains. In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations, frontiers, and applications. We will introduce the fundamental aspects of GNNs ranging from the popular models and their expressive powers, to the scalability, interpretability and robustness of GNNs. Then, we will discuss various frontier research, ranging from graph classification and link prediction, to graph generation and transformation, graph matching and graph structure learning. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our book and summarize the roadmap of the various research topics of GNNs. Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Le Song Mohamed bin Zayed University of Artificial Intelligence, e-mail: dasongle@gmail.com",
      "authors": [
        "Yuyu Zhang",
        "Xinshi Chen",
        "Yuan Yang",
        "Arun Ramamurthy",
        "Bo Li",
        "Yuan Qi",
        "Le Song"
      ],
      "year": 2021,
      "citation_count": 294,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "pdf_link": "",
      "venue": "Deep Learning on Graphs",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "title": "Are Defenses for Graph Neural Networks Robust?",
      "abstract": "A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw - virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering - most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.",
      "authors": [
        "Felix Mujkanovic",
        "Simon Geisler",
        "Stephan Gunnemann",
        "Aleksandar Bojchevski"
      ],
      "year": 2023,
      "citation_count": 62,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d09608593caa20b79a8aaddfe19df7e31513d711",
      "title": "Graph Neural Networks in IoT: A Survey",
      "abstract": "The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.",
      "authors": [
        "Guimin Dong",
        "Mingyue Tang",
        "Zhiyuan Wang",
        "Jiechao Gao",
        "Sikun Guo",
        "Lihua Cai",
        "Robert Gutierrez",
        "Brad Campbell",
        "Laura E. Barnes",
        "M. Boukhechba"
      ],
      "year": 2022,
      "citation_count": 122,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d09608593caa20b79a8aaddfe19df7e31513d711",
      "pdf_link": "",
      "venue": "ACM Trans. Sens. Networks",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
      "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.",
      "authors": [
        "B. Chamberlain",
        "S. Shirobokov",
        "Emanuele Rossi",
        "Fabrizio Frasca",
        "Thomas Markovich",
        "Nils Y. Hammerla",
        "Michael M. Bronstein",
        "Max Hansmire"
      ],
      "year": 2022,
      "citation_count": 98,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "title": "Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities",
      "abstract": "Graph neural networks have emerged as a leading architecture for many graph-level tasks, such as graph classification and graph generation. As an essential component of the architecture, graph pooling is indispensable for obtaining a holistic graph-level representation of the whole graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these works. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods for graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods with a mathematical summary for each category; 2) then, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) next, we further outline the applications that incorporate the idea of graph pooling in a variety of domains; 4) finally, we discuss certain critical challenges facing current studies and share our insights on future potential directions for research on the improvement of graph pooling.",
      "authors": [
        "Chuang Liu",
        "Yibing Zhan",
        "Chang Li",
        "Bo Du",
        "Jia Wu",
        "Wenbin Hu",
        "Tongliang Liu",
        "Dacheng Tao"
      ],
      "year": 2022,
      "citation_count": 96,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3db15a5534050ab2cfc1d09dd772d032395515e1",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "title": "Finding Global Homophily in Graph Neural Networks When Meeting Heterophily",
      "abstract": "We investigate graph neural networks on graphs with heterophily. Some existing methods amplify a node's neighborhood with multi-hop neighbors to include more nodes with homophily. However, it is a significant challenge to set personalized neighborhood sizes for different nodes. Further, for other homophilous nodes excluded in the neighborhood, they are ignored for information aggregation. To address these problems, we propose two models GloGNN and GloGNN++, which generate a node's embedding by aggregating information from global nodes in the graph. In each layer, both models learn a coefficient matrix to capture the correlations between nodes, based on which neighborhood aggregation is performed. The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution. We further accelerate neighborhood aggregation and derive a linear time complexity. We theoretically explain the models' effectiveness by proving that both the coefficient matrix and the generated node embedding matrix have the desired grouping effect. We conduct extensive experiments to compare our models against 11 other competitors on 15 benchmark datasets in a wide range of domains, scales and graph heterophilies. Experimental results show that our methods achieve superior performance and are also very efficient.",
      "authors": [
        "Xiang Li",
        "Renyu Zhu",
        "Yao Cheng",
        "Caihua Shan",
        "Siqiang Luo",
        "Dongsheng Li",
        "Wei Qian"
      ],
      "year": 2022,
      "citation_count": 225,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "title": "Backdoor Attacks to Graph Neural Networks",
      "abstract": "In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a subgraph based backdoor attack to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
      "authors": [
        "Zaixi Zhang",
        "Jinyuan Jia",
        "Binghui Wang",
        "N. Gong"
      ],
      "year": 2020,
      "citation_count": 236,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "pdf_link": "",
      "venue": "ACM Symposium on Access Control Models and Technologies",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "347e837b1aa03c9d17c69a522929000f0a0f0a51",
      "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
      "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
      "authors": [
        "Paul-Edouard Sarlin",
        "Daniel DeTone",
        "Tomasz Malisiewicz",
        "Andrew Rabinovich"
      ],
      "year": 2019,
      "citation_count": 2131,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/347e837b1aa03c9d17c69a522929000f0a0f0a51",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "title": "GNNExplainer: Generating Explanations for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",
      "authors": [
        "Rex Ying",
        "Dylan Bourgeois",
        "Jiaxuan You",
        "M. Zitnik",
        "J. Leskovec"
      ],
      "year": 2019,
      "citation_count": 1436,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/00358a3f17821476d93461192b9229fe7d92bb3f",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0a69c8815536a657668e089e3281ff2e963d947a",
      "title": "Design Space for Graph Neural Networks",
      "abstract": "The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.",
      "authors": [
        "Jiaxuan You",
        "Rex Ying",
        "J. Leskovec"
      ],
      "year": 2020,
      "citation_count": 340,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0a69c8815536a657668e089e3281ff2e963d947a",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
      "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
      "authors": [
        "Zemin Liu",
        "Xingtong Yu",
        "Yuan Fang",
        "Xinming Zhang"
      ],
      "year": 2023,
      "citation_count": 176,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b20589941cd52d199ba381b92e092ba7fb36d689",
      "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
      "abstract": "In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint. Furthermore, We also discuss open research areas and further improvement scopes.",
      "authors": [
        "Shaswata Mitra",
        "Trisha Chakraborty",
        "Subash Neupane",
        "Aritran Piplai",
        "Sudip Mittal"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b20589941cd52d199ba381b92e092ba7fb36d689",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks?",
      "authors": [
        "Ziniu Hu",
        "Yuxiao Dong",
        "Kuansan Wang",
        "Kai-Wei Chang",
        "Yizhou Sun"
      ],
      "year": 2020,
      "citation_count": 591,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.",
      "authors": [
        "Gaspard Michel",
        "Giannis Nikolentzos",
        "J. Lutzeyer",
        "M. Vazirgiannis"
      ],
      "year": 2023,
      "citation_count": 35,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
      "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
      "authors": [
        "Antonio Longa",
        "Veronica Lachi",
        "G. Santin",
        "M. Bianchini",
        "B. Lepri",
        "P. Lió",
        "F. Scarselli",
        "Andrea Passerini"
      ],
      "year": 2023,
      "citation_count": 71,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b88f456daaf29860d2b59c621be3bd878a581a59",
      "pdf_link": "",
      "venue": "Trans. Mach. Learn. Res.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "title": "TDGIA: Effective Injection Attacks on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks. In this paper, we study a recently-introduced realistic attack scenario on graphs---graph injection attack (GIA). In the GIA scenario, the adversary is not able to modify the existing link structure and node attributes of the input graph, instead the attack is performed by injecting adversarial nodes into it. We present an analysis on the topological vulnerability of GNNs under GIA setting, based on which we propose the Topological Defective Graph Injection Attack (TDGIA) for effective injection attacks. TDGIA first introduces the topological defective edge selection strategy to choose the original nodes for connecting with the injected ones. It then designs the smooth feature optimization objective to generate the features for the injected nodes. Extensive experiments on large-scale datasets show that TDGIA can consistently and significantly outperform various attack baselines in attacking dozens of defense GNN models. Notably, the performance drop on target GNNs resultant from TDGIA is more than double the damage brought by the best attack solution among hundreds of submissions on KDD-CUP 2020.",
      "authors": [
        "Xu Zou",
        "Qinkai Zheng",
        "Yuxiao Dong",
        "Xinyu Guan",
        "E. Kharlamov",
        "Jialiang Lu",
        "Jie Tang"
      ],
      "year": 2021,
      "citation_count": 115,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/218223e91f55a1e0186f5b008b55f5e0fe350698",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "993377a3fc8334558463b82053904e3d684f29c0",
      "title": "SIGN: Scalable Inception Graph Neural Networks",
      "abstract": "Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time.",
      "authors": [
        "Emanuele Rossi",
        "Fabrizio Frasca",
        "B. Chamberlain",
        "D. Eynard",
        "M. Bronstein",
        "Federico Monti"
      ],
      "year": 2020,
      "citation_count": 415,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/993377a3fc8334558463b82053904e3d684f29c0",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
      "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions\n made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.\n Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
      "authors": [
        "Zaixin Zhang",
        "Qi Liu",
        "Hao Wang",
        "Chengqiang Lu",
        "Chee-Kong Lee"
      ],
      "year": 2021,
      "citation_count": 140,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7de413da6e0a00e14270cfaed2a31666e7c28747",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "title": "Evaluating explainability for graph neural networks",
      "abstract": "As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, Shape GG en , which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows Shape GG en to mimic the data in various real-world areas. We include Shape GG en and several real-world graph datasets in a graph explainability library, G raph XAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, G raph XAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.",
      "authors": [
        "Chirag Agarwal",
        "Owen Queen",
        "Himabindu Lakkaraju",
        "M. Zitnik"
      ],
      "year": 2022,
      "citation_count": 131,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "pdf_link": "",
      "venue": "Scientific Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "title": "Link Prediction Based on Graph Neural Networks",
      "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",
      "authors": [
        "Muhan Zhang",
        "Yixin Chen"
      ],
      "year": 2018,
      "citation_count": 2060,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2a85846fd827a157b624ee012e75cbe37344281c",
      "title": "Theory of Graph Neural Networks: Representation and Learning",
      "abstract": "Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections.",
      "authors": [
        "S. Jegelka"
      ],
      "year": 2022,
      "citation_count": 76,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2a85846fd827a157b624ee012e75cbe37344281c",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "title": "Spatio-Spectral Graph Neural Networks",
      "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
      "authors": [
        "Simon Geisler",
        "Arthur Kosmala",
        "Daniel Herbst",
        "Stephan Gunnemann"
      ],
      "year": 2024,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
      "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
      "authors": [
        "Ana Lucic",
        "Maartje ter Hoeve",
        "Gabriele Tolomei",
        "M. de Rijke",
        "F. Silvestri"
      ],
      "year": 2021,
      "citation_count": 155,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/11b9f4729c8e355dec7122993076f6e2788c03c4",
      "pdf_link": "",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "title": "Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels",
      "abstract": "Graph Neural Networks (GNNs) have shown their great ability in modeling graph structured data. However, real-world graphs usually contain structure noises and have limited labeled nodes. The performance of GNNs would drop significantly when trained on such graphs, which hinders the adoption of GNNs on many applications. Thus, it is important to develop noise-resistant GNNs with limited labeled nodes. However, the work on this is rather limited. Therefore, we study a novel problem of developing robust GNNs on noisy graphs with limited labeled nodes. Our analysis shows that both the noisy edges and limited labeled nodes could harm the message-passing mechanism of GNNs. To mitigate these issues, we propose a novel framework which adopts the noisy edges as supervision to learn a denoised and dense graph, which can down-weight or eliminate noisy edges and facilitate message passing of GNNs to alleviate the issue of limited labeled nodes. The generated edges are further used to regularize the predictions of unlabeled nodes with label smoothness to better train GNNs. Experimental results on real-world datasets demonstrate the robustness of the proposed framework on noisy graphs with limited labeled nodes.",
      "authors": [
        "Enyan Dai",
        "Wei-dong Jin",
        "Hui Liu",
        "Suhang Wang"
      ],
      "year": 2022,
      "citation_count": 105,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "pdf_link": "",
      "venue": "Web Search and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "title": "Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis",
      "abstract": "Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.",
      "authors": [
        "Hejie Cui",
        "Wei Dai",
        "Yanqiao Zhu",
        "Xiaoxiao Li",
        "Lifang He",
        "Carl Yang"
      ],
      "year": 2022,
      "citation_count": 92,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "pdf_link": "",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
      "abstract": "Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users’ trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
      "authors": [
        "Enyan Dai",
        "Tianxiang Zhao",
        "Huaisheng Zhu",
        "Jun Xu",
        "Zhimeng Guo",
        "Hui Liu",
        "Jiliang Tang",
        "Suhang Wang"
      ],
      "year": 2022,
      "citation_count": 159,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "pdf_link": "",
      "venue": "Machine Intelligence Research",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "398d6f4432e6aa7acf21c0bbaaebac48998faad3",
      "title": "Graph Neural Networks for Social Recommendation",
      "abstract": "In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.",
      "authors": [
        "Wenqi Fan",
        "Yao Ma",
        "Qing Li",
        "Yuan He",
        "Y. Zhao",
        "Jiliang Tang",
        "Dawei Yin"
      ],
      "year": 2019,
      "citation_count": 2019,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/398d6f4432e6aa7acf21c0bbaaebac48998faad3",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "741a7faf9dbefd418cda878c61c5b839ecc02977",
      "title": "A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective",
      "abstract": "Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (e.g., social network analysis and recommender systems), computer vision (e.g., object detection and point cloud learning), and natural language processing (e.g., relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, i.e., 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.",
      "authors": [
        "Chaoqi Chen",
        "Yushuang Wu",
        "Qiyuan Dai",
        "Hong-Yu Zhou",
        "Mutian Xu",
        "Sibei Yang",
        "Xiaoguang Han",
        "Yizhou Yu"
      ],
      "year": 2022,
      "citation_count": 97,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/741a7faf9dbefd418cda878c61c5b839ecc02977",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "35792d528bd07aed95df46f0ecb87019cb123147",
      "title": "Towards Inductive and Efficient Explanations for Graph Neural Networks",
      "abstract": "Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging and nascent problem. The leading method mainly considers the local explanations, i.e., important subgraph structure and node features, to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized at the instance level. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, training the explanation model explaining for each instance is time-consuming for large-scale real-life datasets. In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which renders PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has better generalization ability and can be utilized in an inductive setting without training the model for new instances. Thus, PGExplainer is much more efficient than the leading method with significant speed-up. In addition, the explanation networks can also be utilized as a regularizer to improve the generalization power of existing GNNs when jointly trained with downstream tasks. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7% relative improvement in AUC on explaining graph classification over the leading baseline.",
      "authors": [
        "Dongsheng Luo",
        "Tianxiang Zhao",
        "Wei Cheng",
        "Dongkuan Xu",
        "Feng Han",
        "Wenchao Yu",
        "Xiao Liu",
        "Haifeng Chen",
        "Xiang Zhang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/35792d528bd07aed95df46f0ecb87019cb123147",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
      "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing -- aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
      "authors": [
        "Jiarui Feng",
        "Yixin Chen",
        "Fuhai Li",
        "Anindya Sarkar",
        "Muhan Zhang"
      ],
      "year": 2022,
      "citation_count": 126,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/caf8927cf3c872698a0e97591a1205ba577bbba5",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "title": "A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions",
      "abstract": "Recommender system is one of the most important information services on today’s Internet. Recently, graph neural networks have become the new state-of-the-art approach to recommender systems. In this survey, we conduct a comprehensive review of the literature on graph neural network-based recommender systems. We first introduce the background and the history of the development of both recommender systems and graph neural networks. For recommender systems, in general, there are four aspects for categorizing existing works: stage, scenario, objective, and application. For graph neural networks, the existing methods consist of two categories: spectral models and spatial ones. We then discuss the motivation of applying graph neural networks into recommender systems, mainly consisting of the high-order connectivity, the structural property of data and the enhanced supervision signal. We then systematically analyze the challenges in graph construction, embedding propagation/aggregation, model optimization, and computation efficiency. Afterward and primarily, we provide a comprehensive overview of a multitude of existing works of graph neural network-based recommender systems, following the taxonomy above. Finally, we raise discussions on the open problems and promising future directions in this area. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/GNN-Recommender-Systems.",
      "authors": [
        "Chen Gao",
        "Yu Zheng",
        "Nian Li",
        "Yinfeng Li",
        "Yingrong Qin",
        "J. Piao",
        "Yuhan Quan",
        "Jianxin Chang",
        "Depeng Jin",
        "Xiangnan He",
        "Yong Li"
      ],
      "year": 2021,
      "citation_count": 528,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/071e053890765ecc2ff8ef9054e9c75ec135e167",
      "pdf_link": "",
      "venue": "Trans. Recomm. Syst.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "title": "Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion",
      "abstract": "Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately. Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.",
      "authors": [
        "Haotian Ju",
        "Dongyue Li",
        "Aneesh Sharma",
        "Hongyang Zhang"
      ],
      "year": 2023,
      "citation_count": 42,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fcdd4300f937cef11af297329ed4bd2b611871e7",
      "pdf_link": "",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "title": "Adversarial Training for Graph Neural Networks",
      "abstract": "Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.",
      "authors": [
        "Lukas Gosch",
        "Simon Geisler",
        "Daniel Sturm",
        "Bertrand Charpentier",
        "Daniel Zugner",
        "Stephan Gunnemann"
      ],
      "year": 2023,
      "citation_count": 37,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "title": "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions",
      "abstract": "Deep learning has seen significant growth recently and is now applied to a wide range of conventional use cases, including graphs. Graph data provides relational information between elements and is a standard data format for various machine learning and deep learning tasks. Models that can learn from such inputs are essential for working with graph data effectively. This paper identifies nodes and edges within specific applications, such as text, entities, and relations, to create graph structures. Different applications may require various graph neural network (GNN) models. GNNs facilitate the exchange of information between nodes in a graph, enabling them to understand dependencies within the nodes and edges. The paper delves into specific GNN models like graph convolution networks (GCNs), GraphSAGE, and graph attention networks (GATs), which are widely used in various applications today. It also discusses the message-passing mechanism employed by GNN models and examines the strengths and limitations of these models in different domains. Furthermore, the paper explores the diverse applications of GNNs, the datasets commonly used with them, and the Python libraries that support GNN models. It offers an extensive overview of the landscape of GNN research and its practical implementations.",
      "authors": [
        "Bharti Khemani",
        "S. Patil",
        "K. Kotecha",
        "Sudeep Tanwar"
      ],
      "year": 2024,
      "citation_count": 217,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1fad14bcfc2b75797c686a5a05779076437a683e",
      "pdf_link": "",
      "venue": "Journal of Big Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "title": "The expressive power of pooling in Graph Neural Networks",
      "abstract": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.",
      "authors": [
        "F. Bianchi",
        "Veronica Lachi"
      ],
      "year": 2023,
      "citation_count": 39,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "title": "Nested Graph Neural Networks",
      "abstract": "Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant-factor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.",
      "authors": [
        "Muhan Zhang",
        "Pan Li"
      ],
      "year": 2021,
      "citation_count": 176,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/775a6e0f9104b282ed867871d743e3afd1e66d96",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "title": "On the Expressive Power of Geometric Graph Neural Networks",
      "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
      "authors": [
        "Chaitanya K. Joshi",
        "Simon V. Mathis"
      ],
      "year": 2023,
      "citation_count": 103,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5e6db511e736f77f844bbeebaa2b177427abada1",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "title": "Graph Neural Networks for Text Classification: A Survey",
      "abstract": "Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchmarks. Note that we present a comprehensive comparison between different techniques and identify the pros and cons of various evaluation metrics in this survey.",
      "authors": [
        "Kunze Wang",
        "Yihao Ding",
        "S. Han"
      ],
      "year": 2023,
      "citation_count": 39,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "pdf_link": "",
      "venue": "Artificial Intelligence Review",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "title": "Interpreting and Unifying Graph Neural Networks with An Optimization Framework",
      "abstract": "Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naïve graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",
      "authors": [
        "Meiqi Zhu",
        "Xiao Wang",
        "C. Shi",
        "Houye Ji",
        "Peng Cui"
      ],
      "year": 2021,
      "citation_count": 211,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4b776e7f26464e5b230c1679560f12618730dcc6",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "title": "A Review of Graph Neural Networks in Epidemic Modeling",
      "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
      "authors": [
        "Zewen Liu",
        "Guancheng Wan",
        "B. A. Prakash",
        "M. S. Lau",
        "Wei Jin"
      ],
      "year": 2024,
      "citation_count": 60,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "title": "Graph Structure Learning for Robust Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses. The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",
      "authors": [
        "Wei Jin",
        "Yao Ma",
        "Xiaorui Liu",
        "Xianfeng Tang",
        "Suhang Wang",
        "Jiliang Tang"
      ],
      "year": 2020,
      "citation_count": 748,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "title": "Graph Neural Networks in Recommender Systems: A Survey",
      "abstract": "With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.",
      "authors": [
        "Shiwen Wu",
        "Fei Sun",
        "Fei Sun",
        "Bin Cui"
      ],
      "year": 2020,
      "citation_count": 1358,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "pdf_link": "",
      "venue": "ACM Computing Surveys",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "title": "Unnoticeable Backdoor Attacks on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.",
      "authors": [
        "Enyan Dai",
        "M. Lin",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "year": 2023,
      "citation_count": 64,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "title": "Learning Invariant Representations of Graph Neural Networks via Cluster Generalization",
      "abstract": "Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.",
      "authors": [
        "Donglin Xia",
        "Xiao Wang",
        "Nian Liu",
        "Chuan Shi"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c9845a625e2dac5e32db172d353f81d377760a5f",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "title": "Dirichlet Energy Constrained Learning for Deep Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) integrate deep architectures and topological structure modeling in an effective way. However, the performance of existing GNNs would decrease significantly when they stack many layers, because of the over-smoothing issue. Node embeddings tend to converge to similar vectors when GNNs keep recursively aggregating the representations of neighbors. To enable deep GNNs, several methods have been explored recently. But they are developed from either techniques in convolutional neural networks or heuristic strategies. There is no generalizable and theoretical principle to guide the design of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by leveraging the Dirichlet energy of node embeddings, and propose a generalizable principle to guide the training of deep GNNs. Based on it, a novel deep GNN framework -- EGNN is designed. It could provide lower and upper constraints in terms of Dirichlet energy at each layer to avoid over-smoothing. Experimental results demonstrate that EGNN achieves state-of-the-art performance by using deep layers.",
      "authors": [
        "Kaixiong Zhou",
        "Xiao Huang",
        "D. Zha",
        "Rui Chen",
        "Li Li",
        "Soo-Hyun Choi",
        "Xia Hu"
      ],
      "year": 2021,
      "citation_count": 128,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/faa6fce9a16925eb3091271281f923bc95291ebb",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "title": "Identity-aware Graph Neural Networks",
      "abstract": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes’ identities during message passing. To embed a given node, ID-GNN first extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified but faster version of ID-GNN that injects node identity information as augmented node features. Alto- gether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs demonstrate improved or comparable performance over other task-specific graph networks.",
      "authors": [
        "Jiaxuan You",
        "Jonathan M. Gomes-Selman",
        "Rex Ying",
        "J. Leskovec"
      ],
      "year": 2021,
      "citation_count": 274,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/44b9f16ba417b90e2e7c42f9074378dd06415809",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
      "title": "How Powerful are Graph Neural Networks?",
      "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
      "authors": [
        "Keyulu Xu",
        "Weihua Hu",
        "J. Leskovec",
        "S. Jegelka"
      ],
      "year": 2018,
      "citation_count": 8157,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "24b2aed0f130e5278325b5055711de44d247460e",
      "title": "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure",
      "abstract": "Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability. By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspiring by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly, we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability. Code and data are available at: https://github.com/googlebaba/DisC.",
      "authors": [
        "Shaohua Fan",
        "Xiao Wang",
        "Yanhu Mo",
        "Chuan Shi",
        "Jian Tang"
      ],
      "year": 2022,
      "citation_count": 109,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/24b2aed0f130e5278325b5055711de44d247460e",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "123139463809b5acf98b95d4c8e958be334a32b5",
      "title": "On Explainability of Graph Neural Networks via Subgraph Explorations",
      "abstract": "We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.",
      "authors": [
        "Hao Yuan",
        "Haiyang Yu",
        "Jie Wang",
        "Kang Li",
        "Shuiwang Ji"
      ],
      "year": 2021,
      "citation_count": 431,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/123139463809b5acf98b95d4c8e958be334a32b5",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "target": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "weight": 0.3034041111274235
    },
    {
      "source": "707142f242ee4e40489062870ca53810cb33d404",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.5835799821895081
    },
    {
      "source": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "target": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "weight": 0.05589604852236925
    },
    {
      "source": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.3569036477867422
    },
    {
      "source": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "target": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "weight": 0.0616603423651383
    },
    {
      "source": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "target": "7456dea3a3646f2df6392773a196a5abd0d53b11",
      "weight": 0.2167519384284699
    },
    {
      "source": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.2993428240239445
    },
    {
      "source": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.2511256977326387
    },
    {
      "source": "6c96c2d4a3fbd572fef2d59cb856521ee1746789",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.24309660480930656
    },
    {
      "source": "6c96c2d4a3fbd572fef2d59cb856521ee1746789",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.28380234520780595
    },
    {
      "source": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.3053960100588009
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.2687040919643823
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.3272139207491768
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.37685118398572043
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "weight": 0.32312318915504157
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.337894996820586
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "weight": 0.27910866100913634
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.27581997559458693
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.31750717447923327
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "weight": 0.292908931827871
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "weight": 0.2945394313592089
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "weight": 0.2651531180418707
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "aafe1338caef4682069e92378f1190785ec24c2c",
      "weight": 0.3167733509581203
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "db5d583782264529456a475ce8e9a90823b3a2b5",
      "weight": 0.2446625415034586
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "weight": 0.2688671341367862
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "weight": 0.07926078555458943
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "weight": 0.29994830486017793
    },
    {
      "source": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "target": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "weight": 0.3014614505534322
    },
    {
      "source": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.09364914759296031
    },
    {
      "source": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "target": "4fa31616b834c377c4995c346a2b17464f25692a",
      "weight": 0.09330260908015098
    },
    {
      "source": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "target": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "weight": 0.04592317368741353
    },
    {
      "source": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.10640981479430464
    },
    {
      "source": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.29495316480102607
    },
    {
      "source": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.31126085075206134
    },
    {
      "source": "0d4184cff17f093e0487b27180be515c385feff6",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.06706171825086443
    },
    {
      "source": "0d4184cff17f093e0487b27180be515c385feff6",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.25279872189478414
    },
    {
      "source": "0d4184cff17f093e0487b27180be515c385feff6",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.28772212390224006
    },
    {
      "source": "0d4184cff17f093e0487b27180be515c385feff6",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.022918861435199823
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.24745567734654256
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.22891083953377364
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.2626477488135156
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "weight": 0.2316828184103534
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.23506822694450424
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.30727894787473065
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.03906381054401055
    },
    {
      "source": "ac225094aab9e7b629bc5b3343e026dea0200c70",
      "target": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "weight": 0.2412013493425573
    },
    {
      "source": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2909652333045765
    },
    {
      "source": "db5d583782264529456a475ce8e9a90823b3a2b5",
      "target": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "weight": 0.03031915759268226
    },
    {
      "source": "db5d583782264529456a475ce8e9a90823b3a2b5",
      "target": "3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "weight": 0.5658839280686667
    },
    {
      "source": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.3645381749965994
    },
    {
      "source": "b4895de425a02af87713bd78ed1a29fe425753af",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.05695194111927432
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.2203892591082713
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.2561479255688501
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "weight": 0.04280953705437394
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.23715428439219272
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.10954815778515209
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "weight": 0.31222763500556383
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "weight": 0.23338252936292792
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2755072975906969
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.3043322627584866
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.05916080945441117
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "d09608593caa20b79a8aaddfe19df7e31513d711",
      "weight": 0.2384980415667088
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "weight": 0.2379526176090735
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "weight": 0.2912097161832733
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "db5d583782264529456a475ce8e9a90823b3a2b5",
      "weight": 0.25430021748512077
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "weight": 0.41989684283780393
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "123139463809b5acf98b95d4c8e958be334a32b5",
      "weight": 0.24108156311514728
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "weight": 0.036907565466481766
    },
    {
      "source": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.06369749632344676
    },
    {
      "source": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.11192599593545295
    },
    {
      "source": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.25343934018382897
    },
    {
      "source": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.25343934018382897
    },
    {
      "source": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.36890838159061423
    },
    {
      "source": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.3790096648477488
    },
    {
      "source": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "target": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "weight": 0.23513666480814405
    },
    {
      "source": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "target": "993377a3fc8334558463b82053904e3d684f29c0",
      "weight": 0.2840909279633011
    },
    {
      "source": "f55781f7ce6fd31e946f0efe76d5bf89858391d1",
      "target": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "weight": 0.251556296243836
    },
    {
      "source": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "target": "458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "weight": 0.3877455162938268
    },
    {
      "source": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "target": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "weight": 0.26880600331942506
    },
    {
      "source": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.28430139829347656
    },
    {
      "source": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.3230759824376244
    },
    {
      "source": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "target": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "weight": 0.30805633247888414
    },
    {
      "source": "75e924bd79d27a23f3f93d9b1ab62a779505c8d2",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.2904116233744292
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.3922562768064789
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "weight": 0.2627004377346991
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.32067512044313634
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.33001663588890046
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.28974214712808727
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.26703985636348737
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.07837552826030046
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "weight": 0.22492773709024344
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "weight": 0.3212561655357744
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "aafe1338caef4682069e92378f1190785ec24c2c",
      "weight": 0.35499401274419917
    },
    {
      "source": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "target": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "weight": 0.06698102218366984
    },
    {
      "source": "4fa31616b834c377c4995c346a2b17464f25692a",
      "target": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "weight": 0.13003817872196166
    },
    {
      "source": "4fa31616b834c377c4995c346a2b17464f25692a",
      "target": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "weight": 0.3089333229717089
    },
    {
      "source": "4fa31616b834c377c4995c346a2b17464f25692a",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.1373486286818223
    },
    {
      "source": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "target": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "weight": 0.04365753537797497
    },
    {
      "source": "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e",
      "target": "252351936bd6fabf4b6cd2962fa0ee613772278d",
      "weight": 0.015487145980645572
    },
    {
      "source": "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.33557088717756267
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.31576337629446777
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.2414950119135531
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.2528381438361042
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.3723268512142247
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2896657753360148
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.33467499410845414
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "weight": 0.34116938375074013
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "weight": 0.06966571435922307
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "weight": 0.2890616659851905
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "123139463809b5acf98b95d4c8e958be334a32b5",
      "weight": 0.3810469947860644
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "weight": 0.10360183104369469
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "weight": 0.3101948898008551
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "weight": 0.2625457136305905
    },
    {
      "source": "75c8466a0c1c3b9fe595efc83671984ef95bd679",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.07573281308916442
    },
    {
      "source": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.2397901130961067
    },
    {
      "source": "3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.2365864223473686
    },
    {
      "source": "3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "target": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "weight": 0.4709543938968133
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.07765968897630227
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.3582828594419274
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.3198490228032391
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "weight": 0.32676125813636303
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.28078837055065825
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.3003748401109385
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.3708206960671391
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.31380544900617413
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "weight": 0.28259280487405164
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "0d4184cff17f093e0487b27180be515c385feff6",
      "weight": 0.2853066584335143
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "weight": 0.28005316653547585
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "weight": 0.04889364067642127
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "aafe1338caef4682069e92378f1190785ec24c2c",
      "weight": 0.2608364617014143
    },
    {
      "source": "aa1cda29362b9d670d602c7fc6964499d2a364bd",
      "target": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "weight": 0.008337743295613811
    },
    {
      "source": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.29449381013912246
    },
    {
      "source": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.22552963428196884
    },
    {
      "source": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "target": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "weight": 0.2868883465504882
    },
    {
      "source": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "target": "24b2aed0f130e5278325b5055711de44d247460e",
      "weight": 0.37113232388079853
    },
    {
      "source": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.22298172586951998
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.3022573882854318
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.04546296508948148
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.41109895248889844
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.43116189764377344
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "weight": 0.2798142559834801
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "weight": 0.30415345629625534
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.09584167309397677
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.3478735083894565
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.24303633247236878
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.2274687777880354
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "weight": 0.05352004384433118
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.28434889133607033
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.2725988071469916
    },
    {
      "source": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.3851154155103795
    },
    {
      "source": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.2835076651990257
    },
    {
      "source": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "target": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "weight": 0.2573995815153124
    },
    {
      "source": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.29477158869547937
    },
    {
      "source": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.27124750474816806
    },
    {
      "source": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "target": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "weight": 0.2649248215846347
    },
    {
      "source": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "target": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "weight": 0.35064030182309036
    },
    {
      "source": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.25541316153388255
    },
    {
      "source": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.3273056952635736
    },
    {
      "source": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2869315185516653
    },
    {
      "source": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2939639128571139
    },
    {
      "source": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "target": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "weight": 0.24226394713066884
    },
    {
      "source": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.25443973951358345
    },
    {
      "source": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.306324100295024
    },
    {
      "source": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "target": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "weight": 0.3693595902610631
    },
    {
      "source": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "target": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "weight": 0.31169351714712557
    },
    {
      "source": "252351936bd6fabf4b6cd2962fa0ee613772278d",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.11444980227731695
    },
    {
      "source": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "target": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "weight": 0.04065326512561852
    },
    {
      "source": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.24694554859427129
    },
    {
      "source": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "target": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "weight": 0.27524777076069107
    },
    {
      "source": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.27088686120894
    },
    {
      "source": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "target": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "weight": 0.508354644139733
    },
    {
      "source": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "target": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "weight": 0.2945394313592089
    },
    {
      "source": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.24964177705158358
    },
    {
      "source": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47",
      "target": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "weight": 0.35426686522163775
    },
    {
      "source": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47",
      "target": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "weight": 0.42700840215178937
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.2508829283155436
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.22147666814290534
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.3098175392969032
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.03174841104179996
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "24b2aed0f130e5278325b5055711de44d247460e",
      "weight": 0.22572770285432683
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "weight": 0.25931618780071336
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "weight": 0.23157423769615332
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.22731350607590703
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "weight": 0.24053394410923168
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2396047313583192
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "weight": 0.26442385644334543
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "123139463809b5acf98b95d4c8e958be334a32b5",
      "weight": 0.2466776584877866
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "weight": 0.06786521247828944
    },
    {
      "source": "a8ae2d8232db04d88cf622e5fabd11da3163aa8f",
      "target": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "weight": 0.22046062355708199
    },
    {
      "source": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.38253089764743986
    },
    {
      "source": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.617223036931845
    },
    {
      "source": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.6295763220110167
    },
    {
      "source": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.3799176692683899
    },
    {
      "source": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.5532020534454427
    },
    {
      "source": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.5713041804680468
    },
    {
      "source": "94497472eecb7530a2b75c564548c540ebd61e9b",
      "target": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "weight": 0.5912286219198434
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.2654168265918803
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.3404408593556459
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.2230260719077228
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.08648321716854725
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "weight": 0.23257289461104516
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.23787268162355052
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "weight": 0.040232506893483774
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "b4895de425a02af87713bd78ed1a29fe425753af",
      "weight": 0.0815494110269558
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "weight": 0.2641065939128362
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "weight": 0.3194780814518645
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "weight": 0.015339099344208268
    },
    {
      "source": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "target": "993377a3fc8334558463b82053904e3d684f29c0",
      "weight": 0.25236017838674984
    },
    {
      "source": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.3639162462685907
    },
    {
      "source": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.36799769419441175
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.38504990805670936
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.06995679524777977
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.6380367250283743
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "weight": 0.2848422326580188
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.6450072081051033
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "weight": 0.6579816969702452
    },
    {
      "source": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2609845014881752
    },
    {
      "source": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.2810090603654869
    },
    {
      "source": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "target": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "weight": 0.2370083927445338
    },
    {
      "source": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "target": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "weight": 0.2951128264656909
    },
    {
      "source": "27d5be9322d71b6fd2faa8a6b87250127a12c0cf",
      "target": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "weight": 0.30788368776068287
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.28185767466439465
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "weight": 0.47830930099064545
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.2370339933314593
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2578071086865653
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "weight": 0.054004067591467766
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "weight": 0.369457871434237
    },
    {
      "source": "94194703e83b5447f519fd8bcbb903916e05aaf9",
      "target": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "weight": 0.26294693815194237
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "weight": 0.3385415570843774
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.2731753884210883
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "weight": 0.5830088446871888
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.28056655067711245
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.3578959718714332
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "4fa31616b834c377c4995c346a2b17464f25692a",
      "weight": 0.05635355067338987
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "weight": 0.4212849840733924
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "weight": 0.04299904399843133
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02",
      "weight": 0.3096452112798626
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "weight": 0.34456298175541533
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "weight": 0.26066737846076127
    },
    {
      "source": "0b33c9c2eec5e7a71e1c051ec76e601e76152146",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.04507825623719482
    },
    {
      "source": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.2935021297571618
    },
    {
      "source": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.24238559809545868
    },
    {
      "source": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.29700764700286125
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.24975186911083286
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.24750071101743712
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "weight": 0.2657500563041579
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "weight": 0.4295717061518116
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.261862153387902
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.26923544253921594
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2572645861401872
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.3046027960719945
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "0d4184cff17f093e0487b27180be515c385feff6",
      "weight": 0.2990956087664507
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "5542d0ff99767f75f8c8a329fc3d88d73ff470c3",
      "weight": 0.2960056720200258
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "weight": 0.009918561115453186
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "3bfa808ce20b2736708c3fc0b9443635e3f133a7",
      "weight": 0.31053815938465235
    },
    {
      "source": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5",
      "target": "536da0e76290aea9cbe75c29bac096aeb45ef875",
      "weight": 0.26698102218366987
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.2863188023683902
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "weight": 0.2740803772217458
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.217728331532176
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "weight": 0.26732942821361966
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "weight": 0.27981742626618605
    },
    {
      "source": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "target": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "weight": 0.2593533552793985
    },
    {
      "source": "aafe1338caef4682069e92378f1190785ec24c2c",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.35883623200785514
    },
    {
      "source": "aafe1338caef4682069e92378f1190785ec24c2c",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.38341830625762474
    },
    {
      "source": "aafe1338caef4682069e92378f1190785ec24c2c",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.3926811954173127
    },
    {
      "source": "aafe1338caef4682069e92378f1190785ec24c2c",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.35972066396088803
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "weight": 0.39679347671617093
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.33854090286458743
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.26494370259113814
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2664282437446439
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.28131552579787067
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "weight": 0.27555682313641505
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "weight": 0.3027403446515706
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "weight": 0.178620595712421
    },
    {
      "source": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "target": "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d",
      "weight": 0.23888071476278483
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "weight": 0.3431967255091272
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.23659724802289955
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "weight": 0.2828560552899719
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2608032041641622
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.36648826937283796
    },
    {
      "source": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.10578900198507854
    },
    {
      "source": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "target": "741a7faf9dbefd418cda878c61c5b839ecc02977",
      "weight": 0.024559349167646072
    },
    {
      "source": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.24293243716307633
    },
    {
      "source": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.3023255947646943
    },
    {
      "source": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.3646970538038318
    },
    {
      "source": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.09409643384000815
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.22224993867662718
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "b20589941cd52d199ba381b92e092ba7fb36d689",
      "weight": 0.053498570600799523
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "weight": 0.3942852470262781
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.3537481753281947
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.33010124210739145
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.24111035049954838
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2716524524312115
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "weight": 0.23345005672963787
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "weight": 0.13967300704591717
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "weight": 0.27704577809874303
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "weight": 0.2765697805356254
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "weight": 0.30692933418516305
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "weight": 0.4132791378333919
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "weight": 0.3858259135473219
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "weight": 0.4053059858091247
    },
    {
      "source": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
      "target": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "weight": 0.38339463990937184
    },
    {
      "source": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "target": "252351936bd6fabf4b6cd2962fa0ee613772278d",
      "weight": 0.045564011537552906
    },
    {
      "source": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.22515290713902916
    },
    {
      "source": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.2436386597645416
    },
    {
      "source": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "target": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "weight": 0.2354707539508235
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "weight": 0.47478968513804665
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.24605780748000228
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.39728306212572045
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.30299660577267085
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.3112915841614548
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.30139877343992666
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18",
      "weight": 0.22254088129276228
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "weight": 0.06238272285482699
    },
    {
      "source": "341880efaef452f631a4a5cd61bef5dae47741d7",
      "target": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "weight": 0.3398302472958876
    },
    {
      "source": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.27206193419149827
    },
    {
      "source": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "target": "d09608593caa20b79a8aaddfe19df7e31513d711",
      "weight": 0.2907445938246057
    },
    {
      "source": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "target": "454304628bf10f02aba1c2cfc95891e94d09208e",
      "weight": 0.2069221389909243
    },
    {
      "source": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.019356619088273234
    },
    {
      "source": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.28984578604058187
    },
    {
      "source": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "target": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "weight": 0.035934949093586646
    },
    {
      "source": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
      "target": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
      "weight": 0.24237652974510795
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.2923594072538632
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "weight": 0.32543767406746144
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "0a8f340f094da212dcb50f310e3bd5fb676e2454",
      "weight": 0.2749338275802302
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "weight": 0.3407167276588461
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "b4895de425a02af87713bd78ed1a29fe425753af",
      "weight": 0.10994411363636555
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "weight": 0.2731167389727685
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "weight": 0.3522115484270696
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "weight": 0.2695492227131097
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "weight": 0.26819110314115147
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "weight": 0.287846240590679
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "weight": 0.2576428410722845
    },
    {
      "source": "639206a9a32d91386924f1c94e9760dfb43df72e",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.09140275886409485
    },
    {
      "source": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.08511146187615228
    },
    {
      "source": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2986737314853698
    },
    {
      "source": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "target": "d09608593caa20b79a8aaddfe19df7e31513d711",
      "weight": 0.2488879044690505
    },
    {
      "source": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.052612709670622496
    },
    {
      "source": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "target": "ace7550acb19dd4b55fd7f10c400de24b1a87d23",
      "weight": 0.26558148662770065
    },
    {
      "source": "d09608593caa20b79a8aaddfe19df7e31513d711",
      "target": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
      "weight": 0.07190631674523902
    },
    {
      "source": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "target": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "weight": 0.2840835314956557
    },
    {
      "source": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "target": "03d1fd385dc204e4e7445c5204ed15bd5e96a99d",
      "weight": 0.3252066485515181
    },
    {
      "source": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "target": "f5aa366ff70215f06ae6501c322eba2f0934a7c3",
      "weight": 0.2756057803884919
    },
    {
      "source": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.21492930711229108
    },
    {
      "source": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.28782365451686975
    },
    {
      "source": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "target": "4becb19c87f0526d9a3a2c15497e0b1c40b576e2",
      "weight": 0.3201015968724399
    },
    {
      "source": "b25b4d70b62d8482c98c2b901f4a7e1600df3a72",
      "target": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "weight": 0.2823566937449645
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "b20589941cd52d199ba381b92e092ba7fb36d689",
      "weight": 0.13649871568016292
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.7189089135827109
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.22588499382472435
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.23064004365416532
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "weight": 0.2381090321337342
    },
    {
      "source": "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee",
      "target": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "weight": 0.3370512176055608
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.32869315003527555
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.30850885834114966
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.30284899323717784
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.06980577238354455
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "b20589941cd52d199ba381b92e092ba7fb36d689",
      "weight": 0.03367307003046814
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.24137716835708972
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.08373235301713744
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "24b2aed0f130e5278325b5055711de44d247460e",
      "weight": 0.2544222408684143
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "weight": 0.3383571817812232
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "81fee2fd4bc007fda9a1b1d81e4de66ded867215",
      "weight": 0.0762986850075589
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "weight": 0.25377460211429614
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.24147830222656472
    },
    {
      "source": "00358a3f17821476d93461192b9229fe7d92bb3f",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2794584257825232
    },
    {
      "source": "0a69c8815536a657668e089e3281ff2e963d947a",
      "target": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "weight": 0.24701439799725652
    },
    {
      "source": "0a69c8815536a657668e089e3281ff2e963d947a",
      "target": "b4895de425a02af87713bd78ed1a29fe425753af",
      "weight": 0.09424340861644175
    },
    {
      "source": "0a69c8815536a657668e089e3281ff2e963d947a",
      "target": "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e",
      "weight": 0.257193762560585
    },
    {
      "source": "0a69c8815536a657668e089e3281ff2e963d947a",
      "target": "d596ac251729fc3647b08b51c5208fdf5414c7c1",
      "weight": 0.028981738551007195
    },
    {
      "source": "0a69c8815536a657668e089e3281ff2e963d947a",
      "target": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "weight": 0.2962433613307117
    },
    {
      "source": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.3717006137337681
    },
    {
      "source": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.6268450011800353
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "14c59d6dab548ef023b8a49df4a26b966fe9d00a",
      "weight": 0.33902679496335014
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.48302896432339926
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.4244513701247638
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "weight": 0.4998604746026214
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2848366514864038
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "8d68eae4068fca5ae3e9660c2a87857c89d30f73",
      "weight": 0.258235966803114
    },
    {
      "source": "04faf433934486c41d082e8d75ccfe5dc2f69fef",
      "target": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "weight": 0.37473972333124433
    },
    {
      "source": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "target": "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf",
      "weight": 0.3128313873084287
    },
    {
      "source": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.23464515633531885
    },
    {
      "source": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.43342046452188854
    },
    {
      "source": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2683141555721555
    },
    {
      "source": "218223e91f55a1e0186f5b008b55f5e0fe350698",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2745167967797872
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "9b451516b9432318d81aef2a5bdc0135d2285a5d",
      "weight": 0.27658894071874385
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.2746810627348083
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "b4895de425a02af87713bd78ed1a29fe425753af",
      "weight": 0.06874965160688892
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "5822490cf59df7f7ccb92b8901f244850b867a66",
      "weight": 0.078535767491257
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "cf30fb61a5943781144c8442563e3ef9c38df871",
      "weight": 0.2708360001120115
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.033908479117521824
    },
    {
      "source": "993377a3fc8334558463b82053904e3d684f29c0",
      "target": "d08a0eb7024dff5c4fabd58144a38031633d4e1a",
      "weight": 0.26969949349779243
    },
    {
      "source": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.34166777002929316
    },
    {
      "source": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.2299323843457203
    },
    {
      "source": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.24159737813577054
    },
    {
      "source": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2767911921460624
    },
    {
      "source": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.2563810734568087
    },
    {
      "source": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.15432468427471732
    },
    {
      "source": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "target": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
      "weight": 0.2791666869445334
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "weight": 0.2576447452512309
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.2394075085113545
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "weight": 0.08356898106879414
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.291981897821017
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "f442378ead6282024cf5b9046daa10422fe9fc5f",
      "weight": 0.3615086362625563
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.23498046703245387
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.2629949509105141
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
      "weight": 0.22688301817828416
    },
    {
      "source": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.03596181845285596
    },
    {
      "source": "2a85846fd827a157b624ee012e75cbe37344281c",
      "target": "fcdd4300f937cef11af297329ed4bd2b611871e7",
      "weight": 0.32525616983747235
    },
    {
      "source": "2a85846fd827a157b624ee012e75cbe37344281c",
      "target": "5e6db511e736f77f844bbeebaa2b177427abada1",
      "weight": 0.2632321616664367
    },
    {
      "source": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "target": "24b2aed0f130e5278325b5055711de44d247460e",
      "weight": 0.24152207905885842
    },
    {
      "source": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "target": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "weight": 0.3766292661106131
    },
    {
      "source": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.23788210389489373
    },
    {
      "source": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.08456374631536255
    },
    {
      "source": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.29705521893135073
    },
    {
      "source": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.32680538266139514
    },
    {
      "source": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.2584942375543723
    },
    {
      "source": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2384510704685361
    },
    {
      "source": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
      "target": "3850d1914120c0f4e0a5e10432ee5429982a98b3",
      "weight": 0.4709543938968133
    },
    {
      "source": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "target": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "weight": 0.2979783169938396
    },
    {
      "source": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.48014098820882006
    },
    {
      "source": "398d6f4432e6aa7acf21c0bbaaebac48998faad3",
      "target": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "weight": 0.09072192315132405
    },
    {
      "source": "398d6f4432e6aa7acf21c0bbaaebac48998faad3",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.24443429930516897
    },
    {
      "source": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.20982710938132695
    },
    {
      "source": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.35540178566231073
    },
    {
      "source": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.2811023714165476
    },
    {
      "source": "071e053890765ecc2ff8ef9054e9c75ec135e167",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.5531016596213066
    },
    {
      "source": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "target": "b88f456daaf29860d2b59c621be3bd878a581a59",
      "weight": 0.0425000032423474
    },
    {
      "source": "018abe2e4fa7ed08b4d0556d4e1238d40b89688c",
      "target": "3db15a5534050ab2cfc1d09dd772d032395515e1",
      "weight": 0.3164776826063879
    },
    {
      "source": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.3383805054238262
    },
    {
      "source": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.3017110783773506
    },
    {
      "source": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "target": "85f578d2df32bdc3f42fdaa9b65a1904b680a262",
      "weight": 0.22647706823543354
    },
    {
      "source": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.28602887216895057
    },
    {
      "source": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.2387096375233749
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "c9845a625e2dac5e32db172d353f81d377760a5f",
      "weight": 0.30709804868034185
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "707142f242ee4e40489062870ca53810cb33d404",
      "weight": 0.24938366949598115
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "641828b8ca714a0f70ccdac17d7e9dff485877c2",
      "weight": 0.33405921418183593
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "weight": 0.2945515213560548
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "weight": 0.34603942152191763
    },
    {
      "source": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "target": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
      "weight": 0.26863683024497226
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "900fc1f1d2b9ceeacbc92d74491b0a19c823af20",
      "weight": 0.24323822612809706
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f",
      "weight": 0.01676969542515122
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "c7ac48f6e7a621375785efe3b3f32deec407efb0",
      "weight": 0.25554078568675326
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "3efa96570a10fecba0f93e0f62e95d41ce7b624b",
      "weight": 0.42569113394428293
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.30380740490789626
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.34493329767114217
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f",
      "weight": 0.30649131929227386
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
      "weight": 0.32709932132620056
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "cd551790992d16148fe2e5ff2cc76861195e2191",
      "weight": 0.2757563511563228
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "510b5b370211d2d85d43475d28bfd40fd48a6a22",
      "weight": 0.031352317165989727
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
      "weight": 0.36572028903828746
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "4dc3c61426a3332238ea0feb23f2113a96aef0d4",
      "weight": 0.273411325281985
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "4b776e7f26464e5b230c1679560f12618730dcc6",
      "weight": 0.2653167986449454
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "73366d75289c5e37481639fb54fdee28a664e2b3",
      "weight": 0.3736568132379341
    },
    {
      "source": "011a1bbb4059b703d9b366468ef9effdb49f4df9",
      "target": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
      "weight": 0.29230239105743133
    },
    {
      "source": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "target": "1fad14bcfc2b75797c686a5a05779076437a683e",
      "weight": 0.14807930152811213
    },
    {
      "source": "530cc6baebee5ee9005ec2f5e8629764f43c0f02",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.2979783169938396
    },
    {
      "source": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "target": "90dead8a056b848be164c2e5cdadfa2e191c3265",
      "weight": 0.37055079849470224
    },
    {
      "source": "faa6fce9a16925eb3091271281f923bc95291ebb",
      "target": "e60ad3d4ed3273af6a94745689783b83f59c8b4a",
      "weight": 0.2606724786719252
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "46291f6917088b5cd1ee80f134bf7dfcb2a02868",
      "weight": 0.17404528061635408
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
      "weight": 0.3637506679377678
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac",
      "weight": 0.3276233261218461
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
      "weight": 0.30403909284650166
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "caf8927cf3c872698a0e97591a1205ba577bbba5",
      "weight": 0.4300030235107805
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "2a85846fd827a157b624ee012e75cbe37344281c",
      "weight": 0.30914209656060176
    },
    {
      "source": "44b9f16ba417b90e2e7c42f9074378dd06415809",
      "target": "775a6e0f9104b282ed867871d743e3afd1e66d96",
      "weight": 0.3278360836401442
    },
    {
      "source": "24b2aed0f130e5278325b5055711de44d247460e",
      "target": "458ab8a8a5e139cb744167f5b0890de0b2112b53",
      "weight": 0.3230694781285566
    },
    {
      "source": "24b2aed0f130e5278325b5055711de44d247460e",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.23517151085814367
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "6dc0932670a0b5140a426ca310bbb03783ff2240",
      "weight": 0.27071480625050043
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "854342cf063eef4428a5441c8d317dfbabb8117f",
      "weight": 0.2886054253792204
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "a5ef3aac578a430a5624e666ac5d496175cbd99b",
      "weight": 0.3351777319296808
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "35792d528bd07aed95df46f0ecb87019cb123147",
      "weight": 0.31414142203671347
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c",
      "weight": 0.05149742161327954
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "b20589941cd52d199ba381b92e092ba7fb36d689",
      "weight": 0.00592828054667742
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "24b2aed0f130e5278325b5055711de44d247460e",
      "weight": 0.29887130547835816
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
      "weight": 0.2576983847933402
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
      "weight": 0.22837417726371498
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
      "weight": 0.24527671363190898
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
      "weight": 0.24604028913105616
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "7de413da6e0a00e14270cfaed2a31666e7c28747",
      "weight": 0.2949839514183532
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "11b9f4729c8e355dec7122993076f6e2788c03c4",
      "weight": 0.13941815439180408
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "6ae2967bb0a5e57cc545176120a4845576e068a3",
      "weight": 0.24108156311514728
    },
    {
      "source": "123139463809b5acf98b95d4c8e958be334a32b5",
      "target": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
      "weight": 0.03165335949793262
    }
  ]
}