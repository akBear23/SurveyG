{
    "81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf": {
        "title": "A Comprehensive Survey on Graph Neural Networks",
        "authors": [
            "Zonghan Wu",
            "Shirui Pan",
            "Fengwen Chen",
            "Guodong Long",
            "Chengqi Zhang",
            "Philip S. Yu"
        ],
        "published_date": "2019",
        "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial\u2013temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
        "file_path": "paper_data/Graph_Neural_Networks/info/81a4fd3004df0eb05d6c1cef96ad33d5407820df.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 9135,
        "score": 1522.5
    },
    "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf": {
        "title": "How Powerful are Graph Neural Networks?",
        "authors": [
            "Keyulu Xu",
            "Weihua Hu",
            "J. Leskovec",
            "S. Jegelka"
        ],
        "published_date": "2018",
        "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
        "file_path": "paper_data/Graph_Neural_Networks/info/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 8157,
        "score": 1165.2857142857142
    },
    "ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf": {
        "title": "Graph Neural Networks: A Review of Methods and Applications",
        "authors": [
            "Jie Zhou",
            "Ganqu Cui",
            "Zhengyan Zhang",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693.pdf",
        "venue": "AI Open",
        "citationCount": 5934,
        "score": 847.7142857142857
    },
    "6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf": {
        "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
        "authors": [
            "Rex Ying",
            "Ruining He",
            "Kaifeng Chen",
            "Pong Eksombatchai",
            "William L. Hamilton",
            "J. Leskovec"
        ],
        "published_date": "2018",
        "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6c96c2d4a3fbd572fef2d59cb856521ee1746789.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 3730,
        "score": 532.8571428571428
    },
    "7456dea3a3646f2df6392773a196a5abd0d53b11.pdf": {
        "title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
        "authors": [
            "Simon L. Batzner",
            "Albert Musaelian",
            "Lixin Sun",
            "M. Geiger",
            "J. Mailoa",
            "M. Kornbluth",
            "N. Molinari",
            "T. Smidt",
            "B. Kozinsky"
        ],
        "published_date": "2021",
        "abstract": "This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. An E(3)-equivariant deep learning interatomic potential is introduced for accelerating molecular dynamics simulations. The method obtains state-of-the-art accuracy, can faithfully describe dynamics of complex systems with remarkable sample efficiency.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7456dea3a3646f2df6392773a196a5abd0d53b11.pdf",
        "venue": "Nature Communications",
        "citationCount": 1496,
        "score": 374.0
    },
    "347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf": {
        "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
        "authors": [
            "Paul-Edouard Sarlin",
            "Daniel DeTone",
            "Tomasz Malisiewicz",
            "Andrew Rabinovich"
        ],
        "published_date": "2019",
        "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.",
        "file_path": "paper_data/Graph_Neural_Networks/info/347e837b1aa03c9d17c69a522929000f0a0f0a51.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 2131,
        "score": 355.16666666666663
    },
    "398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf": {
        "title": "Graph Neural Networks for Social Recommendation",
        "authors": [
            "Wenqi Fan",
            "Yao Ma",
            "Qing Li",
            "Yuan He",
            "Y. Zhao",
            "Jiliang Tang",
            "Dawei Yin"
        ],
        "published_date": "2019",
        "abstract": "In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.",
        "file_path": "paper_data/Graph_Neural_Networks/info/398d6f4432e6aa7acf21c0bbaaebac48998faad3.pdf",
        "venue": "The Web Conference",
        "citationCount": 2019,
        "score": 336.5
    },
    "75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf": {
        "title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks",
        "authors": [
            "Zonghan Wu",
            "Shirui Pan",
            "Guodong Long",
            "Jing Jiang",
            "Xiaojun Chang",
            "Chengqi Zhang"
        ],
        "published_date": "2020",
        "abstract": "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.",
        "file_path": "paper_data/Graph_Neural_Networks/info/75e924bd79d27a23f3f93d9b1ab62a779505c8d2.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 1580,
        "score": 316.0
    },
    "e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf": {
        "title": "Link Prediction Based on Graph Neural Networks",
        "authors": [
            "Muhan Zhang",
            "Yixin Chen"
        ],
        "published_date": "2018",
        "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e4715a13f6364b1c81e64f247651c3d9e80b6808.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 2060,
        "score": 294.2857142857143
    },
    "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf": {
        "title": "E(n) Equivariant Graph Neural Networks",
        "authors": [
            "Victor Garcia Satorras",
            "Emiel Hoogeboom",
            "M. Welling"
        ],
        "published_date": "2021",
        "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8ea9cb53779a8c1bb0e53764f88669bd7edf38f0.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 1132,
        "score": 283.0
    },
    "3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf": {
        "title": "Graph Neural Networks in Recommender Systems: A Survey",
        "authors": [
            "Shiwen Wu",
            "Fei Sun",
            "Fei Sun",
            "Bin Cui"
        ],
        "published_date": "2020",
        "abstract": "With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3443efc855cebd17d1512d1a703b6e9ee2e4da8b.pdf",
        "venue": "ACM Computing Surveys",
        "citationCount": 1358,
        "score": 271.6
    },
    "ac225094aab9e7b629bc5b3343e026dea0200c70.pdf": {
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
        "authors": [
            "Johannes Klicpera",
            "Aleksandar Bojchevski",
            "Stephan G\u00fcnnemann"
        ],
        "published_date": "2018",
        "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ac225094aab9e7b629bc5b3343e026dea0200c70.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1769,
        "score": 252.7142857142857
    },
    "789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf": {
        "title": "Strategies for Pre-training Graph Neural Networks",
        "authors": [
            "Weihua Hu",
            "Bowen Liu",
            "Joseph Gomes",
            "M. Zitnik",
            "Percy Liang",
            "V. Pande",
            "J. Leskovec"
        ],
        "published_date": "2019",
        "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.",
        "file_path": "paper_data/Graph_Neural_Networks/info/789a7069d1a2d02d784e4821685b216cc63e6ec8.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1499,
        "score": 249.83333333333331
    },
    "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf": {
        "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
        "authors": [
            "Christopher Morris",
            "Martin Ritzert",
            "Matthias Fey",
            "William L. Hamilton",
            "J. E. Lenssen",
            "Gaurav Rattan",
            "Martin Grohe"
        ],
        "published_date": "2018",
        "abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically\u2014showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 1734,
        "score": 247.7142857142857
    },
    "00358a3f17821476d93461192b9229fe7d92bb3f.pdf": {
        "title": "GNNExplainer: Generating Explanations for Graph Neural Networks",
        "authors": [
            "Rex Ying",
            "Dylan Bourgeois",
            "Jiaxuan You",
            "M. Zitnik",
            "J. Leskovec"
        ],
        "published_date": "2019",
        "abstract": "Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/00358a3f17821476d93461192b9229fe7d92bb3f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 1436,
        "score": 239.33333333333331
    },
    "0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf": {
        "title": "Session-based Recommendation with Graph Neural Networks",
        "authors": [
            "Shu Wu",
            "Yuyuan Tang",
            "Yanqiao Zhu",
            "Liang Wang",
            "Xing Xie",
            "T. Tan"
        ],
        "published_date": "2018",
        "abstract": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0c7e1338a9c7914a3b9a5bdc42b457b3f272160e.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 1635,
        "score": 233.57142857142856
    },
    "21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf": {
        "title": "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs",
        "authors": [
            "Jiong Zhu",
            "Yujun Yan",
            "Lingxiao Zhao",
            "Mark Heimann",
            "L. Akoglu",
            "Danai Koutra"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/21e33bd0ad95ee1f79d8b778e693fd316cbb72d4.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 1079,
        "score": 215.8
    },
    "94194703e83b5447f519fd8bcbb903916e05aaf9.pdf": {
        "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View",
        "authors": [
            "Deli Chen",
            "Yankai Lin",
            "Wei Li",
            "Peng Li",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published_date": "2019",
        "abstract": "Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.",
        "file_path": "paper_data/Graph_Neural_Networks/info/94194703e83b5447f519fd8bcbb903916e05aaf9.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 1180,
        "score": 196.66666666666666
    },
    "381411d740562de1e766dc8cc833844eb99dde01.pdf": {
        "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.",
        "authors": [
            "Minjie Wang",
            "Da Zheng",
            "Zihao Ye",
            "Quan Gan",
            "Mufei Li",
            "Xiang Song",
            "Jinjing Zhou",
            "Chao Ma",
            "Lingfan Yu",
            "Yujie Gai",
            "Tianjun Xiao",
            "Tong He",
            "G. Karypis",
            "Jinyang Li",
            "Zheng Zhang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/381411d740562de1e766dc8cc833844eb99dde01.pdf",
        "venue": "",
        "citationCount": 1171,
        "score": 195.16666666666666
    },
    "24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf": {
        "title": "Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting",
        "authors": [
            "Mengzhang Li",
            "Zhanxing Zhu"
        ],
        "published_date": "2020",
        "abstract": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks usually utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. Furthermore, existing methods were out at elbows when solving complicated spatial-temporal data: they usually utilize separate modules for spatial and temporal correlations, or they only use independent components capturing localized or global heterogeneous dependencies. To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. First, a data-driven method of generating \u201ctemporal graph\u201d is proposed to compensate several genuine correlations that spatial graph may not reflect. STFGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer parallelly, STFGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
        "file_path": "paper_data/Graph_Neural_Networks/info/24cff2aafcd66e1b7be4f647e478e8e73cf410a5.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 849,
        "score": 169.8
    },
    "81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf": {
        "title": "Graph neural networks for materials science and chemistry",
        "authors": [
            "Patrick Reiser",
            "Marlen Neubert",
            "Andr'e Eberhard",
            "Luca Torresi",
            "Chen Zhou",
            "Chen Shao",
            "Houssam Metni",
            "Clint van Hoesel",
            "Henrik Schopmans",
            "T. Sommer",
            "Pascal Friederich"
        ],
        "published_date": "2022",
        "abstract": "Machine learning plays an increasingly important role in many areas of chemistry and materials science, being used to predict materials properties, accelerate simulations, design new structures, and predict synthesis routes of new materials. Graph neural networks (GNNs) are one of the fastest growing classes of machine learning models. They are of particular relevance for chemistry and materials science, as they directly work on a graph or structural representation of molecules and materials and therefore have full access to all relevant information required to characterize materials. In this Review, we provide an overview of the basic principles of GNNs, widely used datasets, and state-of-the-art architectures, followed by a discussion of a wide range of recent applications of GNNs in chemistry and materials science, and concluding with a road-map for the further development and application of GNNs. Graph neural networks are machine learning models that directly access the structural representation of molecules and materials. This Review discusses state-of-the-art architectures and applications of graph neural networks in materials science and chemistry, indicating a possible road-map for their further development.",
        "file_path": "paper_data/Graph_Neural_Networks/info/81fee2fd4bc007fda9a1b1d81e4de66ded867215.pdf",
        "venue": "Communications Materials",
        "citationCount": 492,
        "score": 164.0
    },
    "572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf": {
        "title": "Few-Shot Learning with Graph Neural Networks",
        "authors": [
            "Victor Garcia Satorras",
            "Joan Bruna"
        ],
        "published_date": "2017",
        "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/572a1f77306e160c3893299c18f3ed862fb5f6d9.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1261,
        "score": 157.625
    },
    "3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf": {
        "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
        "authors": [
            "Uri Alon",
            "Eran Yahav"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3bfa808ce20b2736708c3fc0b9443635e3f133a7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 763,
        "score": 152.6
    },
    "011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf": {
        "title": "Graph Structure Learning for Robust Graph Neural Networks",
        "authors": [
            "Wei Jin",
            "Yao Ma",
            "Xiaorui Liu",
            "Xianfeng Tang",
            "Suhang Wang",
            "Jiliang Tang"
        ],
        "published_date": "2020",
        "abstract": "Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses. The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/011a1bbb4059b703d9b366468ef9effdb49f4df9.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 748,
        "score": 149.6
    },
    "00549af4bc3270e0f688acbf694f912d7ee39cad.pdf": {
        "title": "Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks",
        "authors": [
            "Yaqin Zhou",
            "Shangqing Liu",
            "J. Siow",
            "Xiaoning Du",
            "Yang Liu"
        ],
        "published_date": "2019",
        "abstract": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",
        "file_path": "paper_data/Graph_Neural_Networks/info/00549af4bc3270e0f688acbf694f912d7ee39cad.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 849,
        "score": 141.5
    },
    "6ae2967bb0a5e57cc545176120a4845576e068a3.pdf": {
        "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
        "authors": [
            "Hao Yuan",
            "Haiyang Yu",
            "Shurui Gui",
            "Shuiwang Ji"
        ],
        "published_date": "2020",
        "abstract": "Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6ae2967bb0a5e57cc545176120a4845576e068a3.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 670,
        "score": 134.0
    },
    "071e053890765ecc2ff8ef9054e9c75ec135e167.pdf": {
        "title": "A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions",
        "authors": [
            "Chen Gao",
            "Yu Zheng",
            "Nian Li",
            "Yinfeng Li",
            "Yingrong Qin",
            "J. Piao",
            "Yuhan Quan",
            "Jianxin Chang",
            "Depeng Jin",
            "Xiangnan He",
            "Yong Li"
        ],
        "published_date": "2021",
        "abstract": "Recommender system is one of the most important information services on today\u2019s Internet. Recently, graph neural networks have become the new state-of-the-art approach to recommender systems. In this survey, we conduct a comprehensive review of the literature on graph neural network-based recommender systems. We first introduce the background and the history of the development of both recommender systems and graph neural networks. For recommender systems, in general, there are four aspects for categorizing existing works: stage, scenario, objective, and application. For graph neural networks, the existing methods consist of two categories: spectral models and spatial ones. We then discuss the motivation of applying graph neural networks into recommender systems, mainly consisting of the high-order connectivity, the structural property of data and the enhanced supervision signal. We then systematically analyze the challenges in graph construction, embedding propagation/aggregation, model optimization, and computation efficiency. Afterward and primarily, we provide a comprehensive overview of a multitude of existing works of graph neural network-based recommender systems, following the taxonomy above. Finally, we raise discussions on the open problems and promising future directions in this area. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/GNN-Recommender-Systems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/071e053890765ecc2ff8ef9054e9c75ec135e167.pdf",
        "venue": "Trans. Recomm. Syst.",
        "citationCount": 528,
        "score": 132.0
    },
    "639206a9a32d91386924f1c94e9760dfb43df72e.pdf": {
        "title": "Towards Deeper Graph Neural Networks",
        "authors": [
            "Meng Liu",
            "Hongyang Gao",
            "Shuiwang Ji"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/639206a9a32d91386924f1c94e9760dfb43df72e.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 640,
        "score": 128.0
    },
    "5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf": {
        "title": "GemNet: Universal Directional Graph Neural Networks for Molecules",
        "authors": [
            "Johannes Klicpera",
            "Florian Becker",
            "Stephan Gunnemann"
        ],
        "published_date": "2021",
        "abstract": "Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5542d0ff99767f75f8c8a329fc3d88d73ff470c3.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 499,
        "score": 124.75
    },
    "4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf": {
        "title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification",
        "authors": [
            "Kenta Oono",
            "Taiji Suzuki"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/4ce9c20642dce5eb7930966053a1e3da4ef617f2.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 734,
        "score": 122.33333333333333
    },
    "68a024d7b70ef3989a6751678f635cbe754440fc.pdf": {
        "title": "Skeleton-Based Action Recognition With Directed Graph Neural Networks",
        "authors": [
            "Lei Shi",
            "Yifan Zhang",
            "Jian Cheng",
            "Hanqing Lu"
        ],
        "published_date": "2019",
        "abstract": "The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.",
        "file_path": "paper_data/Graph_Neural_Networks/info/68a024d7b70ef3989a6751678f635cbe754440fc.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 717,
        "score": 119.5
    },
    "04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf": {
        "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
        "authors": [
            "Ziniu Hu",
            "Yuxiao Dong",
            "Kuansan Wang",
            "Kai-Wei Chang",
            "Yizhou Sun"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks?",
        "file_path": "paper_data/Graph_Neural_Networks/info/04faf433934486c41d082e8d75ccfe5dc2f69fef.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 591,
        "score": 118.2
    },
    "2b8a207189bc02d73d1dce850bcde24dbd984483.pdf": {
        "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention",
        "authors": [
            "Zhanghao Wu",
            "Paras Jain",
            "Matthew A. Wright",
            "Azalia Mirhoseini",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2b8a207189bc02d73d1dce850bcde24dbd984483.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 336,
        "score": 112.0
    },
    "123139463809b5acf98b95d4c8e958be334a32b5.pdf": {
        "title": "On Explainability of Graph Neural Networks via Subgraph Explorations",
        "authors": [
            "Hao Yuan",
            "Haiyang Yu",
            "Jie Wang",
            "Kang Li",
            "Shuiwang Ji"
        ],
        "published_date": "2021",
        "abstract": "We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.",
        "file_path": "paper_data/Graph_Neural_Networks/info/123139463809b5acf98b95d4c8e958be334a32b5.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 431,
        "score": 107.75
    },
    "b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf": {
        "title": "Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
        "authors": [
            "Ziyang Wang",
            "Wei Wei",
            "G. Cong",
            "Xiaoli Li",
            "Xian-Ling Mao",
            "Minghui Qiu"
        ],
        "published_date": "2020",
        "abstract": "Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b05a5424d0fce45896b6b8a847cf540a38f556bc.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 511,
        "score": 102.2
    },
    "6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf": {
        "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
        "authors": [
            "Daniel Z\u00fcgner",
            "Stephan G\u00fcnnemann"
        ],
        "published_date": "2019",
        "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6f5b1076ebacd30849d86e5f5787e3d43b65911f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 590,
        "score": 98.33333333333333
    },
    "594dc362b4332ae661e3d71da17d097bb4a357dd.pdf": {
        "title": "Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems",
        "authors": [
            "Hongwei Wang",
            "Fuzheng Zhang",
            "Mengdi Zhang",
            "J. Leskovec",
            "Miao Zhao",
            "Wenjie Li",
            "Zhongyuan Wang"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
        "file_path": "paper_data/Graph_Neural_Networks/info/594dc362b4332ae661e3d71da17d097bb4a357dd.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 583,
        "score": 97.16666666666666
    },
    "d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf": {
        "title": "Combinatorial optimization and reasoning with graph neural networks",
        "authors": [
            "Quentin Cappart",
            "D. Ch\u00e9telat",
            "Elias Boutros Khalil",
            "Andrea Lodi",
            "Christopher Morris",
            "Petar Velickovic"
        ],
        "published_date": "2021",
        "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have mostly focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing the former. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at researchers in both optimization and machine learning.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d596ac251729fc3647b08b51c5208fdf5414c7c1.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 388,
        "score": 97.0
    },
    "0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf": {
        "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
        "authors": [
            "Peixiang Zhong",
            "Di Wang",
            "C. Miao"
        ],
        "published_date": "2019",
        "abstract": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0c57e17102896e9bf356e89d5daca93f8ef7a2f7.pdf",
        "venue": "IEEE Transactions on Affective Computing",
        "citationCount": 578,
        "score": 96.33333333333333
    },
    "fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf": {
        "title": "Sequential Recommendation with Graph Neural Networks",
        "authors": [
            "Jianxin Chang",
            "Chen Gao",
            "Y. Zheng",
            "Yiqun Hui",
            "Yanan Niu",
            "Yang Song",
            "Depeng Jin",
            "Yong Li"
        ],
        "published_date": "2021",
        "abstract": "Sequential recommendation aims to leverage users' historical behaviors to predict their next interaction. Existing works have not yet addressed two main challenges in sequential recommendation. First, user behaviors in their rich historical sequences are often implicit and noisy preference signals, they cannot sufficiently reflect users' actual preferences. In addition, users' dynamic preferences often change rapidly over time, and hence it is difficult to capture user patterns in their historical sequences. In this work, we propose a graph neural network model called SURGE (short forSeqUential Recommendation with Graph neural nEtworks) to address these two issues. Specifically, SURGE integrates different types of preferences in long-term user behaviors into clusters in the graph by re-constructing loose item sequences into tight item-item interest graphs based on metric learning. This helps explicitly distinguish users' core interests, by forming dense clusters in the interest graph. Then, we perform cluster-aware and query-aware graph convolutional propagation and graph pooling on the constructed graph. It dynamically fuses and extracts users' current activated core interests from noisy user behavior sequences. We conduct extensive experiments on both public and proprietary industrial datasets. Experimental results demonstrate significant performance gains of our proposed method compared to state-of-the-art methods. Further studies on sequence length confirm that our method can model long behavioral sequences effectively and efficiently.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fbb87e32fd63ae980c0ff6cf6878fd5e4d7bda1d.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 381,
        "score": 95.25
    },
    "454304628bf10f02aba1c2cfc95891e94d09208e.pdf": {
        "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
        "authors": [
            "Vijay Prakash Dwivedi",
            "A. Luu",
            "T. Laurent",
            "Yoshua Bengio",
            "X. Bresson"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.",
        "file_path": "paper_data/Graph_Neural_Networks/info/454304628bf10f02aba1c2cfc95891e94d09208e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 370,
        "score": 92.5
    },
    "2b1eae2cceb377cb9267b2c96294228d5e583136.pdf": {
        "title": "GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks",
        "authors": [
            "Tianxiang Zhao",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "published_date": "2021",
        "abstract": "Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority over-sampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority over-sampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, \\method, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2b1eae2cceb377cb9267b2c96294228d5e583136.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 364,
        "score": 91.0
    },
    "cda969fd7362bdf21aa1f3398078982dcb350d76.pdf": {
        "title": "Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks",
        "authors": [
            "Qingsong Lv",
            "Ming Ding",
            "Qiang Liu",
            "Yuxiang Chen",
            "Wenzheng Feng",
            "Siming He",
            "Chang Zhou",
            "Jianguo Jiang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published_date": "2021",
        "abstract": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cda969fd7362bdf21aa1f3398078982dcb350d76.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 363,
        "score": 90.75
    },
    "ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf": {
        "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
        "authors": [
            "Yu Chen",
            "Lingfei Wu",
            "Mohammed J. Zaki"
        ],
        "published_date": "2020",
        "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ff6a4a9a41b78c8b1fcab185db780266bbb06caf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 436,
        "score": 87.2
    },
    "27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf": {
        "title": "Data Augmentation for Graph Neural Networks",
        "authors": [
            "Tong Zhao",
            "Yozen Liu",
            "Leonardo Neves",
            "Oliver J. Woodford",
            "Meng Jiang",
            "Neil Shah"
        ],
        "published_date": "2020",
        "abstract": "Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/27d5be9322d71b6fd2faa8a6b87250127a12c0cf.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 433,
        "score": 86.60000000000001
    },
    "1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf": {
        "title": "DAG-GNN: DAG Structure Learning with Graph Neural Networks",
        "authors": [
            "Yue Yu",
            "Jie Chen",
            "Tian Gao",
            "Mo Yu"
        ],
        "published_date": "2019",
        "abstract": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1d6b8803f6f6b188802275210eb5d7839644a8b5.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 518,
        "score": 86.33333333333333
    },
    "75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf": {
        "title": "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
        "authors": [
            "Haonan Yuan",
            "Jiliang Tang",
            "Xia Hu",
            "Shuiwang Ji"
        ],
        "published_date": "2020",
        "abstract": "Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/75c8466a0c1c3b9fe595efc83671984ef95bd679.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 431,
        "score": 86.2
    },
    "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf": {
        "title": "Discovering Invariant Rationales for Graph Neural Networks",
        "authors": [
            "Yingmin Wu",
            "Xiang Wang",
            "An Zhang",
            "Xiangnan He",
            "Tat-seng Chua"
        ],
        "published_date": "2022",
        "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 251,
        "score": 83.66666666666666
    },
    "4fa31616b834c377c4995c346a2b17464f25692a.pdf": {
        "title": "Graph Neural Networks for Recommender System",
        "authors": [
            "Chen Gao",
            "Xiang Wang",
            "Xiangnan He",
            "Yong Li"
        ],
        "published_date": "2022",
        "abstract": "Recently, graph neural network (GNN) has become the new state-of-the-art approach in many recommendation problems, with its strong ability to handle structured data and to explore high-order information. However, as the recommendation tasks are diverse and various in the real world, it is quite challenging to design proper GNN methods for specific problems. In this tutorial, we focus on the critical challenges of GNN-based recommendation and the potential solutions. Specifically, we start from an extensive background of recommender systems and graph neural networks. Then we fully discuss why GNNs are required in recommender systems and the four parts of challenges, including graph construction, network design, optimization, and computation efficiency. Then, we discuss how to address these challenges by elaborating on the recent advances of GNN-based recommendation models, with a systematic taxonomy from four critical perspectives: stages, scenarios, objectives, and applications. Last, we finalize this tutorial with conclusions and discuss important future directions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/4fa31616b834c377c4995c346a2b17464f25692a.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 251,
        "score": 83.66666666666666
    },
    "53e80869c6582d7f95ef0a351170736afd1742d0.pdf": {
        "title": "Rethinking Graph Neural Networks for Anomaly Detection",
        "authors": [
            "Jianheng Tang",
            "Jiajin Li",
            "Zi-Chao Gao",
            "Jia Li"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
        "file_path": "paper_data/Graph_Neural_Networks/info/53e80869c6582d7f95ef0a351170736afd1742d0.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 250,
        "score": 83.33333333333333
    },
    "993377a3fc8334558463b82053904e3d684f29c0.pdf": {
        "title": "SIGN: Scalable Inception Graph Neural Networks",
        "authors": [
            "Emanuele Rossi",
            "Fabrizio Frasca",
            "B. Chamberlain",
            "D. Eynard",
            "M. Bronstein",
            "Federico Monti"
        ],
        "published_date": "2020",
        "abstract": "Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time.",
        "file_path": "paper_data/Graph_Neural_Networks/info/993377a3fc8334558463b82053904e3d684f29c0.pdf",
        "venue": "arXiv.org",
        "citationCount": 415,
        "score": 83.0
    },
    "02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf": {
        "title": "Graph Neural Networks for Graphs with Heterophily: A Survey",
        "authors": [
            "Xin Zheng",
            "Yixin Liu",
            "Shirui Pan",
            "Miao Zhang",
            "Di Jin",
            "Philip S. Yu"
        ],
        "published_date": "2022",
        "abstract": "Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we discuss the correlation between graph heterophily and various graph research domains, aiming to facilitate the development of more effective GNNs across a spectrum of practical applications and learning tasks in the graph research community. In the end, we point out the potential directions to advance and stimulate more future research and applications on heterophilic graph learning with GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/02a3452a5f7fe42ba32bbf30af28b7845b2d6857.pdf",
        "venue": "arXiv.org",
        "citationCount": 243,
        "score": 81.0
    },
    "341880efaef452f631a4a5cd61bef5dae47741d7.pdf": {
        "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
        "authors": [
            "Kaidi Xu",
            "Hongge Chen",
            "Sijia Liu",
            "Pin-Yu Chen",
            "Tsui-Wei Weng",
            "Mingyi Hong",
            "Xue Lin"
        ],
        "published_date": "2019",
        "abstract": "Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrifice classification accuracy on original graph.",
        "file_path": "paper_data/Graph_Neural_Networks/info/341880efaef452f631a4a5cd61bef5dae47741d7.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 474,
        "score": 79.0
    },
    "641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf": {
        "title": "How Powerful are Spectral Graph Neural Networks",
        "authors": [
            "Xiyuan Wang",
            "Muhan Zhang"
        ],
        "published_date": "2022",
        "abstract": "Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/641828b8ca714a0f70ccdac17d7e9dff485877c2.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 232,
        "score": 77.33333333333333
    },
    "4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf": {
        "title": "Revisiting Heterophily For Graph Neural Networks",
        "authors": [
            "Sitao Luan",
            "Chenqing Hua",
            "Qincheng Lu",
            "Jiaqi Zhu",
            "Mingde Zhao",
            "Shuyuan Zhang",
            "Xiaoming Chang",
            "Doina Precup"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
        "file_path": "paper_data/Graph_Neural_Networks/info/4becb19c87f0526d9a3a2c15497e0b1c40b576e2.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 225,
        "score": 75.0
    },
    "b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf": {
        "title": "Finding Global Homophily in Graph Neural Networks When Meeting Heterophily",
        "authors": [
            "Xiang Li",
            "Renyu Zhu",
            "Yao Cheng",
            "Caihua Shan",
            "Siqiang Luo",
            "Dongsheng Li",
            "Wei Qian"
        ],
        "published_date": "2022",
        "abstract": "We investigate graph neural networks on graphs with heterophily. Some existing methods amplify a node's neighborhood with multi-hop neighbors to include more nodes with homophily. However, it is a significant challenge to set personalized neighborhood sizes for different nodes. Further, for other homophilous nodes excluded in the neighborhood, they are ignored for information aggregation. To address these problems, we propose two models GloGNN and GloGNN++, which generate a node's embedding by aggregating information from global nodes in the graph. In each layer, both models learn a coefficient matrix to capture the correlations between nodes, based on which neighborhood aggregation is performed. The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution. We further accelerate neighborhood aggregation and derive a linear time complexity. We theoretically explain the models' effectiveness by proving that both the coefficient matrix and the generated node embedding matrix have the desired grouping effect. We conduct extensive experiments to compare our models against 11 other competitors on 15 benchmark datasets in a wide range of domains, scales and graph heterophilies. Experimental results show that our methods achieve superior performance and are also very efficient.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b25b4d70b62d8482c98c2b901f4a7e1600df3a72.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 225,
        "score": 75.0
    },
    "a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf": {
        "title": "PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks",
        "authors": [
            "Minh N. Vu",
            "M. Thai"
        ],
        "published_date": "2020",
        "abstract": "In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a8ae2d8232db04d88cf622e5fabd11da3163aa8f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 360,
        "score": 72.0
    },
    "f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf": {
        "title": "Graph Neural Networks With Convolutional ARMA Filters",
        "authors": [
            "F. Bianchi",
            "Daniele Grattarola",
            "L. Livi",
            "C. Alippi"
        ],
        "published_date": "2019",
        "abstract": "Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f55781f7ce6fd31e946f0efe76d5bf89858391d1.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 427,
        "score": 71.16666666666666
    },
    "44b9f16ba417b90e2e7c42f9074378dd06415809.pdf": {
        "title": "Identity-aware Graph Neural Networks",
        "authors": [
            "Jiaxuan You",
            "Jonathan M. Gomes-Selman",
            "Rex Ying",
            "J. Leskovec"
        ],
        "published_date": "2021",
        "abstract": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes\u2019 identities during message passing. To embed a given node, ID-GNN first extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified but faster version of ID-GNN that injects node identity information as augmented node features. Alto- gether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs demonstrate improved or comparable performance over other task-specific graph networks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/44b9f16ba417b90e2e7c42f9074378dd06415809.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 274,
        "score": 68.5
    },
    "0a69c8815536a657668e089e3281ff2e963d947a.pdf": {
        "title": "Design Space for Graph Neural Networks",
        "authors": [
            "Jiaxuan You",
            "Rex Ying",
            "J. Leskovec"
        ],
        "published_date": "2020",
        "abstract": "The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0a69c8815536a657668e089e3281ff2e963d947a.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 340,
        "score": 68.0
    },
    "6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf": {
        "title": "Heterogeneous Graph Structure Learning for Graph Neural Networks",
        "authors": [
            "Jianan Zhao",
            "Xiao Wang",
            "C. Shi",
            "Binbin Hu",
            "Guojie Song",
            "Yanfang Ye"
        ],
        "published_date": "2021",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameters learning for classification task. Different from traditional GSL on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph independently. Specifically, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6e2bfca21d3c2bacb578b288148c3c1795b8c205.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 272,
        "score": 68.0
    },
    "536da0e76290aea9cbe75c29bac096aeb45ef875.pdf": {
        "title": "Can graph neural networks count substructures?",
        "authors": [
            "Zhengdao Chen",
            "Lei Chen",
            "Soledad Villar",
            "Joan Bruna"
        ],
        "published_date": "2020",
        "abstract": "The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting, we propose a local relational pooling approach with inspirations from Murphy et al. (2019) and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on real-world tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/536da0e76290aea9cbe75c29bac096aeb45ef875.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 338,
        "score": 67.60000000000001
    },
    "80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf": {
        "title": "Forecasting Global Weather with Graph Neural Networks",
        "authors": [
            "R. Keisler"
        ],
        "published_date": "2022",
        "abstract": "We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.",
        "file_path": "paper_data/Graph_Neural_Networks/info/80d7b9180299ed1954dbc3acde4ad4efa8974e0a.pdf",
        "venue": "arXiv.org",
        "citationCount": 196,
        "score": 65.33333333333333
    },
    "1478c3c0225368419f68aabc6b67033531d3b4c1.pdf": {
        "title": "Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction",
        "authors": [
            "Maosen Li",
            "Siheng Chen",
            "Yangheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Qi Tian"
        ],
        "published_date": "2020",
        "abstract": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1478c3c0225368419f68aabc6b67033531d3b4c1.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 322,
        "score": 64.4
    },
    "73366d75289c5e37481639fb54fdee28a664e2b3.pdf": {
        "title": "GNNGuard: Defending Graph Neural Networks against Adversarial Attacks",
        "authors": [
            "Xiang Zhang",
            "M. Zitnik"
        ],
        "published_date": "2020",
        "abstract": "Deep learning methods for graphs achieve remarkable performance on many tasks. However, despite the proliferation of such methods and their success, recent findings indicate that small, unnoticeable perturbations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop GNNGuard, a general defense approach against a variety of training-time attacks that perturb the discrete graph structure. GNNGuard can be straightforwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack. GNNGuard uses network theory of homophily to learn how best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges then allow the underlying GNN to robustly propagate neural messages in the graph. GNNGuard introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across five GNNs, three defense methods, and four datasets, including a challenging human disease graph, experiments show that GNNGuard outperforms existing defense approaches by 15.3% on average. Remarkably, GNNGuard can effectively restore the state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/73366d75289c5e37481639fb54fdee28a664e2b3.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 319,
        "score": 63.800000000000004
    },
    "b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf": {
        "title": "Graph Neural Networks: Foundation, Frontiers and Applications",
        "authors": [
            "Lingfei Wu",
            "P. Cui",
            "Jian Pei",
            "Liang Zhao",
            "Xiaojie Guo"
        ],
        "published_date": "2022",
        "abstract": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b2ea3564e8d763a00d733a3dc44f85550a995fd0.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 190,
        "score": 63.33333333333333
    },
    "8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf": {
        "title": "Is Homophily a Necessity for Graph Neural Networks?",
        "authors": [
            "Yao Ma",
            "Xiaorui Liu",
            "Neil Shah",
            "Jiliang Tang"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\"like attracts like\"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions, and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8a1e3d41ea3d730e562d8c19b2bdb50a23208842.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 246,
        "score": 61.5
    },
    "030046515a20a4b4f86c290361881923694e458a.pdf": {
        "title": "Combining Graph Neural Networks With Expert Knowledge for Smart Contract Vulnerability Detection",
        "authors": [
            "Zhenguang Liu",
            "Peng Qian",
            "Xiaoyang Wang",
            "Yuan Zhuang",
            "Lin Qiu",
            "Xun Wang"
        ],
        "published_date": "2021",
        "abstract": "Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.",
        "file_path": "paper_data/Graph_Neural_Networks/info/030046515a20a4b4f86c290361881923694e458a.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 243,
        "score": 60.75
    },
    "e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf": {
        "title": "From Canonical Correlation Analysis to Self-supervised Graph Neural Networks",
        "authors": [
            "Hengrui Zhang",
            "Qitian Wu",
            "Junchi Yan",
            "D. Wipf",
            "Philip S. Yu"
        ],
        "published_date": "2021",
        "abstract": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e7b666c5ff82321cd35cfe5af3deb026ea0d3059.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 241,
        "score": 60.25
    },
    "c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf": {
        "title": "A Note on Over-Smoothing for Graph Neural Networks",
        "authors": [
            "Chen Cai",
            "Yusu Wang"
        ],
        "published_date": "2020",
        "abstract": "Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \\cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure \"expressiveness\" of embedding is conceptually clean; it leads to simpler proofs than \\cite{oono2019graph} and can handle more non-linearities.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c6d550c3fcecf27b979be84c4cd444cc1c72bf47.pdf",
        "venue": "arXiv.org",
        "citationCount": 299,
        "score": 59.800000000000004
    },
    "16351ff232f2e475c8d8347809ef905d67998fa5.pdf": {
        "title": "Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis",
        "authors": [
            "Yifei Shen",
            "Yuanming Shi",
            "Jun Zhang",
            "K. Letaief"
        ],
        "published_date": "2020",
        "abstract": "Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.",
        "file_path": "paper_data/Graph_Neural_Networks/info/16351ff232f2e475c8d8347809ef905d67998fa5.pdf",
        "venue": "IEEE Journal on Selected Areas in Communications",
        "citationCount": 296,
        "score": 59.2
    },
    "23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf": {
        "title": "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks",
        "authors": [
            "Yufeng Zhang",
            "Xueli Yu",
            "Zeyu Cui",
            "Shu Wu",
            "Zhongzheng Wen",
            "Liang Wang"
        ],
        "published_date": "2020",
        "abstract": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/23ce8950b9360158c04ab0c1dcf9a73022b60673.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 296,
        "score": 59.2
    },
    "f1e5e65941617604923225cc4bf464e370fcae67.pdf": {
        "title": "Combining Label Propagation and Simple Models Out-performs Graph Neural Networks",
        "authors": [
            "Qian Huang",
            "Horace He",
            "Abhay Singh",
            "Ser-Nam Lim",
            "Austin R. Benson"
        ],
        "published_date": "2020",
        "abstract": "Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f1e5e65941617604923225cc4bf464e370fcae67.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 293,
        "score": 58.6
    },
    "e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf": {
        "title": "GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks",
        "authors": [
            "Mingchen Sun",
            "Kaixiong Zhou",
            "Xingbo He",
            "Ying Wang",
            "Xin Wang"
        ],
        "published_date": "2022",
        "abstract": "Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis and accelerating the model convergence up to 4.32X. The code is available in: https://github.com/MingChen-Sun/GPPT.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e60ad3d4ed3273af6a94745689783b83f59c8b4a.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 167,
        "score": 55.666666666666664
    },
    "2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf": {
        "title": "AEGNN: Asynchronous Event-based Graph Neural Networks",
        "authors": [
            "S. Schaefer",
            "Daniel Gehrig",
            "D. Scaramuzza"
        ],
        "published_date": "2022",
        "abstract": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as \u201cstatic\u201d spatio-temporal graphs, which are inherently \u201csparse\u201d. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as \u201cevolving\u201d spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, \u201casynchronous\u201d networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2e1ea76e8e9b7576cab57408e2abe7295df76948.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 165,
        "score": 55.0
    },
    "0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf": {
        "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information",
        "authors": [
            "Enyan Dai",
            "Suhang Wang"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0b33c9c2eec5e7a71e1c051ec76e601e76152146.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 268,
        "score": 53.6
    },
    "b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf": {
        "title": "Handling Information Loss of Graph Neural Networks for Session-based Recommendation",
        "authors": [
            "Tianwen Chen",
            "R. C. Wong"
        ],
        "published_date": "2020",
        "abstract": "Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b9631936ab9e41511f0eb85adb0fc7b8efb7983e.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 267,
        "score": 53.400000000000006
    },
    "cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf": {
        "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
        "authors": [
            "Enyan Dai",
            "Tianxiang Zhao",
            "Huaisheng Zhu",
            "Jun Xu",
            "Zhimeng Guo",
            "Hui Liu",
            "Jiliang Tang",
            "Suhang Wang"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users\u2019 trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cfb35f8c18fbc5baa453280ecd0aa8148bbba659.pdf",
        "venue": "Machine Intelligence Research",
        "citationCount": 159,
        "score": 53.0
    },
    "b4895de425a02af87713bd78ed1a29fe425753af.pdf": {
        "title": "Decoupling the Depth and Scope of Graph Neural Networks",
        "authors": [
            "Hanqing Zeng",
            "Muhan Zhang",
            "Yinglong Xia",
            "Ajitesh Srivastava",
            "Andrey Malevich",
            "R. Kannan",
            "V. Prasanna",
            "Long Jin",
            "Ren Chen"
        ],
        "published_date": "2022",
        "abstract": "State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -- to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into\"white noise\". Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b4895de425a02af87713bd78ed1a29fe425753af.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 154,
        "score": 51.33333333333333
    },
    "1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf": {
        "title": "Graph Neural Networks for Wireless Communications: From Theory to Practice",
        "authors": [
            "Yifei Shen",
            "Jun Zhang",
            "Shenghui Song",
            "K. Letaief"
        ],
        "published_date": "2022",
        "abstract": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often yield poor performance in large scale networks (i.e., poor scalability) and unseen network settings (i.e., poor generalization). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communications problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-node graph (where the nodes may represent users, base stations, or antennas), GNNs\u2019 generalization error and required number of training samples are <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n)$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {O}(n^{2})$ </tex-math></inline-formula> times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and the effectiveness of the proposed design framework.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1ec4f1f88ba8bc12eaf3fe5d2fa7b997294b8c92.pdf",
        "venue": "IEEE Transactions on Wireless Communications",
        "citationCount": 152,
        "score": 50.666666666666664
    },
    "250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf": {
        "title": "A Survey of Graph Neural Networks for Social Recommender Systems",
        "authors": [
            "Kartik Sharma",
            "Yeon-Chang Lee",
            "S. Nambi",
            "Aditya Salian",
            "Shlok Shah",
            "Sang-Wook Kim",
            "Srijan Kumar"
        ],
        "published_date": "2022",
        "abstract": "Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users\u2019 tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS. In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at https://github.com/claws-lab/awesome-GNN-social-recsys",
        "file_path": "paper_data/Graph_Neural_Networks/info/250e1fa7d898f5e6db8138ad7c9e1aa004707fcc.pdf",
        "venue": "ACM Computing Surveys",
        "citationCount": 147,
        "score": 49.0
    },
    "3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf": {
        "title": "BrainGB: A Benchmark for Brain Network Analysis With Graph Neural Networks",
        "authors": [
            "Hejie Cui",
            "Wei Dai",
            "Yanqiao Zhu",
            "Xuan Kan",
            "Antonio Aodong Chen Gu",
            "Joshua Lukemire",
            "L. Zhan",
            "Lifang He",
            "Ying Guo",
            "Carl Yang"
        ],
        "published_date": "2022",
        "abstract": "Mapping the connectome of the human brain using structural or functional connectivity has become one of the most pervasive paradigms for neuroimaging analysis. Recently, Graph Neural Networks (GNNs) motivated from geometric deep learning have attracted broad interest due to their established power for modeling complex networked data. Despite their superior performance in many fields, there has not yet been a systematic study of how to design effective GNNs for brain network analysis. To bridge this gap, we present BrainGB, a benchmark for brain network analysis with GNNs. BrainGB standardizes the process by (1) summarizing brain network construction pipelines for both functional and structural neuroimaging modalities and (2) modularizing the implementation of GNN designs. We conduct extensive experiments on datasets across cohorts and modalities and recommend a set of general recipes for effective GNN designs on brain networks. To support open and reproducible research on GNN-based brain network analysis, we host the BrainGB website at https://braingb.us with models, tutorials, examples, as well as an out-of-box Python package. We hope that this work will provide useful empirical evidence and offer insights for future research in this novel and promising direction.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3850d1914120c0f4e0a5e10432ee5429982a98b3.pdf",
        "venue": "IEEE Transactions on Medical Imaging",
        "citationCount": 145,
        "score": 48.33333333333333
    },
    "aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf": {
        "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization",
        "authors": [
            "Ralph Abboud",
            ".Ismail .Ilkan Ceylan",
            "Martin Grohe",
            "Thomas Lukasiewicz"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/aa1cda29362b9d670d602c7fc6964499d2a364bd.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 238,
        "score": 47.6
    },
    "2028710190373ef893e3055c9113e04274a152d7.pdf": {
        "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
        "authors": [
            "Tianlong Chen",
            "Yongduo Sui",
            "Xuxi Chen",
            "Aston Zhang",
            "Zhangyang Wang"
        ],
        "published_date": "2021",
        "abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2028710190373ef893e3055c9113e04274a152d7.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 188,
        "score": 47.0
    },
    "18b2c7dd5f37818f74407a69985322f8a109f75f.pdf": {
        "title": "Hierarchical Graph Neural Networks for Few-Shot Learning",
        "authors": [
            "Cen Chen",
            "Kenli Li",
            "Wei Wei",
            "Joey Tianyi Zhou",
            "Zeng Zeng"
        ],
        "published_date": "2022",
        "abstract": "Recent graph neural network (GNN) based methods for few-shot learning (FSL) represent the samples of interest as a fully-connected graph and conduct reasoning on the nodes flatly, which ignores the hierarchical correlations among nodes. However, real-world categories may have hierarchical structures, and for FSL, it is important to extract the distinguishing features of the categories from individual samples. To explore this, we propose a novel hierarchical graph neural network (HGNN) for FSL, which consists of three parts, i.e., bottom-up reasoning, top-down reasoning, and skip connections, to enable the efficient learning of multi-level relationships. For the bottom-up reasoning, we design intra-class k-nearest neighbor pooling (intra-class knnPool) and inter-class knnPool layers, to conduct hierarchical learning for both the intra- and inter-class nodes. For the top-down reasoning, we propose to utilize graph unpooling (gUnpool) layers to restore the down-sampled graph into its original size. Skip connections are proposed to fuse multi-level features for the final node classification. The parameters of HGNN are learned by episodic training with the signal of node losses, which aims to train a well-generalizable model for recognizing unseen classes with few labeled data. Experimental results on benchmark datasets have demonstrated that HGNN outperforms other state-of-the-art GNN based methods significantly, for both transductive and non-transductive FSL tasks. The dataset as well as the source code can be downloaded online1",
        "file_path": "paper_data/Graph_Neural_Networks/info/18b2c7dd5f37818f74407a69985322f8a109f75f.pdf",
        "venue": "IEEE transactions on circuits and systems for video technology (Print)",
        "citationCount": 137,
        "score": 45.666666666666664
    },
    "5ab6888c67d2877f15c2b065da4216538835d141.pdf": {
        "title": "Cell clustering for spatial transcriptomics data with graph neural networks",
        "authors": [
            "Jiachen Li",
            "Siheng Chen",
            "Xiaoyong Pan",
            "Ye Yuan",
            "Hongbin Shen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/5ab6888c67d2877f15c2b065da4216538835d141.pdf",
        "venue": "Nature Computational Science",
        "citationCount": 136,
        "score": 45.33333333333333
    },
    "38e320f860d54e4071d68955c774b3e4a091bfe0.pdf": {
        "title": "Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction",
        "authors": [
            "Seongjun Yun",
            "Seoyoon Kim",
            "Junhyun Lee",
            "Jaewoo Kang",
            "Hyunwoo J. Kim"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/38e320f860d54e4071d68955c774b3e4a091bfe0.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 134,
        "score": 44.666666666666664
    },
    "775a6e0f9104b282ed867871d743e3afd1e66d96.pdf": {
        "title": "Nested Graph Neural Networks",
        "authors": [
            "Muhan Zhang",
            "Pan Li"
        ],
        "published_date": "2021",
        "abstract": "Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant-factor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/775a6e0f9104b282ed867871d743e3afd1e66d96.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 176,
        "score": 44.0
    },
    "f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf": {
        "title": "Evaluating explainability for graph neural networks",
        "authors": [
            "Chirag Agarwal",
            "Owen Queen",
            "Himabindu Lakkaraju",
            "M. Zitnik"
        ],
        "published_date": "2022",
        "abstract": "As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, Shape GG en , which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows Shape GG en to mimic the data in various real-world areas. We include Shape GG en and several real-world graph datasets in a graph explainability library, G raph XAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, G raph XAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f50c92916832fba9e0e56fa781b0a03b3e07f3d4.pdf",
        "venue": "Scientific Data",
        "citationCount": 131,
        "score": 43.666666666666664
    },
    "5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf": {
        "title": "Graph Condensation for Graph Neural Networks",
        "authors": [
            "Wei Jin",
            "Lingxiao Zhao",
            "Shichang Zhang",
            "Yozen Liu",
            "Jiliang Tang",
            "Neil Shah"
        ],
        "published_date": "2021",
        "abstract": "Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures.Code is released at https://github.com/ChandlerBang/GCond.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5f3173e24d17b92a96e82d0499b365f341edfcd2.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 170,
        "score": 42.5
    },
    "caf8927cf3c872698a0e97591a1205ba577bbba5.pdf": {
        "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
        "authors": [
            "Jiarui Feng",
            "Yixin Chen",
            "Fuhai Li",
            "Anindya Sarkar",
            "Muhan Zhang"
        ],
        "published_date": "2022",
        "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing -- aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/caf8927cf3c872698a0e97591a1205ba577bbba5.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 126,
        "score": 42.0
    },
    "21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf": {
        "title": "Trustworthy Graph Neural Networks: Aspects, Methods, and Trends",
        "authors": [
            "He Zhang",
            "Bang Wu",
            "Xingliang Yuan",
            "Shirui Pan",
            "Hanghang Tong",
            "Jian Pei"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications such as recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects, such as vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterized by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarize existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. In addition, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialization of trustworthy GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/21913eb287f8fc33db8f6274fd2a07072c4e11eb.pdf",
        "venue": "Proceedings of the IEEE",
        "citationCount": 124,
        "score": 41.33333333333333
    },
    "d09608593caa20b79a8aaddfe19df7e31513d711.pdf": {
        "title": "Graph Neural Networks in IoT: A Survey",
        "authors": [
            "Guimin Dong",
            "Mingyue Tang",
            "Zhiyuan Wang",
            "Jiechao Gao",
            "Sikun Guo",
            "Lihua Cai",
            "Robert Gutierrez",
            "Brad Campbell",
            "Laura E. Barnes",
            "M. Boukhechba"
        ],
        "published_date": "2022",
        "abstract": "The Internet of Things (IoT) boom has revolutionized almost every corner of people\u2019s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d09608593caa20b79a8aaddfe19df7e31513d711.pdf",
        "venue": "ACM Trans. Sens. Networks",
        "citationCount": 122,
        "score": 40.666666666666664
    },
    "24b2aed0f130e5278325b5055711de44d247460e.pdf": {
        "title": "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure",
        "authors": [
            "Shaohua Fan",
            "Xiao Wang",
            "Yanhu Mo",
            "Chuan Shi",
            "Jian Tang"
        ],
        "published_date": "2022",
        "abstract": "Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability. By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspiring by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly, we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability. Code and data are available at: https://github.com/googlebaba/DisC.",
        "file_path": "paper_data/Graph_Neural_Networks/info/24b2aed0f130e5278325b5055711de44d247460e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 109,
        "score": 36.33333333333333
    },
    "bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf": {
        "title": "Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels",
        "authors": [
            "Enyan Dai",
            "Wei-dong Jin",
            "Hui Liu",
            "Suhang Wang"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) have shown their great ability in modeling graph structured data. However, real-world graphs usually contain structure noises and have limited labeled nodes. The performance of GNNs would drop significantly when trained on such graphs, which hinders the adoption of GNNs on many applications. Thus, it is important to develop noise-resistant GNNs with limited labeled nodes. However, the work on this is rather limited. Therefore, we study a novel problem of developing robust GNNs on noisy graphs with limited labeled nodes. Our analysis shows that both the noisy edges and limited labeled nodes could harm the message-passing mechanism of GNNs. To mitigate these issues, we propose a novel framework which adopts the noisy edges as supervision to learn a denoised and dense graph, which can down-weight or eliminate noisy edges and facilitate message passing of GNNs to alleviate the issue of limited labeled nodes. The generated edges are further used to regularize the predictions of unlabeled nodes with label smoothness to better train GNNs. Experimental results on real-world datasets demonstrate the robustness of the proposed framework on noisy graphs with limited labeled nodes.",
        "file_path": "paper_data/Graph_Neural_Networks/info/bbd4287a43f6c1b94d40b673e0efaaac9659cc0f.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 105,
        "score": 35.0
    },
    "aafe1338caef4682069e92378f1190785ec24c2c.pdf": {
        "title": "Breaking the Limits of Message Passing Graph Neural Networks",
        "authors": [
            "M. Balcilar",
            "P. H\u00e9roux",
            "Benoit Ga\u00fcz\u00e8re",
            "P. Vasseur",
            "S\u00e9bastien Adam",
            "P. Honeine"
        ],
        "published_date": "2021",
        "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/aafe1338caef4682069e92378f1190785ec24c2c.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 137,
        "score": 34.25
    },
    "6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf": {
        "title": "A New Perspective on \"How Graph Neural Networks Go Beyond Weisfeiler-Lehman?\"",
        "authors": [
            "Asiri Wijesinghe",
            "Qing Wang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/6a0cbf943183a6751ff438c16ad75434b4cf47da.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 101,
        "score": 33.666666666666664
    },
    "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf": {
        "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
        "authors": [
            "B. Chamberlain",
            "S. Shirobokov",
            "Emanuele Rossi",
            "Fabrizio Frasca",
            "Thomas Markovich",
            "Nils Y. Hammerla",
            "Michael M. Bronstein",
            "Max Hansmire"
        ],
        "published_date": "2022",
        "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.",
        "file_path": "paper_data/Graph_Neural_Networks/info/68baa11061a8da3a9e6c6cd0ff075bd5cc72376d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 98,
        "score": 32.666666666666664
    },
    "2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf": {
        "title": "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks",
        "authors": [
            "Andrea Cini",
            "Ivan Marisca",
            "C. Alippi"
        ],
        "published_date": "2021",
        "abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 127,
        "score": 31.75
    },
    "3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf": {
        "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration",
        "authors": [
            "Xiao Wang",
            "Hongrui Liu",
            "Chuan Shi",
            "Cheng Yang"
        ],
        "published_date": "2021",
        "abstract": "Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy, whether the results are trustworthy is still unexplored. Previous studies suggest that many modern neural networks are over-confident on the predictions, however, surprisingly, we discover that GNNs are primarily in the opposite direction, i.e., GNNs are under-confident. Therefore, the confidence calibration for GNNs is highly desired. In this paper, we propose a novel trustworthy GNN model by designing a topology-aware post-hoc calibration function. Specifically, we first verify that the confidence distribution in a graph has homophily property, and this finding inspires us to design a calibration GNN model (CaGCN) to learn the calibration function. CaGCN is able to obtain a unique transformation from logits of GNNs to the calibrated confidence for each node, meanwhile, such transformation is able to preserve the order between classes, satisfying the accuracy-preserving property. Moreover, we apply the calibration GNN to self-training framework, showing that more trustworthy pseudo labels can be obtained with the calibrated confidence and further improve the performance. Extensive experiments demonstrate the effectiveness of our proposed model in terms of both calibration and accuracy.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3b2f5884e8199544375ddcdb4fa58f44df0b1a7e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 126,
        "score": 31.5
    },
    "d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf": {
        "title": "Benchmarking Graph Neural Networks",
        "authors": [
            "Vijay Prakash Dwivedi",
            "Chaitanya K. Joshi",
            "T. Laurent",
            "Yoshua Bengio",
            "X. Bresson"
        ],
        "published_date": "2023",
        "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d08a0eb7024dff5c4fabd58144a38031633d4e1a.pdf",
        "venue": "Journal of machine learning research",
        "citationCount": 1025,
        "score": 512.5
    },
    "1fad14bcfc2b75797c686a5a05779076437a683e.pdf": {
        "title": "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions",
        "authors": [
            "Bharti Khemani",
            "S. Patil",
            "K. Kotecha",
            "Sudeep Tanwar"
        ],
        "published_date": "2024",
        "abstract": "Deep learning has seen significant growth recently and is now applied to a wide range of conventional use cases, including graphs. Graph data provides relational information between elements and is a standard data format for various machine learning and deep learning tasks. Models that can learn from such inputs are essential for working with graph data effectively. This paper identifies nodes and edges within specific applications, such as text, entities, and relations, to create graph structures. Different applications may require various graph neural network (GNN) models. GNNs facilitate the exchange of information between nodes in a graph, enabling them to understand dependencies within the nodes and edges. The paper delves into specific GNN models like graph convolution networks (GCNs), GraphSAGE, and graph attention networks (GATs), which are widely used in various applications today. It also discusses the message-passing mechanism employed by GNN models and examines the strengths and limitations of these models in different domains. Furthermore, the paper explores the diverse applications of GNNs, the datasets commonly used with them, and the Python libraries that support GNN models. It offers an extensive overview of the landscape of GNN research and its practical implementations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1fad14bcfc2b75797c686a5a05779076437a683e.pdf",
        "venue": "Journal of Big Data",
        "citationCount": 217,
        "score": 217.0
    },
    "da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf": {
        "title": "Graph Neural Networks for Natural Language Processing: A Survey",
        "authors": [
            "Lingfei Wu",
            "Yu Chen",
            "Kai Shen",
            "Xiaojie Guo",
            "Hanning Gao",
            "Shucheng Li",
            "J. Pei",
            "Bo Long"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/da923d1ccfd4927fae7c2a835c7979e3a4dec159.pdf",
        "venue": "Found. Trends Mach. Learn.",
        "citationCount": 325,
        "score": 162.5
    },
    "91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf": {
        "title": "The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study",
        "authors": [
            "Tianfu Li",
            "Zheng Zhou",
            "Sinan Li",
            "Chuang Sun",
            "Ruqiang Yan",
            "Xuefeng Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/91b0abfd8da2ea223e54ef6d33571140bc916f93.pdf",
        "venue": "Mechanical systems and signal processing",
        "citationCount": 413,
        "score": 137.66666666666666
    },
    "252351936bd6fabf4b6cd2962fa0ee613772278d.pdf": {
        "title": "Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey",
        "authors": [
            "G. Jin",
            "Yuxuan Liang",
            "Yuchen Fang",
            "Zezhi Shao",
            "Jincai Huang",
            "Junbo Zhang",
            "Yu Zheng"
        ],
        "published_date": "2023",
        "abstract": "With recent advances in sensing technologies, a myriad of spatio-temporal data has been generated and recorded in smart cities. Forecasting the evolution patterns of spatio-temporal data is an important yet demanding aspect of urban computing, which can enhance intelligent management decisions in various fields, including transportation, environment, climate, public safety, healthcare, and others. Traditional statistical and deep learning methods struggle to capture complex correlations in urban spatio-temporal data. To this end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed, achieving great promise in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. In this manuscript, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. Firstly, we provide a brief introduction to the construction methods of spatio-temporal graph data and the prevalent deep-learning architectures used in STGNNs. We then sort out the primary application domains and specific predictive learning tasks based on existing literature. Afterward, we scrutinize the design of STGNNs and their combination with some advanced technologies in recent years. Finally, we conclude the limitations of existing research and suggest potential directions for future work.",
        "file_path": "paper_data/Graph_Neural_Networks/info/252351936bd6fabf4b6cd2962fa0ee613772278d.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 265,
        "score": 132.5
    },
    "90dead8a056b848be164c2e5cdadfa2e191c3265.pdf": {
        "title": "A Survey on Oversmoothing in Graph Neural Networks",
        "authors": [
            "T. Konstantin Rusch",
            "Michael M. Bronstein",
            "Siddhartha Mishra"
        ],
        "published_date": "2023",
        "abstract": "Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/90dead8a056b848be164c2e5cdadfa2e191c3265.pdf",
        "venue": "arXiv.org",
        "citationCount": 252,
        "score": 126.0
    },
    "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf": {
        "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
        "authors": [
            "Ming Jin",
            "Huan Yee Koh",
            "Qingsong Wen",
            "Daniele Zambon",
            "C. Alippi",
            "G. I. Webb",
            "Irwin King",
            "Shirui Pan"
        ],
        "published_date": "2023",
        "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 222,
        "score": 111.0
    },
    "4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf": {
        "title": "Everything is Connected: Graph Neural Networks",
        "authors": [
            "Petar Velickovic"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/4d4f41fa429f37ce41de0422938affb7805cf9a8.pdf",
        "venue": "Current Opinion in Structural Biology",
        "citationCount": 218,
        "score": 109.0
    },
    "08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf": {
        "title": "Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models",
        "authors": [
            "Dejun Jiang",
            "Zhenxing Wu",
            "Chang-Yu Hsieh",
            "Guangyong Chen",
            "B. Liao",
            "Zhe Wang",
            "Chao Shen",
            "Dongsheng Cao",
            "Jian Wu",
            "Tingjun Hou"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.",
        "file_path": "paper_data/Graph_Neural_Networks/info/08257eb1faa19c29ddcc31d7d749c9bd262213c5.pdf",
        "venue": "Journal of Cheminformatics",
        "citationCount": 499,
        "score": 99.80000000000001
    },
    "8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf": {
        "title": "Self-Supervised Learning of Graph Neural Networks: A Unified Review",
        "authors": [
            "Yaochen Xie",
            "Zhao Xu",
            "Zhengyang Wang",
            "Shuiwang Ji"
        ],
        "published_date": "2021",
        "abstract": "Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8d68eae4068fca5ae3e9660c2a87857c89d30f73.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 353,
        "score": 88.25
    },
    "e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf": {
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
        "authors": [
            "Zemin Liu",
            "Xingtong Yu",
            "Yuan Fang",
            "Xinming Zhang"
        ],
        "published_date": "2023",
        "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the \u201cpre-train, fine-tune\u201d and \u201cpre-train, prompt\u201d paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e147cc46b7f441a68706ca53549d45e9a9843fb6.pdf",
        "venue": "The Web Conference",
        "citationCount": 176,
        "score": 88.0
    },
    "80c698688bb4488beaceaab5c64f701a946cb7ae.pdf": {
        "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
        "authors": [
            "Xiangguo Sun",
            "Hongtao Cheng",
            "Jia Li",
            "Bo Liu",
            "J. Guan"
        ],
        "published_date": "2023",
        "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
        "file_path": "paper_data/Graph_Neural_Networks/info/80c698688bb4488beaceaab5c64f701a946cb7ae.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 174,
        "score": 87.0
    },
    "19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf": {
        "title": "Graph Neural Networks and Their Current Applications in Bioinformatics",
        "authors": [
            "Xiao-Meng Zhang",
            "Li Liang",
            "Lin Liu",
            "Mingjing Tang"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",
        "file_path": "paper_data/Graph_Neural_Networks/info/19fd00f8540a5728a21593b2e62e4f9a8abf74d6.pdf",
        "venue": "Frontiers in Genetics",
        "citationCount": 322,
        "score": 80.5
    },
    "3da4626411d83c19c9919bb41dba94fff88da90e.pdf": {
        "title": "Scaling Graph Neural Networks with Approximate PageRank",
        "authors": [
            "Aleksandar Bojchevski",
            "Johannes Klicpera",
            "Bryan Perozzi",
            "Amol Kapoor",
            "Martin J. Blais",
            "Benedek R'ozemberczki",
            "Michal Lukasik",
            "Stephan Gunnemann"
        ],
        "published_date": "2020",
        "abstract": "Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge -- many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3da4626411d83c19c9919bb41dba94fff88da90e.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 392,
        "score": 78.4
    },
    "f470ac3537339514bb9d88fcad9c075441906d45.pdf": {
        "title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules",
        "authors": [
            "Jun Xia",
            "Chengshuai Zhao",
            "Bozhen Hu",
            "Zhangyang Gao",
            "Cheng Tan",
            "Yue Liu",
            "Siyuan Li",
            "Stan Z. Li"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/f470ac3537339514bb9d88fcad9c075441906d45.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 155,
        "score": 77.5
    },
    "116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf": {
        "title": "Graph Neural Networks for Intelligent Transportation Systems: A Survey",
        "authors": [
            "Saeed Rahmani",
            "Asiye Baghbani",
            "N. Bouguila",
            "Z. Patterson"
        ],
        "published_date": "2023",
        "abstract": "Graph neural networks (GNNs) have been extensively used in a wide variety of domains in recent years. Owing to their power in analyzing graph-structured data, they have become broadly popular in intelligent transportation systems (ITS) applications as well. Despite their widespread applications in different transportation domains, there is no comprehensive review of recent advancements and future research directions that covers all transportation areas. Accordingly, in this survey, for the first time, we provide an overview of GNN studies in the general domain of ITS. Unlike previous surveys, which have been limited to traffic forecasting problems, we explore how GNN frameworks have evolved for different ITS applications, including traffic forecasting, demand prediction, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety. Also, we micro-categorize the studies based on their transportation application to identify domain-specific research directions, opportunities, and challenges, which have been missing in previous surveys. Moreover, we identify unique and undiscussed research opportunities and directions, which is the result of reviewing a wide range of transportation applications. The neglected role of edge and graph learning in ITS applications, developing multi-modal models, and exploiting the power of unsupervised and reinforcement learning methods for developing more powerful GNNs are some examples of such new discussions in this survey. Finally, we have identified popular baseline models and datasets in each transportation domain, which facilitate the development and evaluation of future GNN-based frameworks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8.pdf",
        "venue": "IEEE transactions on intelligent transportation systems (Print)",
        "citationCount": 154,
        "score": 77.0
    },
    "510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf": {
        "title": "Graph Neural Networks",
        "authors": [
            "Yuyu Zhang",
            "Xinshi Chen",
            "Yuan Yang",
            "Arun Ramamurthy",
            "Bo Li",
            "Yuan Qi",
            "Le Song"
        ],
        "published_date": "2021",
        "abstract": "Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Although conventional deep learning techniques have achieved huge successes on Euclidean data such as images, or sequence data such as text, there are many applications that are naturally or best represented with a graph structure. This gap has driven a tide in research for deep learning on graphs, among them Graph Neural Networks (GNNs) are the most successful in coping with various learning tasks across a large number of application domains. In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations, frontiers, and applications. We will introduce the fundamental aspects of GNNs ranging from the popular models and their expressive powers, to the scalability, interpretability and robustness of GNNs. Then, we will discuss various frontier research, ranging from graph classification and link prediction, to graph generation and transformation, graph matching and graph structure learning. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our book and summarize the roadmap of the various research topics of GNNs. Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Le Song Mohamed bin Zayed University of Artificial Intelligence, e-mail: dasongle@gmail.com",
        "file_path": "paper_data/Graph_Neural_Networks/info/510b5b370211d2d85d43475d28bfd40fd48a6a22.pdf",
        "venue": "Deep Learning on Graphs",
        "citationCount": 294,
        "score": 73.5
    },
    "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf": {
        "title": "Generalization and Representational Limits of Graph Neural Networks",
        "authors": [
            "Vikas K. Garg",
            "S. Jegelka",
            "T. Jaakkola"
        ],
        "published_date": "2020",
        "abstract": "We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3a5af4545ee3ac3f413841c10c7605a1cefeb9e5.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 331,
        "score": 66.2
    },
    "6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf": {
        "title": "Macro Graph Neural Networks for Online Billion-Scale Recommender Systems",
        "authors": [
            "Hao Chen",
            "Yuan-Qi Bei",
            "Qijie Shen",
            "Yue Xu",
            "Sheng Zhou",
            "Wenbing Huang",
            "Feiran Huang",
            "Senzhang Wang",
            "Xiao Huang"
        ],
        "published_date": "2024",
        "abstract": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation grap\" and introduce a revolutionizing MAcro Recommendation Graph (MAG) for billion-scale recommendations to reduce the neighbor count from billions to hundreds in the graph structure infrastructure. Specifically, We group micro nodes (users and items) with similar behavior patterns to form macro nodes and then MAG directly describes the relation between the user/item and the hundred of macro nodes rather than the billions of micro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6ad1607eb66cc1f65cff07134c65184b577e5c11.pdf",
        "venue": "The Web Conference",
        "citationCount": 66,
        "score": 66.0
    },
    "2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf": {
        "title": "A Review of Graph Neural Networks and Their Applications in Power Systems",
        "authors": [
            "Wenlong Liao",
            "B. Bak\u2010Jensen",
            "J. Pillai",
            "Yuelong Wang",
            "Yusen Wang"
        ],
        "published_date": "2021",
        "abstract": "Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2fb16966229a3097598ccdfcc9797efba0b93bb5.pdf",
        "venue": "Journal of Modern Power Systems and Clean Energy",
        "citationCount": 252,
        "score": 63.0
    },
    "cf30fb61a5943781144c8442563e3ef9c38df871.pdf": {
        "title": "Training Graph Neural Networks with 1000 Layers",
        "authors": [
            "Guohao Li",
            "Matthias M\u00fcller",
            "Bernard Ghanem",
            "V. Koltun"
        ],
        "published_date": "2021",
        "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.24 \\pm 0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude. Please visit our project website https://www.deepgcns.org/arch/gnn1000 for more information.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cf30fb61a5943781144c8442563e3ef9c38df871.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 251,
        "score": 62.75
    },
    "5822490cf59df7f7ccb92b8901f244850b867a66.pdf": {
        "title": "ETA Prediction with Graph Neural Networks in Google Maps",
        "authors": [
            "Austin Derrow-Pinion",
            "Jennifer She",
            "David Wong",
            "Oliver Lange",
            "Todd Hester",
            "L. Perez",
            "Marc Nunkesser",
            "Seongjae Lee",
            "Xueying Guo",
            "Brett Wiltshire",
            "P. Battaglia",
            "Vishal Gupta",
            "Ang Li",
            "Zhongwen Xu",
            "Alvaro Sanchez-Gonzalez",
            "Yujia Li",
            "Petar Velivckovi'c"
        ],
        "published_date": "2021",
        "abstract": "Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events---such as rush hours---that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",
        "file_path": "paper_data/Graph_Neural_Networks/info/5822490cf59df7f7ccb92b8901f244850b867a66.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 251,
        "score": 62.75
    },
    "94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf": {
        "title": "LineVD: Statement-level Vulnerability Detection using Graph Neural Networks",
        "authors": [
            "David Hin",
            "Andrey Kan",
            "Huaming Chen",
            "M. Babar"
        ],
        "published_date": "2022",
        "abstract": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.",
        "file_path": "paper_data/Graph_Neural_Networks/info/94f9a28783cff6981099b88f2f0f8b65b83d7268.pdf",
        "venue": "IEEE Working Conference on Mining Software Repositories",
        "citationCount": 187,
        "score": 62.33333333333333
    },
    "81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf": {
        "title": "Graph Clustering with Graph Neural Networks",
        "authors": [
            "Anton Tsitsulin",
            "John Palowitch",
            "Bryan Perozzi",
            "Emmanuel M\u00fcller"
        ],
        "published_date": "2020",
        "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.",
        "file_path": "paper_data/Graph_Neural_Networks/info/81a5cdc8fb5c58e7876b60fb735a785a9b16f62f.pdf",
        "venue": "Journal of machine learning research",
        "citationCount": 309,
        "score": 61.800000000000004
    },
    "d08167fd8583b0f70ba8a26821c29ea8af420826.pdf": {
        "title": "Benchmarking graph neural networks for materials chemistry",
        "authors": [
            "Victor Fung",
            "Jiaxin Zhang",
            "Eric Juarez",
            "B. Sumpter"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d08167fd8583b0f70ba8a26821c29ea8af420826.pdf",
        "venue": "npj Computational Materials",
        "citationCount": 246,
        "score": 61.5
    },
    "9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf": {
        "title": "A Review of Graph Neural Networks in Epidemic Modeling",
        "authors": [
            "Zewen Liu",
            "Guancheng Wan",
            "B. A. Prakash",
            "M. S. Lau",
            "Wei Jin"
        ],
        "published_date": "2024",
        "abstract": "Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9dfba8d0fda3c6d8664ca14fe6d161c89bfa1e5f.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 60,
        "score": 60.0
    },
    "e243c89ac61aae7330792c6c3f8791f07f40d031.pdf": {
        "title": "Joint Object Detection and Multi-Object Tracking with Graph Neural Networks",
        "authors": [
            "Yongxin Wang",
            "Kris Kitani",
            "Xinshuo Weng"
        ],
        "published_date": "2021",
        "abstract": "Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e243c89ac61aae7330792c6c3f8791f07f40d031.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 240,
        "score": 60.0
    },
    "3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf": {
        "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
        "authors": [
            "Danqing Wang",
            "Pengfei Liu",
            "Y. Zheng",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published_date": "2020",
        "abstract": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3b05f71ae4532aa4e66d6bb6e88a763e4770c2a7.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 283,
        "score": 56.6
    },
    "fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf": {
        "title": "Prediction of protein\u2013protein interaction using graph neural networks",
        "authors": [
            "Kanchan Jha",
            "S. Saha",
            "Hiteshi Singh"
        ],
        "published_date": "2022",
        "abstract": "Proteins are the essential biological macromolecules required to perform nearly all biological processes, and cellular functions. Proteins rarely carry out their tasks in isolation but interact with other proteins (known as protein\u2013protein interaction) present in their surroundings to complete biological activities. The knowledge of protein\u2013protein interactions (PPIs) unravels the cellular behavior and its functionality. The computational methods automate the prediction of PPI and are less expensive than experimental methods in terms of resources and time. So far, most of the works on PPI have mainly focused on sequence information. Here, we use graph convolutional network (GCN) and graph attention network (GAT) to predict the interaction between proteins by utilizing protein\u2019s structural information and sequence features. We build the graphs of proteins from their PDB files, which contain 3D coordinates of atoms. The protein graph represents the amino acid network, also known as residue contact network, where each node is a residue. Two nodes are connected if they have a pair of atoms (one from each node) within the threshold distance. To extract the node/residue features, we use the protein language model. The input to the language model is the protein sequence, and the output is the feature vector for each amino acid of the underlying sequence. We validate the predictive capability of the proposed graph-based approach on two PPI datasets: Human and S. cerevisiae. Obtained results demonstrate the effectiveness of the proposed approach as it outperforms the previous leading methods. The source code for training and data to train the model are available at https://github.com/JhaKanchan15/PPI_GNN.git.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fb7c75c4087da0d4c79bc1c1ed1f90f6d4772fa4.pdf",
        "venue": "Scientific Reports",
        "citationCount": 164,
        "score": 54.666666666666664
    },
    "db5d583782264529456a475ce8e9a90823b3a2b5.pdf": {
        "title": "Graph Neural Networks in Network Neuroscience",
        "authors": [
            "Alaa Bessadok",
            "M. Mahjoub",
            "I. Rekik"
        ],
        "published_date": "2021",
        "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain \u2013namely brain graph. Relying on its non-euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience.",
        "file_path": "paper_data/Graph_Neural_Networks/info/db5d583782264529456a475ce8e9a90823b3a2b5.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 214,
        "score": 53.5
    },
    "84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf": {
        "title": "Combinatorial optimization with physics-inspired graph neural networks",
        "authors": [
            "M. Schuetz",
            "J. K. Brubaker",
            "H. Katzgraber"
        ],
        "published_date": "2021",
        "abstract": "Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables. Combinatorial optimization, the search for the minimum of an objective function within a finite but very large set of candidate solutions, finds many important and challenging applications in science and industry. A new graph neural network deep learning approach that incorporates concepts from statistical physics is used to develop a robust solver that can tackle a large class of NP-hard combinatorial optimization problems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/84dbf3f70e0192e74674df29be5bb2bc7e3d9d97.pdf",
        "venue": "Nature Machine Intelligence",
        "citationCount": 212,
        "score": 53.0
    },
    "94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf": {
        "title": "Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks",
        "authors": [
            "Meng Shen",
            "Jinpeng Zhang",
            "Liehuang Zhu",
            "Ke Xu",
            "Xiaojiang Du"
        ],
        "published_date": "2021",
        "abstract": "Decentralized Applications (DApps) are increasingly developed and deployed on blockchain platforms such as Ethereum. DApp fingerprinting can identify users\u2019 visits to specific DApps by analyzing the resulting network traffic, revealing much sensitive information about the users, such as their real identities, financial conditions and religious or political preferences. DApps deployed on the same platform usually adopt the same communication interface and similar traffic encryption settings, making the resulting traffic less discriminative. Existing encrypted traffic classification methods either require hand-crafted and fine-tuning features or suffer from low accuracy. It remains a challenging task to conduct DApp fingerprinting in an accurate and efficient way. In this paper, we present GraphDApp, a novel DApp fingerprinting method using Graph Neural Networks (GNNs). We propose a graph structure named Traffic Interaction Graph (TIG) as an information-rich representation of encrypted DApp flows, which implicitly reserves multiple dimensional features in bidirectional client-server interactions. Using TIG, we turn DApp fingerprinting into a graph classification problem and design a powerful GNN-based classifier. We collect real-world traffic datasets from 1,300 DApps with more than 169,000 flows. The experimental results show that GraphDApp is superior to the other state-of-the-art methods in terms of classification accuracy in both closed- and open-world scenarios. In addition, GraphDApp maintains its high accuracy when being applied to the traditional mobile application classification.",
        "file_path": "paper_data/Graph_Neural_Networks/info/94b97bb584f6109caed8465dbbb3c2865edae4bc.pdf",
        "venue": "IEEE Transactions on Information Forensics and Security",
        "citationCount": 211,
        "score": 52.75
    },
    "4b776e7f26464e5b230c1679560f12618730dcc6.pdf": {
        "title": "Interpreting and Unifying Graph Neural Networks with An Optimization Framework",
        "authors": [
            "Meiqi Zhu",
            "Xiao Wang",
            "C. Shi",
            "Houye Ji",
            "Peng Cui"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize na\u00efve graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",
        "file_path": "paper_data/Graph_Neural_Networks/info/4b776e7f26464e5b230c1679560f12618730dcc6.pdf",
        "venue": "The Web Conference",
        "citationCount": 211,
        "score": 52.75
    },
    "5e6db511e736f77f844bbeebaa2b177427abada1.pdf": {
        "title": "On the Expressive Power of Geometric Graph Neural Networks",
        "authors": [
            "Chaitanya K. Joshi",
            "Simon V. Mathis"
        ],
        "published_date": "2023",
        "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at \\url{https://github.com/chaitjo/geometric-gnn-dojo}",
        "file_path": "paper_data/Graph_Neural_Networks/info/5e6db511e736f77f844bbeebaa2b177427abada1.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 103,
        "score": 51.5
    },
    "c193011099906126fe7b6cfcb04062cf4591ccf9.pdf": {
        "title": "Specformer: Spectral Graph Neural Networks Meet Transformers",
        "authors": [
            "Deyu Bo",
            "Chuan Shi",
            "Lele Wang",
            "Renjie Liao"
        ],
        "published_date": "2023",
        "abstract": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c193011099906126fe7b6cfcb04062cf4591ccf9.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 102,
        "score": 51.0
    },
    "286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf": {
        "title": "Dynamic Graph Neural Networks for Sequential Recommendation",
        "authors": [
            "Mengqi Zhang",
            "Shu Wu",
            "Xueli Yu",
            "Liang Wang"
        ],
        "published_date": "2021",
        "abstract": "Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users\u2019 interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users\u2019 preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user's preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.",
        "file_path": "paper_data/Graph_Neural_Networks/info/286d2e0f3d882a37f486623c716d8a54a4a58fdc.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 202,
        "score": 50.5
    },
    "bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf": {
        "title": "Backdoor Attacks to Graph Neural Networks",
        "authors": [
            "Zaixi Zhang",
            "Jinyuan Jia",
            "Binghui Wang",
            "N. Gong"
        ],
        "published_date": "2020",
        "abstract": "In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a subgraph based backdoor attack to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee.pdf",
        "venue": "ACM Symposium on Access Control Models and Technologies",
        "citationCount": 236,
        "score": 47.2
    },
    "c00673042d8cc539d903c4f30b55a71487f5c701.pdf": {
        "title": "Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks",
        "authors": [
            "Jianjun Wei",
            "Yue Liu",
            "Xin Huang",
            "Xin Zhang",
            "Wenyi Liu",
            "Xu Yan"
        ],
        "published_date": "2024",
        "abstract": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c00673042d8cc539d903c4f30b55a71487f5c701.pdf",
        "venue": "2024 5th International Conference on Machine Learning and Computer Application (ICMLCA)",
        "citationCount": 47,
        "score": 47.0
    },
    "aac77c36a9a5c24aa135538c32950096e59ba442.pdf": {
        "title": "TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation",
        "authors": [
            "Feng Yu",
            "Yanqiao Zhu",
            "Q. Liu",
            "Shu Wu",
            "Liang Wang",
            "T. Tan"
        ],
        "published_date": "2020",
        "abstract": "Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/aac77c36a9a5c24aa135538c32950096e59ba442.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 234,
        "score": 46.800000000000004
    },
    "86ad9d1dd6626921297a8456b048f4bccafe967c.pdf": {
        "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks",
        "authors": [
            "Chaoyang He",
            "Keshav Balasubramanian",
            "Emir Ceyani",
            "Yu Rong",
            "P. Zhao",
            "Junzhou Huang",
            "M. Annavaram",
            "S. Avestimehr"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/86ad9d1dd6626921297a8456b048f4bccafe967c.pdf",
        "venue": "arXiv.org",
        "citationCount": 183,
        "score": 45.75
    },
    "d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf": {
        "title": "Graph Neural Networks for Anomaly Detection in Industrial Internet of Things",
        "authors": [
            "Yulei Wu",
            "Hongning Dai",
            "Haina Tang"
        ],
        "published_date": "2021",
        "abstract": "The Industrial Internet of Things (IIoT) plays an important role in digital transformation of traditional industries toward Industry 4.0. By connecting sensors, instruments, and other industry devices to the Internet, IIoT facilitates the data collection, data analysis, and automated control, thereby improving the productivity and efficiency of the business as well as the resulting economic benefits. Due to the complex IIoT infrastructure, anomaly detection becomes an important tool to ensure the success of IIoT. Due to the nature of IIoT, graph-level anomaly detection has been a promising means to detect and predict anomalies in many different domains, such as transportation, energy, and factory, as well as for dynamically evolving networks. This article provides a useful investigation on graph neural networks (GNNs) for anomaly detection in IIoT-enabled smart transportation, smart energy, and smart factory. In addition to the GNN-empowered anomaly detection solutions on point, contextual, and collective types of anomalies, useful data sets, challenges, and open issues for each type of anomalies in the three identified industry sectors (i.e., smart transportation, smart energy, and smart factory) are also provided and discussed, which will be useful for future research in this area. To demonstrate the use of GNN in concrete scenarios, we show three case studies in smart transportation, smart energy, and smart factory, respectively.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d6dd35559b75774a4f97695249abe0bac8d4c86c.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 182,
        "score": 45.5
    },
    "fc580c211689663a64f42e2ba92c864cb134ba9b.pdf": {
        "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
        "authors": [
            "Miltiadis Kofinas",
            "Boris Knyazev",
            "Yan Zhang",
            "Yunlu Chen",
            "G. Burghouts",
            "E. Gavves",
            "Cees G. M. Snoek",
            "David W. Zhang"
        ],
        "published_date": "2024",
        "abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fc580c211689663a64f42e2ba92c864cb134ba9b.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 45,
        "score": 45.0
    },
    "ef41b29312860bc284640e35ab499053f4966bbf.pdf": {
        "title": "Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity",
        "authors": [
            "Shuangli Li",
            "Jingbo Zhou",
            "Tong Xu",
            "Liang Huang",
            "Fan Wang",
            "Haoyi Xiong",
            "Weili Huang",
            "D. Dou",
            "Hui Xiong"
        ],
        "published_date": "2021",
        "abstract": "Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the biomolecular structural information is not fully utilized. The essential long-range interactions among atoms are also neglected in GNN models. To this end, we propose a structure-aware interactive graph neural network (SIGN) which consists of two components: polar-inspired graph attention layers (PGAL) and pairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs the node-edge aggregation process to update embeddings of nodes and edges while preserving the distance and angle information among atoms. Then, PiPool is adopted to gather interactive edges with a subsequent reconstruction loss to reflect the global interactions. Exhaustive experimental study on two benchmarks verifies the superiority of SIGN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ef41b29312860bc284640e35ab499053f4966bbf.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 179,
        "score": 44.75
    },
    "064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf": {
        "title": "Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective",
        "authors": [
            "M. Balcilar",
            "G. Renton",
            "P. H\u00e9roux",
            "Benoit Ga\u00fcz\u00e8re",
            "S\u00e9bastien Adam",
            "P. Honeine"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/064c71e4bf43e49e3a7cefc29b570c60cbfcfc4f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 173,
        "score": 43.25
    },
    "99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf": {
        "title": "Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning",
        "authors": [
            "Muhan Zhang",
            "Pan Li",
            "Yinglong Xia",
            "Kai Wang",
            "Long Jin"
        ],
        "published_date": "2020",
        "abstract": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
        "file_path": "paper_data/Graph_Neural_Networks/info/99ec4cad0f17f25f5b3c1f9be7de25868901943b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 216,
        "score": 43.2
    },
    "4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf": {
        "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks",
        "authors": [
            "Bahare Fatemi",
            "Layla El Asri",
            "Seyed Mehran Kazemi"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/4dc3c61426a3332238ea0feb23f2113a96aef0d4.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 172,
        "score": 43.0
    },
    "dc5e841197165c3c38940cc9f607fce7b09116cf.pdf": {
        "title": "Graph Neural Networks for Intrusion Detection: A Survey",
        "authors": [
            "Tristan Bilot",
            "Nour El Madhoun",
            "K. A. Agha",
            "Anis Zouaoui"
        ],
        "published_date": "2023",
        "abstract": "Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/dc5e841197165c3c38940cc9f607fce7b09116cf.pdf",
        "venue": "IEEE Access",
        "citationCount": 83,
        "score": 41.5
    },
    "facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf": {
        "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
        "authors": [
            "Hongya Wang",
            "Haoteng Yin",
            "Muhan Zhang",
            "Pan Li"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and imposes $O(p)$ (orthogonal group) equivariance w.r.t. the positional features simultaneously, where $p$ is the dimension of used positional features. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.",
        "file_path": "paper_data/Graph_Neural_Networks/info/facf11419e149a03bd4a9bffdda2ebb433a59d85.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 119,
        "score": 39.666666666666664
    },
    "0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf": {
        "title": "Energy-based Out-of-Distribution Detection for Graph Neural Networks",
        "authors": [
            "Qitian Wu",
            "Yiting Chen",
            "Chenxiao Yang",
            "Junchi Yan"
        ],
        "published_date": "2023",
        "abstract": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0dcf24bb23ce5bc17aab8138903af5f049a4db91.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 78,
        "score": 39.0
    },
    "11b9f4729c8e355dec7122993076f6e2788c03c4.pdf": {
        "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
        "authors": [
            "Ana Lucic",
            "Maartje ter Hoeve",
            "Gabriele Tolomei",
            "M. de Rijke",
            "F. Silvestri"
        ],
        "published_date": "2021",
        "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/11b9f4729c8e355dec7122993076f6e2788c03c4.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 155,
        "score": 38.75
    },
    "5f1913828e30c3070f32c154d2d142ec17e91189.pdf": {
        "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation",
        "authors": [
            "P. Huang",
            "Han-Hung Lee",
            "Hwann-Tzong Chen",
            "Tyng-Luh Liu"
        ],
        "published_date": "2021",
        "abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5f1913828e30c3070f32c154d2d142ec17e91189.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 155,
        "score": 38.75
    },
    "f442378ead6282024cf5b9046daa10422fe9fc5f.pdf": {
        "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
        "authors": [
            "Juanhui Li",
            "Harry Shomer",
            "Haitao Mao",
            "Shenglai Zeng",
            "Yao Ma",
            "Neil Shah",
            "Jiliang Tang",
            "Dawei Yin"
        ],
        "published_date": "2023",
        "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT",
        "file_path": "paper_data/Graph_Neural_Networks/info/f442378ead6282024cf5b9046daa10422fe9fc5f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 76,
        "score": 38.0
    },
    "e4b1d7553020258d7e537e2cfa53865359389eac.pdf": {
        "title": "Stealing Links from Graph Neural Networks",
        "authors": [
            "Xinlei He",
            "Jinyuan Jia",
            "M. Backes",
            "N. Gong",
            "Yang Zhang"
        ],
        "published_date": "2020",
        "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e4b1d7553020258d7e537e2cfa53865359389eac.pdf",
        "venue": "USENIX Security Symposium",
        "citationCount": 190,
        "score": 38.0
    },
    "0d4184cff17f093e0487b27180be515c385feff6.pdf": {
        "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks",
        "authors": [
            "P. Papp",
            "Karolis Martinkus",
            "Lukas Faber",
            "Roger Wattenhofer"
        ],
        "published_date": "2021",
        "abstract": "This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the final result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical findings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0d4184cff17f093e0487b27180be515c385feff6.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 147,
        "score": 36.75
    },
    "4bc7d63595d194a6e0930019764216e6b42da0d4.pdf": {
        "title": "Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns",
        "authors": [
            "Susheel Suresh",
            "Vinith Budde",
            "Jennifer Neville",
            "Pan Li",
            "Jianzhu Ma"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) have achieved tremendous success on multiple graph-based learning tasks by fusing network structure and node features. Modern GNN models are built upon iterative aggregation of neighbor's/proximity features by message passing. Its prediction performance has been shown to be strongly bounded by assortative mixing in the graph, a key property wherein nodes with similar attributes mix/connect with each other. We observe that real world networks exhibit heterogeneous or diverse mixing patterns and the conventional global measurement of assortativity, such as global assortativity coefficient, may not be a representative statistic in quantifying this mixing. We adopt a generalized concept, node-level assortativity, one that is based at the node level to better represent the diverse patterns and accurately quantify the learnability of GNNs. We find that the prediction performance of a wide range of GNN models is highly correlated with the node level assortativity. To break this limit, in this work, we focus on transforming the input graph into a computation graph which contains both proximity and structural information as distinct type of edges. The resulted multi-relational graph has an enhanced level of assortativity and, more importantly, preserves rich information from the original graph. We then propose to run GNNs on this computation graph and show that adaptively choosing between structure and proximity leads to improved performance under diverse mixing. Empirically, we show the benefits of adopting our transformation framework for semi-supervised node classification task on a variety of real world graph learning benchmarks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/4bc7d63595d194a6e0930019764216e6b42da0d4.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 147,
        "score": 36.75
    },
    "bda290f54791e4719917e44a7e6441d000c43ab3.pdf": {
        "title": "Federated Graph Neural Networks: Overview, Techniques, and Challenges",
        "authors": [
            "R. Liu",
            "Han Yu"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/bda290f54791e4719917e44a7e6441d000c43ab3.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 110,
        "score": 36.666666666666664
    },
    "3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf": {
        "title": "Robustness of Graph Neural Networks at Scale",
        "authors": [
            "Simon Geisler",
            "Tobias Schmidt",
            "Hakan cSirin",
            "Daniel Zugner",
            "Aleksandar Bojchevski",
            "Stephan Gunnemann"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3328a42bdc552fbfba5dbd5b6c16b8aff26fea18.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 146,
        "score": 36.5
    },
    "5b1978e8284c8514165938bff6e3276977088f94.pdf": {
        "title": "CausalGNN: Causal-Based Graph Neural Networks for Spatio-Temporal Epidemic Forecasting",
        "authors": [
            "Lijing Wang",
            "A. Adiga",
            "Jiangzhuo Chen",
            "A. Sadilek",
            "S. Venkatramanan",
            "M. Marathe"
        ],
        "published_date": "2022",
        "abstract": "Infectious disease forecasting has been a key focus in the recent past owing to the COVID-19 pandemic and has proved to be an important tool in controlling the pandemic. With the advent of reliable spatiotemporal data, graph neural network models have been able to successfully model the inter-relation between the cross-region signals to produce quality forecasts, but like most deep-learning models they do not explicitly incorporate the underlying causal mechanisms. In this work, we employ a causal mechanistic model to guide the learning of the graph embeddings and propose a novel learning framework -- Causal-based Graph Neural Network (CausalGNN) that learns spatiotemporal embedding in a latent space where graph input features and epidemiological context are combined via a mutually learning mechanism using graph-based non-linear transformations. We design an attention-based dynamic GNN module to capture spatial and temporal disease dynamics. A causal module is added to the framework to provide epidemiological context for node embedding via ordinary differential equations. Extensive experiments on forecasting daily new cases of COVID-19 at global, US state, and US county levels show that the proposed method outperforms a broad range of baselines. The learned model which incorporates epidemiological context organizes the embedding in an efficient way by keeping the parameter size small leading to robust and accurate forecasting performance across various datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5b1978e8284c8514165938bff6e3276977088f94.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 107,
        "score": 35.666666666666664
    },
    "b88f456daaf29860d2b59c621be3bd878a581a59.pdf": {
        "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
        "authors": [
            "Antonio Longa",
            "Veronica Lachi",
            "G. Santin",
            "M. Bianchini",
            "B. Lepri",
            "P. Li\u00f3",
            "F. Scarselli",
            "Andrea Passerini"
        ],
        "published_date": "2023",
        "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b88f456daaf29860d2b59c621be3bd878a581a59.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 71,
        "score": 35.5
    },
    "7de413da6e0a00e14270cfaed2a31666e7c28747.pdf": {
        "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
        "authors": [
            "Zaixin Zhang",
            "Qi Liu",
            "Hao Wang",
            "Chengqiang Lu",
            "Chee-Kong Lee"
        ],
        "published_date": "2021",
        "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions\n made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.\n Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7de413da6e0a00e14270cfaed2a31666e7c28747.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 140,
        "score": 35.0
    },
    "8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf": {
        "title": "Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay",
        "authors": [
            "Fan Zhou",
            "Chengtai Cao"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8afb82f6c2a48f5ff6f9c70de3594e4a14c11b93.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 139,
        "score": 34.75
    },
    "b0d62f38592dbae23628d9700490cb11ac873182.pdf": {
        "title": "DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks",
        "authors": [
            "Vasimuddin",
            "Sanchit Misra",
            "Guixiang Ma",
            "Ramanarayan Mohanty",
            "E. Georganas",
            "A. Heinecke",
            "Dhiraj D. Kalamkar",
            "Nesreen Ahmed",
            "Sasikanth Avancha"
        ],
        "published_date": "2021",
        "abstract": "Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7\u00d7 speed-up using a single CPU socket and up to 97\u00d7 speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b0d62f38592dbae23628d9700490cb11ac873182.pdf",
        "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis",
        "citationCount": 139,
        "score": 34.75
    },
    "54ff6c9ad037792e938e05985720d313512539b7.pdf": {
        "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations",
        "authors": [
            "Moshe Eliasof",
            "E. Haber",
            "Eran Treister"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
        "file_path": "paper_data/Graph_Neural_Networks/info/54ff6c9ad037792e938e05985720d313512539b7.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 139,
        "score": 34.75
    },
    "569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf": {
        "title": "Uncertainty Quantification over Graph with Conformalized Graph Neural Networks",
        "authors": [
            "Kexin Huang",
            "Ying Jin",
            "E. Cand\u00e8s",
            "J. Leskovec"
        ],
        "published_date": "2023",
        "abstract": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features.",
        "file_path": "paper_data/Graph_Neural_Networks/info/569140ad11310f71c5fcc0ecaa6810d12bee3416.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 69,
        "score": 34.5
    },
    "cd551790992d16148fe2e5ff2cc76861195e2191.pdf": {
        "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
        "authors": [
            "Yushun Dong",
            "Ninghao Liu",
            "B. Jalaeian",
            "Jundong Li"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) have shown superior performance in analyzing attributed networks in various web-based applications such as social recommendation and web search. Nevertheless, in high-stake decision-making scenarios such as online fraud detection, there is an increasing societal concern that GNNs could make discriminatory decisions towards certain demographic groups. Despite recent explorations on fair GNNs, these works are tailored for a specific GNN model. However, myriads of GNN variants have been proposed for different applications, and it is costly to fine-tune existing debiasing algorithms for each specific GNN architecture. Different from existing works that debias GNN models, we aim to debias the input attributed network to achieve fairer GNNs through feeding GNNs with less biased data. Specifically, we propose novel definitions and metrics to measure the bias in an attributed network, which leads to the optimization objective to mitigate bias. We then develop a framework EDITS to mitigate the bias in attributed networks while maintaining the performance of GNNs in downstream tasks. EDITS works in a model-agnostic manner, i.e., it is independent of any specific GNN. Experiments demonstrate the validity of the proposed bias metrics and the superiority of EDITS on both bias mitigation and utility maintenance. Open-source implementation: https://github.com/yushundong/EDITS.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cd551790992d16148fe2e5ff2cc76861195e2191.pdf",
        "venue": "The Web Conference",
        "citationCount": 138,
        "score": 34.5
    },
    "e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf": {
        "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings",
        "authors": [
            "Matthias Fey",
            "J. E. Lenssen",
            "F. Weichert",
            "J. Leskovec"
        ],
        "published_date": "2021",
        "abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e3c1bb88b4b8299d331d83e4dce7837caa6db93d.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 136,
        "score": 34.0
    },
    "2efc9d5bb114f7114b041d621e002b1562366903.pdf": {
        "title": "ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection",
        "authors": [
            "Van-Anh Nguyen",
            "D. Q. Nguyen",
            "Van Nguyen",
            "Trung Le",
            "Quan Hung Tran",
            "Dinh Q. Phung"
        ],
        "published_date": "2021",
        "abstract": "Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2efc9d5bb114f7114b041d621e002b1562366903.pdf",
        "venue": "2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
        "citationCount": 133,
        "score": 33.25
    },
    "94497472eecb7530a2b75c564548c540ebd61e9b.pdf": {
        "title": "Learning to Pre-train Graph Neural Networks",
        "authors": [
            "Yuanfu Lu",
            "Xunqiang Jiang",
            "Yuan Fang",
            "C. Shi"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) have become the defacto standard for representation learning on graphs, which derive effective node representations by recursively aggregating information from graph neighborhoods. \nWhile GNNs can be trained from scratch, pre-training GNNs to learn transferable knowledge for downstream tasks has recently been demonstrated to improve the state of the art. \nHowever, conventional GNN pre-training methods follow a two-step paradigm: 1) pre-training on abundant unlabeled data and 2) fine-tuning on downstream labeled data, between which there exists a significant gap due to the divergence of optimization objectives in the two steps. \nIn this paper, we conduct an analysis to show the divergence between pre-training and fine-tuning, and to alleviate such divergence, we propose L2P-GNN, a self-supervised pre-training strategy for GNNs. \nThe key insight is that L2P-GNN attempts to learn how to fine-tune during the pre-training process in the form of transferable prior knowledge. To encode both local and global information into the prior, L2P-GNN is further designed with a dual adaptation mechanism at both node and graph levels. \nFinally, we conduct a systematic empirical study on the pre-training of various GNN models, using both a public collection of protein graphs and a new compilation of bibliographic graphs for pre-training. Experimental results show that L2P-GNN is capable of learning effective and transferable prior knowledge that yields powerful representations for downstream tasks. \n(Code and datasets are available at https://github.com/rootlu/L2P-GNN.)",
        "file_path": "paper_data/Graph_Neural_Networks/info/94497472eecb7530a2b75c564548c540ebd61e9b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 132,
        "score": 33.0
    },
    "84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf": {
        "title": "Financial Fraud Detection using Quantum Graph Neural Networks",
        "authors": [
            "Nouhaila Innan",
            "Abhishek Sawaika",
            "Ashim Dhor",
            "Siddhant Dutta",
            "Sairupa Thota",
            "Husayn Gokal",
            "Nandan Patel",
            "Muhammad Al-Zafar Khan",
            "Ioannis Theodonis",
            "Mohamed Bennai"
        ],
        "published_date": "2023",
        "abstract": "Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.",
        "file_path": "paper_data/Graph_Neural_Networks/info/84d85df95ee022efbc2a7cad07aa444dfb2eb5d4.pdf",
        "venue": "Quantum Machine Intelligence",
        "citationCount": 65,
        "score": 32.5
    },
    "91b9fa72da566afc77a07ec856c3d8da23714367.pdf": {
        "title": "Learning Power Allocation for Multi-Cell-Multi-User Systems With Heterogeneous Graph Neural Networks",
        "authors": [
            "Jia Guo",
            "Chenyang Yang"
        ],
        "published_date": "2022",
        "abstract": "A well-trained deep neural network (DNN) enables real-time resource allocation by learning the relationship between a policy and its impacting parameters. When wireless systems operate in dynamic environments, the DNN has to be re-trained frequently and hence training complexity should be low. A promising approach to deal with this issue is to construct DNNs with prior knowledge. In this paper, we show that the power allocation policy in multi-cell-multi-user systems exhibits a combination of permutation equivariance properties, which can be harnessed by graph neural networks (GNNs). In particular, we construct a heterogeneous graph and resort to heterogeneous GNN for learning the policy, whose outputs are only equivariant to some permutations of vertexes rather than arbitrary permutations as homogeneous GNNs. We prove that the properties of the functions learned by existing heterogeneous GNN for the formulated graph are inconsistent with the properties of the policy. To avoid the performance degradation by embedding wrong priors, we design a parameter sharing scheme for heterogeneous GNN such that the learned relationship satisfies the desired properties. Simulation results show that the sample and computational complexities for training the constructed GNN are much lower than existing DNNs to achieve the same sum rate.",
        "file_path": "paper_data/Graph_Neural_Networks/info/91b9fa72da566afc77a07ec856c3d8da23714367.pdf",
        "venue": "IEEE Transactions on Wireless Communications",
        "citationCount": 97,
        "score": 32.33333333333333
    },
    "741a7faf9dbefd418cda878c61c5b839ecc02977.pdf": {
        "title": "A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective",
        "authors": [
            "Chaoqi Chen",
            "Yushuang Wu",
            "Qiyuan Dai",
            "Hong-Yu Zhou",
            "Mutian Xu",
            "Sibei Yang",
            "Xiaoguang Han",
            "Yizhou Yu"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (e.g., social network analysis and recommender systems), computer vision (e.g., object detection and point cloud learning), and natural language processing (e.g., relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, i.e., 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/741a7faf9dbefd418cda878c61c5b839ecc02977.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 97,
        "score": 32.33333333333333
    },
    "530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf": {
        "title": "Unnoticeable Backdoor Attacks on Graph Neural Networks",
        "authors": [
            "Enyan Dai",
            "M. Lin",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "published_date": "2023",
        "abstract": "Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/530cc6baebee5ee9005ec2f5e8629764f43c0f02.pdf",
        "venue": "The Web Conference",
        "citationCount": 64,
        "score": 32.0
    },
    "3db15a5534050ab2cfc1d09dd772d032395515e1.pdf": {
        "title": "Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities",
        "authors": [
            "Chuang Liu",
            "Yibing Zhan",
            "Chang Li",
            "Bo Du",
            "Jia Wu",
            "Wenbin Hu",
            "Tongliang Liu",
            "Dacheng Tao"
        ],
        "published_date": "2022",
        "abstract": "Graph neural networks have emerged as a leading architecture for many graph-level tasks, such as graph classification and graph generation. As an essential component of the architecture, graph pooling is indispensable for obtaining a holistic graph-level representation of the whole graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these works. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods for graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods with a mathematical summary for each category; 2) then, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) next, we further outline the applications that incorporate the idea of graph pooling in a variety of domains; 4) finally, we discuss certain critical challenges facing current studies and share our insights on future potential directions for research on the improvement of graph pooling.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3db15a5534050ab2cfc1d09dd772d032395515e1.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 96,
        "score": 32.0
    },
    "066dd731b5aaeede92d129344776783590c338d2.pdf": {
        "title": "Predicting stress, strain and deformation fields in materials and structures with graph neural networks",
        "authors": [
            "M. Maurizi",
            "Chao Gao",
            "F. Berto"
        ],
        "published_date": "2022",
        "abstract": "Developing accurate yet fast computational tools to simulate complex physical phenomena is a long-standing problem. Recent advances in machine learning have revolutionized the way simulations are approached, shifting from a purely physics- to AI-based paradigm. Although impressive achievements have been reached, efficiently predicting complex physical phenomena in materials and structures remains a challenge. Here, we present an AI-based general framework, implemented through graph neural networks, able to learn complex mechanical behavior of materials from a few hundreds data. Harnessing the natural mesh-to-graph mapping, our deep learning model predicts deformation, stress, and strain fields in various material systems, like fiber and stratified composites, and lattice metamaterials. The model can capture complex nonlinear phenomena, from plasticity to buckling instability, seemingly learning physical relationships between the predicted physical fields. Owing to its flexibility, this graph-based framework aims at connecting materials\u2019 microstructure, base materials\u2019 properties, and boundary conditions to a physical response, opening new avenues towards graph-AI-based surrogate modeling.",
        "file_path": "paper_data/Graph_Neural_Networks/info/066dd731b5aaeede92d129344776783590c338d2.pdf",
        "venue": "Scientific Reports",
        "citationCount": 96,
        "score": 32.0
    },
    "60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf": {
        "title": "A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs",
        "authors": [
            "Zi Ye",
            "Y. J. Kumar",
            "G. O. Sing",
            "Fengyan Song",
            "Junsong Wang"
        ],
        "published_date": "2022",
        "abstract": "The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/60a6b17f28e88f17e58f60923d98674358dbd0e4.pdf",
        "venue": "IEEE Access",
        "citationCount": 96,
        "score": 32.0
    },
    "ac44bbb4c62033d558aad57712438e5571069d9c.pdf": {
        "title": "Tail-GNN: Tail-Node Graph Neural Networks",
        "authors": [
            "Zemin Liu",
            "Trung-Kien Nguyen",
            "Yuan Fang"
        ],
        "published_date": "2021",
        "abstract": "The prevalence of graph structures in real-world scenarios enables important tasks such as node classification and link prediction. Graphs in many domains follow a long-tailed distribution in their node degrees, i.e., a significant fraction of nodes are tail nodes with a small degree. Although recent graph neural networks (GNNs) can learn powerful node representations, they treat all nodes uniformly and are not tailored to the large group of tail nodes. In particular, there is limited structural information (i.e., links) on tail nodes, resulting in inferior performance. Toward robust tail node embedding, in this paper we propose a novel graph neural network called Tail-GNN. It hinges on the novel concept of transferable neighborhood translation, to model the variable ties between a target node and its neighbors. On one hand, Tail-GNN learns a neighborhood translation from the structurally rich head nodes (i.e., high-degree nodes), which can be further transferred to the structurally limited tail nodes to enhance their representations. On the other hand, the ties with the neighbors are variable across different parts of the graph, and a global neighborhood translation is inflexible. Thus, we devise a node-wise adaptation to localize the global translation w.r.t. each node. Extensive experiments on five benchmark datasets demonstrate that our proposed Tail-GNN significantly outperforms the state-of-the-art baselines.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ac44bbb4c62033d558aad57712438e5571069d9c.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 128,
        "score": 32.0
    },
    "faa6fce9a16925eb3091271281f923bc95291ebb.pdf": {
        "title": "Dirichlet Energy Constrained Learning for Deep Graph Neural Networks",
        "authors": [
            "Kaixiong Zhou",
            "Xiao Huang",
            "D. Zha",
            "Rui Chen",
            "Li Li",
            "Soo-Hyun Choi",
            "Xia Hu"
        ],
        "published_date": "2021",
        "abstract": "Graph neural networks (GNNs) integrate deep architectures and topological structure modeling in an effective way. However, the performance of existing GNNs would decrease significantly when they stack many layers, because of the over-smoothing issue. Node embeddings tend to converge to similar vectors when GNNs keep recursively aggregating the representations of neighbors. To enable deep GNNs, several methods have been explored recently. But they are developed from either techniques in convolutional neural networks or heuristic strategies. There is no generalizable and theoretical principle to guide the design of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by leveraging the Dirichlet energy of node embeddings, and propose a generalizable principle to guide the training of deep GNNs. Based on it, a novel deep GNN framework -- EGNN is designed. It could provide lower and upper constraints in terms of Dirichlet energy at each layer to avoid over-smoothing. Experimental results demonstrate that EGNN achieves state-of-the-art performance by using deep layers.",
        "file_path": "paper_data/Graph_Neural_Networks/info/faa6fce9a16925eb3091271281f923bc95291ebb.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 128,
        "score": 32.0
    },
    "cc827043e6c5be8337df4edb155096f9d0006020.pdf": {
        "title": "GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily",
        "authors": [
            "Lun Du",
            "Xiaozhou Shi",
            "Qiang Fu",
            "Hengyu Liu",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) are widely used on a variety of graph-based machine learning tasks. For node-level tasks, GNNs have strong power to model the homophily property of graphs (i.e., connected nodes are more similar), while their ability to capture heterophily property is often doubtful. This is partially caused by the design of the feature transformation with the same kernel for the nodes in the same hop and the followed aggregation operator. One kernel cannot model the similarity and the dissimilarity (i.e., the positive and negative correlation) between node features simultaneously even though we use attention mechanisms like Graph Attention Network (GAT), since the weight calculated by attention is always a positive value. In this paper, we propose a novel GNN model based on a bi-kernel feature transformation and a selection gate. Two kernels capture homophily and heterophily information respectively, and the gate is introduced to select which kernel we should use for the given node pairs. We conduct extensive experiments on various datasets with different homophily-heterophily properties. The experimental results show consistent and significant improvements against state-of-the-art GNN methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cc827043e6c5be8337df4edb155096f9d0006020.pdf",
        "venue": "The Web Conference",
        "citationCount": 127,
        "score": 31.75
    },
    "fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf": {
        "title": "Individual Fairness for Graph Neural Networks: A Ranking based Approach",
        "authors": [
            "Yushun Dong",
            "Jian Kang",
            "H. Tong",
            "Jundong Li"
        ],
        "published_date": "2021",
        "abstract": "Recent years have witnessed the pivotal role of Graph Neural Networks (GNNs) in various high-stake decision-making scenarios due to their superior learning capability. Close on the heels of the successful adoption of GNNs in different application domains has been the increasing societal concern that conventional GNNs often do not have fairness considerations. Although some research progress has been made to improve the fairness of GNNs, these works mainly focus on the notion of group fairness regarding different subgroups defined by a protected attribute such as gender, age, and race. Beyond that, it is also essential to study the GNN fairness at a much finer granularity (i.e., at the node level) to ensure that GNNs render similar prediction results for similar individuals to achieve the notion of individual fairness. Toward this goal, in this paper, we make an initial investigation to enhance the individual fairness of GNNs and propose a novel ranking based framework---REDRESS. Specifically, we refine the notion of individual fairness from a ranking perspective, and formulate the ranking based individual fairness promotion problem. This naturally addresses the issue of Lipschitz constant specification and distance calibration resulted from the Lipschitz condition in the conventional individual fairness definition. Our proposed framework REDRESS encapsulates the GNN model utility maximization and the ranking-based individual fairness promotion in a joint framework to enable end-to-end training. It is noteworthy mentioning that REDRESS is a plug-and-play framework and can be easily generalized to any prevalent GNN architectures. Extensive experiments on multiple real-world graphs demonstrate the superiority of REDRESS in achieving a good balance between model utility maximization and individual fairness promotion. Our open source code can be found here: https://github.com/yushundong/REDRESS.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fa98db551fdec0a4c5c1beb25f8aa3df378b8c02.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 127,
        "score": 31.75
    },
    "0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf": {
        "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage",
        "authors": [
            "Yu Wang",
            "Yuying Zhao",
            "Yushun Dong",
            "Huiyuan Chen",
            "Jundong Li",
            "Tyler Derr"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs) have shown great power in learning node representations on graphs. However, they may inherit historical prejudices from training data, leading to discriminatory bias in predictions. Although some work has developed fair GNNs, most of them directly borrow fair representation learning techniques from non-graph domains without considering the potential problem of sensitive attribute leakage caused by feature propagation in GNNs. However, we empirically observe that feature propagation could vary the correlation of previously innocuous non-sensitive features to the sensitive ones. This can be viewed as a leakage of sensitive information which could further exacerbate discrimination in predictions. Thus, we design two feature masking strategies according to feature correlations to highlight the importance of considering feature propagation and correlation variation in alleviating discrimination. Motivated by our analysis, we propose Fair View Graph Neural Network (FairVGNN) to generate fair views of features by automatically identifying and masking sensitive-correlated features considering correlation variation after feature propagation. Given the learned fair views, we adaptively clamp weights of the encoder to avoid using sensitive-related features. Experiments on real-world datasets demonstrate that FairVGNN enjoys a better trade-off between model utility and fairness.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0a8f340f094da212dcb50f310e3bd5fb676e2454.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 95,
        "score": 31.666666666666664
    },
    "741bf9081fe341c173f36739a50606bf2a159610.pdf": {
        "title": "Evidence-aware Fake News Detection with Graph Neural Networks",
        "authors": [
            "Weizhi Xu",
            "Jun Wu",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published_date": "2022",
        "abstract": "The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts.",
        "file_path": "paper_data/Graph_Neural_Networks/info/741bf9081fe341c173f36739a50606bf2a159610.pdf",
        "venue": "The Web Conference",
        "citationCount": 95,
        "score": 31.666666666666664
    },
    "f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf": {
        "title": "Echo state graph neural networks with analogue random resistive memory arrays",
        "authors": [
            "Shaocong Wang",
            "Yi Li",
            "Dingchen Wang",
            "Woyu Zhang",
            "X. Chen",
            "Danian Dong",
            "Song-jian Wang",
            "Xumeng Zhang",
            "Peng Lin",
            "Claudio Gallicchio",
            "Xiaoxin Xu",
            "Qi Liu",
            "K. Cheng",
            "Zhongrui Wang",
            "Dashan Shang",
            "Meilin Liu"
        ],
        "published_date": "2023",
        "abstract": "Co-designing hardware platforms and neural network software can help improve the computational efficiency and training affordability of deep learning implementations. A new approach designed for graph learning with echo state neural networks makes use of in-memory computing with resistive memory and shows up to a 35 times improvement in the energy efficiency and 99% reduction in training cost for graph classification on large datasets. Recent years have witnessed a surge of interest in learning representations of graph-structured data, with applications from social networks to drug discovery. However, graph neural networks, the machine learning models for handling graph-structured data, face significant challenges when running on conventional digital hardware, including the slowdown of Moore\u2019s law due to transistor scaling limits and the von Neumann bottleneck incurred by physically separated memory and processing units, as well as a high training cost. Here we present a hardware\u2013software co-design to address these challenges, by designing an echo state graph neural network based on random resistive memory arrays, which are built from low-cost, nanoscale and stackable resistors for efficient in-memory computing. This approach leverages the intrinsic stochasticity of dielectric breakdown in resistive switching to implement random projections in hardware for an echo state network that effectively minimizes the training complexity thanks to its fixed and random weights. The system demonstrates state-of-the-art performance on both graph classification using the MUTAG and COLLAB datasets and node classification using the CORA dataset, achieving 2.16\u00d7, 35.42\u00d7 and 40.37\u00d7 improvements in energy efficiency for a projected random resistive memory-based hybrid analogue\u2013digital system over a state-of-the-art graphics processing unit and 99.35%, 99.99% and 91.40% reductions of backward pass complexity compared with conventional graph learning. The results point to a promising direction for next-generation artificial intelligence systems for graph learning.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f64163a2ff1e9f3e81ef788c29b330edc5908f21.pdf",
        "venue": "Nature Machine Intelligence",
        "citationCount": 63,
        "score": 31.5
    },
    "e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf": {
        "title": "Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications",
        "authors": [
            "Rui Bing",
            "Guan Yuan",
            "Mu Zhu",
            "Fanrong Meng",
            "Huifang Ma",
            "Shaojie Qiao"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/e025c3a3f2c628b9f40ba6e0d3cfb89faac2802a.pdf",
        "venue": "Artificial Intelligence Review",
        "citationCount": 94,
        "score": 31.333333333333332
    },
    "3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf": {
        "title": "Are Defenses for Graph Neural Networks Robust?",
        "authors": [
            "Felix Mujkanovic",
            "Simon Geisler",
            "Stephan Gunnemann",
            "Aleksandar Bojchevski"
        ],
        "published_date": "2023",
        "abstract": "A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw - virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering - most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3efa96570a10fecba0f93e0f62e95d41ce7b624b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 62,
        "score": 31.0
    },
    "e46aa831aac38520249dff35916937f0d094f32e.pdf": {
        "title": "Knowledge Enhanced Graph Neural Networks for Explainable Recommendation",
        "authors": [
            "Ziyu Lyu",
            "Yue Wu",
            "Junjie Lai",
            "Min Yang",
            "Chengming Li",
            "Wei Zhou"
        ],
        "published_date": "2023",
        "abstract": "Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e46aa831aac38520249dff35916937f0d094f32e.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 62,
        "score": 31.0
    },
    "a52ae33c11309a98887405db21e930a1f298d865.pdf": {
        "title": "Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks",
        "authors": [
            "Hao Peng",
            "Ruitong Zhang",
            "Yingtong Dou",
            "Renyu Yang",
            "Jingyi Zhang",
            "Philip S. Yu"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a52ae33c11309a98887405db21e930a1f298d865.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 123,
        "score": 30.75
    },
    "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf": {
        "title": "Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis",
        "authors": [
            "Hejie Cui",
            "Wei Dai",
            "Yanqiao Zhu",
            "Xiaoxiao Li",
            "Lifang He",
            "Carl Yang"
        ],
        "published_date": "2022",
        "abstract": "Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 92,
        "score": 30.666666666666664
    },
    "a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf": {
        "title": "GraphBind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues",
        "authors": [
            "Ying Xia",
            "Chun-Qiu Xia",
            "Xiaoyong Pan",
            "Hongbin Shen"
        ],
        "published_date": "2021",
        "abstract": "Abstract Knowledge of the interactions between proteins and nucleic acids is the basis of understanding various biological activities and designing new drugs. How to accurately identify the nucleic-acid-binding residues remains a challenging task. In this paper, we propose an accurate predictor, GraphBind, for identifying nucleic-acid-binding residues on proteins based on an end-to-end graph neural network. Considering that binding sites often behave in highly conservative patterns on local tertiary structures, we first construct graphs based on the structural contexts of target residues and their spatial neighborhood. Then, hierarchical graph neural networks (HGNNs) are used to embed the latent local patterns of structural and bio-physicochemical characteristics for binding residue recognition. We comprehensively evaluate GraphBind on DNA/RNA benchmark datasets. The results demonstrate the superior performance of GraphBind than state-of-the-art methods. Moreover, GraphBind is extended to other ligand-binding residue prediction to verify its generalization capability. Web server of GraphBind is freely available at http://www.csbio.sjtu.edu.cn/bioinf/GraphBind/.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a35e56a1fba0ee6cf3c1f6a0d0a1eee27c04c71e.pdf",
        "venue": "Nucleic Acids Research",
        "citationCount": 122,
        "score": 30.5
    },
    "5fb4947831352af6d6231a830a943f0f2069ee8b.pdf": {
        "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
        "authors": [
            "Aosong Feng",
            "Chenyu You",
            "Shiqiang Wang",
            "L. Tassiulas"
        ],
        "published_date": "2022",
        "abstract": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5fb4947831352af6d6231a830a943f0f2069ee8b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 90,
        "score": 30.0
    },
    "cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf": {
        "title": "Graph Neural Networks: Foundations, Frontiers, and Applications",
        "authors": [],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/cd5dd2c47a3077fc0a4e4487ef7cd2cbcb900810.pdf",
        "venue": "",
        "citationCount": 88,
        "score": 29.333333333333332
    },
    "7544db2ae3140081b1581a99eee88960cc31415a.pdf": {
        "title": "Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?",
        "authors": [
            "Sitao Luan",
            "Chenqing Hua",
            "Qincheng Lu",
            "Jiaqi Zhu",
            "Mingde Zhao",
            "Shuyuan Zhang",
            "Xiaoming Chang",
            "Doina Precup"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7544db2ae3140081b1581a99eee88960cc31415a.pdf",
        "venue": "arXiv.org",
        "citationCount": 117,
        "score": 29.25
    },
    "cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf": {
        "title": "A survey of graph neural networks in various learning paradigms: methods, applications, and challenges",
        "authors": [
            "Lilapati Waikhom",
            "Ripon Patgiri"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/cbb0aee609f9cee64df027d5d2050ebecfaf4332.pdf",
        "venue": "Artificial Intelligence Review",
        "citationCount": 87,
        "score": 29.0
    },
    "0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf": {
        "title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis",
        "authors": [
            "Siyi Tang",
            "Jared A. Dunnmon",
            "Khaled Saab",
            "Xuan Zhang",
            "Qianying Huang",
            "Florian Dubost",
            "D. Rubin",
            "Christopher Lee-Messer"
        ],
        "published_date": "2021",
        "abstract": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset, we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types. Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0a4d5fdfaba62390b25c725badffee524bbcf0a6.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 115,
        "score": 28.75
    },
    "c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf": {
        "title": "Directed Acyclic Graph Neural Networks",
        "authors": [
            "Veronika Thost",
            "Jie Chen"
        ],
        "published_date": "2021",
        "abstract": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \\emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c6337dc83db09c9648ae850c71937eb8e5fd7a43.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 115,
        "score": 28.75
    },
    "218223e91f55a1e0186f5b008b55f5e0fe350698.pdf": {
        "title": "TDGIA: Effective Injection Attacks on Graph Neural Networks",
        "authors": [
            "Xu Zou",
            "Qinkai Zheng",
            "Yuxiao Dong",
            "Xinyu Guan",
            "E. Kharlamov",
            "Jialiang Lu",
            "Jie Tang"
        ],
        "published_date": "2021",
        "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks. In this paper, we study a recently-introduced realistic attack scenario on graphs---graph injection attack (GIA). In the GIA scenario, the adversary is not able to modify the existing link structure and node attributes of the input graph, instead the attack is performed by injecting adversarial nodes into it. We present an analysis on the topological vulnerability of GNNs under GIA setting, based on which we propose the Topological Defective Graph Injection Attack (TDGIA) for effective injection attacks. TDGIA first introduces the topological defective edge selection strategy to choose the original nodes for connecting with the injected ones. It then designs the smooth feature optimization objective to generate the features for the injected nodes. Extensive experiments on large-scale datasets show that TDGIA can consistently and significantly outperform various attack baselines in attacking dozens of defense GNN models. Notably, the performance drop on target GNNs resultant from TDGIA is more than double the damage brought by the best attack solution among hundreds of submissions on KDD-CUP 2020.",
        "file_path": "paper_data/Graph_Neural_Networks/info/218223e91f55a1e0186f5b008b55f5e0fe350698.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 115,
        "score": 28.75
    },
    "b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf": {
        "title": "Can Abnormality be Detected by Graph Neural Networks?",
        "authors": [
            "Ziwei Chai",
            "Siqi You",
            "Yang Yang",
            "Shiliang Pu",
            "Jiarong Xu",
            "Haoyang Cai",
            "Weihao Jiang"
        ],
        "published_date": "2022",
        "abstract": "Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?\n\n\n\nIn this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b7394e219eb2b3f39db5bfc49187e91bb09a902d.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 86,
        "score": 28.666666666666664
    },
    "9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf": {
        "title": "Attention-based graph neural networks: a survey",
        "authors": [
            "Chengcheng Sun",
            "Chenhao Li",
            "Xiang Lin",
            "Tianji Zheng",
            "Fanrong Meng",
            "Xiaobin Rui",
            "Zhixi Wang"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/9a671c37e83dbbf8428a1638a8274ed7ed756fc1.pdf",
        "venue": "Artificial Intelligence Review",
        "citationCount": 57,
        "score": 28.5
    },
    "140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf": {
        "title": "Elastic Graph Neural Networks",
        "authors": [
            "Xiaorui Liu",
            "W. Jin",
            "Yao Ma",
            "Yaxin Li",
            "Hua Liu",
            "Yiqi Wang",
            "Ming Yan",
            "Jiliang Tang"
        ],
        "published_date": "2021",
        "abstract": "While many existing graph neural networks (GNNs) have been proven to perform $\\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \\url{https://github.com/lxiaorui/ElasticGNN}.",
        "file_path": "paper_data/Graph_Neural_Networks/info/140f168d8f4e5d110416eb23bf53be7ac4d090cd.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 113,
        "score": 28.25
    },
    "cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf": {
        "title": "Personalized Graph Neural Networks With Attention Mechanism for Session-Aware Recommendation",
        "authors": [
            "Mengqi Zhang",
            "Shu Wu",
            "Meng Gao",
            "Xin Jiang",
            "Ke Xu",
            "Liang Wang"
        ],
        "published_date": "2022",
        "abstract": "The problem of session-aware recommendation aims to predict users\u2019 next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cd2a182430f65f72cc3d03a092b9ca5cc771e150.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 84,
        "score": 28.0
    },
    "6b72135bf31e78ccee78478228b635201326d217.pdf": {
        "title": "Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications",
        "authors": [
            "Sai Munikoti",
            "D. Agarwal",
            "L. Das",
            "M. Halappanavar",
            "Balasubramaniam Natarajan"
        ],
        "published_date": "2022",
        "abstract": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other\u2019s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6b72135bf31e78ccee78478228b635201326d217.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 83,
        "score": 27.666666666666664
    },
    "77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf": {
        "title": "Flow-Based Encrypted Network Traffic Classification With Graph Neural Networks",
        "authors": [
            "Ting-Li Huoh",
            "Yan Luo",
            "Peilong Li",
            "Tong Zhang"
        ],
        "published_date": "2023",
        "abstract": "Classifying encrypted traffic from emerging applications is important but challenging as many conventional traffic classification approaches are ineffective, thus calling for novel methods for identifying encrypted network flows. Recent machine learning and deep learning-based approaches are severely limited by their feature selection and inherent neural network architecture. More importantly, they overlook the opportunity to capture latent information in the temporal dimension of packets. As network data by nature are of non-Euclidean distance space and carry abundant chronological and temporal relations, we are inspired to utilize geometric deep learning that simultaneously takes into account packet raw bytes, metadata and packet relations for classifying encrypted network traffic. Our proposed graph neural network (GNN) model outperforms the two reference methods, convolutional neural networks (CNN) and recurrent neural networks (RNN) quantitatively as indicated by three metrics: sensitivity, precision and F1 score.",
        "file_path": "paper_data/Graph_Neural_Networks/info/77c8d5e28dee05ed63cab3fda87a4b8abf88d5ea.pdf",
        "venue": "IEEE Transactions on Network and Service Management",
        "citationCount": 55,
        "score": 27.5
    },
    "cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf": {
        "title": "Geometrically Equivariant Graph Neural Networks: A Survey",
        "authors": [
            "Jiaqi Han",
            "Yu Rong",
            "Tingyang Xu",
            "Wenbing Huang"
        ],
        "published_date": "2022",
        "abstract": "Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation. The prospect for future potential directions is also provided.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cb298417e52720ed2bb2db711907a4cec6d8f41f.pdf",
        "venue": "arXiv.org",
        "citationCount": 82,
        "score": 27.333333333333332
    },
    "20309e3990cd612a13e389e1572786e55100f03d.pdf": {
        "title": "Graph Anomaly Detection With Graph Neural Networks: Current Status and Challenges",
        "authors": [
            "Hwan Kim",
            "Byung Suk Lee",
            "Won-Yong Shin",
            "Sungsu Lim"
        ],
        "published_date": "2022",
        "abstract": "Graphs are used widely to model complex systems, and detecting anomalies in a graph is an important task in the analysis of complex systems. Graph anomalies are patterns in a graph that do not conform to normal patterns expected of the attributes and/or structures of the graph. In recent years, graph neural networks (GNNs) have been studied extensively and have successfully performed difficult machine learning tasks in node classification, link prediction, and graph classification thanks to the highly expressive capability via message passing in effectively learning graph representations. To solve the graph anomaly detection problem, GNN-based methods leverage information about the graph attributes (or features) and/or structures to learn to score anomalies appropriately. In this survey, we review the recent advances made in detecting graph anomalies using GNN models. Specifically, we summarize GNN-based methods according to the graph type (i.e., static and dynamic), the anomaly type (i.e., node, edge, subgraph, and whole graph), and the network architecture (e.g., graph autoencoder, graph convolutional network). To the best of our knowledge, this survey is the first comprehensive review of graph anomaly detection methods based on GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/20309e3990cd612a13e389e1572786e55100f03d.pdf",
        "venue": "IEEE Access",
        "citationCount": 80,
        "score": 26.666666666666664
    },
    "bd4b8cad70faa48605163eaede13d62fb671f5de.pdf": {
        "title": "Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift",
        "authors": [
            "Zeyang Zhang",
            "Xin Wang",
            "Ziwei Zhang",
            "Haoyang Li",
            "Zhou Qin",
            "Wenwu Zhu"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/bd4b8cad70faa48605163eaede13d62fb671f5de.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 79,
        "score": 26.333333333333332
    },
    "c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf": {
        "title": "Learning Strong Graph Neural Networks with Weak Information",
        "authors": [
            "Yixin Liu",
            "Kaize Ding",
            "Jianling Wang",
            "Vincent Lee",
            "Huan Liu",
            "Shirui Pan"
        ],
        "published_date": "2023",
        "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c7ac48f6e7a621375785efe3b3f32deec407efb0.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 52,
        "score": 26.0
    },
    "e925e38c5bade594237439c1d4a77e1376535697.pdf": {
        "title": "Multi-Robot Collaborative Perception With Graph Neural Networks",
        "authors": [
            "Yang Zhou",
            "Jiuhong Xiao",
            "Yuee Zhou",
            "Giuseppe Loianno"
        ],
        "published_date": "2022",
        "abstract": "Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots\u2019 inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots\u2019 viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e925e38c5bade594237439c1d4a77e1376535697.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 77,
        "score": 25.666666666666664
    },
    "8b9f01585a679dffe92261ecdec56425db9ef97f.pdf": {
        "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
        "authors": [
            "Xinyi Wu",
            "A. Ajorlou",
            "Zihui Wu",
            "A. Jadbabaie"
        ],
        "published_date": "2023",
        "abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8b9f01585a679dffe92261ecdec56425db9ef97f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 51,
        "score": 25.5
    },
    "2a85846fd827a157b624ee012e75cbe37344281c.pdf": {
        "title": "Theory of Graph Neural Networks: Representation and Learning",
        "authors": [
            "S. Jegelka"
        ],
        "published_date": "2022",
        "abstract": "Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2a85846fd827a157b624ee012e75cbe37344281c.pdf",
        "venue": "arXiv.org",
        "citationCount": 76,
        "score": 25.333333333333332
    },
    "ab27a370d87617255455a05cb2d98c268b5fa06b.pdf": {
        "title": "Pre-training graph neural networks for link prediction in biomedical networks",
        "authors": [
            "Yahui Long",
            "Min Wu",
            "Yong Liu",
            "Yuan Fang",
            "C. Kwoh",
            "Jiawei Luo",
            "Xiaoli Li"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/ab27a370d87617255455a05cb2d98c268b5fa06b.pdf",
        "venue": "Bioinform.",
        "citationCount": 76,
        "score": 25.333333333333332
    },
    "85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf": {
        "title": "Universal Prompt Tuning for Graph Neural Networks",
        "authors": [
            "Taoran Fang",
            "Yunchao Zhang",
            "Yang Yang",
            "Chunping Wang",
            "Lei Chen"
        ],
        "published_date": "2022",
        "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/85f578d2df32bdc3f42fdaa9b65a1904b680a262.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 73,
        "score": 24.333333333333332
    },
    "5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf": {
        "title": "Scalable Spatiotemporal Graph Neural Networks",
        "authors": [
            "Andrea Cini",
            "Ivan Marisca",
            "F. Bianchi",
            "C. Alippi"
        ],
        "published_date": "2022",
        "abstract": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5e60dc704e7933e2a3e83512f345bba0debfe3f3.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 73,
        "score": 24.333333333333332
    },
    "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf": {
        "title": "Substructure Aware Graph Neural Networks",
        "authors": [
            "DingYi Zeng",
            "Wanlong Liu",
            "Wenyu Chen",
            "Li Zhou",
            "Malu Zhang",
            "Hong Qu"
        ],
        "published_date": "2023",
        "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f70fbf51b5ff4ba4c6a0766bc77831aff9176d16.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 47,
        "score": 23.5
    },
    "5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf": {
        "title": "Factor Graph Neural Networks",
        "authors": [
            "Zhen Zhang",
            "Mohammed Haroon Dupty",
            "Fan Wu",
            "Fan Wu"
        ],
        "published_date": "2023",
        "abstract": "In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5a6adf8a3f9f041d11ad8087e079bf0c9d2eb77d.pdf",
        "venue": "Journal of machine learning research",
        "citationCount": 43,
        "score": 21.5
    },
    "985a47671c30e2d059c568ba8eb8e2813bab9423.pdf": {
        "title": "Bundle Recommendation and Generation With Graph Neural Networks",
        "authors": [
            "Jianxin Chang",
            "Chen Gao",
            "Xiangnan He",
            "Depeng Jin",
            "Yong Li"
        ],
        "published_date": "2023",
        "abstract": "Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform's prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Convolutional</italic> <italic>Network</italic>) for prebuilt bundle recommendation, and a BGGN model (short for <italic/><italic>Bundle</italic> <italic>Graph</italic> <italic>Generation</italic> <italic>Network</italic>) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user's supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77% to 23.18% and 20.90% to 64.52%, respectively. We have released the datasets and codes at this link: <uri>https://github.com/cjx0525/BGCN</uri>.",
        "file_path": "paper_data/Graph_Neural_Networks/info/985a47671c30e2d059c568ba8eb8e2813bab9423.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 43,
        "score": 21.5
    },
    "fcdd4300f937cef11af297329ed4bd2b611871e7.pdf": {
        "title": "Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion",
        "authors": [
            "Haotian Ju",
            "Dongyue Li",
            "Aneesh Sharma",
            "Hongyang Zhang"
        ],
        "published_date": "2023",
        "abstract": "Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately. Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fcdd4300f937cef11af297329ed4bd2b611871e7.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 42,
        "score": 21.0
    },
    "707142f242ee4e40489062870ca53810cb33d404.pdf": {
        "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
        "authors": [
            "Haitao Mao",
            "Zhikai Chen",
            "Wei Jin",
            "Haoyu Han",
            "Yao Ma",
            "Tong Zhao",
            "Neil Shah",
            "Jiliang Tang"
        ],
        "published_date": "2023",
        "abstract": "Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then propose a rigorous, non-i.i.d PAC-Bayesian generalization bound for GNNs, revealing reasons for the performance disparity, namely the aggregated feature distance and homophily ratio difference between training and testing nodes. Furthermore, we demonstrate the practical implications of our new findings via (1) elucidating the effectiveness of deeper GNNs; and (2) revealing an over-looked distribution shift factor on graph out-of-distribution problem and proposing a new scenario accordingly.",
        "file_path": "paper_data/Graph_Neural_Networks/info/707142f242ee4e40489062870ca53810cb33d404.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 41,
        "score": 20.5
    },
    "018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf": {
        "title": "The expressive power of pooling in Graph Neural Networks",
        "authors": [
            "F. Bianchi",
            "Veronica Lachi"
        ],
        "published_date": "2023",
        "abstract": "In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.",
        "file_path": "paper_data/Graph_Neural_Networks/info/018abe2e4fa7ed08b4d0556d4e1238d40b89688c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 39,
        "score": 19.5
    },
    "03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf": {
        "title": "Graph Neural Networks for Text Classification: A Survey",
        "authors": [
            "Kunze Wang",
            "Yihao Ding",
            "S. Han"
        ],
        "published_date": "2023",
        "abstract": "Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchmarks. Note that we present a comprehensive comparison between different techniques and identify the pros and cons of various evaluation metrics in this survey.",
        "file_path": "paper_data/Graph_Neural_Networks/info/03d1fd385dc204e4e7445c5204ed15bd5e96a99d.pdf",
        "venue": "Artificial Intelligence Review",
        "citationCount": 39,
        "score": 19.5
    },
    "46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf": {
        "title": "Cooperative Graph Neural Networks",
        "authors": [
            "Ben Finkelshtein",
            "Xingyue Huang",
            "Michael M. Bronstein",
            "I. Ceylan"
        ],
        "published_date": "2023",
        "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/46291f6917088b5cd1ee80f134bf7dfcb2a02868.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 37,
        "score": 18.5
    },
    "ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf": {
        "title": "Adversarial Training for Graph Neural Networks",
        "authors": [
            "Lukas Gosch",
            "Simon Geisler",
            "Daniel Sturm",
            "Bertrand Charpentier",
            "Daniel Zugner",
            "Stephan Gunnemann"
        ],
        "published_date": "2023",
        "abstract": "Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ace7550acb19dd4b55fd7f10c400de24b1a87d23.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 37,
        "score": 18.5
    },
    "20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf": {
        "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks",
        "authors": [
            "Gaspard Michel",
            "Giannis Nikolentzos",
            "J. Lutzeyer",
            "M. Vazirgiannis"
        ],
        "published_date": "2023",
        "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/20f6884a0fa579dfe572fd62a3ad9f2068b8b6ac.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 35,
        "score": 17.5
    },
    "9208290fd7948ed14ebe55718118c401e8396159.pdf": {
        "title": "A survey on graph neural networks",
        "authors": [
            "J. Wang"
        ],
        "published_date": "2023",
        "abstract": "In recent years, we have witnessed the developments that deep learning has brought to machine learning. It has solved many\nproblems in the areas of computer vision, speech recognition, natural language processing, and various other tasks with state\nof-the-art performance. However, the data in these tasks is typically represented in Euclidean space. As technology develops,\nmore and more applications are generating data from non-Euclidean domains and representing them as graphs with complex\nrelationships and interdependencies between objects. This poses a significant challenge to deep learning algorithms. This is\nbecause, due to the uniqueness of graphs, applying deep learning to the ubiquitous graph data is not an easy task. To solve\nthe problem in non-Euclidean domains, Graph Neural Networks (GNNs) have emerged. A Graph Neural Network (GNN)\nis a neural model that captures dependencies between graphs by passing messages between graph nodes. This paper\nintroduces commonly used graph neural networks, their learning methods, and common datasets for graph neural networks.\nIt also provides an outlook on the future of Graph Neural Networks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9208290fd7948ed14ebe55718118c401e8396159.pdf",
        "venue": "EAI Endorsed Trans. e Learn.",
        "citationCount": 35,
        "score": 17.5
    },
    "3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf": {
        "title": "Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis",
        "authors": [
            "Xusheng Zhao",
            "Jia Wu",
            "Hao Peng",
            "A. Beheshti",
            "Jessica J. M. Monaghan",
            "D. McAlpine",
            "Heivet Hern\u00e1ndez-P\u00e9rez",
            "M. Dras",
            "Qiong Dai",
            "Yangyang Li",
            "Philip S. Yu",
            "Lifang He"
        ],
        "published_date": "2022",
        "abstract": "Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks' structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs' performance in eight brain network disease analysis tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3e6c84302b2b56cf8369253d6168b852d0aa1fd6.pdf",
        "venue": "Neural Networks",
        "citationCount": 49,
        "score": 16.333333333333332
    },
    "ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf": {
        "title": "Spatio-Temporal Graph Neural Networks: A Survey",
        "authors": [
            "Zahraa Al Sahili",
            "M. Awad"
        ],
        "published_date": "2023",
        "abstract": "Graph Neural Networks have gained huge interest in the past few years. These powerful algorithms expanded deep learning models to non-Euclidean space and were able to achieve state of art performance in various applications including recommender systems and social networks. However, this performance is based on static graph structures assumption which limits the Graph Neural Networks performance when the data varies with time. Spatiotemporal Graph Neural Networks are extension of Graph Neural Networks that takes the time factor into account. Recently, various Spatiotemporal Graph Neural Network algorithms were proposed and achieved superior performance compared to other deep learning algorithms in several time dependent applications. This survey discusses interesting topics related to Spatiotemporal Graph Neural Networks, including algorithms, applications, and open challenges.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ca4b56aa674bba3c7d10d1645cc31cc3a61fc0dc.pdf",
        "venue": "arXiv.org",
        "citationCount": 25,
        "score": 12.5
    },
    "f42898181e56cb6fee860143c96663ed361449e0.pdf": {
        "title": "A graphon-signal analysis of graph neural networks",
        "authors": [
            "R. Levie"
        ],
        "published_date": "2023",
        "abstract": "We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space -- the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they ``look like'' they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f42898181e56cb6fee860143c96663ed361449e0.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 24,
        "score": 12.0
    },
    "df519a15af1e83824340212477d9d356f86f15ec.pdf": {
        "title": "Incorporating syntax and semantics with dual graph neural networks for aspect-level sentiment analysis",
        "authors": [
            "Pengcheng Wang",
            "Linping Tao",
            "Mingwei Tang",
            "Liuxuan Wang",
            "Yangsheng Xu",
            "Mingfeng Zhao"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/df519a15af1e83824340212477d9d356f86f15ec.pdf",
        "venue": "Engineering applications of artificial intelligence",
        "citationCount": 20,
        "score": 20.0
    },
    "181ff84051e375be829ec230c1e65439a199171c.pdf": {
        "title": "Dynamic link prediction by learning the representation of node-pair via graph neural networks",
        "authors": [
            "Hu Dong",
            "Longjie Li",
            "Dongwen Tian",
            "Yiyang Sun",
            "Yuncong Zhao"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/181ff84051e375be829ec230c1e65439a199171c.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 17,
        "score": 17.0
    },
    "cfc041534d57719d893ec5af01a7065621f7c410.pdf": {
        "title": "Beam layout design of shear wall structures based on graph neural networks",
        "authors": [
            "Pengju Zhao",
            "Wenjie Liao",
            "Yuli Huang",
            "Xinzheng Lu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/cfc041534d57719d893ec5af01a7065621f7c410.pdf",
        "venue": "Automation in Construction",
        "citationCount": 16,
        "score": 16.0
    },
    "613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf": {
        "title": "Drug-Target Interactions Prediction Based on Signed Heterogeneous Graph Neural Networks",
        "authors": [
            "Ming Chen",
            "Yajian Jiang",
            "Xiujuan Lei",
            "Yi Pan",
            "Chunyan Ji",
            "Wei Jiang",
            "Hongkai Xiong"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/613959cdb62ffbe60991e0b0630f96ee97fb73ec.pdf",
        "venue": "Chinese journal of electronics",
        "citationCount": 16,
        "score": 16.0
    },
    "a55c59163cf138d31994afc875d46997d3ef5c4b.pdf": {
        "title": "Deep Learning-Based Spatial-Temporal Graph Neural Networks for Price Movement Classification in Crude Oil and Precious Metal Markets",
        "authors": [
            "P. Foroutan",
            "Salim Lahmiri"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/a55c59163cf138d31994afc875d46997d3ef5c4b.pdf",
        "venue": "Machine Learning with Applications",
        "citationCount": 15,
        "score": 15.0
    },
    "3492576dae538ad34a6ecae5b631651e8ddebf92.pdf": {
        "title": "CatTSunami: Accelerating Transition State Energy Calculations with Pretrained Graph Neural Networks",
        "authors": [
            "Brook Wander",
            "Muhammed Shuaibi",
            "John R. Kitchin",
            "Zachary W. Ulissi",
            "C. L. Zitnick"
        ],
        "published_date": "2024",
        "abstract": "Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3492576dae538ad34a6ecae5b631651e8ddebf92.pdf",
        "venue": "ACS Catalysis",
        "citationCount": 14,
        "score": 14.0
    },
    "4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf": {
        "title": "Homogeneous graph neural networks for third-party library recommendation",
        "authors": [
            "Duantengchuan Li",
            "Yuxuan Gao",
            "Zhihao Wang",
            "Hua Qiu",
            "Pan Liu",
            "Zhuoran Xiong",
            "Zilong Zhang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/4f5bac0cf74495b537322baa2f7443edaf117f4e.pdf",
        "venue": "Information Processing & Management",
        "citationCount": 14,
        "score": 14.0
    },
    "ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf": {
        "title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks",
        "authors": [
            "Yifan Duan",
            "Guibin Zhang",
            "Shilong Wang",
            "Xiaojiang Peng",
            "Ziqi Wang",
            "Junyuan Mao",
            "Hao Wu",
            "Xinke Jiang",
            "Kun Wang"
        ],
        "published_date": "2024",
        "abstract": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ca2bc1ca078250372d673b47f3b6786eef4cf7fb.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0
    },
    "5ca285d36255114938751e1787681fa17073a313.pdf": {
        "title": "Enhancing Intellectual Property Rights(IPR) Transparency with Blockchain and Dual Graph Neural Networks",
        "authors": [
            "R. Praveen",
            "Aktalina Torogeldieva",
            "B. Saravanan",
            "Ajay Kumar",
            "Pushpa Rani",
            "B. P. Gajbhare"
        ],
        "published_date": "2024",
        "abstract": "One way that technology is changing the legal profession is by increasing the role of neural networks in intellectual property rights (IPR). The present status of intellectual property rights might be drastically changed if neural networks were to be used to improve the efficiency, accuracy, and cost-effectiveness of copyright, patent, and trademark procedures. Neural networks have had a significant influence on several IP-related applications, such as patent analysis and search, copyright infringement detection, and trademark search. Included in the suggested method are model training, feature extraction, and pre-processing. The goal of pre-processing is to eliminate or replace irrelevant or noisy data from each tweet so that sentiment classification can proceed more effectively. Algorithms for sentiment categorization and information content analysis make up feature extraction. The training process always made use of the DGNN model. This cutting-edge approach outperforms CNN and GNN with an average accuracy of ${9 1. 4 5 \\%}$.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5ca285d36255114938751e1787681fa17073a313.pdf",
        "venue": "2024 First International Conference on Software, Systems and Information Technology (SSITCON)",
        "citationCount": 13,
        "score": 13.0
    },
    "adf1318ee484fe32d227a5084ed981eedb828c72.pdf": {
        "title": "Explanatory subgraph attacks against Graph Neural Networks",
        "authors": [
            "Huiwei Wang",
            "Tianhua Liu",
            "Ziyu Sheng",
            "Huaqing Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/adf1318ee484fe32d227a5084ed981eedb828c72.pdf",
        "venue": "Neural Networks",
        "citationCount": 13,
        "score": 13.0
    },
    "7a42822cb3102041bad5ff7058451e35e48fd15f.pdf": {
        "title": "Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation",
        "authors": [
            "Baoyu Jing",
            "Dawei Zhou",
            "Kan Ren",
            "Carl Yang"
        ],
        "published_date": "2024",
        "abstract": "Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7a42822cb3102041bad5ff7058451e35e48fd15f.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 13,
        "score": 13.0
    },
    "c9845a625e2dac5e32db172d353f81d377760a5f.pdf": {
        "title": "Learning Invariant Representations of Graph Neural Networks via Cluster Generalization",
        "authors": [
            "Donglin Xia",
            "Xiao Wang",
            "Nian Liu",
            "Chuan Shi"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c9845a625e2dac5e32db172d353f81d377760a5f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 12,
        "score": 12.0
    },
    "cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf": {
        "title": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?",
        "authors": [
            "Zhongjian Zhang",
            "Xiao Wang",
            "Huichi Zhou",
            "Yue Yu",
            "Mengmei Zhang",
            "Cheng Yang",
            "Chuan Shi"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cefee18a6e90e747b94dc25e71993cd0bcdbecbe.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 12,
        "score": 12.0
    },
    "3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf": {
        "title": "Counting Graph Substructures with Graph Neural Networks",
        "authors": [
            "Charilaos I. Kanatsoulis",
            "Alejandro Ribeiro"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/3d13e2afd2cb68651ea15bb9fc7f82bb0362ce1a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 12,
        "score": 12.0
    },
    "c1fbf79a695352b906d8c980608fccb99d3366ee.pdf": {
        "title": "Knowledge graph driven medicine recommendation system using graph neural networks on longitudinal medical records",
        "authors": [
            "Rajat Mishra",
            "S. Shridevi"
        ],
        "published_date": "2024",
        "abstract": "Medicine recommendation systems are designed to aid healthcare professionals by analysing a patient\u2019s admission data to recommend safe and effective medications. These systems are categorised into two types: instance-based and longitudinal-based. Instance-based models only consider the current admission, while longitudinal models consider the patient\u2019s medical history. Electronic Health Records are used to incorporate medical history into longitudinal models. This project proposes a novel Knowledge Graph-Driven Medicine Recommendation System using Graph Neural Networks, KGDNet, that utilises longitudinal EHR data along with ontologies and Drug-Drug Interaction knowledge to construct admission-wise clinical and medicine Knowledge Graphs for every patient. Recurrent Neural Networks are employed to model a patient\u2019s historical data, and Graph Neural Networks are used to learn embeddings from the Knowledge Graphs. A Transformer-based Attention mechanism is then used to generate medication recommendations for the patient, considering their current clinical state, medication history, and joint medical records. The model is evaluated on the MIMIC-IV EHR data and outperforms existing methods in terms of precision, recall, F1 score, Jaccard score, and Drug-Drug Interaction control. An ablation study on our models various inputs and components to provide evidence for the importance of each component in providing the best performance. Case study is also performed to demonstrate the real-world effectiveness of KGDNet.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c1fbf79a695352b906d8c980608fccb99d3366ee.pdf",
        "venue": "Scientific Reports",
        "citationCount": 12,
        "score": 12.0
    },
    "cd90ab144ca439fad38ad952d254ef2036da6d96.pdf": {
        "title": "Towards accurate prediction of configurational disorder properties in materials using graph neural networks",
        "authors": [
            "Zhenyao Fang",
            "Qimin Yan"
        ],
        "published_date": "2024",
        "abstract": "The prediction of configurational disorder properties, such as configurational entropy and order-disorder phase transition temperature, of compound materials relies on efficient and accurate evaluations of configurational energies. Previous cluster expansion methods are not applicable to configurationally-complex material systems, including those with atomic distortions and long-range orders. In this work, we propose to leverage the versatile expressive capabilities of graph neural networks (GNNs) for efficient evaluations of configurational energies and present a workflow combining attention-based GNNs and Monte Carlo simulations to calculate the disorder properties. Using the dataset of face-centered tetragonal gold copper without and with local atomic distortions as an example, we demonstrate that the proposed data-driven framework enables the prediction of phase transition temperatures close to experimental values. We also elucidate that the variance of the energy deviations among configurations controls the prediction accuracy of disorder properties and can be used as the target loss function when training and selecting the GNN models. The work serves as a fundamental step toward a data-driven paradigm for the accelerated design of configurationally-complex functional material systems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cd90ab144ca439fad38ad952d254ef2036da6d96.pdf",
        "venue": "npj Computational Materials",
        "citationCount": 12,
        "score": 12.0
    },
    "1697df4909875d593e1f82aeba49f2861640017a.pdf": {
        "title": "Descriptor-Free Collective Variables from Geometric Graph Neural Networks.",
        "authors": [
            "Jintu Zhang",
            "Luigi Bonati",
            "Enrico Trizio",
            "Odin Zhang",
            "Yu Kang",
            "Tingjun Hou",
            "M. Parrinello"
        ],
        "published_date": "2024",
        "abstract": "Enhanced sampling simulations make the computational study of rare events feasible. A large family of such methods crucially depends on the definition of some collective variables (CVs) that could provide a low-dimensional representation of the relevant physics of the process. Recently, many methods have been proposed to semiautomatize the CV design by using machine learning tools to learn the variables directly from the simulation data. However, most methods are based on feedforward neural networks and require some user-defined physical descriptors. Here, we propose bypassing this step using a graph neural network to directly use the atomic coordinates as input for the CV model. This way, we achieve a fully automatic approach to CV determination that provides variables invariant under the relevant symmetries, especially the permutational one. Furthermore, we provide different analysis tools to favor the physical interpretation of the final CV. We prove the robustness of our approach using different methods from the literature for the optimization of the CV, and we prove its efficacy on several systems, including a small peptide, an ion dissociation in explicit solvent, and a simple chemical reaction.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1697df4909875d593e1f82aeba49f2861640017a.pdf",
        "venue": "Journal of Chemical Theory and Computation",
        "citationCount": 12,
        "score": 12.0
    },
    "cb2a45084f0c7bdc38271e94205603d1237945d8.pdf": {
        "title": "Continuous Spiking Graph Neural Networks",
        "authors": [
            "Nan Yin",
            "Mengzhu Wang",
            "Li Shen",
            "Hitesh Laxmichand Patel",
            "Baopu Li",
            "Bin Gu",
            "Huan Xiong"
        ],
        "published_date": "2024",
        "abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cb2a45084f0c7bdc38271e94205603d1237945d8.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0
    },
    "21dce0407d0ee3bec185b0361593d73bb26a532e.pdf": {
        "title": "XGBoost-Enhanced Graph Neural Networks: A New Architecture for Heterogeneous Tabular Data",
        "authors": [
            "Liuxi Yan",
            "Yaoqun Xu"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.",
        "file_path": "paper_data/Graph_Neural_Networks/info/21dce0407d0ee3bec185b0361593d73bb26a532e.pdf",
        "venue": "Applied Sciences",
        "citationCount": 12,
        "score": 12.0
    },
    "bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf": {
        "title": "Graph Rewiring and Preprocessing for Graph Neural Networks Based on Effective Resistance",
        "authors": [
            "Xu Shen",
            "P. Li\u00f3",
            "Lintao Yang",
            "Ru Yuan",
            "Yuyang Zhang",
            "Chengbin Peng"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.",
        "file_path": "paper_data/Graph_Neural_Networks/info/bdc0bf4808c0fc3af5113aee1b75aa7ec3865bfe.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 12,
        "score": 12.0
    },
    "105191ce014da7d36d93d405c920a261dba3e937.pdf": {
        "title": "Graph Neural Networks for Resource Allocation Optimization in Healthcare Industry",
        "authors": [
            "S. K. Manivannan",
            "Venkatesh Kavididevi",
            "D. Muthukumaran",
            "R. Nandhini",
            "V. Vakula",
            "C. Srinivasan"
        ],
        "published_date": "2024",
        "abstract": "Optimizing the allocation of healthcare resources has never been easier than with the help of Graph Neural Networks (GNNs). In healthcare systems, where complications abound, GNNs may help with the allocation of resources in a way that improves both patient outcomes and operational efficiency. The goal is to use GNNs\u2019 graph-modelling capabilities to describe healthcare networks, with hospitals, doctors, and other medical resources acting as nodes and potential transfers of patients or shared resources as edges. Using both past and present data, the goal is to optimize resource allocation by forecasting the most effective distribution patterns. The decision-making process is improved by using important methods including attention mechanisms, message-passing algorithms, and node embeddings. The accuracy and scalability of healthcare resource management are both enhanced by GNNs, which capture spatial and temporal dynamics. The general efficacy of healthcare delivery systems is improved by this method, which permits more precise forecasts and efficient use of resources. The results highlight how GNNs can change healthcare resource allocation models. From GitHub repository data the short interfering RNAs (siRNAs) and microRNAs (miRNAs) data with efficacy is calculated. Out of the 8 data collected as samples for various siRNAs the efficacy minimum value is 0.07 and the maximum value is 0.946 and in other data samples collected it is ${0. 3 8 6}$ and 0.777. For miRNAs it is 0.934 and 0.394 and in other data sample collected it is 0.738 and 0.432.",
        "file_path": "paper_data/Graph_Neural_Networks/info/105191ce014da7d36d93d405c920a261dba3e937.pdf",
        "venue": "2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)",
        "citationCount": 12,
        "score": 12.0
    },
    "10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf": {
        "title": "Graph neural networks in recommender systems",
        "authors": [
            "Xingyang He"
        ],
        "published_date": "2024",
        "abstract": "As a way to alleviate the information overload problem arisen with the development of the internet, recommender systems receive a lot of attention from academia and industry. Due to its superiority in graph data, graph neural networks are widely adopted in recommender systems. This survey offers a comprehensive review of the latest research and innovative approaches in GNN-based recommender systems. This survey introduces a new taxonomy by the construction of GNN models and explores the challenges these models face. This paper also discusses new approaches, i.e., using social graphs and knowledge graphs as side information, and evaluates their strengths and limitations. Finally, this paper suggests some potential directions for future research in this field.",
        "file_path": "paper_data/Graph_Neural_Networks/info/10a1ca056d531d8bce0b392e686a2cd940f244a5.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 12,
        "score": 12.0
    },
    "7779b880700e9e3495557e076d60594d18d69277.pdf": {
        "title": "A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks",
        "authors": [
            "Zhe Zhao",
            "Pengkun Wang",
            "Haibin Wen",
            "Yudong Zhang",
            "Zhengyang Zhou",
            "Yang Wang"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7779b880700e9e3495557e076d60594d18d69277.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 12,
        "score": 12.0
    },
    "599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf": {
        "title": "Investigation of Customized Medical Decision Algorithms Utilizing Graph Neural Networks",
        "authors": [
            "Yafeng Yan",
            "Shuyao He",
            "Zhou Yu",
            "Jiajie Yuan",
            "Ziang Liu",
            "Yan Chen"
        ],
        "published_date": "2024",
        "abstract": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
        "file_path": "paper_data/Graph_Neural_Networks/info/599f965bfc8309f8d0563836bf7b3efbd961c7dc.pdf",
        "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
        "citationCount": 12,
        "score": 12.0
    },
    "900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf": {
        "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
        "authors": [
            "Qiyu Kang",
            "Kai Zhao",
            "Qinxu Ding",
            "Feng Ji",
            "Xuhao Li",
            "Wenfei Liang",
            "Yang Song",
            "Wee Peng Tay"
        ],
        "published_date": "2024",
        "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.",
        "file_path": "paper_data/Graph_Neural_Networks/info/900fc1f1d2b9ceeacbc92d74491b0a19c823af20.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 12,
        "score": 12.0
    },
    "687abbb274492f95b2c0fe82137c009754456d4c.pdf": {
        "title": "GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
        "authors": [
            "Zaishuo Xia",
            "Han Yang",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/687abbb274492f95b2c0fe82137c009754456d4c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 12,
        "score": 12.0
    },
    "3911024df853ccf11138d35835572ce863df51bf.pdf": {
        "title": "Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction",
        "authors": [
            "Yicheng Zhou",
            "P. Wang",
            "Hao Dong",
            "Denghui Zhang",
            "Dingqi Yang",
            "Yanjie Fu",
            "Pengyang Wang"
        ],
        "published_date": "2024",
        "abstract": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3911024df853ccf11138d35835572ce863df51bf.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 11.0
    },
    "cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf": {
        "title": "GOAt: Explaining Graph Neural Networks via Graph Output Attribution",
        "authors": [
            "Shengyao Lu",
            "Keith G. Mills",
            "Jiao He",
            "Bang Liu",
            "Di Niu"
        ],
        "published_date": "2024",
        "abstract": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cbfaf72253203f4160830d1af76c2b5a4a46406b.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 11,
        "score": 11.0
    },
    "774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf": {
        "title": "A Manifold Perspective on the Statistical Generalization of Graph Neural Networks",
        "authors": [
            "Zhiyang Wang",
            "J. Cervi\u00f1o",
            "Alejandro Ribeiro"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes -- a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/774f8bfe58d15deeea791248766f5e7dc7a4623b.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0
    },
    "e49178ea82233947837c135ec303852dc776dbde.pdf": {
        "title": "Graph Neural Networks in Point Clouds: A Survey",
        "authors": [
            "Dilong Li",
            "Chenghui Lu",
            "Zi-xing Chen",
            "Jianlong Guan",
            "Jing Zhao",
            "Jixiang Du"
        ],
        "published_date": "2024",
        "abstract": "With the advancement of 3D sensing technologies, point clouds are gradually becoming the main type of data representation in applications such as autonomous driving, robotics, and augmented reality. Nevertheless, the irregularity inherent in point clouds presents numerous challenges for traditional deep learning frameworks. Graph neural networks (GNNs) have demonstrated their tremendous potential in processing graph-structured data and are widely applied in various domains including social media data analysis, molecular structure calculation, and computer vision. GNNs, with their capability to handle non-Euclidean data, offer a novel approach for addressing these challenges. Additionally, drawing inspiration from the achievements of transformers in natural language processing, graph transformers have propelled models towards global awareness, overcoming the limitations of local aggregation mechanisms inherent in early GNN architectures. This paper provides a comprehensive review of GNNs and graph-based methods in point cloud applications, adopting a task-oriented perspective to analyze this field. We categorize GNN methods for point clouds based on fundamental tasks, such as segmentation, classification, object detection, registration, and other related tasks. For each category, we summarize the existing mainstream methods, conduct a comprehensive analysis of their performance on various datasets, and discuss the development trends and future prospects of graph-based methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/e49178ea82233947837c135ec303852dc776dbde.pdf",
        "venue": "Remote Sensing",
        "citationCount": 11,
        "score": 11.0
    },
    "0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf": {
        "title": "Graph-Based Audio Classification Using Pre-Trained Models and Graph Neural Networks",
        "authors": [
            "A. Castro-Ospina",
            "M. Solarte-Sanchez",
            "L. Vega-Escobar",
            "C. Isaza",
            "J. D. Mart\u00ednez-Vargas"
        ],
        "published_date": "2024",
        "abstract": "Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0611aecf6ab7a34d45e1fbe4294e4d941c507a6a.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 11,
        "score": 11.0
    },
    "046f6abdbf63fbb80d831102e7889c6801ad3545.pdf": {
        "title": "Conformalized Link Prediction on Graph Neural Networks",
        "authors": [
            "Tianyi Zhao",
            "Jian Kang",
            "Lu Cheng"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/046f6abdbf63fbb80d831102e7889c6801ad3545.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 11,
        "score": 11.0
    },
    "458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf": {
        "title": "Rethinking Fair Graph Neural Networks from Re-balancing",
        "authors": [
            "Zhixun Li",
            "Yushun Dong",
            "Qiang Liu",
            "Jeffrey Xu Yu"
        ],
        "published_date": "2024",
        "abstract": "Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.",
        "file_path": "paper_data/Graph_Neural_Networks/info/458ab8a8a5e139cb744167f5b0890de0b2112b53.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 11,
        "score": 11.0
    },
    "14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf": {
        "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?",
        "authors": [
            "Li",
            "Lecheng Zheng",
            "Bowen Jin",
            "Dongqi Fu",
            "Baoyu Jing",
            "Yikun Ban",
            "Jingrui He",
            "Jiawei Han"
        ],
        "published_date": "2024",
        "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",
        "file_path": "paper_data/Graph_Neural_Networks/info/14c59d6dab548ef023b8a49df4a26b966fe9d00a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 11,
        "score": 11.0
    },
    "fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf": {
        "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
        "authors": [
            "Wei Duan",
            "Jie Lu",
            "Yu Guang Wang",
            "Junyu Xuan"
        ],
        "published_date": "2024",
        "abstract": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
        "file_path": "paper_data/Graph_Neural_Networks/info/fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 11,
        "score": 11.0
    },
    "cfff81fc166668790f4099cebd785cdd20f25b6d.pdf": {
        "title": "Graph Neural Networks for Brain Graph Learning: A Survey",
        "authors": [
            "Xuexiong Luo",
            "Jia Wu",
            "Jian Yang",
            "Shan Xue",
            "Amin Beheshti",
            "Quan Z. Sheng",
            "David Mcalpine",
            "P. Sowman",
            "Alexis Giral",
            "Philip S. Yu"
        ],
        "published_date": "2024",
        "abstract": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cfff81fc166668790f4099cebd785cdd20f25b6d.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 11.0
    },
    "01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf": {
        "title": "Predicting ADMET Properties from Molecule SMILE: A Bottom-Up Approach Using Attention-Based Graph Neural Networks",
        "authors": [
            "Alessandro De Carlo",
            "D. Ronchi",
            "Marco Piastra",
            "E. Tosca",
            "P. Magni"
        ],
        "published_date": "2024",
        "abstract": "Understanding the pharmacokinetics, safety and efficacy of candidate drugs is crucial for their success. One key aspect is the characterization of absorption, distribution, metabolism, excretion and toxicity (ADMET) properties, which require early assessment in the drug discovery and development process. This study aims to present an innovative approach for predicting ADMET properties using attention-based graph neural networks (GNNs). The model utilizes a graph-based representation of molecules directly derived from Simplified Molecular Input Line Entry System (SMILE) notation. Information is processed sequentially, from substructures to the whole molecule, employing a bottom-up approach. The developed GNN is tested and compared with existing approaches using six benchmark datasets and by encompassing regression (lipophilicity and aqueous solubility) and classification (CYP2C9, CYP2C19, CYP2D6 and CYP3A4 inhibition) tasks. Results show the effectiveness of our model, which bypasses the computationally expensive retrieval and selection of molecular descriptors. This approach provides a valuable tool for high-throughput screening, facilitating early assessment of ADMET properties and enhancing the likelihood of drug success in the development pipeline.",
        "file_path": "paper_data/Graph_Neural_Networks/info/01453cd5518b0593e0b01cf8fcaabf43951b2ae4.pdf",
        "venue": "Pharmaceutics",
        "citationCount": 11,
        "score": 11.0
    },
    "0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf": {
        "title": "Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction",
        "authors": [
            "Sahab Zandi",
            "Kamesh Korangi",
            "Mar'ia 'Oskarsd'ottir",
            "Christophe Mues",
            "Cristi'an Bravo"
        ],
        "published_date": "2024",
        "abstract": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0f12a8208c3c586ba2c90c536108dd6f1ac99271.pdf",
        "venue": "European Journal of Operational Research",
        "citationCount": 11,
        "score": 11.0
    },
    "df2701c0fabf50b511182a287d112dfcc84c59b3.pdf": {
        "title": "Effective Fault Scenario Identification for Communication Networks via Knowledge-Enhanced Graph Neural Networks",
        "authors": [
            "Haihong Zhao",
            "Bo Yang",
            "Jiaxu Cui",
            "Qianli Xing",
            "Jiaxing Shen",
            "Fujin Zhu",
            "Jiannong Cao"
        ],
        "published_date": "2024",
        "abstract": "Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.",
        "file_path": "paper_data/Graph_Neural_Networks/info/df2701c0fabf50b511182a287d112dfcc84c59b3.pdf",
        "venue": "IEEE Transactions on Mobile Computing",
        "citationCount": 11,
        "score": 11.0
    },
    "1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf": {
        "title": "Knowledge mapping of graph neural networks for drug discovery: a bibliometric and visualized analysis",
        "authors": [
            "Rufan Yao",
            "Zhenhua Shen",
            "Xinyi Xu",
            "Guixia Ling",
            "Rongwu Xiang",
            "Tingyan Song",
            "Fei Zhai",
            "Yuxuan Zhai"
        ],
        "published_date": "2024",
        "abstract": "Introduction In recent years, graph neural network has been extensively applied to drug discovery research. Although researchers have made significant progress in this field, there is less research on bibliometrics. The purpose of this study is to conduct a comprehensive bibliometric analysis of graph neural network applications in drug discovery in order to identify current research hotspots and trends, as well as serve as a reference for future research. Methods Publications from 2017 to 2023 about the application of graph neural network in drug discovery were collected from the Web of Science Core Collection. Bibliometrix, VOSviewer, and Citespace were mainly used for bibliometric studies. Results and Discussion In this paper, a total of 652 papers from 48 countries/regions were included. Research interest in this field is continuously increasing. China and the United States have a significant advantage in terms of funding, the number of publications, and collaborations with other institutions and countries. Although some cooperation networks have been formed in this field, extensive worldwide cooperation still needs to be strengthened. The results of the keyword analysis clarified that graph neural network has primarily been applied to drug-target interaction, drug repurposing, and drug-drug interaction, while graph convolutional neural network and its related optimization methods are currently the core algorithms in this field. Data availability and ethical supervision, balancing computing resources, and developing novel graph neural network models with better interpretability are the key technical issues currently faced. This paper analyzes the current state, hot spots, and trends of graph neural network applications in drug discovery through bibliometric approaches, as well as the current issues and challenges in this field. These findings provide researchers with valuable insights on the current status and future directions of this field.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1c7ed2d3fa82c0e1c010f3c2fd0fb5c33d71e050.pdf",
        "venue": "Frontiers in Pharmacology",
        "citationCount": 11,
        "score": 11.0
    },
    "7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf": {
        "title": "Predicting Cardiotoxicity of Molecules Using Attention-Based Graph Neural Networks",
        "authors": [
            "Tuan Vinh",
            "Loc Nguyen",
            "Quang H. Trinh",
            "T. Nguyen-Vo",
            "Binh P. Nguyen"
        ],
        "published_date": "2024",
        "abstract": "In drug discovery, the search for new and effective medications is often hindered by concerns about toxicity. Numerous promising molecules fail to pass the later phases of drug development due to strict toxicity assessments. This challenge significantly increases the cost, time, and human effort needed to discover new therapeutic molecules. Additionally, a considerable number of drugs already on the market have been withdrawn or re-evaluated because of their unwanted side effects. Among the various types of toxicity, drug-induced heart damage is a severe adverse effect commonly associated with several medications, especially those used in cancer treatments. Although a number of computational approaches have been proposed to identify the cardiotoxicity of molecules, the performance and interpretability of the existing approaches are limited. In our study, we proposed a more effective computational framework to predict the cardiotoxicity of molecules using an attention-based graph neural network. Experimental results indicated that the proposed framework outperformed the other methods. The stability of the model was also confirmed by our experiments. To assist researchers in evaluating the cardiotoxicity of molecules, we have developed an easy-to-use online web server that incorporates our model.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7b40447c5acab7a7bf9a3a94dc0dfc05097de70f.pdf",
        "venue": "Journal of Chemical Information and Modeling",
        "citationCount": 11,
        "score": 11.0
    },
    "326430bd401c2ac820fc08a0a198ceacf1cde506.pdf": {
        "title": "Physics-Informed Graph Neural Networks for Water Distribution Systems",
        "authors": [
            "Inaam Ashraf",
            "Janine Strotherm",
            "L. Hermes",
            "Barbara Hammer"
        ],
        "published_date": "2024",
        "abstract": "Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/326430bd401c2ac820fc08a0a198ceacf1cde506.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 11.0
    },
    "2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf": {
        "title": "Graph Attention Site Prediction (GrASP): Identifying Druggable Binding Sites Using Graph Neural Networks with Attention",
        "authors": [
            "Zachary Smith",
            "Michael Strobel",
            "Bodhi P. Vani",
            "P. Tiwary"
        ],
        "published_date": "2024",
        "abstract": "Identifying and discovering druggable protein binding sites is an important early step in computer-aided drug discovery, but it remains a difficult task where most campaigns rely on a priori knowledge of binding sites from experiments. Here, we present a binding site prediction method called Graph Attention Site Prediction (GrASP) and re-evaluate assumptions in nearly every step in the site prediction workflow from data set preparation to model evaluation. GrASP is able to achieve state-of-the-art performance at recovering binding sites in PDB structures while maintaining a high degree of precision which will minimize wasted computation in downstream tasks such as docking and free energy perturbation.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2ac95dc1a4cbae71805481ac4e2d20fe611d4a24.pdf",
        "venue": "Journal of Chemical Information and Modeling",
        "citationCount": 11,
        "score": 11.0
    },
    "c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf": {
        "title": "Graph neural networks for electroencephalogram analysis: Alzheimer's disease and epilepsy use cases",
        "authors": [
            "S. Abadal",
            "Pablo Galv\u00e1n",
            "Alberto M\u00e1rmol",
            "N. Mammone",
            "C. Ieracitano",
            "Michele Lo Giudice",
            "Alessandro Salvini",
            "F. Morabito"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/c37bdefdf1ea06d47c5cc157d383019bb38f7b86.pdf",
        "venue": "Neural Networks",
        "citationCount": 11,
        "score": 11.0
    },
    "478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf": {
        "title": "Recurrent Graph Neural Networks and Their Connections to Bisimulation and Logic",
        "authors": [
            "Maximilian Pflueger",
            "David J. Tena Cucala",
            "Egor V. Kostylev"
        ],
        "published_date": "2024",
        "abstract": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion.",
        "file_path": "paper_data/Graph_Neural_Networks/info/478dfb1bb3b634176c06631b3c53c01bfc566fbc.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 10,
        "score": 10.0
    },
    "817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf": {
        "title": "Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions",
        "authors": [
            "H. Mohammadi",
            "Waldemar Karwowski"
        ],
        "published_date": "2024",
        "abstract": "Brain connectivity analysis plays a crucial role in unraveling the complex network dynamics of the human brain, providing insights into cognitive functions, behaviors, and neurological disorders. Traditional graph-theoretical methods, while foundational, often fall short in capturing the high-dimensional and dynamic nature of brain connectivity. Graph Neural Networks (GNNs) have recently emerged as a powerful approach for this purpose, with the potential to improve diagnostics, prognostics, and personalized interventions. This review examines recent studies leveraging GNNs in brain connectivity analysis, focusing on key methodological advancements in multimodal data integration, dynamic connectivity, and interpretability across various imaging modalities, including fMRI, MRI, DTI, PET, and EEG. Findings reveal that GNNs excel in modeling complex, non-linear connectivity patterns and enable the integration of multiple neuroimaging modalities to provide richer insights into both healthy and pathological brain networks. However, challenges remain, particularly in interpretability, data scarcity, and multimodal integration, limiting the full clinical utility of GNNs. Addressing these limitations through enhanced interpretability, optimized multimodal techniques, and expanded labeled datasets is crucial to fully harness the potential of GNNs for neuroscience research and clinical applications.",
        "file_path": "paper_data/Graph_Neural_Networks/info/817a464866200a7e9f2b84dbdc01b94eaa8b958d.pdf",
        "venue": "Brain Science",
        "citationCount": 10,
        "score": 10.0
    },
    "0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf": {
        "title": "Inductive Lottery Ticket Learning for Graph Neural Networks",
        "authors": [
            "Yongduo Sui",
            "Xiang Wang",
            "Tianlong Chen",
            "Meng Wang",
            "Xiang He",
            "Tat-Seng Chua"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/0cc4ae73151d9d6071a64bf1f59a1d76e1d61752.pdf",
        "venue": "Journal of Computational Science and Technology",
        "citationCount": 10,
        "score": 10.0
    },
    "ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf": {
        "title": "PowerGraph: A power grid benchmark dataset for graph neural networks",
        "authors": [
            "Anna Varbella",
            "Kenza Amara",
            "B. Gjorgiev",
            "G. Sansavini"
        ],
        "published_date": "2024",
        "abstract": "Power grids are critical infrastructures of paramount importance to modern society and, therefore, engineered to operate under diverse conditions and failures. The ongoing energy transition poses new challenges for the decision-makers and system operators. Therefore, developing grid analysis algorithms is important for supporting reliable operations. These key tools include power flow analysis and system security analysis, both needed for effective operational and strategic planning. The literature review shows a growing trend of machine learning (ML) models that perform these analyses effectively. In particular, Graph Neural Networks (GNNs) stand out in such applications because of the graph-based structure of power grids. However, there is a lack of publicly available graph datasets for training and benchmarking ML models in electrical power grid applications. First, we present PowerGraph, which comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. Second, we provide ground-truth explanations for the cascading failure analysis. Finally, we perform a complete benchmarking of GNN methods for node-level and graph-level tasks and explainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse tasks that includes power flow and fault scenarios with real-world explanations, providing a valuable resource for developing improved GNN models for node-level, graph-level tasks and explainability methods in power system modeling. The dataset is available at https://figshare.com/articles/dataset/PowerGraph/22820534 and the code at https://github.com/PowerGraph-Datasets.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ef1edab0efdf0ecb4d0578c003ed097a4d607e4c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 10,
        "score": 10.0
    },
    "93ad698088aa72fcbd5004bd59ff38c25054f319.pdf": {
        "title": "Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep Graph Neural Networks",
        "authors": [
            "Jie Peng",
            "Runlin Lei",
            "Zhewei Wei"
        ],
        "published_date": "2024",
        "abstract": "The drastic performance degradation of Graph Neural Networks (GNNs) as the depth of the graph propagation layers exceeds 8-10 is widely attributed to a phenomenon of Over-smoothing. Although recent research suggests that Over-smoothing may not be the dominant reason for such a performance degradation, they have not provided rigorous analysis from a theoretical view, which warrants further investigation In this paper, we systematically analyze the real dominant problem in deep GNNs and identify the issues that these GNNs towards addressing Over-smoothing essentially work on via empirical experiments and theoretical gradient analysis. We theoretically prove that the difficult training problem of deep MLPs is actually the main challenge, and various existing methods that supposedly tackle Over-smoothing actually improve the trainability of MLPs, which is the main reason for their performance gains. Our further investigation into trainability issues reveals that properly constrained smaller upper bounds of gradient flow notably enhance the trainability of GNNs. Experimental results on diverse datasets demonstrate consistency between our theoretical findings and empirical evidence. Our analysis provides new insights in constructing deep graph models.",
        "file_path": "paper_data/Graph_Neural_Networks/info/93ad698088aa72fcbd5004bd59ff38c25054f319.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 10,
        "score": 10.0
    },
    "854342cf063eef4428a5441c8d317dfbabb8117f.pdf": {
        "title": "How Interpretable Are Interpretable Graph Neural Networks?",
        "authors": [
            "Yongqiang Chen",
            "Yatao Bian",
            "Bo Han",
            "James Cheng"
        ],
        "published_date": "2024",
        "abstract": "Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/854342cf063eef4428a5441c8d317dfbabb8117f.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 10,
        "score": 10.0
    },
    "c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf": {
        "title": "Causal Graph Neural Networks for Wildfire Danger Prediction",
        "authors": [
            "Shan Zhao",
            "Ioannis Prapas",
            "Ilektra Karasante",
            "Zhitong Xiong",
            "Ioannis Papoutsis",
            "G. Camps-Valls",
            "Xiao Xiang Zhu"
        ],
        "published_date": "2024",
        "abstract": "Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c54e9dd9f47b983e64fa1c7849c1acbbb708d53c.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "9391738dff06189f64ced951df6c1848311731dc.pdf": {
        "title": "X-MeshGraphNet: Scalable Multi-Scale Graph Neural Networks for Physics Simulation",
        "authors": [
            "M. A. Nabian"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have gained significant traction for simulating complex physical systems, with models like MeshGraphNet demonstrating strong performance on unstructured simulation meshes. However, these models face several limitations, including scalability issues, requirement for meshing at inference, and challenges in handling long-range interactions. In this work, we introduce X-MeshGraphNet, a scalable, multi-scale extension of MeshGraphNet designed to address these challenges. X-MeshGraphNet overcomes the scalability bottleneck by partitioning large graphs and incorporating halo regions that enable seamless message passing across partitions. This, combined with gradient aggregation, ensures that training across partitions is equivalent to processing the entire graph at once. To remove the dependency on simulation meshes, X-MeshGraphNet constructs custom graphs directly from tessellated geometry files (e.g., STLs) by generating point clouds on the surface or volume of the object and connecting k-nearest neighbors. Additionally, our model builds multi-scale graphs by iteratively combining coarse and fine-resolution point clouds, where each level refines the previous, allowing for efficient long-range interactions. Our experiments demonstrate that X-MeshGraphNet maintains the predictive accuracy of full-graph GNNs while significantly improving scalability and flexibility. This approach eliminates the need for time-consuming mesh generation at inference, offering a practical solution for real-time simulation across a wide range of applications. The code for reproducing the results presented in this paper is available through NVIDIA Modulus.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9391738dff06189f64ced951df6c1848311731dc.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "d0cd5ede6535f617e40b58517fe593b648b737b0.pdf": {
        "title": "Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?",
        "authors": [
            "Jiacheng Cen",
            "Anyi Li",
            "Ning Lin",
            "Yuxiang Ren",
            "Zihe Wang",
            "Wenbing Huang"
        ],
        "published_date": "2024",
        "abstract": "Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d0cd5ede6535f617e40b58517fe593b648b737b0.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 10,
        "score": 10.0
    },
    "819d9ef75975c78c5ce12e54af93737f4b698f55.pdf": {
        "title": "Graph Neural Networks with Soft Association between Topology and Attribute",
        "authors": [
            "Yachao Yang",
            "Yanfeng Sun",
            "Shaofan Wang",
            "Jipeng Guo",
            "Junbin Gao",
            "Fujiao Ju",
            "Baocai Yin"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA.",
        "file_path": "paper_data/Graph_Neural_Networks/info/819d9ef75975c78c5ce12e54af93737f4b698f55.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 10,
        "score": 10.0
    },
    "6826db50e96adb61ecc437809a361b16ea7546a7.pdf": {
        "title": "Hybrid-LLM-GNN: Integrating Large Language Models and Graph Neural Networks for Enhanced Materials Property Prediction",
        "authors": [
            "Youjia Li",
            "Vishu Gupta",
            "Muhammed Nur Talha Kilic",
            "Kamal Choudhary",
            "D. Wines",
            "Wei-keng Liao",
            "Alok N. Choudhary",
            "Ankit Agrawal"
        ],
        "published_date": "2024",
        "abstract": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction...",
        "file_path": "paper_data/Graph_Neural_Networks/info/6826db50e96adb61ecc437809a361b16ea7546a7.pdf",
        "venue": "Digital Discovery",
        "citationCount": 10,
        "score": 10.0
    },
    "9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf": {
        "title": "ConfigReco: Network Configuration Recommendation With Graph Neural Networks",
        "authors": [
            "Zhenbei Guo",
            "Fuliang Li",
            "Jiaxing Shen",
            "Tangzheng Xie",
            "Shan Jiang",
            "Xingwei Wang"
        ],
        "published_date": "2024",
        "abstract": "Configuration synthesis is a fundamental technology in the context of self-driving networks, aimed at mitigating network outages by intelligently and automatically generating configurations that align with network intents. However, existing tools often fall short in meeting the practical requirements of network operators, particularly in terms of generality and scalability. Moreover, these tools disregard manual configuration which remains the primary method employed for daily network management. To address these challenges, this paper introduces ConfigReco, a novel, versatile, and scalable configuration recommendation tool tailored for manual configuration. ConfigReco facilitates the automatic generation of configuration templates based on the network operator\u2019s intent. First, ConfigReco leverages existing configurations as input and models them using a knowledge graph. Second, graph neural networks are employed by ConfigReco to estimate the significance of nodes within the configuration knowledge graph. Lastly, configuration recommendations are made by ConfigReco based on the computed importance scores. A prototype system has been implemented to substantiate the effectiveness of ConfigReco, and its performance has been evaluated using real-world configurations. The experimental results demonstrate that ConfigReco achieves a coverage rate of 93.35% while concurrently maintaining a redundancy rate of 23.07% within a configuration knowledge graph comprising 890,464 edges and 40,885 nodes. Furthermore, ConfigReco exhibits high scalability, enabling its applicability to arbitrary datasets, while simultaneously providing efficient recommendations within a response time of 1 second.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9098bfc2cbc5e8b31d0ed9d36dd3a93e2eed9ce8.pdf",
        "venue": "IEEE Network",
        "citationCount": 10,
        "score": 10.0
    },
    "8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf": {
        "title": "Enhanced Drug-Drug Interaction Prediction with Graph Neural Networks and SVM",
        "authors": [
            "A. Gnanabaskaran",
            "K. Bharathi",
            "S. P. Nandakumar",
            "B. Sanjay",
            "R. Praveen"
        ],
        "published_date": "2024",
        "abstract": "Predicting drug-drug interactions (DDIs) is crucial for medication safety and efficacy. Traditional methods often face challenges in capturing complex interactions within large-scale drug networks. In this study, we propose an enhanced approach leveraging Graph Neural Networks (GNNs) and Support Vector Machines (SVM) to improve DDI prediction accuracy. Our method integrates graph representation learning with SVM-based classification, effectively capturing intricate relationships between drugs and their interactions. Through extensive experimentation on benchmark datasets, we demonstrate superior performance compared to existing methods, achieving higher prediction accuracy of ${9 7 \\%}$ and f1 measure of ${9 8 \\%}$. Moreover, our approach offers interpretability, enabling insights into underlying interaction mechanisms. Overall, our study highlights the potential of combining GNNs and SVM for advancing drug interaction prediction, with implications for enhancing medication safety and clinical decision-making.",
        "file_path": "paper_data/Graph_Neural_Networks/info/8e70cc96f707340e9b802d03bc1ab4b41d6ef6cf.pdf",
        "venue": "2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)",
        "citationCount": 10,
        "score": 10.0
    },
    "edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf": {
        "title": "Spatio-Spectral Graph Neural Networks",
        "authors": [
            "Simon Geisler",
            "Arthur Kosmala",
            "Daniel Herbst",
            "Stephan Gunnemann"
        ],
        "published_date": "2024",
        "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their\"receptive field\"is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
        "file_path": "paper_data/Graph_Neural_Networks/info/edfedeb1e9cf26b3537f87926d6f6cd3a05efcaf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 10,
        "score": 10.0
    },
    "a6c060ab3b997675075415253e0a6bc81591f32e.pdf": {
        "title": "FL-GNNs: Robust Network Representation via Feature Learning Guided Graph Neural Networks",
        "authors": [
            "Beibei Wang",
            "Bo Jiang",
            "Chris H. Q. Ding"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have been widely developed and grown rapidly to address representation and learning for attribute graph data <inline-formula><tex-math notation=\"LaTeX\">$G(A,X)$</tex-math></inline-formula>. However, existing studies on GNNs mainly focus on the message passing on graph <inline-formula><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> for layer-wise propagation while pay less attention to the robust learning for the input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>, which thus make existing GNNs often perform susceptibility w.r.t feature noises and adversarial perturbations in <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula>. In this article, we propose a novel Feature Learning guided Graph Neural Networks (FL-GNNs) by incorporating robust feature learning into GNNs. The core of FL-GNNs is trying to recover (or learn) a more clean and optimal feature data <inline-formula><tex-math notation=\"LaTeX\">$Z$</tex-math></inline-formula> from input features <inline-formula><tex-math notation=\"LaTeX\">$X$</tex-math></inline-formula> that better serves GNNs learning by jointly conducting feature reconstruction and GNNs' learning simultaneously. FL-GNNs is general and can be incorporated into any specific GNN models to enhance their robustness. An efficient algorithm has been derived to optimize FL-GNNs. Experimental results show that FL-GNNs can obviously enhance the robustness of existing GCN and GAT w.r.t feature noises and adversarial perturbations.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a6c060ab3b997675075415253e0a6bc81591f32e.pdf",
        "venue": "IEEE Transactions on Network Science and Engineering",
        "citationCount": 10,
        "score": 10.0
    },
    "73765285b243a53143912b501f7afab98a0c8cb0.pdf": {
        "title": "Power Control for 6G In-Factory Subnetworks With Partial Channel Information Using Graph Neural Networks",
        "authors": [
            "Daniel Abode",
            "Ramoni O. Adeogun",
            "Gilberto Berardinelli"
        ],
        "published_date": "2024",
        "abstract": "Transmit power control (PC) will become increasingly crucial in alleviating interference as the densification of the wireless networks continues towards 6G. However, the practicality of most PC methods suffers from high complexity, including the sensing and signalling overhead needed to obtain channel state information. In a highly dense scenario such as the deployment of short-range cells installed within production entities, termed in-factory subnetworks (InF-S), sensing and signalling overhead become a major limitation. In this paper, we represent the InF-S as a graph and resort to graph neural networks for solving the power control problem. We present four graph-attribution methods requiring different degrees of channel information corresponding to different levels of sensing and signalling overhead and study the complexity and performance tradeoffs of the resulting power control graph neural network (PCGNN) algorithms. We then propose a PCGNN method with scalable sensing and signalling graph attribution which can meet the stringent outage target while maximizing the global performance by 10% relative to fixed power control. We further verified the size generalizability and robustness of the PCGNN methods to different network settings.",
        "file_path": "paper_data/Graph_Neural_Networks/info/73765285b243a53143912b501f7afab98a0c8cb0.pdf",
        "venue": "IEEE Open Journal of the Communications Society",
        "citationCount": 10,
        "score": 10.0
    },
    "f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf": {
        "title": "Disambiguated Node Classification with Graph Neural Networks",
        "authors": [
            "Tianxiang Zhao",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, DisamGCL which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. DisamGCL identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. DisamGCL promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of DisamGCL and highlight its potential to improve GNN performance in underrepresented graph regions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f594bdb17619192c0db95fbda124ab0d7c6a02fe.pdf",
        "venue": "The Web Conference",
        "citationCount": 10,
        "score": 10.0
    },
    "aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf": {
        "title": "Collaborative weighting in federated graph neural networks for disease classification with the human-in-the-loop",
        "authors": [
            "Christian Hausleitner",
            "Heimo Mueller",
            "Andreas Holzinger",
            "B. Pfeifer"
        ],
        "published_date": "2024",
        "abstract": "The authors introduce a novel framework that integrates federated learning with Graph Neural Networks (GNNs) to classify diseases, incorporating Human-in-the-Loop methodologies. This advanced framework innovatively employs collaborative voting mechanisms on subgraphs within a Protein-Protein Interaction (PPI) network, situated in a federated ensemble-based deep learning context. This methodological approach marks a significant stride in the development of explainable and privacy-aware Artificial Intelligence, significantly contributing to the progression of personalized digital medicine in a responsible and transparent manner.",
        "file_path": "paper_data/Graph_Neural_Networks/info/aeef4d9ded9a979ef042c8e32ac024ea55a352f3.pdf",
        "venue": "Scientific Reports",
        "citationCount": 10,
        "score": 10.0
    },
    "dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf": {
        "title": "Beyond Grid Data: Exploring graph neural networks for Earth observation",
        "authors": [
            "Shan Zhao",
            "Zhaiyu Chen",
            "Zhitong Xiong",
            "Yilei Shi",
            "Sudipan Saha",
            "Xiao Xiang Zhu"
        ],
        "published_date": "2024",
        "abstract": "Earth observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph neural networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs\u2019 applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight the methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the article by comparing them with other popular architectures, like Transformers, and analyzing their potential synergies.",
        "file_path": "paper_data/Graph_Neural_Networks/info/dc6a9f5692981e39ce572f01e1ebc21073adf2e1.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Magazine",
        "citationCount": 10,
        "score": 10.0
    },
    "9fb72c9292bf80f9825e0038d34cef57468a2757.pdf": {
        "title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks",
        "authors": [
            "T. Konstantin Rusch",
            "Nathan Kirk",
            "M. Bronstein",
            "Christiane Lemieux",
            "Daniela Rus"
        ],
        "published_date": "2024",
        "abstract": "Significance This article introduces Message-Passing Monte Carlo (MPMC), a machine learning approach for generating low-discrepancy point sets which are essential for efficiently filling space in a uniform manner, and thus play a central role in many problems in science and engineering. To accomplish this, MPMC utilizes tools from Geometric Deep Learning, specifically by employing graph neural networks. MPMC can be extended to a higher-dimensional case which further allows for generating custom-made points. Finally, MPMC point sets significantly outperform previous methods, achieving near-optimal discrepancy in practice for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. This advancement holds promise for enhancing efficiency in fields like scientific computing, computer vision, machine learning, and simulation.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9fb72c9292bf80f9825e0038d34cef57468a2757.pdf",
        "venue": "Proceedings of the National Academy of Sciences of the United States of America",
        "citationCount": 10,
        "score": 10.0
    },
    "b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf": {
        "title": "Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels",
        "authors": [
            "Fali Wang",
            "Tianxiang Zhao",
            "Suhang Wang"
        ],
        "published_date": "2024",
        "abstract": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identify pseudo-labeled nodes that both are informative and capable of redeeming the distribution discrepancy and formulate it as a differentiable optimization task. A distribution-shift-aware edge predictor is further adopted to augment the graph and increase the model's generalizability in assigning pseudo labels. We evaluate our proposed method on four publicly available benchmark datasets and extensive experiments demonstrate that our framework consistently outperforms state-of-the-art baselines.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b9fd7e5f9480166b6c42c4c1010c3fecb8b0c41e.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 10,
        "score": 10.0
    },
    "2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf": {
        "title": "Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment",
        "authors": [
            "Bingyao Liu",
            "Iris Li",
            "Jianhua Yao",
            "Yuan Chen",
            "Guanming Huang",
            "Jiajing Wang"
        ],
        "published_date": "2024",
        "abstract": "This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good \u201crobustness\u201d.",
        "file_path": "paper_data/Graph_Neural_Networks/info/2ff0612d73f1e6c4b0cf8b1923dca9b400c1fd38.pdf",
        "venue": "2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)",
        "citationCount": 10,
        "score": 10.0
    },
    "5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf": {
        "title": "Leveraging Persistent Homology Features for Accurate Defect Formation Energy Predictions via Graph Neural Networks",
        "authors": [
            "Zhenyao Fang",
            "Qimin Yan"
        ],
        "published_date": "2024",
        "abstract": "In machine-learning-assisted high-throughput defect studies, a defect-aware latent representation of the supercell structure is crucial for the accurate prediction of defect properties. The performance of current graph neural network (GNN) models is limited due to the fact that defect properties depend strongly on the local atomic configurations near the defect sites and due to the oversmoothing problem of GNN. Herein, we demonstrate that persistent homology features, which encode the topological information on the local chemical environment around each atomic site, can characterize the structural information on defects. Using the dataset containing a wide spectrum of O-based perovskites with all available vacancies as an example, we show that incorporating the persistent homology features, along with proper choices of graph pooling operations, significantly increases the prediction accuracy, with the MAE reduced by 55%. Those features can be easily integrated into the state-of-the-art GNN models, including the graph Transformer network and the equivariant neural network, and universally improve their performance. Besides, our model also overcomes the convergence issue with respect to the supercell size that was present in previous GNN models. Furthermore, using the datasets of defective BaTiO3 with multiple substitutions and multiple vacancies as examples, our GNN model can also predict the defect\u2013defect interactions accurately. These results suggest that persistent homology features can effectively improve the performance of machine learning models and assist the accelerated discovery of functional defects for technological applications.",
        "file_path": "paper_data/Graph_Neural_Networks/info/5fbadeb6453d814f09c611e1eb41a1414690e5b0.pdf",
        "venue": "Chemistry of Materials",
        "citationCount": 10,
        "score": 10.0
    },
    "242425415e28da757bb9c7d24dd0a99654d66027.pdf": {
        "title": "Decidability of Graph Neural Networks via Logical Characterizations",
        "authors": [
            "Michael Benedikt",
            "Chia-Hsuan Lu",
            "Boris Motik",
            "Tony Tan"
        ],
        "published_date": "2024",
        "abstract": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving\"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
        "file_path": "paper_data/Graph_Neural_Networks/info/242425415e28da757bb9c7d24dd0a99654d66027.pdf",
        "venue": "International Colloquium on Automata, Languages and Programming",
        "citationCount": 9,
        "score": 9.0
    },
    "da3c1508794ba0d4f070a9bc47b06575422f456f.pdf": {
        "title": "Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning",
        "authors": [
            "Yuelin Zhang",
            "Jiacheng Cen",
            "Jiaqi Han",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "Wenbing Huang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/da3c1508794ba0d4f070a9bc47b06575422f456f.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 9,
        "score": 9.0
    },
    "0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf": {
        "title": "The Expressive Power of Path-Based Graph Neural Networks",
        "authors": [
            "Caterina Graziani",
            "Tamara Drucks",
            "Fabian Jogl",
            "M. Bianchini",
            "F. Scarselli",
            "Thomas G\u00e4rtner"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Graph_Neural_Networks/info/0d4ff749c180b305cf85ed36cb4243efbbd975f0.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 9,
        "score": 9.0
    },
    "f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf": {
        "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
        "authors": [
            "Haoyu Han",
            "Juanhui Li",
            "Wei Huang",
            "Xianfeng Tang",
            "Hanqing Lu",
            "Chen Luo",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f5aa366ff70215f06ae6501c322eba2f0934a7c3.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0
    },
    "a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf": {
        "title": "Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks",
        "authors": [
            "Dai Shi",
            "Andi Han",
            "Lequan Lin",
            "Yi Guo",
            "Zhiyong Wang",
            "Junbin Gao"
        ],
        "published_date": "2024",
        "abstract": "Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a8a45eaa5bb86cb50620ab984be5ec82a1bf558d.pdf",
        "venue": "International Journal of Machine Learning and Cybernetics",
        "citationCount": 9,
        "score": 9.0
    },
    "c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf": {
        "title": "R-GNN: recurrent graph neural networks for font classification of oracle bone inscriptions",
        "authors": [
            "Jiang Yuan",
            "Shanxiong Chen",
            "Bofeng Mo",
            "Yuqi Ma",
            "Wenjun Zheng",
            "Chongsheng Zhang"
        ],
        "published_date": "2024",
        "abstract": "Font classification of oracle bone inscriptions serves as a crucial basis for determining the historical period to which they belong and holds significant importance in reconstructing significant historical events. However, conventional methods for font classification in oracle bone inscriptions heavily rely on expert knowledge, resulting in low efficiency and time-consuming procedures. In this paper, we proposed a novel recurrent graph neural network (R-GNN) for the automatic recognition of oracle bone inscription fonts. The proposed method used convolutional neural networks (CNNs) to perform local feature extraction and downsampling on oracle bone inscriptions. Furthermore, it employed graph neural networks (GNNs) to model the complex topologiure and global contextual information of oracle bone inscriptions. Finally, we used recurrent neural networks (RNNs) to effectively combine the extracted local features and global contextual information, thereby enhancing the discriminative power of the R-GNN. Extensive experiments on our benchmark dataset demonstrate that the proposed method achieves a Top-1 accuracy of 88.2%, significantly outperforming the competing approaches. The method presented in this paper further advances the integration of oracle bone inscriptions research and artificial intelligence. The code is publicly available at: https://github.com/yj3214/oracle-font-classification .",
        "file_path": "paper_data/Graph_Neural_Networks/info/c684c041c90ccac42d3b8cced9ec2b25f1a905c0.pdf",
        "venue": "Heritage Science",
        "citationCount": 9,
        "score": 9.0
    },
    "67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf": {
        "title": "A Gearbox Fault Diagnosis Method Based on Graph Neural Networks and Markov Transform Fields",
        "authors": [
            "Haitao Wang",
            "Zelin Liu",
            "Mingjun Li",
            "Xiyang Dai",
            "Ruihua Wang",
            "Lichen Shi"
        ],
        "published_date": "2024",
        "abstract": "Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/67b40db25bf26b14664b0de0f0c9ef7c5d9daf51.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 9,
        "score": 9.0
    },
    "12e60b9fd69f18c1c01996d108229051432b6090.pdf": {
        "title": "Comorbidity-based framework for Alzheimer\u2019s disease classification using graph neural networks",
        "authors": [
            "Ferial Abuhantash",
            "Mohd Khalil Abu Hantash",
            "Aamna AlShehhi"
        ],
        "published_date": "2024",
        "abstract": "Alzheimer\u2019s disease (AD), the most prevalent form of dementia, requires early prediction for timely intervention. Current deep learning approaches, particularly those using traditional neural networks, face challenges such as handling high-dimensional data, interpreting complex relationships, and managing data bias. To address these limitations, we propose a framework utilizing graph neural networks (GNNs), which excel in modeling relationships within graph-structured data. Our study employs GNNs on data from the Alzheimer\u2019s Disease Neuroimaging Initiative for binary and multi-class classification across the three stages of AD: cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD). By incorporating comorbidity data derived from electronic health records, we achieved the most effective multi-classification results. Notably, the GNN model (Chebyshev Convolutional Neural Networks) demonstrated superior performance with a 0.98 accuracy in multi-class classification and 0.99, 0.93, and 0.94 in the AD/CN, AD/MCI, and CN/MCI binary tasks, respectively. The model\u2019s robustness was further validated using the Australian Imaging, Biomarker & Lifestyle dataset as an external validation set. This work contributes to the field by offering a robust, accurate, and cost-effective method for early AD prediction (CN vs. MCI), addressing key challenges in existing deep learning approaches.",
        "file_path": "paper_data/Graph_Neural_Networks/info/12e60b9fd69f18c1c01996d108229051432b6090.pdf",
        "venue": "Scientific Reports",
        "citationCount": 9,
        "score": 9.0
    },
    "c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf": {
        "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
        "authors": [
            "Yassine Abbahaddou",
            "Sofiane Ennadir",
            "J. Lutzeyer",
            "M. Vazirgiannis",
            "Henrik Bostr\u00f6m"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: \\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
        "file_path": "paper_data/Graph_Neural_Networks/info/c4bcb54e36945fc7ddd7892c8c94c4948be1967c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 9,
        "score": 9.0
    },
    "b20589941cd52d199ba381b92e092ba7fb36d689.pdf": {
        "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
        "authors": [
            "Shaswata Mitra",
            "Trisha Chakraborty",
            "Subash Neupane",
            "Aritran Piplai",
            "Sudip Mittal"
        ],
        "published_date": "2024",
        "abstract": "In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint. Furthermore, We also discuss open research areas and further improvement scopes.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b20589941cd52d199ba381b92e092ba7fb36d689.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0
    },
    "35792d528bd07aed95df46f0ecb87019cb123147.pdf": {
        "title": "Towards Inductive and Efficient Explanations for Graph Neural Networks",
        "authors": [
            "Dongsheng Luo",
            "Tianxiang Zhao",
            "Wei Cheng",
            "Dongkuan Xu",
            "Feng Han",
            "Wenchao Yu",
            "Xiao Liu",
            "Haifeng Chen",
            "Xiang Zhang"
        ],
        "published_date": "2024",
        "abstract": "Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging and nascent problem. The leading method mainly considers the local explanations, i.e., important subgraph structure and node features, to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized at the instance level. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, training the explanation model explaining for each instance is time-consuming for large-scale real-life datasets. In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which renders PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has better generalization ability and can be utilized in an inductive setting without training the model for new instances. Thus, PGExplainer is much more efficient than the leading method with significant speed-up. In addition, the explanation networks can also be utilized as a regularizer to improve the generalization power of existing GNNs when jointly trained with downstream tasks. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7% relative improvement in AUC on explaining graph classification over the leading baseline.",
        "file_path": "paper_data/Graph_Neural_Networks/info/35792d528bd07aed95df46f0ecb87019cb123147.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 9,
        "score": 9.0
    },
    "7bda10706047e154e22259c4b20d70240296963e.pdf": {
        "title": "Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Networks",
        "authors": [
            "Renhong Huang",
            "Jiarong Xu",
            "Xin Jiang",
            "Chenglu Pan",
            "Zhiming Yang",
            "Chunping Wang",
            "Yang Yang"
        ],
        "published_date": "2024",
        "abstract": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/7bda10706047e154e22259c4b20d70240296963e.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 9,
        "score": 9.0
    },
    "a3340cd6f24e4c83bec616c7bda719737492fe74.pdf": {
        "title": "Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces",
        "authors": [
            "Yue Jiang",
            "Changkong Zhou",
            "Vikas Garg",
            "A. Oulasvirta"
        ],
        "published_date": "2024",
        "abstract": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning\u2019s potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements\u2019 properties and their semantic\u2014visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model\u2019s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a3340cd6f24e4c83bec616c7bda719737492fe74.pdf",
        "venue": "International Conference on Human Factors in Computing Systems",
        "citationCount": 9,
        "score": 9.0
    },
    "abd2ac274abe25f40f5268324d4774e67b467ef8.pdf": {
        "title": "Gear Fault Diagnosis Method Based on the Optimized Graph Neural Networks",
        "authors": [
            "Bin Wang",
            "Yadong Xu",
            "Manyi Wang",
            "Yanze Li"
        ],
        "published_date": "2024",
        "abstract": "The gear fault diagnosis technology based on the signal is crucial for maintaining the normal operation of the gear in the motor drive chain. In some cases, it is challenging to add sensors on the unit of the motor transmission chain for collecting vibration signals in practical engineering applications. However, the current signal can be collected. Nonetheless, due to the long distance between the collection point and the fault source, it becomes difficult to extract the features of the weak gear fault from the current signal. In order to solve the aforementioned problems efficiently, an optimized principal neighborhood aggregation (OPNA) graph neural network (GNN) was proposed to diagnose gear faults in the motor drive chain. First, the current signal is reconstructed to obtain the topological data graph sample by the graph sample construction method proposed in this article. Second, OPNA, an architecture that combines multiple message aggregators with a degree scaler, was designed to extract the features of nodes and edges. Subsequently, the embedding and the particular pooling improvement were used to reduce the number of nodes and achieve steady and rapid classification. Finally, the experimental studies, based on the current signal of the gear dataset, were conducted to validate the effectiveness of the proposed method and its superiority over the traditional methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/abd2ac274abe25f40f5268324d4774e67b467ef8.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 9,
        "score": 9.0
    },
    "3d0911fabeebc22506ac3b006a553448debf03a5.pdf": {
        "title": "Using graph neural networks to predict local culture",
        "authors": [
            "Thiago H. Silva",
            "Daniel Silver"
        ],
        "published_date": "2024",
        "abstract": "Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.",
        "file_path": "paper_data/Graph_Neural_Networks/info/3d0911fabeebc22506ac3b006a553448debf03a5.pdf",
        "venue": "Environment and Planning B Urban Analytics and City Science",
        "citationCount": 9,
        "score": 9.0
    },
    "1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf": {
        "title": "Robust Graph Neural Networks for Stability Analysis in Dynamic Networks",
        "authors": [
            "Xin Zhang",
            "Zhen Xu",
            "Yue Liu",
            "Mengfang Sun",
            "Tong Zhou",
            "Wenying Sun"
        ],
        "published_date": "2024",
        "abstract": "In the current context of accelerated globalization and digitalization, the complexity and uncertainty of financial markets are increasing, and the identification and prevention of economic risks have become a key link in maintaining the stability of the financial system. Traditional risk identification methods often have limitations because they are difficult to cope with the multi-level and dynamically changing complex relationships in financial networks. With the rapid development of financial technology, graph neural network (GNN) technology, as an emerging deep learning method, has gradually shown great potential in the field of financial risk management. GNN can map transaction behaviors, financial institutions, individuals and their interactive relationships in financial networks into graph structures, and effectively capture potential patterns and abnormal signals in financial data through embedded representation learning. Using this technology, financial institutions can extract valuable information from complex transaction networks, identify hidden dangers or abnormal behaviors that may cause systemic risks in a timely manner, optimize decision-making processes, and improve the accuracy of risk warnings. This paper explores the economic risk identification algorithm based on the GNN algorithm, aiming to provide financial institutions and regulators with more intelligent technical tools to help maintain the security and stability of the financial market. Improving the efficiency of economic risk identification through innovative technical means is expected to further enhance the risk resistance of the financial system and lay the foundation for building a robust global financial system.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1dce1fbef38c60eda4786c52b21423d2af6c7098.pdf",
        "venue": "2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)",
        "citationCount": 9,
        "score": 9.0
    },
    "413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf": {
        "title": "Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis",
        "authors": [
            "Mengfang Sun",
            "Wenying Sun",
            "Ying Sun",
            "Shaobo Liu",
            "Mohan Jiang",
            "Zhen Xu"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.",
        "file_path": "paper_data/Graph_Neural_Networks/info/413b9e59f0fc8f6282a8c05701988633ef4a3812.pdf",
        "venue": "2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)",
        "citationCount": 9,
        "score": 9.0
    },
    "f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf": {
        "title": "GNNGL-PPI: multi-category prediction of protein-protein interactions using graph neural networks based on global graphs and local subgraphs",
        "authors": [
            "Xin Zeng",
            "Fan-Fang Meng",
            "Meng-Liang Wen",
            "Shu-Juan Li",
            "Yi Li"
        ],
        "published_date": "2024",
        "abstract": "Most proteins exert their functions by interacting with other proteins, making the identification of protein-protein interactions (PPI) crucial for understanding biological activities, pathological mechanisms, and clinical therapies. Developing effective and reliable computational methods for predicting PPI can significantly reduce the time-consuming and labor-intensive associated traditional biological experiments. However, accurately identifying the specific categories of protein-protein interactions and improving the prediction accuracy of the computational methods remain dual challenges. To tackle these challenges, we proposed a novel graph neural network method called GNNGL-PPI for multi-category prediction of PPI based on global graphs and local subgraphs. GNNGL-PPI consisted of two main components: using Graph Isomorphism Network (GIN) to extract global graph features from PPI network graph, and employing GIN As Kernel (GIN-AK) to extract local subgraph features from the subgraphs of protein vertices. Additionally, considering the imbalanced distribution of samples in each category within the benchmark datasets, we introduced an Asymmetric Loss (ASL) function to further enhance the predictive performance of the method. Through evaluations on six benchmark test sets formed by three different dataset partitioning algorithms (Random, BFS, DFS), GNNGL-PPI outperformed the state-of-the-art multi-category prediction methods of PPI, as measured by the comprehensive performance evaluation metric F1-measure. Furthermore, interpretability analysis confirmed the effectiveness of GNNGL-PPI as a reliable multi-category prediction method for predicting protein-protein interactions.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f5b2b6fdb71cadef87a87f0ff49b96d6453661ce.pdf",
        "venue": "BMC Genomics",
        "citationCount": 9,
        "score": 9.0
    },
    "1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf": {
        "title": "Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs",
        "authors": [
            "Ziang Chen",
            "Xiaohan Chen",
            "Jialin Liu",
            "Xinshang Wang",
            "Wotao Yin"
        ],
        "published_date": "2024",
        "abstract": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.",
        "file_path": "paper_data/Graph_Neural_Networks/info/1f96eda505cdf04f3b472a8e67fe93fddfcc9784.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf": {
        "title": "Superhypergraph Neural Networks and Plithogenic Graph Neural Networks: Theoretical Foundations",
        "authors": [
            "Takaaki Fujita"
        ],
        "published_date": "2024",
        "abstract": "Hypergraphs extend traditional graphs by allowing edges to connect multiple nodes, while superhypergraphs further generalize this concept to represent even more complex relationships. Neural networks, inspired by biological systems, are widely used for tasks such as pattern recognition, data classification, and prediction. Graph Neural Networks (GNNs), a well-established framework, have recently been extended to Hypergraph Neural Networks (HGNNs), with their properties and applications being actively studied. The Plithogenic Graph framework enhances graph representations by integrating multi-valued attributes, as well as membership and contradiction functions, enabling the detailed modeling of complex relationships. In the context of handling uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained prominence. It is well established that Plithogenic Graphs serve as a generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the Fuzzy Graph Neural Network has been proposed and is an active area of research. This paper establishes the theoretical foundation for the development of SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks, expanding the applicability of neural networks to these advanced graph structures. While mathematical generalizations and proofs are presented, future computational experiments are anticipated.",
        "file_path": "paper_data/Graph_Neural_Networks/info/cfd0ad57ba860c21f876acdc698d1eacd77a4d5c.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf": {
        "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
        "authors": [
            "Ngoc Bui",
            "Hieu Trung Nguyen",
            "Viet Anh Nguyen",
            "Rex Ying"
        ],
        "published_date": "2024",
        "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/a5ef3aac578a430a5624e666ac5d496175cbd99b.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 8,
        "score": 8.0
    },
    "ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf": {
        "title": "Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact",
        "authors": [
            "Mahdi Saleh",
            "Michael Sommersperger",
            "N. Navab",
            "F. Tombari"
        ],
        "published_date": "2024",
        "abstract": "In robotics, it\u2019s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We\u2019ve made our code and dataset public to advance research in robotic simulation and grasping.\u2020",
        "file_path": "paper_data/Graph_Neural_Networks/info/ac7c8f970ffeda45009fc1c3dd8974dde806f6d6.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 8,
        "score": 8.0
    },
    "99d50bb7b0155203c908228d086eb232c34ee0a6.pdf": {
        "title": "Topological Adversarial Attacks on Graph Neural Networks Via Projected Meta Learning",
        "authors": [
            "Mohammed Aburidi",
            "Roummel F. Marcia"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success across diverse domains like social and biological networks. However, their susceptibility to adversarial attacks presents substantial risks in security-sensitive contexts. Even imperceptible perturbations within graphs can lead to considerable performance degradation, highlighting the urgent need for robust GNN models to ensure safety and privacy in critical applications. To address this challenge, we propose training-time optimization-based attacks on GNNs, specifically targeting modifications to graph structures. Our approach revolves around utilizing meta-gradients to tackle the two-level problem inherent in training-time attacks. This involves treating the graph as a hyperparameter to optimize, followed by leveraging convex relaxation and projected momentum optimization techniques to generate the attacks. In our evaluation on node classification tasks, our attacks surpass state-of-the-art methods within the same perturbation budget, underscoring the effectiveness of our approach. Our experiments consistently demonstrate that even minor graph perturbations result in a significant performance decline for graph convolutional networks. Our attacks do not require any prior knowledge of or access to the target classifiers. This research contributes significantly to bolstering the resilience of GNNs against adversarial manipulations in real-world scenarios.",
        "file_path": "paper_data/Graph_Neural_Networks/info/99d50bb7b0155203c908228d086eb232c34ee0a6.pdf",
        "venue": "IEEE Conference on Evolving and Adaptive Intelligent Systems",
        "citationCount": 8,
        "score": 8.0
    },
    "9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf": {
        "title": "Non-convolutional Graph Neural Networks",
        "authors": [
            "Yuanqing Wang",
            "Kyunghyun Cho"
        ],
        "published_date": "2024",
        "abstract": "Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.",
        "file_path": "paper_data/Graph_Neural_Networks/info/9b451516b9432318d81aef2a5bdc0135d2285a5d.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 8,
        "score": 8.0
    },
    "6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf": {
        "title": "NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise",
        "authors": [
            "Zhonghao Wang",
            "Danyu Sun",
            "Sheng Zhou",
            "Haobo Wang",
            "Jiapei Fan",
            "Longtao Huang",
            "Jiajun Bu"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6cd3ac1e47ed0283aa31a9e8710960cc2e217200.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 7,
        "score": 7.0
    },
    "6dc0932670a0b5140a426ca310bbb03783ff2240.pdf": {
        "title": "Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks",
        "authors": [
            "Yuwen Wang",
            "Shunyu Liu",
            "Tongya Zheng",
            "Kaixuan Chen",
            "Mingli Song"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available\u00b9.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6dc0932670a0b5140a426ca310bbb03783ff2240.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 7,
        "score": 7.0
    },
    "ae683dbd44ec508f63254d864f83d6c1006dd652.pdf": {
        "title": "Expressiveness of Graph Neural Networks in Planning Domains",
        "authors": [
            "Rostislav Horc\u00edk",
            "Gustav S\u00edr"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have become the standard method of choice for learning with structured data, demonstrating particular promise in classical planning. Their inherent invariance under symmetries of the input graphs endows them with superior generalization capabilities, compared to their symmetry-oblivious counterparts. However, this comes at the cost of limited expressive power. Particularly, GNNs cannot distinguish between graphs that satisfy identical sentences of C2 logic.\n \nTo leverage GNNs for learning policies in PDDL domains, one needs to encode the contextual representation of the planning states as graphs. The expressiveness of this encoding, coupled with a specific GNN architecture, then hinges on the absence of indistinguishable states necessitating distinct actions. This paper provides a comprehensive theoretical and statistical exploration of such situations in PDDL domains across diverse natural encoding schemes and GNN models.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ae683dbd44ec508f63254d864f83d6c1006dd652.pdf",
        "venue": "International Conference on Automated Planning and Scheduling",
        "citationCount": 7,
        "score": 7.0
    },
    "6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf": {
        "title": "Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation",
        "authors": [
            "Jianshan Sun",
            "Suyuan Mei",
            "Kun Yuan",
            "Yuanchun Jiang",
            "Jie Cao"
        ],
        "published_date": "2024",
        "abstract": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user\u2019s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user\u2019s complex learning patterns, we build an item graph and a category graph from the user\u2019s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user\u2019s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
        "file_path": "paper_data/Graph_Neural_Networks/info/6bcdd7d3f42ebe2d673d2488b31b8e8342e47d58.pdf",
        "venue": "ACM Transactions on Knowledge Discovery from Data",
        "citationCount": 7,
        "score": 7.0
    },
    "675a150a89f5c3dd44bf8312b00a896716c7082b.pdf": {
        "title": "Advancing Cybersecurity: Graph Neural Networks in Threat Intelligence Knowledge Graphs",
        "authors": [
            "Langsha Li",
            "Feng Qiang",
            "Li Ma"
        ],
        "published_date": "2024",
        "abstract": "This paper explores the escalating complexity of network security threats and the critical need for sophisticated analytical tools to understand and combat these threats effectively. We highlight the efficacy of knowledge graphs in elucidating the complex relationships between various network threats and related entities such as threat actors, attack techniques, and vulnerabilities. Traditional machine learning methods often struggle to grasp the intricate dependencies within these graphs, prompting the adoption of Graph Neural Networks (GNNs). GNNs stand out for their ability to learn representations of graph nodes and edges, capturing and propagating relational information to unearth hidden patterns and facilitate advanced threat analysis. We discuss the advantages of GNNs, including their capacity to integrate diverse data types, handle large-scale knowledge graphs, and reveal critical insights that aid in predicting and mitigating network security threats. Through leveraging GNNs' structural and relational analysis capabilities, this paper demonstrates how organizations can enhance their threat intelligence and develop more robust defense mechanisms against the evolving landscape of network security threats.",
        "file_path": "paper_data/Graph_Neural_Networks/info/675a150a89f5c3dd44bf8312b00a896716c7082b.pdf",
        "venue": "International Conference on Algorithms, Software Engineering, and Network Security",
        "citationCount": 7,
        "score": 7.0
    },
    "d42ebd3b0673341125e374223e0882e99557cc8c.pdf": {
        "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
        "authors": [
            "Renqiang Luo",
            "Huafei Huang",
            "Shuo Yu",
            "Zhuoyang Han",
            "Estrid He",
            "Xiuzhen Zhang",
            "Feng Xia"
        ],
        "published_date": "2024",
        "abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
        "file_path": "paper_data/Graph_Neural_Networks/info/d42ebd3b0673341125e374223e0882e99557cc8c.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 7,
        "score": 7.0
    },
    "13391f9fb2094227ecc567fef76fd95adc57e972.pdf": {
        "title": "Towards Bridging Generalization and Expressivity of Graph Neural Networks",
        "authors": [
            "Shouheng Li",
            "F. Geerts",
            "Dongwoo Kim",
            "Qing Wang"
        ],
        "published_date": "2024",
        "abstract": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data. In this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
        "file_path": "paper_data/Graph_Neural_Networks/info/13391f9fb2094227ecc567fef76fd95adc57e972.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 6,
        "score": 6.0
    },
    "406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf": {
        "title": "Graph Neural Networks on Quantum Computers",
        "authors": [
            "Yidong Liao",
            "Xiao-Ming Zhang",
            "Chris Ferrie"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
        "file_path": "paper_data/Graph_Neural_Networks/info/406dc9a0bd5041b4ee9aa87588d239bffe3631b1.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0
    },
    "0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf": {
        "title": "TGLite: A Lightweight Programming Framework for Continuous-Time Temporal Graph Neural Networks",
        "authors": [
            "Yufeng Wang",
            "Charith Mendis"
        ],
        "published_date": "2024",
        "abstract": "In recent years, Temporal Graph Neural Networks (TGNNs) have achieved great success in learning tasks for graphs that change over time. These dynamic/temporal graphs represent topology changes as either discrete static graph snapshots (called DTDGs), or a continuous stream of timestamped edges (called CTDGs). Because continuous-time graphs have richer time information, it will be crucial to have abstractions for programming CTDG-based models so that practitioners can easily explore new designs and optimizations in this space. A few recent frameworks have been proposed for programming and accelerating TGNN models, but these either do not support continuous-time graphs, lack easy composability, and/or do not facilitate CTDG-specific optimizations. In this paper, we propose a lightweight framework called TGLite to fill this apparent gap in the status quo. It provides abstractions that serve as composable building blocks for implementing TGNN models for CTDGs. It introduces a novel TBlock representation for capturing message-flow dependencies between nodes, with explicit support for temporal-related attributes, which is well-suited for common TGNN computation patterns. TBlocks serve as a central representation on which many different operators can be defined, such as temporal neighborhood sampling, scatter/segmented computations, as well as optimizations tailored to CTDGs. We use TGLite to implement four existing TGNN models. Compared to the TGL framework, TGLite is able to accelerate runtime performance of training (1.06 -- 3.43\u00d7) and inference (1.09 -- 4.65\u00d7) of these models on V100 and A100 GPUs across different experimental settings. Notably, when scaling to larger datasets, TGL runs out-of-memory in some cases on the V100 while TGLite is able to run successfully.",
        "file_path": "paper_data/Graph_Neural_Networks/info/0e942b5b1fac76d06807c6a4aeaa884503f534ba.pdf",
        "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
        "citationCount": 6,
        "score": 6.0
    },
    "f60492aece8e86203ed95303cb809332a11d74b5.pdf": {
        "title": "LinkSAGE: Optimizing Job Matching Using Graph Neural Networks",
        "authors": [
            "Ping Liu",
            "Haichao Wei",
            "Xiaochen Hou",
            "Jianqiang Shen",
            "Shihai He",
            "Qianqi Shen",
            "Zhujun Chen",
            "Fedor Borisyuk",
            "Daniel Hewlett",
            "Liang Wu",
            "S. Veeraraghavan",
            "Alex Tsun",
            "Chen-Chen Jiang",
            "Wenjing Zhang"
        ],
        "published_date": "2024",
        "abstract": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
        "file_path": "paper_data/Graph_Neural_Networks/info/f60492aece8e86203ed95303cb809332a11d74b5.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 6,
        "score": 6.0
    },
    "b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf": {
        "title": "SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification",
        "authors": [
            "Alan John Varghese",
            "Zhen Zhang",
            "G. Karniadakis"
        ],
        "published_date": "2024",
        "abstract": "Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.",
        "file_path": "paper_data/Graph_Neural_Networks/info/b06cdd3841e0982e2bed42d856959f8555c2ced0.pdf",
        "venue": "Neural Networks",
        "citationCount": 5,
        "score": 5.0
    },
    "ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf": {
        "title": "VC dimension of Graph Neural Networks with Pfaffian activation functions",
        "authors": [
            "Giuseppe Alessio D\u2019Inverno",
            "M. Bianchini",
            "F. Scarselli"
        ],
        "published_date": "2024",
        "abstract": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion. Based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they were demonstrated to be equivalent (Morris et al., 2019 and Xu et al., 2019). From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability - related to the Vapnik Chervonekis (VC) dimension (Scarselli et al., 2018) - has recently been investigated for GNNs with piecewise polynomial activation functions (Morris et al., 2023). The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as the sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to the architecture parameters (depth, number of neurons, input size) as well as with respect to the number of colors resulting from the 1-WL test applied on the graph domain. The theoretical analysis is supported by a preliminary experimental study.",
        "file_path": "paper_data/Graph_Neural_Networks/info/ce8165ad603302f9aa5d411702a2e5dfb568f6a5.pdf",
        "venue": "Neural Networks",
        "citationCount": 3,
        "score": 3.0
    }
}