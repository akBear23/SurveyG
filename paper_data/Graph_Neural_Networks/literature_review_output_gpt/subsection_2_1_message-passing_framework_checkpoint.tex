\subsection{Message-Passing Framework}

The message-passing paradigm serves as the core mechanism of Graph Neural Networks (GNNs), enabling nodes to exchange information with their neighbors to update their representations. This section delves into the mathematical formulation of message passing, encompassing aggregation and update functions, and underscores the significance of this framework in facilitating GNNs' ability to learn from graph-structured data effectively.

A foundational work by Morris et al. (2018) introduced higher-order GNNs, specifically k-GNNs, which extend the expressiveness of traditional GNNs beyond the limitations imposed by the 1-Weisfeiler-Leman (1-WL) test. By allowing message passing between k-tuples of nodes, k-GNNs can capture complex structural patterns that standard GNNs fail to distinguish, thereby enhancing their performance on tasks requiring higher-order feature recognition \cite{morris20185sd}. However, these models often incur significant computational costs, which can limit their practical applicability.

Building on this foundation, Satorras et al. (2021) proposed E(n)-equivariant GNNs, which maintain equivariance to Euclidean transformations across n-dimensional spaces. This approach not only addresses the need for expressive power in physical and geometric tasks but also emphasizes the importance of incorporating symmetry into the message-passing framework. E(n)-equivariant networks demonstrate that GNNs can effectively model complex interactions while adhering to the principles of equivariance, thus enhancing their robustness and generalization capabilities \cite{satorras2021pzl}.

The exploration of positional encodings has also gained traction as a means to enrich node representations. Wang et al. (2022) introduced a learnable positional encoding framework that decouples structural and positional information within GNNs, allowing for more expressive and adaptable models. This method addresses the inherent limitations of traditional positional encodings, which often fail to capture the nuanced roles of nodes within their graph contexts \cite{wang2022p2r}. By learning positional information in conjunction with structural features, GNNs can better differentiate nodes that may otherwise appear identical based on local neighborhood structures.

In the context of scalability, Geisler et al. (2024) proposed Spatio-Spectral Graph Neural Networks (S2GNNs), which integrate spatial message passing with spectral filtering to enable long-range information propagation. This hybrid approach effectively mitigates the over-squashing phenomenon, allowing GNNs to maintain expressivity while scaling to larger graphs \cite{geisler2024wli}. By combining the strengths of both spatial and spectral methods, S2GNNs represent a significant advancement in the message-passing framework, addressing the challenges of traditional GNN architectures.

Despite these advancements, challenges remain in achieving a balance between expressiveness, computational efficiency, and generalization. For instance, while higher-order GNNs enhance expressive power, they often come at the cost of increased complexity and memory requirements. Future research may focus on developing hybrid models that effectively integrate various approaches, including higher-order representations, positional encodings, and efficient training techniques, to further push the boundaries of GNN capabilities.