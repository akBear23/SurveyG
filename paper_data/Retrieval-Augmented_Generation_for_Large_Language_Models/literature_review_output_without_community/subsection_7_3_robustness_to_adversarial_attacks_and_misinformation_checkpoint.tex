\subsection*{Robustness to Adversarial Attacks and Misinformation}

The integration of Large Language Models (LLMs) with retrieval mechanisms in Retrieval-Augmented Generation (RAG) systems, while promising enhanced factual accuracy, concurrently introduces significant vulnerabilities to adversarial attacks and the inadvertent propagation of misinformation. These challenges critically undermine the integrity and trustworthiness that RAG aims to provide, as carefully crafted malicious inputs or subtly manipulated retrieved documents can lead the LLM to generate incorrect, biased, or even harmful responses. Indeed, a comprehensive survey by \cite{zhou20248fu} highlights that while RAG mitigates some LLM limitations, it can reintroduce safety issues like information leakage and unfairness if retrieved data is problematic, emphasizing the need for a unified framework for RAG trustworthiness encompassing dimensions like factuality and robustness.

Adversarial attacks on RAG systems primarily exploit two main surfaces: the user query (prompt) and the external knowledge base.
**Knowledge Base Poisoning and Manipulation** represents a potent attack vector where malicious content is injected into the retrieval corpus, influencing the LLM's generation. \cite{zou2024haa} introduced PoisonedRAG, a knowledge corruption attack that demonstrates an attacker can inject a few malicious texts into a large knowledge database (e.g., 5 texts per target question in millions of texts) to induce the LLM to generate attacker-chosen target answers for specific questions, achieving a 90\% attack success rate. Similarly, \cite{zhang2024rwm} proposed HijackRAG, a retrieval prompt hijack attack that manipulates the retrieval mechanisms by injecting malicious texts, causing the RAG system to generate pre-determined answers instead of correct ones. Their experiments showed high attack success rates and transferability across different retriever models, underscoring widespread risk. Beyond direct answer manipulation, \cite{clop2024zs2} investigated backdoor attacks on dense retrievers during fine-tuning, demonstrating that corpus poisoning can achieve significant prompt injection success rates with a small number of compromised documents. These attacks aim to insert harmful links, promote unauthorized services, or initiate denial-of-service behaviors. Furthermore, \cite{chen20247nc} explored black-box opinion manipulation attacks, where manipulating the ranking results of the retrieval model can significantly alter the opinion polarity of RAG-generated content, thereby impacting user cognition and decision-making. A particularly concerning form of this vulnerability is demonstrated by \cite{deng2024k1b} with Pandora, a RAG poisoning technique used to jailbreak OpenAI's GPTs. By uploading strategically crafted malicious documents as the RAG knowledge source and using tailored in-built prompts, they achieved jailbreak success rates of 64.3\% for GPT-3.5 and 34.8\% for GPT-4, bypassing existing safety filters.

The implications of such attacks extend beyond mere factual errors, fundamentally impacting the system's trustworthiness and potential for widespread misinformation dissemination. \cite{zhang2025byv} conducted a detailed comparative analysis across eleven LLMs, finding that RAG can paradoxically make models *less safe* and significantly alter their safety profile. They observed that even combinations of ostensibly safe models with safe documents could lead to unsafe generations, highlighting a critical gap in current AI safety research which often focuses on non-RAG LLMs. This suggests that the perceived authority of RAG-generated content, if compromised, makes it a potent vector for spreading misinformation, as users are more likely to trust information presented by an advanced AI system.

Recognizing these vulnerabilities, the research community has begun to explore robust defense mechanisms. \cite{fang2024gh6} proposed Retrieval-augmented Adaptive Adversarial Training (RAAT) to enhance noise robustness in RAG systems. RAAT systematically categorizes retrieval noises into relevant, irrelevant, and counterfactual types, and employs an adaptive adversarial training mechanism that dynamically adjusts the training process based on the model's sensitivity to these diverse noises. Their experiments demonstrated significant improvements in F1 and Exact Match scores for models trained with RAAT under various noise conditions. Another approach, C-RAG, introduced by \cite{kang2024hrb}, provides the first framework to certify generation risks for RAG models. It offers provable guarantees on generation risks, demonstrating that RAG can achieve lower conformal generation risks than vanilla LLMs when the quality of the retrieval model and transformer is non-trivial. General defense strategies, such as rigorous input validation to detect malicious prompts and source verification for retrieved documents, remain crucial. However, the sophistication of attacks like PR-Attack \cite{jiao20259xa}, which uses bilevel optimization to coordinate prompt and RAG attacks for high effectiveness and stealth with limited poisoned texts, necessitates more advanced, multi-layered defenses.

To systematically evaluate and foster the development of secure RAG systems, specialized benchmarks are emerging. \cite{liang2025f4q} introduced SafeRAG, a benchmark designed to assess RAG security across various attack tasks, including silver noise, inter-context conflict, soft advertisements, and white Denial-of-Service. Their experiments on 14 RAG components revealed significant vulnerabilities, with even apparent attacks easily bypassing existing retrievers, filters, or advanced LLMs. Similarly, \cite{fang2024gh6} established RAG-Bench to specifically assess the noise robustness of RAG models.

Despite these promising developments, the arms race between attackers and defenders in the realm of RAG systems continues. The findings from \cite{zou2024haa} and \cite{zhang2024rwm} indicate that existing defenses are often insufficient against advanced knowledge corruption and hijacking attacks. Future research must prioritize holistic defense frameworks that combine advanced input validation, multi-layered source verification, and inherent LLM robustness, alongside enhanced transparency and provenance tracking for retrieved information. Developing real-time, scalable source verification for dynamic web content and robust defenses against zero-day adversarial attacks that exploit novel vulnerabilities are critical to ensure the reliability, factual accuracy, and overall trustworthiness of RAG outputs in an increasingly complex and adversarial information landscape.