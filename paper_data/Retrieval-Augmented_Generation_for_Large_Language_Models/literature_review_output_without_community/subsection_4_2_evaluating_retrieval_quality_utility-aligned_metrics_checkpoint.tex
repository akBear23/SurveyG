\subsection*{Evaluating Retrieval Quality: Utility-Aligned Metrics}

The effective evaluation of retrieval quality within Retrieval-Augmented Generation (RAG) systems is critical for their optimization, yet it presents unique challenges that necessitate a departure from traditional relevance assessment. This subsection explores advanced methodologies that directly measure a document's actual utility to the consuming RAG Large Language Model (LLM), moving beyond external judgments to foster more accurate and transparent feedback.

Historically, evaluating the retrieval component of RAG systems has largely relied on metrics that often prove insufficient or misaligned with the LLM's true needs. Traditional information retrieval (IR) metrics, such as Precision@k, Recall, Mean Average Precision (MAP), or Normalized Discounted Cumulative Gain (NDCG), typically assess document relevance based on human annotations or keyword matching \cite{huang2024a59}. While valuable for standalone IR systems, these metrics often fail to capture whether a retrieved document genuinely contributes to the LLM's ability to generate a correct, comprehensive, and non-hallucinatory answer \cite{salemi2024om5, friel20241ct}. What is deemed "relevant" by human annotators or simple keyword overlap may not translate into actual utility for the LLM, leading to suboptimal retriever development. Furthermore, traditional end-to-end RAG evaluations, while comprehensive in assessing the final output, are computationally expensive and offer only list-level feedback, lacking the granularity to pinpoint which specific documents contribute to the final output \cite{salemi2024om5, rau20244nr}. This opacity makes it difficult to effectively optimize retriever models. The use of external LLMs as binary judges for relevance also introduces additional computational costs, memory constraints, and the potential for a mismatch between the judging LLM and the RAG LLM itself, alongside sensitivity to evaluation prompts \cite{salemi2024om5, liu2025sy0}.

Addressing these critical limitations, a paradigm shift towards "utility-aligned metrics" has emerged, redefining relevance as a document's direct contribution to the RAG LLM's downstream task performance. These methodologies aim to intrinsically link retrieval quality to the generator's actual usage and the final output's accuracy.

One prominent approach is eRAG, introduced by \cite{salemi2024om5}. This methodology directly measures a document's value by leveraging the *RAG system's own LLM* as the arbiter for generating document-level relevance labels. Instead of processing all retrieved documents simultaneously, eRAG feeds each document in the retrieval list individually to the LLM, alongside the query. The LLM's output for each single document is then evaluated against the downstream task's ground truth labels (e.g., Exact Match for question answering or F1 for generation). This performance score for each individual document serves as its utility-aligned relevance label, which can then be aggregated using standard set-based or ranking metrics like MAP or NDCG to assess the overall retrieval list \cite{salemi2024om5}. eRAG offers several technical advantages: it provides a novel paradigm for evaluating retrieval quality by intrinsically aligning relevance labels with how the LLM actually utilizes the retrieved information, offering more accurate feedback for retriever optimization. Moreover, it boasts substantial computational advantages, consuming up to 50 times less GPU memory and improving runtime compared to traditional end-to-end RAG evaluation methods, making it a practical and scalable tool \cite{salemi2024om5}. Experimental validation across various knowledge-intensive language tasks from the KILT benchmark demonstrated that eRAG consistently achieves the highest correlation with downstream RAG performance, with absolute improvements in Kendall's $\tau$ ranging from 0.168 to 0.494 over baselines \cite{salemi2024om5}.

Beyond eRAG, other utility-aligned metrics further emphasize the direct measurement of context utilization. The TRACe evaluation framework, proposed by \cite{friel20241ct}, introduces several metrics, notably "Context Utilization." This metric specifically measures the fraction of retrieved context *actually used* by the generator in formulating its response. Unlike eRAG, which assesses a document's potential utility when presented alone, Context Utilization directly observes the LLM's behavior when processing the full context, often relying on span-level annotations to identify which parts of the retrieved text are incorporated into the output. Similarly, for long-form generation tasks, \cite{qi2024tlf} introduces the Key Point Recall (KPR) metric within the Long$^2$RAG benchmark. KPR evaluates the extent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses, providing a nuanced assessment of their ability to exploit retrieved information for comprehensive answers. This also aligns with the "utility-oriented thought" proposed by MetRag, which moves beyond mere similarity to consider a document's direct usefulness for the generation task \cite{gan2024id0}.

A critical aspect of these LLM-centric evaluation methods is the reliability of the LLM acting as a judge. \cite{liu2025sy0} highlights that while LLM-based judgment models have the potential for high-quality evaluations, they are highly sensitive to evaluation prompts, leading to inconsistencies. To address this, they introduce the Judge-Consistency (ConsJudge) method, which enhances LLMs to generate more accurate evaluations for RAG models by prompting LLMs to generate different judgments based on various combinations of judgment dimensions, utilizing judge-consistency to evaluate these judgments, and selecting accepted/rejected judgments for DPO training. This approach is crucial for improving the robustness and trustworthiness of any utility-aligned metric that relies on LLMs for annotation or judgment, such as eRAG's document-level scoring or TRACe's ground truth generation via GPT-4 \cite{salemi2024om5, friel20241ct}.

Collectively, these utility-aligned metrics represent a significant advancement in RAG evaluation. They move beyond superficial relevance to provide a deeper, more actionable understanding of how retrieved information truly impacts the LLM's performance. While eRAG, Context Utilization, and KPR each offer distinct mechanisms for assessing utility—from individual document contribution to actual usage in generation—they all share the common goal of bridging the gap between retrieval and generation. However, limitations persist; eRAG still relies on the availability of ground truth labels for the downstream task, which may not be universally available \cite{salemi2024om5}. Additionally, by evaluating documents individually, eRAG might not fully capture complex synergistic interactions that could occur when an LLM processes multiple documents simultaneously \cite{salemi2024om5}. Future research directions could explore methods to incorporate potential synergistic effects of multiple documents while maintaining computational efficiency, and to extend these utility-aligned evaluation frameworks to scenarios where explicit ground truth labels are scarce or where LLM judges can be further refined to ensure consistent and unbiased assessments. The development of standardized benchmarks like RAGBench \cite{friel20241ct} and BERGEN \cite{rau20244nr} will continue to be vital in rigorously testing and comparing these evolving utility-aligned evaluation methodologies.