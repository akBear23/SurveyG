\subsection{Background: Large Language Models (LLMs) and their Limitations}

Large Language Models (LLMs) have ushered in a new era of natural language processing, demonstrating remarkable capabilities in generating coherent, contextually relevant, and often creative text across a myriad of tasks, from content creation and summarization to translation and question answering. Their widespread adoption has revolutionized human-computer interaction and significantly advanced the state-of-the-art in various domains. However, despite these impressive generative abilities, a critical examination reveals inherent shortcomings that fundamentally limit their reliability and utility in knowledge-intensive applications, serving as the primary motivation for the development of external knowledge integration mechanisms like Retrieval-Augmented Generation (RAG).

One of the most significant limitations of LLMs is their propensity for generating factually incorrect or nonsensical information, commonly referred to as "hallucinations" \cite{gao20238ea, he20248lp, yan202437z}. LLMs, by design, are trained to predict the next token based on patterns learned from vast datasets, not to guarantee factual accuracy or truthfulness. This inherent characteristic means they can confidently produce plausible-sounding but entirely false statements. For instance, \cite{chen2023nzb} empirically demonstrated that LLMs frequently fail to reject answering when no relevant information is available in external documents, instead generating incorrect answers. Furthermore, even when provided with warnings about potentially incorrect retrieved information, LLMs tend to prioritize and trust the (incorrect) retrieved data over their own internal knowledge, exacerbating the hallucination problem \cite{chen2023nzb}. This issue is not confined to general text; it extends to specialized domains, with \cite{he20248lp} noting that LLMs are prone to hallucinating content even in structured graph settings.

Another critical constraint stems from LLMs' reliance on static and potentially outdated training data. The parametric knowledge embedded within an LLM is frozen at the time of its last training cut-off, rendering it incapable of accessing real-time events, dynamic information, or newly emerging facts \cite{gao20238ea, yan202437z}. This limitation means that LLMs cannot provide up-to-date information on current affairs, recent scientific discoveries, or evolving domain-specific knowledge, significantly restricting their applicability in dynamic environments. The construction of benchmarks, such as the Retrieval-Augmented Generation Benchmark (RGB) by \cite{chen2023nzb}, specifically uses the latest news information to mitigate bias from LLMs' pre-existing internal knowledge, implicitly acknowledging the static nature of their training data and the need for external, current information. The inability to dynamically update their knowledge base without expensive and time-consuming retraining cycles makes LLMs inherently unsuitable for tasks requiring access to the most current information.

Finally, LLMs face significant constraints in processing very long or complex contexts due to their limited context window sizes. While modern LLMs have expanded their context windows considerably, they still struggle to process and reason over entire lengthy documents, extensive codebases, or multi-hour multimodal inputs like videos \cite{amugongo202530u}. This limitation means that even if relevant information exists within a vast document, the LLM may not be able to ingest it all directly into its input, leading to information loss or an inability to perform comprehensive in-context learning. \cite{he20248lp} highlights this challenge in the context of textual graphs, where converting large graphs into text sequences for LLMs often leads to excessive token counts that exceed context windows and cause information loss. Similarly, \cite{chen2023nzb} observed that LLMs often lack the ability to summarize and integrate information from multiple documents, a task that becomes increasingly difficult with longer or more numerous contexts.

These inherent challenges—hallucinations, reliance on static data, and context window limitations—underscore the fundamental necessity for external knowledge integration mechanisms. Without such mechanisms, LLMs remain prone to factual errors, quickly become outdated, and cannot effectively process the vast, complex information landscapes of real-world applications. This critical gap serves as the primary impetus for the development of Retrieval-Augmented Generation (RAG), which seeks to dynamically inject relevant, up-to-date, and accurate external knowledge into the LLM's generation process, thereby significantly enhancing its reliability, factual accuracy, and overall utility.