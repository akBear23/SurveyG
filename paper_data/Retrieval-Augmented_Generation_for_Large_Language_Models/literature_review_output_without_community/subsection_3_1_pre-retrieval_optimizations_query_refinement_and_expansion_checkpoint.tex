\subsection*{Pre-Retrieval Optimizations: Query Refinement and Expansion}
Initial Retrieval-Augmented Generation (RAG) systems often suffer from suboptimal retrieval due to ambiguous, overly broad, or complex user queries. These raw queries can lead to the fetching of irrelevant or insufficient context, thereby degrading the quality and factual accuracy of the Large Language Model's (LLM) output. Pre-retrieval optimizations are a critical class of techniques designed to enhance the initial user query before it interacts with the retriever, ensuring more precise and effective document fetching \cite{gao20238ea, huang2024a59, zhao2024931}. This section explores various strategies, including query expansion, rewriting, and decomposition, which proactively address the limitations of naive query formulation.

The landscape of RAG optimizations, as systematically categorized by surveys like \cite{gao20238ea} and \cite{huang2024a59}, highlights query manipulation as a foundational pre-retrieval step. These methods aim to transform the original query into a more retriever-friendly format.

\subsubsection*{Query Expansion}
Query expansion involves augmenting the original query with additional terms to broaden the search scope and capture more relevant documents, especially when the initial query is too concise or uses non-standard terminology. Traditional methods might include adding synonyms, related concepts, or thematic terms. More advanced approaches leverage LLMs to generate multiple reformulations or perspectives of the original query. For instance, for broad, open-ended queries that encompass diverse sub-intents, systems like RichRAG \cite{wang20245w8} employ a "sub-aspect explorer" to identify potential sub-aspects. This effectively expands the query by generating related facets, which are then used by a "multi-faceted retriever" to build a diverse candidate pool of documents. This intelligent expansion ensures that the final generated response is rich and comprehensive, covering various relevant aspects of the user's implicit information need.

\subsubsection*{Query Rewriting and Transformation}
Query rewriting focuses on rephrasing the original query to improve its compatibility with the underlying search index and retrieval model. A prominent technique in this category is Hypothetical Document Embeddings (HyDE), which involves generating a hypothetical, yet plausible, answer to the original query using an LLM \cite{gao20238ea}. This hypothetical document is then embedded, and its embedding is used to retrieve actual documents from the knowledge base. The rationale is that the embedding of a well-formed hypothetical answer is often semantically closer to relevant documents than the embedding of a short, potentially ambiguous query. While effective for dense retrieval, a limitation of HyDE is its reliance on the LLM's ability to generate a non-hallucinatory hypothetical answer, which can be challenging for complex or out-of-domain queries.

Moving beyond generating hypothetical documents, recent advancements have focused on explicit, learned query refinement. RQ-RAG \cite{chan2024u69} represents a significant innovation by training an LLM to dynamically refine search queries through rewriting, decomposition, and disambiguation *during* the generation process. This end-to-end learned approach directly addresses ambiguous or complex initial queries by enabling the LLM to proactively generate more effective search terms. RQ-RAG's novelty lies in its dataset construction pipeline, which leverages a powerful external LLM (e.g., ChatGPT) to craft tailored search queries for specific refinement scenarios and to regenerate contextually aligned answers. Furthermore, it incorporates internal trajectory selection strategies (e.g., Perplexity, Confidence, Ensemble-based) to navigate multi-path query refinement at inference time without relying on external LLMs for decision-making. Experimental validation demonstrated RQ-RAG's superior performance on both single-hop and multi-hop QA tasks, significantly outperforming prior RAG methods and even larger proprietary LLMs, showcasing the power of explicit, learned query refinement.

Complementing these, dynamic retrieval systems like DRAGIN \cite{su20241om} further refine queries by actively deciding *when* and *what* to retrieve based on the LLM's real-time information needs during text generation. This approach moves beyond static rules, crafting appropriate queries that span the entire context rather than just the most recent sentence, offering a more adaptive form of query rewriting.

\subsubsection*{Query Decomposition}
For complex questions, especially those requiring multi-hop reasoning, query decomposition is a crucial strategy. This involves breaking down a single, intricate query into a series of simpler, more manageable sub-questions, each of which can be answered by a separate retrieval step. This approach mitigates the "lost-in-the-middle" problem, where LLMs struggle to synthesize information from long, complex retrieved contexts. For instance, IterDRAG \cite{yue2024ump} is designed for complex multi-hop queries, decomposing the input into simpler sub-queries and iteratively performing retrieval and generating intermediate answers to construct reasoning chains. Similarly, in practical applications, systems described by \cite{hikov2024rme} utilize a "sub-questions engine" for in-depth analysis of documents, effectively demonstrating query decomposition for comprehensive information extraction. RichRAG \cite{wang20245w8}, by identifying "sub-aspects" for broad queries, also implicitly performs a form of decomposition, allowing for targeted retrieval for each identified facet.

\subsubsection*{Critical Analysis and Comparison}
The evolution of pre-retrieval optimizations reflects a shift from simpler keyword-based or rule-based expansions to sophisticated, LLM-driven, and often learned approaches. While methods like HyDE offer a computationally lighter way to transform queries for dense retrieval, they rely on the quality of a single hypothetical generation. In contrast, systems like RQ-RAG \cite{chan2024u69} and DRAGIN \cite{su20241om} represent a more explicit and dynamic form of query refinement, capable of handling ambiguity, complexity, and multi-hop reasoning through learned rewriting and decomposition. The trade-off often lies in computational cost and data requirements; training an end-to-end model like RQ-RAG requires a carefully constructed dataset, whereas HyDE can be applied with a pre-trained LLM. However, the benefits of learned refinement, particularly for multi-hop tasks where RQ-RAG demonstrated significant gains, often outweigh these costs by delivering substantially more accurate and comprehensive responses. The integration of query decomposition, as seen in IterDRAG \cite{yue2024ump} and RichRAG \cite{wang20245w8}, is particularly vital for overcoming the limitations of single-shot retrieval on complex information needs.

The progression from diagnosing RAG limitations to systematically categorizing and then implementing explicit, learned query refinement techniques marks a significant advancement in RAG systems. Future research could focus on making these sophisticated refinement processes more computationally efficient, robust to noisy inputs, and universally applicable across diverse domains and a wider array of RAG-enabled NLP tasks beyond question answering.