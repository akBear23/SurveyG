\subsection{RAG with Knowledge Graphs for Enhanced Reasoning}

Conventional Retrieval-Augmented Generation (RAG) systems, primarily relying on semantic similarity over flat text chunks, often encounter significant limitations when tasked with navigating complex, interconnected, and structured data. This inherent deficiency stems from their inability to fully capture and leverage the explicit structural and relational knowledge embedded within rich knowledge bases, leading to suboptimal retrieval accuracy and constrained reasoning capabilities, particularly for multi-hop questions or domain-specific inquiries. To address these challenges, advanced RAG architectures are increasingly integrating Knowledge Graphs (KGs), enabling Large Language Models (LLMs) to move beyond purely semantic search and exploit the inherent structure of information for more precise, context-aware, and scalable knowledge extraction. This paradigm shift enhances the LLM's capacity to "chat with its graph," facilitating deeper understanding and more accurate generation.

A compelling demonstration of KG-enhanced RAG is presented by \cite{xu202412d} in the context of customer service question answering. The authors critically observe that traditional RAG methods, by treating historical customer service tickets as unstructured plain text, inadvertently discard crucial intra-issue structures (e.g., problem-solution relationships within a single ticket) and inter-issue relations (e.g., connections between related tickets). This oversight leads to fragmented context, compromised retrieval accuracy, and reduced answer quality, as fixed-length text chunking often severs vital connections. To overcome this, \cite{xu202412d} proposes a novel methodology centered on constructing a dual-level KG. At the intra-issue level, individual tickets are meticulously parsed into structured trees, capturing the internal flow and components of each issue. These trees are then interconnected at the inter-issue level through both explicitly defined and implicitly inferred relations, forming a comprehensive, domain-specific graph. The construction employs a hybrid parsing approach, combining robust rule-based extraction for structured fields with LLM-based parsing for nuanced unstructured text, ensuring high fidelity in graph representation. Crucially, retrieval is transformed through an LLM-driven subgraph mechanism. This system parses natural language queries to identify named entities and intents, dynamically translating them into precise graph database queries (e.g., Cypher). This allows for the extraction of contextually and structurally pertinent subgraphs, rather than disconnected text snippets. The empirical results are notable, demonstrating a 77.6\% improvement in Mean Reciprocal Rank (MRR) and a 0.32 improvement in BLEU score over conventional RAG baselines. The practical impact was further validated by its deployment at LinkedIn, which reported a 28.6\% reduction in median per-issue resolution time, underscoring the tangible benefits of leveraging structured knowledge in real-world, domain-specific RAG applications.

Building upon the fundamental premise of leveraging structural information, \cite{he20248lp} extends the RAG paradigm to tackle the more generalized and complex challenge of "chatting with graphs" for arbitrary textual graphs, moving beyond the specific document structures explored by \cite{xu202412d}. \cite{he20248lp} identifies that applying RAG to general textual graphs often results in significant challenges, including increased hallucination, scalability issues when attempting to flatten large graphs into linear text, and a persistent failure to fully exploit crucial structural information. To address these systemic limitations, they introduce G-Retriever, posited as the first RAG approach specifically designed for general textual graphs. A core innovation of G-Retriever lies in its ingenious formulation of subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) optimization problem. This formulation allows the system to retrieve graph portions that are not only contextually relevant to the query but also structurally coherent and interconnected, by explicitly considering neighborhood information and optimizing for both "prizes" (relevance of nodes/edges) and "costs" (path length/complexity). This sophisticated approach significantly enhances explainability and demonstrably mitigates hallucination compared to prompt-tuning baselines, which struggle to accurately recall entire graph structures from isolated embeddings. By seamlessly integrating Graph Neural Networks (GNNs) with LLMs and RAG, G-Retriever achieves superior performance and scalability across diverse textual graph tasks, as rigorously evaluated on their newly introduced GraphQA benchmark.

Collectively, both \cite{xu202412d} and \cite{he20248lp} signify a critical evolution in RAG, marking a decisive shift from purely semantic, document-centric retrieval to intelligent, structure-aware information extraction from KGs. While \cite{xu202412d} masterfully showcases the power of dual-level KGs and LLM-driven subgraph retrieval for enhancing RAG in specific document-rich domains like customer service, its methodology for KG construction and query translation is tailored to the structured nature of its input. In contrast, \cite{he20248lp}'s G-Retriever generalizes this concept to arbitrary textual graphs, offering a more broadly applicable framework. Its formalization of subgraph retrieval as a PCST optimization problem provides a robust, principled mechanism for balancing relevance and connectivity, which is crucial for maintaining graph integrity and reducing hallucination in complex, unstructured graph environments. Both approaches explicitly address the limitations of conventional RAG by enabling LLMs to effectively "chat with their graph," leading to more precise, context-aware, and scalable information extraction. Future development in this area will likely focus on automating robust KG construction from diverse unstructured data sources, developing more dynamic and efficient KG update mechanisms, and exploring advanced graph reasoning techniques to further enhance LLM capabilities in increasingly complex, relational knowledge environments.