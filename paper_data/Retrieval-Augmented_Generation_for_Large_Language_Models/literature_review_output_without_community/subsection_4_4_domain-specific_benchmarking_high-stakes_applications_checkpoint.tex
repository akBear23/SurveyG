\subsection*{Domain-Specific Benchmarking: High-Stakes Applications}

The deployment of Large Language Models (LLMs) in high-stakes domains, such as medicine, law, and finance, mandates an unwavering commitment to trustworthiness, factual accuracy, and reliability. In these critical applications, the consequences of hallucinations, outdated knowledge, or imprecise information retrieval can be severe, ranging from misdiagnosis in healthcare to erroneous legal advice or financial decisions \cite{zhou20248fu}. While Retrieval-Augmented Generation (RAG) offers a promising paradigm to ground LLMs with verifiable external information and mitigate these risks, a significant historical gap has existed in the systematic and domain-specific evaluation of RAG systems tailored for these sensitive fields. This deficiency has hindered the establishment of robust best practices and reliable deployment strategies.

Addressing this crucial need, recent advancements have focused on developing specialized benchmarks that reflect the unique complexities and stringent requirements of high-stakes environments. In the medical domain, \cite{xiong2024exb} introduced a pivotal methodological progression with the **MIRAGE benchmark** (Medical Information Retrieval-Augmented Generation Evaluation) and the accompanying **MEDRAG toolkit**. Prior to this work, RAG research in biomedicine often involved LLM improvements for information-seeking, but evaluations frequently lacked the systematic rigor required for clinical settings \cite{liu2025p6t, low2025gjc}. Existing systematic evaluations in biomedicine typically focused on vanilla LLMs, overlooking the unique challenges and opportunities presented by RAG \cite{xiong2024exb}. MIRAGE directly tackles these limitations by providing the first systematic evaluations of RAG systems tailored for medicine, thereby resolving the problem of unreliable LLM outputs and the absence of standardized performance metrics in this domain.

MIRAGE is meticulously designed for medical RAG, incorporating 7,663 questions from five diverse medical QA datasets to ensure broad coverage of clinical scenarios \cite{xiong2024exb}. A key innovation in its methodology is the enforcement of realistic evaluation settings, most notably "Question-Only Retrieval" (QOR), where answer options are withheld during the retrieval phase. This approach accurately simulates real-world medical question-answering, where a clinician formulates a query without pre-existing answer choices, thus ensuring the benchmark's findings are more applicable to practical medical applications than some previous, less stringent evaluation methods \cite{xiong2024exb}. The **MEDRAG toolkit** complements MIRAGE by providing a comprehensive platform integrating novel corpora like StatPearls alongside established sources (PubMed, medical textbooks), a mix of lexical, general semantic, scientific, and biomedical-domain specific retrievers (e.g., BM25, MedCPT, enhanced by Reciprocal Rank Fusion), and a broad selection of LLMs \cite{xiong2024exb}. This robust framework enables the systematic evaluation of 41 distinct RAG configurations, utilizing Chain-of-Thought (CoT) prompting with prepended retrieved snippets, representing a significant leap in methodological rigor. Through extensive experimentation, \cite{xiong2024exb} demonstrated that RAG can improve LLM accuracy by up to 18\% relative to CoT prompting alone, and notably, elevate the performance of smaller models like GPT-3.5 and Mixtral to rival GPT-4 (without RAG) on the MIRAGE benchmark. The study also yielded crucial empirical discoveries, such as the log-linear scaling property between model performance and the number of retrieved snippets, and the observation of the "lost-in-the-middle" phenomenon in medical RAG, providing practical guidelines for optimizing RAG system design. While comprehensive, the study acknowledged limitations, such as constraints on retriever diversity due to computational resources and less pronounced RAG improvements for highly complex examination questions where retrieving truly helpful snippets remains challenging \cite{xiong2024exb}. Other medical applications, such as the radiology-specific RAG developed by \cite{weinert2025cxo}, also demonstrate RAG's potential to improve LLM performance on examinations by providing citable, up-to-date information, further underscoring the value of domain-specific grounding.

Beyond medicine, the legal domain presents another high-stakes application where precise and verifiable information is paramount. \cite{pipitone2024sfx} introduced **LegalBench-RAG**, a benchmark specifically designed to evaluate the *retrieval component* of RAG systems in legal contexts. Unlike general RAG benchmarks (e.g., RGB \cite{chen2023retrieval}) that often focus on overall generation quality or broad document recall, LegalBench-RAG emphasizes the extraction of *minimal, highly relevant text segments* (snippets) from legal documents. This focus is critical because imprecise retrieval in legal settings can lead to exceeding LLM context windows, inducing hallucinations, and preventing the generation of precise citationsâ€”all significant risks in the legal industry \cite{pipitone2024sfx}. LegalBench-RAG innovates by meticulously tracing contexts from existing legal datasets (like LegalBench \cite{guha2023legalbench}) back to their original locations within a large legal corpus to deduce precise character index spans for relevant information. This provides a granular, human-annotated ground truth for retrieval quality, a significant departure from benchmarks that bypass the retrieval step or accept less precise document-level relevance. However, LegalBench-RAG primarily assesses retrieval within a single document, limiting its scope for evaluating multi-document reasoning or multi-hop retrieval across multiple sources \cite{pipitone2024sfx}.

Similarly, the financial domain demands high accuracy and up-to-date information. \cite{zhao2024go5} conducted a systematic investigation into optimizing RAG pipelines in the financial domain. Their work benchmarked six LLMs across 15 retrieval scenarios and nine prompts over two real-world financial datasets. While not introducing a new benchmark framework like MIRAGE or LegalBench-RAG, their study thoroughly discussed the impact of each RAG component (retrieval pipeline, prompts, generation models) on answer generation quality, formulating specific recommendations for RAG system design in finance \cite{zhao2024go5}. This focus on component-level impact analysis complements the broader benchmarking efforts by providing actionable insights for fine-tuning RAG systems for domain-specific performance.

Collectively, these domain-specific benchmarking efforts highlight a critical intellectual trajectory in RAG research: moving beyond general evaluations to address the unique data characteristics, user needs, and trustworthiness requirements of high-stakes applications. They underscore the necessity for realistic evaluation settings (e.g., QOR in MIRAGE), granular ground truth (e.g., precise snippet retrieval in LegalBench-RAG), and systematic component analysis (e.g., in financial RAG optimization). These advancements directly contribute to enhancing the factuality, robustness, and accountability of RAG systems, which are key dimensions of trustworthiness identified by surveys on RAG trustworthiness \cite{zhou20248fu}. While significant progress has been made, challenges persist, including the computational resources required for exhaustive evaluation, the complexity of multi-hop reasoning in highly nuanced domains, and the continuous need for diverse retrieval strategies that can adapt to evolving knowledge bases. Nevertheless, these domain-specific benchmarks establish a foundational framework for rigorously validating and optimizing RAG in critical applications, guiding future research towards more reliable and ethically sound AI deployments.