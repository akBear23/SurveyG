\subsection{Basic RAG Architecture: Retrieve-then-Generate}

The fundamental 'Retrieve-then-Generate' paradigm, often referred to as 'Naive RAG' or 'Retrieve-Read', constitutes the foundational architecture of Retrieval-Augmented Generation (RAG). This innovative approach marked a pivotal advancement beyond standalone Large Language Models (LLMs) by integrating external, verifiable information directly into the generation process \cite{lewis2020pwr, gao20238ea}. Its primary objective was to address inherent limitations of LLMs, such as their propensity for hallucination, reliance on static training data, and lack of transparency regarding factual sources.

Conceived as a sequential, two-stage process, the basic RAG architecture operates as follows:
\begin{enumerate}
    \item \textbf{Retrieval Stage}: Upon receiving a user's input query, a dedicated retriever component initiates a search within an external knowledge base. This knowledge base typically comprises a vast collection of documents, passages, or data indexed for efficient search, often using dense vector embeddings for semantic similarity matching \cite{lewis2020pwr}. The retriever's task is to identify and fetch a set of documents or passages deemed most relevant to the query. This step is crucial for accessing up-to-date, domain-specific, or proprietary information that would otherwise be inaccessible to the LLM.
    \item \textbf{Generation Stage}: The retrieved contexts, along with the original user query, are then fed as input to a Large Language Model (LLM). The LLM's role is to synthesize a coherent and informative answer by processing and reasoning over this augmented input. By grounding its response in the provided external evidence, the LLM aims to mitigate factual inaccuracies and hallucinations, thereby enhancing the trustworthiness and factual basis of the generated output \cite{gao20238ea}.
\end{enumerate}

The seminal work by Lewis et al. (2020) formally introduced the Retrieval-Augmented Generation framework, demonstrating its efficacy in knowledge-intensive NLP tasks. They proposed RAG models where a pre-trained sequence-to-sequence model (the generator) was augmented by a non-parametric memory, specifically a dense vector index of Wikipedia, accessed by a pre-trained neural retriever. This architecture allowed the LLM to dynamically consult an external knowledge source during generation, providing a mechanism for accessing and precisely manipulating knowledge beyond its parametric memory \cite{lewis2020pwr}. This was a significant conceptual leap, moving from models that solely relied on memorized knowledge to those that could actively seek and integrate new information.

The initial appeal of this 'Retrieve-then-Generate' paradigm stemmed from its intuitive solution to several critical LLM challenges. Firstly, it offered a direct mechanism to combat hallucinations by providing factual anchors from external sources. Secondly, it allowed LLMs to stay current with rapidly evolving information, as the external knowledge base could be updated independently of the LLM's training cycle. Thirdly, by presenting the retrieved documents alongside the generated answer, RAG inherently offered a degree of transparency and traceability, allowing users to verify the sources of information. This sequential integration of a retriever and a generator thus marked a significant methodological step, transforming LLMs from isolated knowledge systems into dynamic, externally-grounded reasoning agents \cite{gao20238ea}.

While conceptually powerful and a clear improvement over standalone LLMs, this basic 'Retrieve-then-Generate' architecture, in its naive implementation, quickly revealed a new set of challenges and limitations. These early hurdles, concerning the LLM's ability to effectively utilize, filter, and reason over the retrieved context, necessitated further research and the development of more sophisticated RAG paradigms, which will be explored in subsequent sections.