\subsection{Evaluating Retrieval Quality and Multi-Hop Reasoning}
The efficacy of Retrieval-Augmented Generation (RAG) systems hinges critically on the quality of retrieved information and the Large Language Model's (LLM) ability to synthesize it, especially for complex, multi-hop queries. This necessitates advanced evaluation methodologies that move beyond simple fact-checking to assess intrinsic retrieval utility and sophisticated reasoning capabilities. Early RAG benchmarks, such as the Retrieval-Augmented Generation Benchmark (RGB) by \textcite{chen2023nzb} and the medical RAG benchmark MIRAGE by \textcite{xiong2024exb}, laid foundational work by diagnosing LLMs' performance across general abilities like noise robustness and information integration. While valuable, these often focused on scenarios where answers could be derived from single pieces of evidence, highlighting a need for more complex assessments.

A significant gap emerged in evaluating RAG systems on tasks requiring complex information synthesis across multiple sources, leading to the development of benchmarks specifically targeting multi-hop queries. \textcite{tang2024i5r} directly addressed this with \textit{MultiHop-RAG}, the first dedicated benchmark for multi-hop queries. This dataset, generated via a sophisticated GPT-4-driven pipeline, categorizes queries into Inference, Comparison, Temporal, and Null types, revealing that current state-of-the-art RAG systems perform unsatisfactorily on these complex reasoning tasks. Complementing this, \textcite{krishna2024qsh} introduced FRAMES, a unified evaluation framework that rigorously tests LLMs on fact retrieval, reasoning across multiple constraints, and accurate information synthesis in an end-to-end RAG scenario, particularly for multi-document and multi-hop contexts. Further extending the scope to longer interactions, \textcite{qi2024tlf} introduced LONG$^2$RAG, a benchmark designed to evaluate long-context and long-form RAG. It features questions spanning diverse domains with lengthy retrieved documents and proposes the Key Point Recall (KPR) metric, which offers a nuanced assessment of how effectively LLMs incorporate critical information from extensive contexts into their generated long-form responses. These efforts collectively underscore the limitations of existing RAG systems in handling nuanced, multi-source information needs and generating comprehensive outputs.

Beyond assessing multi-hop reasoning, a crucial methodological innovation has been the direct evaluation of the *retrieval component's utility to the LLM*. Prior evaluation methods, relying on expensive end-to-end RAG evaluations or human-annotated relevance labels, often showed only a minor correlation with the actual downstream performance of the RAG LLM. This mismatch arises because a document's "relevance" to a human might not equate to its "utility" for an LLM in generating a correct answer. To address this, \textcite{salemi2024om5} proposed \textit{eRAG}, a novel approach that uses the RAG system's *own LLM* to determine a document's value. By feeding each retrieved document individually to the LLM and evaluating its output against ground truth, eRAG provides downstream-aligned relevance labels with significant computational efficiency, consuming up to 50 times less GPU memory than traditional methods. This direct measurement of utility offers more accurate and efficient feedback for optimizing retrieval models. Building on the idea of LLM-as-a-judge, \textcite{liu2025sy0} introduced Judge-Consistency (ConsJudge) to improve the reliability of LLM-based evaluations for RAG, addressing the sensitivity of LLM judges to prompts by leveraging consistency across different judgment dimensions for DPO training, thereby enhancing the accuracy of feedback for RAG optimization.

The field has also seen significant advancements in developing granular, explainable, and domain-specific evaluation frameworks. Recognizing the critical need for precision in high-stakes environments, \textcite{pipitone2024sfx}'s LegalBench-RAG focuses on the retrieval of minimal, highly relevant text snippets in the legal domain, directly addressing the challenge of preventing LLM hallucination and context window overload in specialized fields. Similarly, \textcite{wang2024ac6} introduced DomainRAG, a Chinese benchmark tailored for domain-specific RAG in areas like college enrollment, which evaluates abilities such as conversational RAG, structural information analysis, denoising, and multi-document interactions, highlighting the unique challenges of expert knowledge domains. For broader applicability and interpretability, \textcite{friel20241ct} introduced RAGBench and the TRACe evaluation framework, which provides explainable metrics like Context Relevance, Context Utilization, Completeness, and Adherence. These metrics offer actionable insights into RAG system performance by not only assessing the final output but also diagnosing how effectively the LLM leverages the retrieved context. Further pushing the boundaries of interpretability, \textcite{guinet2024vkg} pioneered an automated evaluation methodology using task-specific exam generation and Item Response Theory (IRT), which can decompose a RAG's overall ability into contributions from its LLM, retrieval method, and in-context learning components, providing unprecedented transparency into system behavior. The CRUD-RAG benchmark by \textcite{lyu2024ngu} extends evaluation to a broader range of RAG applications beyond traditional question answering, including text continuation, multi-document summarization, and hallucination modification, particularly for Chinese LLMs. To foster reproducible research and standardized comparisons, \textcite{rau20244nr} developed BERGEN, an end-to-end benchmarking library for RAG.

As RAG systems become more sophisticated and are deployed in critical applications, evaluating their trustworthiness and safety has emerged as a paramount concern. \textcite{zhou20248fu} proposed a unified framework for RAG trustworthiness, encompassing six key dimensions: Factuality, Robustness, Fairness, Transparency, Accountability, and Privacy. This framework highlights that RAG, while mitigating some LLM issues, can introduce new trustworthiness challenges if retrieved information is inappropriate or poorly utilized. Empirically supporting this, \textcite{zhang2025byv} conducted a safety analysis revealing that RAG can, counter-intuitively, make LLMs *less safe* and alter their safety profiles, even when combining safe models with safe documents. This finding underscores the critical need for RAG-specific safety research and red-teaming methods. Moving towards provable guarantees, \textcite{kang2024hrb} introduced C-RAG, the first framework to certify generation risks for RAG models, providing conformal risk analysis and theoretical guarantees that RAG can achieve lower certified generation risk under certain conditions. These advancements signify a crucial shift towards comprehensive evaluation that extends beyond performance metrics to encompass the ethical and safety implications of RAG deployment.

In conclusion, the field has made substantial progress in developing advanced evaluation methodologies for RAG, shifting from general assessments to highly nuanced, utility-driven, multi-hop, and explainable metrics. The introduction of benchmarks like MultiHop-RAG \textcite{tang2024i5r} and innovative evaluation techniques like eRAG \textcite{salemi2024om5} are critical for understanding the intrinsic utility of retrieved documents and diagnosing the complex reasoning capabilities of RAG systems. However, as RAG architectures continue to evolve in complexity and are deployed in increasingly sensitive domains, the ongoing challenge remains in developing evaluation frameworks that are not only robust and scalable but also provide fine-grained, interpretable feedback to guide the development of truly intelligent and reliable RAG systems. Future research must critically address how to evaluate RAG systems in dynamic, interactive, and conversational settings, balance cost-effective automated metrics with nuanced human assessment, and comprehensively assess trustworthiness, safety, and fairness, integrating the insights from emerging work on RAG-specific safety and ethical considerations.