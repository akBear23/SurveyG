\subsection*{Multimodal RAG: Integrating Diverse Knowledge Sources}

The landscape of Retrieval-Augmented Generation (RAG) is rapidly evolving beyond its foundational text-centric paradigm, moving towards the integration of diverse knowledge modalities to foster more comprehensive and contextually rich responses from Large Language Models (LLMs). This expansion is crucial for enabling LLMs to interact with and understand the real world, which inherently comprises visual, auditory, and other forms of information alongside text. The goal is to create more versatile LLMs capable of understanding and generating responses based on a richer, real-world context, thereby mitigating the limitations of purely textual knowledge bases.

A pivotal step in this direction was the introduction of MuRAG (Multimodal Retrieval-Augmented Generator) by \cite{chen2022j8c}, which pioneered multimodal retrieval-augmented generation for open question answering over images and text. Prior RAG systems were predominantly limited to retrieving textual knowledge, posing a significant challenge for queries requiring visual grounding or multimodal reasoning \cite{chen2022j8c}. MuRAG addresses this by proposing a novel architecture that leverages a unified multimodal encoder, combining pre-trained T5 and ViT models, to process queries and memory candidates across both image and text modalities. Its methodology involves a retriever stage utilizing Maximum Inner Product Search (MIPS) to fetch relevant Top-K multimodal items, which are then fed to a reader stage for text generation \cite{chen2022j8c}. A key innovation lies in its joint pre-training objective, which integrates a contrastive loss for effective retrieval with a generative loss for leveraging multimodal knowledge, alongside an efficient two-stage fine-tuning pipeline designed to manage the computational complexities of large external multimodal memories \cite{chen2022j8c}. While MuRAG demonstrated the substantial benefits of incorporating visual knowledge into the generation process, its monolithic design and joint optimization posed challenges in terms of scalability and adaptability to dynamic, noisy multimodal inputs.

Building upon this foundation, subsequent research has focused on refining the retrieval and integration processes to enhance robustness and accuracy, particularly in the face of real-world complexities. For instance, the challenge of multi-granularity noisy correspondence (MNC) and the static nature of Multimodal Large Language Model (MLLM) training data can hinder accurate retrieval and generation in dynamic contexts. To address these limitations, \cite{chen20245d2} introduced RagVL, a novel framework featuring knowledge-enhanced reranking and noise-injected training. RagVL instruction-tunes an MLLM to serve as a powerful reranker, precisely filtering the top-k retrieved images to improve the quality of augmented information \cite{chen20245d2}. Furthermore, it enhances the generator's robustness by injecting visual noise during training at both data and token levels, thereby making the system more resilient to variations and imperfections in multimodal inputs \cite{chen20245d2}. This approach directly improves upon the concept of multimodal retrieval by ensuring that the retrieved information is not only relevant but also of high quality and effectively utilized by the generator, offering a more modular and robust alternative to MuRAG's end-to-end joint training.

Beyond specific architectural designs, the broader integration of multimodal capabilities into RAG systems is gaining traction. Some comprehensive RAG optimization frameworks, while primarily focused on text, also explore the incorporation of multimodal retrieval. For example, \cite{wang20248gm} investigates best practices across the entire RAG workflow and highlights the significant enhancement of question-answering capabilities on visual inputs, and the acceleration of multimodal content generation through multimodal retrieval techniques, including a "retrieval as generation" strategy. This suggests that the principles of efficient RAG design, such as optimal chunking, embedding, and reranking, are being extended to encompass multimodal data, indicating a convergence of general RAG advancements with multimodal requirements.

Despite these advancements, a critical challenge in multimodal RAG lies in the effective evaluation and utilization of non-textual evidence. Benchmarking efforts have revealed that even state-of-the-art MLLMs struggle to efficiently extract and utilize visual knowledge. \cite{wu2025eum} introduced Visual-RAG, a question-answering benchmark specifically designed for visually grounded, knowledge-intensive queries that require text-to-image retrieval and the integration of retrieved clue images to extract visual evidence. Their findings underscore the persistent need for improved visual retrieval, grounding, and attribution mechanisms within multimodal RAG systems, highlighting a gap in current models' ability to fully leverage visual context. This points to a deeper issue beyond mere retrieval accuracy: the capacity of the MLLM to *reason* effectively with the retrieved visual information.

The practical impact of multimodal RAG is particularly evident in high-stakes domains where factual accuracy and hallucination reduction are paramount. In healthcare, for instance, Multimodal Large Language Models (MLLMs) face significant challenges with hallucination, especially when generating medical reports from images. To address this, \cite{chu2025wz5} demonstrated how Visual RAG (V-RAG), incorporating both text and visual data from retrieved images, can significantly improve the accuracy of entity probing in medical image caption generation and chest X-ray report generation. By grounding medical entities in visual evidence, V-RAG enhances clinical accuracy and reduces hallucinations, showcasing the transformative potential of multimodal RAG in critical applications. This work highlights that multimodal RAG is not just about expanding input modalities, but about enhancing trustworthiness and reliability in sensitive contexts.

The progression from pioneering multimodal retrieval to refining its components and addressing its evaluation challenges highlights a critical trajectory in RAG research. While significant strides have been made in enabling LLMs to integrate diverse knowledge sources, challenges persist in scaling these systems to even larger and more heterogeneous multimodal knowledge bases. Future directions include developing more sophisticated cross-modal reasoning capabilities that go beyond simple concatenation of modalities, improving the efficiency of multimodal indexing and retrieval for real-time applications involving massive datasets (e.g., millions of video or audio segments), and exploring novel ways to synthesize information from an ever-increasing array of modalities beyond just images and text, such as video, audio, and sensor data. Furthermore, the development of robust evaluation metrics for visual grounding and attribution, as highlighted by \cite{wu2025eum}, remains a critical need. The ultimate goal remains the creation of truly versatile LLMs capable of understanding and generating responses based on a richer, real-world context, while ensuring faithfulness and interpretability across all modalities.