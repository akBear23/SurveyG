\subsection*{Core Components of RAG: Retriever and Generator}

Retrieval-Augmented Generation (RAG) systems fundamentally address the limitations of Large Language Models (LLMs) in accessing and leveraging external, up-to-date, and factual knowledge by integrating a dynamic information retrieval mechanism. At the heart of every RAG system are two indispensable components: the retriever and the generator, working in concert to produce informed and coherent responses \cite{fan2024pf1}.

The \textbf{retriever} is responsible for efficiently searching and fetching relevant documents or passages from a vast external knowledge base based on a given user query. This component acts as the system's dynamic memory, providing access to information beyond the LLM's static parametric knowledge \cite{lewis2020pwr, fan2024pf1}. While traditional information retrieval methods, often termed sparse retrievers, such as TF-IDF or BM25, rely on lexical matching and keyword overlap to identify relevant documents \cite{chen20247c1, fan2024pf1}, early RAG systems predominantly adopted dense passage retrievers (DPRs). DPRs map both the query and the documents into a shared high-dimensional embedding space, typically using neural networks, to capture semantic similarity \cite{lewis2020pwr, fan2024pf1}. By computing the similarity between the query embedding and document embeddings, the retriever can quickly identify and rank the most semantically relevant passages, even when there is no exact keyword match. This semantic understanding allows DPRs to overcome the limitations of sparse methods, which often struggle with synonyms, polysemy, or conceptual relevance \cite{fan2024pf1}. For instance, \cite{lewis2020pwr} introduced a neural retriever pre-trained on question-answer pairs, enabling it to access a dense vector index of Wikipedia and retrieve passages that are semantically similar to the input query, thereby dynamically augmenting the LLM's knowledge.

Concurrently, the \textbf{generator} component synthesizes a coherent and accurate response by leveraging both the original user query and the context provided by the retrieved passages \cite{lewis2020pwr}. Its primary role is to ground the LLM's output in factual information, thereby mitigating hallucinations and improving the factual accuracy of its outputs. Early RAG systems commonly employed sequence-to-sequence Large Language Models (LLMs) like BART or T5 as their generators \cite{lewis2020pwr}. These models receive the query and the top-$k$ retrieved documents as augmented input, learning to condition their output on this combined context. This conditioning can be applied uniformly across the entire generated sequence or dynamically for each token, demonstrating flexibility in how the generator integrates retrieved information \cite{lewis2020pwr}. More recently, with the advent of increasingly powerful decoder-only LLMs, these models are frequently adapted to serve as RAG generators, leveraging their advanced generative capabilities to produce nuanced and contextually rich responses based on the retrieved evidence \cite{fan2024pf1}.

This foundational retriever-generator paradigm underscores RAG's ability to combine the strengths of information retrieval with the generative prowess of LLMs. The effectiveness of RAG systems critically hinges on the synergistic operation of these two components. However, the overall performance remains highly sensitive to the quality and relevance of the retrieved documents, as well as the generator's capacity to effectively discern and utilize pertinent information from potentially noisy or redundant contexts \cite{fan2024pf1}. Challenges such as irrelevant or insufficient retrievals can still lead to suboptimal generations, necessitating advanced strategies for corrective retrieval and context optimization \cite{yan202437z}. These inherent complexities drive continuous advancements aimed at enhancing both retrieval efficacy and the generator's contextual understanding, which will be explored in subsequent sections.