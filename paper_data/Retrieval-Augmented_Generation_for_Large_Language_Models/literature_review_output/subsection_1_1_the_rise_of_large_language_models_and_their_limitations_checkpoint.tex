\subsection{The Rise of Large Language Models and Their Limitations}

The advent of Large Language Models (LLMs) has marked a transformative era in artificial intelligence, showcasing unprecedented capabilities in natural language understanding and generation. These powerful generative AI systems, trained on vast corpora of text and code, have demonstrated remarkable proficiency in tasks ranging from complex question answering and summarization to creative content generation and code synthesis. However, despite their impressive performance, LLMs are inherently constrained by several critical limitations that significantly impact their reliability and trustworthiness, thereby underscoring the necessity for external knowledge augmentation mechanisms like Retrieval-Augmented Generation (RAG).

One of the most prominent limitations of LLMs is their propensity for **hallucination**, which refers to the generation of factually incorrect, nonsensical, or fabricated information presented as truth \cite{gao20238ea, chen2023nzb}. This issue arises because LLMs are trained to predict the most probable next token based on patterns in their training data, rather than possessing a true understanding of facts or the world. Consequently, when faced with queries outside their precise knowledge or when prompted ambiguously, they can confidently produce plausible-sounding but entirely false statements. For instance, an LLM might invent non-existent historical events, attribute quotes to the wrong individuals, or generate incorrect medical advice, posing significant risks in sensitive applications \cite{yan202437z}. This inherent tendency to hallucinate undermines the factual accuracy and trustworthiness of LLM outputs, making them unreliable for knowledge-intensive tasks.

Another significant challenge is the **knowledge cutoff problem**. LLMs' knowledge is static, being confined to the information present in their training datasets up to a specific point in time \cite{gao20238ea, chen2023nzb}. They lack the ability to access or incorporate real-time, up-to-date information from the internet or proprietary databases beyond their last training update. This means that LLMs cannot provide current news, recent scientific discoveries, or evolving policy changes, rendering them obsolete for dynamic information environments. For example, an LLM trained in 2022 would be unable to answer questions about events from 2023 or 2024, leading to outdated or incomplete responses. This limitation severely restricts their utility in applications requiring contemporary or rapidly changing information.

Furthermore, LLMs often suffer from a **lack of transparency in their reasoning processes** \cite{gao20238ea}. As complex neural networks, their internal mechanisms for arriving at an answer are largely opaque, making it difficult for human users to understand *how* a particular conclusion was reached or to verify the factual basis of a generated response. This "black box" nature hinders debugging, auditing, and building trust, especially in critical domains where explainability is paramount. When an LLM provides an incorrect answer, it is challenging to pinpoint whether the error stems from a misunderstanding of the query, a misinterpretation of internal knowledge, or a hallucination.

These inherent limitations of LLMs—hallucination, the knowledge cutoff, and lack of transparency—were recognized early in their development. They highlighted a critical need for mechanisms that could augment LLMs with external, up-to-date, and verifiable knowledge. This necessity directly paved the way for the development and widespread adoption of Retrieval-Augmented Generation (RAG) systems. RAG emerged as a promising paradigm to address these shortcomings by enabling LLMs to dynamically fetch relevant information from external knowledge bases during the generation process, thereby mitigating hallucinations, overcoming knowledge cutoffs, and offering a degree of verifiability by citing sources. Understanding these foundational limitations is crucial for appreciating the value and architectural evolution of RAG, as subsequent research has largely focused on refining how LLMs interact with and leverage external knowledge to overcome these challenges \cite{chen2023nzb, gao20238ea, yan202437z}. Despite the promise of RAG, the fundamental challenges of effectively integrating and reasoning over external knowledge, especially in the presence of noisy or irrelevant information, continue to drive ongoing research into more robust and intelligent augmentation strategies.